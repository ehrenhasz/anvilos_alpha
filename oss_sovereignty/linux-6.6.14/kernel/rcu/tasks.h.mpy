{
  "module_name": "tasks.h",
  "hash_id": "c581d38c06125a89d0d41be10460006321a7ca6ebffafb8217cbdaf0a0919daf",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/tasks.h",
  "human_readable_source": " \n \n\n#ifdef CONFIG_TASKS_RCU_GENERIC\n#include \"rcu_segcblist.h\"\n\n\n\n\n\nstruct rcu_tasks;\ntypedef void (*rcu_tasks_gp_func_t)(struct rcu_tasks *rtp);\ntypedef void (*pregp_func_t)(struct list_head *hop);\ntypedef void (*pertask_func_t)(struct task_struct *t, struct list_head *hop);\ntypedef void (*postscan_func_t)(struct list_head *hop);\ntypedef void (*holdouts_func_t)(struct list_head *hop, bool ndrpt, bool *frptp);\ntypedef void (*postgp_func_t)(struct rcu_tasks *rtp);\n\n \nstruct rcu_tasks_percpu {\n\tstruct rcu_segcblist cblist;\n\traw_spinlock_t __private lock;\n\tunsigned long rtp_jiffies;\n\tunsigned long rtp_n_lock_retries;\n\tstruct timer_list lazy_timer;\n\tunsigned int urgent_gp;\n\tstruct work_struct rtp_work;\n\tstruct irq_work rtp_irq_work;\n\tstruct rcu_head barrier_q_head;\n\tstruct list_head rtp_blkd_tasks;\n\tint cpu;\n\tstruct rcu_tasks *rtpp;\n};\n\n \nstruct rcu_tasks {\n\tstruct rcuwait cbs_wait;\n\traw_spinlock_t cbs_gbl_lock;\n\tstruct mutex tasks_gp_mutex;\n\tint gp_state;\n\tint gp_sleep;\n\tint init_fract;\n\tunsigned long gp_jiffies;\n\tunsigned long gp_start;\n\tunsigned long tasks_gp_seq;\n\tunsigned long n_ipis;\n\tunsigned long n_ipis_fails;\n\tstruct task_struct *kthread_ptr;\n\tunsigned long lazy_jiffies;\n\trcu_tasks_gp_func_t gp_func;\n\tpregp_func_t pregp_func;\n\tpertask_func_t pertask_func;\n\tpostscan_func_t postscan_func;\n\tholdouts_func_t holdouts_func;\n\tpostgp_func_t postgp_func;\n\tcall_rcu_func_t call_func;\n\tstruct rcu_tasks_percpu __percpu *rtpcpu;\n\tint percpu_enqueue_shift;\n\tint percpu_enqueue_lim;\n\tint percpu_dequeue_lim;\n\tunsigned long percpu_dequeue_gpseq;\n\tstruct mutex barrier_q_mutex;\n\tatomic_t barrier_q_count;\n\tstruct completion barrier_q_completion;\n\tunsigned long barrier_q_seq;\n\tchar *name;\n\tchar *kname;\n};\n\nstatic void call_rcu_tasks_iw_wakeup(struct irq_work *iwp);\n\n#define DEFINE_RCU_TASKS(rt_name, gp, call, n)\t\t\t\t\t\t\\\nstatic DEFINE_PER_CPU(struct rcu_tasks_percpu, rt_name ## __percpu) = {\t\t\t\\\n\t.lock = __RAW_SPIN_LOCK_UNLOCKED(rt_name ## __percpu.cbs_pcpu_lock),\t\t\\\n\t.rtp_irq_work = IRQ_WORK_INIT_HARD(call_rcu_tasks_iw_wakeup),\t\t\t\\\n};\t\t\t\t\t\t\t\t\t\t\t\\\nstatic struct rcu_tasks rt_name =\t\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\t\t\\\n\t.cbs_wait = __RCUWAIT_INITIALIZER(rt_name.wait),\t\t\t\t\\\n\t.cbs_gbl_lock = __RAW_SPIN_LOCK_UNLOCKED(rt_name.cbs_gbl_lock),\t\t\t\\\n\t.tasks_gp_mutex = __MUTEX_INITIALIZER(rt_name.tasks_gp_mutex),\t\t\t\\\n\t.gp_func = gp,\t\t\t\t\t\t\t\t\t\\\n\t.call_func = call,\t\t\t\t\t\t\t\t\\\n\t.rtpcpu = &rt_name ## __percpu,\t\t\t\t\t\t\t\\\n\t.lazy_jiffies = DIV_ROUND_UP(HZ, 4),\t\t\t\t\t\t\\\n\t.name = n,\t\t\t\t\t\t\t\t\t\\\n\t.percpu_enqueue_shift = order_base_2(CONFIG_NR_CPUS),\t\t\t\t\\\n\t.percpu_enqueue_lim = 1,\t\t\t\t\t\t\t\\\n\t.percpu_dequeue_lim = 1,\t\t\t\t\t\t\t\\\n\t.barrier_q_mutex = __MUTEX_INITIALIZER(rt_name.barrier_q_mutex),\t\t\\\n\t.barrier_q_seq = (0UL - 50UL) << RCU_SEQ_CTR_SHIFT,\t\t\t\t\\\n\t.kname = #rt_name,\t\t\t\t\t\t\t\t\\\n}\n\n#ifdef CONFIG_TASKS_RCU\n \nDEFINE_STATIC_SRCU(tasks_rcu_exit_srcu);\n\n \nstatic void tasks_rcu_exit_srcu_stall(struct timer_list *unused);\nstatic DEFINE_TIMER(tasks_rcu_exit_srcu_stall_timer, tasks_rcu_exit_srcu_stall);\n#endif\n\n \n#define RCU_TASK_IPI_DELAY (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB) ? HZ / 2 : 0)\nstatic int rcu_task_ipi_delay __read_mostly = RCU_TASK_IPI_DELAY;\nmodule_param(rcu_task_ipi_delay, int, 0644);\n\n \n#define RCU_TASK_BOOT_STALL_TIMEOUT (HZ * 30)\n#define RCU_TASK_STALL_TIMEOUT (HZ * 60 * 10)\nstatic int rcu_task_stall_timeout __read_mostly = RCU_TASK_STALL_TIMEOUT;\nmodule_param(rcu_task_stall_timeout, int, 0644);\n#define RCU_TASK_STALL_INFO (HZ * 10)\nstatic int rcu_task_stall_info __read_mostly = RCU_TASK_STALL_INFO;\nmodule_param(rcu_task_stall_info, int, 0644);\nstatic int rcu_task_stall_info_mult __read_mostly = 3;\nmodule_param(rcu_task_stall_info_mult, int, 0444);\n\nstatic int rcu_task_enqueue_lim __read_mostly = -1;\nmodule_param(rcu_task_enqueue_lim, int, 0444);\n\nstatic bool rcu_task_cb_adjust;\nstatic int rcu_task_contend_lim __read_mostly = 100;\nmodule_param(rcu_task_contend_lim, int, 0444);\nstatic int rcu_task_collapse_lim __read_mostly = 10;\nmodule_param(rcu_task_collapse_lim, int, 0444);\nstatic int rcu_task_lazy_lim __read_mostly = 32;\nmodule_param(rcu_task_lazy_lim, int, 0444);\n\n \n#define RTGS_INIT\t\t 0\n#define RTGS_WAIT_WAIT_CBS\t 1\n#define RTGS_WAIT_GP\t\t 2\n#define RTGS_PRE_WAIT_GP\t 3\n#define RTGS_SCAN_TASKLIST\t 4\n#define RTGS_POST_SCAN_TASKLIST\t 5\n#define RTGS_WAIT_SCAN_HOLDOUTS\t 6\n#define RTGS_SCAN_HOLDOUTS\t 7\n#define RTGS_POST_GP\t\t 8\n#define RTGS_WAIT_READERS\t 9\n#define RTGS_INVOKE_CBS\t\t10\n#define RTGS_WAIT_CBS\t\t11\n#ifndef CONFIG_TINY_RCU\nstatic const char * const rcu_tasks_gp_state_names[] = {\n\t\"RTGS_INIT\",\n\t\"RTGS_WAIT_WAIT_CBS\",\n\t\"RTGS_WAIT_GP\",\n\t\"RTGS_PRE_WAIT_GP\",\n\t\"RTGS_SCAN_TASKLIST\",\n\t\"RTGS_POST_SCAN_TASKLIST\",\n\t\"RTGS_WAIT_SCAN_HOLDOUTS\",\n\t\"RTGS_SCAN_HOLDOUTS\",\n\t\"RTGS_POST_GP\",\n\t\"RTGS_WAIT_READERS\",\n\t\"RTGS_INVOKE_CBS\",\n\t\"RTGS_WAIT_CBS\",\n};\n#endif  \n\n\n\n\n\nstatic void rcu_tasks_invoke_cbs_wq(struct work_struct *wp);\n\n \nstatic void set_tasks_gp_state(struct rcu_tasks *rtp, int newstate)\n{\n\trtp->gp_state = newstate;\n\trtp->gp_jiffies = jiffies;\n}\n\n#ifndef CONFIG_TINY_RCU\n \nstatic const char *tasks_gp_state_getname(struct rcu_tasks *rtp)\n{\n\tint i = data_race(rtp->gp_state); \n\tint j = READ_ONCE(i); \n\n\tif (j >= ARRAY_SIZE(rcu_tasks_gp_state_names))\n\t\treturn \"???\";\n\treturn rcu_tasks_gp_state_names[j];\n}\n#endif  \n\n\n\nstatic void cblist_init_generic(struct rcu_tasks *rtp)\n{\n\tint cpu;\n\tunsigned long flags;\n\tint lim;\n\tint shift;\n\n\tif (rcu_task_enqueue_lim < 0) {\n\t\trcu_task_enqueue_lim = 1;\n\t\trcu_task_cb_adjust = true;\n\t} else if (rcu_task_enqueue_lim == 0) {\n\t\trcu_task_enqueue_lim = 1;\n\t}\n\tlim = rcu_task_enqueue_lim;\n\n\tif (lim > nr_cpu_ids)\n\t\tlim = nr_cpu_ids;\n\tshift = ilog2(nr_cpu_ids / lim);\n\tif (((nr_cpu_ids - 1) >> shift) >= lim)\n\t\tshift++;\n\tWRITE_ONCE(rtp->percpu_enqueue_shift, shift);\n\tWRITE_ONCE(rtp->percpu_dequeue_lim, lim);\n\tsmp_store_release(&rtp->percpu_enqueue_lim, lim);\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rcu_tasks_percpu *rtpcp = per_cpu_ptr(rtp->rtpcpu, cpu);\n\n\t\tWARN_ON_ONCE(!rtpcp);\n\t\tif (cpu)\n\t\t\traw_spin_lock_init(&ACCESS_PRIVATE(rtpcp, lock));\n\t\tlocal_irq_save(flags);  \n\t\tif (rcu_segcblist_empty(&rtpcp->cblist))\n\t\t\trcu_segcblist_init(&rtpcp->cblist);\n\t\tlocal_irq_restore(flags);\n\t\tINIT_WORK(&rtpcp->rtp_work, rcu_tasks_invoke_cbs_wq);\n\t\trtpcp->cpu = cpu;\n\t\trtpcp->rtpp = rtp;\n\t\tif (!rtpcp->rtp_blkd_tasks.next)\n\t\t\tINIT_LIST_HEAD(&rtpcp->rtp_blkd_tasks);\n\t}\n\n\tpr_info(\"%s: Setting shift to %d and lim to %d rcu_task_cb_adjust=%d.\\n\", rtp->name,\n\t\t\tdata_race(rtp->percpu_enqueue_shift), data_race(rtp->percpu_enqueue_lim), rcu_task_cb_adjust);\n}\n\n\nstatic unsigned long rcu_tasks_lazy_time(struct rcu_tasks *rtp)\n{\n\treturn jiffies + rtp->lazy_jiffies;\n}\n\n\nstatic void call_rcu_tasks_generic_timer(struct timer_list *tlp)\n{\n\tunsigned long flags;\n\tbool needwake = false;\n\tstruct rcu_tasks *rtp;\n\tstruct rcu_tasks_percpu *rtpcp = from_timer(rtpcp, tlp, lazy_timer);\n\n\trtp = rtpcp->rtpp;\n\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\tif (!rcu_segcblist_empty(&rtpcp->cblist) && rtp->lazy_jiffies) {\n\t\tif (!rtpcp->urgent_gp)\n\t\t\trtpcp->urgent_gp = 1;\n\t\tneedwake = true;\n\t\tmod_timer(&rtpcp->lazy_timer, rcu_tasks_lazy_time(rtp));\n\t}\n\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\tif (needwake)\n\t\trcuwait_wake_up(&rtp->cbs_wait);\n}\n\n\nstatic void call_rcu_tasks_iw_wakeup(struct irq_work *iwp)\n{\n\tstruct rcu_tasks *rtp;\n\tstruct rcu_tasks_percpu *rtpcp = container_of(iwp, struct rcu_tasks_percpu, rtp_irq_work);\n\n\trtp = rtpcp->rtpp;\n\trcuwait_wake_up(&rtp->cbs_wait);\n}\n\n\nstatic void call_rcu_tasks_generic(struct rcu_head *rhp, rcu_callback_t func,\n\t\t\t\t   struct rcu_tasks *rtp)\n{\n\tint chosen_cpu;\n\tunsigned long flags;\n\tbool havekthread = smp_load_acquire(&rtp->kthread_ptr);\n\tint ideal_cpu;\n\tunsigned long j;\n\tbool needadjust = false;\n\tbool needwake;\n\tstruct rcu_tasks_percpu *rtpcp;\n\n\trhp->next = NULL;\n\trhp->func = func;\n\tlocal_irq_save(flags);\n\trcu_read_lock();\n\tideal_cpu = smp_processor_id() >> READ_ONCE(rtp->percpu_enqueue_shift);\n\tchosen_cpu = cpumask_next(ideal_cpu - 1, cpu_possible_mask);\n\trtpcp = per_cpu_ptr(rtp->rtpcpu, chosen_cpu);\n\tif (!raw_spin_trylock_rcu_node(rtpcp)) { \n\t\traw_spin_lock_rcu_node(rtpcp); \n\t\tj = jiffies;\n\t\tif (rtpcp->rtp_jiffies != j) {\n\t\t\trtpcp->rtp_jiffies = j;\n\t\t\trtpcp->rtp_n_lock_retries = 0;\n\t\t}\n\t\tif (rcu_task_cb_adjust && ++rtpcp->rtp_n_lock_retries > rcu_task_contend_lim &&\n\t\t    READ_ONCE(rtp->percpu_enqueue_lim) != nr_cpu_ids)\n\t\t\tneedadjust = true;  \n\t}\n\t\n\tif (WARN_ON_ONCE(!rcu_segcblist_is_enabled(&rtpcp->cblist)))\n\t\trcu_segcblist_init(&rtpcp->cblist);\n\tneedwake = (func == wakeme_after_rcu) ||\n\t\t   (rcu_segcblist_n_cbs(&rtpcp->cblist) == rcu_task_lazy_lim);\n\tif (havekthread && !needwake && !timer_pending(&rtpcp->lazy_timer)) {\n\t\tif (rtp->lazy_jiffies)\n\t\t\tmod_timer(&rtpcp->lazy_timer, rcu_tasks_lazy_time(rtp));\n\t\telse\n\t\t\tneedwake = rcu_segcblist_empty(&rtpcp->cblist);\n\t}\n\tif (needwake)\n\t\trtpcp->urgent_gp = 3;\n\trcu_segcblist_enqueue(&rtpcp->cblist, rhp);\n\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\tif (unlikely(needadjust)) {\n\t\traw_spin_lock_irqsave(&rtp->cbs_gbl_lock, flags);\n\t\tif (rtp->percpu_enqueue_lim != nr_cpu_ids) {\n\t\t\tWRITE_ONCE(rtp->percpu_enqueue_shift, 0);\n\t\t\tWRITE_ONCE(rtp->percpu_dequeue_lim, nr_cpu_ids);\n\t\t\tsmp_store_release(&rtp->percpu_enqueue_lim, nr_cpu_ids);\n\t\t\tpr_info(\"Switching %s to per-CPU callback queuing.\\n\", rtp->name);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rtp->cbs_gbl_lock, flags);\n\t}\n\trcu_read_unlock();\n\t \n\tif (needwake && READ_ONCE(rtp->kthread_ptr))\n\t\tirq_work_queue(&rtpcp->rtp_irq_work);\n}\n\n \nstatic void rcu_barrier_tasks_generic_cb(struct rcu_head *rhp)\n{\n\tstruct rcu_tasks *rtp;\n\tstruct rcu_tasks_percpu *rtpcp;\n\n\trtpcp = container_of(rhp, struct rcu_tasks_percpu, barrier_q_head);\n\trtp = rtpcp->rtpp;\n\tif (atomic_dec_and_test(&rtp->barrier_q_count))\n\t\tcomplete(&rtp->barrier_q_completion);\n}\n\n \n \nstatic void rcu_barrier_tasks_generic(struct rcu_tasks *rtp)\n{\n\tint cpu;\n\tunsigned long flags;\n\tstruct rcu_tasks_percpu *rtpcp;\n\tunsigned long s = rcu_seq_snap(&rtp->barrier_q_seq);\n\n\tmutex_lock(&rtp->barrier_q_mutex);\n\tif (rcu_seq_done(&rtp->barrier_q_seq, s)) {\n\t\tsmp_mb();\n\t\tmutex_unlock(&rtp->barrier_q_mutex);\n\t\treturn;\n\t}\n\trcu_seq_start(&rtp->barrier_q_seq);\n\tinit_completion(&rtp->barrier_q_completion);\n\tatomic_set(&rtp->barrier_q_count, 2);\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpu >= smp_load_acquire(&rtp->percpu_dequeue_lim))\n\t\t\tbreak;\n\t\trtpcp = per_cpu_ptr(rtp->rtpcpu, cpu);\n\t\trtpcp->barrier_q_head.func = rcu_barrier_tasks_generic_cb;\n\t\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\t\tif (rcu_segcblist_entrain(&rtpcp->cblist, &rtpcp->barrier_q_head))\n\t\t\tatomic_inc(&rtp->barrier_q_count);\n\t\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\t}\n\tif (atomic_sub_and_test(2, &rtp->barrier_q_count))\n\t\tcomplete(&rtp->barrier_q_completion);\n\twait_for_completion(&rtp->barrier_q_completion);\n\trcu_seq_end(&rtp->barrier_q_seq);\n\tmutex_unlock(&rtp->barrier_q_mutex);\n}\n\n \n \nstatic int rcu_tasks_need_gpcb(struct rcu_tasks *rtp)\n{\n\tint cpu;\n\tunsigned long flags;\n\tbool gpdone = poll_state_synchronize_rcu(rtp->percpu_dequeue_gpseq);\n\tlong n;\n\tlong ncbs = 0;\n\tlong ncbsnz = 0;\n\tint needgpcb = 0;\n\n\tfor (cpu = 0; cpu < smp_load_acquire(&rtp->percpu_dequeue_lim); cpu++) {\n\t\tstruct rcu_tasks_percpu *rtpcp = per_cpu_ptr(rtp->rtpcpu, cpu);\n\n\t\t \n\t\tif (!rcu_segcblist_n_cbs(&rtpcp->cblist))\n\t\t\tcontinue;\n\t\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\t\t \n\t\tn = rcu_segcblist_n_cbs(&rtpcp->cblist);\n\t\tif (n) {\n\t\t\tncbs += n;\n\t\t\tif (cpu > 0)\n\t\t\t\tncbsnz += n;\n\t\t}\n\t\trcu_segcblist_advance(&rtpcp->cblist, rcu_seq_current(&rtp->tasks_gp_seq));\n\t\t(void)rcu_segcblist_accelerate(&rtpcp->cblist, rcu_seq_snap(&rtp->tasks_gp_seq));\n\t\tif (rtpcp->urgent_gp > 0 && rcu_segcblist_pend_cbs(&rtpcp->cblist)) {\n\t\t\tif (rtp->lazy_jiffies)\n\t\t\t\trtpcp->urgent_gp--;\n\t\t\tneedgpcb |= 0x3;\n\t\t} else if (rcu_segcblist_empty(&rtpcp->cblist)) {\n\t\t\trtpcp->urgent_gp = 0;\n\t\t}\n\t\tif (rcu_segcblist_ready_cbs(&rtpcp->cblist))\n\t\t\tneedgpcb |= 0x1;\n\t\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\t}\n\n\t \n\t \n\t \n\t \n\t \n\t \n\t \n\tif (rcu_task_cb_adjust && ncbs <= rcu_task_collapse_lim) {\n\t\traw_spin_lock_irqsave(&rtp->cbs_gbl_lock, flags);\n\t\tif (rtp->percpu_enqueue_lim > 1) {\n\t\t\tWRITE_ONCE(rtp->percpu_enqueue_shift, order_base_2(nr_cpu_ids));\n\t\t\tsmp_store_release(&rtp->percpu_enqueue_lim, 1);\n\t\t\trtp->percpu_dequeue_gpseq = get_state_synchronize_rcu();\n\t\t\tgpdone = false;\n\t\t\tpr_info(\"Starting switch %s to CPU-0 callback queuing.\\n\", rtp->name);\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rtp->cbs_gbl_lock, flags);\n\t}\n\tif (rcu_task_cb_adjust && !ncbsnz && gpdone) {\n\t\traw_spin_lock_irqsave(&rtp->cbs_gbl_lock, flags);\n\t\tif (rtp->percpu_enqueue_lim < rtp->percpu_dequeue_lim) {\n\t\t\tWRITE_ONCE(rtp->percpu_dequeue_lim, 1);\n\t\t\tpr_info(\"Completing switch %s to CPU-0 callback queuing.\\n\", rtp->name);\n\t\t}\n\t\tif (rtp->percpu_dequeue_lim == 1) {\n\t\t\tfor (cpu = rtp->percpu_dequeue_lim; cpu < nr_cpu_ids; cpu++) {\n\t\t\t\tstruct rcu_tasks_percpu *rtpcp = per_cpu_ptr(rtp->rtpcpu, cpu);\n\n\t\t\t\tWARN_ON_ONCE(rcu_segcblist_n_cbs(&rtpcp->cblist));\n\t\t\t}\n\t\t}\n\t\traw_spin_unlock_irqrestore(&rtp->cbs_gbl_lock, flags);\n\t}\n\n\treturn needgpcb;\n}\n\n \nstatic void rcu_tasks_invoke_cbs(struct rcu_tasks *rtp, struct rcu_tasks_percpu *rtpcp)\n{\n\tint cpu;\n\tint cpunext;\n\tint cpuwq;\n\tunsigned long flags;\n\tint len;\n\tstruct rcu_head *rhp;\n\tstruct rcu_cblist rcl = RCU_CBLIST_INITIALIZER(rcl);\n\tstruct rcu_tasks_percpu *rtpcp_next;\n\n\tcpu = rtpcp->cpu;\n\tcpunext = cpu * 2 + 1;\n\tif (cpunext < smp_load_acquire(&rtp->percpu_dequeue_lim)) {\n\t\trtpcp_next = per_cpu_ptr(rtp->rtpcpu, cpunext);\n\t\tcpuwq = rcu_cpu_beenfullyonline(cpunext) ? cpunext : WORK_CPU_UNBOUND;\n\t\tqueue_work_on(cpuwq, system_wq, &rtpcp_next->rtp_work);\n\t\tcpunext++;\n\t\tif (cpunext < smp_load_acquire(&rtp->percpu_dequeue_lim)) {\n\t\t\trtpcp_next = per_cpu_ptr(rtp->rtpcpu, cpunext);\n\t\t\tcpuwq = rcu_cpu_beenfullyonline(cpunext) ? cpunext : WORK_CPU_UNBOUND;\n\t\t\tqueue_work_on(cpuwq, system_wq, &rtpcp_next->rtp_work);\n\t\t}\n\t}\n\n\tif (rcu_segcblist_empty(&rtpcp->cblist) || !cpu_possible(cpu))\n\t\treturn;\n\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\trcu_segcblist_advance(&rtpcp->cblist, rcu_seq_current(&rtp->tasks_gp_seq));\n\trcu_segcblist_extract_done_cbs(&rtpcp->cblist, &rcl);\n\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\tlen = rcl.len;\n\tfor (rhp = rcu_cblist_dequeue(&rcl); rhp; rhp = rcu_cblist_dequeue(&rcl)) {\n\t\tlocal_bh_disable();\n\t\trhp->func(rhp);\n\t\tlocal_bh_enable();\n\t\tcond_resched();\n\t}\n\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\trcu_segcblist_add_len(&rtpcp->cblist, -len);\n\t(void)rcu_segcblist_accelerate(&rtpcp->cblist, rcu_seq_snap(&rtp->tasks_gp_seq));\n\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n}\n\n \nstatic void rcu_tasks_invoke_cbs_wq(struct work_struct *wp)\n{\n\tstruct rcu_tasks *rtp;\n\tstruct rcu_tasks_percpu *rtpcp = container_of(wp, struct rcu_tasks_percpu, rtp_work);\n\n\trtp = rtpcp->rtpp;\n\trcu_tasks_invoke_cbs(rtp, rtpcp);\n}\n\n \nstatic void rcu_tasks_one_gp(struct rcu_tasks *rtp, bool midboot)\n{\n\tint needgpcb;\n\n\tmutex_lock(&rtp->tasks_gp_mutex);\n\n\t \n\tif (unlikely(midboot)) {\n\t\tneedgpcb = 0x2;\n\t} else {\n\t\tmutex_unlock(&rtp->tasks_gp_mutex);\n\t\tset_tasks_gp_state(rtp, RTGS_WAIT_CBS);\n\t\trcuwait_wait_event(&rtp->cbs_wait,\n\t\t\t\t   (needgpcb = rcu_tasks_need_gpcb(rtp)),\n\t\t\t\t   TASK_IDLE);\n\t\tmutex_lock(&rtp->tasks_gp_mutex);\n\t}\n\n\tif (needgpcb & 0x2) {\n\t\t \n\t\tset_tasks_gp_state(rtp, RTGS_WAIT_GP);\n\t\trtp->gp_start = jiffies;\n\t\trcu_seq_start(&rtp->tasks_gp_seq);\n\t\trtp->gp_func(rtp);\n\t\trcu_seq_end(&rtp->tasks_gp_seq);\n\t}\n\n\t \n\tset_tasks_gp_state(rtp, RTGS_INVOKE_CBS);\n\trcu_tasks_invoke_cbs(rtp, per_cpu_ptr(rtp->rtpcpu, 0));\n\tmutex_unlock(&rtp->tasks_gp_mutex);\n}\n\n \nstatic int __noreturn rcu_tasks_kthread(void *arg)\n{\n\tint cpu;\n\tstruct rcu_tasks *rtp = arg;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rcu_tasks_percpu *rtpcp = per_cpu_ptr(rtp->rtpcpu, cpu);\n\n\t\ttimer_setup(&rtpcp->lazy_timer, call_rcu_tasks_generic_timer, 0);\n\t\trtpcp->urgent_gp = 1;\n\t}\n\n\t \n\thousekeeping_affine(current, HK_TYPE_RCU);\n\tsmp_store_release(&rtp->kthread_ptr, current);  \n\n\t \n\tfor (;;) {\n\t\t \n\t\t \n\t\trcu_tasks_one_gp(rtp, false);\n\n\t\t \n\t\tschedule_timeout_idle(rtp->gp_sleep);\n\t}\n}\n\n \nstatic void synchronize_rcu_tasks_generic(struct rcu_tasks *rtp)\n{\n\t \n\tif (WARN_ONCE(rcu_scheduler_active == RCU_SCHEDULER_INACTIVE,\n\t\t\t \"synchronize_%s() called too soon\", rtp->name))\n\t\treturn;\n\n\t \n\tif (READ_ONCE(rtp->kthread_ptr)) {\n\t\twait_rcu_gp(rtp->call_func);\n\t\treturn;\n\t}\n\trcu_tasks_one_gp(rtp, true);\n}\n\n \nstatic void __init rcu_spawn_tasks_kthread_generic(struct rcu_tasks *rtp)\n{\n\tstruct task_struct *t;\n\n\tt = kthread_run(rcu_tasks_kthread, rtp, \"%s_kthread\", rtp->kname);\n\tif (WARN_ONCE(IS_ERR(t), \"%s: Could not start %s grace-period kthread, OOM is now expected behavior\\n\", __func__, rtp->name))\n\t\treturn;\n\tsmp_mb();  \n}\n\n#ifndef CONFIG_TINY_RCU\n\n \nstatic void __init rcu_tasks_bootup_oddness(void)\n{\n#if defined(CONFIG_TASKS_RCU) || defined(CONFIG_TASKS_TRACE_RCU)\n\tint rtsimc;\n\n\tif (rcu_task_stall_timeout != RCU_TASK_STALL_TIMEOUT)\n\t\tpr_info(\"\\tTasks-RCU CPU stall warnings timeout set to %d (rcu_task_stall_timeout).\\n\", rcu_task_stall_timeout);\n\trtsimc = clamp(rcu_task_stall_info_mult, 1, 10);\n\tif (rtsimc != rcu_task_stall_info_mult) {\n\t\tpr_info(\"\\tTasks-RCU CPU stall info multiplier clamped to %d (rcu_task_stall_info_mult).\\n\", rtsimc);\n\t\trcu_task_stall_info_mult = rtsimc;\n\t}\n#endif  \n#ifdef CONFIG_TASKS_RCU\n\tpr_info(\"\\tTrampoline variant of Tasks RCU enabled.\\n\");\n#endif  \n#ifdef CONFIG_TASKS_RUDE_RCU\n\tpr_info(\"\\tRude variant of Tasks RCU enabled.\\n\");\n#endif  \n#ifdef CONFIG_TASKS_TRACE_RCU\n\tpr_info(\"\\tTracing variant of Tasks RCU enabled.\\n\");\n#endif  \n}\n\n#endif  \n\n#ifndef CONFIG_TINY_RCU\n \nstatic void show_rcu_tasks_generic_gp_kthread(struct rcu_tasks *rtp, char *s)\n{\n\tint cpu;\n\tbool havecbs = false;\n\tbool haveurgent = false;\n\tbool haveurgentcbs = false;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct rcu_tasks_percpu *rtpcp = per_cpu_ptr(rtp->rtpcpu, cpu);\n\n\t\tif (!data_race(rcu_segcblist_empty(&rtpcp->cblist)))\n\t\t\thavecbs = true;\n\t\tif (data_race(rtpcp->urgent_gp))\n\t\t\thaveurgent = true;\n\t\tif (!data_race(rcu_segcblist_empty(&rtpcp->cblist)) && data_race(rtpcp->urgent_gp))\n\t\t\thaveurgentcbs = true;\n\t\tif (havecbs && haveurgent && haveurgentcbs)\n\t\t\tbreak;\n\t}\n\tpr_info(\"%s: %s(%d) since %lu g:%lu i:%lu/%lu %c%c%c%c l:%lu %s\\n\",\n\t\trtp->kname,\n\t\ttasks_gp_state_getname(rtp), data_race(rtp->gp_state),\n\t\tjiffies - data_race(rtp->gp_jiffies),\n\t\tdata_race(rcu_seq_current(&rtp->tasks_gp_seq)),\n\t\tdata_race(rtp->n_ipis_fails), data_race(rtp->n_ipis),\n\t\t\".k\"[!!data_race(rtp->kthread_ptr)],\n\t\t\".C\"[havecbs],\n\t\t\".u\"[haveurgent],\n\t\t\".U\"[haveurgentcbs],\n\t\trtp->lazy_jiffies,\n\t\ts);\n}\n#endif  \n\nstatic void exit_tasks_rcu_finish_trace(struct task_struct *t);\n\n#if defined(CONFIG_TASKS_RCU) || defined(CONFIG_TASKS_TRACE_RCU)\n\n \n \n \n\n \nstatic void rcu_tasks_wait_gp(struct rcu_tasks *rtp)\n{\n\tstruct task_struct *g;\n\tint fract;\n\tLIST_HEAD(holdouts);\n\tunsigned long j;\n\tunsigned long lastinfo;\n\tunsigned long lastreport;\n\tbool reported = false;\n\tint rtsi;\n\tstruct task_struct *t;\n\n\tset_tasks_gp_state(rtp, RTGS_PRE_WAIT_GP);\n\trtp->pregp_func(&holdouts);\n\n\t \n\tset_tasks_gp_state(rtp, RTGS_SCAN_TASKLIST);\n\tif (rtp->pertask_func) {\n\t\trcu_read_lock();\n\t\tfor_each_process_thread(g, t)\n\t\t\trtp->pertask_func(t, &holdouts);\n\t\trcu_read_unlock();\n\t}\n\n\tset_tasks_gp_state(rtp, RTGS_POST_SCAN_TASKLIST);\n\trtp->postscan_func(&holdouts);\n\n\t \n\tlastreport = jiffies;\n\tlastinfo = lastreport;\n\trtsi = READ_ONCE(rcu_task_stall_info);\n\n\t \n\tfract = rtp->init_fract;\n\n\twhile (!list_empty(&holdouts)) {\n\t\tktime_t exp;\n\t\tbool firstreport;\n\t\tbool needreport;\n\t\tint rtst;\n\n\t\t \n\t\tset_tasks_gp_state(rtp, RTGS_WAIT_SCAN_HOLDOUTS);\n\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\tschedule_timeout_idle(fract);\n\t\t} else {\n\t\t\texp = jiffies_to_nsecs(fract);\n\t\t\t__set_current_state(TASK_IDLE);\n\t\t\tschedule_hrtimeout_range(&exp, jiffies_to_nsecs(HZ / 2), HRTIMER_MODE_REL_HARD);\n\t\t}\n\n\t\tif (fract < HZ)\n\t\t\tfract++;\n\n\t\trtst = READ_ONCE(rcu_task_stall_timeout);\n\t\tneedreport = rtst > 0 && time_after(jiffies, lastreport + rtst);\n\t\tif (needreport) {\n\t\t\tlastreport = jiffies;\n\t\t\treported = true;\n\t\t}\n\t\tfirstreport = true;\n\t\tWARN_ON(signal_pending(current));\n\t\tset_tasks_gp_state(rtp, RTGS_SCAN_HOLDOUTS);\n\t\trtp->holdouts_func(&holdouts, needreport, &firstreport);\n\n\t\t \n\t\tj = jiffies;\n\t\tif (rtsi > 0 && !reported && time_after(j, lastinfo + rtsi)) {\n\t\t\tlastinfo = j;\n\t\t\trtsi = rtsi * rcu_task_stall_info_mult;\n\t\t\tpr_info(\"%s: %s grace period number %lu (since boot) is %lu jiffies old.\\n\",\n\t\t\t\t__func__, rtp->kname, rtp->tasks_gp_seq, j - rtp->gp_start);\n\t\t}\n\t}\n\n\tset_tasks_gp_state(rtp, RTGS_POST_GP);\n\trtp->postgp_func(rtp);\n}\n\n#endif  \n\n#ifdef CONFIG_TASKS_RCU\n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nstatic void rcu_tasks_pregp_step(struct list_head *hop)\n{\n\t \n\tsynchronize_rcu();\n}\n\n \nstatic bool rcu_tasks_is_holdout(struct task_struct *t)\n{\n\tint cpu;\n\n\t \n\tif (!READ_ONCE(t->on_rq))\n\t\treturn false;\n\n\t \n\tif (is_idle_task(t))\n\t\treturn false;\n\n\tcpu = task_cpu(t);\n\n\t \n\tif (t == idle_task(cpu) && !rcu_cpu_online(cpu))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic void rcu_tasks_pertask(struct task_struct *t, struct list_head *hop)\n{\n\tif (t != current && rcu_tasks_is_holdout(t)) {\n\t\tget_task_struct(t);\n\t\tt->rcu_tasks_nvcsw = READ_ONCE(t->nvcsw);\n\t\tWRITE_ONCE(t->rcu_tasks_holdout, true);\n\t\tlist_add(&t->rcu_tasks_holdout_list, hop);\n\t}\n}\n\n \nstatic void rcu_tasks_postscan(struct list_head *hop)\n{\n\tint rtsi = READ_ONCE(rcu_task_stall_info);\n\n\tif (!IS_ENABLED(CONFIG_TINY_RCU)) {\n\t\ttasks_rcu_exit_srcu_stall_timer.expires = jiffies + rtsi;\n\t\tadd_timer(&tasks_rcu_exit_srcu_stall_timer);\n\t}\n\n\t \n\tsynchronize_srcu(&tasks_rcu_exit_srcu);\n\n\tif (!IS_ENABLED(CONFIG_TINY_RCU))\n\t\tdel_timer_sync(&tasks_rcu_exit_srcu_stall_timer);\n}\n\n \nstatic void check_holdout_task(struct task_struct *t,\n\t\t\t       bool needreport, bool *firstreport)\n{\n\tint cpu;\n\n\tif (!READ_ONCE(t->rcu_tasks_holdout) ||\n\t    t->rcu_tasks_nvcsw != READ_ONCE(t->nvcsw) ||\n\t    !rcu_tasks_is_holdout(t) ||\n\t    (IS_ENABLED(CONFIG_NO_HZ_FULL) &&\n\t     !is_idle_task(t) && t->rcu_tasks_idle_cpu >= 0)) {\n\t\tWRITE_ONCE(t->rcu_tasks_holdout, false);\n\t\tlist_del_init(&t->rcu_tasks_holdout_list);\n\t\tput_task_struct(t);\n\t\treturn;\n\t}\n\trcu_request_urgent_qs_task(t);\n\tif (!needreport)\n\t\treturn;\n\tif (*firstreport) {\n\t\tpr_err(\"INFO: rcu_tasks detected stalls on tasks:\\n\");\n\t\t*firstreport = false;\n\t}\n\tcpu = task_cpu(t);\n\tpr_alert(\"%p: %c%c nvcsw: %lu/%lu holdout: %d idle_cpu: %d/%d\\n\",\n\t\t t, \".I\"[is_idle_task(t)],\n\t\t \"N.\"[cpu < 0 || !tick_nohz_full_cpu(cpu)],\n\t\t t->rcu_tasks_nvcsw, t->nvcsw, t->rcu_tasks_holdout,\n\t\t t->rcu_tasks_idle_cpu, cpu);\n\tsched_show_task(t);\n}\n\n \nstatic void check_all_holdout_tasks(struct list_head *hop,\n\t\t\t\t    bool needreport, bool *firstreport)\n{\n\tstruct task_struct *t, *t1;\n\n\tlist_for_each_entry_safe(t, t1, hop, rcu_tasks_holdout_list) {\n\t\tcheck_holdout_task(t, needreport, firstreport);\n\t\tcond_resched();\n\t}\n}\n\n \nstatic void rcu_tasks_postgp(struct rcu_tasks *rtp)\n{\n\t \n\tsynchronize_rcu();\n}\n\nvoid call_rcu_tasks(struct rcu_head *rhp, rcu_callback_t func);\nDEFINE_RCU_TASKS(rcu_tasks, rcu_tasks_wait_gp, call_rcu_tasks, \"RCU Tasks\");\n\nstatic void tasks_rcu_exit_srcu_stall(struct timer_list *unused)\n{\n#ifndef CONFIG_TINY_RCU\n\tint rtsi;\n\n\trtsi = READ_ONCE(rcu_task_stall_info);\n\tpr_info(\"%s: %s grace period number %lu (since boot) gp_state: %s is %lu jiffies old.\\n\",\n\t\t__func__, rcu_tasks.kname, rcu_tasks.tasks_gp_seq,\n\t\ttasks_gp_state_getname(&rcu_tasks), jiffies - rcu_tasks.gp_jiffies);\n\tpr_info(\"Please check any exiting tasks stuck between calls to exit_tasks_rcu_start() and exit_tasks_rcu_finish()\\n\");\n\ttasks_rcu_exit_srcu_stall_timer.expires = jiffies + rtsi;\n\tadd_timer(&tasks_rcu_exit_srcu_stall_timer);\n#endif \n}\n\n \nvoid call_rcu_tasks(struct rcu_head *rhp, rcu_callback_t func)\n{\n\tcall_rcu_tasks_generic(rhp, func, &rcu_tasks);\n}\nEXPORT_SYMBOL_GPL(call_rcu_tasks);\n\n \nvoid synchronize_rcu_tasks(void)\n{\n\tsynchronize_rcu_tasks_generic(&rcu_tasks);\n}\nEXPORT_SYMBOL_GPL(synchronize_rcu_tasks);\n\n \nvoid rcu_barrier_tasks(void)\n{\n\trcu_barrier_tasks_generic(&rcu_tasks);\n}\nEXPORT_SYMBOL_GPL(rcu_barrier_tasks);\n\nint rcu_tasks_lazy_ms = -1;\nmodule_param(rcu_tasks_lazy_ms, int, 0444);\n\nstatic int __init rcu_spawn_tasks_kthread(void)\n{\n\tcblist_init_generic(&rcu_tasks);\n\trcu_tasks.gp_sleep = HZ / 10;\n\trcu_tasks.init_fract = HZ / 10;\n\tif (rcu_tasks_lazy_ms >= 0)\n\t\trcu_tasks.lazy_jiffies = msecs_to_jiffies(rcu_tasks_lazy_ms);\n\trcu_tasks.pregp_func = rcu_tasks_pregp_step;\n\trcu_tasks.pertask_func = rcu_tasks_pertask;\n\trcu_tasks.postscan_func = rcu_tasks_postscan;\n\trcu_tasks.holdouts_func = check_all_holdout_tasks;\n\trcu_tasks.postgp_func = rcu_tasks_postgp;\n\trcu_spawn_tasks_kthread_generic(&rcu_tasks);\n\treturn 0;\n}\n\n#if !defined(CONFIG_TINY_RCU)\nvoid show_rcu_tasks_classic_gp_kthread(void)\n{\n\tshow_rcu_tasks_generic_gp_kthread(&rcu_tasks, \"\");\n}\nEXPORT_SYMBOL_GPL(show_rcu_tasks_classic_gp_kthread);\n#endif \n\nstruct task_struct *get_rcu_tasks_gp_kthread(void)\n{\n\treturn rcu_tasks.kthread_ptr;\n}\nEXPORT_SYMBOL_GPL(get_rcu_tasks_gp_kthread);\n\n \nvoid exit_tasks_rcu_start(void) __acquires(&tasks_rcu_exit_srcu)\n{\n\tcurrent->rcu_tasks_idx = __srcu_read_lock(&tasks_rcu_exit_srcu);\n}\n\n \nvoid exit_tasks_rcu_stop(void) __releases(&tasks_rcu_exit_srcu)\n{\n\tstruct task_struct *t = current;\n\n\t__srcu_read_unlock(&tasks_rcu_exit_srcu, t->rcu_tasks_idx);\n}\n\n \nvoid exit_tasks_rcu_finish(void)\n{\n\texit_tasks_rcu_stop();\n\texit_tasks_rcu_finish_trace(current);\n}\n\n#else  \nvoid exit_tasks_rcu_start(void) { }\nvoid exit_tasks_rcu_stop(void) { }\nvoid exit_tasks_rcu_finish(void) { exit_tasks_rcu_finish_trace(current); }\n#endif  \n\n#ifdef CONFIG_TASKS_RUDE_RCU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatic void rcu_tasks_be_rude(struct work_struct *work)\n{\n}\n\n\nstatic void rcu_tasks_rude_wait_gp(struct rcu_tasks *rtp)\n{\n\trtp->n_ipis += cpumask_weight(cpu_online_mask);\n\tschedule_on_each_cpu(rcu_tasks_be_rude);\n}\n\nvoid call_rcu_tasks_rude(struct rcu_head *rhp, rcu_callback_t func);\nDEFINE_RCU_TASKS(rcu_tasks_rude, rcu_tasks_rude_wait_gp, call_rcu_tasks_rude,\n\t\t \"RCU Tasks Rude\");\n\n \nvoid call_rcu_tasks_rude(struct rcu_head *rhp, rcu_callback_t func)\n{\n\tcall_rcu_tasks_generic(rhp, func, &rcu_tasks_rude);\n}\nEXPORT_SYMBOL_GPL(call_rcu_tasks_rude);\n\n \nvoid synchronize_rcu_tasks_rude(void)\n{\n\tsynchronize_rcu_tasks_generic(&rcu_tasks_rude);\n}\nEXPORT_SYMBOL_GPL(synchronize_rcu_tasks_rude);\n\n \nvoid rcu_barrier_tasks_rude(void)\n{\n\trcu_barrier_tasks_generic(&rcu_tasks_rude);\n}\nEXPORT_SYMBOL_GPL(rcu_barrier_tasks_rude);\n\nint rcu_tasks_rude_lazy_ms = -1;\nmodule_param(rcu_tasks_rude_lazy_ms, int, 0444);\n\nstatic int __init rcu_spawn_tasks_rude_kthread(void)\n{\n\tcblist_init_generic(&rcu_tasks_rude);\n\trcu_tasks_rude.gp_sleep = HZ / 10;\n\tif (rcu_tasks_rude_lazy_ms >= 0)\n\t\trcu_tasks_rude.lazy_jiffies = msecs_to_jiffies(rcu_tasks_rude_lazy_ms);\n\trcu_spawn_tasks_kthread_generic(&rcu_tasks_rude);\n\treturn 0;\n}\n\n#if !defined(CONFIG_TINY_RCU)\nvoid show_rcu_tasks_rude_gp_kthread(void)\n{\n\tshow_rcu_tasks_generic_gp_kthread(&rcu_tasks_rude, \"\");\n}\nEXPORT_SYMBOL_GPL(show_rcu_tasks_rude_gp_kthread);\n#endif \n\nstruct task_struct *get_rcu_tasks_rude_gp_kthread(void)\n{\n\treturn rcu_tasks_rude.kthread_ptr;\n}\nEXPORT_SYMBOL_GPL(get_rcu_tasks_rude_gp_kthread);\n\n#endif  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\nstatic struct lock_class_key rcu_lock_trace_key;\nstruct lockdep_map rcu_trace_lock_map =\n\tSTATIC_LOCKDEP_MAP_INIT(\"rcu_read_lock_trace\", &rcu_lock_trace_key);\nEXPORT_SYMBOL_GPL(rcu_trace_lock_map);\n#endif  \n\n#ifdef CONFIG_TASKS_TRACE_RCU\n\n\nstatic DEFINE_PER_CPU(bool, trc_ipi_to_cpu);\n\n\n\nstatic unsigned long n_heavy_reader_attempts;\nstatic unsigned long n_heavy_reader_updates;\nstatic unsigned long n_heavy_reader_ofl_updates;\nstatic unsigned long n_trc_holdouts;\n\nvoid call_rcu_tasks_trace(struct rcu_head *rhp, rcu_callback_t func);\nDEFINE_RCU_TASKS(rcu_tasks_trace, rcu_tasks_wait_gp, call_rcu_tasks_trace,\n\t\t \"RCU Tasks Trace\");\n\n \nstatic u8 rcu_ld_need_qs(struct task_struct *t)\n{\n\tsmp_mb(); \n\treturn smp_load_acquire(&t->trc_reader_special.b.need_qs);\n}\n\n \nstatic void rcu_st_need_qs(struct task_struct *t, u8 v)\n{\n\tsmp_store_release(&t->trc_reader_special.b.need_qs, v);\n\tsmp_mb(); \n}\n\n \nu8 rcu_trc_cmpxchg_need_qs(struct task_struct *t, u8 old, u8 new)\n{\n\tunion rcu_special ret;\n\tunion rcu_special trs_old = READ_ONCE(t->trc_reader_special);\n\tunion rcu_special trs_new = trs_old;\n\n\tif (trs_old.b.need_qs != old)\n\t\treturn trs_old.b.need_qs;\n\ttrs_new.b.need_qs = new;\n\tret.s = cmpxchg(&t->trc_reader_special.s, trs_old.s, trs_new.s);\n\treturn ret.b.need_qs;\n}\nEXPORT_SYMBOL_GPL(rcu_trc_cmpxchg_need_qs);\n\n \nvoid rcu_read_unlock_trace_special(struct task_struct *t)\n{\n\tunsigned long flags;\n\tstruct rcu_tasks_percpu *rtpcp;\n\tunion rcu_special trs;\n\n\t\n\tsmp_mb(); \n\ttrs = smp_load_acquire(&t->trc_reader_special);\n\n\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB) && t->trc_reader_special.b.need_mb)\n\t\tsmp_mb(); \n\t\n\tif (trs.b.need_qs == (TRC_NEED_QS_CHECKED | TRC_NEED_QS)) {\n\t\tu8 result = rcu_trc_cmpxchg_need_qs(t, TRC_NEED_QS_CHECKED | TRC_NEED_QS,\n\t\t\t\t\t\t       TRC_NEED_QS_CHECKED);\n\n\t\tWARN_ONCE(result != trs.b.need_qs, \"%s: result = %d\", __func__, result);\n\t}\n\tif (trs.b.blocked) {\n\t\trtpcp = per_cpu_ptr(rcu_tasks_trace.rtpcpu, t->trc_blkd_cpu);\n\t\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\t\tlist_del_init(&t->trc_blkd_node);\n\t\tWRITE_ONCE(t->trc_reader_special.b.blocked, false);\n\t\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\t}\n\tWRITE_ONCE(t->trc_reader_nesting, 0);\n}\nEXPORT_SYMBOL_GPL(rcu_read_unlock_trace_special);\n\n \nvoid rcu_tasks_trace_qs_blkd(struct task_struct *t)\n{\n\tunsigned long flags;\n\tstruct rcu_tasks_percpu *rtpcp;\n\n\tlocal_irq_save(flags);\n\trtpcp = this_cpu_ptr(rcu_tasks_trace.rtpcpu);\n\traw_spin_lock_rcu_node(rtpcp);  \n\tt->trc_blkd_cpu = smp_processor_id();\n\tif (!rtpcp->rtp_blkd_tasks.next)\n\t\tINIT_LIST_HEAD(&rtpcp->rtp_blkd_tasks);\n\tlist_add(&t->trc_blkd_node, &rtpcp->rtp_blkd_tasks);\n\tWRITE_ONCE(t->trc_reader_special.b.blocked, true);\n\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n}\nEXPORT_SYMBOL_GPL(rcu_tasks_trace_qs_blkd);\n\n \nstatic void trc_add_holdout(struct task_struct *t, struct list_head *bhp)\n{\n\tif (list_empty(&t->trc_holdout_list)) {\n\t\tget_task_struct(t);\n\t\tlist_add(&t->trc_holdout_list, bhp);\n\t\tn_trc_holdouts++;\n\t}\n}\n\n \nstatic void trc_del_holdout(struct task_struct *t)\n{\n\tif (!list_empty(&t->trc_holdout_list)) {\n\t\tlist_del_init(&t->trc_holdout_list);\n\t\tput_task_struct(t);\n\t\tn_trc_holdouts--;\n\t}\n}\n\n \nstatic void trc_read_check_handler(void *t_in)\n{\n\tint nesting;\n\tstruct task_struct *t = current;\n\tstruct task_struct *texp = t_in;\n\n\t \n\tif (unlikely(texp != t))\n\t\tgoto reset_ipi;  \n\n\t \n\t \n\tnesting = READ_ONCE(t->trc_reader_nesting);\n\tif (likely(!nesting)) {\n\t\trcu_trc_cmpxchg_need_qs(t, 0, TRC_NEED_QS_CHECKED);\n\t\tgoto reset_ipi;\n\t}\n\t \n\tif (unlikely(nesting < 0))\n\t\tgoto reset_ipi;\n\n\t \n\t \n\t \n\trcu_trc_cmpxchg_need_qs(t, 0, TRC_NEED_QS | TRC_NEED_QS_CHECKED);\n\nreset_ipi:\n\t \n\t \n\t \n\tsmp_store_release(per_cpu_ptr(&trc_ipi_to_cpu, smp_processor_id()), false);  \n\tsmp_store_release(&texp->trc_ipi_to_cpu, -1);  \n}\n\n \nstatic int trc_inspect_reader(struct task_struct *t, void *bhp_in)\n{\n\tstruct list_head *bhp = bhp_in;\n\tint cpu = task_cpu(t);\n\tint nesting;\n\tbool ofl = cpu_is_offline(cpu);\n\n\tif (task_curr(t) && !ofl) {\n\t\t \n\t\tif (!IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\t \n\t\t \n\t\tn_heavy_reader_attempts++;\n\t\t \n\t\tif (!rcu_dynticks_zero_in_eqs(cpu, &t->trc_reader_nesting))\n\t\t\treturn -EINVAL;  \n\t\tn_heavy_reader_updates++;\n\t\tnesting = 0;\n\t} else {\n\t\t \n\t\tnesting = t->trc_reader_nesting;\n\t\tWARN_ON_ONCE(ofl && task_curr(t) && (t != idle_task(task_cpu(t))));\n\t\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB) && ofl)\n\t\t\tn_heavy_reader_ofl_updates++;\n\t}\n\n\t \n\t \n\t \n\tif (!nesting) {\n\t\trcu_trc_cmpxchg_need_qs(t, 0, TRC_NEED_QS_CHECKED);\n\t\treturn 0;   \n\t}\n\tif (nesting < 0)\n\t\treturn -EINVAL;  \n\n\t \n\t \n\t \n\tif (!rcu_trc_cmpxchg_need_qs(t, 0, TRC_NEED_QS | TRC_NEED_QS_CHECKED))\n\t\ttrc_add_holdout(t, bhp);\n\treturn 0;\n}\n\n \nstatic void trc_wait_for_one_reader(struct task_struct *t,\n\t\t\t\t    struct list_head *bhp)\n{\n\tint cpu;\n\n\t \n\tif (smp_load_acquire(&t->trc_ipi_to_cpu) != -1)  \n\t\treturn;\n\n\t \n\tif (t == current) {\n\t\trcu_trc_cmpxchg_need_qs(t, 0, TRC_NEED_QS_CHECKED);\n\t\tWARN_ON_ONCE(READ_ONCE(t->trc_reader_nesting));\n\t\treturn;\n\t}\n\n\t \n\tget_task_struct(t);\n\tif (!task_call_func(t, trc_inspect_reader, bhp)) {\n\t\tput_task_struct(t);\n\t\treturn;\n\t}\n\tput_task_struct(t);\n\n\t \n\t \n\t \n\t \n\t \n\n\t \n\ttrc_add_holdout(t, bhp);\n\tif (task_curr(t) &&\n\t    time_after(jiffies + 1, rcu_tasks_trace.gp_start + rcu_task_ipi_delay)) {\n\t\t \n\t\tcpu = task_cpu(t);\n\n\t\t \n\t\tif (per_cpu(trc_ipi_to_cpu, cpu) || t->trc_ipi_to_cpu >= 0)\n\t\t\treturn;\n\n\t\tper_cpu(trc_ipi_to_cpu, cpu) = true;\n\t\tt->trc_ipi_to_cpu = cpu;\n\t\trcu_tasks_trace.n_ipis++;\n\t\tif (smp_call_function_single(cpu, trc_read_check_handler, t, 0)) {\n\t\t\t \n\t\t\t \n\t\t\tWARN_ONCE(1, \"%s():  smp_call_function_single() failed for CPU: %d\\n\",\n\t\t\t\t  __func__, cpu);\n\t\t\trcu_tasks_trace.n_ipis_fails++;\n\t\t\tper_cpu(trc_ipi_to_cpu, cpu) = false;\n\t\t\tt->trc_ipi_to_cpu = -1;\n\t\t}\n\t}\n}\n\n \nstatic bool rcu_tasks_trace_pertask_prep(struct task_struct *t, bool notself)\n{\n\t \n\t \n\t \n\t \n\tif (unlikely(t == NULL) || (t == current && notself) || !list_empty(&t->trc_holdout_list))\n\t\treturn false;\n\n\trcu_st_need_qs(t, 0);\n\tt->trc_ipi_to_cpu = -1;\n\treturn true;\n}\n\n \nstatic void rcu_tasks_trace_pertask(struct task_struct *t, struct list_head *hop)\n{\n\tif (rcu_tasks_trace_pertask_prep(t, true))\n\t\ttrc_wait_for_one_reader(t, hop);\n}\n\n \nstatic void rcu_tasks_trace_pregp_step(struct list_head *hop)\n{\n\tLIST_HEAD(blkd_tasks);\n\tint cpu;\n\tunsigned long flags;\n\tstruct rcu_tasks_percpu *rtpcp;\n\tstruct task_struct *t;\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tWARN_ON_ONCE(per_cpu(trc_ipi_to_cpu, cpu));\n\n\t\n\t\n\t\n\tcpus_read_lock();\n\n\t\n\t\n\tfor_each_online_cpu(cpu) {\n\t\trcu_read_lock();\n\t\tt = cpu_curr_snapshot(cpu);\n\t\tif (rcu_tasks_trace_pertask_prep(t, true))\n\t\t\ttrc_add_holdout(t, hop);\n\t\trcu_read_unlock();\n\t\tcond_resched_tasks_rcu_qs();\n\t}\n\n\t\n\t\n\t\n\tfor_each_possible_cpu(cpu) {\n\t\trtpcp = per_cpu_ptr(rcu_tasks_trace.rtpcpu, cpu);\n\t\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\t\tlist_splice_init(&rtpcp->rtp_blkd_tasks, &blkd_tasks);\n\t\twhile (!list_empty(&blkd_tasks)) {\n\t\t\trcu_read_lock();\n\t\t\tt = list_first_entry(&blkd_tasks, struct task_struct, trc_blkd_node);\n\t\t\tlist_del_init(&t->trc_blkd_node);\n\t\t\tlist_add(&t->trc_blkd_node, &rtpcp->rtp_blkd_tasks);\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\t\t\trcu_tasks_trace_pertask(t, hop);\n\t\t\trcu_read_unlock();\n\t\t\traw_spin_lock_irqsave_rcu_node(rtpcp, flags);\n\t\t}\n\t\traw_spin_unlock_irqrestore_rcu_node(rtpcp, flags);\n\t\tcond_resched_tasks_rcu_qs();\n\t}\n\n\t\n\tcpus_read_unlock();\n}\n\n \nstatic void rcu_tasks_trace_postscan(struct list_head *hop)\n{\n\t\n\t\n\n\t\n\tsynchronize_rcu();\n\t\n\t\n}\n\n \nstruct trc_stall_chk_rdr {\n\tint nesting;\n\tint ipi_to_cpu;\n\tu8 needqs;\n};\n\nstatic int trc_check_slow_task(struct task_struct *t, void *arg)\n{\n\tstruct trc_stall_chk_rdr *trc_rdrp = arg;\n\n\tif (task_curr(t) && cpu_online(task_cpu(t)))\n\t\treturn false; \n\ttrc_rdrp->nesting = READ_ONCE(t->trc_reader_nesting);\n\ttrc_rdrp->ipi_to_cpu = READ_ONCE(t->trc_ipi_to_cpu);\n\ttrc_rdrp->needqs = rcu_ld_need_qs(t);\n\treturn true;\n}\n\n \nstatic void show_stalled_task_trace(struct task_struct *t, bool *firstreport)\n{\n\tint cpu;\n\tstruct trc_stall_chk_rdr trc_rdr;\n\tbool is_idle_tsk = is_idle_task(t);\n\n\tif (*firstreport) {\n\t\tpr_err(\"INFO: rcu_tasks_trace detected stalls on tasks:\\n\");\n\t\t*firstreport = false;\n\t}\n\tcpu = task_cpu(t);\n\tif (!task_call_func(t, trc_check_slow_task, &trc_rdr))\n\t\tpr_alert(\"P%d: %c%c\\n\",\n\t\t\t t->pid,\n\t\t\t \".I\"[t->trc_ipi_to_cpu >= 0],\n\t\t\t \".i\"[is_idle_tsk]);\n\telse\n\t\tpr_alert(\"P%d: %c%c%c%c nesting: %d%c%c cpu: %d%s\\n\",\n\t\t\t t->pid,\n\t\t\t \".I\"[trc_rdr.ipi_to_cpu >= 0],\n\t\t\t \".i\"[is_idle_tsk],\n\t\t\t \".N\"[cpu >= 0 && tick_nohz_full_cpu(cpu)],\n\t\t\t \".B\"[!!data_race(t->trc_reader_special.b.blocked)],\n\t\t\t trc_rdr.nesting,\n\t\t\t \" !CN\"[trc_rdr.needqs & 0x3],\n\t\t\t \" ?\"[trc_rdr.needqs > 0x3],\n\t\t\t cpu, cpu_online(cpu) ? \"\" : \"(offline)\");\n\tsched_show_task(t);\n}\n\n \nstatic void show_stalled_ipi_trace(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tif (per_cpu(trc_ipi_to_cpu, cpu))\n\t\t\tpr_alert(\"\\tIPI outstanding to CPU %d\\n\", cpu);\n}\n\n \nstatic void check_all_holdout_tasks_trace(struct list_head *hop,\n\t\t\t\t\t  bool needreport, bool *firstreport)\n{\n\tstruct task_struct *g, *t;\n\n\t\n\tcpus_read_lock();\n\n\tlist_for_each_entry_safe(t, g, hop, trc_holdout_list) {\n\t\t\n\t\tif (READ_ONCE(t->trc_ipi_to_cpu) == -1 &&\n\t\t    !(rcu_ld_need_qs(t) & TRC_NEED_QS_CHECKED))\n\t\t\ttrc_wait_for_one_reader(t, hop);\n\n\t\t\n\t\tif (smp_load_acquire(&t->trc_ipi_to_cpu) == -1 &&\n\t\t    rcu_ld_need_qs(t) == TRC_NEED_QS_CHECKED)\n\t\t\ttrc_del_holdout(t);\n\t\telse if (needreport)\n\t\t\tshow_stalled_task_trace(t, firstreport);\n\t\tcond_resched_tasks_rcu_qs();\n\t}\n\n\t\n\tcpus_read_unlock();\n\n\tif (needreport) {\n\t\tif (*firstreport)\n\t\t\tpr_err(\"INFO: rcu_tasks_trace detected stalls? (Late IPI?)\\n\");\n\t\tshow_stalled_ipi_trace();\n\t}\n}\n\nstatic void rcu_tasks_trace_empty_fn(void *unused)\n{\n}\n\n \nstatic void rcu_tasks_trace_postgp(struct rcu_tasks *rtp)\n{\n\tint cpu;\n\n\t\n\t\n\t\n\t\n\t\n\tfor_each_online_cpu(cpu)\n\t\tif (WARN_ON_ONCE(smp_load_acquire(per_cpu_ptr(&trc_ipi_to_cpu, cpu))))\n\t\t\tsmp_call_function_single(cpu, rcu_tasks_trace_empty_fn, NULL, 1);\n\n\tsmp_mb(); \n\t\t  \n}\n\n \nstatic void exit_tasks_rcu_finish_trace(struct task_struct *t)\n{\n\tunion rcu_special trs = READ_ONCE(t->trc_reader_special);\n\n\trcu_trc_cmpxchg_need_qs(t, 0, TRC_NEED_QS_CHECKED);\n\tWARN_ON_ONCE(READ_ONCE(t->trc_reader_nesting));\n\tif (WARN_ON_ONCE(rcu_ld_need_qs(t) & TRC_NEED_QS || trs.b.blocked))\n\t\trcu_read_unlock_trace_special(t);\n\telse\n\t\tWRITE_ONCE(t->trc_reader_nesting, 0);\n}\n\n \nvoid call_rcu_tasks_trace(struct rcu_head *rhp, rcu_callback_t func)\n{\n\tcall_rcu_tasks_generic(rhp, func, &rcu_tasks_trace);\n}\nEXPORT_SYMBOL_GPL(call_rcu_tasks_trace);\n\n \nvoid synchronize_rcu_tasks_trace(void)\n{\n\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_trace_lock_map), \"Illegal synchronize_rcu_tasks_trace() in RCU Tasks Trace read-side critical section\");\n\tsynchronize_rcu_tasks_generic(&rcu_tasks_trace);\n}\nEXPORT_SYMBOL_GPL(synchronize_rcu_tasks_trace);\n\n \nvoid rcu_barrier_tasks_trace(void)\n{\n\trcu_barrier_tasks_generic(&rcu_tasks_trace);\n}\nEXPORT_SYMBOL_GPL(rcu_barrier_tasks_trace);\n\nint rcu_tasks_trace_lazy_ms = -1;\nmodule_param(rcu_tasks_trace_lazy_ms, int, 0444);\n\nstatic int __init rcu_spawn_tasks_trace_kthread(void)\n{\n\tcblist_init_generic(&rcu_tasks_trace);\n\tif (IS_ENABLED(CONFIG_TASKS_TRACE_RCU_READ_MB)) {\n\t\trcu_tasks_trace.gp_sleep = HZ / 10;\n\t\trcu_tasks_trace.init_fract = HZ / 10;\n\t} else {\n\t\trcu_tasks_trace.gp_sleep = HZ / 200;\n\t\tif (rcu_tasks_trace.gp_sleep <= 0)\n\t\t\trcu_tasks_trace.gp_sleep = 1;\n\t\trcu_tasks_trace.init_fract = HZ / 200;\n\t\tif (rcu_tasks_trace.init_fract <= 0)\n\t\t\trcu_tasks_trace.init_fract = 1;\n\t}\n\tif (rcu_tasks_trace_lazy_ms >= 0)\n\t\trcu_tasks_trace.lazy_jiffies = msecs_to_jiffies(rcu_tasks_trace_lazy_ms);\n\trcu_tasks_trace.pregp_func = rcu_tasks_trace_pregp_step;\n\trcu_tasks_trace.postscan_func = rcu_tasks_trace_postscan;\n\trcu_tasks_trace.holdouts_func = check_all_holdout_tasks_trace;\n\trcu_tasks_trace.postgp_func = rcu_tasks_trace_postgp;\n\trcu_spawn_tasks_kthread_generic(&rcu_tasks_trace);\n\treturn 0;\n}\n\n#if !defined(CONFIG_TINY_RCU)\nvoid show_rcu_tasks_trace_gp_kthread(void)\n{\n\tchar buf[64];\n\n\tsprintf(buf, \"N%lu h:%lu/%lu/%lu\",\n\t\tdata_race(n_trc_holdouts),\n\t\tdata_race(n_heavy_reader_ofl_updates),\n\t\tdata_race(n_heavy_reader_updates),\n\t\tdata_race(n_heavy_reader_attempts));\n\tshow_rcu_tasks_generic_gp_kthread(&rcu_tasks_trace, buf);\n}\nEXPORT_SYMBOL_GPL(show_rcu_tasks_trace_gp_kthread);\n#endif \n\nstruct task_struct *get_rcu_tasks_trace_gp_kthread(void)\n{\n\treturn rcu_tasks_trace.kthread_ptr;\n}\nEXPORT_SYMBOL_GPL(get_rcu_tasks_trace_gp_kthread);\n\n#else  \nstatic void exit_tasks_rcu_finish_trace(struct task_struct *t) { }\n#endif  \n\n#ifndef CONFIG_TINY_RCU\nvoid show_rcu_tasks_gp_kthreads(void)\n{\n\tshow_rcu_tasks_classic_gp_kthread();\n\tshow_rcu_tasks_rude_gp_kthread();\n\tshow_rcu_tasks_trace_gp_kthread();\n}\n#endif  \n\n#ifdef CONFIG_PROVE_RCU\nstruct rcu_tasks_test_desc {\n\tstruct rcu_head rh;\n\tconst char *name;\n\tbool notrun;\n\tunsigned long runstart;\n};\n\nstatic struct rcu_tasks_test_desc tests[] = {\n\t{\n\t\t.name = \"call_rcu_tasks()\",\n\t\t \n\t\t.notrun = IS_ENABLED(CONFIG_TASKS_RCU),\n\t},\n\t{\n\t\t.name = \"call_rcu_tasks_rude()\",\n\t\t \n\t\t.notrun = IS_ENABLED(CONFIG_TASKS_RUDE_RCU),\n\t},\n\t{\n\t\t.name = \"call_rcu_tasks_trace()\",\n\t\t \n\t\t.notrun = IS_ENABLED(CONFIG_TASKS_TRACE_RCU)\n\t}\n};\n\nstatic void test_rcu_tasks_callback(struct rcu_head *rhp)\n{\n\tstruct rcu_tasks_test_desc *rttd =\n\t\tcontainer_of(rhp, struct rcu_tasks_test_desc, rh);\n\n\tpr_info(\"Callback from %s invoked.\\n\", rttd->name);\n\n\trttd->notrun = false;\n}\n\nstatic void rcu_tasks_initiate_self_tests(void)\n{\n\tpr_info(\"Running RCU-tasks wait API self tests\\n\");\n#ifdef CONFIG_TASKS_RCU\n\ttests[0].runstart = jiffies;\n\tsynchronize_rcu_tasks();\n\tcall_rcu_tasks(&tests[0].rh, test_rcu_tasks_callback);\n#endif\n\n#ifdef CONFIG_TASKS_RUDE_RCU\n\ttests[1].runstart = jiffies;\n\tsynchronize_rcu_tasks_rude();\n\tcall_rcu_tasks_rude(&tests[1].rh, test_rcu_tasks_callback);\n#endif\n\n#ifdef CONFIG_TASKS_TRACE_RCU\n\ttests[2].runstart = jiffies;\n\tsynchronize_rcu_tasks_trace();\n\tcall_rcu_tasks_trace(&tests[2].rh, test_rcu_tasks_callback);\n#endif\n}\n\n \nstatic int rcu_tasks_verify_self_tests(void)\n{\n\tint ret = 0;\n\tint i;\n\tunsigned long bst = rcu_task_stall_timeout;\n\n\tif (bst <= 0 || bst > RCU_TASK_BOOT_STALL_TIMEOUT)\n\t\tbst = RCU_TASK_BOOT_STALL_TIMEOUT;\n\tfor (i = 0; i < ARRAY_SIZE(tests); i++) {\n\t\twhile (tests[i].notrun) {\t\t\n\t\t\tif (time_after(jiffies, tests[i].runstart + bst)) {\n\t\t\t\tpr_err(\"%s has failed boot-time tests.\\n\", tests[i].name);\n\t\t\t\tret = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tWARN_ON(ret < 0);\n\n\treturn ret;\n}\n\n \nstatic struct delayed_work rcu_tasks_verify_work;\nstatic void rcu_tasks_verify_work_fn(struct work_struct *work __maybe_unused)\n{\n\tint ret = rcu_tasks_verify_self_tests();\n\n\tif (ret <= 0)\n\t\treturn;\n\n\t \n\tschedule_delayed_work(&rcu_tasks_verify_work, HZ);\n}\n\nstatic int rcu_tasks_verify_schedule_work(void)\n{\n\tINIT_DELAYED_WORK(&rcu_tasks_verify_work, rcu_tasks_verify_work_fn);\n\trcu_tasks_verify_work_fn(NULL);\n\treturn 0;\n}\nlate_initcall(rcu_tasks_verify_schedule_work);\n#else  \nstatic void rcu_tasks_initiate_self_tests(void) { }\n#endif  \n\nvoid __init rcu_init_tasks_generic(void)\n{\n#ifdef CONFIG_TASKS_RCU\n\trcu_spawn_tasks_kthread();\n#endif\n\n#ifdef CONFIG_TASKS_RUDE_RCU\n\trcu_spawn_tasks_rude_kthread();\n#endif\n\n#ifdef CONFIG_TASKS_TRACE_RCU\n\trcu_spawn_tasks_trace_kthread();\n#endif\n\n\t\n\trcu_tasks_initiate_self_tests();\n}\n\n#else  \nstatic inline void rcu_tasks_bootup_oddness(void) {}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}