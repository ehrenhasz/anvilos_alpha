{
  "module_name": "update.c",
  "hash_id": "c8b8ce0ec1b27e4948a6be4d9b16b87e04ac9d0cd235cf0a73b929dae3f76958",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/update.c",
  "human_readable_source": "\n \n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/interrupt.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/debug.h>\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/percpu.h>\n#include <linux/notifier.h>\n#include <linux/cpu.h>\n#include <linux/mutex.h>\n#include <linux/export.h>\n#include <linux/hardirq.h>\n#include <linux/delay.h>\n#include <linux/moduleparam.h>\n#include <linux/kthread.h>\n#include <linux/tick.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/sched/isolation.h>\n#include <linux/kprobes.h>\n#include <linux/slab.h>\n#include <linux/irq_work.h>\n#include <linux/rcupdate_trace.h>\n\n#define CREATE_TRACE_POINTS\n\n#include \"rcu.h\"\n\n#ifdef MODULE_PARAM_PREFIX\n#undef MODULE_PARAM_PREFIX\n#endif\n#define MODULE_PARAM_PREFIX \"rcupdate.\"\n\n#ifndef CONFIG_TINY_RCU\nmodule_param(rcu_expedited, int, 0444);\nmodule_param(rcu_normal, int, 0444);\nstatic int rcu_normal_after_boot = IS_ENABLED(CONFIG_PREEMPT_RT);\n#if !defined(CONFIG_PREEMPT_RT) || defined(CONFIG_NO_HZ_FULL)\nmodule_param(rcu_normal_after_boot, int, 0444);\n#endif\n#endif  \n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n \nstatic bool rcu_read_lock_held_common(bool *ret)\n{\n\tif (!debug_lockdep_rcu_enabled()) {\n\t\t*ret = true;\n\t\treturn true;\n\t}\n\tif (!rcu_is_watching()) {\n\t\t*ret = false;\n\t\treturn true;\n\t}\n\tif (!rcu_lockdep_current_cpu_online()) {\n\t\t*ret = false;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nint rcu_read_lock_sched_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\treturn lock_is_held(&rcu_sched_lock_map) || !preemptible();\n}\nEXPORT_SYMBOL(rcu_read_lock_sched_held);\n#endif\n\n#ifndef CONFIG_TINY_RCU\n\n \nbool rcu_gp_is_normal(void)\n{\n\treturn READ_ONCE(rcu_normal) &&\n\t       rcu_scheduler_active != RCU_SCHEDULER_INIT;\n}\nEXPORT_SYMBOL_GPL(rcu_gp_is_normal);\n\nstatic atomic_t rcu_async_hurry_nesting = ATOMIC_INIT(1);\n \nbool rcu_async_should_hurry(void)\n{\n\treturn !IS_ENABLED(CONFIG_RCU_LAZY) ||\n\t       atomic_read(&rcu_async_hurry_nesting);\n}\nEXPORT_SYMBOL_GPL(rcu_async_should_hurry);\n\n \nvoid rcu_async_hurry(void)\n{\n\tif (IS_ENABLED(CONFIG_RCU_LAZY))\n\t\tatomic_inc(&rcu_async_hurry_nesting);\n}\nEXPORT_SYMBOL_GPL(rcu_async_hurry);\n\n \nvoid rcu_async_relax(void)\n{\n\tif (IS_ENABLED(CONFIG_RCU_LAZY))\n\t\tatomic_dec(&rcu_async_hurry_nesting);\n}\nEXPORT_SYMBOL_GPL(rcu_async_relax);\n\nstatic atomic_t rcu_expedited_nesting = ATOMIC_INIT(1);\n \nbool rcu_gp_is_expedited(void)\n{\n\treturn rcu_expedited || atomic_read(&rcu_expedited_nesting);\n}\nEXPORT_SYMBOL_GPL(rcu_gp_is_expedited);\n\n \nvoid rcu_expedite_gp(void)\n{\n\tatomic_inc(&rcu_expedited_nesting);\n}\nEXPORT_SYMBOL_GPL(rcu_expedite_gp);\n\n \nvoid rcu_unexpedite_gp(void)\n{\n\tatomic_dec(&rcu_expedited_nesting);\n}\nEXPORT_SYMBOL_GPL(rcu_unexpedite_gp);\n\nstatic bool rcu_boot_ended __read_mostly;\n\n \nvoid rcu_end_inkernel_boot(void)\n{\n\trcu_unexpedite_gp();\n\trcu_async_relax();\n\tif (rcu_normal_after_boot)\n\t\tWRITE_ONCE(rcu_normal, 1);\n\trcu_boot_ended = true;\n}\n\n \nbool rcu_inkernel_boot_has_ended(void)\n{\n\treturn rcu_boot_ended;\n}\nEXPORT_SYMBOL_GPL(rcu_inkernel_boot_has_ended);\n\n#endif  \n\n \nvoid rcu_test_sync_prims(void)\n{\n\tif (!IS_ENABLED(CONFIG_PROVE_RCU))\n\t\treturn;\n\tpr_info(\"Running RCU synchronous self tests\\n\");\n\tsynchronize_rcu();\n\tsynchronize_rcu_expedited();\n}\n\n#if !defined(CONFIG_TINY_RCU)\n\n \nstatic int __init rcu_set_runtime_mode(void)\n{\n\trcu_test_sync_prims();\n\trcu_scheduler_active = RCU_SCHEDULER_RUNNING;\n\tkfree_rcu_scheduler_running();\n\trcu_test_sync_prims();\n\treturn 0;\n}\ncore_initcall(rcu_set_runtime_mode);\n\n#endif  \n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\nstatic struct lock_class_key rcu_lock_key;\nstruct lockdep_map rcu_lock_map = {\n\t.name = \"rcu_read_lock\",\n\t.key = &rcu_lock_key,\n\t.wait_type_outer = LD_WAIT_FREE,\n\t.wait_type_inner = LD_WAIT_CONFIG,  \n};\nEXPORT_SYMBOL_GPL(rcu_lock_map);\n\nstatic struct lock_class_key rcu_bh_lock_key;\nstruct lockdep_map rcu_bh_lock_map = {\n\t.name = \"rcu_read_lock_bh\",\n\t.key = &rcu_bh_lock_key,\n\t.wait_type_outer = LD_WAIT_FREE,\n\t.wait_type_inner = LD_WAIT_CONFIG,  \n};\nEXPORT_SYMBOL_GPL(rcu_bh_lock_map);\n\nstatic struct lock_class_key rcu_sched_lock_key;\nstruct lockdep_map rcu_sched_lock_map = {\n\t.name = \"rcu_read_lock_sched\",\n\t.key = &rcu_sched_lock_key,\n\t.wait_type_outer = LD_WAIT_FREE,\n\t.wait_type_inner = LD_WAIT_SPIN,\n};\nEXPORT_SYMBOL_GPL(rcu_sched_lock_map);\n\n\nstatic struct lock_class_key rcu_callback_key;\nstruct lockdep_map rcu_callback_map =\n\tSTATIC_LOCKDEP_MAP_INIT(\"rcu_callback\", &rcu_callback_key);\nEXPORT_SYMBOL_GPL(rcu_callback_map);\n\nnoinstr int notrace debug_lockdep_rcu_enabled(void)\n{\n\treturn rcu_scheduler_active != RCU_SCHEDULER_INACTIVE && READ_ONCE(debug_locks) &&\n\t       current->lockdep_recursion == 0;\n}\nEXPORT_SYMBOL_GPL(debug_lockdep_rcu_enabled);\n\n \nint rcu_read_lock_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\treturn lock_is_held(&rcu_lock_map);\n}\nEXPORT_SYMBOL_GPL(rcu_read_lock_held);\n\n \nint rcu_read_lock_bh_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\treturn in_softirq() || irqs_disabled();\n}\nEXPORT_SYMBOL_GPL(rcu_read_lock_bh_held);\n\nint rcu_read_lock_any_held(void)\n{\n\tbool ret;\n\n\tif (rcu_read_lock_held_common(&ret))\n\t\treturn ret;\n\tif (lock_is_held(&rcu_lock_map) ||\n\t    lock_is_held(&rcu_bh_lock_map) ||\n\t    lock_is_held(&rcu_sched_lock_map))\n\t\treturn 1;\n\treturn !preemptible();\n}\nEXPORT_SYMBOL_GPL(rcu_read_lock_any_held);\n\n#endif  \n\n \nvoid wakeme_after_rcu(struct rcu_head *head)\n{\n\tstruct rcu_synchronize *rcu;\n\n\trcu = container_of(head, struct rcu_synchronize, head);\n\tcomplete(&rcu->completion);\n}\nEXPORT_SYMBOL_GPL(wakeme_after_rcu);\n\nvoid __wait_rcu_gp(bool checktiny, int n, call_rcu_func_t *crcu_array,\n\t\t   struct rcu_synchronize *rs_array)\n{\n\tint i;\n\tint j;\n\n\t \n\tfor (i = 0; i < n; i++) {\n\t\tif (checktiny &&\n\t\t    (crcu_array[i] == call_rcu)) {\n\t\t\tmight_sleep();\n\t\t\tcontinue;\n\t\t}\n\t\tfor (j = 0; j < i; j++)\n\t\t\tif (crcu_array[j] == crcu_array[i])\n\t\t\t\tbreak;\n\t\tif (j == i) {\n\t\t\tinit_rcu_head_on_stack(&rs_array[i].head);\n\t\t\tinit_completion(&rs_array[i].completion);\n\t\t\t(crcu_array[i])(&rs_array[i].head, wakeme_after_rcu);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < n; i++) {\n\t\tif (checktiny &&\n\t\t    (crcu_array[i] == call_rcu))\n\t\t\tcontinue;\n\t\tfor (j = 0; j < i; j++)\n\t\t\tif (crcu_array[j] == crcu_array[i])\n\t\t\t\tbreak;\n\t\tif (j == i) {\n\t\t\twait_for_completion(&rs_array[i].completion);\n\t\t\tdestroy_rcu_head_on_stack(&rs_array[i].head);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(__wait_rcu_gp);\n\nvoid finish_rcuwait(struct rcuwait *w)\n{\n\trcu_assign_pointer(w->task, NULL);\n\t__set_current_state(TASK_RUNNING);\n}\nEXPORT_SYMBOL_GPL(finish_rcuwait);\n\n#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD\nvoid init_rcu_head(struct rcu_head *head)\n{\n\tdebug_object_init(head, &rcuhead_debug_descr);\n}\nEXPORT_SYMBOL_GPL(init_rcu_head);\n\nvoid destroy_rcu_head(struct rcu_head *head)\n{\n\tdebug_object_free(head, &rcuhead_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_rcu_head);\n\nstatic bool rcuhead_is_static_object(void *addr)\n{\n\treturn true;\n}\n\n \nvoid init_rcu_head_on_stack(struct rcu_head *head)\n{\n\tdebug_object_init_on_stack(head, &rcuhead_debug_descr);\n}\nEXPORT_SYMBOL_GPL(init_rcu_head_on_stack);\n\n \nvoid destroy_rcu_head_on_stack(struct rcu_head *head)\n{\n\tdebug_object_free(head, &rcuhead_debug_descr);\n}\nEXPORT_SYMBOL_GPL(destroy_rcu_head_on_stack);\n\nconst struct debug_obj_descr rcuhead_debug_descr = {\n\t.name = \"rcu_head\",\n\t.is_static_object = rcuhead_is_static_object,\n};\nEXPORT_SYMBOL_GPL(rcuhead_debug_descr);\n#endif  \n\n#if defined(CONFIG_TREE_RCU) || defined(CONFIG_RCU_TRACE)\nvoid do_trace_rcu_torture_read(const char *rcutorturename, struct rcu_head *rhp,\n\t\t\t       unsigned long secs,\n\t\t\t       unsigned long c_old, unsigned long c)\n{\n\ttrace_rcu_torture_read(rcutorturename, rhp, secs, c_old, c);\n}\nEXPORT_SYMBOL_GPL(do_trace_rcu_torture_read);\n#else\n#define do_trace_rcu_torture_read(rcutorturename, rhp, secs, c_old, c) \\\n\tdo { } while (0)\n#endif\n\n#if IS_ENABLED(CONFIG_RCU_TORTURE_TEST) || IS_MODULE(CONFIG_RCU_TORTURE_TEST)\n \nlong rcutorture_sched_setaffinity(pid_t pid, const struct cpumask *in_mask)\n{\n\tint ret;\n\n\tret = sched_setaffinity(pid, in_mask);\n\tWARN_ONCE(ret, \"%s: sched_setaffinity() returned %d\\n\", __func__, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rcutorture_sched_setaffinity);\n#endif\n\n#ifdef CONFIG_RCU_STALL_COMMON\nint rcu_cpu_stall_ftrace_dump __read_mostly;\nmodule_param(rcu_cpu_stall_ftrace_dump, int, 0644);\nint rcu_cpu_stall_suppress __read_mostly; \nEXPORT_SYMBOL_GPL(rcu_cpu_stall_suppress);\nmodule_param(rcu_cpu_stall_suppress, int, 0644);\nint rcu_cpu_stall_timeout __read_mostly = CONFIG_RCU_CPU_STALL_TIMEOUT;\nmodule_param(rcu_cpu_stall_timeout, int, 0644);\nint rcu_exp_cpu_stall_timeout __read_mostly = CONFIG_RCU_EXP_CPU_STALL_TIMEOUT;\nmodule_param(rcu_exp_cpu_stall_timeout, int, 0644);\nint rcu_cpu_stall_cputime __read_mostly = IS_ENABLED(CONFIG_RCU_CPU_STALL_CPUTIME);\nmodule_param(rcu_cpu_stall_cputime, int, 0644);\nbool rcu_exp_stall_task_details __read_mostly;\nmodule_param(rcu_exp_stall_task_details, bool, 0644);\n#endif  \n\n\n\nint rcu_cpu_stall_suppress_at_boot __read_mostly; \nEXPORT_SYMBOL_GPL(rcu_cpu_stall_suppress_at_boot);\nmodule_param(rcu_cpu_stall_suppress_at_boot, int, 0444);\n\n \nunsigned long get_completed_synchronize_rcu(void)\n{\n\treturn RCU_GET_STATE_COMPLETED;\n}\nEXPORT_SYMBOL_GPL(get_completed_synchronize_rcu);\n\n#ifdef CONFIG_PROVE_RCU\n\n \nstatic bool rcu_self_test;\nmodule_param(rcu_self_test, bool, 0444);\n\nstatic int rcu_self_test_counter;\n\nstatic void test_callback(struct rcu_head *r)\n{\n\trcu_self_test_counter++;\n\tpr_info(\"RCU test callback executed %d\\n\", rcu_self_test_counter);\n}\n\nDEFINE_STATIC_SRCU(early_srcu);\nstatic unsigned long early_srcu_cookie;\n\nstruct early_boot_kfree_rcu {\n\tstruct rcu_head rh;\n};\n\nstatic void early_boot_test_call_rcu(void)\n{\n\tstatic struct rcu_head head;\n\tint idx;\n\tstatic struct rcu_head shead;\n\tstruct early_boot_kfree_rcu *rhp;\n\n\tidx = srcu_down_read(&early_srcu);\n\tsrcu_up_read(&early_srcu, idx);\n\tcall_rcu(&head, test_callback);\n\tearly_srcu_cookie = start_poll_synchronize_srcu(&early_srcu);\n\tcall_srcu(&early_srcu, &shead, test_callback);\n\trhp = kmalloc(sizeof(*rhp), GFP_KERNEL);\n\tif (!WARN_ON_ONCE(!rhp))\n\t\tkfree_rcu(rhp, rh);\n}\n\nvoid rcu_early_boot_tests(void)\n{\n\tpr_info(\"Running RCU self tests\\n\");\n\n\tif (rcu_self_test)\n\t\tearly_boot_test_call_rcu();\n\trcu_test_sync_prims();\n}\n\nstatic int rcu_verify_early_boot_tests(void)\n{\n\tint ret = 0;\n\tint early_boot_test_counter = 0;\n\n\tif (rcu_self_test) {\n\t\tearly_boot_test_counter++;\n\t\trcu_barrier();\n\t\tearly_boot_test_counter++;\n\t\tsrcu_barrier(&early_srcu);\n\t\tWARN_ON_ONCE(!poll_state_synchronize_srcu(&early_srcu, early_srcu_cookie));\n\t\tcleanup_srcu_struct(&early_srcu);\n\t}\n\tif (rcu_self_test_counter != early_boot_test_counter) {\n\t\tWARN_ON(1);\n\t\tret = -1;\n\t}\n\n\treturn ret;\n}\nlate_initcall(rcu_verify_early_boot_tests);\n#else\nvoid rcu_early_boot_tests(void) {}\n#endif  \n\n#include \"tasks.h\"\n\n#ifndef CONFIG_TINY_RCU\n\n \nvoid __init rcupdate_announce_bootup_oddness(void)\n{\n\tif (rcu_normal)\n\t\tpr_info(\"\\tNo expedited grace period (rcu_normal).\\n\");\n\telse if (rcu_normal_after_boot)\n\t\tpr_info(\"\\tNo expedited grace period (rcu_normal_after_boot).\\n\");\n\telse if (rcu_expedited)\n\t\tpr_info(\"\\tAll grace periods are expedited (rcu_expedited).\\n\");\n\tif (rcu_cpu_stall_suppress)\n\t\tpr_info(\"\\tRCU CPU stall warnings suppressed (rcu_cpu_stall_suppress).\\n\");\n\tif (rcu_cpu_stall_timeout != CONFIG_RCU_CPU_STALL_TIMEOUT)\n\t\tpr_info(\"\\tRCU CPU stall warnings timeout set to %d (rcu_cpu_stall_timeout).\\n\", rcu_cpu_stall_timeout);\n\trcu_tasks_bootup_oddness();\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}