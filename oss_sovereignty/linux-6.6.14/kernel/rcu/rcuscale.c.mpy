{
  "module_name": "rcuscale.c",
  "hash_id": "77a35cccda1560189c57dd36a53662b0a6ce7ced600c125a00009b21484d44ac",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/rcuscale.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) fmt\n\n#include <linux/types.h>\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/kthread.h>\n#include <linux/err.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/rcupdate.h>\n#include <linux/interrupt.h>\n#include <linux/sched.h>\n#include <uapi/linux/sched/types.h>\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/completion.h>\n#include <linux/moduleparam.h>\n#include <linux/percpu.h>\n#include <linux/notifier.h>\n#include <linux/reboot.h>\n#include <linux/freezer.h>\n#include <linux/cpu.h>\n#include <linux/delay.h>\n#include <linux/stat.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <asm/byteorder.h>\n#include <linux/torture.h>\n#include <linux/vmalloc.h>\n#include <linux/rcupdate_trace.h>\n\n#include \"rcu.h\"\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Paul E. McKenney <paulmck@linux.ibm.com>\");\n\n#define SCALE_FLAG \"-scale:\"\n#define SCALEOUT_STRING(s) \\\n\tpr_alert(\"%s\" SCALE_FLAG \" %s\\n\", scale_type, s)\n#define VERBOSE_SCALEOUT_STRING(s) \\\n\tdo { if (verbose) pr_alert(\"%s\" SCALE_FLAG \" %s\\n\", scale_type, s); } while (0)\n#define SCALEOUT_ERRSTRING(s) \\\n\tpr_alert(\"%s\" SCALE_FLAG \"!!! %s\\n\", scale_type, s)\n\n \n\n#ifdef MODULE\n# define RCUSCALE_SHUTDOWN 0\n#else\n# define RCUSCALE_SHUTDOWN 1\n#endif\n\ntorture_param(bool, gp_async, false, \"Use asynchronous GP wait primitives\");\ntorture_param(int, gp_async_max, 1000, \"Max # outstanding waits per writer\");\ntorture_param(bool, gp_exp, false, \"Use expedited GP wait primitives\");\ntorture_param(int, holdoff, 10, \"Holdoff time before test start (s)\");\ntorture_param(int, minruntime, 0, \"Minimum run time (s)\");\ntorture_param(int, nreaders, -1, \"Number of RCU reader threads\");\ntorture_param(int, nwriters, -1, \"Number of RCU updater threads\");\ntorture_param(bool, shutdown, RCUSCALE_SHUTDOWN,\n\t      \"Shutdown at end of scalability tests.\");\ntorture_param(int, verbose, 1, \"Enable verbose debugging printk()s\");\ntorture_param(int, writer_holdoff, 0, \"Holdoff (us) between GPs, zero to disable\");\ntorture_param(int, writer_holdoff_jiffies, 0, \"Holdoff (jiffies) between GPs, zero to disable\");\ntorture_param(int, kfree_rcu_test, 0, \"Do we run a kfree_rcu() scale test?\");\ntorture_param(int, kfree_mult, 1, \"Multiple of kfree_obj size to allocate.\");\ntorture_param(int, kfree_by_call_rcu, 0, \"Use call_rcu() to emulate kfree_rcu()?\");\n\nstatic char *scale_type = \"rcu\";\nmodule_param(scale_type, charp, 0444);\nMODULE_PARM_DESC(scale_type, \"Type of RCU to scalability-test (rcu, srcu, ...)\");\n\nstatic int nrealreaders;\nstatic int nrealwriters;\nstatic struct task_struct **writer_tasks;\nstatic struct task_struct **reader_tasks;\nstatic struct task_struct *shutdown_task;\n\nstatic u64 **writer_durations;\nstatic int *writer_n_durations;\nstatic atomic_t n_rcu_scale_reader_started;\nstatic atomic_t n_rcu_scale_writer_started;\nstatic atomic_t n_rcu_scale_writer_finished;\nstatic wait_queue_head_t shutdown_wq;\nstatic u64 t_rcu_scale_writer_started;\nstatic u64 t_rcu_scale_writer_finished;\nstatic unsigned long b_rcu_gp_test_started;\nstatic unsigned long b_rcu_gp_test_finished;\nstatic DEFINE_PER_CPU(atomic_t, n_async_inflight);\n\n#define MAX_MEAS 10000\n#define MIN_MEAS 100\n\n \n\nstruct rcu_scale_ops {\n\tint ptype;\n\tvoid (*init)(void);\n\tvoid (*cleanup)(void);\n\tint (*readlock)(void);\n\tvoid (*readunlock)(int idx);\n\tunsigned long (*get_gp_seq)(void);\n\tunsigned long (*gp_diff)(unsigned long new, unsigned long old);\n\tunsigned long (*exp_completed)(void);\n\tvoid (*async)(struct rcu_head *head, rcu_callback_t func);\n\tvoid (*gp_barrier)(void);\n\tvoid (*sync)(void);\n\tvoid (*exp_sync)(void);\n\tstruct task_struct *(*rso_gp_kthread)(void);\n\tconst char *name;\n};\n\nstatic struct rcu_scale_ops *cur_ops;\n\n \n\nstatic int rcu_scale_read_lock(void) __acquires(RCU)\n{\n\trcu_read_lock();\n\treturn 0;\n}\n\nstatic void rcu_scale_read_unlock(int idx) __releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic unsigned long __maybe_unused rcu_no_completed(void)\n{\n\treturn 0;\n}\n\nstatic void rcu_sync_scale_init(void)\n{\n}\n\nstatic struct rcu_scale_ops rcu_ops = {\n\t.ptype\t\t= RCU_FLAVOR,\n\t.init\t\t= rcu_sync_scale_init,\n\t.readlock\t= rcu_scale_read_lock,\n\t.readunlock\t= rcu_scale_read_unlock,\n\t.get_gp_seq\t= rcu_get_gp_seq,\n\t.gp_diff\t= rcu_seq_diff,\n\t.exp_completed\t= rcu_exp_batches_completed,\n\t.async\t\t= call_rcu_hurry,\n\t.gp_barrier\t= rcu_barrier,\n\t.sync\t\t= synchronize_rcu,\n\t.exp_sync\t= synchronize_rcu_expedited,\n\t.name\t\t= \"rcu\"\n};\n\n \n\nDEFINE_STATIC_SRCU(srcu_ctl_scale);\nstatic struct srcu_struct *srcu_ctlp = &srcu_ctl_scale;\n\nstatic int srcu_scale_read_lock(void) __acquires(srcu_ctlp)\n{\n\treturn srcu_read_lock(srcu_ctlp);\n}\n\nstatic void srcu_scale_read_unlock(int idx) __releases(srcu_ctlp)\n{\n\tsrcu_read_unlock(srcu_ctlp, idx);\n}\n\nstatic unsigned long srcu_scale_completed(void)\n{\n\treturn srcu_batches_completed(srcu_ctlp);\n}\n\nstatic void srcu_call_rcu(struct rcu_head *head, rcu_callback_t func)\n{\n\tcall_srcu(srcu_ctlp, head, func);\n}\n\nstatic void srcu_rcu_barrier(void)\n{\n\tsrcu_barrier(srcu_ctlp);\n}\n\nstatic void srcu_scale_synchronize(void)\n{\n\tsynchronize_srcu(srcu_ctlp);\n}\n\nstatic void srcu_scale_synchronize_expedited(void)\n{\n\tsynchronize_srcu_expedited(srcu_ctlp);\n}\n\nstatic struct rcu_scale_ops srcu_ops = {\n\t.ptype\t\t= SRCU_FLAVOR,\n\t.init\t\t= rcu_sync_scale_init,\n\t.readlock\t= srcu_scale_read_lock,\n\t.readunlock\t= srcu_scale_read_unlock,\n\t.get_gp_seq\t= srcu_scale_completed,\n\t.gp_diff\t= rcu_seq_diff,\n\t.exp_completed\t= srcu_scale_completed,\n\t.async\t\t= srcu_call_rcu,\n\t.gp_barrier\t= srcu_rcu_barrier,\n\t.sync\t\t= srcu_scale_synchronize,\n\t.exp_sync\t= srcu_scale_synchronize_expedited,\n\t.name\t\t= \"srcu\"\n};\n\nstatic struct srcu_struct srcud;\n\nstatic void srcu_sync_scale_init(void)\n{\n\tsrcu_ctlp = &srcud;\n\tinit_srcu_struct(srcu_ctlp);\n}\n\nstatic void srcu_sync_scale_cleanup(void)\n{\n\tcleanup_srcu_struct(srcu_ctlp);\n}\n\nstatic struct rcu_scale_ops srcud_ops = {\n\t.ptype\t\t= SRCU_FLAVOR,\n\t.init\t\t= srcu_sync_scale_init,\n\t.cleanup\t= srcu_sync_scale_cleanup,\n\t.readlock\t= srcu_scale_read_lock,\n\t.readunlock\t= srcu_scale_read_unlock,\n\t.get_gp_seq\t= srcu_scale_completed,\n\t.gp_diff\t= rcu_seq_diff,\n\t.exp_completed\t= srcu_scale_completed,\n\t.async\t\t= srcu_call_rcu,\n\t.gp_barrier\t= srcu_rcu_barrier,\n\t.sync\t\t= srcu_scale_synchronize,\n\t.exp_sync\t= srcu_scale_synchronize_expedited,\n\t.name\t\t= \"srcud\"\n};\n\n#ifdef CONFIG_TASKS_RCU\n\n \n\nstatic int tasks_scale_read_lock(void)\n{\n\treturn 0;\n}\n\nstatic void tasks_scale_read_unlock(int idx)\n{\n}\n\nstatic struct rcu_scale_ops tasks_ops = {\n\t.ptype\t\t= RCU_TASKS_FLAVOR,\n\t.init\t\t= rcu_sync_scale_init,\n\t.readlock\t= tasks_scale_read_lock,\n\t.readunlock\t= tasks_scale_read_unlock,\n\t.get_gp_seq\t= rcu_no_completed,\n\t.gp_diff\t= rcu_seq_diff,\n\t.async\t\t= call_rcu_tasks,\n\t.gp_barrier\t= rcu_barrier_tasks,\n\t.sync\t\t= synchronize_rcu_tasks,\n\t.exp_sync\t= synchronize_rcu_tasks,\n\t.rso_gp_kthread\t= get_rcu_tasks_gp_kthread,\n\t.name\t\t= \"tasks\"\n};\n\n#define TASKS_OPS &tasks_ops,\n\n#else  \n\n#define TASKS_OPS\n\n#endif  \n\n#ifdef CONFIG_TASKS_RUDE_RCU\n\n \n\nstatic int tasks_rude_scale_read_lock(void)\n{\n\treturn 0;\n}\n\nstatic void tasks_rude_scale_read_unlock(int idx)\n{\n}\n\nstatic struct rcu_scale_ops tasks_rude_ops = {\n\t.ptype\t\t= RCU_TASKS_RUDE_FLAVOR,\n\t.init\t\t= rcu_sync_scale_init,\n\t.readlock\t= tasks_rude_scale_read_lock,\n\t.readunlock\t= tasks_rude_scale_read_unlock,\n\t.get_gp_seq\t= rcu_no_completed,\n\t.gp_diff\t= rcu_seq_diff,\n\t.async\t\t= call_rcu_tasks_rude,\n\t.gp_barrier\t= rcu_barrier_tasks_rude,\n\t.sync\t\t= synchronize_rcu_tasks_rude,\n\t.exp_sync\t= synchronize_rcu_tasks_rude,\n\t.rso_gp_kthread\t= get_rcu_tasks_rude_gp_kthread,\n\t.name\t\t= \"tasks-rude\"\n};\n\n#define TASKS_RUDE_OPS &tasks_rude_ops,\n\n#else  \n\n#define TASKS_RUDE_OPS\n\n#endif  \n\n#ifdef CONFIG_TASKS_TRACE_RCU\n\n \n\nstatic int tasks_trace_scale_read_lock(void)\n{\n\trcu_read_lock_trace();\n\treturn 0;\n}\n\nstatic void tasks_trace_scale_read_unlock(int idx)\n{\n\trcu_read_unlock_trace();\n}\n\nstatic struct rcu_scale_ops tasks_tracing_ops = {\n\t.ptype\t\t= RCU_TASKS_FLAVOR,\n\t.init\t\t= rcu_sync_scale_init,\n\t.readlock\t= tasks_trace_scale_read_lock,\n\t.readunlock\t= tasks_trace_scale_read_unlock,\n\t.get_gp_seq\t= rcu_no_completed,\n\t.gp_diff\t= rcu_seq_diff,\n\t.async\t\t= call_rcu_tasks_trace,\n\t.gp_barrier\t= rcu_barrier_tasks_trace,\n\t.sync\t\t= synchronize_rcu_tasks_trace,\n\t.exp_sync\t= synchronize_rcu_tasks_trace,\n\t.rso_gp_kthread\t= get_rcu_tasks_trace_gp_kthread,\n\t.name\t\t= \"tasks-tracing\"\n};\n\n#define TASKS_TRACING_OPS &tasks_tracing_ops,\n\n#else  \n\n#define TASKS_TRACING_OPS\n\n#endif  \n\nstatic unsigned long rcuscale_seq_diff(unsigned long new, unsigned long old)\n{\n\tif (!cur_ops->gp_diff)\n\t\treturn new - old;\n\treturn cur_ops->gp_diff(new, old);\n}\n\n \nstatic void rcu_scale_wait_shutdown(void)\n{\n\tcond_resched_tasks_rcu_qs();\n\tif (atomic_read(&n_rcu_scale_writer_finished) < nrealwriters)\n\t\treturn;\n\twhile (!torture_must_stop())\n\t\tschedule_timeout_uninterruptible(1);\n}\n\n \nstatic int\nrcu_scale_reader(void *arg)\n{\n\tunsigned long flags;\n\tint idx;\n\tlong me = (long)arg;\n\n\tVERBOSE_SCALEOUT_STRING(\"rcu_scale_reader task started\");\n\tset_cpus_allowed_ptr(current, cpumask_of(me % nr_cpu_ids));\n\tset_user_nice(current, MAX_NICE);\n\tatomic_inc(&n_rcu_scale_reader_started);\n\n\tdo {\n\t\tlocal_irq_save(flags);\n\t\tidx = cur_ops->readlock();\n\t\tcur_ops->readunlock(idx);\n\t\tlocal_irq_restore(flags);\n\t\trcu_scale_wait_shutdown();\n\t} while (!torture_must_stop());\n\ttorture_kthread_stopping(\"rcu_scale_reader\");\n\treturn 0;\n}\n\n \nstatic void rcu_scale_async_cb(struct rcu_head *rhp)\n{\n\tatomic_dec(this_cpu_ptr(&n_async_inflight));\n\tkfree(rhp);\n}\n\n \nstatic int\nrcu_scale_writer(void *arg)\n{\n\tint i = 0;\n\tint i_max;\n\tunsigned long jdone;\n\tlong me = (long)arg;\n\tstruct rcu_head *rhp = NULL;\n\tbool started = false, done = false, alldone = false;\n\tu64 t;\n\tDEFINE_TORTURE_RANDOM(tr);\n\tu64 *wdp;\n\tu64 *wdpp = writer_durations[me];\n\n\tVERBOSE_SCALEOUT_STRING(\"rcu_scale_writer task started\");\n\tWARN_ON(!wdpp);\n\tset_cpus_allowed_ptr(current, cpumask_of(me % nr_cpu_ids));\n\tcurrent->flags |= PF_NO_SETAFFINITY;\n\tsched_set_fifo_low(current);\n\n\tif (holdoff)\n\t\tschedule_timeout_idle(holdoff * HZ);\n\n\t \n\twhile (!gp_exp && system_state != SYSTEM_RUNNING)\n\t\tschedule_timeout_uninterruptible(1);\n\n\tt = ktime_get_mono_fast_ns();\n\tif (atomic_inc_return(&n_rcu_scale_writer_started) >= nrealwriters) {\n\t\tt_rcu_scale_writer_started = t;\n\t\tif (gp_exp) {\n\t\t\tb_rcu_gp_test_started =\n\t\t\t\tcur_ops->exp_completed() / 2;\n\t\t} else {\n\t\t\tb_rcu_gp_test_started = cur_ops->get_gp_seq();\n\t\t}\n\t}\n\n\tjdone = jiffies + minruntime * HZ;\n\tdo {\n\t\tif (writer_holdoff)\n\t\t\tudelay(writer_holdoff);\n\t\tif (writer_holdoff_jiffies)\n\t\t\tschedule_timeout_idle(torture_random(&tr) % writer_holdoff_jiffies + 1);\n\t\twdp = &wdpp[i];\n\t\t*wdp = ktime_get_mono_fast_ns();\n\t\tif (gp_async) {\nretry:\n\t\t\tif (!rhp)\n\t\t\t\trhp = kmalloc(sizeof(*rhp), GFP_KERNEL);\n\t\t\tif (rhp && atomic_read(this_cpu_ptr(&n_async_inflight)) < gp_async_max) {\n\t\t\t\tatomic_inc(this_cpu_ptr(&n_async_inflight));\n\t\t\t\tcur_ops->async(rhp, rcu_scale_async_cb);\n\t\t\t\trhp = NULL;\n\t\t\t} else if (!kthread_should_stop()) {\n\t\t\t\tcur_ops->gp_barrier();\n\t\t\t\tgoto retry;\n\t\t\t} else {\n\t\t\t\tkfree(rhp);  \n\t\t\t}\n\t\t} else if (gp_exp) {\n\t\t\tcur_ops->exp_sync();\n\t\t} else {\n\t\t\tcur_ops->sync();\n\t\t}\n\t\tt = ktime_get_mono_fast_ns();\n\t\t*wdp = t - *wdp;\n\t\ti_max = i;\n\t\tif (!started &&\n\t\t    atomic_read(&n_rcu_scale_writer_started) >= nrealwriters)\n\t\t\tstarted = true;\n\t\tif (!done && i >= MIN_MEAS && time_after(jiffies, jdone)) {\n\t\t\tdone = true;\n\t\t\tsched_set_normal(current, 0);\n\t\t\tpr_alert(\"%s%s rcu_scale_writer %ld has %d measurements\\n\",\n\t\t\t\t scale_type, SCALE_FLAG, me, MIN_MEAS);\n\t\t\tif (atomic_inc_return(&n_rcu_scale_writer_finished) >=\n\t\t\t    nrealwriters) {\n\t\t\t\tschedule_timeout_interruptible(10);\n\t\t\t\trcu_ftrace_dump(DUMP_ALL);\n\t\t\t\tSCALEOUT_STRING(\"Test complete\");\n\t\t\t\tt_rcu_scale_writer_finished = t;\n\t\t\t\tif (gp_exp) {\n\t\t\t\t\tb_rcu_gp_test_finished =\n\t\t\t\t\t\tcur_ops->exp_completed() / 2;\n\t\t\t\t} else {\n\t\t\t\t\tb_rcu_gp_test_finished =\n\t\t\t\t\t\tcur_ops->get_gp_seq();\n\t\t\t\t}\n\t\t\t\tif (shutdown) {\n\t\t\t\t\tsmp_mb();  \n\t\t\t\t\twake_up(&shutdown_wq);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (done && !alldone &&\n\t\t    atomic_read(&n_rcu_scale_writer_finished) >= nrealwriters)\n\t\t\talldone = true;\n\t\tif (started && !alldone && i < MAX_MEAS - 1)\n\t\t\ti++;\n\t\trcu_scale_wait_shutdown();\n\t} while (!torture_must_stop());\n\tif (gp_async) {\n\t\tcur_ops->gp_barrier();\n\t}\n\twriter_n_durations[me] = i_max + 1;\n\ttorture_kthread_stopping(\"rcu_scale_writer\");\n\treturn 0;\n}\n\nstatic void\nrcu_scale_print_module_parms(struct rcu_scale_ops *cur_ops, const char *tag)\n{\n\tpr_alert(\"%s\" SCALE_FLAG\n\t\t \"--- %s: gp_async=%d gp_async_max=%d gp_exp=%d holdoff=%d minruntime=%d nreaders=%d nwriters=%d writer_holdoff=%d writer_holdoff_jiffies=%d verbose=%d shutdown=%d\\n\",\n\t\t scale_type, tag, gp_async, gp_async_max, gp_exp, holdoff, minruntime, nrealreaders, nrealwriters, writer_holdoff, writer_holdoff_jiffies, verbose, shutdown);\n}\n\n \nstatic int compute_real(int n)\n{\n\tint nr;\n\n\tif (n >= 0) {\n\t\tnr = n;\n\t} else {\n\t\tnr = num_online_cpus() + 1 + n;\n\t\tif (nr <= 0)\n\t\t\tnr = 1;\n\t}\n\treturn nr;\n}\n\n \n\ntorture_param(int, kfree_nthreads, -1, \"Number of threads running loops of kfree_rcu().\");\ntorture_param(int, kfree_alloc_num, 8000, \"Number of allocations and frees done in an iteration.\");\ntorture_param(int, kfree_loops, 10, \"Number of loops doing kfree_alloc_num allocations and frees.\");\ntorture_param(bool, kfree_rcu_test_double, false, \"Do we run a kfree_rcu() double-argument scale test?\");\ntorture_param(bool, kfree_rcu_test_single, false, \"Do we run a kfree_rcu() single-argument scale test?\");\n\nstatic struct task_struct **kfree_reader_tasks;\nstatic int kfree_nrealthreads;\nstatic atomic_t n_kfree_scale_thread_started;\nstatic atomic_t n_kfree_scale_thread_ended;\nstatic struct task_struct *kthread_tp;\nstatic u64 kthread_stime;\n\nstruct kfree_obj {\n\tchar kfree_obj[8];\n\tstruct rcu_head rh;\n};\n\n \nstatic void kfree_call_rcu(struct rcu_head *rh)\n{\n\tstruct kfree_obj *obj = container_of(rh, struct kfree_obj, rh);\n\n\tkfree(obj);\n}\n\nstatic int\nkfree_scale_thread(void *arg)\n{\n\tint i, loop = 0;\n\tlong me = (long)arg;\n\tstruct kfree_obj *alloc_ptr;\n\tu64 start_time, end_time;\n\tlong long mem_begin, mem_during = 0;\n\tbool kfree_rcu_test_both;\n\tDEFINE_TORTURE_RANDOM(tr);\n\n\tVERBOSE_SCALEOUT_STRING(\"kfree_scale_thread task started\");\n\tset_cpus_allowed_ptr(current, cpumask_of(me % nr_cpu_ids));\n\tset_user_nice(current, MAX_NICE);\n\tkfree_rcu_test_both = (kfree_rcu_test_single == kfree_rcu_test_double);\n\n\tstart_time = ktime_get_mono_fast_ns();\n\n\tif (atomic_inc_return(&n_kfree_scale_thread_started) >= kfree_nrealthreads) {\n\t\tif (gp_exp)\n\t\t\tb_rcu_gp_test_started = cur_ops->exp_completed() / 2;\n\t\telse\n\t\t\tb_rcu_gp_test_started = cur_ops->get_gp_seq();\n\t}\n\n\tdo {\n\t\tif (!mem_during) {\n\t\t\tmem_during = mem_begin = si_mem_available();\n\t\t} else if (loop % (kfree_loops / 4) == 0) {\n\t\t\tmem_during = (mem_during + si_mem_available()) / 2;\n\t\t}\n\n\t\tfor (i = 0; i < kfree_alloc_num; i++) {\n\t\t\talloc_ptr = kmalloc(kfree_mult * sizeof(struct kfree_obj), GFP_KERNEL);\n\t\t\tif (!alloc_ptr)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tif (kfree_by_call_rcu) {\n\t\t\t\tcall_rcu(&(alloc_ptr->rh), kfree_call_rcu);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\tif ((kfree_rcu_test_single && !kfree_rcu_test_double) ||\n\t\t\t\t\t(kfree_rcu_test_both && torture_random(&tr) & 0x800))\n\t\t\t\tkfree_rcu_mightsleep(alloc_ptr);\n\t\t\telse\n\t\t\t\tkfree_rcu(alloc_ptr, rh);\n\t\t}\n\n\t\tcond_resched();\n\t} while (!torture_must_stop() && ++loop < kfree_loops);\n\n\tif (atomic_inc_return(&n_kfree_scale_thread_ended) >= kfree_nrealthreads) {\n\t\tend_time = ktime_get_mono_fast_ns();\n\n\t\tif (gp_exp)\n\t\t\tb_rcu_gp_test_finished = cur_ops->exp_completed() / 2;\n\t\telse\n\t\t\tb_rcu_gp_test_finished = cur_ops->get_gp_seq();\n\n\t\tpr_alert(\"Total time taken by all kfree'ers: %llu ns, loops: %d, batches: %ld, memory footprint: %lldMB\\n\",\n\t\t       (unsigned long long)(end_time - start_time), kfree_loops,\n\t\t       rcuscale_seq_diff(b_rcu_gp_test_finished, b_rcu_gp_test_started),\n\t\t       (mem_begin - mem_during) >> (20 - PAGE_SHIFT));\n\n\t\tif (shutdown) {\n\t\t\tsmp_mb();  \n\t\t\twake_up(&shutdown_wq);\n\t\t}\n\t}\n\n\ttorture_kthread_stopping(\"kfree_scale_thread\");\n\treturn 0;\n}\n\nstatic void\nkfree_scale_cleanup(void)\n{\n\tint i;\n\n\tif (torture_cleanup_begin())\n\t\treturn;\n\n\tif (kfree_reader_tasks) {\n\t\tfor (i = 0; i < kfree_nrealthreads; i++)\n\t\t\ttorture_stop_kthread(kfree_scale_thread,\n\t\t\t\t\t     kfree_reader_tasks[i]);\n\t\tkfree(kfree_reader_tasks);\n\t}\n\n\ttorture_cleanup_end();\n}\n\n \nstatic int\nkfree_scale_shutdown(void *arg)\n{\n\twait_event_idle(shutdown_wq,\n\t\t\tatomic_read(&n_kfree_scale_thread_ended) >= kfree_nrealthreads);\n\n\tsmp_mb();  \n\n\tkfree_scale_cleanup();\n\tkernel_power_off();\n\treturn -EINVAL;\n}\n\n\nstatic unsigned long jiffies_at_lazy_cb;\nstatic struct rcu_head lazy_test1_rh;\nstatic int rcu_lazy_test1_cb_called;\nstatic void call_rcu_lazy_test1(struct rcu_head *rh)\n{\n\tjiffies_at_lazy_cb = jiffies;\n\tWRITE_ONCE(rcu_lazy_test1_cb_called, 1);\n}\n\nstatic int __init\nkfree_scale_init(void)\n{\n\tint firsterr = 0;\n\tlong i;\n\tunsigned long jif_start;\n\tunsigned long orig_jif;\n\n\tpr_alert(\"%s\" SCALE_FLAG\n\t\t \"--- kfree_rcu_test: kfree_mult=%d kfree_by_call_rcu=%d kfree_nthreads=%d kfree_alloc_num=%d kfree_loops=%d kfree_rcu_test_double=%d kfree_rcu_test_single=%d\\n\",\n\t\t scale_type, kfree_mult, kfree_by_call_rcu, kfree_nthreads, kfree_alloc_num, kfree_loops, kfree_rcu_test_double, kfree_rcu_test_single);\n\n\t\n\t\n\tif (kfree_by_call_rcu && !IS_ENABLED(CONFIG_RCU_LAZY)) {\n\t\tpr_alert(\"CONFIG_RCU_LAZY is disabled, falling back to kfree_rcu() for delayed RCU kfree'ing\\n\");\n\t\tkfree_by_call_rcu = 0;\n\t}\n\n\tif (kfree_by_call_rcu) {\n\t\t \n\t\torig_jif = rcu_lazy_get_jiffies_till_flush();\n\n\t\trcu_lazy_set_jiffies_till_flush(2 * HZ);\n\t\trcu_barrier();\n\n\t\tjif_start = jiffies;\n\t\tjiffies_at_lazy_cb = 0;\n\t\tcall_rcu(&lazy_test1_rh, call_rcu_lazy_test1);\n\n\t\tsmp_cond_load_relaxed(&rcu_lazy_test1_cb_called, VAL == 1);\n\n\t\trcu_lazy_set_jiffies_till_flush(orig_jif);\n\n\t\tif (WARN_ON_ONCE(jiffies_at_lazy_cb - jif_start < 2 * HZ)) {\n\t\t\tpr_alert(\"ERROR: call_rcu() CBs are not being lazy as expected!\\n\");\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (WARN_ON_ONCE(jiffies_at_lazy_cb - jif_start > 3 * HZ)) {\n\t\t\tpr_alert(\"ERROR: call_rcu() CBs are being too lazy!\\n\");\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tkfree_nrealthreads = compute_real(kfree_nthreads);\n\t \n\tif (shutdown) {\n\t\tinit_waitqueue_head(&shutdown_wq);\n\t\tfirsterr = torture_create_kthread(kfree_scale_shutdown, NULL,\n\t\t\t\t\t\t  shutdown_task);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\n\tpr_alert(\"kfree object size=%zu, kfree_by_call_rcu=%d\\n\",\n\t\t\tkfree_mult * sizeof(struct kfree_obj),\n\t\t\tkfree_by_call_rcu);\n\n\tkfree_reader_tasks = kcalloc(kfree_nrealthreads, sizeof(kfree_reader_tasks[0]),\n\t\t\t       GFP_KERNEL);\n\tif (kfree_reader_tasks == NULL) {\n\t\tfirsterr = -ENOMEM;\n\t\tgoto unwind;\n\t}\n\n\tfor (i = 0; i < kfree_nrealthreads; i++) {\n\t\tfirsterr = torture_create_kthread(kfree_scale_thread, (void *)i,\n\t\t\t\t\t\t  kfree_reader_tasks[i]);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t}\n\n\twhile (atomic_read(&n_kfree_scale_thread_started) < kfree_nrealthreads)\n\t\tschedule_timeout_uninterruptible(1);\n\n\ttorture_init_end();\n\treturn 0;\n\nunwind:\n\ttorture_init_end();\n\tkfree_scale_cleanup();\n\treturn firsterr;\n}\n\nstatic void\nrcu_scale_cleanup(void)\n{\n\tint i;\n\tint j;\n\tint ngps = 0;\n\tu64 *wdp;\n\tu64 *wdpp;\n\n\t \n\tif (rcu_gp_is_expedited() && !rcu_gp_is_normal() && !gp_exp)\n\t\tSCALEOUT_ERRSTRING(\"All grace periods expedited, no normal ones to measure!\");\n\tif (rcu_gp_is_normal() && gp_exp)\n\t\tSCALEOUT_ERRSTRING(\"All grace periods normal, no expedited ones to measure!\");\n\tif (gp_exp && gp_async)\n\t\tSCALEOUT_ERRSTRING(\"No expedited async GPs, so went with async!\");\n\n\t\n\tif (IS_BUILTIN(CONFIG_RCU_SCALE_TEST) && !kthread_tp && cur_ops->rso_gp_kthread)\n\t\tkthread_tp = cur_ops->rso_gp_kthread();\n\tif (kthread_tp) {\n\t\tu32 ns;\n\t\tu64 us;\n\n\t\tkthread_stime = kthread_tp->stime - kthread_stime;\n\t\tus = div_u64_rem(kthread_stime, 1000, &ns);\n\t\tpr_info(\"rcu_scale: Grace-period kthread CPU time: %llu.%03u us\\n\", us, ns);\n\t\tshow_rcu_gp_kthreads();\n\t}\n\tif (kfree_rcu_test) {\n\t\tkfree_scale_cleanup();\n\t\treturn;\n\t}\n\n\tif (torture_cleanup_begin())\n\t\treturn;\n\tif (!cur_ops) {\n\t\ttorture_cleanup_end();\n\t\treturn;\n\t}\n\n\tif (reader_tasks) {\n\t\tfor (i = 0; i < nrealreaders; i++)\n\t\t\ttorture_stop_kthread(rcu_scale_reader,\n\t\t\t\t\t     reader_tasks[i]);\n\t\tkfree(reader_tasks);\n\t}\n\n\tif (writer_tasks) {\n\t\tfor (i = 0; i < nrealwriters; i++) {\n\t\t\ttorture_stop_kthread(rcu_scale_writer,\n\t\t\t\t\t     writer_tasks[i]);\n\t\t\tif (!writer_n_durations)\n\t\t\t\tcontinue;\n\t\t\tj = writer_n_durations[i];\n\t\t\tpr_alert(\"%s%s writer %d gps: %d\\n\",\n\t\t\t\t scale_type, SCALE_FLAG, i, j);\n\t\t\tngps += j;\n\t\t}\n\t\tpr_alert(\"%s%s start: %llu end: %llu duration: %llu gps: %d batches: %ld\\n\",\n\t\t\t scale_type, SCALE_FLAG,\n\t\t\t t_rcu_scale_writer_started, t_rcu_scale_writer_finished,\n\t\t\t t_rcu_scale_writer_finished -\n\t\t\t t_rcu_scale_writer_started,\n\t\t\t ngps,\n\t\t\t rcuscale_seq_diff(b_rcu_gp_test_finished,\n\t\t\t\t\t   b_rcu_gp_test_started));\n\t\tfor (i = 0; i < nrealwriters; i++) {\n\t\t\tif (!writer_durations)\n\t\t\t\tbreak;\n\t\t\tif (!writer_n_durations)\n\t\t\t\tcontinue;\n\t\t\twdpp = writer_durations[i];\n\t\t\tif (!wdpp)\n\t\t\t\tcontinue;\n\t\t\tfor (j = 0; j < writer_n_durations[i]; j++) {\n\t\t\t\twdp = &wdpp[j];\n\t\t\t\tpr_alert(\"%s%s %4d writer-duration: %5d %llu\\n\",\n\t\t\t\t\tscale_type, SCALE_FLAG,\n\t\t\t\t\ti, j, *wdp);\n\t\t\t\tif (j % 100 == 0)\n\t\t\t\t\tschedule_timeout_uninterruptible(1);\n\t\t\t}\n\t\t\tkfree(writer_durations[i]);\n\t\t}\n\t\tkfree(writer_tasks);\n\t\tkfree(writer_durations);\n\t\tkfree(writer_n_durations);\n\t}\n\n\t \n\tif (cur_ops->cleanup != NULL)\n\t\tcur_ops->cleanup();\n\n\ttorture_cleanup_end();\n}\n\n \nstatic int\nrcu_scale_shutdown(void *arg)\n{\n\twait_event_idle(shutdown_wq, atomic_read(&n_rcu_scale_writer_finished) >= nrealwriters);\n\tsmp_mb();  \n\trcu_scale_cleanup();\n\tkernel_power_off();\n\treturn -EINVAL;\n}\n\nstatic int __init\nrcu_scale_init(void)\n{\n\tlong i;\n\tint firsterr = 0;\n\tstatic struct rcu_scale_ops *scale_ops[] = {\n\t\t&rcu_ops, &srcu_ops, &srcud_ops, TASKS_OPS TASKS_RUDE_OPS TASKS_TRACING_OPS\n\t};\n\n\tif (!torture_init_begin(scale_type, verbose))\n\t\treturn -EBUSY;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(scale_ops); i++) {\n\t\tcur_ops = scale_ops[i];\n\t\tif (strcmp(scale_type, cur_ops->name) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(scale_ops)) {\n\t\tpr_alert(\"rcu-scale: invalid scale type: \\\"%s\\\"\\n\", scale_type);\n\t\tpr_alert(\"rcu-scale types:\");\n\t\tfor (i = 0; i < ARRAY_SIZE(scale_ops); i++)\n\t\t\tpr_cont(\" %s\", scale_ops[i]->name);\n\t\tpr_cont(\"\\n\");\n\t\tfirsterr = -EINVAL;\n\t\tcur_ops = NULL;\n\t\tgoto unwind;\n\t}\n\tif (cur_ops->init)\n\t\tcur_ops->init();\n\n\tif (cur_ops->rso_gp_kthread) {\n\t\tkthread_tp = cur_ops->rso_gp_kthread();\n\t\tif (kthread_tp)\n\t\t\tkthread_stime = kthread_tp->stime;\n\t}\n\tif (kfree_rcu_test)\n\t\treturn kfree_scale_init();\n\n\tnrealwriters = compute_real(nwriters);\n\tnrealreaders = compute_real(nreaders);\n\tatomic_set(&n_rcu_scale_reader_started, 0);\n\tatomic_set(&n_rcu_scale_writer_started, 0);\n\tatomic_set(&n_rcu_scale_writer_finished, 0);\n\trcu_scale_print_module_parms(cur_ops, \"Start of test\");\n\n\t \n\n\tif (shutdown) {\n\t\tinit_waitqueue_head(&shutdown_wq);\n\t\tfirsterr = torture_create_kthread(rcu_scale_shutdown, NULL,\n\t\t\t\t\t\t  shutdown_task);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\treader_tasks = kcalloc(nrealreaders, sizeof(reader_tasks[0]),\n\t\t\t       GFP_KERNEL);\n\tif (reader_tasks == NULL) {\n\t\tSCALEOUT_ERRSTRING(\"out of memory\");\n\t\tfirsterr = -ENOMEM;\n\t\tgoto unwind;\n\t}\n\tfor (i = 0; i < nrealreaders; i++) {\n\t\tfirsterr = torture_create_kthread(rcu_scale_reader, (void *)i,\n\t\t\t\t\t\t  reader_tasks[i]);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t}\n\twhile (atomic_read(&n_rcu_scale_reader_started) < nrealreaders)\n\t\tschedule_timeout_uninterruptible(1);\n\twriter_tasks = kcalloc(nrealwriters, sizeof(reader_tasks[0]),\n\t\t\t       GFP_KERNEL);\n\twriter_durations = kcalloc(nrealwriters, sizeof(*writer_durations),\n\t\t\t\t   GFP_KERNEL);\n\twriter_n_durations =\n\t\tkcalloc(nrealwriters, sizeof(*writer_n_durations),\n\t\t\tGFP_KERNEL);\n\tif (!writer_tasks || !writer_durations || !writer_n_durations) {\n\t\tSCALEOUT_ERRSTRING(\"out of memory\");\n\t\tfirsterr = -ENOMEM;\n\t\tgoto unwind;\n\t}\n\tfor (i = 0; i < nrealwriters; i++) {\n\t\twriter_durations[i] =\n\t\t\tkcalloc(MAX_MEAS, sizeof(*writer_durations[i]),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!writer_durations[i]) {\n\t\t\tfirsterr = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\t\tfirsterr = torture_create_kthread(rcu_scale_writer, (void *)i,\n\t\t\t\t\t\t  writer_tasks[i]);\n\t\tif (torture_init_error(firsterr))\n\t\t\tgoto unwind;\n\t}\n\ttorture_init_end();\n\treturn 0;\n\nunwind:\n\ttorture_init_end();\n\trcu_scale_cleanup();\n\tif (shutdown) {\n\t\tWARN_ON(!IS_MODULE(CONFIG_RCU_SCALE_TEST));\n\t\tkernel_power_off();\n\t}\n\treturn firsterr;\n}\n\nmodule_init(rcu_scale_init);\nmodule_exit(rcu_scale_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}