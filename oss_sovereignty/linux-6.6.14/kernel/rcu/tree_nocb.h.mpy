{
  "module_name": "tree_nocb.h",
  "hash_id": "dca756adc2441bc90d2c33c030bc7aa682b24ef00dad0ba37009dffca80f0053",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/tree_nocb.h",
  "human_readable_source": " \n \n\n#ifdef CONFIG_RCU_NOCB_CPU\nstatic cpumask_var_t rcu_nocb_mask;  \nstatic bool __read_mostly rcu_nocb_poll;     \nstatic inline int rcu_lockdep_is_held_nocb(struct rcu_data *rdp)\n{\n\treturn lockdep_is_held(&rdp->nocb_lock);\n}\n\nstatic inline bool rcu_current_is_nocb_kthread(struct rcu_data *rdp)\n{\n\t \n\tif (!rdp->nocb_cb_kthread || !rdp->nocb_gp_kthread)\n\t\treturn true;\n\n\tif (current == rdp->nocb_cb_kthread || current == rdp->nocb_gp_kthread)\n\t\tif (in_task())\n\t\t\treturn true;\n\treturn false;\n}\n\n \n\n\n \nstatic int __init rcu_nocb_setup(char *str)\n{\n\talloc_bootmem_cpumask_var(&rcu_nocb_mask);\n\tif (*str == '=') {\n\t\tif (cpulist_parse(++str, rcu_nocb_mask)) {\n\t\t\tpr_warn(\"rcu_nocbs= bad CPU range, all CPUs set\\n\");\n\t\t\tcpumask_setall(rcu_nocb_mask);\n\t\t}\n\t}\n\trcu_state.nocb_is_setup = true;\n\treturn 1;\n}\n__setup(\"rcu_nocbs\", rcu_nocb_setup);\n\nstatic int __init parse_rcu_nocb_poll(char *arg)\n{\n\trcu_nocb_poll = true;\n\treturn 1;\n}\n__setup(\"rcu_nocb_poll\", parse_rcu_nocb_poll);\n\n \nstatic int nocb_nobypass_lim_per_jiffy = 16 * 1000 / HZ;\nmodule_param(nocb_nobypass_lim_per_jiffy, int, 0);\n\n \nstatic void rcu_nocb_bypass_lock(struct rcu_data *rdp)\n\t__acquires(&rdp->nocb_bypass_lock)\n{\n\tlockdep_assert_irqs_disabled();\n\tif (raw_spin_trylock(&rdp->nocb_bypass_lock))\n\t\treturn;\n\tatomic_inc(&rdp->nocb_lock_contended);\n\tWARN_ON_ONCE(smp_processor_id() != rdp->cpu);\n\tsmp_mb__after_atomic();  \n\traw_spin_lock(&rdp->nocb_bypass_lock);\n\tsmp_mb__before_atomic();  \n\tatomic_dec(&rdp->nocb_lock_contended);\n}\n\n \nstatic void rcu_nocb_wait_contended(struct rcu_data *rdp)\n{\n\tWARN_ON_ONCE(smp_processor_id() != rdp->cpu);\n\twhile (WARN_ON_ONCE(atomic_read(&rdp->nocb_lock_contended)))\n\t\tcpu_relax();\n}\n\n \nstatic bool rcu_nocb_bypass_trylock(struct rcu_data *rdp)\n{\n\tlockdep_assert_irqs_disabled();\n\treturn raw_spin_trylock(&rdp->nocb_bypass_lock);\n}\n\n \nstatic void rcu_nocb_bypass_unlock(struct rcu_data *rdp)\n\t__releases(&rdp->nocb_bypass_lock)\n{\n\tlockdep_assert_irqs_disabled();\n\traw_spin_unlock(&rdp->nocb_bypass_lock);\n}\n\n \nstatic void rcu_nocb_lock(struct rcu_data *rdp)\n{\n\tlockdep_assert_irqs_disabled();\n\tif (!rcu_rdp_is_offloaded(rdp))\n\t\treturn;\n\traw_spin_lock(&rdp->nocb_lock);\n}\n\n \nstatic void rcu_nocb_unlock(struct rcu_data *rdp)\n{\n\tif (rcu_rdp_is_offloaded(rdp)) {\n\t\tlockdep_assert_irqs_disabled();\n\t\traw_spin_unlock(&rdp->nocb_lock);\n\t}\n}\n\n \nstatic void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,\n\t\t\t\t       unsigned long flags)\n{\n\tif (rcu_rdp_is_offloaded(rdp)) {\n\t\tlockdep_assert_irqs_disabled();\n\t\traw_spin_unlock_irqrestore(&rdp->nocb_lock, flags);\n\t} else {\n\t\tlocal_irq_restore(flags);\n\t}\n}\n\n \nstatic void rcu_lockdep_assert_cblist_protected(struct rcu_data *rdp)\n{\n\tlockdep_assert_irqs_disabled();\n\tif (rcu_rdp_is_offloaded(rdp))\n\t\tlockdep_assert_held(&rdp->nocb_lock);\n}\n\n \nstatic void rcu_nocb_gp_cleanup(struct swait_queue_head *sq)\n{\n\tswake_up_all(sq);\n}\n\nstatic struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp)\n{\n\treturn &rnp->nocb_gp_wq[rcu_seq_ctr(rnp->gp_seq) & 0x1];\n}\n\nstatic void rcu_init_one_nocb(struct rcu_node *rnp)\n{\n\tinit_swait_queue_head(&rnp->nocb_gp_wq[0]);\n\tinit_swait_queue_head(&rnp->nocb_gp_wq[1]);\n}\n\nstatic bool __wake_nocb_gp(struct rcu_data *rdp_gp,\n\t\t\t   struct rcu_data *rdp,\n\t\t\t   bool force, unsigned long flags)\n\t__releases(rdp_gp->nocb_gp_lock)\n{\n\tbool needwake = false;\n\n\tif (!READ_ONCE(rdp_gp->nocb_gp_kthread)) {\n\t\traw_spin_unlock_irqrestore(&rdp_gp->nocb_gp_lock, flags);\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t    TPS(\"AlreadyAwake\"));\n\t\treturn false;\n\t}\n\n\tif (rdp_gp->nocb_defer_wakeup > RCU_NOCB_WAKE_NOT) {\n\t\tWRITE_ONCE(rdp_gp->nocb_defer_wakeup, RCU_NOCB_WAKE_NOT);\n\t\tdel_timer(&rdp_gp->nocb_timer);\n\t}\n\n\tif (force || READ_ONCE(rdp_gp->nocb_gp_sleep)) {\n\t\tWRITE_ONCE(rdp_gp->nocb_gp_sleep, false);\n\t\tneedwake = true;\n\t}\n\traw_spin_unlock_irqrestore(&rdp_gp->nocb_gp_lock, flags);\n\tif (needwake) {\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"DoWake\"));\n\t\twake_up_process(rdp_gp->nocb_gp_kthread);\n\t}\n\n\treturn needwake;\n}\n\n \nstatic bool wake_nocb_gp(struct rcu_data *rdp, bool force)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp_gp = rdp->nocb_gp_rdp;\n\n\traw_spin_lock_irqsave(&rdp_gp->nocb_gp_lock, flags);\n\treturn __wake_nocb_gp(rdp_gp, rdp, force, flags);\n}\n\n \n#define LAZY_FLUSH_JIFFIES (10 * HZ)\nstatic unsigned long jiffies_till_flush = LAZY_FLUSH_JIFFIES;\n\n#ifdef CONFIG_RCU_LAZY\n\nvoid rcu_lazy_set_jiffies_till_flush(unsigned long jif)\n{\n\tjiffies_till_flush = jif;\n}\nEXPORT_SYMBOL(rcu_lazy_set_jiffies_till_flush);\n\nunsigned long rcu_lazy_get_jiffies_till_flush(void)\n{\n\treturn jiffies_till_flush;\n}\nEXPORT_SYMBOL(rcu_lazy_get_jiffies_till_flush);\n#endif\n\n \nstatic void wake_nocb_gp_defer(struct rcu_data *rdp, int waketype,\n\t\t\t       const char *reason)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp_gp = rdp->nocb_gp_rdp;\n\n\traw_spin_lock_irqsave(&rdp_gp->nocb_gp_lock, flags);\n\n\t \n\tif (waketype == RCU_NOCB_WAKE_LAZY &&\n\t    rdp->nocb_defer_wakeup == RCU_NOCB_WAKE_NOT) {\n\t\tmod_timer(&rdp_gp->nocb_timer, jiffies + jiffies_till_flush);\n\t\tWRITE_ONCE(rdp_gp->nocb_defer_wakeup, waketype);\n\t} else if (waketype == RCU_NOCB_WAKE_BYPASS) {\n\t\tmod_timer(&rdp_gp->nocb_timer, jiffies + 2);\n\t\tWRITE_ONCE(rdp_gp->nocb_defer_wakeup, waketype);\n\t} else {\n\t\tif (rdp_gp->nocb_defer_wakeup < RCU_NOCB_WAKE)\n\t\t\tmod_timer(&rdp_gp->nocb_timer, jiffies + 1);\n\t\tif (rdp_gp->nocb_defer_wakeup < waketype)\n\t\t\tWRITE_ONCE(rdp_gp->nocb_defer_wakeup, waketype);\n\t}\n\n\traw_spin_unlock_irqrestore(&rdp_gp->nocb_gp_lock, flags);\n\n\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, reason);\n}\n\n \nstatic bool rcu_nocb_do_flush_bypass(struct rcu_data *rdp, struct rcu_head *rhp_in,\n\t\t\t\t     unsigned long j, bool lazy)\n{\n\tstruct rcu_cblist rcl;\n\tstruct rcu_head *rhp = rhp_in;\n\n\tWARN_ON_ONCE(!rcu_rdp_is_offloaded(rdp));\n\trcu_lockdep_assert_cblist_protected(rdp);\n\tlockdep_assert_held(&rdp->nocb_bypass_lock);\n\tif (rhp && !rcu_cblist_n_cbs(&rdp->nocb_bypass)) {\n\t\traw_spin_unlock(&rdp->nocb_bypass_lock);\n\t\treturn false;\n\t}\n\t \n\tif (rhp)\n\t\trcu_segcblist_inc_len(&rdp->cblist);  \n\n\t \n\tif (lazy && rhp) {\n\t\trcu_cblist_enqueue(&rdp->nocb_bypass, rhp);\n\t\trhp = NULL;\n\t}\n\trcu_cblist_flush_enqueue(&rcl, &rdp->nocb_bypass, rhp);\n\tWRITE_ONCE(rdp->lazy_len, 0);\n\n\trcu_segcblist_insert_pend_cbs(&rdp->cblist, &rcl);\n\tWRITE_ONCE(rdp->nocb_bypass_first, j);\n\trcu_nocb_bypass_unlock(rdp);\n\treturn true;\n}\n\n \nstatic bool rcu_nocb_flush_bypass(struct rcu_data *rdp, struct rcu_head *rhp,\n\t\t\t\t  unsigned long j, bool lazy)\n{\n\tif (!rcu_rdp_is_offloaded(rdp))\n\t\treturn true;\n\trcu_lockdep_assert_cblist_protected(rdp);\n\trcu_nocb_bypass_lock(rdp);\n\treturn rcu_nocb_do_flush_bypass(rdp, rhp, j, lazy);\n}\n\n \nstatic void rcu_nocb_try_flush_bypass(struct rcu_data *rdp, unsigned long j)\n{\n\trcu_lockdep_assert_cblist_protected(rdp);\n\tif (!rcu_rdp_is_offloaded(rdp) ||\n\t    !rcu_nocb_bypass_trylock(rdp))\n\t\treturn;\n\tWARN_ON_ONCE(!rcu_nocb_do_flush_bypass(rdp, NULL, j, false));\n}\n\n \nstatic bool rcu_nocb_try_bypass(struct rcu_data *rdp, struct rcu_head *rhp,\n\t\t\t\tbool *was_alldone, unsigned long flags,\n\t\t\t\tbool lazy)\n{\n\tunsigned long c;\n\tunsigned long cur_gp_seq;\n\tunsigned long j = jiffies;\n\tlong ncbs = rcu_cblist_n_cbs(&rdp->nocb_bypass);\n\tbool bypass_is_lazy = (ncbs == READ_ONCE(rdp->lazy_len));\n\n\tlockdep_assert_irqs_disabled();\n\n\t\n\t\n\tif (!rcu_rdp_is_offloaded(rdp)) {\n\t\t*was_alldone = !rcu_segcblist_pend_cbs(&rdp->cblist);\n\t\treturn false;\n\t}\n\n\t\n\t\n\tif (!rcu_segcblist_completely_offloaded(&rdp->cblist)) {\n\t\trcu_nocb_lock(rdp);\n\t\t*was_alldone = !rcu_segcblist_pend_cbs(&rdp->cblist);\n\t\treturn false;  \n\t}\n\n\t\n\tif (rcu_scheduler_active != RCU_SCHEDULER_RUNNING) {\n\t\trcu_nocb_lock(rdp);\n\t\tWARN_ON_ONCE(rcu_cblist_n_cbs(&rdp->nocb_bypass));\n\t\t*was_alldone = !rcu_segcblist_pend_cbs(&rdp->cblist);\n\t\treturn false;\n\t}\n\n\t\n\t\n\tif (j == rdp->nocb_nobypass_last) {\n\t\tc = rdp->nocb_nobypass_count + 1;\n\t} else {\n\t\tWRITE_ONCE(rdp->nocb_nobypass_last, j);\n\t\tc = rdp->nocb_nobypass_count - nocb_nobypass_lim_per_jiffy;\n\t\tif (ULONG_CMP_LT(rdp->nocb_nobypass_count,\n\t\t\t\t nocb_nobypass_lim_per_jiffy))\n\t\t\tc = 0;\n\t\telse if (c > nocb_nobypass_lim_per_jiffy)\n\t\t\tc = nocb_nobypass_lim_per_jiffy;\n\t}\n\tWRITE_ONCE(rdp->nocb_nobypass_count, c);\n\n\t\n\t\n\t\n\t\n\tif (rdp->nocb_nobypass_count < nocb_nobypass_lim_per_jiffy && !lazy) {\n\t\trcu_nocb_lock(rdp);\n\t\t*was_alldone = !rcu_segcblist_pend_cbs(&rdp->cblist);\n\t\tif (*was_alldone)\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t    TPS(\"FirstQ\"));\n\n\t\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, j, false));\n\t\tWARN_ON_ONCE(rcu_cblist_n_cbs(&rdp->nocb_bypass));\n\t\treturn false; \n\t}\n\n\t\n\t\n\tif ((ncbs && !bypass_is_lazy && j != READ_ONCE(rdp->nocb_bypass_first)) ||\n\t    (ncbs &&  bypass_is_lazy &&\n\t     (time_after(j, READ_ONCE(rdp->nocb_bypass_first) + jiffies_till_flush))) ||\n\t    ncbs >= qhimark) {\n\t\trcu_nocb_lock(rdp);\n\t\t*was_alldone = !rcu_segcblist_pend_cbs(&rdp->cblist);\n\n\t\tif (!rcu_nocb_flush_bypass(rdp, rhp, j, lazy)) {\n\t\t\tif (*was_alldone)\n\t\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t\t    TPS(\"FirstQ\"));\n\t\t\tWARN_ON_ONCE(rcu_cblist_n_cbs(&rdp->nocb_bypass));\n\t\t\treturn false; \n\t\t}\n\t\tif (j != rdp->nocb_gp_adv_time &&\n\t\t    rcu_segcblist_nextgp(&rdp->cblist, &cur_gp_seq) &&\n\t\t    rcu_seq_done(&rdp->mynode->gp_seq, cur_gp_seq)) {\n\t\t\trcu_advance_cbs_nowake(rdp->mynode, rdp);\n\t\t\trdp->nocb_gp_adv_time = j;\n\t\t}\n\n\t\t\n\t\t\n\t\t\n\t\t__call_rcu_nocb_wake(rdp, *was_alldone, flags);\n\n\t\treturn true; \n\t}\n\n\t\n\trcu_nocb_wait_contended(rdp);\n\trcu_nocb_bypass_lock(rdp);\n\tncbs = rcu_cblist_n_cbs(&rdp->nocb_bypass);\n\trcu_segcblist_inc_len(&rdp->cblist);  \n\trcu_cblist_enqueue(&rdp->nocb_bypass, rhp);\n\n\tif (lazy)\n\t\tWRITE_ONCE(rdp->lazy_len, rdp->lazy_len + 1);\n\n\tif (!ncbs) {\n\t\tWRITE_ONCE(rdp->nocb_bypass_first, j);\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"FirstBQ\"));\n\t}\n\trcu_nocb_bypass_unlock(rdp);\n\tsmp_mb();  \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\tif (ncbs && (!bypass_is_lazy || lazy)) {\n\t\tlocal_irq_restore(flags);\n\t} else {\n\t\t\n\t\trcu_nocb_lock(rdp); \n\t\tif (!rcu_segcblist_pend_cbs(&rdp->cblist)) {\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t    TPS(\"FirstBQwake\"));\n\t\t\t__call_rcu_nocb_wake(rdp, true, flags);\n\t\t} else {\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t    TPS(\"FirstBQnoWake\"));\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t}\n\t}\n\treturn true; \n}\n\n \nstatic void __call_rcu_nocb_wake(struct rcu_data *rdp, bool was_alldone,\n\t\t\t\t unsigned long flags)\n\t\t\t\t __releases(rdp->nocb_lock)\n{\n\tlong bypass_len;\n\tunsigned long cur_gp_seq;\n\tunsigned long j;\n\tlong lazy_len;\n\tlong len;\n\tstruct task_struct *t;\n\n\t\n\tt = READ_ONCE(rdp->nocb_gp_kthread);\n\tif (rcu_nocb_poll || !t) {\n\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t    TPS(\"WakeNotPoll\"));\n\t\treturn;\n\t}\n\t\n\tlen = rcu_segcblist_n_cbs(&rdp->cblist);\n\tbypass_len = rcu_cblist_n_cbs(&rdp->nocb_bypass);\n\tlazy_len = READ_ONCE(rdp->lazy_len);\n\tif (was_alldone) {\n\t\trdp->qlen_last_fqs_check = len;\n\t\t\n\t\tif (lazy_len && bypass_len == lazy_len) {\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\twake_nocb_gp_defer(rdp, RCU_NOCB_WAKE_LAZY,\n\t\t\t\t\t   TPS(\"WakeLazy\"));\n\t\t} else if (!irqs_disabled_flags(flags)) {\n\t\t\t \n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\twake_nocb_gp(rdp, false);\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t    TPS(\"WakeEmpty\"));\n\t\t} else {\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\twake_nocb_gp_defer(rdp, RCU_NOCB_WAKE,\n\t\t\t\t\t   TPS(\"WakeEmptyIsDeferred\"));\n\t\t}\n\t} else if (len > rdp->qlen_last_fqs_check + qhimark) {\n\t\t \n\t\trdp->qlen_last_fqs_check = len;\n\t\tj = jiffies;\n\t\tif (j != rdp->nocb_gp_adv_time &&\n\t\t    rcu_segcblist_nextgp(&rdp->cblist, &cur_gp_seq) &&\n\t\t    rcu_seq_done(&rdp->mynode->gp_seq, cur_gp_seq)) {\n\t\t\trcu_advance_cbs_nowake(rdp->mynode, rdp);\n\t\t\trdp->nocb_gp_adv_time = j;\n\t\t}\n\t\tsmp_mb();  \n\t\tif ((rdp->nocb_cb_sleep ||\n\t\t     !rcu_segcblist_ready_cbs(&rdp->cblist)) &&\n\t\t    !timer_pending(&rdp->nocb_timer)) {\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\twake_nocb_gp_defer(rdp, RCU_NOCB_WAKE_FORCE,\n\t\t\t\t\t   TPS(\"WakeOvfIsDeferred\"));\n\t\t} else {\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"WakeNot\"));\n\t\t}\n\t} else {\n\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"WakeNot\"));\n\t}\n}\n\nstatic int nocb_gp_toggle_rdp(struct rcu_data *rdp,\n\t\t\t       bool *wake_state)\n{\n\tstruct rcu_segcblist *cblist = &rdp->cblist;\n\tunsigned long flags;\n\tint ret;\n\n\trcu_nocb_lock_irqsave(rdp, flags);\n\tif (rcu_segcblist_test_flags(cblist, SEGCBLIST_OFFLOADED) &&\n\t    !rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP)) {\n\t\t \n\t\trcu_segcblist_set_flags(cblist, SEGCBLIST_KTHREAD_GP);\n\t\tif (rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB))\n\t\t\t*wake_state = true;\n\t\tret = 1;\n\t} else if (!rcu_segcblist_test_flags(cblist, SEGCBLIST_OFFLOADED) &&\n\t\t   rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP)) {\n\t\t \n\t\trcu_segcblist_clear_flags(cblist, SEGCBLIST_KTHREAD_GP);\n\t\tif (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB))\n\t\t\t*wake_state = true;\n\t\tret = 0;\n\t} else {\n\t\tWARN_ON_ONCE(1);\n\t\tret = -1;\n\t}\n\n\trcu_nocb_unlock_irqrestore(rdp, flags);\n\n\treturn ret;\n}\n\nstatic void nocb_gp_sleep(struct rcu_data *my_rdp, int cpu)\n{\n\ttrace_rcu_nocb_wake(rcu_state.name, cpu, TPS(\"Sleep\"));\n\tswait_event_interruptible_exclusive(my_rdp->nocb_gp_wq,\n\t\t\t\t\t!READ_ONCE(my_rdp->nocb_gp_sleep));\n\ttrace_rcu_nocb_wake(rcu_state.name, cpu, TPS(\"EndSleep\"));\n}\n\n \nstatic void nocb_gp_wait(struct rcu_data *my_rdp)\n{\n\tbool bypass = false;\n\tint __maybe_unused cpu = my_rdp->cpu;\n\tunsigned long cur_gp_seq;\n\tunsigned long flags;\n\tbool gotcbs = false;\n\tunsigned long j = jiffies;\n\tbool lazy = false;\n\tbool needwait_gp = false; \n\tbool needwake;\n\tbool needwake_gp;\n\tstruct rcu_data *rdp, *rdp_toggling = NULL;\n\tstruct rcu_node *rnp;\n\tunsigned long wait_gp_seq = 0; \n\tbool wasempty = false;\n\n\t \n\tWARN_ON_ONCE(my_rdp->nocb_gp_rdp != my_rdp);\n\t \n\tlist_for_each_entry(rdp, &my_rdp->nocb_head_rdp, nocb_entry_rdp) {\n\t\tlong bypass_ncbs;\n\t\tbool flush_bypass = false;\n\t\tlong lazy_ncbs;\n\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"Check\"));\n\t\trcu_nocb_lock_irqsave(rdp, flags);\n\t\tlockdep_assert_held(&rdp->nocb_lock);\n\t\tbypass_ncbs = rcu_cblist_n_cbs(&rdp->nocb_bypass);\n\t\tlazy_ncbs = READ_ONCE(rdp->lazy_len);\n\n\t\tif (bypass_ncbs && (lazy_ncbs == bypass_ncbs) &&\n\t\t    (time_after(j, READ_ONCE(rdp->nocb_bypass_first) + jiffies_till_flush) ||\n\t\t     bypass_ncbs > 2 * qhimark)) {\n\t\t\tflush_bypass = true;\n\t\t} else if (bypass_ncbs && (lazy_ncbs != bypass_ncbs) &&\n\t\t    (time_after(j, READ_ONCE(rdp->nocb_bypass_first) + 1) ||\n\t\t     bypass_ncbs > 2 * qhimark)) {\n\t\t\tflush_bypass = true;\n\t\t} else if (!bypass_ncbs && rcu_segcblist_empty(&rdp->cblist)) {\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\tcontinue;  \n\t\t}\n\n\t\tif (flush_bypass) {\n\t\t\t \n\t\t\t(void)rcu_nocb_try_flush_bypass(rdp, j);\n\t\t\tbypass_ncbs = rcu_cblist_n_cbs(&rdp->nocb_bypass);\n\t\t\tlazy_ncbs = READ_ONCE(rdp->lazy_len);\n\t\t}\n\n\t\tif (bypass_ncbs) {\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t    bypass_ncbs == lazy_ncbs ? TPS(\"Lazy\") : TPS(\"Bypass\"));\n\t\t\tif (bypass_ncbs == lazy_ncbs)\n\t\t\t\tlazy = true;\n\t\t\telse\n\t\t\t\tbypass = true;\n\t\t}\n\t\trnp = rdp->mynode;\n\n\t\t \n\t\tneedwake_gp = false;\n\t\tif (!rcu_segcblist_restempty(&rdp->cblist,\n\t\t\t\t\t     RCU_NEXT_READY_TAIL) ||\n\t\t    (rcu_segcblist_nextgp(&rdp->cblist, &cur_gp_seq) &&\n\t\t     rcu_seq_done(&rnp->gp_seq, cur_gp_seq))) {\n\t\t\traw_spin_lock_rcu_node(rnp);  \n\t\t\tneedwake_gp = rcu_advance_cbs(rnp, rdp);\n\t\t\twasempty = rcu_segcblist_restempty(&rdp->cblist,\n\t\t\t\t\t\t\t   RCU_NEXT_READY_TAIL);\n\t\t\traw_spin_unlock_rcu_node(rnp);  \n\t\t}\n\t\t \n\t\tWARN_ON_ONCE(wasempty &&\n\t\t\t     !rcu_segcblist_restempty(&rdp->cblist,\n\t\t\t\t\t\t      RCU_NEXT_READY_TAIL));\n\t\tif (rcu_segcblist_nextgp(&rdp->cblist, &cur_gp_seq)) {\n\t\t\tif (!needwait_gp ||\n\t\t\t    ULONG_CMP_LT(cur_gp_seq, wait_gp_seq))\n\t\t\t\twait_gp_seq = cur_gp_seq;\n\t\t\tneedwait_gp = true;\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu,\n\t\t\t\t\t    TPS(\"NeedWaitGP\"));\n\t\t}\n\t\tif (rcu_segcblist_ready_cbs(&rdp->cblist)) {\n\t\t\tneedwake = rdp->nocb_cb_sleep;\n\t\t\tWRITE_ONCE(rdp->nocb_cb_sleep, false);\n\t\t\tsmp_mb();  \n\t\t} else {\n\t\t\tneedwake = false;\n\t\t}\n\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\tif (needwake) {\n\t\t\tswake_up_one(&rdp->nocb_cb_wq);\n\t\t\tgotcbs = true;\n\t\t}\n\t\tif (needwake_gp)\n\t\t\trcu_gp_kthread_wake();\n\t}\n\n\tmy_rdp->nocb_gp_bypass = bypass;\n\tmy_rdp->nocb_gp_gp = needwait_gp;\n\tmy_rdp->nocb_gp_seq = needwait_gp ? wait_gp_seq : 0;\n\n\t \n\t \n\tif (!rcu_nocb_poll) {\n\t\t \n\t\tif (lazy && !bypass) {\n\t\t\twake_nocb_gp_defer(my_rdp, RCU_NOCB_WAKE_LAZY,\n\t\t\t\t\tTPS(\"WakeLazyIsDeferred\"));\n\t\t \n\t\t} else if (bypass) {\n\t\t\twake_nocb_gp_defer(my_rdp, RCU_NOCB_WAKE_BYPASS,\n\t\t\t\t\tTPS(\"WakeBypassIsDeferred\"));\n\t\t}\n\t}\n\n\tif (rcu_nocb_poll) {\n\t\t \n\t\tif (gotcbs)\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, cpu, TPS(\"Poll\"));\n\t\tif (list_empty(&my_rdp->nocb_head_rdp)) {\n\t\t\traw_spin_lock_irqsave(&my_rdp->nocb_gp_lock, flags);\n\t\t\tif (!my_rdp->nocb_toggling_rdp)\n\t\t\t\tWRITE_ONCE(my_rdp->nocb_gp_sleep, true);\n\t\t\traw_spin_unlock_irqrestore(&my_rdp->nocb_gp_lock, flags);\n\t\t\t \n\t\t\tnocb_gp_sleep(my_rdp, cpu);\n\t\t} else {\n\t\t\tschedule_timeout_idle(1);\n\t\t}\n\t} else if (!needwait_gp) {\n\t\t \n\t\tnocb_gp_sleep(my_rdp, cpu);\n\t} else {\n\t\trnp = my_rdp->mynode;\n\t\ttrace_rcu_this_gp(rnp, my_rdp, wait_gp_seq, TPS(\"StartWait\"));\n\t\tswait_event_interruptible_exclusive(\n\t\t\trnp->nocb_gp_wq[rcu_seq_ctr(wait_gp_seq) & 0x1],\n\t\t\trcu_seq_done(&rnp->gp_seq, wait_gp_seq) ||\n\t\t\t!READ_ONCE(my_rdp->nocb_gp_sleep));\n\t\ttrace_rcu_this_gp(rnp, my_rdp, wait_gp_seq, TPS(\"EndWait\"));\n\t}\n\n\tif (!rcu_nocb_poll) {\n\t\traw_spin_lock_irqsave(&my_rdp->nocb_gp_lock, flags);\n\t\t \n\t\trdp_toggling = my_rdp->nocb_toggling_rdp;\n\t\tif (rdp_toggling)\n\t\t\tmy_rdp->nocb_toggling_rdp = NULL;\n\n\t\tif (my_rdp->nocb_defer_wakeup > RCU_NOCB_WAKE_NOT) {\n\t\t\tWRITE_ONCE(my_rdp->nocb_defer_wakeup, RCU_NOCB_WAKE_NOT);\n\t\t\tdel_timer(&my_rdp->nocb_timer);\n\t\t}\n\t\tWRITE_ONCE(my_rdp->nocb_gp_sleep, true);\n\t\traw_spin_unlock_irqrestore(&my_rdp->nocb_gp_lock, flags);\n\t} else {\n\t\trdp_toggling = READ_ONCE(my_rdp->nocb_toggling_rdp);\n\t\tif (rdp_toggling) {\n\t\t\t \n\t\t\traw_spin_lock_irqsave(&my_rdp->nocb_gp_lock, flags);\n\t\t\tmy_rdp->nocb_toggling_rdp = NULL;\n\t\t\traw_spin_unlock_irqrestore(&my_rdp->nocb_gp_lock, flags);\n\t\t}\n\t}\n\n\tif (rdp_toggling) {\n\t\tbool wake_state = false;\n\t\tint ret;\n\n\t\tret = nocb_gp_toggle_rdp(rdp_toggling, &wake_state);\n\t\tif (ret == 1)\n\t\t\tlist_add_tail(&rdp_toggling->nocb_entry_rdp, &my_rdp->nocb_head_rdp);\n\t\telse if (ret == 0)\n\t\t\tlist_del(&rdp_toggling->nocb_entry_rdp);\n\t\tif (wake_state)\n\t\t\tswake_up_one(&rdp_toggling->nocb_state_wq);\n\t}\n\n\tmy_rdp->nocb_gp_seq = -1;\n\tWARN_ON(signal_pending(current));\n}\n\n \nstatic int rcu_nocb_gp_kthread(void *arg)\n{\n\tstruct rcu_data *rdp = arg;\n\n\tfor (;;) {\n\t\tWRITE_ONCE(rdp->nocb_gp_loops, rdp->nocb_gp_loops + 1);\n\t\tnocb_gp_wait(rdp);\n\t\tcond_resched_tasks_rcu_qs();\n\t}\n\treturn 0;\n}\n\nstatic inline bool nocb_cb_can_run(struct rcu_data *rdp)\n{\n\tu8 flags = SEGCBLIST_OFFLOADED | SEGCBLIST_KTHREAD_CB;\n\n\treturn rcu_segcblist_test_flags(&rdp->cblist, flags);\n}\n\nstatic inline bool nocb_cb_wait_cond(struct rcu_data *rdp)\n{\n\treturn nocb_cb_can_run(rdp) && !READ_ONCE(rdp->nocb_cb_sleep);\n}\n\n \nstatic void nocb_cb_wait(struct rcu_data *rdp)\n{\n\tstruct rcu_segcblist *cblist = &rdp->cblist;\n\tunsigned long cur_gp_seq;\n\tunsigned long flags;\n\tbool needwake_state = false;\n\tbool needwake_gp = false;\n\tbool can_sleep = true;\n\tstruct rcu_node *rnp = rdp->mynode;\n\n\tdo {\n\t\tswait_event_interruptible_exclusive(rdp->nocb_cb_wq,\n\t\t\t\t\t\t    nocb_cb_wait_cond(rdp));\n\n\t\t \n\t\tif (smp_load_acquire(&rdp->nocb_cb_sleep)) {  \n\t\t\tWARN_ON(signal_pending(current));\n\t\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"WokeEmpty\"));\n\t\t}\n\t} while (!nocb_cb_can_run(rdp));\n\n\n\tlocal_irq_save(flags);\n\trcu_momentary_dyntick_idle();\n\tlocal_irq_restore(flags);\n\t \n\tlocal_bh_disable();\n\trcu_do_batch(rdp);\n\tlocal_bh_enable();\n\tlockdep_assert_irqs_enabled();\n\trcu_nocb_lock_irqsave(rdp, flags);\n\tif (rcu_segcblist_nextgp(cblist, &cur_gp_seq) &&\n\t    rcu_seq_done(&rnp->gp_seq, cur_gp_seq) &&\n\t    raw_spin_trylock_rcu_node(rnp)) {  \n\t\tneedwake_gp = rcu_advance_cbs(rdp->mynode, rdp);\n\t\traw_spin_unlock_rcu_node(rnp);  \n\t}\n\n\tif (rcu_segcblist_test_flags(cblist, SEGCBLIST_OFFLOADED)) {\n\t\tif (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB)) {\n\t\t\trcu_segcblist_set_flags(cblist, SEGCBLIST_KTHREAD_CB);\n\t\t\tif (rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP))\n\t\t\t\tneedwake_state = true;\n\t\t}\n\t\tif (rcu_segcblist_ready_cbs(cblist))\n\t\t\tcan_sleep = false;\n\t} else {\n\t\t \n\t\tWARN_ON_ONCE(!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB));\n\t\trcu_segcblist_clear_flags(cblist, SEGCBLIST_KTHREAD_CB);\n\t\tif (!rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP))\n\t\t\tneedwake_state = true;\n\t}\n\n\tWRITE_ONCE(rdp->nocb_cb_sleep, can_sleep);\n\n\tif (rdp->nocb_cb_sleep)\n\t\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"CBSleep\"));\n\n\trcu_nocb_unlock_irqrestore(rdp, flags);\n\tif (needwake_gp)\n\t\trcu_gp_kthread_wake();\n\n\tif (needwake_state)\n\t\tswake_up_one(&rdp->nocb_state_wq);\n}\n\n \nstatic int rcu_nocb_cb_kthread(void *arg)\n{\n\tstruct rcu_data *rdp = arg;\n\n\t\n\t\n\tfor (;;) {\n\t\tnocb_cb_wait(rdp);\n\t\tcond_resched_tasks_rcu_qs();\n\t}\n\treturn 0;\n}\n\n \nstatic int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp, int level)\n{\n\treturn READ_ONCE(rdp->nocb_defer_wakeup) >= level;\n}\n\n \nstatic bool do_nocb_deferred_wakeup_common(struct rcu_data *rdp_gp,\n\t\t\t\t\t   struct rcu_data *rdp, int level,\n\t\t\t\t\t   unsigned long flags)\n\t__releases(rdp_gp->nocb_gp_lock)\n{\n\tint ndw;\n\tint ret;\n\n\tif (!rcu_nocb_need_deferred_wakeup(rdp_gp, level)) {\n\t\traw_spin_unlock_irqrestore(&rdp_gp->nocb_gp_lock, flags);\n\t\treturn false;\n\t}\n\n\tndw = rdp_gp->nocb_defer_wakeup;\n\tret = __wake_nocb_gp(rdp_gp, rdp, ndw == RCU_NOCB_WAKE_FORCE, flags);\n\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"DeferredWake\"));\n\n\treturn ret;\n}\n\n \nstatic void do_nocb_deferred_wakeup_timer(struct timer_list *t)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp = from_timer(rdp, t, nocb_timer);\n\n\tWARN_ON_ONCE(rdp->nocb_gp_rdp != rdp);\n\ttrace_rcu_nocb_wake(rcu_state.name, rdp->cpu, TPS(\"Timer\"));\n\n\traw_spin_lock_irqsave(&rdp->nocb_gp_lock, flags);\n\tsmp_mb__after_spinlock();  \n\tdo_nocb_deferred_wakeup_common(rdp, rdp, RCU_NOCB_WAKE_BYPASS, flags);\n}\n\n \nstatic bool do_nocb_deferred_wakeup(struct rcu_data *rdp)\n{\n\tunsigned long flags;\n\tstruct rcu_data *rdp_gp = rdp->nocb_gp_rdp;\n\n\tif (!rdp_gp || !rcu_nocb_need_deferred_wakeup(rdp_gp, RCU_NOCB_WAKE))\n\t\treturn false;\n\n\traw_spin_lock_irqsave(&rdp_gp->nocb_gp_lock, flags);\n\treturn do_nocb_deferred_wakeup_common(rdp_gp, rdp, RCU_NOCB_WAKE, flags);\n}\n\nvoid rcu_nocb_flush_deferred_wakeup(void)\n{\n\tdo_nocb_deferred_wakeup(this_cpu_ptr(&rcu_data));\n}\nEXPORT_SYMBOL_GPL(rcu_nocb_flush_deferred_wakeup);\n\nstatic int rdp_offload_toggle(struct rcu_data *rdp,\n\t\t\t       bool offload, unsigned long flags)\n\t__releases(rdp->nocb_lock)\n{\n\tstruct rcu_segcblist *cblist = &rdp->cblist;\n\tstruct rcu_data *rdp_gp = rdp->nocb_gp_rdp;\n\tbool wake_gp = false;\n\n\trcu_segcblist_offload(cblist, offload);\n\n\tif (rdp->nocb_cb_sleep)\n\t\trdp->nocb_cb_sleep = false;\n\trcu_nocb_unlock_irqrestore(rdp, flags);\n\n\t \n\tswake_up_one(&rdp->nocb_cb_wq);\n\n\traw_spin_lock_irqsave(&rdp_gp->nocb_gp_lock, flags);\n\t\n\tWRITE_ONCE(rdp_gp->nocb_toggling_rdp, rdp);\n\tif (rdp_gp->nocb_gp_sleep) {\n\t\trdp_gp->nocb_gp_sleep = false;\n\t\twake_gp = true;\n\t}\n\traw_spin_unlock_irqrestore(&rdp_gp->nocb_gp_lock, flags);\n\n\treturn wake_gp;\n}\n\nstatic long rcu_nocb_rdp_deoffload(void *arg)\n{\n\tstruct rcu_data *rdp = arg;\n\tstruct rcu_segcblist *cblist = &rdp->cblist;\n\tunsigned long flags;\n\tint wake_gp;\n\tstruct rcu_data *rdp_gp = rdp->nocb_gp_rdp;\n\n\t \n\tWARN_ON_ONCE((rdp->cpu != raw_smp_processor_id()) && cpu_online(rdp->cpu));\n\n\tpr_info(\"De-offloading %d\\n\", rdp->cpu);\n\n\trcu_nocb_lock_irqsave(rdp, flags);\n\t \n\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));\n\t \n\trcu_segcblist_set_flags(cblist, SEGCBLIST_RCU_CORE);\n\tinvoke_rcu_core();\n\twake_gp = rdp_offload_toggle(rdp, false, flags);\n\n\tmutex_lock(&rdp_gp->nocb_gp_kthread_mutex);\n\tif (rdp_gp->nocb_gp_kthread) {\n\t\tif (wake_gp)\n\t\t\twake_up_process(rdp_gp->nocb_gp_kthread);\n\n\t\t \n\t\tif (!rdp->nocb_cb_kthread) {\n\t\t\trcu_nocb_lock_irqsave(rdp, flags);\n\t\t\trcu_segcblist_clear_flags(&rdp->cblist, SEGCBLIST_KTHREAD_CB);\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t}\n\n\t\tswait_event_exclusive(rdp->nocb_state_wq,\n\t\t\t\t\t!rcu_segcblist_test_flags(cblist,\n\t\t\t\t\t  SEGCBLIST_KTHREAD_CB | SEGCBLIST_KTHREAD_GP));\n\t} else {\n\t\t \n\t\trcu_nocb_lock_irqsave(rdp, flags);\n\t\trcu_segcblist_clear_flags(&rdp->cblist,\n\t\t\t\tSEGCBLIST_KTHREAD_CB | SEGCBLIST_KTHREAD_GP);\n\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\n\t\tlist_del(&rdp->nocb_entry_rdp);\n\t}\n\tmutex_unlock(&rdp_gp->nocb_gp_kthread_mutex);\n\n\t \n\trcu_nocb_lock_irqsave(rdp, flags);\n\t \n\trcu_segcblist_clear_flags(cblist, SEGCBLIST_LOCKING);\n\t \n\traw_spin_unlock_irqrestore(&rdp->nocb_lock, flags);\n\n\t \n\tWARN_ON_ONCE(rcu_cblist_n_cbs(&rdp->nocb_bypass));\n\n\n\treturn 0;\n}\n\nint rcu_nocb_cpu_deoffload(int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tint ret = 0;\n\n\tcpus_read_lock();\n\tmutex_lock(&rcu_state.barrier_mutex);\n\tif (rcu_rdp_is_offloaded(rdp)) {\n\t\tif (cpu_online(cpu)) {\n\t\t\tret = work_on_cpu(cpu, rcu_nocb_rdp_deoffload, rdp);\n\t\t\tif (!ret)\n\t\t\t\tcpumask_clear_cpu(cpu, rcu_nocb_mask);\n\t\t} else {\n\t\t\tpr_info(\"NOCB: Cannot CB-deoffload offline CPU %d\\n\", rdp->cpu);\n\t\t\tret = -EINVAL;\n\t\t}\n\t}\n\tmutex_unlock(&rcu_state.barrier_mutex);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rcu_nocb_cpu_deoffload);\n\nstatic long rcu_nocb_rdp_offload(void *arg)\n{\n\tstruct rcu_data *rdp = arg;\n\tstruct rcu_segcblist *cblist = &rdp->cblist;\n\tunsigned long flags;\n\tint wake_gp;\n\tstruct rcu_data *rdp_gp = rdp->nocb_gp_rdp;\n\n\tWARN_ON_ONCE(rdp->cpu != raw_smp_processor_id());\n\t \n\tif (!rdp->nocb_gp_rdp)\n\t\treturn -EINVAL;\n\n\tif (WARN_ON_ONCE(!rdp_gp->nocb_gp_kthread))\n\t\treturn -EINVAL;\n\n\tpr_info(\"Offloading %d\\n\", rdp->cpu);\n\n\t \n\traw_spin_lock_irqsave(&rdp->nocb_lock, flags);\n\n\t \n\twake_gp = rdp_offload_toggle(rdp, true, flags);\n\tif (wake_gp)\n\t\twake_up_process(rdp_gp->nocb_gp_kthread);\n\tswait_event_exclusive(rdp->nocb_state_wq,\n\t\t\t      rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_CB) &&\n\t\t\t      rcu_segcblist_test_flags(cblist, SEGCBLIST_KTHREAD_GP));\n\n\t \n\trcu_nocb_lock_irqsave(rdp, flags);\n\trcu_segcblist_clear_flags(cblist, SEGCBLIST_RCU_CORE);\n\trcu_nocb_unlock_irqrestore(rdp, flags);\n\n\treturn 0;\n}\n\nint rcu_nocb_cpu_offload(int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tint ret = 0;\n\n\tcpus_read_lock();\n\tmutex_lock(&rcu_state.barrier_mutex);\n\tif (!rcu_rdp_is_offloaded(rdp)) {\n\t\tif (cpu_online(cpu)) {\n\t\t\tret = work_on_cpu(cpu, rcu_nocb_rdp_offload, rdp);\n\t\t\tif (!ret)\n\t\t\t\tcpumask_set_cpu(cpu, rcu_nocb_mask);\n\t\t} else {\n\t\t\tpr_info(\"NOCB: Cannot CB-offload offline CPU %d\\n\", rdp->cpu);\n\t\t\tret = -EINVAL;\n\t\t}\n\t}\n\tmutex_unlock(&rcu_state.barrier_mutex);\n\tcpus_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(rcu_nocb_cpu_offload);\n\n#ifdef CONFIG_RCU_LAZY\nstatic unsigned long\nlazy_rcu_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tint cpu;\n\tunsigned long count = 0;\n\n\tif (WARN_ON_ONCE(!cpumask_available(rcu_nocb_mask)))\n\t\treturn 0;\n\n\t \n\tif (!mutex_trylock(&rcu_state.barrier_mutex))\n\t\treturn 0;\n\n\t \n\tfor_each_cpu(cpu, rcu_nocb_mask) {\n\t\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\n\t\tcount +=  READ_ONCE(rdp->lazy_len);\n\t}\n\n\tmutex_unlock(&rcu_state.barrier_mutex);\n\n\treturn count ? count : SHRINK_EMPTY;\n}\n\nstatic unsigned long\nlazy_rcu_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tint cpu;\n\tunsigned long flags;\n\tunsigned long count = 0;\n\n\tif (WARN_ON_ONCE(!cpumask_available(rcu_nocb_mask)))\n\t\treturn 0;\n\t \n\tif (!mutex_trylock(&rcu_state.barrier_mutex)) {\n\t\t \n\t\treturn 0;\n\t}\n\n\t \n\tfor_each_cpu(cpu, rcu_nocb_mask) {\n\t\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tint _count;\n\n\t\tif (WARN_ON_ONCE(!rcu_rdp_is_offloaded(rdp)))\n\t\t\tcontinue;\n\n\t\tif (!READ_ONCE(rdp->lazy_len))\n\t\t\tcontinue;\n\n\t\trcu_nocb_lock_irqsave(rdp, flags);\n\t\t \n\t\t_count = READ_ONCE(rdp->lazy_len);\n\t\tif (!_count) {\n\t\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\t\tcontinue;\n\t\t}\n\t\tWARN_ON_ONCE(!rcu_nocb_flush_bypass(rdp, NULL, jiffies, false));\n\t\trcu_nocb_unlock_irqrestore(rdp, flags);\n\t\twake_nocb_gp(rdp, false);\n\t\tsc->nr_to_scan -= _count;\n\t\tcount += _count;\n\t\tif (sc->nr_to_scan <= 0)\n\t\t\tbreak;\n\t}\n\n\tmutex_unlock(&rcu_state.barrier_mutex);\n\n\treturn count ? count : SHRINK_STOP;\n}\n\nstatic struct shrinker lazy_rcu_shrinker = {\n\t.count_objects = lazy_rcu_shrink_count,\n\t.scan_objects = lazy_rcu_shrink_scan,\n\t.batch = 0,\n\t.seeks = DEFAULT_SEEKS,\n};\n#endif \n\nvoid __init rcu_init_nohz(void)\n{\n\tint cpu;\n\tstruct rcu_data *rdp;\n\tconst struct cpumask *cpumask = NULL;\n\n#if defined(CONFIG_NO_HZ_FULL)\n\tif (tick_nohz_full_running && !cpumask_empty(tick_nohz_full_mask))\n\t\tcpumask = tick_nohz_full_mask;\n#endif\n\n\tif (IS_ENABLED(CONFIG_RCU_NOCB_CPU_DEFAULT_ALL) &&\n\t    !rcu_state.nocb_is_setup && !cpumask)\n\t\tcpumask = cpu_possible_mask;\n\n\tif (cpumask) {\n\t\tif (!cpumask_available(rcu_nocb_mask)) {\n\t\t\tif (!zalloc_cpumask_var(&rcu_nocb_mask, GFP_KERNEL)) {\n\t\t\t\tpr_info(\"rcu_nocb_mask allocation failed, callback offloading disabled.\\n\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tcpumask_or(rcu_nocb_mask, rcu_nocb_mask, cpumask);\n\t\trcu_state.nocb_is_setup = true;\n\t}\n\n\tif (!rcu_state.nocb_is_setup)\n\t\treturn;\n\n#ifdef CONFIG_RCU_LAZY\n\tif (register_shrinker(&lazy_rcu_shrinker, \"rcu-lazy\"))\n\t\tpr_err(\"Failed to register lazy_rcu shrinker!\\n\");\n#endif \n\n\tif (!cpumask_subset(rcu_nocb_mask, cpu_possible_mask)) {\n\t\tpr_info(\"\\tNote: kernel parameter 'rcu_nocbs=', 'nohz_full', or 'isolcpus=' contains nonexistent CPUs.\\n\");\n\t\tcpumask_and(rcu_nocb_mask, cpu_possible_mask,\n\t\t\t    rcu_nocb_mask);\n\t}\n\tif (cpumask_empty(rcu_nocb_mask))\n\t\tpr_info(\"\\tOffload RCU callbacks from CPUs: (none).\\n\");\n\telse\n\t\tpr_info(\"\\tOffload RCU callbacks from CPUs: %*pbl.\\n\",\n\t\t\tcpumask_pr_args(rcu_nocb_mask));\n\tif (rcu_nocb_poll)\n\t\tpr_info(\"\\tPoll for callbacks from no-CBs CPUs.\\n\");\n\n\tfor_each_cpu(cpu, rcu_nocb_mask) {\n\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tif (rcu_segcblist_empty(&rdp->cblist))\n\t\t\trcu_segcblist_init(&rdp->cblist);\n\t\trcu_segcblist_offload(&rdp->cblist, true);\n\t\trcu_segcblist_set_flags(&rdp->cblist, SEGCBLIST_KTHREAD_CB | SEGCBLIST_KTHREAD_GP);\n\t\trcu_segcblist_clear_flags(&rdp->cblist, SEGCBLIST_RCU_CORE);\n\t}\n\trcu_organize_nocb_kthreads();\n}\n\n \nstatic void __init rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp)\n{\n\tinit_swait_queue_head(&rdp->nocb_cb_wq);\n\tinit_swait_queue_head(&rdp->nocb_gp_wq);\n\tinit_swait_queue_head(&rdp->nocb_state_wq);\n\traw_spin_lock_init(&rdp->nocb_lock);\n\traw_spin_lock_init(&rdp->nocb_bypass_lock);\n\traw_spin_lock_init(&rdp->nocb_gp_lock);\n\ttimer_setup(&rdp->nocb_timer, do_nocb_deferred_wakeup_timer, 0);\n\trcu_cblist_init(&rdp->nocb_bypass);\n\tWRITE_ONCE(rdp->lazy_len, 0);\n\tmutex_init(&rdp->nocb_gp_kthread_mutex);\n}\n\n \nstatic void rcu_spawn_cpu_nocb_kthread(int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n\tstruct rcu_data *rdp_gp;\n\tstruct task_struct *t;\n\tstruct sched_param sp;\n\n\tif (!rcu_scheduler_fully_active || !rcu_state.nocb_is_setup)\n\t\treturn;\n\n\t \n\tif (rdp->nocb_cb_kthread)\n\t\treturn;\n\n\t \n\tsp.sched_priority = kthread_prio;\n\trdp_gp = rdp->nocb_gp_rdp;\n\tmutex_lock(&rdp_gp->nocb_gp_kthread_mutex);\n\tif (!rdp_gp->nocb_gp_kthread) {\n\t\tt = kthread_run(rcu_nocb_gp_kthread, rdp_gp,\n\t\t\t\t\"rcuog/%d\", rdp_gp->cpu);\n\t\tif (WARN_ONCE(IS_ERR(t), \"%s: Could not start rcuo GP kthread, OOM is now expected behavior\\n\", __func__)) {\n\t\t\tmutex_unlock(&rdp_gp->nocb_gp_kthread_mutex);\n\t\t\tgoto end;\n\t\t}\n\t\tWRITE_ONCE(rdp_gp->nocb_gp_kthread, t);\n\t\tif (kthread_prio)\n\t\t\tsched_setscheduler_nocheck(t, SCHED_FIFO, &sp);\n\t}\n\tmutex_unlock(&rdp_gp->nocb_gp_kthread_mutex);\n\n\t \n\tt = kthread_run(rcu_nocb_cb_kthread, rdp,\n\t\t\t\"rcuo%c/%d\", rcu_state.abbr, cpu);\n\tif (WARN_ONCE(IS_ERR(t), \"%s: Could not start rcuo CB kthread, OOM is now expected behavior\\n\", __func__))\n\t\tgoto end;\n\n\tif (IS_ENABLED(CONFIG_RCU_NOCB_CPU_CB_BOOST) && kthread_prio)\n\t\tsched_setscheduler_nocheck(t, SCHED_FIFO, &sp);\n\n\tWRITE_ONCE(rdp->nocb_cb_kthread, t);\n\tWRITE_ONCE(rdp->nocb_gp_kthread, rdp_gp->nocb_gp_kthread);\n\treturn;\nend:\n\tmutex_lock(&rcu_state.barrier_mutex);\n\tif (rcu_rdp_is_offloaded(rdp)) {\n\t\trcu_nocb_rdp_deoffload(rdp);\n\t\tcpumask_clear_cpu(cpu, rcu_nocb_mask);\n\t}\n\tmutex_unlock(&rcu_state.barrier_mutex);\n}\n\n \nstatic int rcu_nocb_gp_stride = -1;\nmodule_param(rcu_nocb_gp_stride, int, 0444);\n\n \nstatic void __init rcu_organize_nocb_kthreads(void)\n{\n\tint cpu;\n\tbool firsttime = true;\n\tbool gotnocbs = false;\n\tbool gotnocbscbs = true;\n\tint ls = rcu_nocb_gp_stride;\n\tint nl = 0;   \n\tstruct rcu_data *rdp;\n\tstruct rcu_data *rdp_gp = NULL;   \n\n\tif (!cpumask_available(rcu_nocb_mask))\n\t\treturn;\n\tif (ls == -1) {\n\t\tls = nr_cpu_ids / int_sqrt(nr_cpu_ids);\n\t\trcu_nocb_gp_stride = ls;\n\t}\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tif (rdp->cpu >= nl) {\n\t\t\t \n\t\t\tgotnocbs = true;\n\t\t\tnl = DIV_ROUND_UP(rdp->cpu + 1, ls) * ls;\n\t\t\trdp_gp = rdp;\n\t\t\tINIT_LIST_HEAD(&rdp->nocb_head_rdp);\n\t\t\tif (dump_tree) {\n\t\t\t\tif (!firsttime)\n\t\t\t\t\tpr_cont(\"%s\\n\", gotnocbscbs\n\t\t\t\t\t\t\t? \"\" : \" (self only)\");\n\t\t\t\tgotnocbscbs = false;\n\t\t\t\tfirsttime = false;\n\t\t\t\tpr_alert(\"%s: No-CB GP kthread CPU %d:\",\n\t\t\t\t\t __func__, cpu);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tgotnocbscbs = true;\n\t\t\tif (dump_tree)\n\t\t\t\tpr_cont(\" %d\", cpu);\n\t\t}\n\t\trdp->nocb_gp_rdp = rdp_gp;\n\t\tif (cpumask_test_cpu(cpu, rcu_nocb_mask))\n\t\t\tlist_add_tail(&rdp->nocb_entry_rdp, &rdp_gp->nocb_head_rdp);\n\t}\n\tif (gotnocbs && dump_tree)\n\t\tpr_cont(\"%s\\n\", gotnocbscbs ? \"\" : \" (self only)\");\n}\n\n \nvoid rcu_bind_current_to_nocb(void)\n{\n\tif (cpumask_available(rcu_nocb_mask) && !cpumask_empty(rcu_nocb_mask))\n\t\tWARN_ON(sched_setaffinity(current->pid, rcu_nocb_mask));\n}\nEXPORT_SYMBOL_GPL(rcu_bind_current_to_nocb);\n\n\n#ifdef CONFIG_SMP\nstatic char *show_rcu_should_be_on_cpu(struct task_struct *tsp)\n{\n\treturn tsp && task_is_running(tsp) && !tsp->on_cpu ? \"!\" : \"\";\n}\n#else \nstatic char *show_rcu_should_be_on_cpu(struct task_struct *tsp)\n{\n\treturn \"\";\n}\n#endif \n\n \nstatic void show_rcu_nocb_gp_state(struct rcu_data *rdp)\n{\n\tstruct rcu_node *rnp = rdp->mynode;\n\n\tpr_info(\"nocb GP %d %c%c%c%c%c %c[%c%c] %c%c:%ld rnp %d:%d %lu %c CPU %d%s\\n\",\n\t\trdp->cpu,\n\t\t\"kK\"[!!rdp->nocb_gp_kthread],\n\t\t\"lL\"[raw_spin_is_locked(&rdp->nocb_gp_lock)],\n\t\t\"dD\"[!!rdp->nocb_defer_wakeup],\n\t\t\"tT\"[timer_pending(&rdp->nocb_timer)],\n\t\t\"sS\"[!!rdp->nocb_gp_sleep],\n\t\t\".W\"[swait_active(&rdp->nocb_gp_wq)],\n\t\t\".W\"[swait_active(&rnp->nocb_gp_wq[0])],\n\t\t\".W\"[swait_active(&rnp->nocb_gp_wq[1])],\n\t\t\".B\"[!!rdp->nocb_gp_bypass],\n\t\t\".G\"[!!rdp->nocb_gp_gp],\n\t\t(long)rdp->nocb_gp_seq,\n\t\trnp->grplo, rnp->grphi, READ_ONCE(rdp->nocb_gp_loops),\n\t\trdp->nocb_gp_kthread ? task_state_to_char(rdp->nocb_gp_kthread) : '.',\n\t\trdp->nocb_gp_kthread ? (int)task_cpu(rdp->nocb_gp_kthread) : -1,\n\t\tshow_rcu_should_be_on_cpu(rdp->nocb_gp_kthread));\n}\n\n \nstatic void show_rcu_nocb_state(struct rcu_data *rdp)\n{\n\tchar bufw[20];\n\tchar bufr[20];\n\tstruct rcu_data *nocb_next_rdp;\n\tstruct rcu_segcblist *rsclp = &rdp->cblist;\n\tbool waslocked;\n\tbool wassleep;\n\n\tif (rdp->nocb_gp_rdp == rdp)\n\t\tshow_rcu_nocb_gp_state(rdp);\n\n\tnocb_next_rdp = list_next_or_null_rcu(&rdp->nocb_gp_rdp->nocb_head_rdp,\n\t\t\t\t\t      &rdp->nocb_entry_rdp,\n\t\t\t\t\t      typeof(*rdp),\n\t\t\t\t\t      nocb_entry_rdp);\n\n\tsprintf(bufw, \"%ld\", rsclp->gp_seq[RCU_WAIT_TAIL]);\n\tsprintf(bufr, \"%ld\", rsclp->gp_seq[RCU_NEXT_READY_TAIL]);\n\tpr_info(\"   CB %d^%d->%d %c%c%c%c%c%c F%ld L%ld C%d %c%c%s%c%s%c%c q%ld %c CPU %d%s\\n\",\n\t\trdp->cpu, rdp->nocb_gp_rdp->cpu,\n\t\tnocb_next_rdp ? nocb_next_rdp->cpu : -1,\n\t\t\"kK\"[!!rdp->nocb_cb_kthread],\n\t\t\"bB\"[raw_spin_is_locked(&rdp->nocb_bypass_lock)],\n\t\t\"cC\"[!!atomic_read(&rdp->nocb_lock_contended)],\n\t\t\"lL\"[raw_spin_is_locked(&rdp->nocb_lock)],\n\t\t\"sS\"[!!rdp->nocb_cb_sleep],\n\t\t\".W\"[swait_active(&rdp->nocb_cb_wq)],\n\t\tjiffies - rdp->nocb_bypass_first,\n\t\tjiffies - rdp->nocb_nobypass_last,\n\t\trdp->nocb_nobypass_count,\n\t\t\".D\"[rcu_segcblist_ready_cbs(rsclp)],\n\t\t\".W\"[!rcu_segcblist_segempty(rsclp, RCU_WAIT_TAIL)],\n\t\trcu_segcblist_segempty(rsclp, RCU_WAIT_TAIL) ? \"\" : bufw,\n\t\t\".R\"[!rcu_segcblist_segempty(rsclp, RCU_NEXT_READY_TAIL)],\n\t\trcu_segcblist_segempty(rsclp, RCU_NEXT_READY_TAIL) ? \"\" : bufr,\n\t\t\".N\"[!rcu_segcblist_segempty(rsclp, RCU_NEXT_TAIL)],\n\t\t\".B\"[!!rcu_cblist_n_cbs(&rdp->nocb_bypass)],\n\t\trcu_segcblist_n_cbs(&rdp->cblist),\n\t\trdp->nocb_cb_kthread ? task_state_to_char(rdp->nocb_cb_kthread) : '.',\n\t\trdp->nocb_cb_kthread ? (int)task_cpu(rdp->nocb_cb_kthread) : -1,\n\t\tshow_rcu_should_be_on_cpu(rdp->nocb_cb_kthread));\n\n\t \n\tif (rdp->nocb_gp_rdp == rdp)\n\t\treturn;\n\n\twaslocked = raw_spin_is_locked(&rdp->nocb_gp_lock);\n\twassleep = swait_active(&rdp->nocb_gp_wq);\n\tif (!rdp->nocb_gp_sleep && !waslocked && !wassleep)\n\t\treturn;   \n\n\tpr_info(\"   nocb GP activity on CB-only CPU!!! %c%c%c %c\\n\",\n\t\t\"lL\"[waslocked],\n\t\t\"dD\"[!!rdp->nocb_defer_wakeup],\n\t\t\"sS\"[!!rdp->nocb_gp_sleep],\n\t\t\".W\"[wassleep]);\n}\n\n#else  \n\nstatic inline int rcu_lockdep_is_held_nocb(struct rcu_data *rdp)\n{\n\treturn 0;\n}\n\nstatic inline bool rcu_current_is_nocb_kthread(struct rcu_data *rdp)\n{\n\treturn false;\n}\n\n \nstatic void rcu_nocb_lock(struct rcu_data *rdp)\n{\n}\n\n \nstatic void rcu_nocb_unlock(struct rcu_data *rdp)\n{\n}\n\n \nstatic void rcu_nocb_unlock_irqrestore(struct rcu_data *rdp,\n\t\t\t\t       unsigned long flags)\n{\n\tlocal_irq_restore(flags);\n}\n\n \nstatic void rcu_lockdep_assert_cblist_protected(struct rcu_data *rdp)\n{\n\tlockdep_assert_irqs_disabled();\n}\n\nstatic void rcu_nocb_gp_cleanup(struct swait_queue_head *sq)\n{\n}\n\nstatic struct swait_queue_head *rcu_nocb_gp_get(struct rcu_node *rnp)\n{\n\treturn NULL;\n}\n\nstatic void rcu_init_one_nocb(struct rcu_node *rnp)\n{\n}\n\nstatic bool wake_nocb_gp(struct rcu_data *rdp, bool force)\n{\n\treturn false;\n}\n\nstatic bool rcu_nocb_flush_bypass(struct rcu_data *rdp, struct rcu_head *rhp,\n\t\t\t\t  unsigned long j, bool lazy)\n{\n\treturn true;\n}\n\nstatic bool rcu_nocb_try_bypass(struct rcu_data *rdp, struct rcu_head *rhp,\n\t\t\t\tbool *was_alldone, unsigned long flags, bool lazy)\n{\n\treturn false;\n}\n\nstatic void __call_rcu_nocb_wake(struct rcu_data *rdp, bool was_empty,\n\t\t\t\t unsigned long flags)\n{\n\tWARN_ON_ONCE(1);   \n}\n\nstatic void __init rcu_boot_init_nocb_percpu_data(struct rcu_data *rdp)\n{\n}\n\nstatic int rcu_nocb_need_deferred_wakeup(struct rcu_data *rdp, int level)\n{\n\treturn false;\n}\n\nstatic bool do_nocb_deferred_wakeup(struct rcu_data *rdp)\n{\n\treturn false;\n}\n\nstatic void rcu_spawn_cpu_nocb_kthread(int cpu)\n{\n}\n\nstatic void show_rcu_nocb_state(struct rcu_data *rdp)\n{\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}