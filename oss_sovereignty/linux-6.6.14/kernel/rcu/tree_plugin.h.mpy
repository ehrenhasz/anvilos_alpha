{
  "module_name": "tree_plugin.h",
  "hash_id": "d484ad255c83502af9ab53e6ee266d7c6694f2fc7616a3b2072e7418eb0abb68",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/tree_plugin.h",
  "human_readable_source": " \n \n\n#include \"../locking/rtmutex_common.h\"\n\nstatic bool rcu_rdp_is_offloaded(struct rcu_data *rdp)\n{\n\t \n\tRCU_LOCKDEP_WARN(\n\t\t!(lockdep_is_held(&rcu_state.barrier_mutex) ||\n\t\t  (IS_ENABLED(CONFIG_HOTPLUG_CPU) && lockdep_is_cpus_held()) ||\n\t\t  rcu_lockdep_is_held_nocb(rdp) ||\n\t\t  (rdp == this_cpu_ptr(&rcu_data) &&\n\t\t   !(IS_ENABLED(CONFIG_PREEMPT_COUNT) && preemptible())) ||\n\t\t  rcu_current_is_nocb_kthread(rdp)),\n\t\t\"Unsafe read of RCU_NOCB offloaded state\"\n\t);\n\n\treturn rcu_segcblist_is_offloaded(&rdp->cblist);\n}\n\n \nstatic void __init rcu_bootup_announce_oddness(void)\n{\n\tif (IS_ENABLED(CONFIG_RCU_TRACE))\n\t\tpr_info(\"\\tRCU event tracing is enabled.\\n\");\n\tif ((IS_ENABLED(CONFIG_64BIT) && RCU_FANOUT != 64) ||\n\t    (!IS_ENABLED(CONFIG_64BIT) && RCU_FANOUT != 32))\n\t\tpr_info(\"\\tCONFIG_RCU_FANOUT set to non-default value of %d.\\n\",\n\t\t\tRCU_FANOUT);\n\tif (rcu_fanout_exact)\n\t\tpr_info(\"\\tHierarchical RCU autobalancing is disabled.\\n\");\n\tif (IS_ENABLED(CONFIG_PROVE_RCU))\n\t\tpr_info(\"\\tRCU lockdep checking is enabled.\\n\");\n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))\n\t\tpr_info(\"\\tRCU strict (and thus non-scalable) grace periods are enabled.\\n\");\n\tif (RCU_NUM_LVLS >= 4)\n\t\tpr_info(\"\\tFour(or more)-level hierarchy is enabled.\\n\");\n\tif (RCU_FANOUT_LEAF != 16)\n\t\tpr_info(\"\\tBuild-time adjustment of leaf fanout to %d.\\n\",\n\t\t\tRCU_FANOUT_LEAF);\n\tif (rcu_fanout_leaf != RCU_FANOUT_LEAF)\n\t\tpr_info(\"\\tBoot-time adjustment of leaf fanout to %d.\\n\",\n\t\t\trcu_fanout_leaf);\n\tif (nr_cpu_ids != NR_CPUS)\n\t\tpr_info(\"\\tRCU restricting CPUs from NR_CPUS=%d to nr_cpu_ids=%u.\\n\", NR_CPUS, nr_cpu_ids);\n#ifdef CONFIG_RCU_BOOST\n\tpr_info(\"\\tRCU priority boosting: priority %d delay %d ms.\\n\",\n\t\tkthread_prio, CONFIG_RCU_BOOST_DELAY);\n#endif\n\tif (blimit != DEFAULT_RCU_BLIMIT)\n\t\tpr_info(\"\\tBoot-time adjustment of callback invocation limit to %ld.\\n\", blimit);\n\tif (qhimark != DEFAULT_RCU_QHIMARK)\n\t\tpr_info(\"\\tBoot-time adjustment of callback high-water mark to %ld.\\n\", qhimark);\n\tif (qlowmark != DEFAULT_RCU_QLOMARK)\n\t\tpr_info(\"\\tBoot-time adjustment of callback low-water mark to %ld.\\n\", qlowmark);\n\tif (qovld != DEFAULT_RCU_QOVLD)\n\t\tpr_info(\"\\tBoot-time adjustment of callback overload level to %ld.\\n\", qovld);\n\tif (jiffies_till_first_fqs != ULONG_MAX)\n\t\tpr_info(\"\\tBoot-time adjustment of first FQS scan delay to %ld jiffies.\\n\", jiffies_till_first_fqs);\n\tif (jiffies_till_next_fqs != ULONG_MAX)\n\t\tpr_info(\"\\tBoot-time adjustment of subsequent FQS scan delay to %ld jiffies.\\n\", jiffies_till_next_fqs);\n\tif (jiffies_till_sched_qs != ULONG_MAX)\n\t\tpr_info(\"\\tBoot-time adjustment of scheduler-enlistment delay to %ld jiffies.\\n\", jiffies_till_sched_qs);\n\tif (rcu_kick_kthreads)\n\t\tpr_info(\"\\tKick kthreads if too-long grace period.\\n\");\n\tif (IS_ENABLED(CONFIG_DEBUG_OBJECTS_RCU_HEAD))\n\t\tpr_info(\"\\tRCU callback double-/use-after-free debug is enabled.\\n\");\n\tif (gp_preinit_delay)\n\t\tpr_info(\"\\tRCU debug GP pre-init slowdown %d jiffies.\\n\", gp_preinit_delay);\n\tif (gp_init_delay)\n\t\tpr_info(\"\\tRCU debug GP init slowdown %d jiffies.\\n\", gp_init_delay);\n\tif (gp_cleanup_delay)\n\t\tpr_info(\"\\tRCU debug GP cleanup slowdown %d jiffies.\\n\", gp_cleanup_delay);\n\tif (!use_softirq)\n\t\tpr_info(\"\\tRCU_SOFTIRQ processing moved to rcuc kthreads.\\n\");\n\tif (IS_ENABLED(CONFIG_RCU_EQS_DEBUG))\n\t\tpr_info(\"\\tRCU debug extended QS entry/exit.\\n\");\n\trcupdate_announce_bootup_oddness();\n}\n\n#ifdef CONFIG_PREEMPT_RCU\n\nstatic void rcu_report_exp_rnp(struct rcu_node *rnp, bool wake);\nstatic void rcu_read_unlock_special(struct task_struct *t);\n\n \nstatic void __init rcu_bootup_announce(void)\n{\n\tpr_info(\"Preemptible hierarchical RCU implementation.\\n\");\n\trcu_bootup_announce_oddness();\n}\n\n \n#define RCU_GP_TASKS\t0x8\n#define RCU_EXP_TASKS\t0x4\n#define RCU_GP_BLKD\t0x2\n#define RCU_EXP_BLKD\t0x1\n\n \nstatic void rcu_preempt_ctxt_queue(struct rcu_node *rnp, struct rcu_data *rdp)\n\t__releases(rnp->lock)  \n{\n\tint blkd_state = (rnp->gp_tasks ? RCU_GP_TASKS : 0) +\n\t\t\t (rnp->exp_tasks ? RCU_EXP_TASKS : 0) +\n\t\t\t (rnp->qsmask & rdp->grpmask ? RCU_GP_BLKD : 0) +\n\t\t\t (rnp->expmask & rdp->grpmask ? RCU_EXP_BLKD : 0);\n\tstruct task_struct *t = current;\n\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tWARN_ON_ONCE(rdp->mynode != rnp);\n\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp));\n\t \n\tWARN_ON_ONCE(rnp->qsmaskinitnext & ~rnp->qsmaskinit & rnp->qsmask &\n\t\t     rdp->grpmask);\n\n\t \n\tswitch (blkd_state) {\n\tcase 0:\n\tcase                RCU_EXP_TASKS:\n\tcase                RCU_EXP_TASKS + RCU_GP_BLKD:\n\tcase RCU_GP_TASKS:\n\tcase RCU_GP_TASKS + RCU_EXP_TASKS:\n\n\t\t \n\t\tlist_add(&t->rcu_node_entry, &rnp->blkd_tasks);\n\t\tbreak;\n\n\tcase                                              RCU_EXP_BLKD:\n\tcase                                RCU_GP_BLKD:\n\tcase                                RCU_GP_BLKD + RCU_EXP_BLKD:\n\tcase RCU_GP_TASKS +                               RCU_EXP_BLKD:\n\tcase RCU_GP_TASKS +                 RCU_GP_BLKD + RCU_EXP_BLKD:\n\tcase RCU_GP_TASKS + RCU_EXP_TASKS + RCU_GP_BLKD + RCU_EXP_BLKD:\n\n\t\t \n\t\tlist_add_tail(&t->rcu_node_entry, &rnp->blkd_tasks);\n\t\tbreak;\n\n\tcase                RCU_EXP_TASKS +               RCU_EXP_BLKD:\n\tcase                RCU_EXP_TASKS + RCU_GP_BLKD + RCU_EXP_BLKD:\n\tcase RCU_GP_TASKS + RCU_EXP_TASKS +               RCU_EXP_BLKD:\n\n\t\t \n\t\tlist_add(&t->rcu_node_entry, rnp->exp_tasks);\n\t\tbreak;\n\n\tcase RCU_GP_TASKS +                 RCU_GP_BLKD:\n\tcase RCU_GP_TASKS + RCU_EXP_TASKS + RCU_GP_BLKD:\n\n\t\t \n\t\tlist_add(&t->rcu_node_entry, rnp->gp_tasks);\n\t\tbreak;\n\n\tdefault:\n\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n\n\t \n\tif (!rnp->gp_tasks && (blkd_state & RCU_GP_BLKD)) {\n\t\tWRITE_ONCE(rnp->gp_tasks, &t->rcu_node_entry);\n\t\tWARN_ON_ONCE(rnp->completedqs == rnp->gp_seq);\n\t}\n\tif (!rnp->exp_tasks && (blkd_state & RCU_EXP_BLKD))\n\t\tWRITE_ONCE(rnp->exp_tasks, &t->rcu_node_entry);\n\tWARN_ON_ONCE(!(blkd_state & RCU_GP_BLKD) !=\n\t\t     !(rnp->qsmask & rdp->grpmask));\n\tWARN_ON_ONCE(!(blkd_state & RCU_EXP_BLKD) !=\n\t\t     !(rnp->expmask & rdp->grpmask));\n\traw_spin_unlock_rcu_node(rnp);  \n\n\t \n\tif (blkd_state & RCU_EXP_BLKD && rdp->cpu_no_qs.b.exp)\n\t\trcu_report_exp_rdp(rdp);\n\telse\n\t\tWARN_ON_ONCE(rdp->cpu_no_qs.b.exp);\n}\n\n \nstatic void rcu_qs(void)\n{\n\tRCU_LOCKDEP_WARN(preemptible(), \"rcu_qs() invoked with preemption enabled!!!\\n\");\n\tif (__this_cpu_read(rcu_data.cpu_no_qs.b.norm)) {\n\t\ttrace_rcu_grace_period(TPS(\"rcu_preempt\"),\n\t\t\t\t       __this_cpu_read(rcu_data.gp_seq),\n\t\t\t\t       TPS(\"cpuqs\"));\n\t\t__this_cpu_write(rcu_data.cpu_no_qs.b.norm, false);\n\t\tbarrier();  \n\t\tWRITE_ONCE(current->rcu_read_unlock_special.b.need_qs, false);\n\t}\n}\n\n \nvoid rcu_note_context_switch(bool preempt)\n{\n\tstruct task_struct *t = current;\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\tstruct rcu_node *rnp;\n\n\ttrace_rcu_utilization(TPS(\"Start context switch\"));\n\tlockdep_assert_irqs_disabled();\n\tWARN_ONCE(!preempt && rcu_preempt_depth() > 0, \"Voluntary context switch within RCU read-side critical section!\");\n\tif (rcu_preempt_depth() > 0 &&\n\t    !t->rcu_read_unlock_special.b.blocked) {\n\n\t\t \n\t\trnp = rdp->mynode;\n\t\traw_spin_lock_rcu_node(rnp);\n\t\tt->rcu_read_unlock_special.b.blocked = true;\n\t\tt->rcu_blocked_node = rnp;\n\n\t\t \n\t\tWARN_ON_ONCE(!rcu_rdp_cpu_online(rdp));\n\t\tWARN_ON_ONCE(!list_empty(&t->rcu_node_entry));\n\t\ttrace_rcu_preempt_task(rcu_state.name,\n\t\t\t\t       t->pid,\n\t\t\t\t       (rnp->qsmask & rdp->grpmask)\n\t\t\t\t       ? rnp->gp_seq\n\t\t\t\t       : rcu_seq_snap(&rnp->gp_seq));\n\t\trcu_preempt_ctxt_queue(rnp, rdp);\n\t} else {\n\t\trcu_preempt_deferred_qs(t);\n\t}\n\n\t \n\trcu_qs();\n\tif (rdp->cpu_no_qs.b.exp)\n\t\trcu_report_exp_rdp(rdp);\n\trcu_tasks_qs(current, preempt);\n\ttrace_rcu_utilization(TPS(\"End context switch\"));\n}\nEXPORT_SYMBOL_GPL(rcu_note_context_switch);\n\n \nstatic int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp)\n{\n\treturn READ_ONCE(rnp->gp_tasks) != NULL;\n}\n\n \n#define RCU_NEST_PMAX (INT_MAX / 2)\n\nstatic void rcu_preempt_read_enter(void)\n{\n\tWRITE_ONCE(current->rcu_read_lock_nesting, READ_ONCE(current->rcu_read_lock_nesting) + 1);\n}\n\nstatic int rcu_preempt_read_exit(void)\n{\n\tint ret = READ_ONCE(current->rcu_read_lock_nesting) - 1;\n\n\tWRITE_ONCE(current->rcu_read_lock_nesting, ret);\n\treturn ret;\n}\n\nstatic void rcu_preempt_depth_set(int val)\n{\n\tWRITE_ONCE(current->rcu_read_lock_nesting, val);\n}\n\n \nvoid __rcu_read_lock(void)\n{\n\trcu_preempt_read_enter();\n\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\tWARN_ON_ONCE(rcu_preempt_depth() > RCU_NEST_PMAX);\n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD) && rcu_state.gp_kthread)\n\t\tWRITE_ONCE(current->rcu_read_unlock_special.b.need_qs, true);\n\tbarrier();   \n}\nEXPORT_SYMBOL_GPL(__rcu_read_lock);\n\n \nvoid __rcu_read_unlock(void)\n{\n\tstruct task_struct *t = current;\n\n\tbarrier();  \n\tif (rcu_preempt_read_exit() == 0) {\n\t\tbarrier();  \n\t\tif (unlikely(READ_ONCE(t->rcu_read_unlock_special.s)))\n\t\t\trcu_read_unlock_special(t);\n\t}\n\tif (IS_ENABLED(CONFIG_PROVE_LOCKING)) {\n\t\tint rrln = rcu_preempt_depth();\n\n\t\tWARN_ON_ONCE(rrln < 0 || rrln > RCU_NEST_PMAX);\n\t}\n}\nEXPORT_SYMBOL_GPL(__rcu_read_unlock);\n\n \nstatic struct list_head *rcu_next_node_entry(struct task_struct *t,\n\t\t\t\t\t     struct rcu_node *rnp)\n{\n\tstruct list_head *np;\n\n\tnp = t->rcu_node_entry.next;\n\tif (np == &rnp->blkd_tasks)\n\t\tnp = NULL;\n\treturn np;\n}\n\n \nstatic bool rcu_preempt_has_tasks(struct rcu_node *rnp)\n{\n\treturn !list_empty(&rnp->blkd_tasks);\n}\n\n \nstatic notrace void\nrcu_preempt_deferred_qs_irqrestore(struct task_struct *t, unsigned long flags)\n{\n\tbool empty_exp;\n\tbool empty_norm;\n\tbool empty_exp_now;\n\tstruct list_head *np;\n\tbool drop_boost_mutex = false;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp;\n\tunion rcu_special special;\n\n\t \n\tspecial = t->rcu_read_unlock_special;\n\trdp = this_cpu_ptr(&rcu_data);\n\tif (!special.s && !rdp->cpu_no_qs.b.exp) {\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\tt->rcu_read_unlock_special.s = 0;\n\tif (special.b.need_qs) {\n\t\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD)) {\n\t\t\trdp->cpu_no_qs.b.norm = false;\n\t\t\trcu_report_qs_rdp(rdp);\n\t\t\tudelay(rcu_unlock_delay);\n\t\t} else {\n\t\t\trcu_qs();\n\t\t}\n\t}\n\n\t \n\tif (rdp->cpu_no_qs.b.exp)\n\t\trcu_report_exp_rdp(rdp);\n\n\t \n\tif (special.b.blocked) {\n\n\t\t \n\t\trnp = t->rcu_blocked_node;\n\t\traw_spin_lock_rcu_node(rnp);  \n\t\tWARN_ON_ONCE(rnp != t->rcu_blocked_node);\n\t\tWARN_ON_ONCE(!rcu_is_leaf_node(rnp));\n\t\tempty_norm = !rcu_preempt_blocked_readers_cgp(rnp);\n\t\tWARN_ON_ONCE(rnp->completedqs == rnp->gp_seq &&\n\t\t\t     (!empty_norm || rnp->qsmask));\n\t\tempty_exp = sync_rcu_exp_done(rnp);\n\t\tsmp_mb();  \n\t\tnp = rcu_next_node_entry(t, rnp);\n\t\tlist_del_init(&t->rcu_node_entry);\n\t\tt->rcu_blocked_node = NULL;\n\t\ttrace_rcu_unlock_preempted_task(TPS(\"rcu_preempt\"),\n\t\t\t\t\t\trnp->gp_seq, t->pid);\n\t\tif (&t->rcu_node_entry == rnp->gp_tasks)\n\t\t\tWRITE_ONCE(rnp->gp_tasks, np);\n\t\tif (&t->rcu_node_entry == rnp->exp_tasks)\n\t\t\tWRITE_ONCE(rnp->exp_tasks, np);\n\t\tif (IS_ENABLED(CONFIG_RCU_BOOST)) {\n\t\t\t \n\t\t\tdrop_boost_mutex = rt_mutex_owner(&rnp->boost_mtx.rtmutex) == t;\n\t\t\tif (&t->rcu_node_entry == rnp->boost_tasks)\n\t\t\t\tWRITE_ONCE(rnp->boost_tasks, np);\n\t\t}\n\n\t\t \n\t\tempty_exp_now = sync_rcu_exp_done(rnp);\n\t\tif (!empty_norm && !rcu_preempt_blocked_readers_cgp(rnp)) {\n\t\t\ttrace_rcu_quiescent_state_report(TPS(\"preempt_rcu\"),\n\t\t\t\t\t\t\t rnp->gp_seq,\n\t\t\t\t\t\t\t 0, rnp->qsmask,\n\t\t\t\t\t\t\t rnp->level,\n\t\t\t\t\t\t\t rnp->grplo,\n\t\t\t\t\t\t\t rnp->grphi,\n\t\t\t\t\t\t\t !!rnp->gp_tasks);\n\t\t\trcu_report_unblock_qs_rnp(rnp, flags);\n\t\t} else {\n\t\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\t}\n\n\t\t \n\t\tif (!empty_exp && empty_exp_now)\n\t\t\trcu_report_exp_rnp(rnp, true);\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_RCU_BOOST) && drop_boost_mutex)\n\t\t\trt_mutex_futex_unlock(&rnp->boost_mtx.rtmutex);\n\t} else {\n\t\tlocal_irq_restore(flags);\n\t}\n}\n\n \nstatic notrace bool rcu_preempt_need_deferred_qs(struct task_struct *t)\n{\n\treturn (__this_cpu_read(rcu_data.cpu_no_qs.b.exp) ||\n\t\tREAD_ONCE(t->rcu_read_unlock_special.s)) &&\n\t       rcu_preempt_depth() == 0;\n}\n\n \nnotrace void rcu_preempt_deferred_qs(struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tif (!rcu_preempt_need_deferred_qs(t))\n\t\treturn;\n\tlocal_irq_save(flags);\n\trcu_preempt_deferred_qs_irqrestore(t, flags);\n}\n\n \nstatic void rcu_preempt_deferred_qs_handler(struct irq_work *iwp)\n{\n\tstruct rcu_data *rdp;\n\n\trdp = container_of(iwp, struct rcu_data, defer_qs_iw);\n\trdp->defer_qs_iw_pending = false;\n}\n\n \nstatic void rcu_read_unlock_special(struct task_struct *t)\n{\n\tunsigned long flags;\n\tbool irqs_were_disabled;\n\tbool preempt_bh_were_disabled =\n\t\t\t!!(preempt_count() & (PREEMPT_MASK | SOFTIRQ_MASK));\n\n\t \n\tif (in_nmi())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tirqs_were_disabled = irqs_disabled_flags(flags);\n\tif (preempt_bh_were_disabled || irqs_were_disabled) {\n\t\tbool expboost; \n\t\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\t\tstruct rcu_node *rnp = rdp->mynode;\n\n\t\texpboost = (t->rcu_blocked_node && READ_ONCE(t->rcu_blocked_node->exp_tasks)) ||\n\t\t\t   (rdp->grpmask & READ_ONCE(rnp->expmask)) ||\n\t\t\t   (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD) &&\n\t\t\t   ((rdp->grpmask & READ_ONCE(rnp->qsmask)) || t->rcu_blocked_node)) ||\n\t\t\t   (IS_ENABLED(CONFIG_RCU_BOOST) && irqs_were_disabled &&\n\t\t\t    t->rcu_blocked_node);\n\t\t\n\t\tif (use_softirq && (in_hardirq() || (expboost && !irqs_were_disabled))) {\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\traise_softirq_irqoff(RCU_SOFTIRQ);\n\t\t} else {\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\n\t\t\tset_tsk_need_resched(current);\n\t\t\tset_preempt_need_resched();\n\t\t\tif (IS_ENABLED(CONFIG_IRQ_WORK) && irqs_were_disabled &&\n\t\t\t    expboost && !rdp->defer_qs_iw_pending && cpu_online(rdp->cpu)) {\n\t\t\t\t\n\t\t\t\t\n\t\t\t\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD) &&\n\t\t\t\t    IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t\t\t\trdp->defer_qs_iw = IRQ_WORK_INIT_HARD(\n\t\t\t\t\t\t\t\trcu_preempt_deferred_qs_handler);\n\t\t\t\telse\n\t\t\t\t\tinit_irq_work(&rdp->defer_qs_iw,\n\t\t\t\t\t\t      rcu_preempt_deferred_qs_handler);\n\t\t\t\trdp->defer_qs_iw_pending = true;\n\t\t\t\tirq_work_queue_on(&rdp->defer_qs_iw, rdp->cpu);\n\t\t\t}\n\t\t}\n\t\tlocal_irq_restore(flags);\n\t\treturn;\n\t}\n\trcu_preempt_deferred_qs_irqrestore(t, flags);\n}\n\n \nstatic void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp)\n{\n\tstruct task_struct *t;\n\n\tRCU_LOCKDEP_WARN(preemptible(), \"rcu_preempt_check_blocked_tasks() invoked with preemption enabled!!!\\n\");\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tif (WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)))\n\t\tdump_blkd_tasks(rnp, 10);\n\tif (rcu_preempt_has_tasks(rnp) &&\n\t    (rnp->qsmaskinit || rnp->wait_blkd_tasks)) {\n\t\tWRITE_ONCE(rnp->gp_tasks, rnp->blkd_tasks.next);\n\t\tt = container_of(rnp->gp_tasks, struct task_struct,\n\t\t\t\t rcu_node_entry);\n\t\ttrace_rcu_unlock_preempted_task(TPS(\"rcu_preempt-GPS\"),\n\t\t\t\t\t\trnp->gp_seq, t->pid);\n\t}\n\tWARN_ON_ONCE(rnp->qsmask);\n}\n\n \nstatic void rcu_flavor_sched_clock_irq(int user)\n{\n\tstruct task_struct *t = current;\n\n\tlockdep_assert_irqs_disabled();\n\tif (rcu_preempt_depth() > 0 ||\n\t    (preempt_count() & (PREEMPT_MASK | SOFTIRQ_MASK))) {\n\t\t \n\t\tif (rcu_preempt_need_deferred_qs(t)) {\n\t\t\tset_tsk_need_resched(t);\n\t\t\tset_preempt_need_resched();\n\t\t}\n\t} else if (rcu_preempt_need_deferred_qs(t)) {\n\t\trcu_preempt_deferred_qs(t);  \n\t\treturn;\n\t} else if (!WARN_ON_ONCE(rcu_preempt_depth())) {\n\t\trcu_qs();  \n\t\treturn;\n\t}\n\n\t \n\tif (rcu_preempt_depth() > 0 &&\n\t    __this_cpu_read(rcu_data.core_needs_qs) &&\n\t    __this_cpu_read(rcu_data.cpu_no_qs.b.norm) &&\n\t    !t->rcu_read_unlock_special.b.need_qs &&\n\t    time_after(jiffies, rcu_state.gp_start + HZ))\n\t\tt->rcu_read_unlock_special.b.need_qs = true;\n}\n\n \nvoid exit_rcu(void)\n{\n\tstruct task_struct *t = current;\n\n\tif (unlikely(!list_empty(&current->rcu_node_entry))) {\n\t\trcu_preempt_depth_set(1);\n\t\tbarrier();\n\t\tWRITE_ONCE(t->rcu_read_unlock_special.b.blocked, true);\n\t} else if (unlikely(rcu_preempt_depth())) {\n\t\trcu_preempt_depth_set(1);\n\t} else {\n\t\treturn;\n\t}\n\t__rcu_read_unlock();\n\trcu_preempt_deferred_qs(current);\n}\n\n \nstatic void\ndump_blkd_tasks(struct rcu_node *rnp, int ncheck)\n{\n\tint cpu;\n\tint i;\n\tstruct list_head *lhp;\n\tstruct rcu_data *rdp;\n\tstruct rcu_node *rnp1;\n\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tpr_info(\"%s: grp: %d-%d level: %d ->gp_seq %ld ->completedqs %ld\\n\",\n\t\t__func__, rnp->grplo, rnp->grphi, rnp->level,\n\t\t(long)READ_ONCE(rnp->gp_seq), (long)rnp->completedqs);\n\tfor (rnp1 = rnp; rnp1; rnp1 = rnp1->parent)\n\t\tpr_info(\"%s: %d:%d ->qsmask %#lx ->qsmaskinit %#lx ->qsmaskinitnext %#lx\\n\",\n\t\t\t__func__, rnp1->grplo, rnp1->grphi, rnp1->qsmask, rnp1->qsmaskinit, rnp1->qsmaskinitnext);\n\tpr_info(\"%s: ->gp_tasks %p ->boost_tasks %p ->exp_tasks %p\\n\",\n\t\t__func__, READ_ONCE(rnp->gp_tasks), data_race(rnp->boost_tasks),\n\t\tREAD_ONCE(rnp->exp_tasks));\n\tpr_info(\"%s: ->blkd_tasks\", __func__);\n\ti = 0;\n\tlist_for_each(lhp, &rnp->blkd_tasks) {\n\t\tpr_cont(\" %p\", lhp);\n\t\tif (++i >= ncheck)\n\t\t\tbreak;\n\t}\n\tpr_cont(\"\\n\");\n\tfor (cpu = rnp->grplo; cpu <= rnp->grphi; cpu++) {\n\t\trdp = per_cpu_ptr(&rcu_data, cpu);\n\t\tpr_info(\"\\t%d: %c online: %ld(%d) offline: %ld(%d)\\n\",\n\t\t\tcpu, \".o\"[rcu_rdp_cpu_online(rdp)],\n\t\t\t(long)rdp->rcu_onl_gp_seq, rdp->rcu_onl_gp_flags,\n\t\t\t(long)rdp->rcu_ofl_gp_seq, rdp->rcu_ofl_gp_flags);\n\t}\n}\n\n#else  \n\n \nvoid rcu_read_unlock_strict(void)\n{\n\tstruct rcu_data *rdp;\n\n\tif (irqs_disabled() || preempt_count() || !rcu_state.gp_kthread)\n\t\treturn;\n\trdp = this_cpu_ptr(&rcu_data);\n\trdp->cpu_no_qs.b.norm = false;\n\trcu_report_qs_rdp(rdp);\n\tudelay(rcu_unlock_delay);\n}\nEXPORT_SYMBOL_GPL(rcu_read_unlock_strict);\n\n \nstatic void __init rcu_bootup_announce(void)\n{\n\tpr_info(\"Hierarchical RCU implementation.\\n\");\n\trcu_bootup_announce_oddness();\n}\n\n \nstatic void rcu_qs(void)\n{\n\tRCU_LOCKDEP_WARN(preemptible(), \"rcu_qs() invoked with preemption enabled!!!\");\n\tif (!__this_cpu_read(rcu_data.cpu_no_qs.s))\n\t\treturn;\n\ttrace_rcu_grace_period(TPS(\"rcu_sched\"),\n\t\t\t       __this_cpu_read(rcu_data.gp_seq), TPS(\"cpuqs\"));\n\t__this_cpu_write(rcu_data.cpu_no_qs.b.norm, false);\n\tif (__this_cpu_read(rcu_data.cpu_no_qs.b.exp))\n\t\trcu_report_exp_rdp(this_cpu_ptr(&rcu_data));\n}\n\n \nvoid rcu_all_qs(void)\n{\n\tunsigned long flags;\n\n\tif (!raw_cpu_read(rcu_data.rcu_urgent_qs))\n\t\treturn;\n\tpreempt_disable();  \n\t \n\tif (!smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs))) {\n\t\tpreempt_enable();\n\t\treturn;\n\t}\n\tthis_cpu_write(rcu_data.rcu_urgent_qs, false);\n\tif (unlikely(raw_cpu_read(rcu_data.rcu_need_heavy_qs))) {\n\t\tlocal_irq_save(flags);\n\t\trcu_momentary_dyntick_idle();\n\t\tlocal_irq_restore(flags);\n\t}\n\trcu_qs();\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(rcu_all_qs);\n\n \nvoid rcu_note_context_switch(bool preempt)\n{\n\ttrace_rcu_utilization(TPS(\"Start context switch\"));\n\trcu_qs();\n\t \n\tif (!smp_load_acquire(this_cpu_ptr(&rcu_data.rcu_urgent_qs)))\n\t\tgoto out;\n\tthis_cpu_write(rcu_data.rcu_urgent_qs, false);\n\tif (unlikely(raw_cpu_read(rcu_data.rcu_need_heavy_qs)))\n\t\trcu_momentary_dyntick_idle();\nout:\n\trcu_tasks_qs(current, preempt);\n\ttrace_rcu_utilization(TPS(\"End context switch\"));\n}\nEXPORT_SYMBOL_GPL(rcu_note_context_switch);\n\n \nstatic int rcu_preempt_blocked_readers_cgp(struct rcu_node *rnp)\n{\n\treturn 0;\n}\n\n \nstatic bool rcu_preempt_has_tasks(struct rcu_node *rnp)\n{\n\treturn false;\n}\n\n \nstatic notrace bool rcu_preempt_need_deferred_qs(struct task_struct *t)\n{\n\treturn false;\n}\n\n\n\n\n\n\n\n\nnotrace void rcu_preempt_deferred_qs(struct task_struct *t)\n{\n\tstruct rcu_data *rdp = this_cpu_ptr(&rcu_data);\n\n\tif (READ_ONCE(rdp->cpu_no_qs.b.exp))\n\t\trcu_report_exp_rdp(rdp);\n}\n\n \nstatic void rcu_preempt_check_blocked_tasks(struct rcu_node *rnp)\n{\n\tWARN_ON_ONCE(rnp->qsmask);\n}\n\n \nstatic void rcu_flavor_sched_clock_irq(int user)\n{\n\tif (user || rcu_is_cpu_rrupt_from_idle()) {\n\n\t\t \n\t\trcu_qs();\n\t}\n}\n\n \nvoid exit_rcu(void)\n{\n}\n\n \nstatic void\ndump_blkd_tasks(struct rcu_node *rnp, int ncheck)\n{\n\tWARN_ON_ONCE(!list_empty(&rnp->blkd_tasks));\n}\n\n#endif  \n\n \nstatic void rcu_cpu_kthread_setup(unsigned int cpu)\n{\n\tstruct rcu_data *rdp = per_cpu_ptr(&rcu_data, cpu);\n#ifdef CONFIG_RCU_BOOST\n\tstruct sched_param sp;\n\n\tsp.sched_priority = kthread_prio;\n\tsched_setscheduler_nocheck(current, SCHED_FIFO, &sp);\n#endif  \n\n\tWRITE_ONCE(rdp->rcuc_activity, jiffies);\n}\n\nstatic bool rcu_is_callbacks_nocb_kthread(struct rcu_data *rdp)\n{\n#ifdef CONFIG_RCU_NOCB_CPU\n\treturn rdp->nocb_cb_kthread == current;\n#else\n\treturn false;\n#endif\n}\n\n \nstatic bool rcu_is_callbacks_kthread(struct rcu_data *rdp)\n{\n\treturn rdp->rcu_cpu_kthread_task == current ||\n\t\t\trcu_is_callbacks_nocb_kthread(rdp);\n}\n\n#ifdef CONFIG_RCU_BOOST\n\n \nstatic int rcu_boost(struct rcu_node *rnp)\n{\n\tunsigned long flags;\n\tstruct task_struct *t;\n\tstruct list_head *tb;\n\n\tif (READ_ONCE(rnp->exp_tasks) == NULL &&\n\t    READ_ONCE(rnp->boost_tasks) == NULL)\n\t\treturn 0;   \n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\n\t \n\tif (rnp->exp_tasks == NULL && rnp->boost_tasks == NULL) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn 0;\n\t}\n\n\t \n\tif (rnp->exp_tasks != NULL)\n\t\ttb = rnp->exp_tasks;\n\telse\n\t\ttb = rnp->boost_tasks;\n\n\t \n\tt = container_of(tb, struct task_struct, rcu_node_entry);\n\trt_mutex_init_proxy_locked(&rnp->boost_mtx.rtmutex, t);\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t \n\trt_mutex_lock(&rnp->boost_mtx);\n\trt_mutex_unlock(&rnp->boost_mtx);   \n\trnp->n_boosts++;\n\n\treturn READ_ONCE(rnp->exp_tasks) != NULL ||\n\t       READ_ONCE(rnp->boost_tasks) != NULL;\n}\n\n \nstatic int rcu_boost_kthread(void *arg)\n{\n\tstruct rcu_node *rnp = (struct rcu_node *)arg;\n\tint spincnt = 0;\n\tint more2boost;\n\n\ttrace_rcu_utilization(TPS(\"Start boost kthread@init\"));\n\tfor (;;) {\n\t\tWRITE_ONCE(rnp->boost_kthread_status, RCU_KTHREAD_WAITING);\n\t\ttrace_rcu_utilization(TPS(\"End boost kthread@rcu_wait\"));\n\t\trcu_wait(READ_ONCE(rnp->boost_tasks) ||\n\t\t\t READ_ONCE(rnp->exp_tasks));\n\t\ttrace_rcu_utilization(TPS(\"Start boost kthread@rcu_wait\"));\n\t\tWRITE_ONCE(rnp->boost_kthread_status, RCU_KTHREAD_RUNNING);\n\t\tmore2boost = rcu_boost(rnp);\n\t\tif (more2boost)\n\t\t\tspincnt++;\n\t\telse\n\t\t\tspincnt = 0;\n\t\tif (spincnt > 10) {\n\t\t\tWRITE_ONCE(rnp->boost_kthread_status, RCU_KTHREAD_YIELDING);\n\t\t\ttrace_rcu_utilization(TPS(\"End boost kthread@rcu_yield\"));\n\t\t\tschedule_timeout_idle(2);\n\t\t\ttrace_rcu_utilization(TPS(\"Start boost kthread@rcu_yield\"));\n\t\t\tspincnt = 0;\n\t\t}\n\t}\n\t \n\ttrace_rcu_utilization(TPS(\"End boost kthread@notreached\"));\n\treturn 0;\n}\n\n \nstatic void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags)\n\t__releases(rnp->lock)\n{\n\traw_lockdep_assert_held_rcu_node(rnp);\n\tif (!rnp->boost_kthread_task ||\n\t    (!rcu_preempt_blocked_readers_cgp(rnp) && !rnp->exp_tasks)) {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\treturn;\n\t}\n\tif (rnp->exp_tasks != NULL ||\n\t    (rnp->gp_tasks != NULL &&\n\t     rnp->boost_tasks == NULL &&\n\t     rnp->qsmask == 0 &&\n\t     (!time_after(rnp->boost_time, jiffies) || rcu_state.cbovld ||\n\t      IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD)))) {\n\t\tif (rnp->exp_tasks == NULL)\n\t\t\tWRITE_ONCE(rnp->boost_tasks, rnp->gp_tasks);\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t\trcu_wake_cond(rnp->boost_kthread_task,\n\t\t\t      READ_ONCE(rnp->boost_kthread_status));\n\t} else {\n\t\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\t}\n}\n\n#define RCU_BOOST_DELAY_JIFFIES DIV_ROUND_UP(CONFIG_RCU_BOOST_DELAY * HZ, 1000)\n\n \nstatic void rcu_preempt_boost_start_gp(struct rcu_node *rnp)\n{\n\trnp->boost_time = jiffies + RCU_BOOST_DELAY_JIFFIES;\n}\n\n \nstatic void rcu_spawn_one_boost_kthread(struct rcu_node *rnp)\n{\n\tunsigned long flags;\n\tint rnp_index = rnp - rcu_get_root();\n\tstruct sched_param sp;\n\tstruct task_struct *t;\n\n\tmutex_lock(&rnp->boost_kthread_mutex);\n\tif (rnp->boost_kthread_task || !rcu_scheduler_fully_active)\n\t\tgoto out;\n\n\tt = kthread_create(rcu_boost_kthread, (void *)rnp,\n\t\t\t   \"rcub/%d\", rnp_index);\n\tif (WARN_ON_ONCE(IS_ERR(t)))\n\t\tgoto out;\n\n\traw_spin_lock_irqsave_rcu_node(rnp, flags);\n\trnp->boost_kthread_task = t;\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n\tsp.sched_priority = kthread_prio;\n\tsched_setscheduler_nocheck(t, SCHED_FIFO, &sp);\n\twake_up_process(t);  \n\n out:\n\tmutex_unlock(&rnp->boost_kthread_mutex);\n}\n\n \nstatic void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu)\n{\n\tstruct task_struct *t = rnp->boost_kthread_task;\n\tunsigned long mask;\n\tcpumask_var_t cm;\n\tint cpu;\n\n\tif (!t)\n\t\treturn;\n\tif (!zalloc_cpumask_var(&cm, GFP_KERNEL))\n\t\treturn;\n\tmutex_lock(&rnp->boost_kthread_mutex);\n\tmask = rcu_rnp_online_cpus(rnp);\n\tfor_each_leaf_node_possible_cpu(rnp, cpu)\n\t\tif ((mask & leaf_node_cpu_bit(rnp, cpu)) &&\n\t\t    cpu != outgoingcpu)\n\t\t\tcpumask_set_cpu(cpu, cm);\n\tcpumask_and(cm, cm, housekeeping_cpumask(HK_TYPE_RCU));\n\tif (cpumask_empty(cm)) {\n\t\tcpumask_copy(cm, housekeeping_cpumask(HK_TYPE_RCU));\n\t\tif (outgoingcpu >= 0)\n\t\t\tcpumask_clear_cpu(outgoingcpu, cm);\n\t}\n\tset_cpus_allowed_ptr(t, cm);\n\tmutex_unlock(&rnp->boost_kthread_mutex);\n\tfree_cpumask_var(cm);\n}\n\n#else  \n\nstatic void rcu_initiate_boost(struct rcu_node *rnp, unsigned long flags)\n\t__releases(rnp->lock)\n{\n\traw_spin_unlock_irqrestore_rcu_node(rnp, flags);\n}\n\nstatic void rcu_preempt_boost_start_gp(struct rcu_node *rnp)\n{\n}\n\nstatic void rcu_spawn_one_boost_kthread(struct rcu_node *rnp)\n{\n}\n\nstatic void rcu_boost_kthread_setaffinity(struct rcu_node *rnp, int outgoingcpu)\n{\n}\n\n#endif  \n\n \nstatic bool rcu_nohz_full_cpu(void)\n{\n#ifdef CONFIG_NO_HZ_FULL\n\tif (tick_nohz_full_cpu(smp_processor_id()) &&\n\t    (!rcu_gp_in_progress() ||\n\t     time_before(jiffies, READ_ONCE(rcu_state.gp_start) + HZ)))\n\t\treturn true;\n#endif  \n\treturn false;\n}\n\n \nstatic void rcu_bind_gp_kthread(void)\n{\n\tif (!tick_nohz_full_enabled())\n\t\treturn;\n\thousekeeping_affine(current, HK_TYPE_RCU);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}