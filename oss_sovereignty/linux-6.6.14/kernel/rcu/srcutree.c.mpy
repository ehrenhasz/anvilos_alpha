{
  "module_name": "srcutree.c",
  "hash_id": "fe05a514213bb6108a2f1eb0b034985ea8b648f4fa430561efb22bead889daa9",
  "original_prompt": "Ingested from linux-6.6.14/kernel/rcu/srcutree.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"rcu: \" fmt\n\n#include <linux/export.h>\n#include <linux/mutex.h>\n#include <linux/percpu.h>\n#include <linux/preempt.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/sched.h>\n#include <linux/smp.h>\n#include <linux/delay.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/srcu.h>\n\n#include \"rcu.h\"\n#include \"rcu_segcblist.h\"\n\n \n#define DEFAULT_SRCU_EXP_HOLDOFF (25 * 1000)\nstatic ulong exp_holdoff = DEFAULT_SRCU_EXP_HOLDOFF;\nmodule_param(exp_holdoff, ulong, 0444);\n\n \nstatic ulong counter_wrap_check = (ULONG_MAX >> 2);\nmodule_param(counter_wrap_check, ulong, 0444);\n\n \n#define SRCU_SIZING_NONE\t0\n#define SRCU_SIZING_INIT\t1\n#define SRCU_SIZING_TORTURE\t2\n#define SRCU_SIZING_AUTO\t3\n#define SRCU_SIZING_CONTEND\t0x10\n#define SRCU_SIZING_IS(x) ((convert_to_big & ~SRCU_SIZING_CONTEND) == x)\n#define SRCU_SIZING_IS_NONE() (SRCU_SIZING_IS(SRCU_SIZING_NONE))\n#define SRCU_SIZING_IS_INIT() (SRCU_SIZING_IS(SRCU_SIZING_INIT))\n#define SRCU_SIZING_IS_TORTURE() (SRCU_SIZING_IS(SRCU_SIZING_TORTURE))\n#define SRCU_SIZING_IS_CONTEND() (convert_to_big & SRCU_SIZING_CONTEND)\nstatic int convert_to_big = SRCU_SIZING_AUTO;\nmodule_param(convert_to_big, int, 0444);\n\n \nstatic int big_cpu_lim __read_mostly = 128;\nmodule_param(big_cpu_lim, int, 0444);\n\n \nstatic int small_contention_lim __read_mostly = 100;\nmodule_param(small_contention_lim, int, 0444);\n\n \nstatic LIST_HEAD(srcu_boot_list);\nstatic bool __read_mostly srcu_init_done;\n\nstatic void srcu_invoke_callbacks(struct work_struct *work);\nstatic void srcu_reschedule(struct srcu_struct *ssp, unsigned long delay);\nstatic void process_srcu(struct work_struct *work);\nstatic void srcu_delay_timer(struct timer_list *t);\n\n \n#define spin_lock_rcu_node(p)\t\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tspin_lock(&ACCESS_PRIVATE(p, lock));\t\t\t\t\t\\\n\tsmp_mb__after_unlock_lock();\t\t\t\t\t\t\\\n} while (0)\n\n#define spin_unlock_rcu_node(p) spin_unlock(&ACCESS_PRIVATE(p, lock))\n\n#define spin_lock_irq_rcu_node(p)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tspin_lock_irq(&ACCESS_PRIVATE(p, lock));\t\t\t\t\\\n\tsmp_mb__after_unlock_lock();\t\t\t\t\t\t\\\n} while (0)\n\n#define spin_unlock_irq_rcu_node(p)\t\t\t\t\t\t\\\n\tspin_unlock_irq(&ACCESS_PRIVATE(p, lock))\n\n#define spin_lock_irqsave_rcu_node(p, flags)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tspin_lock_irqsave(&ACCESS_PRIVATE(p, lock), flags);\t\t\t\\\n\tsmp_mb__after_unlock_lock();\t\t\t\t\t\t\\\n} while (0)\n\n#define spin_trylock_irqsave_rcu_node(p, flags)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\\\n\tbool ___locked = spin_trylock_irqsave(&ACCESS_PRIVATE(p, lock), flags); \\\n\t\t\t\t\t\t\t\t\t\t\\\n\tif (___locked)\t\t\t\t\t\t\t\t\\\n\t\tsmp_mb__after_unlock_lock();\t\t\t\t\t\\\n\t___locked;\t\t\t\t\t\t\t\t\\\n})\n\n#define spin_unlock_irqrestore_rcu_node(p, flags)\t\t\t\t\\\n\tspin_unlock_irqrestore(&ACCESS_PRIVATE(p, lock), flags)\t\t\t\\\n\n \nstatic void init_srcu_struct_data(struct srcu_struct *ssp)\n{\n\tint cpu;\n\tstruct srcu_data *sdp;\n\n\t \n\tWARN_ON_ONCE(ARRAY_SIZE(sdp->srcu_lock_count) !=\n\t\t     ARRAY_SIZE(sdp->srcu_unlock_count));\n\tfor_each_possible_cpu(cpu) {\n\t\tsdp = per_cpu_ptr(ssp->sda, cpu);\n\t\tspin_lock_init(&ACCESS_PRIVATE(sdp, lock));\n\t\trcu_segcblist_init(&sdp->srcu_cblist);\n\t\tsdp->srcu_cblist_invoking = false;\n\t\tsdp->srcu_gp_seq_needed = ssp->srcu_sup->srcu_gp_seq;\n\t\tsdp->srcu_gp_seq_needed_exp = ssp->srcu_sup->srcu_gp_seq;\n\t\tsdp->mynode = NULL;\n\t\tsdp->cpu = cpu;\n\t\tINIT_WORK(&sdp->work, srcu_invoke_callbacks);\n\t\ttimer_setup(&sdp->delay_work, srcu_delay_timer, 0);\n\t\tsdp->ssp = ssp;\n\t}\n}\n\n \n#define SRCU_SNP_INIT_SEQ\t\t0x2\n\n \nstatic inline bool srcu_invl_snp_seq(unsigned long s)\n{\n\treturn s == SRCU_SNP_INIT_SEQ;\n}\n\n \nstatic bool init_srcu_struct_nodes(struct srcu_struct *ssp, gfp_t gfp_flags)\n{\n\tint cpu;\n\tint i;\n\tint level = 0;\n\tint levelspread[RCU_NUM_LVLS];\n\tstruct srcu_data *sdp;\n\tstruct srcu_node *snp;\n\tstruct srcu_node *snp_first;\n\n\t \n\trcu_init_geometry();\n\tssp->srcu_sup->node = kcalloc(rcu_num_nodes, sizeof(*ssp->srcu_sup->node), gfp_flags);\n\tif (!ssp->srcu_sup->node)\n\t\treturn false;\n\n\t \n\tssp->srcu_sup->level[0] = &ssp->srcu_sup->node[0];\n\tfor (i = 1; i < rcu_num_lvls; i++)\n\t\tssp->srcu_sup->level[i] = ssp->srcu_sup->level[i - 1] + num_rcu_lvl[i - 1];\n\trcu_init_levelspread(levelspread, num_rcu_lvl);\n\n\t \n\tsrcu_for_each_node_breadth_first(ssp, snp) {\n\t\tspin_lock_init(&ACCESS_PRIVATE(snp, lock));\n\t\tWARN_ON_ONCE(ARRAY_SIZE(snp->srcu_have_cbs) !=\n\t\t\t     ARRAY_SIZE(snp->srcu_data_have_cbs));\n\t\tfor (i = 0; i < ARRAY_SIZE(snp->srcu_have_cbs); i++) {\n\t\t\tsnp->srcu_have_cbs[i] = SRCU_SNP_INIT_SEQ;\n\t\t\tsnp->srcu_data_have_cbs[i] = 0;\n\t\t}\n\t\tsnp->srcu_gp_seq_needed_exp = SRCU_SNP_INIT_SEQ;\n\t\tsnp->grplo = -1;\n\t\tsnp->grphi = -1;\n\t\tif (snp == &ssp->srcu_sup->node[0]) {\n\t\t\t \n\t\t\tsnp->srcu_parent = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (snp == ssp->srcu_sup->level[level + 1])\n\t\t\tlevel++;\n\t\tsnp->srcu_parent = ssp->srcu_sup->level[level - 1] +\n\t\t\t\t   (snp - ssp->srcu_sup->level[level]) /\n\t\t\t\t   levelspread[level - 1];\n\t}\n\n\t \n\tlevel = rcu_num_lvls - 1;\n\tsnp_first = ssp->srcu_sup->level[level];\n\tfor_each_possible_cpu(cpu) {\n\t\tsdp = per_cpu_ptr(ssp->sda, cpu);\n\t\tsdp->mynode = &snp_first[cpu / levelspread[level]];\n\t\tfor (snp = sdp->mynode; snp != NULL; snp = snp->srcu_parent) {\n\t\t\tif (snp->grplo < 0)\n\t\t\t\tsnp->grplo = cpu;\n\t\t\tsnp->grphi = cpu;\n\t\t}\n\t\tsdp->grpmask = 1UL << (cpu - sdp->mynode->grplo);\n\t}\n\tsmp_store_release(&ssp->srcu_sup->srcu_size_state, SRCU_SIZE_WAIT_BARRIER);\n\treturn true;\n}\n\n \nstatic int init_srcu_struct_fields(struct srcu_struct *ssp, bool is_static)\n{\n\tif (!is_static)\n\t\tssp->srcu_sup = kzalloc(sizeof(*ssp->srcu_sup), GFP_KERNEL);\n\tif (!ssp->srcu_sup)\n\t\treturn -ENOMEM;\n\tif (!is_static)\n\t\tspin_lock_init(&ACCESS_PRIVATE(ssp->srcu_sup, lock));\n\tssp->srcu_sup->srcu_size_state = SRCU_SIZE_SMALL;\n\tssp->srcu_sup->node = NULL;\n\tmutex_init(&ssp->srcu_sup->srcu_cb_mutex);\n\tmutex_init(&ssp->srcu_sup->srcu_gp_mutex);\n\tssp->srcu_idx = 0;\n\tssp->srcu_sup->srcu_gp_seq = 0;\n\tssp->srcu_sup->srcu_barrier_seq = 0;\n\tmutex_init(&ssp->srcu_sup->srcu_barrier_mutex);\n\tatomic_set(&ssp->srcu_sup->srcu_barrier_cpu_cnt, 0);\n\tINIT_DELAYED_WORK(&ssp->srcu_sup->work, process_srcu);\n\tssp->srcu_sup->sda_is_static = is_static;\n\tif (!is_static)\n\t\tssp->sda = alloc_percpu(struct srcu_data);\n\tif (!ssp->sda) {\n\t\tif (!is_static)\n\t\t\tkfree(ssp->srcu_sup);\n\t\treturn -ENOMEM;\n\t}\n\tinit_srcu_struct_data(ssp);\n\tssp->srcu_sup->srcu_gp_seq_needed_exp = 0;\n\tssp->srcu_sup->srcu_last_gp_end = ktime_get_mono_fast_ns();\n\tif (READ_ONCE(ssp->srcu_sup->srcu_size_state) == SRCU_SIZE_SMALL && SRCU_SIZING_IS_INIT()) {\n\t\tif (!init_srcu_struct_nodes(ssp, GFP_ATOMIC)) {\n\t\t\tif (!ssp->srcu_sup->sda_is_static) {\n\t\t\t\tfree_percpu(ssp->sda);\n\t\t\t\tssp->sda = NULL;\n\t\t\t\tkfree(ssp->srcu_sup);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\tWRITE_ONCE(ssp->srcu_sup->srcu_size_state, SRCU_SIZE_BIG);\n\t\t}\n\t}\n\tssp->srcu_sup->srcu_ssp = ssp;\n\tsmp_store_release(&ssp->srcu_sup->srcu_gp_seq_needed, 0);  \n\treturn 0;\n}\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\nint __init_srcu_struct(struct srcu_struct *ssp, const char *name,\n\t\t       struct lock_class_key *key)\n{\n\t \n\tdebug_check_no_locks_freed((void *)ssp, sizeof(*ssp));\n\tlockdep_init_map(&ssp->dep_map, name, key, 0);\n\treturn init_srcu_struct_fields(ssp, false);\n}\nEXPORT_SYMBOL_GPL(__init_srcu_struct);\n\n#else  \n\n \nint init_srcu_struct(struct srcu_struct *ssp)\n{\n\treturn init_srcu_struct_fields(ssp, false);\n}\nEXPORT_SYMBOL_GPL(init_srcu_struct);\n\n#endif  \n\n \nstatic void __srcu_transition_to_big(struct srcu_struct *ssp)\n{\n\tlockdep_assert_held(&ACCESS_PRIVATE(ssp->srcu_sup, lock));\n\tsmp_store_release(&ssp->srcu_sup->srcu_size_state, SRCU_SIZE_ALLOC);\n}\n\n \nstatic void srcu_transition_to_big(struct srcu_struct *ssp)\n{\n\tunsigned long flags;\n\n\t \n\tif (smp_load_acquire(&ssp->srcu_sup->srcu_size_state) != SRCU_SIZE_SMALL)\n\t\treturn;\n\tspin_lock_irqsave_rcu_node(ssp->srcu_sup, flags);\n\tif (smp_load_acquire(&ssp->srcu_sup->srcu_size_state) != SRCU_SIZE_SMALL) {\n\t\tspin_unlock_irqrestore_rcu_node(ssp->srcu_sup, flags);\n\t\treturn;\n\t}\n\t__srcu_transition_to_big(ssp);\n\tspin_unlock_irqrestore_rcu_node(ssp->srcu_sup, flags);\n}\n\n \nstatic void spin_lock_irqsave_check_contention(struct srcu_struct *ssp)\n{\n\tunsigned long j;\n\n\tif (!SRCU_SIZING_IS_CONTEND() || ssp->srcu_sup->srcu_size_state)\n\t\treturn;\n\tj = jiffies;\n\tif (ssp->srcu_sup->srcu_size_jiffies != j) {\n\t\tssp->srcu_sup->srcu_size_jiffies = j;\n\t\tssp->srcu_sup->srcu_n_lock_retries = 0;\n\t}\n\tif (++ssp->srcu_sup->srcu_n_lock_retries <= small_contention_lim)\n\t\treturn;\n\t__srcu_transition_to_big(ssp);\n}\n\n \nstatic void spin_lock_irqsave_sdp_contention(struct srcu_data *sdp, unsigned long *flags)\n{\n\tstruct srcu_struct *ssp = sdp->ssp;\n\n\tif (spin_trylock_irqsave_rcu_node(sdp, *flags))\n\t\treturn;\n\tspin_lock_irqsave_rcu_node(ssp->srcu_sup, *flags);\n\tspin_lock_irqsave_check_contention(ssp);\n\tspin_unlock_irqrestore_rcu_node(ssp->srcu_sup, *flags);\n\tspin_lock_irqsave_rcu_node(sdp, *flags);\n}\n\n \nstatic void spin_lock_irqsave_ssp_contention(struct srcu_struct *ssp, unsigned long *flags)\n{\n\tif (spin_trylock_irqsave_rcu_node(ssp->srcu_sup, *flags))\n\t\treturn;\n\tspin_lock_irqsave_rcu_node(ssp->srcu_sup, *flags);\n\tspin_lock_irqsave_check_contention(ssp);\n}\n\n \nstatic void check_init_srcu_struct(struct srcu_struct *ssp)\n{\n\tunsigned long flags;\n\n\t \n\tif (!rcu_seq_state(smp_load_acquire(&ssp->srcu_sup->srcu_gp_seq_needed)))  \n\t\treturn;  \n\tspin_lock_irqsave_rcu_node(ssp->srcu_sup, flags);\n\tif (!rcu_seq_state(ssp->srcu_sup->srcu_gp_seq_needed)) {\n\t\tspin_unlock_irqrestore_rcu_node(ssp->srcu_sup, flags);\n\t\treturn;\n\t}\n\tinit_srcu_struct_fields(ssp, true);\n\tspin_unlock_irqrestore_rcu_node(ssp->srcu_sup, flags);\n}\n\n \nstatic unsigned long srcu_readers_lock_idx(struct srcu_struct *ssp, int idx)\n{\n\tint cpu;\n\tunsigned long sum = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct srcu_data *cpuc = per_cpu_ptr(ssp->sda, cpu);\n\n\t\tsum += atomic_long_read(&cpuc->srcu_lock_count[idx]);\n\t}\n\treturn sum;\n}\n\n \nstatic unsigned long srcu_readers_unlock_idx(struct srcu_struct *ssp, int idx)\n{\n\tint cpu;\n\tunsigned long mask = 0;\n\tunsigned long sum = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct srcu_data *cpuc = per_cpu_ptr(ssp->sda, cpu);\n\n\t\tsum += atomic_long_read(&cpuc->srcu_unlock_count[idx]);\n\t\tif (IS_ENABLED(CONFIG_PROVE_RCU))\n\t\t\tmask = mask | READ_ONCE(cpuc->srcu_nmi_safety);\n\t}\n\tWARN_ONCE(IS_ENABLED(CONFIG_PROVE_RCU) && (mask & (mask >> 1)),\n\t\t  \"Mixed NMI-safe readers for srcu_struct at %ps.\\n\", ssp);\n\treturn sum;\n}\n\n \nstatic bool srcu_readers_active_idx_check(struct srcu_struct *ssp, int idx)\n{\n\tunsigned long unlocks;\n\n\tunlocks = srcu_readers_unlock_idx(ssp, idx);\n\n\t \n\tsmp_mb();  \n\n\t \n\treturn srcu_readers_lock_idx(ssp, idx) == unlocks;\n}\n\n \nstatic bool srcu_readers_active(struct srcu_struct *ssp)\n{\n\tint cpu;\n\tunsigned long sum = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct srcu_data *cpuc = per_cpu_ptr(ssp->sda, cpu);\n\n\t\tsum += atomic_long_read(&cpuc->srcu_lock_count[0]);\n\t\tsum += atomic_long_read(&cpuc->srcu_lock_count[1]);\n\t\tsum -= atomic_long_read(&cpuc->srcu_unlock_count[0]);\n\t\tsum -= atomic_long_read(&cpuc->srcu_unlock_count[1]);\n\t}\n\treturn sum;\n}\n\n \n#define SRCU_DEFAULT_RETRY_CHECK_DELAY\t\t5\n\nstatic ulong srcu_retry_check_delay = SRCU_DEFAULT_RETRY_CHECK_DELAY;\nmodule_param(srcu_retry_check_delay, ulong, 0444);\n\n#define SRCU_INTERVAL\t\t1\t\t\n#define SRCU_MAX_INTERVAL\t10\t\t\n\n#define SRCU_DEFAULT_MAX_NODELAY_PHASE_LO\t3UL\t\n\t\t\t\t\t\t\t\n#define SRCU_DEFAULT_MAX_NODELAY_PHASE_HI\t1000UL\t\n\t\t\t\t\t\t\t\n\n#define SRCU_UL_CLAMP_LO(val, low)\t((val) > (low) ? (val) : (low))\n#define SRCU_UL_CLAMP_HI(val, high)\t((val) < (high) ? (val) : (high))\n#define SRCU_UL_CLAMP(val, low, high)\tSRCU_UL_CLAMP_HI(SRCU_UL_CLAMP_LO((val), (low)), (high))\n\n\n\n#define SRCU_DEFAULT_MAX_NODELAY_PHASE_ADJUSTED\t\\\n\t(2UL * USEC_PER_SEC / HZ / SRCU_DEFAULT_RETRY_CHECK_DELAY)\n\n\n#define SRCU_DEFAULT_MAX_NODELAY_PHASE\t\\\n\tSRCU_UL_CLAMP(SRCU_DEFAULT_MAX_NODELAY_PHASE_ADJUSTED,\t\\\n\t\t      SRCU_DEFAULT_MAX_NODELAY_PHASE_LO,\t\\\n\t\t      SRCU_DEFAULT_MAX_NODELAY_PHASE_HI)\n\nstatic ulong srcu_max_nodelay_phase = SRCU_DEFAULT_MAX_NODELAY_PHASE;\nmodule_param(srcu_max_nodelay_phase, ulong, 0444);\n\n\n#define SRCU_DEFAULT_MAX_NODELAY\t(SRCU_DEFAULT_MAX_NODELAY_PHASE > 100 ?\t\\\n\t\t\t\t\t SRCU_DEFAULT_MAX_NODELAY_PHASE : 100)\n\nstatic ulong srcu_max_nodelay = SRCU_DEFAULT_MAX_NODELAY;\nmodule_param(srcu_max_nodelay, ulong, 0444);\n\n \nstatic unsigned long srcu_get_delay(struct srcu_struct *ssp)\n{\n\tunsigned long gpstart;\n\tunsigned long j;\n\tunsigned long jbase = SRCU_INTERVAL;\n\tstruct srcu_usage *sup = ssp->srcu_sup;\n\n\tif (ULONG_CMP_LT(READ_ONCE(sup->srcu_gp_seq), READ_ONCE(sup->srcu_gp_seq_needed_exp)))\n\t\tjbase = 0;\n\tif (rcu_seq_state(READ_ONCE(sup->srcu_gp_seq))) {\n\t\tj = jiffies - 1;\n\t\tgpstart = READ_ONCE(sup->srcu_gp_start);\n\t\tif (time_after(j, gpstart))\n\t\t\tjbase += j - gpstart;\n\t\tif (!jbase) {\n\t\t\tWRITE_ONCE(sup->srcu_n_exp_nodelay, READ_ONCE(sup->srcu_n_exp_nodelay) + 1);\n\t\t\tif (READ_ONCE(sup->srcu_n_exp_nodelay) > srcu_max_nodelay_phase)\n\t\t\t\tjbase = 1;\n\t\t}\n\t}\n\treturn jbase > SRCU_MAX_INTERVAL ? SRCU_MAX_INTERVAL : jbase;\n}\n\n \nvoid cleanup_srcu_struct(struct srcu_struct *ssp)\n{\n\tint cpu;\n\tstruct srcu_usage *sup = ssp->srcu_sup;\n\n\tif (WARN_ON(!srcu_get_delay(ssp)))\n\t\treturn;  \n\tif (WARN_ON(srcu_readers_active(ssp)))\n\t\treturn;  \n\tflush_delayed_work(&sup->work);\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct srcu_data *sdp = per_cpu_ptr(ssp->sda, cpu);\n\n\t\tdel_timer_sync(&sdp->delay_work);\n\t\tflush_work(&sdp->work);\n\t\tif (WARN_ON(rcu_segcblist_n_cbs(&sdp->srcu_cblist)))\n\t\t\treturn;  \n\t}\n\tif (WARN_ON(rcu_seq_state(READ_ONCE(sup->srcu_gp_seq)) != SRCU_STATE_IDLE) ||\n\t    WARN_ON(rcu_seq_current(&sup->srcu_gp_seq) != sup->srcu_gp_seq_needed) ||\n\t    WARN_ON(srcu_readers_active(ssp))) {\n\t\tpr_info(\"%s: Active srcu_struct %p read state: %d gp state: %lu/%lu\\n\",\n\t\t\t__func__, ssp, rcu_seq_state(READ_ONCE(sup->srcu_gp_seq)),\n\t\t\trcu_seq_current(&sup->srcu_gp_seq), sup->srcu_gp_seq_needed);\n\t\treturn;  \n\t}\n\tkfree(sup->node);\n\tsup->node = NULL;\n\tsup->srcu_size_state = SRCU_SIZE_SMALL;\n\tif (!sup->sda_is_static) {\n\t\tfree_percpu(ssp->sda);\n\t\tssp->sda = NULL;\n\t\tkfree(sup);\n\t\tssp->srcu_sup = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(cleanup_srcu_struct);\n\n#ifdef CONFIG_PROVE_RCU\n \nvoid srcu_check_nmi_safety(struct srcu_struct *ssp, bool nmi_safe)\n{\n\tint nmi_safe_mask = 1 << nmi_safe;\n\tint old_nmi_safe_mask;\n\tstruct srcu_data *sdp;\n\n\t \n\tWARN_ON_ONCE(!nmi_safe && in_nmi());\n\tsdp = raw_cpu_ptr(ssp->sda);\n\told_nmi_safe_mask = READ_ONCE(sdp->srcu_nmi_safety);\n\tif (!old_nmi_safe_mask) {\n\t\tWRITE_ONCE(sdp->srcu_nmi_safety, nmi_safe_mask);\n\t\treturn;\n\t}\n\tWARN_ONCE(old_nmi_safe_mask != nmi_safe_mask, \"CPU %d old state %d new state %d\\n\", sdp->cpu, old_nmi_safe_mask, nmi_safe_mask);\n}\nEXPORT_SYMBOL_GPL(srcu_check_nmi_safety);\n#endif  \n\n \nint __srcu_read_lock(struct srcu_struct *ssp)\n{\n\tint idx;\n\n\tidx = READ_ONCE(ssp->srcu_idx) & 0x1;\n\tthis_cpu_inc(ssp->sda->srcu_lock_count[idx].counter);\n\tsmp_mb();     \n\treturn idx;\n}\nEXPORT_SYMBOL_GPL(__srcu_read_lock);\n\n \nvoid __srcu_read_unlock(struct srcu_struct *ssp, int idx)\n{\n\tsmp_mb();     \n\tthis_cpu_inc(ssp->sda->srcu_unlock_count[idx].counter);\n}\nEXPORT_SYMBOL_GPL(__srcu_read_unlock);\n\n#ifdef CONFIG_NEED_SRCU_NMI_SAFE\n\n \nint __srcu_read_lock_nmisafe(struct srcu_struct *ssp)\n{\n\tint idx;\n\tstruct srcu_data *sdp = raw_cpu_ptr(ssp->sda);\n\n\tidx = READ_ONCE(ssp->srcu_idx) & 0x1;\n\tatomic_long_inc(&sdp->srcu_lock_count[idx]);\n\tsmp_mb__after_atomic();     \n\treturn idx;\n}\nEXPORT_SYMBOL_GPL(__srcu_read_lock_nmisafe);\n\n \nvoid __srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)\n{\n\tstruct srcu_data *sdp = raw_cpu_ptr(ssp->sda);\n\n\tsmp_mb__before_atomic();     \n\tatomic_long_inc(&sdp->srcu_unlock_count[idx]);\n}\nEXPORT_SYMBOL_GPL(__srcu_read_unlock_nmisafe);\n\n#endif \n\n \nstatic void srcu_gp_start(struct srcu_struct *ssp)\n{\n\tstruct srcu_data *sdp;\n\tint state;\n\n\tif (smp_load_acquire(&ssp->srcu_sup->srcu_size_state) < SRCU_SIZE_WAIT_BARRIER)\n\t\tsdp = per_cpu_ptr(ssp->sda, get_boot_cpu_id());\n\telse\n\t\tsdp = this_cpu_ptr(ssp->sda);\n\tlockdep_assert_held(&ACCESS_PRIVATE(ssp->srcu_sup, lock));\n\tWARN_ON_ONCE(ULONG_CMP_GE(ssp->srcu_sup->srcu_gp_seq, ssp->srcu_sup->srcu_gp_seq_needed));\n\tspin_lock_rcu_node(sdp);   \n\trcu_segcblist_advance(&sdp->srcu_cblist,\n\t\t\t      rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq));\n\tWARN_ON_ONCE(!rcu_segcblist_segempty(&sdp->srcu_cblist, RCU_NEXT_TAIL));\n\tspin_unlock_rcu_node(sdp);   \n\tWRITE_ONCE(ssp->srcu_sup->srcu_gp_start, jiffies);\n\tWRITE_ONCE(ssp->srcu_sup->srcu_n_exp_nodelay, 0);\n\tsmp_mb();  \n\trcu_seq_start(&ssp->srcu_sup->srcu_gp_seq);\n\tstate = rcu_seq_state(ssp->srcu_sup->srcu_gp_seq);\n\tWARN_ON_ONCE(state != SRCU_STATE_SCAN1);\n}\n\n\nstatic void srcu_delay_timer(struct timer_list *t)\n{\n\tstruct srcu_data *sdp = container_of(t, struct srcu_data, delay_work);\n\n\tqueue_work_on(sdp->cpu, rcu_gp_wq, &sdp->work);\n}\n\nstatic void srcu_queue_delayed_work_on(struct srcu_data *sdp,\n\t\t\t\t       unsigned long delay)\n{\n\tif (!delay) {\n\t\tqueue_work_on(sdp->cpu, rcu_gp_wq, &sdp->work);\n\t\treturn;\n\t}\n\n\ttimer_reduce(&sdp->delay_work, jiffies + delay);\n}\n\n \nstatic void srcu_schedule_cbs_sdp(struct srcu_data *sdp, unsigned long delay)\n{\n\tsrcu_queue_delayed_work_on(sdp, delay);\n}\n\n \nstatic void srcu_schedule_cbs_snp(struct srcu_struct *ssp, struct srcu_node *snp,\n\t\t\t\t  unsigned long mask, unsigned long delay)\n{\n\tint cpu;\n\n\tfor (cpu = snp->grplo; cpu <= snp->grphi; cpu++) {\n\t\tif (!(mask & (1UL << (cpu - snp->grplo))))\n\t\t\tcontinue;\n\t\tsrcu_schedule_cbs_sdp(per_cpu_ptr(ssp->sda, cpu), delay);\n\t}\n}\n\n \nstatic void srcu_gp_end(struct srcu_struct *ssp)\n{\n\tunsigned long cbdelay = 1;\n\tbool cbs;\n\tbool last_lvl;\n\tint cpu;\n\tunsigned long flags;\n\tunsigned long gpseq;\n\tint idx;\n\tunsigned long mask;\n\tstruct srcu_data *sdp;\n\tunsigned long sgsne;\n\tstruct srcu_node *snp;\n\tint ss_state;\n\tstruct srcu_usage *sup = ssp->srcu_sup;\n\n\t \n\tmutex_lock(&sup->srcu_cb_mutex);\n\n\t \n\tspin_lock_irq_rcu_node(sup);\n\tidx = rcu_seq_state(sup->srcu_gp_seq);\n\tWARN_ON_ONCE(idx != SRCU_STATE_SCAN2);\n\tif (ULONG_CMP_LT(READ_ONCE(sup->srcu_gp_seq), READ_ONCE(sup->srcu_gp_seq_needed_exp)))\n\t\tcbdelay = 0;\n\n\tWRITE_ONCE(sup->srcu_last_gp_end, ktime_get_mono_fast_ns());\n\trcu_seq_end(&sup->srcu_gp_seq);\n\tgpseq = rcu_seq_current(&sup->srcu_gp_seq);\n\tif (ULONG_CMP_LT(sup->srcu_gp_seq_needed_exp, gpseq))\n\t\tWRITE_ONCE(sup->srcu_gp_seq_needed_exp, gpseq);\n\tspin_unlock_irq_rcu_node(sup);\n\tmutex_unlock(&sup->srcu_gp_mutex);\n\t \n\n\t \n\tss_state = smp_load_acquire(&sup->srcu_size_state);\n\tif (ss_state < SRCU_SIZE_WAIT_BARRIER) {\n\t\tsrcu_schedule_cbs_sdp(per_cpu_ptr(ssp->sda, get_boot_cpu_id()),\n\t\t\t\t\tcbdelay);\n\t} else {\n\t\tidx = rcu_seq_ctr(gpseq) % ARRAY_SIZE(snp->srcu_have_cbs);\n\t\tsrcu_for_each_node_breadth_first(ssp, snp) {\n\t\t\tspin_lock_irq_rcu_node(snp);\n\t\t\tcbs = false;\n\t\t\tlast_lvl = snp >= sup->level[rcu_num_lvls - 1];\n\t\t\tif (last_lvl)\n\t\t\t\tcbs = ss_state < SRCU_SIZE_BIG || snp->srcu_have_cbs[idx] == gpseq;\n\t\t\tsnp->srcu_have_cbs[idx] = gpseq;\n\t\t\trcu_seq_set_state(&snp->srcu_have_cbs[idx], 1);\n\t\t\tsgsne = snp->srcu_gp_seq_needed_exp;\n\t\t\tif (srcu_invl_snp_seq(sgsne) || ULONG_CMP_LT(sgsne, gpseq))\n\t\t\t\tWRITE_ONCE(snp->srcu_gp_seq_needed_exp, gpseq);\n\t\t\tif (ss_state < SRCU_SIZE_BIG)\n\t\t\t\tmask = ~0;\n\t\t\telse\n\t\t\t\tmask = snp->srcu_data_have_cbs[idx];\n\t\t\tsnp->srcu_data_have_cbs[idx] = 0;\n\t\t\tspin_unlock_irq_rcu_node(snp);\n\t\t\tif (cbs)\n\t\t\t\tsrcu_schedule_cbs_snp(ssp, snp, mask, cbdelay);\n\t\t}\n\t}\n\n\t \n\tif (!(gpseq & counter_wrap_check))\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tsdp = per_cpu_ptr(ssp->sda, cpu);\n\t\t\tspin_lock_irqsave_rcu_node(sdp, flags);\n\t\t\tif (ULONG_CMP_GE(gpseq, sdp->srcu_gp_seq_needed + 100))\n\t\t\t\tsdp->srcu_gp_seq_needed = gpseq;\n\t\t\tif (ULONG_CMP_GE(gpseq, sdp->srcu_gp_seq_needed_exp + 100))\n\t\t\t\tsdp->srcu_gp_seq_needed_exp = gpseq;\n\t\t\tspin_unlock_irqrestore_rcu_node(sdp, flags);\n\t\t}\n\n\t \n\tmutex_unlock(&sup->srcu_cb_mutex);\n\n\t \n\tspin_lock_irq_rcu_node(sup);\n\tgpseq = rcu_seq_current(&sup->srcu_gp_seq);\n\tif (!rcu_seq_state(gpseq) &&\n\t    ULONG_CMP_LT(gpseq, sup->srcu_gp_seq_needed)) {\n\t\tsrcu_gp_start(ssp);\n\t\tspin_unlock_irq_rcu_node(sup);\n\t\tsrcu_reschedule(ssp, 0);\n\t} else {\n\t\tspin_unlock_irq_rcu_node(sup);\n\t}\n\n\t \n\tif (ss_state != SRCU_SIZE_SMALL && ss_state != SRCU_SIZE_BIG) {\n\t\tif (ss_state == SRCU_SIZE_ALLOC)\n\t\t\tinit_srcu_struct_nodes(ssp, GFP_KERNEL);\n\t\telse\n\t\t\tsmp_store_release(&sup->srcu_size_state, ss_state + 1);\n\t}\n}\n\n \nstatic void srcu_funnel_exp_start(struct srcu_struct *ssp, struct srcu_node *snp,\n\t\t\t\t  unsigned long s)\n{\n\tunsigned long flags;\n\tunsigned long sgsne;\n\n\tif (snp)\n\t\tfor (; snp != NULL; snp = snp->srcu_parent) {\n\t\t\tsgsne = READ_ONCE(snp->srcu_gp_seq_needed_exp);\n\t\t\tif (WARN_ON_ONCE(rcu_seq_done(&ssp->srcu_sup->srcu_gp_seq, s)) ||\n\t\t\t    (!srcu_invl_snp_seq(sgsne) && ULONG_CMP_GE(sgsne, s)))\n\t\t\t\treturn;\n\t\t\tspin_lock_irqsave_rcu_node(snp, flags);\n\t\t\tsgsne = snp->srcu_gp_seq_needed_exp;\n\t\t\tif (!srcu_invl_snp_seq(sgsne) && ULONG_CMP_GE(sgsne, s)) {\n\t\t\t\tspin_unlock_irqrestore_rcu_node(snp, flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tWRITE_ONCE(snp->srcu_gp_seq_needed_exp, s);\n\t\t\tspin_unlock_irqrestore_rcu_node(snp, flags);\n\t\t}\n\tspin_lock_irqsave_ssp_contention(ssp, &flags);\n\tif (ULONG_CMP_LT(ssp->srcu_sup->srcu_gp_seq_needed_exp, s))\n\t\tWRITE_ONCE(ssp->srcu_sup->srcu_gp_seq_needed_exp, s);\n\tspin_unlock_irqrestore_rcu_node(ssp->srcu_sup, flags);\n}\n\n \nstatic void srcu_funnel_gp_start(struct srcu_struct *ssp, struct srcu_data *sdp,\n\t\t\t\t unsigned long s, bool do_norm)\n{\n\tunsigned long flags;\n\tint idx = rcu_seq_ctr(s) % ARRAY_SIZE(sdp->mynode->srcu_have_cbs);\n\tunsigned long sgsne;\n\tstruct srcu_node *snp;\n\tstruct srcu_node *snp_leaf;\n\tunsigned long snp_seq;\n\tstruct srcu_usage *sup = ssp->srcu_sup;\n\n\t \n\tif (smp_load_acquire(&sup->srcu_size_state) < SRCU_SIZE_WAIT_BARRIER)\n\t\tsnp_leaf = NULL;\n\telse\n\t\tsnp_leaf = sdp->mynode;\n\n\tif (snp_leaf)\n\t\t \n\t\tfor (snp = snp_leaf; snp != NULL; snp = snp->srcu_parent) {\n\t\t\tif (WARN_ON_ONCE(rcu_seq_done(&sup->srcu_gp_seq, s)) && snp != snp_leaf)\n\t\t\t\treturn;  \n\t\t\tspin_lock_irqsave_rcu_node(snp, flags);\n\t\t\tsnp_seq = snp->srcu_have_cbs[idx];\n\t\t\tif (!srcu_invl_snp_seq(snp_seq) && ULONG_CMP_GE(snp_seq, s)) {\n\t\t\t\tif (snp == snp_leaf && snp_seq == s)\n\t\t\t\t\tsnp->srcu_data_have_cbs[idx] |= sdp->grpmask;\n\t\t\t\tspin_unlock_irqrestore_rcu_node(snp, flags);\n\t\t\t\tif (snp == snp_leaf && snp_seq != s) {\n\t\t\t\t\tsrcu_schedule_cbs_sdp(sdp, do_norm ? SRCU_INTERVAL : 0);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tif (!do_norm)\n\t\t\t\t\tsrcu_funnel_exp_start(ssp, snp, s);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tsnp->srcu_have_cbs[idx] = s;\n\t\t\tif (snp == snp_leaf)\n\t\t\t\tsnp->srcu_data_have_cbs[idx] |= sdp->grpmask;\n\t\t\tsgsne = snp->srcu_gp_seq_needed_exp;\n\t\t\tif (!do_norm && (srcu_invl_snp_seq(sgsne) || ULONG_CMP_LT(sgsne, s)))\n\t\t\t\tWRITE_ONCE(snp->srcu_gp_seq_needed_exp, s);\n\t\t\tspin_unlock_irqrestore_rcu_node(snp, flags);\n\t\t}\n\n\t \n\tspin_lock_irqsave_ssp_contention(ssp, &flags);\n\tif (ULONG_CMP_LT(sup->srcu_gp_seq_needed, s)) {\n\t\t \n\t\tsmp_store_release(&sup->srcu_gp_seq_needed, s);  \n\t}\n\tif (!do_norm && ULONG_CMP_LT(sup->srcu_gp_seq_needed_exp, s))\n\t\tWRITE_ONCE(sup->srcu_gp_seq_needed_exp, s);\n\n\t \n\tif (!WARN_ON_ONCE(rcu_seq_done(&sup->srcu_gp_seq, s)) &&\n\t    rcu_seq_state(sup->srcu_gp_seq) == SRCU_STATE_IDLE) {\n\t\tWARN_ON_ONCE(ULONG_CMP_GE(sup->srcu_gp_seq, sup->srcu_gp_seq_needed));\n\t\tsrcu_gp_start(ssp);\n\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tif (likely(srcu_init_done))\n\t\t\tqueue_delayed_work(rcu_gp_wq, &sup->work,\n\t\t\t\t\t   !!srcu_get_delay(ssp));\n\t\telse if (list_empty(&sup->work.work.entry))\n\t\t\tlist_add(&sup->work.work.entry, &srcu_boot_list);\n\t}\n\tspin_unlock_irqrestore_rcu_node(sup, flags);\n}\n\n \nstatic bool try_check_zero(struct srcu_struct *ssp, int idx, int trycount)\n{\n\tunsigned long curdelay;\n\n\tcurdelay = !srcu_get_delay(ssp);\n\n\tfor (;;) {\n\t\tif (srcu_readers_active_idx_check(ssp, idx))\n\t\t\treturn true;\n\t\tif ((--trycount + curdelay) <= 0)\n\t\t\treturn false;\n\t\tudelay(srcu_retry_check_delay);\n\t}\n}\n\n \nstatic void srcu_flip(struct srcu_struct *ssp)\n{\n\t \n\tsmp_mb();     \n\n\tWRITE_ONCE(ssp->srcu_idx, ssp->srcu_idx + 1);  \n\n\t \n\tsmp_mb();     \n}\n\n \nstatic bool srcu_might_be_idle(struct srcu_struct *ssp)\n{\n\tunsigned long curseq;\n\tunsigned long flags;\n\tstruct srcu_data *sdp;\n\tunsigned long t;\n\tunsigned long tlast;\n\n\tcheck_init_srcu_struct(ssp);\n\t \n\tsdp = raw_cpu_ptr(ssp->sda);\n\tspin_lock_irqsave_rcu_node(sdp, flags);\n\tif (rcu_segcblist_pend_cbs(&sdp->srcu_cblist)) {\n\t\tspin_unlock_irqrestore_rcu_node(sdp, flags);\n\t\treturn false;  \n\t}\n\tspin_unlock_irqrestore_rcu_node(sdp, flags);\n\n\t \n\n\t \n\tt = ktime_get_mono_fast_ns();\n\ttlast = READ_ONCE(ssp->srcu_sup->srcu_last_gp_end);\n\tif (exp_holdoff == 0 ||\n\t    time_in_range_open(t, tlast, tlast + exp_holdoff))\n\t\treturn false;  \n\n\t \n\tcurseq = rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq);\n\tsmp_mb();  \n\tif (ULONG_CMP_LT(curseq, READ_ONCE(ssp->srcu_sup->srcu_gp_seq_needed)))\n\t\treturn false;  \n\tsmp_mb();  \n\tif (curseq != rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq))\n\t\treturn false;  \n\treturn true;  \n}\n\n \nstatic void srcu_leak_callback(struct rcu_head *rhp)\n{\n}\n\n \nstatic unsigned long srcu_gp_start_if_needed(struct srcu_struct *ssp,\n\t\t\t\t\t     struct rcu_head *rhp, bool do_norm)\n{\n\tunsigned long flags;\n\tint idx;\n\tbool needexp = false;\n\tbool needgp = false;\n\tunsigned long s;\n\tstruct srcu_data *sdp;\n\tstruct srcu_node *sdp_mynode;\n\tint ss_state;\n\n\tcheck_init_srcu_struct(ssp);\n\t \n\tidx = __srcu_read_lock_nmisafe(ssp);\n\tss_state = smp_load_acquire(&ssp->srcu_sup->srcu_size_state);\n\tif (ss_state < SRCU_SIZE_WAIT_CALL)\n\t\tsdp = per_cpu_ptr(ssp->sda, get_boot_cpu_id());\n\telse\n\t\tsdp = raw_cpu_ptr(ssp->sda);\n\tspin_lock_irqsave_sdp_contention(sdp, &flags);\n\tif (rhp)\n\t\trcu_segcblist_enqueue(&sdp->srcu_cblist, rhp);\n\t \n\ts = rcu_seq_snap(&ssp->srcu_sup->srcu_gp_seq);\n\trcu_segcblist_advance(&sdp->srcu_cblist,\n\t\t\t      rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq));\n\tWARN_ON_ONCE(!rcu_segcblist_accelerate(&sdp->srcu_cblist, s) && rhp);\n\tif (ULONG_CMP_LT(sdp->srcu_gp_seq_needed, s)) {\n\t\tsdp->srcu_gp_seq_needed = s;\n\t\tneedgp = true;\n\t}\n\tif (!do_norm && ULONG_CMP_LT(sdp->srcu_gp_seq_needed_exp, s)) {\n\t\tsdp->srcu_gp_seq_needed_exp = s;\n\t\tneedexp = true;\n\t}\n\tspin_unlock_irqrestore_rcu_node(sdp, flags);\n\n\t \n\tif (ss_state < SRCU_SIZE_WAIT_BARRIER)\n\t\tsdp_mynode = NULL;\n\telse\n\t\tsdp_mynode = sdp->mynode;\n\n\tif (needgp)\n\t\tsrcu_funnel_gp_start(ssp, sdp, s, do_norm);\n\telse if (needexp)\n\t\tsrcu_funnel_exp_start(ssp, sdp_mynode, s);\n\t__srcu_read_unlock_nmisafe(ssp, idx);\n\treturn s;\n}\n\n \nstatic void __call_srcu(struct srcu_struct *ssp, struct rcu_head *rhp,\n\t\t\trcu_callback_t func, bool do_norm)\n{\n\tif (debug_rcu_head_queue(rhp)) {\n\t\t \n\t\tWRITE_ONCE(rhp->func, srcu_leak_callback);\n\t\tWARN_ONCE(1, \"call_srcu(): Leaked duplicate callback\\n\");\n\t\treturn;\n\t}\n\trhp->func = func;\n\t(void)srcu_gp_start_if_needed(ssp, rhp, do_norm);\n}\n\n \nvoid call_srcu(struct srcu_struct *ssp, struct rcu_head *rhp,\n\t       rcu_callback_t func)\n{\n\t__call_srcu(ssp, rhp, func, true);\n}\nEXPORT_SYMBOL_GPL(call_srcu);\n\n \nstatic void __synchronize_srcu(struct srcu_struct *ssp, bool do_norm)\n{\n\tstruct rcu_synchronize rcu;\n\n\tsrcu_lock_sync(&ssp->dep_map);\n\n\tRCU_LOCKDEP_WARN(lockdep_is_held(ssp) ||\n\t\t\t lock_is_held(&rcu_bh_lock_map) ||\n\t\t\t lock_is_held(&rcu_lock_map) ||\n\t\t\t lock_is_held(&rcu_sched_lock_map),\n\t\t\t \"Illegal synchronize_srcu() in same-type SRCU (or in RCU) read-side critical section\");\n\n\tif (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)\n\t\treturn;\n\tmight_sleep();\n\tcheck_init_srcu_struct(ssp);\n\tinit_completion(&rcu.completion);\n\tinit_rcu_head_on_stack(&rcu.head);\n\t__call_srcu(ssp, &rcu.head, wakeme_after_rcu, do_norm);\n\twait_for_completion(&rcu.completion);\n\tdestroy_rcu_head_on_stack(&rcu.head);\n\n\t \n\tsmp_mb();\n}\n\n \nvoid synchronize_srcu_expedited(struct srcu_struct *ssp)\n{\n\t__synchronize_srcu(ssp, rcu_gp_is_normal());\n}\nEXPORT_SYMBOL_GPL(synchronize_srcu_expedited);\n\n \nvoid synchronize_srcu(struct srcu_struct *ssp)\n{\n\tif (srcu_might_be_idle(ssp) || rcu_gp_is_expedited())\n\t\tsynchronize_srcu_expedited(ssp);\n\telse\n\t\t__synchronize_srcu(ssp, true);\n}\nEXPORT_SYMBOL_GPL(synchronize_srcu);\n\n \nunsigned long get_state_synchronize_srcu(struct srcu_struct *ssp)\n{\n\t \n\t \n\tsmp_mb();\n\treturn rcu_seq_snap(&ssp->srcu_sup->srcu_gp_seq);\n}\nEXPORT_SYMBOL_GPL(get_state_synchronize_srcu);\n\n \nunsigned long start_poll_synchronize_srcu(struct srcu_struct *ssp)\n{\n\treturn srcu_gp_start_if_needed(ssp, NULL, true);\n}\nEXPORT_SYMBOL_GPL(start_poll_synchronize_srcu);\n\n \nbool poll_state_synchronize_srcu(struct srcu_struct *ssp, unsigned long cookie)\n{\n\tif (!rcu_seq_done(&ssp->srcu_sup->srcu_gp_seq, cookie))\n\t\treturn false;\n\t\n\t\n\tsmp_mb(); \n\treturn true;\n}\nEXPORT_SYMBOL_GPL(poll_state_synchronize_srcu);\n\n \nstatic void srcu_barrier_cb(struct rcu_head *rhp)\n{\n\tstruct srcu_data *sdp;\n\tstruct srcu_struct *ssp;\n\n\tsdp = container_of(rhp, struct srcu_data, srcu_barrier_head);\n\tssp = sdp->ssp;\n\tif (atomic_dec_and_test(&ssp->srcu_sup->srcu_barrier_cpu_cnt))\n\t\tcomplete(&ssp->srcu_sup->srcu_barrier_completion);\n}\n\n \nstatic void srcu_barrier_one_cpu(struct srcu_struct *ssp, struct srcu_data *sdp)\n{\n\tspin_lock_irq_rcu_node(sdp);\n\tatomic_inc(&ssp->srcu_sup->srcu_barrier_cpu_cnt);\n\tsdp->srcu_barrier_head.func = srcu_barrier_cb;\n\tdebug_rcu_head_queue(&sdp->srcu_barrier_head);\n\tif (!rcu_segcblist_entrain(&sdp->srcu_cblist,\n\t\t\t\t   &sdp->srcu_barrier_head)) {\n\t\tdebug_rcu_head_unqueue(&sdp->srcu_barrier_head);\n\t\tatomic_dec(&ssp->srcu_sup->srcu_barrier_cpu_cnt);\n\t}\n\tspin_unlock_irq_rcu_node(sdp);\n}\n\n \nvoid srcu_barrier(struct srcu_struct *ssp)\n{\n\tint cpu;\n\tint idx;\n\tunsigned long s = rcu_seq_snap(&ssp->srcu_sup->srcu_barrier_seq);\n\n\tcheck_init_srcu_struct(ssp);\n\tmutex_lock(&ssp->srcu_sup->srcu_barrier_mutex);\n\tif (rcu_seq_done(&ssp->srcu_sup->srcu_barrier_seq, s)) {\n\t\tsmp_mb();  \n\t\tmutex_unlock(&ssp->srcu_sup->srcu_barrier_mutex);\n\t\treturn;  \n\t}\n\trcu_seq_start(&ssp->srcu_sup->srcu_barrier_seq);\n\tinit_completion(&ssp->srcu_sup->srcu_barrier_completion);\n\n\t \n\tatomic_set(&ssp->srcu_sup->srcu_barrier_cpu_cnt, 1);\n\n\tidx = __srcu_read_lock_nmisafe(ssp);\n\tif (smp_load_acquire(&ssp->srcu_sup->srcu_size_state) < SRCU_SIZE_WAIT_BARRIER)\n\t\tsrcu_barrier_one_cpu(ssp, per_cpu_ptr(ssp->sda,\tget_boot_cpu_id()));\n\telse\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tsrcu_barrier_one_cpu(ssp, per_cpu_ptr(ssp->sda, cpu));\n\t__srcu_read_unlock_nmisafe(ssp, idx);\n\n\t \n\tif (atomic_dec_and_test(&ssp->srcu_sup->srcu_barrier_cpu_cnt))\n\t\tcomplete(&ssp->srcu_sup->srcu_barrier_completion);\n\twait_for_completion(&ssp->srcu_sup->srcu_barrier_completion);\n\n\trcu_seq_end(&ssp->srcu_sup->srcu_barrier_seq);\n\tmutex_unlock(&ssp->srcu_sup->srcu_barrier_mutex);\n}\nEXPORT_SYMBOL_GPL(srcu_barrier);\n\n \nunsigned long srcu_batches_completed(struct srcu_struct *ssp)\n{\n\treturn READ_ONCE(ssp->srcu_idx);\n}\nEXPORT_SYMBOL_GPL(srcu_batches_completed);\n\n \nstatic void srcu_advance_state(struct srcu_struct *ssp)\n{\n\tint idx;\n\n\tmutex_lock(&ssp->srcu_sup->srcu_gp_mutex);\n\n\t \n\tidx = rcu_seq_state(smp_load_acquire(&ssp->srcu_sup->srcu_gp_seq));  \n\tif (idx == SRCU_STATE_IDLE) {\n\t\tspin_lock_irq_rcu_node(ssp->srcu_sup);\n\t\tif (ULONG_CMP_GE(ssp->srcu_sup->srcu_gp_seq, ssp->srcu_sup->srcu_gp_seq_needed)) {\n\t\t\tWARN_ON_ONCE(rcu_seq_state(ssp->srcu_sup->srcu_gp_seq));\n\t\t\tspin_unlock_irq_rcu_node(ssp->srcu_sup);\n\t\t\tmutex_unlock(&ssp->srcu_sup->srcu_gp_mutex);\n\t\t\treturn;\n\t\t}\n\t\tidx = rcu_seq_state(READ_ONCE(ssp->srcu_sup->srcu_gp_seq));\n\t\tif (idx == SRCU_STATE_IDLE)\n\t\t\tsrcu_gp_start(ssp);\n\t\tspin_unlock_irq_rcu_node(ssp->srcu_sup);\n\t\tif (idx != SRCU_STATE_IDLE) {\n\t\t\tmutex_unlock(&ssp->srcu_sup->srcu_gp_mutex);\n\t\t\treturn;  \n\t\t}\n\t}\n\n\tif (rcu_seq_state(READ_ONCE(ssp->srcu_sup->srcu_gp_seq)) == SRCU_STATE_SCAN1) {\n\t\tidx = 1 ^ (ssp->srcu_idx & 1);\n\t\tif (!try_check_zero(ssp, idx, 1)) {\n\t\t\tmutex_unlock(&ssp->srcu_sup->srcu_gp_mutex);\n\t\t\treturn;  \n\t\t}\n\t\tsrcu_flip(ssp);\n\t\tspin_lock_irq_rcu_node(ssp->srcu_sup);\n\t\trcu_seq_set_state(&ssp->srcu_sup->srcu_gp_seq, SRCU_STATE_SCAN2);\n\t\tssp->srcu_sup->srcu_n_exp_nodelay = 0;\n\t\tspin_unlock_irq_rcu_node(ssp->srcu_sup);\n\t}\n\n\tif (rcu_seq_state(READ_ONCE(ssp->srcu_sup->srcu_gp_seq)) == SRCU_STATE_SCAN2) {\n\n\t\t \n\t\tidx = 1 ^ (ssp->srcu_idx & 1);\n\t\tif (!try_check_zero(ssp, idx, 2)) {\n\t\t\tmutex_unlock(&ssp->srcu_sup->srcu_gp_mutex);\n\t\t\treturn;  \n\t\t}\n\t\tssp->srcu_sup->srcu_n_exp_nodelay = 0;\n\t\tsrcu_gp_end(ssp);   \n\t}\n}\n\n \nstatic void srcu_invoke_callbacks(struct work_struct *work)\n{\n\tlong len;\n\tbool more;\n\tstruct rcu_cblist ready_cbs;\n\tstruct rcu_head *rhp;\n\tstruct srcu_data *sdp;\n\tstruct srcu_struct *ssp;\n\n\tsdp = container_of(work, struct srcu_data, work);\n\n\tssp = sdp->ssp;\n\trcu_cblist_init(&ready_cbs);\n\tspin_lock_irq_rcu_node(sdp);\n\tWARN_ON_ONCE(!rcu_segcblist_segempty(&sdp->srcu_cblist, RCU_NEXT_TAIL));\n\trcu_segcblist_advance(&sdp->srcu_cblist,\n\t\t\t      rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq));\n\tif (sdp->srcu_cblist_invoking ||\n\t    !rcu_segcblist_ready_cbs(&sdp->srcu_cblist)) {\n\t\tspin_unlock_irq_rcu_node(sdp);\n\t\treturn;   \n\t}\n\n\t \n\tsdp->srcu_cblist_invoking = true;\n\trcu_segcblist_extract_done_cbs(&sdp->srcu_cblist, &ready_cbs);\n\tlen = ready_cbs.len;\n\tspin_unlock_irq_rcu_node(sdp);\n\trhp = rcu_cblist_dequeue(&ready_cbs);\n\tfor (; rhp != NULL; rhp = rcu_cblist_dequeue(&ready_cbs)) {\n\t\tdebug_rcu_head_unqueue(rhp);\n\t\tlocal_bh_disable();\n\t\trhp->func(rhp);\n\t\tlocal_bh_enable();\n\t}\n\tWARN_ON_ONCE(ready_cbs.len);\n\n\t \n\tspin_lock_irq_rcu_node(sdp);\n\trcu_segcblist_add_len(&sdp->srcu_cblist, -len);\n\tsdp->srcu_cblist_invoking = false;\n\tmore = rcu_segcblist_ready_cbs(&sdp->srcu_cblist);\n\tspin_unlock_irq_rcu_node(sdp);\n\tif (more)\n\t\tsrcu_schedule_cbs_sdp(sdp, 0);\n}\n\n \nstatic void srcu_reschedule(struct srcu_struct *ssp, unsigned long delay)\n{\n\tbool pushgp = true;\n\n\tspin_lock_irq_rcu_node(ssp->srcu_sup);\n\tif (ULONG_CMP_GE(ssp->srcu_sup->srcu_gp_seq, ssp->srcu_sup->srcu_gp_seq_needed)) {\n\t\tif (!WARN_ON_ONCE(rcu_seq_state(ssp->srcu_sup->srcu_gp_seq))) {\n\t\t\t \n\t\t\tpushgp = false;\n\t\t}\n\t} else if (!rcu_seq_state(ssp->srcu_sup->srcu_gp_seq)) {\n\t\t \n\t\tsrcu_gp_start(ssp);\n\t}\n\tspin_unlock_irq_rcu_node(ssp->srcu_sup);\n\n\tif (pushgp)\n\t\tqueue_delayed_work(rcu_gp_wq, &ssp->srcu_sup->work, delay);\n}\n\n \nstatic void process_srcu(struct work_struct *work)\n{\n\tunsigned long curdelay;\n\tunsigned long j;\n\tstruct srcu_struct *ssp;\n\tstruct srcu_usage *sup;\n\n\tsup = container_of(work, struct srcu_usage, work.work);\n\tssp = sup->srcu_ssp;\n\n\tsrcu_advance_state(ssp);\n\tcurdelay = srcu_get_delay(ssp);\n\tif (curdelay) {\n\t\tWRITE_ONCE(sup->reschedule_count, 0);\n\t} else {\n\t\tj = jiffies;\n\t\tif (READ_ONCE(sup->reschedule_jiffies) == j) {\n\t\t\tWRITE_ONCE(sup->reschedule_count, READ_ONCE(sup->reschedule_count) + 1);\n\t\t\tif (READ_ONCE(sup->reschedule_count) > srcu_max_nodelay)\n\t\t\t\tcurdelay = 1;\n\t\t} else {\n\t\t\tWRITE_ONCE(sup->reschedule_count, 1);\n\t\t\tWRITE_ONCE(sup->reschedule_jiffies, j);\n\t\t}\n\t}\n\tsrcu_reschedule(ssp, curdelay);\n}\n\nvoid srcutorture_get_gp_data(enum rcutorture_type test_type,\n\t\t\t     struct srcu_struct *ssp, int *flags,\n\t\t\t     unsigned long *gp_seq)\n{\n\tif (test_type != SRCU_FLAVOR)\n\t\treturn;\n\t*flags = 0;\n\t*gp_seq = rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq);\n}\nEXPORT_SYMBOL_GPL(srcutorture_get_gp_data);\n\nstatic const char * const srcu_size_state_name[] = {\n\t\"SRCU_SIZE_SMALL\",\n\t\"SRCU_SIZE_ALLOC\",\n\t\"SRCU_SIZE_WAIT_BARRIER\",\n\t\"SRCU_SIZE_WAIT_CALL\",\n\t\"SRCU_SIZE_WAIT_CBS1\",\n\t\"SRCU_SIZE_WAIT_CBS2\",\n\t\"SRCU_SIZE_WAIT_CBS3\",\n\t\"SRCU_SIZE_WAIT_CBS4\",\n\t\"SRCU_SIZE_BIG\",\n\t\"SRCU_SIZE_???\",\n};\n\nvoid srcu_torture_stats_print(struct srcu_struct *ssp, char *tt, char *tf)\n{\n\tint cpu;\n\tint idx;\n\tunsigned long s0 = 0, s1 = 0;\n\tint ss_state = READ_ONCE(ssp->srcu_sup->srcu_size_state);\n\tint ss_state_idx = ss_state;\n\n\tidx = ssp->srcu_idx & 0x1;\n\tif (ss_state < 0 || ss_state >= ARRAY_SIZE(srcu_size_state_name))\n\t\tss_state_idx = ARRAY_SIZE(srcu_size_state_name) - 1;\n\tpr_alert(\"%s%s Tree SRCU g%ld state %d (%s)\",\n\t\t tt, tf, rcu_seq_current(&ssp->srcu_sup->srcu_gp_seq), ss_state,\n\t\t srcu_size_state_name[ss_state_idx]);\n\tif (!ssp->sda) {\n\t\t\n\t\tpr_cont(\" No per-CPU srcu_data structures (->sda == NULL).\\n\");\n\t} else {\n\t\tpr_cont(\" per-CPU(idx=%d):\", idx);\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tunsigned long l0, l1;\n\t\t\tunsigned long u0, u1;\n\t\t\tlong c0, c1;\n\t\t\tstruct srcu_data *sdp;\n\n\t\t\tsdp = per_cpu_ptr(ssp->sda, cpu);\n\t\t\tu0 = data_race(atomic_long_read(&sdp->srcu_unlock_count[!idx]));\n\t\t\tu1 = data_race(atomic_long_read(&sdp->srcu_unlock_count[idx]));\n\n\t\t\t \n\t\t\tsmp_rmb();\n\n\t\t\tl0 = data_race(atomic_long_read(&sdp->srcu_lock_count[!idx]));\n\t\t\tl1 = data_race(atomic_long_read(&sdp->srcu_lock_count[idx]));\n\n\t\t\tc0 = l0 - u0;\n\t\t\tc1 = l1 - u1;\n\t\t\tpr_cont(\" %d(%ld,%ld %c)\",\n\t\t\t\tcpu, c0, c1,\n\t\t\t\t\"C.\"[rcu_segcblist_empty(&sdp->srcu_cblist)]);\n\t\t\ts0 += c0;\n\t\t\ts1 += c1;\n\t\t}\n\t\tpr_cont(\" T(%ld,%ld)\\n\", s0, s1);\n\t}\n\tif (SRCU_SIZING_IS_TORTURE())\n\t\tsrcu_transition_to_big(ssp);\n}\nEXPORT_SYMBOL_GPL(srcu_torture_stats_print);\n\nstatic int __init srcu_bootup_announce(void)\n{\n\tpr_info(\"Hierarchical SRCU implementation.\\n\");\n\tif (exp_holdoff != DEFAULT_SRCU_EXP_HOLDOFF)\n\t\tpr_info(\"\\tNon-default auto-expedite holdoff of %lu ns.\\n\", exp_holdoff);\n\tif (srcu_retry_check_delay != SRCU_DEFAULT_RETRY_CHECK_DELAY)\n\t\tpr_info(\"\\tNon-default retry check delay of %lu us.\\n\", srcu_retry_check_delay);\n\tif (srcu_max_nodelay != SRCU_DEFAULT_MAX_NODELAY)\n\t\tpr_info(\"\\tNon-default max no-delay of %lu.\\n\", srcu_max_nodelay);\n\tpr_info(\"\\tMax phase no-delay instances is %lu.\\n\", srcu_max_nodelay_phase);\n\treturn 0;\n}\nearly_initcall(srcu_bootup_announce);\n\nvoid __init srcu_init(void)\n{\n\tstruct srcu_usage *sup;\n\n\t \n\tif (SRCU_SIZING_IS(SRCU_SIZING_AUTO)) {\n\t\tif (nr_cpu_ids >= big_cpu_lim) {\n\t\t\tconvert_to_big = SRCU_SIZING_INIT; \n\t\t\tpr_info(\"%s: Setting srcu_struct sizes to big.\\n\", __func__);\n\t\t} else {\n\t\t\tconvert_to_big = SRCU_SIZING_NONE | SRCU_SIZING_CONTEND;\n\t\t\tpr_info(\"%s: Setting srcu_struct sizes based on contention.\\n\", __func__);\n\t\t}\n\t}\n\n\t \n\tsrcu_init_done = true;\n\twhile (!list_empty(&srcu_boot_list)) {\n\t\tsup = list_first_entry(&srcu_boot_list, struct srcu_usage,\n\t\t\t\t      work.work.entry);\n\t\tlist_del_init(&sup->work.work.entry);\n\t\tif (SRCU_SIZING_IS(SRCU_SIZING_INIT) &&\n\t\t    sup->srcu_size_state == SRCU_SIZE_SMALL)\n\t\t\tsup->srcu_size_state = SRCU_SIZE_ALLOC;\n\t\tqueue_work(rcu_gp_wq, &sup->work.work);\n\t}\n}\n\n#ifdef CONFIG_MODULES\n\n \nstatic int srcu_module_coming(struct module *mod)\n{\n\tint i;\n\tstruct srcu_struct *ssp;\n\tstruct srcu_struct **sspp = mod->srcu_struct_ptrs;\n\n\tfor (i = 0; i < mod->num_srcu_structs; i++) {\n\t\tssp = *(sspp++);\n\t\tssp->sda = alloc_percpu(struct srcu_data);\n\t\tif (WARN_ON_ONCE(!ssp->sda))\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\n \nstatic void srcu_module_going(struct module *mod)\n{\n\tint i;\n\tstruct srcu_struct *ssp;\n\tstruct srcu_struct **sspp = mod->srcu_struct_ptrs;\n\n\tfor (i = 0; i < mod->num_srcu_structs; i++) {\n\t\tssp = *(sspp++);\n\t\tif (!rcu_seq_state(smp_load_acquire(&ssp->srcu_sup->srcu_gp_seq_needed)) &&\n\t\t    !WARN_ON_ONCE(!ssp->srcu_sup->sda_is_static))\n\t\t\tcleanup_srcu_struct(ssp);\n\t\tif (!WARN_ON(srcu_readers_active(ssp)))\n\t\t\tfree_percpu(ssp->sda);\n\t}\n}\n\n \nstatic int srcu_module_notify(struct notifier_block *self,\n\t\t\t      unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\tint ret = 0;\n\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\tret = srcu_module_coming(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\tsrcu_module_going(mod);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic struct notifier_block srcu_module_nb = {\n\t.notifier_call = srcu_module_notify,\n\t.priority = 0,\n};\n\nstatic __init int init_srcu_module_notifier(void)\n{\n\tint ret;\n\n\tret = register_module_notifier(&srcu_module_nb);\n\tif (ret)\n\t\tpr_warn(\"Failed to register srcu module notifier\\n\");\n\treturn ret;\n}\nlate_initcall(init_srcu_module_notifier);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}