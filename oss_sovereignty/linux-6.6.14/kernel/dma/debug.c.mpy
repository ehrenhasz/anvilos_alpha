{
  "module_name": "debug.c",
  "hash_id": "b8e8232573c9a87f4ad7abfa16edb5cc06dcc7bee45025160af0751d8634591e",
  "original_prompt": "Ingested from linux-6.6.14/kernel/dma/debug.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"DMA-API: \" fmt\n\n#include <linux/sched/task_stack.h>\n#include <linux/scatterlist.h>\n#include <linux/dma-map-ops.h>\n#include <linux/sched/task.h>\n#include <linux/stacktrace.h>\n#include <linux/spinlock.h>\n#include <linux/vmalloc.h>\n#include <linux/debugfs.h>\n#include <linux/uaccess.h>\n#include <linux/export.h>\n#include <linux/device.h>\n#include <linux/types.h>\n#include <linux/sched.h>\n#include <linux/ctype.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <asm/sections.h>\n#include \"debug.h\"\n\n#define HASH_SIZE       16384ULL\n#define HASH_FN_SHIFT   13\n#define HASH_FN_MASK    (HASH_SIZE - 1)\n\n#define PREALLOC_DMA_DEBUG_ENTRIES (1 << 16)\n \n#define DMA_DEBUG_DYNAMIC_ENTRIES (PAGE_SIZE / sizeof(struct dma_debug_entry))\n\nenum {\n\tdma_debug_single,\n\tdma_debug_sg,\n\tdma_debug_coherent,\n\tdma_debug_resource,\n};\n\nenum map_err_types {\n\tMAP_ERR_CHECK_NOT_APPLICABLE,\n\tMAP_ERR_NOT_CHECKED,\n\tMAP_ERR_CHECKED,\n};\n\n#define DMA_DEBUG_STACKTRACE_ENTRIES 5\n\n \nstruct dma_debug_entry {\n\tstruct list_head list;\n\tstruct device    *dev;\n\tu64              dev_addr;\n\tu64              size;\n\tint              type;\n\tint              direction;\n\tint\t\t sg_call_ents;\n\tint\t\t sg_mapped_ents;\n\tunsigned long\t pfn;\n\tsize_t\t\t offset;\n\tenum map_err_types  map_err_type;\n#ifdef CONFIG_STACKTRACE\n\tunsigned int\tstack_len;\n\tunsigned long\tstack_entries[DMA_DEBUG_STACKTRACE_ENTRIES];\n#endif\n} ____cacheline_aligned_in_smp;\n\ntypedef bool (*match_fn)(struct dma_debug_entry *, struct dma_debug_entry *);\n\nstruct hash_bucket {\n\tstruct list_head list;\n\tspinlock_t lock;\n};\n\n \nstatic struct hash_bucket dma_entry_hash[HASH_SIZE];\n \nstatic LIST_HEAD(free_entries);\n \nstatic DEFINE_SPINLOCK(free_entries_lock);\n\n \nstatic bool global_disable __read_mostly;\n\n \nstatic bool dma_debug_initialized __read_mostly;\n\nstatic inline bool dma_debug_disabled(void)\n{\n\treturn global_disable || !dma_debug_initialized;\n}\n\n \nstatic u32 error_count;\n\n \nstatic u32 show_all_errors __read_mostly;\n \nstatic u32 show_num_errors = 1;\n\nstatic u32 num_free_entries;\nstatic u32 min_free_entries;\nstatic u32 nr_total_entries;\n\n \nstatic u32 nr_prealloc_entries = PREALLOC_DMA_DEBUG_ENTRIES;\n\n \n\n#define NAME_MAX_LEN\t64\n\nstatic char                  current_driver_name[NAME_MAX_LEN] __read_mostly;\nstatic struct device_driver *current_driver                    __read_mostly;\n\nstatic DEFINE_RWLOCK(driver_name_lock);\n\nstatic const char *const maperr2str[] = {\n\t[MAP_ERR_CHECK_NOT_APPLICABLE] = \"dma map error check not applicable\",\n\t[MAP_ERR_NOT_CHECKED] = \"dma map error not checked\",\n\t[MAP_ERR_CHECKED] = \"dma map error checked\",\n};\n\nstatic const char *type2name[] = {\n\t[dma_debug_single] = \"single\",\n\t[dma_debug_sg] = \"scather-gather\",\n\t[dma_debug_coherent] = \"coherent\",\n\t[dma_debug_resource] = \"resource\",\n};\n\nstatic const char *dir2name[] = {\n\t[DMA_BIDIRECTIONAL]\t= \"DMA_BIDIRECTIONAL\",\n\t[DMA_TO_DEVICE]\t\t= \"DMA_TO_DEVICE\",\n\t[DMA_FROM_DEVICE]\t= \"DMA_FROM_DEVICE\",\n\t[DMA_NONE]\t\t= \"DMA_NONE\",\n};\n\n \nstatic inline void dump_entry_trace(struct dma_debug_entry *entry)\n{\n#ifdef CONFIG_STACKTRACE\n\tif (entry) {\n\t\tpr_warn(\"Mapped at:\\n\");\n\t\tstack_trace_print(entry->stack_entries, entry->stack_len, 0);\n\t}\n#endif\n}\n\nstatic bool driver_filter(struct device *dev)\n{\n\tstruct device_driver *drv;\n\tunsigned long flags;\n\tbool ret;\n\n\t \n\tif (likely(!current_driver_name[0]))\n\t\treturn true;\n\n\t \n\tif (current_driver && dev && dev->driver == current_driver)\n\t\treturn true;\n\n\t \n\tif (!dev)\n\t\treturn false;\n\n\tif (current_driver || !current_driver_name[0])\n\t\treturn false;\n\n\t \n\tdrv = dev->driver;\n\tif (!drv)\n\t\treturn false;\n\n\t \n\tread_lock_irqsave(&driver_name_lock, flags);\n\n\tret = false;\n\tif (drv->name &&\n\t    strncmp(current_driver_name, drv->name, NAME_MAX_LEN - 1) == 0) {\n\t\tcurrent_driver = drv;\n\t\tret = true;\n\t}\n\n\tread_unlock_irqrestore(&driver_name_lock, flags);\n\n\treturn ret;\n}\n\n#define err_printk(dev, entry, format, arg...) do {\t\t\t\\\n\t\terror_count += 1;\t\t\t\t\t\\\n\t\tif (driver_filter(dev) &&\t\t\t\t\\\n\t\t    (show_all_errors || show_num_errors > 0)) {\t\t\\\n\t\t\tWARN(1, pr_fmt(\"%s %s: \") format,\t\t\\\n\t\t\t     dev ? dev_driver_string(dev) : \"NULL\",\t\\\n\t\t\t     dev ? dev_name(dev) : \"NULL\", ## arg);\t\\\n\t\t\tdump_entry_trace(entry);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tif (!show_all_errors && show_num_errors > 0)\t\t\\\n\t\t\tshow_num_errors -= 1;\t\t\t\t\\\n\t} while (0);\n\n \nstatic int hash_fn(struct dma_debug_entry *entry)\n{\n\t \n\treturn (entry->dev_addr >> HASH_FN_SHIFT) & HASH_FN_MASK;\n}\n\n \nstatic struct hash_bucket *get_hash_bucket(struct dma_debug_entry *entry,\n\t\t\t\t\t   unsigned long *flags)\n\t__acquires(&dma_entry_hash[idx].lock)\n{\n\tint idx = hash_fn(entry);\n\tunsigned long __flags;\n\n\tspin_lock_irqsave(&dma_entry_hash[idx].lock, __flags);\n\t*flags = __flags;\n\treturn &dma_entry_hash[idx];\n}\n\n \nstatic void put_hash_bucket(struct hash_bucket *bucket,\n\t\t\t    unsigned long flags)\n\t__releases(&bucket->lock)\n{\n\tspin_unlock_irqrestore(&bucket->lock, flags);\n}\n\nstatic bool exact_match(struct dma_debug_entry *a, struct dma_debug_entry *b)\n{\n\treturn ((a->dev_addr == b->dev_addr) &&\n\t\t(a->dev == b->dev)) ? true : false;\n}\n\nstatic bool containing_match(struct dma_debug_entry *a,\n\t\t\t     struct dma_debug_entry *b)\n{\n\tif (a->dev != b->dev)\n\t\treturn false;\n\n\tif ((b->dev_addr <= a->dev_addr) &&\n\t    ((b->dev_addr + b->size) >= (a->dev_addr + a->size)))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic struct dma_debug_entry *__hash_bucket_find(struct hash_bucket *bucket,\n\t\t\t\t\t\t  struct dma_debug_entry *ref,\n\t\t\t\t\t\t  match_fn match)\n{\n\tstruct dma_debug_entry *entry, *ret = NULL;\n\tint matches = 0, match_lvl, last_lvl = -1;\n\n\tlist_for_each_entry(entry, &bucket->list, list) {\n\t\tif (!match(ref, entry))\n\t\t\tcontinue;\n\n\t\t \n\t\tmatches += 1;\n\t\tmatch_lvl = 0;\n\t\tentry->size         == ref->size         ? ++match_lvl : 0;\n\t\tentry->type         == ref->type         ? ++match_lvl : 0;\n\t\tentry->direction    == ref->direction    ? ++match_lvl : 0;\n\t\tentry->sg_call_ents == ref->sg_call_ents ? ++match_lvl : 0;\n\n\t\tif (match_lvl == 4) {\n\t\t\t \n\t\t\treturn entry;\n\t\t} else if (match_lvl > last_lvl) {\n\t\t\t \n\t\t\tlast_lvl = match_lvl;\n\t\t\tret      = entry;\n\t\t}\n\t}\n\n\t \n\tret = (matches == 1) ? ret : NULL;\n\n\treturn ret;\n}\n\nstatic struct dma_debug_entry *bucket_find_exact(struct hash_bucket *bucket,\n\t\t\t\t\t\t struct dma_debug_entry *ref)\n{\n\treturn __hash_bucket_find(bucket, ref, exact_match);\n}\n\nstatic struct dma_debug_entry *bucket_find_contain(struct hash_bucket **bucket,\n\t\t\t\t\t\t   struct dma_debug_entry *ref,\n\t\t\t\t\t\t   unsigned long *flags)\n{\n\n\tstruct dma_debug_entry *entry, index = *ref;\n\tint limit = min(HASH_SIZE, (index.dev_addr >> HASH_FN_SHIFT) + 1);\n\n\tfor (int i = 0; i < limit; i++) {\n\t\tentry = __hash_bucket_find(*bucket, ref, containing_match);\n\n\t\tif (entry)\n\t\t\treturn entry;\n\n\t\t \n\t\tput_hash_bucket(*bucket, *flags);\n\t\tindex.dev_addr -= (1 << HASH_FN_SHIFT);\n\t\t*bucket = get_hash_bucket(&index, flags);\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void hash_bucket_add(struct hash_bucket *bucket,\n\t\t\t    struct dma_debug_entry *entry)\n{\n\tlist_add_tail(&entry->list, &bucket->list);\n}\n\n \nstatic void hash_bucket_del(struct dma_debug_entry *entry)\n{\n\tlist_del(&entry->list);\n}\n\nstatic unsigned long long phys_addr(struct dma_debug_entry *entry)\n{\n\tif (entry->type == dma_debug_resource)\n\t\treturn __pfn_to_phys(entry->pfn) + entry->offset;\n\n\treturn page_to_phys(pfn_to_page(entry->pfn)) + entry->offset;\n}\n\n \nstatic RADIX_TREE(dma_active_cacheline, GFP_ATOMIC);\nstatic DEFINE_SPINLOCK(radix_lock);\n#define ACTIVE_CACHELINE_MAX_OVERLAP ((1 << RADIX_TREE_MAX_TAGS) - 1)\n#define CACHELINE_PER_PAGE_SHIFT (PAGE_SHIFT - L1_CACHE_SHIFT)\n#define CACHELINES_PER_PAGE (1 << CACHELINE_PER_PAGE_SHIFT)\n\nstatic phys_addr_t to_cacheline_number(struct dma_debug_entry *entry)\n{\n\treturn (entry->pfn << CACHELINE_PER_PAGE_SHIFT) +\n\t\t(entry->offset >> L1_CACHE_SHIFT);\n}\n\nstatic int active_cacheline_read_overlap(phys_addr_t cln)\n{\n\tint overlap = 0, i;\n\n\tfor (i = RADIX_TREE_MAX_TAGS - 1; i >= 0; i--)\n\t\tif (radix_tree_tag_get(&dma_active_cacheline, cln, i))\n\t\t\toverlap |= 1 << i;\n\treturn overlap;\n}\n\nstatic int active_cacheline_set_overlap(phys_addr_t cln, int overlap)\n{\n\tint i;\n\n\tif (overlap > ACTIVE_CACHELINE_MAX_OVERLAP || overlap < 0)\n\t\treturn overlap;\n\n\tfor (i = RADIX_TREE_MAX_TAGS - 1; i >= 0; i--)\n\t\tif (overlap & 1 << i)\n\t\t\tradix_tree_tag_set(&dma_active_cacheline, cln, i);\n\t\telse\n\t\t\tradix_tree_tag_clear(&dma_active_cacheline, cln, i);\n\n\treturn overlap;\n}\n\nstatic void active_cacheline_inc_overlap(phys_addr_t cln)\n{\n\tint overlap = active_cacheline_read_overlap(cln);\n\n\toverlap = active_cacheline_set_overlap(cln, ++overlap);\n\n\t \n\tWARN_ONCE(overlap > ACTIVE_CACHELINE_MAX_OVERLAP,\n\t\t  pr_fmt(\"exceeded %d overlapping mappings of cacheline %pa\\n\"),\n\t\t  ACTIVE_CACHELINE_MAX_OVERLAP, &cln);\n}\n\nstatic int active_cacheline_dec_overlap(phys_addr_t cln)\n{\n\tint overlap = active_cacheline_read_overlap(cln);\n\n\treturn active_cacheline_set_overlap(cln, --overlap);\n}\n\nstatic int active_cacheline_insert(struct dma_debug_entry *entry)\n{\n\tphys_addr_t cln = to_cacheline_number(entry);\n\tunsigned long flags;\n\tint rc;\n\n\t \n\tif (entry->direction == DMA_TO_DEVICE)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&radix_lock, flags);\n\trc = radix_tree_insert(&dma_active_cacheline, cln, entry);\n\tif (rc == -EEXIST)\n\t\tactive_cacheline_inc_overlap(cln);\n\tspin_unlock_irqrestore(&radix_lock, flags);\n\n\treturn rc;\n}\n\nstatic void active_cacheline_remove(struct dma_debug_entry *entry)\n{\n\tphys_addr_t cln = to_cacheline_number(entry);\n\tunsigned long flags;\n\n\t \n\tif (entry->direction == DMA_TO_DEVICE)\n\t\treturn;\n\n\tspin_lock_irqsave(&radix_lock, flags);\n\t \n\tif (active_cacheline_dec_overlap(cln) < 0)\n\t\tradix_tree_delete(&dma_active_cacheline, cln);\n\tspin_unlock_irqrestore(&radix_lock, flags);\n}\n\n \nvoid debug_dma_dump_mappings(struct device *dev)\n{\n\tint idx;\n\tphys_addr_t cln;\n\n\tfor (idx = 0; idx < HASH_SIZE; idx++) {\n\t\tstruct hash_bucket *bucket = &dma_entry_hash[idx];\n\t\tstruct dma_debug_entry *entry;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bucket->lock, flags);\n\t\tlist_for_each_entry(entry, &bucket->list, list) {\n\t\t\tif (!dev || dev == entry->dev) {\n\t\t\t\tcln = to_cacheline_number(entry);\n\t\t\t\tdev_info(entry->dev,\n\t\t\t\t\t \"%s idx %d P=%llx N=%lx D=%llx L=%llx cln=%pa %s %s\\n\",\n\t\t\t\t\t type2name[entry->type], idx,\n\t\t\t\t\t phys_addr(entry), entry->pfn,\n\t\t\t\t\t entry->dev_addr, entry->size,\n\t\t\t\t\t &cln, dir2name[entry->direction],\n\t\t\t\t\t maperr2str[entry->map_err_type]);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&bucket->lock, flags);\n\n\t\tcond_resched();\n\t}\n}\n\n \nstatic int dump_show(struct seq_file *seq, void *v)\n{\n\tint idx;\n\tphys_addr_t cln;\n\n\tfor (idx = 0; idx < HASH_SIZE; idx++) {\n\t\tstruct hash_bucket *bucket = &dma_entry_hash[idx];\n\t\tstruct dma_debug_entry *entry;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bucket->lock, flags);\n\t\tlist_for_each_entry(entry, &bucket->list, list) {\n\t\t\tcln = to_cacheline_number(entry);\n\t\t\tseq_printf(seq,\n\t\t\t\t   \"%s %s %s idx %d P=%llx N=%lx D=%llx L=%llx cln=%pa %s %s\\n\",\n\t\t\t\t   dev_driver_string(entry->dev),\n\t\t\t\t   dev_name(entry->dev),\n\t\t\t\t   type2name[entry->type], idx,\n\t\t\t\t   phys_addr(entry), entry->pfn,\n\t\t\t\t   entry->dev_addr, entry->size,\n\t\t\t\t   &cln, dir2name[entry->direction],\n\t\t\t\t   maperr2str[entry->map_err_type]);\n\t\t}\n\t\tspin_unlock_irqrestore(&bucket->lock, flags);\n\t}\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(dump);\n\n \nstatic void add_dma_entry(struct dma_debug_entry *entry, unsigned long attrs)\n{\n\tstruct hash_bucket *bucket;\n\tunsigned long flags;\n\tint rc;\n\n\tbucket = get_hash_bucket(entry, &flags);\n\thash_bucket_add(bucket, entry);\n\tput_hash_bucket(bucket, flags);\n\n\trc = active_cacheline_insert(entry);\n\tif (rc == -ENOMEM) {\n\t\tpr_err_once(\"cacheline tracking ENOMEM, dma-debug disabled\\n\");\n\t\tglobal_disable = true;\n\t} else if (rc == -EEXIST && !(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {\n\t\terr_printk(entry->dev, entry,\n\t\t\t\"cacheline tracking EEXIST, overlapping mappings aren't supported\\n\");\n\t}\n}\n\nstatic int dma_debug_create_entries(gfp_t gfp)\n{\n\tstruct dma_debug_entry *entry;\n\tint i;\n\n\tentry = (void *)get_zeroed_page(gfp);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < DMA_DEBUG_DYNAMIC_ENTRIES; i++)\n\t\tlist_add_tail(&entry[i].list, &free_entries);\n\n\tnum_free_entries += DMA_DEBUG_DYNAMIC_ENTRIES;\n\tnr_total_entries += DMA_DEBUG_DYNAMIC_ENTRIES;\n\n\treturn 0;\n}\n\nstatic struct dma_debug_entry *__dma_entry_alloc(void)\n{\n\tstruct dma_debug_entry *entry;\n\n\tentry = list_entry(free_entries.next, struct dma_debug_entry, list);\n\tlist_del(&entry->list);\n\tmemset(entry, 0, sizeof(*entry));\n\n\tnum_free_entries -= 1;\n\tif (num_free_entries < min_free_entries)\n\t\tmin_free_entries = num_free_entries;\n\n\treturn entry;\n}\n\n \nstatic void __dma_entry_alloc_check_leak(u32 nr_entries)\n{\n\tu32 tmp = nr_entries % nr_prealloc_entries;\n\n\t \n\tif (tmp < DMA_DEBUG_DYNAMIC_ENTRIES) {\n\t\tpr_info(\"dma_debug_entry pool grown to %u (%u00%%)\\n\",\n\t\t\tnr_entries,\n\t\t\t(nr_entries / nr_prealloc_entries));\n\t}\n}\n\n \nstatic struct dma_debug_entry *dma_entry_alloc(void)\n{\n\tbool alloc_check_leak = false;\n\tstruct dma_debug_entry *entry;\n\tunsigned long flags;\n\tu32 nr_entries;\n\n\tspin_lock_irqsave(&free_entries_lock, flags);\n\tif (num_free_entries == 0) {\n\t\tif (dma_debug_create_entries(GFP_ATOMIC)) {\n\t\t\tglobal_disable = true;\n\t\t\tspin_unlock_irqrestore(&free_entries_lock, flags);\n\t\t\tpr_err(\"debugging out of memory - disabling\\n\");\n\t\t\treturn NULL;\n\t\t}\n\t\talloc_check_leak = true;\n\t\tnr_entries = nr_total_entries;\n\t}\n\n\tentry = __dma_entry_alloc();\n\n\tspin_unlock_irqrestore(&free_entries_lock, flags);\n\n\tif (alloc_check_leak)\n\t\t__dma_entry_alloc_check_leak(nr_entries);\n\n#ifdef CONFIG_STACKTRACE\n\tentry->stack_len = stack_trace_save(entry->stack_entries,\n\t\t\t\t\t    ARRAY_SIZE(entry->stack_entries),\n\t\t\t\t\t    1);\n#endif\n\treturn entry;\n}\n\nstatic void dma_entry_free(struct dma_debug_entry *entry)\n{\n\tunsigned long flags;\n\n\tactive_cacheline_remove(entry);\n\n\t \n\tspin_lock_irqsave(&free_entries_lock, flags);\n\tlist_add(&entry->list, &free_entries);\n\tnum_free_entries += 1;\n\tspin_unlock_irqrestore(&free_entries_lock, flags);\n}\n\n \n\nstatic ssize_t filter_read(struct file *file, char __user *user_buf,\n\t\t\t   size_t count, loff_t *ppos)\n{\n\tchar buf[NAME_MAX_LEN + 1];\n\tunsigned long flags;\n\tint len;\n\n\tif (!current_driver_name[0])\n\t\treturn 0;\n\n\t \n\tread_lock_irqsave(&driver_name_lock, flags);\n\tlen = scnprintf(buf, NAME_MAX_LEN + 1, \"%s\\n\", current_driver_name);\n\tread_unlock_irqrestore(&driver_name_lock, flags);\n\n\treturn simple_read_from_buffer(user_buf, count, ppos, buf, len);\n}\n\nstatic ssize_t filter_write(struct file *file, const char __user *userbuf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tchar buf[NAME_MAX_LEN];\n\tunsigned long flags;\n\tsize_t len;\n\tint i;\n\n\t \n\tlen = min(count, (size_t)(NAME_MAX_LEN - 1));\n\tif (copy_from_user(buf, userbuf, len))\n\t\treturn -EFAULT;\n\n\tbuf[len] = 0;\n\n\twrite_lock_irqsave(&driver_name_lock, flags);\n\n\t \n\tif (!isalnum(buf[0])) {\n\t\t \n\t\tif (current_driver_name[0])\n\t\t\tpr_info(\"switching off dma-debug driver filter\\n\");\n\t\tcurrent_driver_name[0] = 0;\n\t\tcurrent_driver = NULL;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tfor (i = 0; i < NAME_MAX_LEN - 1; ++i) {\n\t\tcurrent_driver_name[i] = buf[i];\n\t\tif (isspace(buf[i]) || buf[i] == ' ' || buf[i] == 0)\n\t\t\tbreak;\n\t}\n\tcurrent_driver_name[i] = 0;\n\tcurrent_driver = NULL;\n\n\tpr_info(\"enable driver filter for driver [%s]\\n\",\n\t\tcurrent_driver_name);\n\nout_unlock:\n\twrite_unlock_irqrestore(&driver_name_lock, flags);\n\n\treturn count;\n}\n\nstatic const struct file_operations filter_fops = {\n\t.read  = filter_read,\n\t.write = filter_write,\n\t.llseek = default_llseek,\n};\n\nstatic int __init dma_debug_fs_init(void)\n{\n\tstruct dentry *dentry = debugfs_create_dir(\"dma-api\", NULL);\n\n\tdebugfs_create_bool(\"disabled\", 0444, dentry, &global_disable);\n\tdebugfs_create_u32(\"error_count\", 0444, dentry, &error_count);\n\tdebugfs_create_u32(\"all_errors\", 0644, dentry, &show_all_errors);\n\tdebugfs_create_u32(\"num_errors\", 0644, dentry, &show_num_errors);\n\tdebugfs_create_u32(\"num_free_entries\", 0444, dentry, &num_free_entries);\n\tdebugfs_create_u32(\"min_free_entries\", 0444, dentry, &min_free_entries);\n\tdebugfs_create_u32(\"nr_total_entries\", 0444, dentry, &nr_total_entries);\n\tdebugfs_create_file(\"driver_filter\", 0644, dentry, NULL, &filter_fops);\n\tdebugfs_create_file(\"dump\", 0444, dentry, NULL, &dump_fops);\n\n\treturn 0;\n}\ncore_initcall_sync(dma_debug_fs_init);\n\nstatic int device_dma_allocations(struct device *dev, struct dma_debug_entry **out_entry)\n{\n\tstruct dma_debug_entry *entry;\n\tunsigned long flags;\n\tint count = 0, i;\n\n\tfor (i = 0; i < HASH_SIZE; ++i) {\n\t\tspin_lock_irqsave(&dma_entry_hash[i].lock, flags);\n\t\tlist_for_each_entry(entry, &dma_entry_hash[i].list, list) {\n\t\t\tif (entry->dev == dev) {\n\t\t\t\tcount += 1;\n\t\t\t\t*out_entry = entry;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&dma_entry_hash[i].lock, flags);\n\t}\n\n\treturn count;\n}\n\nstatic int dma_debug_device_change(struct notifier_block *nb, unsigned long action, void *data)\n{\n\tstruct device *dev = data;\n\tstruct dma_debug_entry *entry;\n\tint count;\n\n\tif (dma_debug_disabled())\n\t\treturn 0;\n\n\tswitch (action) {\n\tcase BUS_NOTIFY_UNBOUND_DRIVER:\n\t\tcount = device_dma_allocations(dev, &entry);\n\t\tif (count == 0)\n\t\t\tbreak;\n\t\terr_printk(dev, entry, \"device driver has pending \"\n\t\t\t\t\"DMA allocations while released from device \"\n\t\t\t\t\"[count=%d]\\n\"\n\t\t\t\t\"One of leaked entries details: \"\n\t\t\t\t\"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t\t\"[mapped with %s] [mapped as %s]\\n\",\n\t\t\tcount, entry->dev_addr, entry->size,\n\t\t\tdir2name[entry->direction], type2name[entry->type]);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nvoid dma_debug_add_bus(struct bus_type *bus)\n{\n\tstruct notifier_block *nb;\n\n\tif (dma_debug_disabled())\n\t\treturn;\n\n\tnb = kzalloc(sizeof(struct notifier_block), GFP_KERNEL);\n\tif (nb == NULL) {\n\t\tpr_err(\"dma_debug_add_bus: out of memory\\n\");\n\t\treturn;\n\t}\n\n\tnb->notifier_call = dma_debug_device_change;\n\n\tbus_register_notifier(bus, nb);\n}\n\nstatic int dma_debug_init(void)\n{\n\tint i, nr_pages;\n\n\t \n\tif (global_disable)\n\t\treturn 0;\n\n\tfor (i = 0; i < HASH_SIZE; ++i) {\n\t\tINIT_LIST_HEAD(&dma_entry_hash[i].list);\n\t\tspin_lock_init(&dma_entry_hash[i].lock);\n\t}\n\n\tnr_pages = DIV_ROUND_UP(nr_prealloc_entries, DMA_DEBUG_DYNAMIC_ENTRIES);\n\tfor (i = 0; i < nr_pages; ++i)\n\t\tdma_debug_create_entries(GFP_KERNEL);\n\tif (num_free_entries >= nr_prealloc_entries) {\n\t\tpr_info(\"preallocated %d debug entries\\n\", nr_total_entries);\n\t} else if (num_free_entries > 0) {\n\t\tpr_warn(\"%d debug entries requested but only %d allocated\\n\",\n\t\t\tnr_prealloc_entries, nr_total_entries);\n\t} else {\n\t\tpr_err(\"debugging out of memory error - disabled\\n\");\n\t\tglobal_disable = true;\n\n\t\treturn 0;\n\t}\n\tmin_free_entries = num_free_entries;\n\n\tdma_debug_initialized = true;\n\n\tpr_info(\"debugging enabled by kernel config\\n\");\n\treturn 0;\n}\ncore_initcall(dma_debug_init);\n\nstatic __init int dma_debug_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (strncmp(str, \"off\", 3) == 0) {\n\t\tpr_info(\"debugging disabled on kernel command line\\n\");\n\t\tglobal_disable = true;\n\t}\n\n\treturn 1;\n}\n\nstatic __init int dma_debug_entries_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\tif (!get_option(&str, &nr_prealloc_entries))\n\t\tnr_prealloc_entries = PREALLOC_DMA_DEBUG_ENTRIES;\n\treturn 1;\n}\n\n__setup(\"dma_debug=\", dma_debug_cmdline);\n__setup(\"dma_debug_entries=\", dma_debug_entries_cmdline);\n\nstatic void check_unmap(struct dma_debug_entry *ref)\n{\n\tstruct dma_debug_entry *entry;\n\tstruct hash_bucket *bucket;\n\tunsigned long flags;\n\n\tbucket = get_hash_bucket(ref, &flags);\n\tentry = bucket_find_exact(bucket, ref);\n\n\tif (!entry) {\n\t\t \n\t\tput_hash_bucket(bucket, flags);\n\n\t\tif (dma_mapping_error(ref->dev, ref->dev_addr)) {\n\t\t\terr_printk(ref->dev, NULL,\n\t\t\t\t   \"device driver tries to free an \"\n\t\t\t\t   \"invalid DMA memory address\\n\");\n\t\t} else {\n\t\t\terr_printk(ref->dev, NULL,\n\t\t\t\t   \"device driver tries to free DMA \"\n\t\t\t\t   \"memory it has not allocated [device \"\n\t\t\t\t   \"address=0x%016llx] [size=%llu bytes]\\n\",\n\t\t\t\t   ref->dev_addr, ref->size);\n\t\t}\n\t\treturn;\n\t}\n\n\tif (ref->size != entry->size) {\n\t\terr_printk(ref->dev, entry, \"device driver frees \"\n\t\t\t   \"DMA memory with different size \"\n\t\t\t   \"[device address=0x%016llx] [map size=%llu bytes] \"\n\t\t\t   \"[unmap size=%llu bytes]\\n\",\n\t\t\t   ref->dev_addr, entry->size, ref->size);\n\t}\n\n\tif (ref->type != entry->type) {\n\t\terr_printk(ref->dev, entry, \"device driver frees \"\n\t\t\t   \"DMA memory with wrong function \"\n\t\t\t   \"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t   \"[mapped as %s] [unmapped as %s]\\n\",\n\t\t\t   ref->dev_addr, ref->size,\n\t\t\t   type2name[entry->type], type2name[ref->type]);\n\t} else if ((entry->type == dma_debug_coherent) &&\n\t\t   (phys_addr(ref) != phys_addr(entry))) {\n\t\terr_printk(ref->dev, entry, \"device driver frees \"\n\t\t\t   \"DMA memory with different CPU address \"\n\t\t\t   \"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t   \"[cpu alloc address=0x%016llx] \"\n\t\t\t   \"[cpu free address=0x%016llx]\",\n\t\t\t   ref->dev_addr, ref->size,\n\t\t\t   phys_addr(entry),\n\t\t\t   phys_addr(ref));\n\t}\n\n\tif (ref->sg_call_ents && ref->type == dma_debug_sg &&\n\t    ref->sg_call_ents != entry->sg_call_ents) {\n\t\terr_printk(ref->dev, entry, \"device driver frees \"\n\t\t\t   \"DMA sg list with different entry count \"\n\t\t\t   \"[map count=%d] [unmap count=%d]\\n\",\n\t\t\t   entry->sg_call_ents, ref->sg_call_ents);\n\t}\n\n\t \n\tif (ref->direction != entry->direction) {\n\t\terr_printk(ref->dev, entry, \"device driver frees \"\n\t\t\t   \"DMA memory with different direction \"\n\t\t\t   \"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t   \"[mapped with %s] [unmapped with %s]\\n\",\n\t\t\t   ref->dev_addr, ref->size,\n\t\t\t   dir2name[entry->direction],\n\t\t\t   dir2name[ref->direction]);\n\t}\n\n\t \n\tif (entry->map_err_type == MAP_ERR_NOT_CHECKED) {\n\t\terr_printk(ref->dev, entry,\n\t\t\t   \"device driver failed to check map error\"\n\t\t\t   \"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t   \"[mapped as %s]\",\n\t\t\t   ref->dev_addr, ref->size,\n\t\t\t   type2name[entry->type]);\n\t}\n\n\thash_bucket_del(entry);\n\tdma_entry_free(entry);\n\n\tput_hash_bucket(bucket, flags);\n}\n\nstatic void check_for_stack(struct device *dev,\n\t\t\t    struct page *page, size_t offset)\n{\n\tvoid *addr;\n\tstruct vm_struct *stack_vm_area = task_stack_vm_area(current);\n\n\tif (!stack_vm_area) {\n\t\t \n\t\tif (PageHighMem(page))\n\t\t\treturn;\n\t\taddr = page_address(page) + offset;\n\t\tif (object_is_on_stack(addr))\n\t\t\terr_printk(dev, NULL, \"device driver maps memory from stack [addr=%p]\\n\", addr);\n\t} else {\n\t\t \n\t\tint i;\n\n\t\tfor (i = 0; i < stack_vm_area->nr_pages; i++) {\n\t\t\tif (page != stack_vm_area->pages[i])\n\t\t\t\tcontinue;\n\n\t\t\taddr = (u8 *)current->stack + i * PAGE_SIZE + offset;\n\t\t\terr_printk(dev, NULL, \"device driver maps memory from stack [probable addr=%p]\\n\", addr);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void check_for_illegal_area(struct device *dev, void *addr, unsigned long len)\n{\n\tif (memory_intersects(_stext, _etext, addr, len) ||\n\t    memory_intersects(__start_rodata, __end_rodata, addr, len))\n\t\terr_printk(dev, NULL, \"device driver maps memory from kernel text or rodata [addr=%p] [len=%lu]\\n\", addr, len);\n}\n\nstatic void check_sync(struct device *dev,\n\t\t       struct dma_debug_entry *ref,\n\t\t       bool to_cpu)\n{\n\tstruct dma_debug_entry *entry;\n\tstruct hash_bucket *bucket;\n\tunsigned long flags;\n\n\tbucket = get_hash_bucket(ref, &flags);\n\n\tentry = bucket_find_contain(&bucket, ref, &flags);\n\n\tif (!entry) {\n\t\terr_printk(dev, NULL, \"device driver tries \"\n\t\t\t\t\"to sync DMA memory it has not allocated \"\n\t\t\t\t\"[device address=0x%016llx] [size=%llu bytes]\\n\",\n\t\t\t\t(unsigned long long)ref->dev_addr, ref->size);\n\t\tgoto out;\n\t}\n\n\tif (ref->size > entry->size) {\n\t\terr_printk(dev, entry, \"device driver syncs\"\n\t\t\t\t\" DMA memory outside allocated range \"\n\t\t\t\t\"[device address=0x%016llx] \"\n\t\t\t\t\"[allocation size=%llu bytes] \"\n\t\t\t\t\"[sync offset+size=%llu]\\n\",\n\t\t\t\tentry->dev_addr, entry->size,\n\t\t\t\tref->size);\n\t}\n\n\tif (entry->direction == DMA_BIDIRECTIONAL)\n\t\tgoto out;\n\n\tif (ref->direction != entry->direction) {\n\t\terr_printk(dev, entry, \"device driver syncs \"\n\t\t\t\t\"DMA memory with different direction \"\n\t\t\t\t\"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t\t\"[mapped with %s] [synced with %s]\\n\",\n\t\t\t\t(unsigned long long)ref->dev_addr, entry->size,\n\t\t\t\tdir2name[entry->direction],\n\t\t\t\tdir2name[ref->direction]);\n\t}\n\n\tif (to_cpu && !(entry->direction == DMA_FROM_DEVICE) &&\n\t\t      !(ref->direction == DMA_TO_DEVICE))\n\t\terr_printk(dev, entry, \"device driver syncs \"\n\t\t\t\t\"device read-only DMA memory for cpu \"\n\t\t\t\t\"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t\t\"[mapped with %s] [synced with %s]\\n\",\n\t\t\t\t(unsigned long long)ref->dev_addr, entry->size,\n\t\t\t\tdir2name[entry->direction],\n\t\t\t\tdir2name[ref->direction]);\n\n\tif (!to_cpu && !(entry->direction == DMA_TO_DEVICE) &&\n\t\t       !(ref->direction == DMA_FROM_DEVICE))\n\t\terr_printk(dev, entry, \"device driver syncs \"\n\t\t\t\t\"device write-only DMA memory to device \"\n\t\t\t\t\"[device address=0x%016llx] [size=%llu bytes] \"\n\t\t\t\t\"[mapped with %s] [synced with %s]\\n\",\n\t\t\t\t(unsigned long long)ref->dev_addr, entry->size,\n\t\t\t\tdir2name[entry->direction],\n\t\t\t\tdir2name[ref->direction]);\n\n\tif (ref->sg_call_ents && ref->type == dma_debug_sg &&\n\t    ref->sg_call_ents != entry->sg_call_ents) {\n\t\terr_printk(ref->dev, entry, \"device driver syncs \"\n\t\t\t   \"DMA sg list with different entry count \"\n\t\t\t   \"[map count=%d] [sync count=%d]\\n\",\n\t\t\t   entry->sg_call_ents, ref->sg_call_ents);\n\t}\n\nout:\n\tput_hash_bucket(bucket, flags);\n}\n\nstatic void check_sg_segment(struct device *dev, struct scatterlist *sg)\n{\n#ifdef CONFIG_DMA_API_DEBUG_SG\n\tunsigned int max_seg = dma_get_max_seg_size(dev);\n\tu64 start, end, boundary = dma_get_seg_boundary(dev);\n\n\t \n\tif (sg->length > max_seg)\n\t\terr_printk(dev, NULL, \"mapping sg segment longer than device claims to support [len=%u] [max=%u]\\n\",\n\t\t\t   sg->length, max_seg);\n\t \n\tstart = sg_dma_address(sg);\n\tend = start + sg_dma_len(sg) - 1;\n\tif ((start ^ end) & ~boundary)\n\t\terr_printk(dev, NULL, \"mapping sg segment across boundary [start=0x%016llx] [end=0x%016llx] [boundary=0x%016llx]\\n\",\n\t\t\t   start, end, boundary);\n#endif\n}\n\nvoid debug_dma_map_single(struct device *dev, const void *addr,\n\t\t\t    unsigned long len)\n{\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tif (!virt_addr_valid(addr))\n\t\terr_printk(dev, NULL, \"device driver maps memory from invalid area [addr=%p] [len=%lu]\\n\",\n\t\t\t   addr, len);\n\n\tif (is_vmalloc_addr(addr))\n\t\terr_printk(dev, NULL, \"device driver maps memory from vmalloc area [addr=%p] [len=%lu]\\n\",\n\t\t\t   addr, len);\n}\nEXPORT_SYMBOL(debug_dma_map_single);\n\nvoid debug_dma_map_page(struct device *dev, struct page *page, size_t offset,\n\t\t\tsize_t size, int direction, dma_addr_t dma_addr,\n\t\t\tunsigned long attrs)\n{\n\tstruct dma_debug_entry *entry;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tif (dma_mapping_error(dev, dma_addr))\n\t\treturn;\n\n\tentry = dma_entry_alloc();\n\tif (!entry)\n\t\treturn;\n\n\tentry->dev       = dev;\n\tentry->type      = dma_debug_single;\n\tentry->pfn\t = page_to_pfn(page);\n\tentry->offset\t = offset;\n\tentry->dev_addr  = dma_addr;\n\tentry->size      = size;\n\tentry->direction = direction;\n\tentry->map_err_type = MAP_ERR_NOT_CHECKED;\n\n\tcheck_for_stack(dev, page, offset);\n\n\tif (!PageHighMem(page)) {\n\t\tvoid *addr = page_address(page) + offset;\n\n\t\tcheck_for_illegal_area(dev, addr, size);\n\t}\n\n\tadd_dma_entry(entry, attrs);\n}\n\nvoid debug_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)\n{\n\tstruct dma_debug_entry ref;\n\tstruct dma_debug_entry *entry;\n\tstruct hash_bucket *bucket;\n\tunsigned long flags;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tref.dev = dev;\n\tref.dev_addr = dma_addr;\n\tbucket = get_hash_bucket(&ref, &flags);\n\n\tlist_for_each_entry(entry, &bucket->list, list) {\n\t\tif (!exact_match(&ref, entry))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (entry->map_err_type == MAP_ERR_NOT_CHECKED) {\n\t\t\tentry->map_err_type = MAP_ERR_CHECKED;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tput_hash_bucket(bucket, flags);\n}\nEXPORT_SYMBOL(debug_dma_mapping_error);\n\nvoid debug_dma_unmap_page(struct device *dev, dma_addr_t dma_addr,\n\t\t\t  size_t size, int direction)\n{\n\tstruct dma_debug_entry ref = {\n\t\t.type           = dma_debug_single,\n\t\t.dev            = dev,\n\t\t.dev_addr       = dma_addr,\n\t\t.size           = size,\n\t\t.direction      = direction,\n\t};\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\tcheck_unmap(&ref);\n}\n\nvoid debug_dma_map_sg(struct device *dev, struct scatterlist *sg,\n\t\t      int nents, int mapped_ents, int direction,\n\t\t      unsigned long attrs)\n{\n\tstruct dma_debug_entry *entry;\n\tstruct scatterlist *s;\n\tint i;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tcheck_for_stack(dev, sg_page(s), s->offset);\n\t\tif (!PageHighMem(sg_page(s)))\n\t\t\tcheck_for_illegal_area(dev, sg_virt(s), s->length);\n\t}\n\n\tfor_each_sg(sg, s, mapped_ents, i) {\n\t\tentry = dma_entry_alloc();\n\t\tif (!entry)\n\t\t\treturn;\n\n\t\tentry->type           = dma_debug_sg;\n\t\tentry->dev            = dev;\n\t\tentry->pfn\t      = page_to_pfn(sg_page(s));\n\t\tentry->offset\t      = s->offset;\n\t\tentry->size           = sg_dma_len(s);\n\t\tentry->dev_addr       = sg_dma_address(s);\n\t\tentry->direction      = direction;\n\t\tentry->sg_call_ents   = nents;\n\t\tentry->sg_mapped_ents = mapped_ents;\n\n\t\tcheck_sg_segment(dev, s);\n\n\t\tadd_dma_entry(entry, attrs);\n\t}\n}\n\nstatic int get_nr_mapped_entries(struct device *dev,\n\t\t\t\t struct dma_debug_entry *ref)\n{\n\tstruct dma_debug_entry *entry;\n\tstruct hash_bucket *bucket;\n\tunsigned long flags;\n\tint mapped_ents;\n\n\tbucket       = get_hash_bucket(ref, &flags);\n\tentry        = bucket_find_exact(bucket, ref);\n\tmapped_ents  = 0;\n\n\tif (entry)\n\t\tmapped_ents = entry->sg_mapped_ents;\n\tput_hash_bucket(bucket, flags);\n\n\treturn mapped_ents;\n}\n\nvoid debug_dma_unmap_sg(struct device *dev, struct scatterlist *sglist,\n\t\t\tint nelems, int dir)\n{\n\tstruct scatterlist *s;\n\tint mapped_ents = 0, i;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tfor_each_sg(sglist, s, nelems, i) {\n\n\t\tstruct dma_debug_entry ref = {\n\t\t\t.type           = dma_debug_sg,\n\t\t\t.dev            = dev,\n\t\t\t.pfn\t\t= page_to_pfn(sg_page(s)),\n\t\t\t.offset\t\t= s->offset,\n\t\t\t.dev_addr       = sg_dma_address(s),\n\t\t\t.size           = sg_dma_len(s),\n\t\t\t.direction      = dir,\n\t\t\t.sg_call_ents   = nelems,\n\t\t};\n\n\t\tif (mapped_ents && i >= mapped_ents)\n\t\t\tbreak;\n\n\t\tif (!i)\n\t\t\tmapped_ents = get_nr_mapped_entries(dev, &ref);\n\n\t\tcheck_unmap(&ref);\n\t}\n}\n\nvoid debug_dma_alloc_coherent(struct device *dev, size_t size,\n\t\t\t      dma_addr_t dma_addr, void *virt,\n\t\t\t      unsigned long attrs)\n{\n\tstruct dma_debug_entry *entry;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tif (unlikely(virt == NULL))\n\t\treturn;\n\n\t \n\tif (!is_vmalloc_addr(virt) && !virt_addr_valid(virt))\n\t\treturn;\n\n\tentry = dma_entry_alloc();\n\tif (!entry)\n\t\treturn;\n\n\tentry->type      = dma_debug_coherent;\n\tentry->dev       = dev;\n\tentry->offset\t = offset_in_page(virt);\n\tentry->size      = size;\n\tentry->dev_addr  = dma_addr;\n\tentry->direction = DMA_BIDIRECTIONAL;\n\n\tif (is_vmalloc_addr(virt))\n\t\tentry->pfn = vmalloc_to_pfn(virt);\n\telse\n\t\tentry->pfn = page_to_pfn(virt_to_page(virt));\n\n\tadd_dma_entry(entry, attrs);\n}\n\nvoid debug_dma_free_coherent(struct device *dev, size_t size,\n\t\t\t void *virt, dma_addr_t dma_addr)\n{\n\tstruct dma_debug_entry ref = {\n\t\t.type           = dma_debug_coherent,\n\t\t.dev            = dev,\n\t\t.offset\t\t= offset_in_page(virt),\n\t\t.dev_addr       = dma_addr,\n\t\t.size           = size,\n\t\t.direction      = DMA_BIDIRECTIONAL,\n\t};\n\n\t \n\tif (!is_vmalloc_addr(virt) && !virt_addr_valid(virt))\n\t\treturn;\n\n\tif (is_vmalloc_addr(virt))\n\t\tref.pfn = vmalloc_to_pfn(virt);\n\telse\n\t\tref.pfn = page_to_pfn(virt_to_page(virt));\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tcheck_unmap(&ref);\n}\n\nvoid debug_dma_map_resource(struct device *dev, phys_addr_t addr, size_t size,\n\t\t\t    int direction, dma_addr_t dma_addr,\n\t\t\t    unsigned long attrs)\n{\n\tstruct dma_debug_entry *entry;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tentry = dma_entry_alloc();\n\tif (!entry)\n\t\treturn;\n\n\tentry->type\t\t= dma_debug_resource;\n\tentry->dev\t\t= dev;\n\tentry->pfn\t\t= PHYS_PFN(addr);\n\tentry->offset\t\t= offset_in_page(addr);\n\tentry->size\t\t= size;\n\tentry->dev_addr\t\t= dma_addr;\n\tentry->direction\t= direction;\n\tentry->map_err_type\t= MAP_ERR_NOT_CHECKED;\n\n\tadd_dma_entry(entry, attrs);\n}\n\nvoid debug_dma_unmap_resource(struct device *dev, dma_addr_t dma_addr,\n\t\t\t      size_t size, int direction)\n{\n\tstruct dma_debug_entry ref = {\n\t\t.type           = dma_debug_resource,\n\t\t.dev            = dev,\n\t\t.dev_addr       = dma_addr,\n\t\t.size           = size,\n\t\t.direction      = direction,\n\t};\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tcheck_unmap(&ref);\n}\n\nvoid debug_dma_sync_single_for_cpu(struct device *dev, dma_addr_t dma_handle,\n\t\t\t\t   size_t size, int direction)\n{\n\tstruct dma_debug_entry ref;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tref.type         = dma_debug_single;\n\tref.dev          = dev;\n\tref.dev_addr     = dma_handle;\n\tref.size         = size;\n\tref.direction    = direction;\n\tref.sg_call_ents = 0;\n\n\tcheck_sync(dev, &ref, true);\n}\n\nvoid debug_dma_sync_single_for_device(struct device *dev,\n\t\t\t\t      dma_addr_t dma_handle, size_t size,\n\t\t\t\t      int direction)\n{\n\tstruct dma_debug_entry ref;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tref.type         = dma_debug_single;\n\tref.dev          = dev;\n\tref.dev_addr     = dma_handle;\n\tref.size         = size;\n\tref.direction    = direction;\n\tref.sg_call_ents = 0;\n\n\tcheck_sync(dev, &ref, false);\n}\n\nvoid debug_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t\t       int nelems, int direction)\n{\n\tstruct scatterlist *s;\n\tint mapped_ents = 0, i;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tfor_each_sg(sg, s, nelems, i) {\n\n\t\tstruct dma_debug_entry ref = {\n\t\t\t.type           = dma_debug_sg,\n\t\t\t.dev            = dev,\n\t\t\t.pfn\t\t= page_to_pfn(sg_page(s)),\n\t\t\t.offset\t\t= s->offset,\n\t\t\t.dev_addr       = sg_dma_address(s),\n\t\t\t.size           = sg_dma_len(s),\n\t\t\t.direction      = direction,\n\t\t\t.sg_call_ents   = nelems,\n\t\t};\n\n\t\tif (!i)\n\t\t\tmapped_ents = get_nr_mapped_entries(dev, &ref);\n\n\t\tif (i >= mapped_ents)\n\t\t\tbreak;\n\n\t\tcheck_sync(dev, &ref, true);\n\t}\n}\n\nvoid debug_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t\t\t  int nelems, int direction)\n{\n\tstruct scatterlist *s;\n\tint mapped_ents = 0, i;\n\n\tif (unlikely(dma_debug_disabled()))\n\t\treturn;\n\n\tfor_each_sg(sg, s, nelems, i) {\n\n\t\tstruct dma_debug_entry ref = {\n\t\t\t.type           = dma_debug_sg,\n\t\t\t.dev            = dev,\n\t\t\t.pfn\t\t= page_to_pfn(sg_page(s)),\n\t\t\t.offset\t\t= s->offset,\n\t\t\t.dev_addr       = sg_dma_address(s),\n\t\t\t.size           = sg_dma_len(s),\n\t\t\t.direction      = direction,\n\t\t\t.sg_call_ents   = nelems,\n\t\t};\n\t\tif (!i)\n\t\t\tmapped_ents = get_nr_mapped_entries(dev, &ref);\n\n\t\tif (i >= mapped_ents)\n\t\t\tbreak;\n\n\t\tcheck_sync(dev, &ref, false);\n\t}\n}\n\nstatic int __init dma_debug_driver_setup(char *str)\n{\n\tint i;\n\n\tfor (i = 0; i < NAME_MAX_LEN - 1; ++i, ++str) {\n\t\tcurrent_driver_name[i] = *str;\n\t\tif (*str == 0)\n\t\t\tbreak;\n\t}\n\n\tif (current_driver_name[0])\n\t\tpr_info(\"enable driver filter for driver [%s]\\n\",\n\t\t\tcurrent_driver_name);\n\n\n\treturn 1;\n}\n__setup(\"dma_debug_driver=\", dma_debug_driver_setup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}