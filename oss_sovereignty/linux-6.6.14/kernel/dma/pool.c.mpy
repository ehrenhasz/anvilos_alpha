{
  "module_name": "pool.c",
  "hash_id": "38261f5677a32018e43573b24d0991892564a3ff8ced882b8f130e9507b78abe",
  "original_prompt": "Ingested from linux-6.6.14/kernel/dma/pool.c",
  "human_readable_source": "\n \n#include <linux/cma.h>\n#include <linux/debugfs.h>\n#include <linux/dma-map-ops.h>\n#include <linux/dma-direct.h>\n#include <linux/init.h>\n#include <linux/genalloc.h>\n#include <linux/set_memory.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n\nstatic struct gen_pool *atomic_pool_dma __ro_after_init;\nstatic unsigned long pool_size_dma;\nstatic struct gen_pool *atomic_pool_dma32 __ro_after_init;\nstatic unsigned long pool_size_dma32;\nstatic struct gen_pool *atomic_pool_kernel __ro_after_init;\nstatic unsigned long pool_size_kernel;\n\n \nstatic size_t atomic_pool_size;\n\n \nstatic struct work_struct atomic_pool_work;\n\nstatic int __init early_coherent_pool(char *p)\n{\n\tatomic_pool_size = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"coherent_pool\", early_coherent_pool);\n\nstatic void __init dma_atomic_pool_debugfs_init(void)\n{\n\tstruct dentry *root;\n\n\troot = debugfs_create_dir(\"dma_pools\", NULL);\n\tdebugfs_create_ulong(\"pool_size_dma\", 0400, root, &pool_size_dma);\n\tdebugfs_create_ulong(\"pool_size_dma32\", 0400, root, &pool_size_dma32);\n\tdebugfs_create_ulong(\"pool_size_kernel\", 0400, root, &pool_size_kernel);\n}\n\nstatic void dma_atomic_pool_size_add(gfp_t gfp, size_t size)\n{\n\tif (gfp & __GFP_DMA)\n\t\tpool_size_dma += size;\n\telse if (gfp & __GFP_DMA32)\n\t\tpool_size_dma32 += size;\n\telse\n\t\tpool_size_kernel += size;\n}\n\nstatic bool cma_in_zone(gfp_t gfp)\n{\n\tunsigned long size;\n\tphys_addr_t end;\n\tstruct cma *cma;\n\n\tcma = dev_get_cma_area(NULL);\n\tif (!cma)\n\t\treturn false;\n\n\tsize = cma_get_size(cma);\n\tif (!size)\n\t\treturn false;\n\n\t \n\tend = cma_get_base(cma) + size - 1;\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) && (gfp & GFP_DMA))\n\t\treturn end <= DMA_BIT_MASK(zone_dma_bits);\n\tif (IS_ENABLED(CONFIG_ZONE_DMA32) && (gfp & GFP_DMA32))\n\t\treturn end <= DMA_BIT_MASK(32);\n\treturn true;\n}\n\nstatic int atomic_pool_expand(struct gen_pool *pool, size_t pool_size,\n\t\t\t      gfp_t gfp)\n{\n\tunsigned int order;\n\tstruct page *page = NULL;\n\tvoid *addr;\n\tint ret = -ENOMEM;\n\n\t \n\torder = min(get_order(pool_size), MAX_ORDER);\n\n\tdo {\n\t\tpool_size = 1 << (PAGE_SHIFT + order);\n\t\tif (cma_in_zone(gfp))\n\t\t\tpage = dma_alloc_from_contiguous(NULL, 1 << order,\n\t\t\t\t\t\t\t order, false);\n\t\tif (!page)\n\t\t\tpage = alloc_pages(gfp, order);\n\t} while (!page && order-- > 0);\n\tif (!page)\n\t\tgoto out;\n\n\tarch_dma_prep_coherent(page, pool_size);\n\n#ifdef CONFIG_DMA_DIRECT_REMAP\n\taddr = dma_common_contiguous_remap(page, pool_size,\n\t\t\t\t\t   pgprot_dmacoherent(PAGE_KERNEL),\n\t\t\t\t\t   __builtin_return_address(0));\n\tif (!addr)\n\t\tgoto free_page;\n#else\n\taddr = page_to_virt(page);\n#endif\n\t \n\tret = set_memory_decrypted((unsigned long)page_to_virt(page),\n\t\t\t\t   1 << order);\n\tif (ret)\n\t\tgoto remove_mapping;\n\tret = gen_pool_add_virt(pool, (unsigned long)addr, page_to_phys(page),\n\t\t\t\tpool_size, NUMA_NO_NODE);\n\tif (ret)\n\t\tgoto encrypt_mapping;\n\n\tdma_atomic_pool_size_add(gfp, pool_size);\n\treturn 0;\n\nencrypt_mapping:\n\tret = set_memory_encrypted((unsigned long)page_to_virt(page),\n\t\t\t\t   1 << order);\n\tif (WARN_ON_ONCE(ret)) {\n\t\t \n\t\tgoto out;\n\t}\nremove_mapping:\n#ifdef CONFIG_DMA_DIRECT_REMAP\n\tdma_common_free_remap(addr, pool_size);\nfree_page:\n\t__free_pages(page, order);\n#endif\nout:\n\treturn ret;\n}\n\nstatic void atomic_pool_resize(struct gen_pool *pool, gfp_t gfp)\n{\n\tif (pool && gen_pool_avail(pool) < atomic_pool_size)\n\t\tatomic_pool_expand(pool, gen_pool_size(pool), gfp);\n}\n\nstatic void atomic_pool_work_fn(struct work_struct *work)\n{\n\tif (IS_ENABLED(CONFIG_ZONE_DMA))\n\t\tatomic_pool_resize(atomic_pool_dma,\n\t\t\t\t   GFP_KERNEL | GFP_DMA);\n\tif (IS_ENABLED(CONFIG_ZONE_DMA32))\n\t\tatomic_pool_resize(atomic_pool_dma32,\n\t\t\t\t   GFP_KERNEL | GFP_DMA32);\n\tatomic_pool_resize(atomic_pool_kernel, GFP_KERNEL);\n}\n\nstatic __init struct gen_pool *__dma_atomic_pool_init(size_t pool_size,\n\t\t\t\t\t\t      gfp_t gfp)\n{\n\tstruct gen_pool *pool;\n\tint ret;\n\n\tpool = gen_pool_create(PAGE_SHIFT, NUMA_NO_NODE);\n\tif (!pool)\n\t\treturn NULL;\n\n\tgen_pool_set_algo(pool, gen_pool_first_fit_order_align, NULL);\n\n\tret = atomic_pool_expand(pool, pool_size, gfp);\n\tif (ret) {\n\t\tgen_pool_destroy(pool);\n\t\tpr_err(\"DMA: failed to allocate %zu KiB %pGg pool for atomic allocation\\n\",\n\t\t       pool_size >> 10, &gfp);\n\t\treturn NULL;\n\t}\n\n\tpr_info(\"DMA: preallocated %zu KiB %pGg pool for atomic allocations\\n\",\n\t\tgen_pool_size(pool) >> 10, &gfp);\n\treturn pool;\n}\n\nstatic int __init dma_atomic_pool_init(void)\n{\n\tint ret = 0;\n\n\t \n\tif (!atomic_pool_size) {\n\t\tunsigned long pages = totalram_pages() / (SZ_1G / SZ_128K);\n\t\tpages = min_t(unsigned long, pages, MAX_ORDER_NR_PAGES);\n\t\tatomic_pool_size = max_t(size_t, pages << PAGE_SHIFT, SZ_128K);\n\t}\n\tINIT_WORK(&atomic_pool_work, atomic_pool_work_fn);\n\n\tatomic_pool_kernel = __dma_atomic_pool_init(atomic_pool_size,\n\t\t\t\t\t\t    GFP_KERNEL);\n\tif (!atomic_pool_kernel)\n\t\tret = -ENOMEM;\n\tif (has_managed_dma()) {\n\t\tatomic_pool_dma = __dma_atomic_pool_init(atomic_pool_size,\n\t\t\t\t\t\tGFP_KERNEL | GFP_DMA);\n\t\tif (!atomic_pool_dma)\n\t\t\tret = -ENOMEM;\n\t}\n\tif (IS_ENABLED(CONFIG_ZONE_DMA32)) {\n\t\tatomic_pool_dma32 = __dma_atomic_pool_init(atomic_pool_size,\n\t\t\t\t\t\tGFP_KERNEL | GFP_DMA32);\n\t\tif (!atomic_pool_dma32)\n\t\t\tret = -ENOMEM;\n\t}\n\n\tdma_atomic_pool_debugfs_init();\n\treturn ret;\n}\npostcore_initcall(dma_atomic_pool_init);\n\nstatic inline struct gen_pool *dma_guess_pool(struct gen_pool *prev, gfp_t gfp)\n{\n\tif (prev == NULL) {\n\t\tif (IS_ENABLED(CONFIG_ZONE_DMA32) && (gfp & GFP_DMA32))\n\t\t\treturn atomic_pool_dma32;\n\t\tif (atomic_pool_dma && (gfp & GFP_DMA))\n\t\t\treturn atomic_pool_dma;\n\t\treturn atomic_pool_kernel;\n\t}\n\tif (prev == atomic_pool_kernel)\n\t\treturn atomic_pool_dma32 ? atomic_pool_dma32 : atomic_pool_dma;\n\tif (prev == atomic_pool_dma32)\n\t\treturn atomic_pool_dma;\n\treturn NULL;\n}\n\nstatic struct page *__dma_alloc_from_pool(struct device *dev, size_t size,\n\t\tstruct gen_pool *pool, void **cpu_addr,\n\t\tbool (*phys_addr_ok)(struct device *, phys_addr_t, size_t))\n{\n\tunsigned long addr;\n\tphys_addr_t phys;\n\n\taddr = gen_pool_alloc(pool, size);\n\tif (!addr)\n\t\treturn NULL;\n\n\tphys = gen_pool_virt_to_phys(pool, addr);\n\tif (phys_addr_ok && !phys_addr_ok(dev, phys, size)) {\n\t\tgen_pool_free(pool, addr, size);\n\t\treturn NULL;\n\t}\n\n\tif (gen_pool_avail(pool) < atomic_pool_size)\n\t\tschedule_work(&atomic_pool_work);\n\n\t*cpu_addr = (void *)addr;\n\tmemset(*cpu_addr, 0, size);\n\treturn pfn_to_page(__phys_to_pfn(phys));\n}\n\nstruct page *dma_alloc_from_pool(struct device *dev, size_t size,\n\t\tvoid **cpu_addr, gfp_t gfp,\n\t\tbool (*phys_addr_ok)(struct device *, phys_addr_t, size_t))\n{\n\tstruct gen_pool *pool = NULL;\n\tstruct page *page;\n\n\twhile ((pool = dma_guess_pool(pool, gfp))) {\n\t\tpage = __dma_alloc_from_pool(dev, size, pool, cpu_addr,\n\t\t\t\t\t     phys_addr_ok);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\n\tWARN(1, \"Failed to get suitable pool for %s\\n\", dev_name(dev));\n\treturn NULL;\n}\n\nbool dma_free_from_pool(struct device *dev, void *start, size_t size)\n{\n\tstruct gen_pool *pool = NULL;\n\n\twhile ((pool = dma_guess_pool(pool, 0))) {\n\t\tif (!gen_pool_has_addr(pool, (unsigned long)start, size))\n\t\t\tcontinue;\n\t\tgen_pool_free(pool, (unsigned long)start, size);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}