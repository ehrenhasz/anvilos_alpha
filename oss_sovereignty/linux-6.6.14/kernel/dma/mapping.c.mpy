{
  "module_name": "mapping.c",
  "hash_id": "c70928cf5a57fcec1ca1dd0769605eeb10d694fb1587a8850503844975de6dae",
  "original_prompt": "Ingested from linux-6.6.14/kernel/dma/mapping.c",
  "human_readable_source": "\n \n#include <linux/memblock.h>  \n#include <linux/acpi.h>\n#include <linux/dma-map-ops.h>\n#include <linux/export.h>\n#include <linux/gfp.h>\n#include <linux/kmsan.h>\n#include <linux/of_device.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include \"debug.h\"\n#include \"direct.h\"\n\n#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \\\n\tdefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \\\n\tdefined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)\nbool dma_default_coherent = IS_ENABLED(CONFIG_ARCH_DMA_DEFAULT_COHERENT);\n#endif\n\n \nstruct dma_devres {\n\tsize_t\t\tsize;\n\tvoid\t\t*vaddr;\n\tdma_addr_t\tdma_handle;\n\tunsigned long\tattrs;\n};\n\nstatic void dmam_release(struct device *dev, void *res)\n{\n\tstruct dma_devres *this = res;\n\n\tdma_free_attrs(dev, this->size, this->vaddr, this->dma_handle,\n\t\t\tthis->attrs);\n}\n\nstatic int dmam_match(struct device *dev, void *res, void *match_data)\n{\n\tstruct dma_devres *this = res, *match = match_data;\n\n\tif (this->vaddr == match->vaddr) {\n\t\tWARN_ON(this->size != match->size ||\n\t\t\tthis->dma_handle != match->dma_handle);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nvoid dmam_free_coherent(struct device *dev, size_t size, void *vaddr,\n\t\t\tdma_addr_t dma_handle)\n{\n\tstruct dma_devres match_data = { size, vaddr, dma_handle };\n\n\tdma_free_coherent(dev, size, vaddr, dma_handle);\n\tWARN_ON(devres_destroy(dev, dmam_release, dmam_match, &match_data));\n}\nEXPORT_SYMBOL(dmam_free_coherent);\n\n \nvoid *dmam_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,\n\t\tgfp_t gfp, unsigned long attrs)\n{\n\tstruct dma_devres *dr;\n\tvoid *vaddr;\n\n\tdr = devres_alloc(dmam_release, sizeof(*dr), gfp);\n\tif (!dr)\n\t\treturn NULL;\n\n\tvaddr = dma_alloc_attrs(dev, size, dma_handle, gfp, attrs);\n\tif (!vaddr) {\n\t\tdevres_free(dr);\n\t\treturn NULL;\n\t}\n\n\tdr->vaddr = vaddr;\n\tdr->dma_handle = *dma_handle;\n\tdr->size = size;\n\tdr->attrs = attrs;\n\n\tdevres_add(dev, dr);\n\n\treturn vaddr;\n}\nEXPORT_SYMBOL(dmam_alloc_attrs);\n\nstatic bool dma_go_direct(struct device *dev, dma_addr_t mask,\n\t\tconst struct dma_map_ops *ops)\n{\n\tif (likely(!ops))\n\t\treturn true;\n#ifdef CONFIG_DMA_OPS_BYPASS\n\tif (dev->dma_ops_bypass)\n\t\treturn min_not_zero(mask, dev->bus_dma_limit) >=\n\t\t\t    dma_direct_get_required_mask(dev);\n#endif\n\treturn false;\n}\n\n\n \nstatic inline bool dma_alloc_direct(struct device *dev,\n\t\tconst struct dma_map_ops *ops)\n{\n\treturn dma_go_direct(dev, dev->coherent_dma_mask, ops);\n}\n\nstatic inline bool dma_map_direct(struct device *dev,\n\t\tconst struct dma_map_ops *ops)\n{\n\treturn dma_go_direct(dev, *dev->dma_mask, ops);\n}\n\ndma_addr_t dma_map_page_attrs(struct device *dev, struct page *page,\n\t\tsize_t offset, size_t size, enum dma_data_direction dir,\n\t\tunsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tdma_addr_t addr;\n\n\tBUG_ON(!valid_dma_direction(dir));\n\n\tif (WARN_ON_ONCE(!dev->dma_mask))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tif (dma_map_direct(dev, ops) ||\n\t    arch_dma_map_page_direct(dev, page_to_phys(page) + offset + size))\n\t\taddr = dma_direct_map_page(dev, page, offset, size, dir, attrs);\n\telse\n\t\taddr = ops->map_page(dev, page, offset, size, dir, attrs);\n\tkmsan_handle_dma(page, offset, size, dir);\n\tdebug_dma_map_page(dev, page, offset, size, dir, addr, attrs);\n\n\treturn addr;\n}\nEXPORT_SYMBOL(dma_map_page_attrs);\n\nvoid dma_unmap_page_attrs(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tif (dma_map_direct(dev, ops) ||\n\t    arch_dma_unmap_page_direct(dev, addr + size))\n\t\tdma_direct_unmap_page(dev, addr, size, dir, attrs);\n\telse if (ops->unmap_page)\n\t\tops->unmap_page(dev, addr, size, dir, attrs);\n\tdebug_dma_unmap_page(dev, addr, size, dir);\n}\nEXPORT_SYMBOL(dma_unmap_page_attrs);\n\nstatic int __dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,\n\t int nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tint ents;\n\n\tBUG_ON(!valid_dma_direction(dir));\n\n\tif (WARN_ON_ONCE(!dev->dma_mask))\n\t\treturn 0;\n\n\tif (dma_map_direct(dev, ops) ||\n\t    arch_dma_map_sg_direct(dev, sg, nents))\n\t\tents = dma_direct_map_sg(dev, sg, nents, dir, attrs);\n\telse\n\t\tents = ops->map_sg(dev, sg, nents, dir, attrs);\n\n\tif (ents > 0) {\n\t\tkmsan_handle_dma_sg(sg, nents, dir);\n\t\tdebug_dma_map_sg(dev, sg, nents, ents, dir, attrs);\n\t} else if (WARN_ON_ONCE(ents != -EINVAL && ents != -ENOMEM &&\n\t\t\t\tents != -EIO && ents != -EREMOTEIO)) {\n\t\treturn -EIO;\n\t}\n\n\treturn ents;\n}\n\n \nunsigned int dma_map_sg_attrs(struct device *dev, struct scatterlist *sg,\n\t\t    int nents, enum dma_data_direction dir, unsigned long attrs)\n{\n\tint ret;\n\n\tret = __dma_map_sg_attrs(dev, sg, nents, dir, attrs);\n\tif (ret < 0)\n\t\treturn 0;\n\treturn ret;\n}\nEXPORT_SYMBOL(dma_map_sg_attrs);\n\n \nint dma_map_sgtable(struct device *dev, struct sg_table *sgt,\n\t\t    enum dma_data_direction dir, unsigned long attrs)\n{\n\tint nents;\n\n\tnents = __dma_map_sg_attrs(dev, sgt->sgl, sgt->orig_nents, dir, attrs);\n\tif (nents < 0)\n\t\treturn nents;\n\tsgt->nents = nents;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dma_map_sgtable);\n\nvoid dma_unmap_sg_attrs(struct device *dev, struct scatterlist *sg,\n\t\t\t\t      int nents, enum dma_data_direction dir,\n\t\t\t\t      unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tdebug_dma_unmap_sg(dev, sg, nents, dir);\n\tif (dma_map_direct(dev, ops) ||\n\t    arch_dma_unmap_sg_direct(dev, sg, nents))\n\t\tdma_direct_unmap_sg(dev, sg, nents, dir, attrs);\n\telse if (ops->unmap_sg)\n\t\tops->unmap_sg(dev, sg, nents, dir, attrs);\n}\nEXPORT_SYMBOL(dma_unmap_sg_attrs);\n\ndma_addr_t dma_map_resource(struct device *dev, phys_addr_t phys_addr,\n\t\tsize_t size, enum dma_data_direction dir, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tdma_addr_t addr = DMA_MAPPING_ERROR;\n\n\tBUG_ON(!valid_dma_direction(dir));\n\n\tif (WARN_ON_ONCE(!dev->dma_mask))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tif (dma_map_direct(dev, ops))\n\t\taddr = dma_direct_map_resource(dev, phys_addr, size, dir, attrs);\n\telse if (ops->map_resource)\n\t\taddr = ops->map_resource(dev, phys_addr, size, dir, attrs);\n\n\tdebug_dma_map_resource(dev, phys_addr, size, dir, addr, attrs);\n\treturn addr;\n}\nEXPORT_SYMBOL(dma_map_resource);\n\nvoid dma_unmap_resource(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tif (!dma_map_direct(dev, ops) && ops->unmap_resource)\n\t\tops->unmap_resource(dev, addr, size, dir, attrs);\n\tdebug_dma_unmap_resource(dev, addr, size, dir);\n}\nEXPORT_SYMBOL(dma_unmap_resource);\n\nvoid dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tif (dma_map_direct(dev, ops))\n\t\tdma_direct_sync_single_for_cpu(dev, addr, size, dir);\n\telse if (ops->sync_single_for_cpu)\n\t\tops->sync_single_for_cpu(dev, addr, size, dir);\n\tdebug_dma_sync_single_for_cpu(dev, addr, size, dir);\n}\nEXPORT_SYMBOL(dma_sync_single_for_cpu);\n\nvoid dma_sync_single_for_device(struct device *dev, dma_addr_t addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tif (dma_map_direct(dev, ops))\n\t\tdma_direct_sync_single_for_device(dev, addr, size, dir);\n\telse if (ops->sync_single_for_device)\n\t\tops->sync_single_for_device(dev, addr, size, dir);\n\tdebug_dma_sync_single_for_device(dev, addr, size, dir);\n}\nEXPORT_SYMBOL(dma_sync_single_for_device);\n\nvoid dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\n\t\t    int nelems, enum dma_data_direction dir)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tif (dma_map_direct(dev, ops))\n\t\tdma_direct_sync_sg_for_cpu(dev, sg, nelems, dir);\n\telse if (ops->sync_sg_for_cpu)\n\t\tops->sync_sg_for_cpu(dev, sg, nelems, dir);\n\tdebug_dma_sync_sg_for_cpu(dev, sg, nelems, dir);\n}\nEXPORT_SYMBOL(dma_sync_sg_for_cpu);\n\nvoid dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\n\t\t       int nelems, enum dma_data_direction dir)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tBUG_ON(!valid_dma_direction(dir));\n\tif (dma_map_direct(dev, ops))\n\t\tdma_direct_sync_sg_for_device(dev, sg, nelems, dir);\n\telse if (ops->sync_sg_for_device)\n\t\tops->sync_sg_for_device(dev, sg, nelems, dir);\n\tdebug_dma_sync_sg_for_device(dev, sg, nelems, dir);\n}\nEXPORT_SYMBOL(dma_sync_sg_for_device);\n\n \nint dma_get_sgtable_attrs(struct device *dev, struct sg_table *sgt,\n\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\tunsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (dma_alloc_direct(dev, ops))\n\t\treturn dma_direct_get_sgtable(dev, sgt, cpu_addr, dma_addr,\n\t\t\t\tsize, attrs);\n\tif (!ops->get_sgtable)\n\t\treturn -ENXIO;\n\treturn ops->get_sgtable(dev, sgt, cpu_addr, dma_addr, size, attrs);\n}\nEXPORT_SYMBOL(dma_get_sgtable_attrs);\n\n#ifdef CONFIG_MMU\n \npgprot_t dma_pgprot(struct device *dev, pgprot_t prot, unsigned long attrs)\n{\n\tif (dev_is_dma_coherent(dev))\n\t\treturn prot;\n#ifdef CONFIG_ARCH_HAS_DMA_WRITE_COMBINE\n\tif (attrs & DMA_ATTR_WRITE_COMBINE)\n\t\treturn pgprot_writecombine(prot);\n#endif\n\treturn pgprot_dmacoherent(prot);\n}\n#endif  \n\n \nbool dma_can_mmap(struct device *dev)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (dma_alloc_direct(dev, ops))\n\t\treturn dma_direct_can_mmap(dev);\n\treturn ops->mmap != NULL;\n}\nEXPORT_SYMBOL_GPL(dma_can_mmap);\n\n \nint dma_mmap_attrs(struct device *dev, struct vm_area_struct *vma,\n\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size,\n\t\tunsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (dma_alloc_direct(dev, ops))\n\t\treturn dma_direct_mmap(dev, vma, cpu_addr, dma_addr, size,\n\t\t\t\tattrs);\n\tif (!ops->mmap)\n\t\treturn -ENXIO;\n\treturn ops->mmap(dev, vma, cpu_addr, dma_addr, size, attrs);\n}\nEXPORT_SYMBOL(dma_mmap_attrs);\n\nu64 dma_get_required_mask(struct device *dev)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (dma_alloc_direct(dev, ops))\n\t\treturn dma_direct_get_required_mask(dev);\n\tif (ops->get_required_mask)\n\t\treturn ops->get_required_mask(dev);\n\n\t \n\treturn DMA_BIT_MASK(32);\n}\nEXPORT_SYMBOL_GPL(dma_get_required_mask);\n\nvoid *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,\n\t\tgfp_t flag, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tvoid *cpu_addr;\n\n\tWARN_ON_ONCE(!dev->coherent_dma_mask);\n\n\t \n\tif (WARN_ON_ONCE(flag & __GFP_COMP))\n\t\treturn NULL;\n\n\tif (dma_alloc_from_dev_coherent(dev, size, dma_handle, &cpu_addr))\n\t\treturn cpu_addr;\n\n\t \n\tflag &= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);\n\n\tif (dma_alloc_direct(dev, ops))\n\t\tcpu_addr = dma_direct_alloc(dev, size, dma_handle, flag, attrs);\n\telse if (ops->alloc)\n\t\tcpu_addr = ops->alloc(dev, size, dma_handle, flag, attrs);\n\telse\n\t\treturn NULL;\n\n\tdebug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr, attrs);\n\treturn cpu_addr;\n}\nEXPORT_SYMBOL(dma_alloc_attrs);\n\nvoid dma_free_attrs(struct device *dev, size_t size, void *cpu_addr,\n\t\tdma_addr_t dma_handle, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))\n\t\treturn;\n\t \n\tWARN_ON(irqs_disabled());\n\n\tif (!cpu_addr)\n\t\treturn;\n\n\tdebug_dma_free_coherent(dev, size, cpu_addr, dma_handle);\n\tif (dma_alloc_direct(dev, ops))\n\t\tdma_direct_free(dev, size, cpu_addr, dma_handle, attrs);\n\telse if (ops->free)\n\t\tops->free(dev, size, cpu_addr, dma_handle, attrs);\n}\nEXPORT_SYMBOL(dma_free_attrs);\n\nstatic struct page *__dma_alloc_pages(struct device *dev, size_t size,\n\t\tdma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (WARN_ON_ONCE(!dev->coherent_dma_mask))\n\t\treturn NULL;\n\tif (WARN_ON_ONCE(gfp & (__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM)))\n\t\treturn NULL;\n\tif (WARN_ON_ONCE(gfp & __GFP_COMP))\n\t\treturn NULL;\n\n\tsize = PAGE_ALIGN(size);\n\tif (dma_alloc_direct(dev, ops))\n\t\treturn dma_direct_alloc_pages(dev, size, dma_handle, dir, gfp);\n\tif (!ops->alloc_pages)\n\t\treturn NULL;\n\treturn ops->alloc_pages(dev, size, dma_handle, dir, gfp);\n}\n\nstruct page *dma_alloc_pages(struct device *dev, size_t size,\n\t\tdma_addr_t *dma_handle, enum dma_data_direction dir, gfp_t gfp)\n{\n\tstruct page *page = __dma_alloc_pages(dev, size, dma_handle, dir, gfp);\n\n\tif (page)\n\t\tdebug_dma_map_page(dev, page, 0, size, dir, *dma_handle, 0);\n\treturn page;\n}\nEXPORT_SYMBOL_GPL(dma_alloc_pages);\n\nstatic void __dma_free_pages(struct device *dev, size_t size, struct page *page,\n\t\tdma_addr_t dma_handle, enum dma_data_direction dir)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tsize = PAGE_ALIGN(size);\n\tif (dma_alloc_direct(dev, ops))\n\t\tdma_direct_free_pages(dev, size, page, dma_handle, dir);\n\telse if (ops->free_pages)\n\t\tops->free_pages(dev, size, page, dma_handle, dir);\n}\n\nvoid dma_free_pages(struct device *dev, size_t size, struct page *page,\n\t\tdma_addr_t dma_handle, enum dma_data_direction dir)\n{\n\tdebug_dma_unmap_page(dev, dma_handle, size, dir);\n\t__dma_free_pages(dev, size, page, dma_handle, dir);\n}\nEXPORT_SYMBOL_GPL(dma_free_pages);\n\nint dma_mmap_pages(struct device *dev, struct vm_area_struct *vma,\n\t\tsize_t size, struct page *page)\n{\n\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\n\tif (vma->vm_pgoff >= count || vma_pages(vma) > count - vma->vm_pgoff)\n\t\treturn -ENXIO;\n\treturn remap_pfn_range(vma, vma->vm_start,\n\t\t\t       page_to_pfn(page) + vma->vm_pgoff,\n\t\t\t       vma_pages(vma) << PAGE_SHIFT, vma->vm_page_prot);\n}\nEXPORT_SYMBOL_GPL(dma_mmap_pages);\n\nstatic struct sg_table *alloc_single_sgt(struct device *dev, size_t size,\n\t\tenum dma_data_direction dir, gfp_t gfp)\n{\n\tstruct sg_table *sgt;\n\tstruct page *page;\n\n\tsgt = kmalloc(sizeof(*sgt), gfp);\n\tif (!sgt)\n\t\treturn NULL;\n\tif (sg_alloc_table(sgt, 1, gfp))\n\t\tgoto out_free_sgt;\n\tpage = __dma_alloc_pages(dev, size, &sgt->sgl->dma_address, dir, gfp);\n\tif (!page)\n\t\tgoto out_free_table;\n\tsg_set_page(sgt->sgl, page, PAGE_ALIGN(size), 0);\n\tsg_dma_len(sgt->sgl) = sgt->sgl->length;\n\treturn sgt;\nout_free_table:\n\tsg_free_table(sgt);\nout_free_sgt:\n\tkfree(sgt);\n\treturn NULL;\n}\n\nstruct sg_table *dma_alloc_noncontiguous(struct device *dev, size_t size,\n\t\tenum dma_data_direction dir, gfp_t gfp, unsigned long attrs)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tstruct sg_table *sgt;\n\n\tif (WARN_ON_ONCE(attrs & ~DMA_ATTR_ALLOC_SINGLE_PAGES))\n\t\treturn NULL;\n\tif (WARN_ON_ONCE(gfp & __GFP_COMP))\n\t\treturn NULL;\n\n\tif (ops && ops->alloc_noncontiguous)\n\t\tsgt = ops->alloc_noncontiguous(dev, size, dir, gfp, attrs);\n\telse\n\t\tsgt = alloc_single_sgt(dev, size, dir, gfp);\n\n\tif (sgt) {\n\t\tsgt->nents = 1;\n\t\tdebug_dma_map_sg(dev, sgt->sgl, sgt->orig_nents, 1, dir, attrs);\n\t}\n\treturn sgt;\n}\nEXPORT_SYMBOL_GPL(dma_alloc_noncontiguous);\n\nstatic void free_single_sgt(struct device *dev, size_t size,\n\t\tstruct sg_table *sgt, enum dma_data_direction dir)\n{\n\t__dma_free_pages(dev, size, sg_page(sgt->sgl), sgt->sgl->dma_address,\n\t\t\t dir);\n\tsg_free_table(sgt);\n\tkfree(sgt);\n}\n\nvoid dma_free_noncontiguous(struct device *dev, size_t size,\n\t\tstruct sg_table *sgt, enum dma_data_direction dir)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tdebug_dma_unmap_sg(dev, sgt->sgl, sgt->orig_nents, dir);\n\tif (ops && ops->free_noncontiguous)\n\t\tops->free_noncontiguous(dev, size, sgt, dir);\n\telse\n\t\tfree_single_sgt(dev, size, sgt, dir);\n}\nEXPORT_SYMBOL_GPL(dma_free_noncontiguous);\n\nvoid *dma_vmap_noncontiguous(struct device *dev, size_t size,\n\t\tstruct sg_table *sgt)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\n\tif (ops && ops->alloc_noncontiguous)\n\t\treturn vmap(sgt_handle(sgt)->pages, count, VM_MAP, PAGE_KERNEL);\n\treturn page_address(sg_page(sgt->sgl));\n}\nEXPORT_SYMBOL_GPL(dma_vmap_noncontiguous);\n\nvoid dma_vunmap_noncontiguous(struct device *dev, void *vaddr)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (ops && ops->alloc_noncontiguous)\n\t\tvunmap(vaddr);\n}\nEXPORT_SYMBOL_GPL(dma_vunmap_noncontiguous);\n\nint dma_mmap_noncontiguous(struct device *dev, struct vm_area_struct *vma,\n\t\tsize_t size, struct sg_table *sgt)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (ops && ops->alloc_noncontiguous) {\n\t\tunsigned long count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\n\t\tif (vma->vm_pgoff >= count ||\n\t\t    vma_pages(vma) > count - vma->vm_pgoff)\n\t\t\treturn -ENXIO;\n\t\treturn vm_map_pages(vma, sgt_handle(sgt)->pages, count);\n\t}\n\treturn dma_mmap_pages(dev, vma, size, sg_page(sgt->sgl));\n}\nEXPORT_SYMBOL_GPL(dma_mmap_noncontiguous);\n\nstatic int dma_supported(struct device *dev, u64 mask)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\t \n\tif (!ops)\n\t\treturn dma_direct_supported(dev, mask);\n\tif (!ops->dma_supported)\n\t\treturn 1;\n\treturn ops->dma_supported(dev, mask);\n}\n\nbool dma_pci_p2pdma_supported(struct device *dev)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\t \n\tif (!ops)\n\t\treturn true;\n\n\t \n\n\treturn ops->flags & DMA_F_PCI_P2PDMA_SUPPORTED;\n}\nEXPORT_SYMBOL_GPL(dma_pci_p2pdma_supported);\n\nint dma_set_mask(struct device *dev, u64 mask)\n{\n\t \n\tmask = (dma_addr_t)mask;\n\n\tif (!dev->dma_mask || !dma_supported(dev, mask))\n\t\treturn -EIO;\n\n\tarch_dma_set_mask(dev, mask);\n\t*dev->dma_mask = mask;\n\treturn 0;\n}\nEXPORT_SYMBOL(dma_set_mask);\n\nint dma_set_coherent_mask(struct device *dev, u64 mask)\n{\n\t \n\tmask = (dma_addr_t)mask;\n\n\tif (!dma_supported(dev, mask))\n\t\treturn -EIO;\n\n\tdev->coherent_dma_mask = mask;\n\treturn 0;\n}\nEXPORT_SYMBOL(dma_set_coherent_mask);\n\nsize_t dma_max_mapping_size(struct device *dev)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tsize_t size = SIZE_MAX;\n\n\tif (dma_map_direct(dev, ops))\n\t\tsize = dma_direct_max_mapping_size(dev);\n\telse if (ops && ops->max_mapping_size)\n\t\tsize = ops->max_mapping_size(dev);\n\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(dma_max_mapping_size);\n\nsize_t dma_opt_mapping_size(struct device *dev)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\tsize_t size = SIZE_MAX;\n\n\tif (ops && ops->opt_mapping_size)\n\t\tsize = ops->opt_mapping_size();\n\n\treturn min(dma_max_mapping_size(dev), size);\n}\nEXPORT_SYMBOL_GPL(dma_opt_mapping_size);\n\nbool dma_need_sync(struct device *dev, dma_addr_t dma_addr)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (dma_map_direct(dev, ops))\n\t\treturn dma_direct_need_sync(dev, dma_addr);\n\treturn ops->sync_single_for_cpu || ops->sync_single_for_device;\n}\nEXPORT_SYMBOL_GPL(dma_need_sync);\n\nunsigned long dma_get_merge_boundary(struct device *dev)\n{\n\tconst struct dma_map_ops *ops = get_dma_ops(dev);\n\n\tif (!ops || !ops->get_merge_boundary)\n\t\treturn 0;\t \n\n\treturn ops->get_merge_boundary(dev);\n}\nEXPORT_SYMBOL_GPL(dma_get_merge_boundary);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}