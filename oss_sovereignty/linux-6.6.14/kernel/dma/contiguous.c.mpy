{
  "module_name": "contiguous.c",
  "hash_id": "900136d5410f18f3237cf796a6396574b17e73bcac44e500a2d0df8423f8741d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/dma/contiguous.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"cma: \" fmt\n\n#ifdef CONFIG_CMA_DEBUG\n#ifndef DEBUG\n#  define DEBUG\n#endif\n#endif\n\n#include <asm/page.h>\n\n#include <linux/memblock.h>\n#include <linux/err.h>\n#include <linux/sizes.h>\n#include <linux/dma-map-ops.h>\n#include <linux/cma.h>\n#include <linux/nospec.h>\n\n#ifdef CONFIG_CMA_SIZE_MBYTES\n#define CMA_SIZE_MBYTES CONFIG_CMA_SIZE_MBYTES\n#else\n#define CMA_SIZE_MBYTES 0\n#endif\n\nstruct cma *dma_contiguous_default_area;\n\n \nstatic const phys_addr_t size_bytes __initconst =\n\t(phys_addr_t)CMA_SIZE_MBYTES * SZ_1M;\nstatic phys_addr_t  size_cmdline __initdata = -1;\nstatic phys_addr_t base_cmdline __initdata;\nstatic phys_addr_t limit_cmdline __initdata;\n\nstatic int __init early_cma(char *p)\n{\n\tif (!p) {\n\t\tpr_err(\"Config string not provided\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tsize_cmdline = memparse(p, &p);\n\tif (*p != '@')\n\t\treturn 0;\n\tbase_cmdline = memparse(p + 1, &p);\n\tif (*p != '-') {\n\t\tlimit_cmdline = base_cmdline + size_cmdline;\n\t\treturn 0;\n\t}\n\tlimit_cmdline = memparse(p + 1, &p);\n\n\treturn 0;\n}\nearly_param(\"cma\", early_cma);\n\n#ifdef CONFIG_DMA_NUMA_CMA\n\nstatic struct cma *dma_contiguous_numa_area[MAX_NUMNODES];\nstatic phys_addr_t numa_cma_size[MAX_NUMNODES] __initdata;\nstatic struct cma *dma_contiguous_pernuma_area[MAX_NUMNODES];\nstatic phys_addr_t pernuma_size_bytes __initdata;\n\nstatic int __init early_numa_cma(char *p)\n{\n\tint nid, count = 0;\n\tunsigned long tmp;\n\tchar *s = p;\n\n\twhile (*s) {\n\t\tif (sscanf(s, \"%lu%n\", &tmp, &count) != 1)\n\t\t\tbreak;\n\n\t\tif (s[count] == ':') {\n\t\t\tif (tmp >= MAX_NUMNODES)\n\t\t\t\tbreak;\n\t\t\tnid = array_index_nospec(tmp, MAX_NUMNODES);\n\n\t\t\ts += count + 1;\n\t\t\ttmp = memparse(s, &s);\n\t\t\tnuma_cma_size[nid] = tmp;\n\n\t\t\tif (*s == ',')\n\t\t\t\ts++;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t} else\n\t\t\tbreak;\n\t}\n\n\treturn 0;\n}\nearly_param(\"numa_cma\", early_numa_cma);\n\nstatic int __init early_cma_pernuma(char *p)\n{\n\tpernuma_size_bytes = memparse(p, &p);\n\treturn 0;\n}\nearly_param(\"cma_pernuma\", early_cma_pernuma);\n#endif\n\n#ifdef CONFIG_CMA_SIZE_PERCENTAGE\n\nstatic phys_addr_t __init __maybe_unused cma_early_percent_memory(void)\n{\n\tunsigned long total_pages = PHYS_PFN(memblock_phys_mem_size());\n\n\treturn (total_pages * CONFIG_CMA_SIZE_PERCENTAGE / 100) << PAGE_SHIFT;\n}\n\n#else\n\nstatic inline __maybe_unused phys_addr_t cma_early_percent_memory(void)\n{\n\treturn 0;\n}\n\n#endif\n\n#ifdef CONFIG_DMA_NUMA_CMA\nstatic void __init dma_numa_cma_reserve(void)\n{\n\tint nid;\n\n\tfor_each_node(nid) {\n\t\tint ret;\n\t\tchar name[CMA_MAX_NAME];\n\t\tstruct cma **cma;\n\n\t\tif (!node_online(nid)) {\n\t\t\tif (pernuma_size_bytes || numa_cma_size[nid])\n\t\t\t\tpr_warn(\"invalid node %d specified\\n\", nid);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (pernuma_size_bytes) {\n\n\t\t\tcma = &dma_contiguous_pernuma_area[nid];\n\t\t\tsnprintf(name, sizeof(name), \"pernuma%d\", nid);\n\t\t\tret = cma_declare_contiguous_nid(0, pernuma_size_bytes, 0, 0,\n\t\t\t\t\t\t\t 0, false, name, cma, nid);\n\t\t\tif (ret)\n\t\t\t\tpr_warn(\"%s: reservation failed: err %d, node %d\", __func__,\n\t\t\t\t\tret, nid);\n\t\t}\n\n\t\tif (numa_cma_size[nid]) {\n\n\t\t\tcma = &dma_contiguous_numa_area[nid];\n\t\t\tsnprintf(name, sizeof(name), \"numa%d\", nid);\n\t\t\tret = cma_declare_contiguous_nid(0, numa_cma_size[nid], 0, 0, 0, false,\n\t\t\t\t\t\t\t name, cma, nid);\n\t\t\tif (ret)\n\t\t\t\tpr_warn(\"%s: reservation failed: err %d, node %d\", __func__,\n\t\t\t\t\tret, nid);\n\t\t}\n\t}\n}\n#else\nstatic inline void __init dma_numa_cma_reserve(void)\n{\n}\n#endif\n\n \nvoid __init dma_contiguous_reserve(phys_addr_t limit)\n{\n\tphys_addr_t selected_size = 0;\n\tphys_addr_t selected_base = 0;\n\tphys_addr_t selected_limit = limit;\n\tbool fixed = false;\n\n\tdma_numa_cma_reserve();\n\n\tpr_debug(\"%s(limit %08lx)\\n\", __func__, (unsigned long)limit);\n\n\tif (size_cmdline != -1) {\n\t\tselected_size = size_cmdline;\n\t\tselected_base = base_cmdline;\n\t\tselected_limit = min_not_zero(limit_cmdline, limit);\n\t\tif (base_cmdline + size_cmdline == limit_cmdline)\n\t\t\tfixed = true;\n\t} else {\n#ifdef CONFIG_CMA_SIZE_SEL_MBYTES\n\t\tselected_size = size_bytes;\n#elif defined(CONFIG_CMA_SIZE_SEL_PERCENTAGE)\n\t\tselected_size = cma_early_percent_memory();\n#elif defined(CONFIG_CMA_SIZE_SEL_MIN)\n\t\tselected_size = min(size_bytes, cma_early_percent_memory());\n#elif defined(CONFIG_CMA_SIZE_SEL_MAX)\n\t\tselected_size = max(size_bytes, cma_early_percent_memory());\n#endif\n\t}\n\n\tif (selected_size && !dma_contiguous_default_area) {\n\t\tpr_debug(\"%s: reserving %ld MiB for global area\\n\", __func__,\n\t\t\t (unsigned long)selected_size / SZ_1M);\n\n\t\tdma_contiguous_reserve_area(selected_size, selected_base,\n\t\t\t\t\t    selected_limit,\n\t\t\t\t\t    &dma_contiguous_default_area,\n\t\t\t\t\t    fixed);\n\t}\n}\n\nvoid __weak\ndma_contiguous_early_fixup(phys_addr_t base, unsigned long size)\n{\n}\n\n \nint __init dma_contiguous_reserve_area(phys_addr_t size, phys_addr_t base,\n\t\t\t\t       phys_addr_t limit, struct cma **res_cma,\n\t\t\t\t       bool fixed)\n{\n\tint ret;\n\n\tret = cma_declare_contiguous(base, size, limit, 0, 0, fixed,\n\t\t\t\t\t\"reserved\", res_cma);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tdma_contiguous_early_fixup(cma_get_base(*res_cma),\n\t\t\t\tcma_get_size(*res_cma));\n\n\treturn 0;\n}\n\n \nstruct page *dma_alloc_from_contiguous(struct device *dev, size_t count,\n\t\t\t\t       unsigned int align, bool no_warn)\n{\n\tif (align > CONFIG_CMA_ALIGNMENT)\n\t\talign = CONFIG_CMA_ALIGNMENT;\n\n\treturn cma_alloc(dev_get_cma_area(dev), count, align, no_warn);\n}\n\n \nbool dma_release_from_contiguous(struct device *dev, struct page *pages,\n\t\t\t\t int count)\n{\n\treturn cma_release(dev_get_cma_area(dev), pages, count);\n}\n\nstatic struct page *cma_alloc_aligned(struct cma *cma, size_t size, gfp_t gfp)\n{\n\tunsigned int align = min(get_order(size), CONFIG_CMA_ALIGNMENT);\n\n\treturn cma_alloc(cma, size >> PAGE_SHIFT, align, gfp & __GFP_NOWARN);\n}\n\n \nstruct page *dma_alloc_contiguous(struct device *dev, size_t size, gfp_t gfp)\n{\n#ifdef CONFIG_DMA_NUMA_CMA\n\tint nid = dev_to_node(dev);\n#endif\n\n\t \n\tif (!gfpflags_allow_blocking(gfp))\n\t\treturn NULL;\n\tif (dev->cma_area)\n\t\treturn cma_alloc_aligned(dev->cma_area, size, gfp);\n\tif (size <= PAGE_SIZE)\n\t\treturn NULL;\n\n#ifdef CONFIG_DMA_NUMA_CMA\n\tif (nid != NUMA_NO_NODE && !(gfp & (GFP_DMA | GFP_DMA32))) {\n\t\tstruct cma *cma = dma_contiguous_pernuma_area[nid];\n\t\tstruct page *page;\n\n\t\tif (cma) {\n\t\t\tpage = cma_alloc_aligned(cma, size, gfp);\n\t\t\tif (page)\n\t\t\t\treturn page;\n\t\t}\n\n\t\tcma = dma_contiguous_numa_area[nid];\n\t\tif (cma) {\n\t\t\tpage = cma_alloc_aligned(cma, size, gfp);\n\t\t\tif (page)\n\t\t\t\treturn page;\n\t\t}\n\t}\n#endif\n\tif (!dma_contiguous_default_area)\n\t\treturn NULL;\n\n\treturn cma_alloc_aligned(dma_contiguous_default_area, size, gfp);\n}\n\n \nvoid dma_free_contiguous(struct device *dev, struct page *page, size_t size)\n{\n\tunsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;\n\n\t \n\tif (dev->cma_area) {\n\t\tif (cma_release(dev->cma_area, page, count))\n\t\t\treturn;\n\t} else {\n\t\t \n#ifdef CONFIG_DMA_NUMA_CMA\n\t\tif (cma_release(dma_contiguous_pernuma_area[page_to_nid(page)],\n\t\t\t\t\tpage, count))\n\t\t\treturn;\n\t\tif (cma_release(dma_contiguous_numa_area[page_to_nid(page)],\n\t\t\t\t\tpage, count))\n\t\t\treturn;\n#endif\n\t\tif (cma_release(dma_contiguous_default_area, page, count))\n\t\t\treturn;\n\t}\n\n\t \n\t__free_pages(page, get_order(size));\n}\n\n \n#ifdef CONFIG_OF_RESERVED_MEM\n#include <linux/of.h>\n#include <linux/of_fdt.h>\n#include <linux/of_reserved_mem.h>\n\n#undef pr_fmt\n#define pr_fmt(fmt) fmt\n\nstatic int rmem_cma_device_init(struct reserved_mem *rmem, struct device *dev)\n{\n\tdev->cma_area = rmem->priv;\n\treturn 0;\n}\n\nstatic void rmem_cma_device_release(struct reserved_mem *rmem,\n\t\t\t\t    struct device *dev)\n{\n\tdev->cma_area = NULL;\n}\n\nstatic const struct reserved_mem_ops rmem_cma_ops = {\n\t.device_init\t= rmem_cma_device_init,\n\t.device_release = rmem_cma_device_release,\n};\n\nstatic int __init rmem_cma_setup(struct reserved_mem *rmem)\n{\n\tunsigned long node = rmem->fdt_node;\n\tbool default_cma = of_get_flat_dt_prop(node, \"linux,cma-default\", NULL);\n\tstruct cma *cma;\n\tint err;\n\n\tif (size_cmdline != -1 && default_cma) {\n\t\tpr_info(\"Reserved memory: bypass %s node, using cmdline CMA params instead\\n\",\n\t\t\trmem->name);\n\t\treturn -EBUSY;\n\t}\n\n\tif (!of_get_flat_dt_prop(node, \"reusable\", NULL) ||\n\t    of_get_flat_dt_prop(node, \"no-map\", NULL))\n\t\treturn -EINVAL;\n\n\tif (!IS_ALIGNED(rmem->base | rmem->size, CMA_MIN_ALIGNMENT_BYTES)) {\n\t\tpr_err(\"Reserved memory: incorrect alignment of CMA region\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = cma_init_reserved_mem(rmem->base, rmem->size, 0, rmem->name, &cma);\n\tif (err) {\n\t\tpr_err(\"Reserved memory: unable to setup CMA region\\n\");\n\t\treturn err;\n\t}\n\t \n\tdma_contiguous_early_fixup(rmem->base, rmem->size);\n\n\tif (default_cma)\n\t\tdma_contiguous_default_area = cma;\n\n\trmem->ops = &rmem_cma_ops;\n\trmem->priv = cma;\n\n\tpr_info(\"Reserved memory: created CMA memory pool at %pa, size %ld MiB\\n\",\n\t\t&rmem->base, (unsigned long)rmem->size / SZ_1M);\n\n\treturn 0;\n}\nRESERVEDMEM_OF_DECLARE(cma, \"shared-dma-pool\", rmem_cma_setup);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}