{
  "module_name": "swiotlb.c",
  "hash_id": "d12d953e6a2d1150b4f20a36f72f0894c85ae29ff8e3dfeb94609f69573d6a8d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/dma/swiotlb.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"software IO TLB: \" fmt\n\n#include <linux/cache.h>\n#include <linux/cc_platform.h>\n#include <linux/ctype.h>\n#include <linux/debugfs.h>\n#include <linux/dma-direct.h>\n#include <linux/dma-map-ops.h>\n#include <linux/export.h>\n#include <linux/gfp.h>\n#include <linux/highmem.h>\n#include <linux/io.h>\n#include <linux/iommu-helper.h>\n#include <linux/init.h>\n#include <linux/memblock.h>\n#include <linux/mm.h>\n#include <linux/pfn.h>\n#include <linux/rculist.h>\n#include <linux/scatterlist.h>\n#include <linux/set_memory.h>\n#include <linux/spinlock.h>\n#include <linux/string.h>\n#include <linux/swiotlb.h>\n#include <linux/types.h>\n#ifdef CONFIG_DMA_RESTRICTED_POOL\n#include <linux/of.h>\n#include <linux/of_fdt.h>\n#include <linux/of_reserved_mem.h>\n#include <linux/slab.h>\n#endif\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/swiotlb.h>\n\n#define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))\n\n \n#define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)\n\n#define INVALID_PHYS_ADDR (~(phys_addr_t)0)\n\n \nstruct io_tlb_slot {\n\tphys_addr_t orig_addr;\n\tsize_t alloc_size;\n\tunsigned int list;\n};\n\nstatic bool swiotlb_force_bounce;\nstatic bool swiotlb_force_disable;\n\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\nstatic void swiotlb_dyn_alloc(struct work_struct *work);\n\nstatic struct io_tlb_mem io_tlb_default_mem = {\n\t.lock = __SPIN_LOCK_UNLOCKED(io_tlb_default_mem.lock),\n\t.pools = LIST_HEAD_INIT(io_tlb_default_mem.pools),\n\t.dyn_alloc = __WORK_INITIALIZER(io_tlb_default_mem.dyn_alloc,\n\t\t\t\t\tswiotlb_dyn_alloc),\n};\n\n#else   \n\nstatic struct io_tlb_mem io_tlb_default_mem;\n\n#endif\t \n\nstatic unsigned long default_nslabs = IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT;\nstatic unsigned long default_nareas;\n\n \nstruct io_tlb_area {\n\tunsigned long used;\n\tunsigned int index;\n\tspinlock_t lock;\n};\n\n \nstatic bool round_up_default_nslabs(void)\n{\n\tif (!default_nareas)\n\t\treturn false;\n\n\tif (default_nslabs < IO_TLB_SEGSIZE * default_nareas)\n\t\tdefault_nslabs = IO_TLB_SEGSIZE * default_nareas;\n\telse if (is_power_of_2(default_nslabs))\n\t\treturn false;\n\tdefault_nslabs = roundup_pow_of_two(default_nslabs);\n\treturn true;\n}\n\n \nstatic void swiotlb_adjust_nareas(unsigned int nareas)\n{\n\tif (!nareas)\n\t\tnareas = 1;\n\telse if (!is_power_of_2(nareas))\n\t\tnareas = roundup_pow_of_two(nareas);\n\n\tdefault_nareas = nareas;\n\n\tpr_info(\"area num %d.\\n\", nareas);\n\tif (round_up_default_nslabs())\n\t\tpr_info(\"SWIOTLB bounce buffer size roundup to %luMB\",\n\t\t\t(default_nslabs << IO_TLB_SHIFT) >> 20);\n}\n\n \nstatic unsigned int limit_nareas(unsigned int nareas, unsigned long nslots)\n{\n\tif (nslots < nareas * IO_TLB_SEGSIZE)\n\t\treturn nslots / IO_TLB_SEGSIZE;\n\treturn nareas;\n}\n\nstatic int __init\nsetup_io_tlb_npages(char *str)\n{\n\tif (isdigit(*str)) {\n\t\t \n\t\tdefault_nslabs =\n\t\t\tALIGN(simple_strtoul(str, &str, 0), IO_TLB_SEGSIZE);\n\t}\n\tif (*str == ',')\n\t\t++str;\n\tif (isdigit(*str))\n\t\tswiotlb_adjust_nareas(simple_strtoul(str, &str, 0));\n\tif (*str == ',')\n\t\t++str;\n\tif (!strcmp(str, \"force\"))\n\t\tswiotlb_force_bounce = true;\n\telse if (!strcmp(str, \"noforce\"))\n\t\tswiotlb_force_disable = true;\n\n\treturn 0;\n}\nearly_param(\"swiotlb\", setup_io_tlb_npages);\n\nunsigned long swiotlb_size_or_default(void)\n{\n\treturn default_nslabs << IO_TLB_SHIFT;\n}\n\nvoid __init swiotlb_adjust_size(unsigned long size)\n{\n\t \n\tif (default_nslabs != IO_TLB_DEFAULT_SIZE >> IO_TLB_SHIFT)\n\t\treturn;\n\n\tsize = ALIGN(size, IO_TLB_SIZE);\n\tdefault_nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);\n\tif (round_up_default_nslabs())\n\t\tsize = default_nslabs << IO_TLB_SHIFT;\n\tpr_info(\"SWIOTLB bounce buffer size adjusted to %luMB\", size >> 20);\n}\n\nvoid swiotlb_print_info(void)\n{\n\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;\n\n\tif (!mem->nslabs) {\n\t\tpr_warn(\"No low mem\\n\");\n\t\treturn;\n\t}\n\n\tpr_info(\"mapped [mem %pa-%pa] (%luMB)\\n\", &mem->start, &mem->end,\n\t       (mem->nslabs << IO_TLB_SHIFT) >> 20);\n}\n\nstatic inline unsigned long io_tlb_offset(unsigned long val)\n{\n\treturn val & (IO_TLB_SEGSIZE - 1);\n}\n\nstatic inline unsigned long nr_slots(u64 val)\n{\n\treturn DIV_ROUND_UP(val, IO_TLB_SIZE);\n}\n\n \nvoid __init swiotlb_update_mem_attributes(void)\n{\n\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;\n\tunsigned long bytes;\n\n\tif (!mem->nslabs || mem->late_alloc)\n\t\treturn;\n\tbytes = PAGE_ALIGN(mem->nslabs << IO_TLB_SHIFT);\n\tset_memory_decrypted((unsigned long)mem->vaddr, bytes >> PAGE_SHIFT);\n}\n\nstatic void swiotlb_init_io_tlb_pool(struct io_tlb_pool *mem, phys_addr_t start,\n\t\tunsigned long nslabs, bool late_alloc, unsigned int nareas)\n{\n\tvoid *vaddr = phys_to_virt(start);\n\tunsigned long bytes = nslabs << IO_TLB_SHIFT, i;\n\n\tmem->nslabs = nslabs;\n\tmem->start = start;\n\tmem->end = mem->start + bytes;\n\tmem->late_alloc = late_alloc;\n\tmem->nareas = nareas;\n\tmem->area_nslabs = nslabs / mem->nareas;\n\n\tfor (i = 0; i < mem->nareas; i++) {\n\t\tspin_lock_init(&mem->areas[i].lock);\n\t\tmem->areas[i].index = 0;\n\t\tmem->areas[i].used = 0;\n\t}\n\n\tfor (i = 0; i < mem->nslabs; i++) {\n\t\tmem->slots[i].list = min(IO_TLB_SEGSIZE - io_tlb_offset(i),\n\t\t\t\t\t mem->nslabs - i);\n\t\tmem->slots[i].orig_addr = INVALID_PHYS_ADDR;\n\t\tmem->slots[i].alloc_size = 0;\n\t}\n\n\tmemset(vaddr, 0, bytes);\n\tmem->vaddr = vaddr;\n\treturn;\n}\n\n \nstatic void add_mem_pool(struct io_tlb_mem *mem, struct io_tlb_pool *pool)\n{\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\tspin_lock(&mem->lock);\n\tlist_add_rcu(&pool->node, &mem->pools);\n\tmem->nslabs += pool->nslabs;\n\tspin_unlock(&mem->lock);\n#else\n\tmem->nslabs = pool->nslabs;\n#endif\n}\n\nstatic void __init *swiotlb_memblock_alloc(unsigned long nslabs,\n\t\tunsigned int flags,\n\t\tint (*remap)(void *tlb, unsigned long nslabs))\n{\n\tsize_t bytes = PAGE_ALIGN(nslabs << IO_TLB_SHIFT);\n\tvoid *tlb;\n\n\t \n\tif (flags & SWIOTLB_ANY)\n\t\ttlb = memblock_alloc(bytes, PAGE_SIZE);\n\telse\n\t\ttlb = memblock_alloc_low(bytes, PAGE_SIZE);\n\n\tif (!tlb) {\n\t\tpr_warn(\"%s: Failed to allocate %zu bytes tlb structure\\n\",\n\t\t\t__func__, bytes);\n\t\treturn NULL;\n\t}\n\n\tif (remap && remap(tlb, nslabs) < 0) {\n\t\tmemblock_free(tlb, PAGE_ALIGN(bytes));\n\t\tpr_warn(\"%s: Failed to remap %zu bytes\\n\", __func__, bytes);\n\t\treturn NULL;\n\t}\n\n\treturn tlb;\n}\n\n \nvoid __init swiotlb_init_remap(bool addressing_limit, unsigned int flags,\n\t\tint (*remap)(void *tlb, unsigned long nslabs))\n{\n\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;\n\tunsigned long nslabs;\n\tunsigned int nareas;\n\tsize_t alloc_size;\n\tvoid *tlb;\n\n\tif (!addressing_limit && !swiotlb_force_bounce)\n\t\treturn;\n\tif (swiotlb_force_disable)\n\t\treturn;\n\n\tio_tlb_default_mem.force_bounce =\n\t\tswiotlb_force_bounce || (flags & SWIOTLB_FORCE);\n\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\tif (!remap)\n\t\tio_tlb_default_mem.can_grow = true;\n\tif (flags & SWIOTLB_ANY)\n\t\tio_tlb_default_mem.phys_limit = virt_to_phys(high_memory - 1);\n\telse\n\t\tio_tlb_default_mem.phys_limit = ARCH_LOW_ADDRESS_LIMIT;\n#endif\n\n\tif (!default_nareas)\n\t\tswiotlb_adjust_nareas(num_possible_cpus());\n\n\tnslabs = default_nslabs;\n\tnareas = limit_nareas(default_nareas, nslabs);\n\twhile ((tlb = swiotlb_memblock_alloc(nslabs, flags, remap)) == NULL) {\n\t\tif (nslabs <= IO_TLB_MIN_SLABS)\n\t\t\treturn;\n\t\tnslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);\n\t\tnareas = limit_nareas(nareas, nslabs);\n\t}\n\n\tif (default_nslabs != nslabs) {\n\t\tpr_info(\"SWIOTLB bounce buffer size adjusted %lu -> %lu slabs\",\n\t\t\tdefault_nslabs, nslabs);\n\t\tdefault_nslabs = nslabs;\n\t}\n\n\talloc_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), nslabs));\n\tmem->slots = memblock_alloc(alloc_size, PAGE_SIZE);\n\tif (!mem->slots) {\n\t\tpr_warn(\"%s: Failed to allocate %zu bytes align=0x%lx\\n\",\n\t\t\t__func__, alloc_size, PAGE_SIZE);\n\t\treturn;\n\t}\n\n\tmem->areas = memblock_alloc(array_size(sizeof(struct io_tlb_area),\n\t\tnareas), SMP_CACHE_BYTES);\n\tif (!mem->areas) {\n\t\tpr_warn(\"%s: Failed to allocate mem->areas.\\n\", __func__);\n\t\treturn;\n\t}\n\n\tswiotlb_init_io_tlb_pool(mem, __pa(tlb), nslabs, false, nareas);\n\tadd_mem_pool(&io_tlb_default_mem, mem);\n\n\tif (flags & SWIOTLB_VERBOSE)\n\t\tswiotlb_print_info();\n}\n\nvoid __init swiotlb_init(bool addressing_limit, unsigned int flags)\n{\n\tswiotlb_init_remap(addressing_limit, flags, NULL);\n}\n\n \nint swiotlb_init_late(size_t size, gfp_t gfp_mask,\n\t\tint (*remap)(void *tlb, unsigned long nslabs))\n{\n\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;\n\tunsigned long nslabs = ALIGN(size >> IO_TLB_SHIFT, IO_TLB_SEGSIZE);\n\tunsigned int nareas;\n\tunsigned char *vstart = NULL;\n\tunsigned int order, area_order;\n\tbool retried = false;\n\tint rc = 0;\n\n\tif (io_tlb_default_mem.nslabs)\n\t\treturn 0;\n\n\tif (swiotlb_force_disable)\n\t\treturn 0;\n\n\tio_tlb_default_mem.force_bounce = swiotlb_force_bounce;\n\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\tif (!remap)\n\t\tio_tlb_default_mem.can_grow = true;\n\tif (IS_ENABLED(CONFIG_ZONE_DMA) && (gfp_mask & __GFP_DMA))\n\t\tio_tlb_default_mem.phys_limit = DMA_BIT_MASK(zone_dma_bits);\n\telse if (IS_ENABLED(CONFIG_ZONE_DMA32) && (gfp_mask & __GFP_DMA32))\n\t\tio_tlb_default_mem.phys_limit = DMA_BIT_MASK(32);\n\telse\n\t\tio_tlb_default_mem.phys_limit = virt_to_phys(high_memory - 1);\n#endif\n\n\tif (!default_nareas)\n\t\tswiotlb_adjust_nareas(num_possible_cpus());\n\nretry:\n\torder = get_order(nslabs << IO_TLB_SHIFT);\n\tnslabs = SLABS_PER_PAGE << order;\n\n\twhile ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {\n\t\tvstart = (void *)__get_free_pages(gfp_mask | __GFP_NOWARN,\n\t\t\t\t\t\t  order);\n\t\tif (vstart)\n\t\t\tbreak;\n\t\torder--;\n\t\tnslabs = SLABS_PER_PAGE << order;\n\t\tretried = true;\n\t}\n\n\tif (!vstart)\n\t\treturn -ENOMEM;\n\n\tif (remap)\n\t\trc = remap(vstart, nslabs);\n\tif (rc) {\n\t\tfree_pages((unsigned long)vstart, order);\n\n\t\tnslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);\n\t\tif (nslabs < IO_TLB_MIN_SLABS)\n\t\t\treturn rc;\n\t\tretried = true;\n\t\tgoto retry;\n\t}\n\n\tif (retried) {\n\t\tpr_warn(\"only able to allocate %ld MB\\n\",\n\t\t\t(PAGE_SIZE << order) >> 20);\n\t}\n\n\tnareas = limit_nareas(default_nareas, nslabs);\n\tarea_order = get_order(array_size(sizeof(*mem->areas), nareas));\n\tmem->areas = (struct io_tlb_area *)\n\t\t__get_free_pages(GFP_KERNEL | __GFP_ZERO, area_order);\n\tif (!mem->areas)\n\t\tgoto error_area;\n\n\tmem->slots = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\tget_order(array_size(sizeof(*mem->slots), nslabs)));\n\tif (!mem->slots)\n\t\tgoto error_slots;\n\n\tset_memory_decrypted((unsigned long)vstart,\n\t\t\t     (nslabs << IO_TLB_SHIFT) >> PAGE_SHIFT);\n\tswiotlb_init_io_tlb_pool(mem, virt_to_phys(vstart), nslabs, true,\n\t\t\t\t nareas);\n\tadd_mem_pool(&io_tlb_default_mem, mem);\n\n\tswiotlb_print_info();\n\treturn 0;\n\nerror_slots:\n\tfree_pages((unsigned long)mem->areas, area_order);\nerror_area:\n\tfree_pages((unsigned long)vstart, order);\n\treturn -ENOMEM;\n}\n\nvoid __init swiotlb_exit(void)\n{\n\tstruct io_tlb_pool *mem = &io_tlb_default_mem.defpool;\n\tunsigned long tbl_vaddr;\n\tsize_t tbl_size, slots_size;\n\tunsigned int area_order;\n\n\tif (swiotlb_force_bounce)\n\t\treturn;\n\n\tif (!mem->nslabs)\n\t\treturn;\n\n\tpr_info(\"tearing down default memory pool\\n\");\n\ttbl_vaddr = (unsigned long)phys_to_virt(mem->start);\n\ttbl_size = PAGE_ALIGN(mem->end - mem->start);\n\tslots_size = PAGE_ALIGN(array_size(sizeof(*mem->slots), mem->nslabs));\n\n\tset_memory_encrypted(tbl_vaddr, tbl_size >> PAGE_SHIFT);\n\tif (mem->late_alloc) {\n\t\tarea_order = get_order(array_size(sizeof(*mem->areas),\n\t\t\tmem->nareas));\n\t\tfree_pages((unsigned long)mem->areas, area_order);\n\t\tfree_pages(tbl_vaddr, get_order(tbl_size));\n\t\tfree_pages((unsigned long)mem->slots, get_order(slots_size));\n\t} else {\n\t\tmemblock_free_late(__pa(mem->areas),\n\t\t\tarray_size(sizeof(*mem->areas), mem->nareas));\n\t\tmemblock_free_late(mem->start, tbl_size);\n\t\tmemblock_free_late(__pa(mem->slots), slots_size);\n\t}\n\n\tmemset(mem, 0, sizeof(*mem));\n}\n\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\n \nstatic struct page *alloc_dma_pages(gfp_t gfp, size_t bytes, u64 phys_limit)\n{\n\tunsigned int order = get_order(bytes);\n\tstruct page *page;\n\tphys_addr_t paddr;\n\tvoid *vaddr;\n\n\tpage = alloc_pages(gfp, order);\n\tif (!page)\n\t\treturn NULL;\n\n\tpaddr = page_to_phys(page);\n\tif (paddr + bytes - 1 > phys_limit) {\n\t\t__free_pages(page, order);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\tvaddr = phys_to_virt(paddr);\n\tif (set_memory_decrypted((unsigned long)vaddr, PFN_UP(bytes)))\n\t\tgoto error;\n\treturn page;\n\nerror:\n\t \n\tif (!set_memory_encrypted((unsigned long)vaddr, PFN_UP(bytes)))\n\t\t__free_pages(page, order);\n\treturn NULL;\n}\n\n \nstatic struct page *swiotlb_alloc_tlb(struct device *dev, size_t bytes,\n\t\tu64 phys_limit, gfp_t gfp)\n{\n\tstruct page *page;\n\n\t \n\tif (!gfpflags_allow_blocking(gfp) && dev && force_dma_unencrypted(dev)) {\n\t\tvoid *vaddr;\n\n\t\tif (!IS_ENABLED(CONFIG_DMA_COHERENT_POOL))\n\t\t\treturn NULL;\n\n\t\treturn dma_alloc_from_pool(dev, bytes, &vaddr, gfp,\n\t\t\t\t\t   dma_coherent_ok);\n\t}\n\n\tgfp &= ~GFP_ZONEMASK;\n\tif (phys_limit <= DMA_BIT_MASK(zone_dma_bits))\n\t\tgfp |= __GFP_DMA;\n\telse if (phys_limit <= DMA_BIT_MASK(32))\n\t\tgfp |= __GFP_DMA32;\n\n\twhile (IS_ERR(page = alloc_dma_pages(gfp, bytes, phys_limit))) {\n\t\tif (IS_ENABLED(CONFIG_ZONE_DMA32) &&\n\t\t    phys_limit < DMA_BIT_MASK(64) &&\n\t\t    !(gfp & (__GFP_DMA32 | __GFP_DMA)))\n\t\t\tgfp |= __GFP_DMA32;\n\t\telse if (IS_ENABLED(CONFIG_ZONE_DMA) &&\n\t\t\t !(gfp & __GFP_DMA))\n\t\t\tgfp = (gfp & ~__GFP_DMA32) | __GFP_DMA;\n\t\telse\n\t\t\treturn NULL;\n\t}\n\n\treturn page;\n}\n\n \nstatic void swiotlb_free_tlb(void *vaddr, size_t bytes)\n{\n\tif (IS_ENABLED(CONFIG_DMA_COHERENT_POOL) &&\n\t    dma_free_from_pool(NULL, vaddr, bytes))\n\t\treturn;\n\n\t \n\tif (!set_memory_encrypted((unsigned long)vaddr, PFN_UP(bytes)))\n\t\t__free_pages(virt_to_page(vaddr), get_order(bytes));\n}\n\n \nstatic struct io_tlb_pool *swiotlb_alloc_pool(struct device *dev,\n\t\tunsigned long minslabs, unsigned long nslabs,\n\t\tunsigned int nareas, u64 phys_limit, gfp_t gfp)\n{\n\tstruct io_tlb_pool *pool;\n\tunsigned int slot_order;\n\tstruct page *tlb;\n\tsize_t pool_size;\n\tsize_t tlb_size;\n\n\tif (nslabs > SLABS_PER_PAGE << MAX_ORDER) {\n\t\tnslabs = SLABS_PER_PAGE << MAX_ORDER;\n\t\tnareas = limit_nareas(nareas, nslabs);\n\t}\n\n\tpool_size = sizeof(*pool) + array_size(sizeof(*pool->areas), nareas);\n\tpool = kzalloc(pool_size, gfp);\n\tif (!pool)\n\t\tgoto error;\n\tpool->areas = (void *)pool + sizeof(*pool);\n\n\ttlb_size = nslabs << IO_TLB_SHIFT;\n\twhile (!(tlb = swiotlb_alloc_tlb(dev, tlb_size, phys_limit, gfp))) {\n\t\tif (nslabs <= minslabs)\n\t\t\tgoto error_tlb;\n\t\tnslabs = ALIGN(nslabs >> 1, IO_TLB_SEGSIZE);\n\t\tnareas = limit_nareas(nareas, nslabs);\n\t\ttlb_size = nslabs << IO_TLB_SHIFT;\n\t}\n\n\tslot_order = get_order(array_size(sizeof(*pool->slots), nslabs));\n\tpool->slots = (struct io_tlb_slot *)\n\t\t__get_free_pages(gfp, slot_order);\n\tif (!pool->slots)\n\t\tgoto error_slots;\n\n\tswiotlb_init_io_tlb_pool(pool, page_to_phys(tlb), nslabs, true, nareas);\n\treturn pool;\n\nerror_slots:\n\tswiotlb_free_tlb(page_address(tlb), tlb_size);\nerror_tlb:\n\tkfree(pool);\nerror:\n\treturn NULL;\n}\n\n \nstatic void swiotlb_dyn_alloc(struct work_struct *work)\n{\n\tstruct io_tlb_mem *mem =\n\t\tcontainer_of(work, struct io_tlb_mem, dyn_alloc);\n\tstruct io_tlb_pool *pool;\n\n\tpool = swiotlb_alloc_pool(NULL, IO_TLB_MIN_SLABS, default_nslabs,\n\t\t\t\t  default_nareas, mem->phys_limit, GFP_KERNEL);\n\tif (!pool) {\n\t\tpr_warn_ratelimited(\"Failed to allocate new pool\");\n\t\treturn;\n\t}\n\n\tadd_mem_pool(mem, pool);\n}\n\n \nstatic void swiotlb_dyn_free(struct rcu_head *rcu)\n{\n\tstruct io_tlb_pool *pool = container_of(rcu, struct io_tlb_pool, rcu);\n\tsize_t slots_size = array_size(sizeof(*pool->slots), pool->nslabs);\n\tsize_t tlb_size = pool->end - pool->start;\n\n\tfree_pages((unsigned long)pool->slots, get_order(slots_size));\n\tswiotlb_free_tlb(pool->vaddr, tlb_size);\n\tkfree(pool);\n}\n\n \nstruct io_tlb_pool *swiotlb_find_pool(struct device *dev, phys_addr_t paddr)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\tstruct io_tlb_pool *pool;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pool, &mem->pools, node) {\n\t\tif (paddr >= pool->start && paddr < pool->end)\n\t\t\tgoto out;\n\t}\n\n\tlist_for_each_entry_rcu(pool, &dev->dma_io_tlb_pools, node) {\n\t\tif (paddr >= pool->start && paddr < pool->end)\n\t\t\tgoto out;\n\t}\n\tpool = NULL;\nout:\n\trcu_read_unlock();\n\treturn pool;\n}\n\n \nstatic void swiotlb_del_pool(struct device *dev, struct io_tlb_pool *pool)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->dma_io_tlb_lock, flags);\n\tlist_del_rcu(&pool->node);\n\tspin_unlock_irqrestore(&dev->dma_io_tlb_lock, flags);\n\n\tcall_rcu(&pool->rcu, swiotlb_dyn_free);\n}\n\n#endif\t \n\n \nvoid swiotlb_dev_init(struct device *dev)\n{\n\tdev->dma_io_tlb_mem = &io_tlb_default_mem;\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\tINIT_LIST_HEAD(&dev->dma_io_tlb_pools);\n\tspin_lock_init(&dev->dma_io_tlb_lock);\n\tdev->dma_uses_io_tlb = false;\n#endif\n}\n\n \nstatic unsigned int swiotlb_align_offset(struct device *dev, u64 addr)\n{\n\treturn addr & dma_get_min_align_mask(dev) & (IO_TLB_SIZE - 1);\n}\n\n \nstatic void swiotlb_bounce(struct device *dev, phys_addr_t tlb_addr, size_t size,\n\t\t\t   enum dma_data_direction dir)\n{\n\tstruct io_tlb_pool *mem = swiotlb_find_pool(dev, tlb_addr);\n\tint index = (tlb_addr - mem->start) >> IO_TLB_SHIFT;\n\tphys_addr_t orig_addr = mem->slots[index].orig_addr;\n\tsize_t alloc_size = mem->slots[index].alloc_size;\n\tunsigned long pfn = PFN_DOWN(orig_addr);\n\tunsigned char *vaddr = mem->vaddr + tlb_addr - mem->start;\n\tunsigned int tlb_offset, orig_addr_offset;\n\n\tif (orig_addr == INVALID_PHYS_ADDR)\n\t\treturn;\n\n\ttlb_offset = tlb_addr & (IO_TLB_SIZE - 1);\n\torig_addr_offset = swiotlb_align_offset(dev, orig_addr);\n\tif (tlb_offset < orig_addr_offset) {\n\t\tdev_WARN_ONCE(dev, 1,\n\t\t\t\"Access before mapping start detected. orig offset %u, requested offset %u.\\n\",\n\t\t\torig_addr_offset, tlb_offset);\n\t\treturn;\n\t}\n\n\ttlb_offset -= orig_addr_offset;\n\tif (tlb_offset > alloc_size) {\n\t\tdev_WARN_ONCE(dev, 1,\n\t\t\t\"Buffer overflow detected. Allocation size: %zu. Mapping size: %zu+%u.\\n\",\n\t\t\talloc_size, size, tlb_offset);\n\t\treturn;\n\t}\n\n\torig_addr += tlb_offset;\n\talloc_size -= tlb_offset;\n\n\tif (size > alloc_size) {\n\t\tdev_WARN_ONCE(dev, 1,\n\t\t\t\"Buffer overflow detected. Allocation size: %zu. Mapping size: %zu.\\n\",\n\t\t\talloc_size, size);\n\t\tsize = alloc_size;\n\t}\n\n\tif (PageHighMem(pfn_to_page(pfn))) {\n\t\tunsigned int offset = orig_addr & ~PAGE_MASK;\n\t\tstruct page *page;\n\t\tunsigned int sz = 0;\n\t\tunsigned long flags;\n\n\t\twhile (size) {\n\t\t\tsz = min_t(size_t, PAGE_SIZE - offset, size);\n\n\t\t\tlocal_irq_save(flags);\n\t\t\tpage = pfn_to_page(pfn);\n\t\t\tif (dir == DMA_TO_DEVICE)\n\t\t\t\tmemcpy_from_page(vaddr, page, offset, sz);\n\t\t\telse\n\t\t\t\tmemcpy_to_page(page, offset, vaddr, sz);\n\t\t\tlocal_irq_restore(flags);\n\n\t\t\tsize -= sz;\n\t\t\tpfn++;\n\t\t\tvaddr += sz;\n\t\t\toffset = 0;\n\t\t}\n\t} else if (dir == DMA_TO_DEVICE) {\n\t\tmemcpy(vaddr, phys_to_virt(orig_addr), size);\n\t} else {\n\t\tmemcpy(phys_to_virt(orig_addr), vaddr, size);\n\t}\n}\n\nstatic inline phys_addr_t slot_addr(phys_addr_t start, phys_addr_t idx)\n{\n\treturn start + (idx << IO_TLB_SHIFT);\n}\n\n \nstatic inline unsigned long get_max_slots(unsigned long boundary_mask)\n{\n\treturn (boundary_mask >> IO_TLB_SHIFT) + 1;\n}\n\nstatic unsigned int wrap_area_index(struct io_tlb_pool *mem, unsigned int index)\n{\n\tif (index >= mem->area_nslabs)\n\t\treturn 0;\n\treturn index;\n}\n\n \n#ifdef CONFIG_DEBUG_FS\nstatic void inc_used_and_hiwater(struct io_tlb_mem *mem, unsigned int nslots)\n{\n\tunsigned long old_hiwater, new_used;\n\n\tnew_used = atomic_long_add_return(nslots, &mem->total_used);\n\told_hiwater = atomic_long_read(&mem->used_hiwater);\n\tdo {\n\t\tif (new_used <= old_hiwater)\n\t\t\tbreak;\n\t} while (!atomic_long_try_cmpxchg(&mem->used_hiwater,\n\t\t\t\t\t  &old_hiwater, new_used));\n}\n\nstatic void dec_used(struct io_tlb_mem *mem, unsigned int nslots)\n{\n\tatomic_long_sub(nslots, &mem->total_used);\n}\n\n#else  \nstatic void inc_used_and_hiwater(struct io_tlb_mem *mem, unsigned int nslots)\n{\n}\nstatic void dec_used(struct io_tlb_mem *mem, unsigned int nslots)\n{\n}\n#endif  \n\n \nstatic int swiotlb_area_find_slots(struct device *dev, struct io_tlb_pool *pool,\n\t\tint area_index, phys_addr_t orig_addr, size_t alloc_size,\n\t\tunsigned int alloc_align_mask)\n{\n\tstruct io_tlb_area *area = pool->areas + area_index;\n\tunsigned long boundary_mask = dma_get_seg_boundary(dev);\n\tdma_addr_t tbl_dma_addr =\n\t\tphys_to_dma_unencrypted(dev, pool->start) & boundary_mask;\n\tunsigned long max_slots = get_max_slots(boundary_mask);\n\tunsigned int iotlb_align_mask =\n\t\tdma_get_min_align_mask(dev) | alloc_align_mask;\n\tunsigned int nslots = nr_slots(alloc_size), stride;\n\tunsigned int offset = swiotlb_align_offset(dev, orig_addr);\n\tunsigned int index, slots_checked, count = 0, i;\n\tunsigned long flags;\n\tunsigned int slot_base;\n\tunsigned int slot_index;\n\n\tBUG_ON(!nslots);\n\tBUG_ON(area_index >= pool->nareas);\n\n\t \n\tif (alloc_size >= PAGE_SIZE)\n\t\tiotlb_align_mask |= ~PAGE_MASK;\n\tiotlb_align_mask &= ~(IO_TLB_SIZE - 1);\n\n\t \n\tstride = (iotlb_align_mask >> IO_TLB_SHIFT) + 1;\n\n\tspin_lock_irqsave(&area->lock, flags);\n\tif (unlikely(nslots > pool->area_nslabs - area->used))\n\t\tgoto not_found;\n\n\tslot_base = area_index * pool->area_nslabs;\n\tindex = area->index;\n\n\tfor (slots_checked = 0; slots_checked < pool->area_nslabs; ) {\n\t\tslot_index = slot_base + index;\n\n\t\tif (orig_addr &&\n\t\t    (slot_addr(tbl_dma_addr, slot_index) &\n\t\t     iotlb_align_mask) != (orig_addr & iotlb_align_mask)) {\n\t\t\tindex = wrap_area_index(pool, index + 1);\n\t\t\tslots_checked++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!iommu_is_span_boundary(slot_index, nslots,\n\t\t\t\t\t    nr_slots(tbl_dma_addr),\n\t\t\t\t\t    max_slots)) {\n\t\t\tif (pool->slots[slot_index].list >= nslots)\n\t\t\t\tgoto found;\n\t\t}\n\t\tindex = wrap_area_index(pool, index + stride);\n\t\tslots_checked += stride;\n\t}\n\nnot_found:\n\tspin_unlock_irqrestore(&area->lock, flags);\n\treturn -1;\n\nfound:\n\t \n\tfor (i = slot_index; i < slot_index + nslots; i++) {\n\t\tpool->slots[i].list = 0;\n\t\tpool->slots[i].alloc_size = alloc_size - (offset +\n\t\t\t\t((i - slot_index) << IO_TLB_SHIFT));\n\t}\n\tfor (i = slot_index - 1;\n\t     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 &&\n\t     pool->slots[i].list; i--)\n\t\tpool->slots[i].list = ++count;\n\n\t \n\tarea->index = wrap_area_index(pool, index + nslots);\n\tarea->used += nslots;\n\tspin_unlock_irqrestore(&area->lock, flags);\n\n\tinc_used_and_hiwater(dev->dma_io_tlb_mem, nslots);\n\treturn slot_index;\n}\n\n \nstatic int swiotlb_pool_find_slots(struct device *dev, struct io_tlb_pool *pool,\n\t\tphys_addr_t orig_addr, size_t alloc_size,\n\t\tunsigned int alloc_align_mask)\n{\n\tint start = raw_smp_processor_id() & (pool->nareas - 1);\n\tint i = start, index;\n\n\tdo {\n\t\tindex = swiotlb_area_find_slots(dev, pool, i, orig_addr,\n\t\t\t\t\t\talloc_size, alloc_align_mask);\n\t\tif (index >= 0)\n\t\t\treturn index;\n\t\tif (++i >= pool->nareas)\n\t\t\ti = 0;\n\t} while (i != start);\n\n\treturn -1;\n}\n\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\n \nstatic int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,\n\t\tsize_t alloc_size, unsigned int alloc_align_mask,\n\t\tstruct io_tlb_pool **retpool)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\tstruct io_tlb_pool *pool;\n\tunsigned long nslabs;\n\tunsigned long flags;\n\tu64 phys_limit;\n\tint index;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pool, &mem->pools, node) {\n\t\tindex = swiotlb_pool_find_slots(dev, pool, orig_addr,\n\t\t\t\t\t\talloc_size, alloc_align_mask);\n\t\tif (index >= 0) {\n\t\t\trcu_read_unlock();\n\t\t\tgoto found;\n\t\t}\n\t}\n\trcu_read_unlock();\n\tif (!mem->can_grow)\n\t\treturn -1;\n\n\tschedule_work(&mem->dyn_alloc);\n\n\tnslabs = nr_slots(alloc_size);\n\tphys_limit = min_not_zero(*dev->dma_mask, dev->bus_dma_limit);\n\tpool = swiotlb_alloc_pool(dev, nslabs, nslabs, 1, phys_limit,\n\t\t\t\t  GFP_NOWAIT | __GFP_NOWARN);\n\tif (!pool)\n\t\treturn -1;\n\n\tindex = swiotlb_pool_find_slots(dev, pool, orig_addr,\n\t\t\t\t\talloc_size, alloc_align_mask);\n\tif (index < 0) {\n\t\tswiotlb_dyn_free(&pool->rcu);\n\t\treturn -1;\n\t}\n\n\tpool->transient = true;\n\tspin_lock_irqsave(&dev->dma_io_tlb_lock, flags);\n\tlist_add_rcu(&pool->node, &dev->dma_io_tlb_pools);\n\tspin_unlock_irqrestore(&dev->dma_io_tlb_lock, flags);\n\nfound:\n\tWRITE_ONCE(dev->dma_uses_io_tlb, true);\n\n\t \n\tsmp_mb();\n\n\t*retpool = pool;\n\treturn index;\n}\n\n#else   \n\nstatic int swiotlb_find_slots(struct device *dev, phys_addr_t orig_addr,\n\t\tsize_t alloc_size, unsigned int alloc_align_mask,\n\t\tstruct io_tlb_pool **retpool)\n{\n\t*retpool = &dev->dma_io_tlb_mem->defpool;\n\treturn swiotlb_pool_find_slots(dev, *retpool,\n\t\t\t\t       orig_addr, alloc_size, alloc_align_mask);\n}\n\n#endif  \n\n#ifdef CONFIG_DEBUG_FS\n\n \nstatic unsigned long mem_used(struct io_tlb_mem *mem)\n{\n\treturn atomic_long_read(&mem->total_used);\n}\n\n#else  \n\n \nstatic unsigned long mem_pool_used(struct io_tlb_pool *pool)\n{\n\tint i;\n\tunsigned long used = 0;\n\n\tfor (i = 0; i < pool->nareas; i++)\n\t\tused += pool->areas[i].used;\n\treturn used;\n}\n\n \nstatic unsigned long mem_used(struct io_tlb_mem *mem)\n{\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\tstruct io_tlb_pool *pool;\n\tunsigned long used = 0;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(pool, &mem->pools, node)\n\t\tused += mem_pool_used(pool);\n\trcu_read_unlock();\n\n\treturn used;\n#else\n\treturn mem_pool_used(&mem->defpool);\n#endif\n}\n\n#endif  \n\nphys_addr_t swiotlb_tbl_map_single(struct device *dev, phys_addr_t orig_addr,\n\t\tsize_t mapping_size, size_t alloc_size,\n\t\tunsigned int alloc_align_mask, enum dma_data_direction dir,\n\t\tunsigned long attrs)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\tunsigned int offset = swiotlb_align_offset(dev, orig_addr);\n\tstruct io_tlb_pool *pool;\n\tunsigned int i;\n\tint index;\n\tphys_addr_t tlb_addr;\n\n\tif (!mem || !mem->nslabs) {\n\t\tdev_warn_ratelimited(dev,\n\t\t\t\"Can not allocate SWIOTLB buffer earlier and can't now provide you with the DMA bounce buffer\");\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\tif (cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\tpr_warn_once(\"Memory encryption is active and system is using DMA bounce buffers\\n\");\n\n\tif (mapping_size > alloc_size) {\n\t\tdev_warn_once(dev, \"Invalid sizes (mapping: %zd bytes, alloc: %zd bytes)\",\n\t\t\t      mapping_size, alloc_size);\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\tindex = swiotlb_find_slots(dev, orig_addr,\n\t\t\t\t   alloc_size + offset, alloc_align_mask, &pool);\n\tif (index == -1) {\n\t\tif (!(attrs & DMA_ATTR_NO_WARN))\n\t\t\tdev_warn_ratelimited(dev,\n\t\"swiotlb buffer is full (sz: %zd bytes), total %lu (slots), used %lu (slots)\\n\",\n\t\t\t\t alloc_size, mem->nslabs, mem_used(mem));\n\t\treturn (phys_addr_t)DMA_MAPPING_ERROR;\n\t}\n\n\t \n\tfor (i = 0; i < nr_slots(alloc_size + offset); i++)\n\t\tpool->slots[index + i].orig_addr = slot_addr(orig_addr, i);\n\ttlb_addr = slot_addr(pool->start, index) + offset;\n\t \n\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_TO_DEVICE);\n\treturn tlb_addr;\n}\n\nstatic void swiotlb_release_slots(struct device *dev, phys_addr_t tlb_addr)\n{\n\tstruct io_tlb_pool *mem = swiotlb_find_pool(dev, tlb_addr);\n\tunsigned long flags;\n\tunsigned int offset = swiotlb_align_offset(dev, tlb_addr);\n\tint index = (tlb_addr - offset - mem->start) >> IO_TLB_SHIFT;\n\tint nslots = nr_slots(mem->slots[index].alloc_size + offset);\n\tint aindex = index / mem->area_nslabs;\n\tstruct io_tlb_area *area = &mem->areas[aindex];\n\tint count, i;\n\n\t \n\tBUG_ON(aindex >= mem->nareas);\n\n\tspin_lock_irqsave(&area->lock, flags);\n\tif (index + nslots < ALIGN(index + 1, IO_TLB_SEGSIZE))\n\t\tcount = mem->slots[index + nslots].list;\n\telse\n\t\tcount = 0;\n\n\t \n\tfor (i = index + nslots - 1; i >= index; i--) {\n\t\tmem->slots[i].list = ++count;\n\t\tmem->slots[i].orig_addr = INVALID_PHYS_ADDR;\n\t\tmem->slots[i].alloc_size = 0;\n\t}\n\n\t \n\tfor (i = index - 1;\n\t     io_tlb_offset(i) != IO_TLB_SEGSIZE - 1 && mem->slots[i].list;\n\t     i--)\n\t\tmem->slots[i].list = ++count;\n\tarea->used -= nslots;\n\tspin_unlock_irqrestore(&area->lock, flags);\n\n\tdec_used(dev->dma_io_tlb_mem, nslots);\n}\n\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\n \nstatic bool swiotlb_del_transient(struct device *dev, phys_addr_t tlb_addr)\n{\n\tstruct io_tlb_pool *pool;\n\n\tpool = swiotlb_find_pool(dev, tlb_addr);\n\tif (!pool->transient)\n\t\treturn false;\n\n\tdec_used(dev->dma_io_tlb_mem, pool->nslabs);\n\tswiotlb_del_pool(dev, pool);\n\treturn true;\n}\n\n#else   \n\nstatic inline bool swiotlb_del_transient(struct device *dev,\n\t\t\t\t\t phys_addr_t tlb_addr)\n{\n\treturn false;\n}\n\n#endif\t \n\n \nvoid swiotlb_tbl_unmap_single(struct device *dev, phys_addr_t tlb_addr,\n\t\t\t      size_t mapping_size, enum dma_data_direction dir,\n\t\t\t      unsigned long attrs)\n{\n\t \n\tif (!(attrs & DMA_ATTR_SKIP_CPU_SYNC) &&\n\t    (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL))\n\t\tswiotlb_bounce(dev, tlb_addr, mapping_size, DMA_FROM_DEVICE);\n\n\tif (swiotlb_del_transient(dev, tlb_addr))\n\t\treturn;\n\tswiotlb_release_slots(dev, tlb_addr);\n}\n\nvoid swiotlb_sync_single_for_device(struct device *dev, phys_addr_t tlb_addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL)\n\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_TO_DEVICE);\n\telse\n\t\tBUG_ON(dir != DMA_FROM_DEVICE);\n}\n\nvoid swiotlb_sync_single_for_cpu(struct device *dev, phys_addr_t tlb_addr,\n\t\tsize_t size, enum dma_data_direction dir)\n{\n\tif (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL)\n\t\tswiotlb_bounce(dev, tlb_addr, size, DMA_FROM_DEVICE);\n\telse\n\t\tBUG_ON(dir != DMA_TO_DEVICE);\n}\n\n \ndma_addr_t swiotlb_map(struct device *dev, phys_addr_t paddr, size_t size,\n\t\tenum dma_data_direction dir, unsigned long attrs)\n{\n\tphys_addr_t swiotlb_addr;\n\tdma_addr_t dma_addr;\n\n\ttrace_swiotlb_bounced(dev, phys_to_dma(dev, paddr), size);\n\n\tswiotlb_addr = swiotlb_tbl_map_single(dev, paddr, size, size, 0, dir,\n\t\t\tattrs);\n\tif (swiotlb_addr == (phys_addr_t)DMA_MAPPING_ERROR)\n\t\treturn DMA_MAPPING_ERROR;\n\n\t \n\tdma_addr = phys_to_dma_unencrypted(dev, swiotlb_addr);\n\tif (unlikely(!dma_capable(dev, dma_addr, size, true))) {\n\t\tswiotlb_tbl_unmap_single(dev, swiotlb_addr, size, dir,\n\t\t\tattrs | DMA_ATTR_SKIP_CPU_SYNC);\n\t\tdev_WARN_ONCE(dev, 1,\n\t\t\t\"swiotlb addr %pad+%zu overflow (mask %llx, bus limit %llx).\\n\",\n\t\t\t&dma_addr, size, *dev->dma_mask, dev->bus_dma_limit);\n\t\treturn DMA_MAPPING_ERROR;\n\t}\n\n\tif (!dev_is_dma_coherent(dev) && !(attrs & DMA_ATTR_SKIP_CPU_SYNC))\n\t\tarch_sync_dma_for_device(swiotlb_addr, size, dir);\n\treturn dma_addr;\n}\n\nsize_t swiotlb_max_mapping_size(struct device *dev)\n{\n\tint min_align_mask = dma_get_min_align_mask(dev);\n\tint min_align = 0;\n\n\t \n\tif (min_align_mask)\n\t\tmin_align = roundup(min_align_mask, IO_TLB_SIZE);\n\n\treturn ((size_t)IO_TLB_SIZE) * IO_TLB_SEGSIZE - min_align;\n}\n\n \nbool is_swiotlb_allocated(void)\n{\n\treturn io_tlb_default_mem.nslabs;\n}\n\nbool is_swiotlb_active(struct device *dev)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\n\treturn mem && mem->nslabs;\n}\n\n \nphys_addr_t default_swiotlb_base(void)\n{\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\tio_tlb_default_mem.can_grow = false;\n#endif\n\treturn io_tlb_default_mem.defpool.start;\n}\n\n \nphys_addr_t default_swiotlb_limit(void)\n{\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\treturn io_tlb_default_mem.phys_limit;\n#else\n\treturn io_tlb_default_mem.defpool.end - 1;\n#endif\n}\n\n#ifdef CONFIG_DEBUG_FS\n\nstatic int io_tlb_used_get(void *data, u64 *val)\n{\n\tstruct io_tlb_mem *mem = data;\n\n\t*val = mem_used(mem);\n\treturn 0;\n}\n\nstatic int io_tlb_hiwater_get(void *data, u64 *val)\n{\n\tstruct io_tlb_mem *mem = data;\n\n\t*val = atomic_long_read(&mem->used_hiwater);\n\treturn 0;\n}\n\nstatic int io_tlb_hiwater_set(void *data, u64 val)\n{\n\tstruct io_tlb_mem *mem = data;\n\n\t \n\tif (val != 0)\n\t\treturn -EINVAL;\n\n\tatomic_long_set(&mem->used_hiwater, val);\n\treturn 0;\n}\n\nDEFINE_DEBUGFS_ATTRIBUTE(fops_io_tlb_used, io_tlb_used_get, NULL, \"%llu\\n\");\nDEFINE_DEBUGFS_ATTRIBUTE(fops_io_tlb_hiwater, io_tlb_hiwater_get,\n\t\t\t\tio_tlb_hiwater_set, \"%llu\\n\");\n\nstatic void swiotlb_create_debugfs_files(struct io_tlb_mem *mem,\n\t\t\t\t\t const char *dirname)\n{\n\tatomic_long_set(&mem->total_used, 0);\n\tatomic_long_set(&mem->used_hiwater, 0);\n\n\tmem->debugfs = debugfs_create_dir(dirname, io_tlb_default_mem.debugfs);\n\tif (!mem->nslabs)\n\t\treturn;\n\n\tdebugfs_create_ulong(\"io_tlb_nslabs\", 0400, mem->debugfs, &mem->nslabs);\n\tdebugfs_create_file(\"io_tlb_used\", 0400, mem->debugfs, mem,\n\t\t\t&fops_io_tlb_used);\n\tdebugfs_create_file(\"io_tlb_used_hiwater\", 0600, mem->debugfs, mem,\n\t\t\t&fops_io_tlb_hiwater);\n}\n\nstatic int __init swiotlb_create_default_debugfs(void)\n{\n\tswiotlb_create_debugfs_files(&io_tlb_default_mem, \"swiotlb\");\n\treturn 0;\n}\n\nlate_initcall(swiotlb_create_default_debugfs);\n\n#else   \n\nstatic inline void swiotlb_create_debugfs_files(struct io_tlb_mem *mem,\n\t\t\t\t\t\tconst char *dirname)\n{\n}\n\n#endif\t \n\n#ifdef CONFIG_DMA_RESTRICTED_POOL\n\nstruct page *swiotlb_alloc(struct device *dev, size_t size)\n{\n\tstruct io_tlb_mem *mem = dev->dma_io_tlb_mem;\n\tstruct io_tlb_pool *pool;\n\tphys_addr_t tlb_addr;\n\tint index;\n\n\tif (!mem)\n\t\treturn NULL;\n\n\tindex = swiotlb_find_slots(dev, 0, size, 0, &pool);\n\tif (index == -1)\n\t\treturn NULL;\n\n\ttlb_addr = slot_addr(pool->start, index);\n\n\treturn pfn_to_page(PFN_DOWN(tlb_addr));\n}\n\nbool swiotlb_free(struct device *dev, struct page *page, size_t size)\n{\n\tphys_addr_t tlb_addr = page_to_phys(page);\n\n\tif (!is_swiotlb_buffer(dev, tlb_addr))\n\t\treturn false;\n\n\tswiotlb_release_slots(dev, tlb_addr);\n\n\treturn true;\n}\n\nstatic int rmem_swiotlb_device_init(struct reserved_mem *rmem,\n\t\t\t\t    struct device *dev)\n{\n\tstruct io_tlb_mem *mem = rmem->priv;\n\tunsigned long nslabs = rmem->size >> IO_TLB_SHIFT;\n\n\t \n\tunsigned int nareas = 1;\n\n\tif (PageHighMem(pfn_to_page(PHYS_PFN(rmem->base)))) {\n\t\tdev_err(dev, \"Restricted DMA pool must be accessible within the linear mapping.\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!mem) {\n\t\tstruct io_tlb_pool *pool;\n\n\t\tmem = kzalloc(sizeof(*mem), GFP_KERNEL);\n\t\tif (!mem)\n\t\t\treturn -ENOMEM;\n\t\tpool = &mem->defpool;\n\n\t\tpool->slots = kcalloc(nslabs, sizeof(*pool->slots), GFP_KERNEL);\n\t\tif (!pool->slots) {\n\t\t\tkfree(mem);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tpool->areas = kcalloc(nareas, sizeof(*pool->areas),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!pool->areas) {\n\t\t\tkfree(pool->slots);\n\t\t\tkfree(mem);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tset_memory_decrypted((unsigned long)phys_to_virt(rmem->base),\n\t\t\t\t     rmem->size >> PAGE_SHIFT);\n\t\tswiotlb_init_io_tlb_pool(pool, rmem->base, nslabs,\n\t\t\t\t\t false, nareas);\n\t\tmem->force_bounce = true;\n\t\tmem->for_alloc = true;\n#ifdef CONFIG_SWIOTLB_DYNAMIC\n\t\tspin_lock_init(&mem->lock);\n#endif\n\t\tadd_mem_pool(mem, pool);\n\n\t\trmem->priv = mem;\n\n\t\tswiotlb_create_debugfs_files(mem, rmem->name);\n\t}\n\n\tdev->dma_io_tlb_mem = mem;\n\n\treturn 0;\n}\n\nstatic void rmem_swiotlb_device_release(struct reserved_mem *rmem,\n\t\t\t\t\tstruct device *dev)\n{\n\tdev->dma_io_tlb_mem = &io_tlb_default_mem;\n}\n\nstatic const struct reserved_mem_ops rmem_swiotlb_ops = {\n\t.device_init = rmem_swiotlb_device_init,\n\t.device_release = rmem_swiotlb_device_release,\n};\n\nstatic int __init rmem_swiotlb_setup(struct reserved_mem *rmem)\n{\n\tunsigned long node = rmem->fdt_node;\n\n\tif (of_get_flat_dt_prop(node, \"reusable\", NULL) ||\n\t    of_get_flat_dt_prop(node, \"linux,cma-default\", NULL) ||\n\t    of_get_flat_dt_prop(node, \"linux,dma-default\", NULL) ||\n\t    of_get_flat_dt_prop(node, \"no-map\", NULL))\n\t\treturn -EINVAL;\n\n\trmem->ops = &rmem_swiotlb_ops;\n\tpr_info(\"Reserved memory: created restricted DMA pool at %pa, size %ld MiB\\n\",\n\t\t&rmem->base, (unsigned long)rmem->size / SZ_1M);\n\treturn 0;\n}\n\nRESERVEDMEM_OF_DECLARE(dma, \"restricted-dma-pool\", rmem_swiotlb_setup);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}