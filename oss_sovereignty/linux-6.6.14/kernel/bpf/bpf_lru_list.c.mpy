{
  "module_name": "bpf_lru_list.c",
  "hash_id": "e185bade708b41849505547982cb2204d635b2cbc71de6b7494c2cf677161a29",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/bpf_lru_list.c",
  "human_readable_source": "\n \n#include <linux/cpumask.h>\n#include <linux/spinlock.h>\n#include <linux/percpu.h>\n\n#include \"bpf_lru_list.h\"\n\n#define LOCAL_FREE_TARGET\t\t(128)\n#define LOCAL_NR_SCANS\t\t\tLOCAL_FREE_TARGET\n\n#define PERCPU_FREE_TARGET\t\t(4)\n#define PERCPU_NR_SCANS\t\t\tPERCPU_FREE_TARGET\n\n \n#define LOCAL_LIST_IDX(t)\t((t) - BPF_LOCAL_LIST_T_OFFSET)\n#define LOCAL_FREE_LIST_IDX\tLOCAL_LIST_IDX(BPF_LRU_LOCAL_LIST_T_FREE)\n#define LOCAL_PENDING_LIST_IDX\tLOCAL_LIST_IDX(BPF_LRU_LOCAL_LIST_T_PENDING)\n#define IS_LOCAL_LIST_TYPE(t)\t((t) >= BPF_LOCAL_LIST_T_OFFSET)\n\nstatic int get_next_cpu(int cpu)\n{\n\tcpu = cpumask_next(cpu, cpu_possible_mask);\n\tif (cpu >= nr_cpu_ids)\n\t\tcpu = cpumask_first(cpu_possible_mask);\n\treturn cpu;\n}\n\n \nstatic struct list_head *local_free_list(struct bpf_lru_locallist *loc_l)\n{\n\treturn &loc_l->lists[LOCAL_FREE_LIST_IDX];\n}\n\nstatic struct list_head *local_pending_list(struct bpf_lru_locallist *loc_l)\n{\n\treturn &loc_l->lists[LOCAL_PENDING_LIST_IDX];\n}\n\n \nstatic bool bpf_lru_node_is_ref(const struct bpf_lru_node *node)\n{\n\treturn READ_ONCE(node->ref);\n}\n\nstatic void bpf_lru_node_clear_ref(struct bpf_lru_node *node)\n{\n\tWRITE_ONCE(node->ref, 0);\n}\n\nstatic void bpf_lru_list_count_inc(struct bpf_lru_list *l,\n\t\t\t\t   enum bpf_lru_list_type type)\n{\n\tif (type < NR_BPF_LRU_LIST_COUNT)\n\t\tl->counts[type]++;\n}\n\nstatic void bpf_lru_list_count_dec(struct bpf_lru_list *l,\n\t\t\t\t   enum bpf_lru_list_type type)\n{\n\tif (type < NR_BPF_LRU_LIST_COUNT)\n\t\tl->counts[type]--;\n}\n\nstatic void __bpf_lru_node_move_to_free(struct bpf_lru_list *l,\n\t\t\t\t\tstruct bpf_lru_node *node,\n\t\t\t\t\tstruct list_head *free_list,\n\t\t\t\t\tenum bpf_lru_list_type tgt_free_type)\n{\n\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))\n\t\treturn;\n\n\t \n\tif (&node->list == l->next_inactive_rotation)\n\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;\n\n\tbpf_lru_list_count_dec(l, node->type);\n\n\tnode->type = tgt_free_type;\n\tlist_move(&node->list, free_list);\n}\n\n \nstatic void __bpf_lru_node_move_in(struct bpf_lru_list *l,\n\t\t\t\t   struct bpf_lru_node *node,\n\t\t\t\t   enum bpf_lru_list_type tgt_type)\n{\n\tif (WARN_ON_ONCE(!IS_LOCAL_LIST_TYPE(node->type)) ||\n\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))\n\t\treturn;\n\n\tbpf_lru_list_count_inc(l, tgt_type);\n\tnode->type = tgt_type;\n\tbpf_lru_node_clear_ref(node);\n\tlist_move(&node->list, &l->lists[tgt_type]);\n}\n\n \nstatic void __bpf_lru_node_move(struct bpf_lru_list *l,\n\t\t\t\tstruct bpf_lru_node *node,\n\t\t\t\tenum bpf_lru_list_type tgt_type)\n{\n\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)) ||\n\t    WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(tgt_type)))\n\t\treturn;\n\n\tif (node->type != tgt_type) {\n\t\tbpf_lru_list_count_dec(l, node->type);\n\t\tbpf_lru_list_count_inc(l, tgt_type);\n\t\tnode->type = tgt_type;\n\t}\n\tbpf_lru_node_clear_ref(node);\n\n\t \n\tif (&node->list == l->next_inactive_rotation)\n\t\tl->next_inactive_rotation = l->next_inactive_rotation->prev;\n\n\tlist_move(&node->list, &l->lists[tgt_type]);\n}\n\nstatic bool bpf_lru_list_inactive_low(const struct bpf_lru_list *l)\n{\n\treturn l->counts[BPF_LRU_LIST_T_INACTIVE] <\n\t\tl->counts[BPF_LRU_LIST_T_ACTIVE];\n}\n\n \nstatic void __bpf_lru_list_rotate_active(struct bpf_lru *lru,\n\t\t\t\t\t struct bpf_lru_list *l)\n{\n\tstruct list_head *active = &l->lists[BPF_LRU_LIST_T_ACTIVE];\n\tstruct bpf_lru_node *node, *tmp_node, *first_node;\n\tunsigned int i = 0;\n\n\tfirst_node = list_first_entry(active, struct bpf_lru_node, list);\n\tlist_for_each_entry_safe_reverse(node, tmp_node, active, list) {\n\t\tif (bpf_lru_node_is_ref(node))\n\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);\n\t\telse\n\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);\n\n\t\tif (++i == lru->nr_scans || node == first_node)\n\t\t\tbreak;\n\t}\n}\n\n \nstatic void __bpf_lru_list_rotate_inactive(struct bpf_lru *lru,\n\t\t\t\t\t   struct bpf_lru_list *l)\n{\n\tstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];\n\tstruct list_head *cur, *last, *next = inactive;\n\tstruct bpf_lru_node *node;\n\tunsigned int i = 0;\n\n\tif (list_empty(inactive))\n\t\treturn;\n\n\tlast = l->next_inactive_rotation->next;\n\tif (last == inactive)\n\t\tlast = last->next;\n\n\tcur = l->next_inactive_rotation;\n\twhile (i < lru->nr_scans) {\n\t\tif (cur == inactive) {\n\t\t\tcur = cur->prev;\n\t\t\tcontinue;\n\t\t}\n\n\t\tnode = list_entry(cur, struct bpf_lru_node, list);\n\t\tnext = cur->prev;\n\t\tif (bpf_lru_node_is_ref(node))\n\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);\n\t\tif (cur == last)\n\t\t\tbreak;\n\t\tcur = next;\n\t\ti++;\n\t}\n\n\tl->next_inactive_rotation = next;\n}\n\n \nstatic unsigned int\n__bpf_lru_list_shrink_inactive(struct bpf_lru *lru,\n\t\t\t       struct bpf_lru_list *l,\n\t\t\t       unsigned int tgt_nshrink,\n\t\t\t       struct list_head *free_list,\n\t\t\t       enum bpf_lru_list_type tgt_free_type)\n{\n\tstruct list_head *inactive = &l->lists[BPF_LRU_LIST_T_INACTIVE];\n\tstruct bpf_lru_node *node, *tmp_node;\n\tunsigned int nshrinked = 0;\n\tunsigned int i = 0;\n\n\tlist_for_each_entry_safe_reverse(node, tmp_node, inactive, list) {\n\t\tif (bpf_lru_node_is_ref(node)) {\n\t\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_ACTIVE);\n\t\t} else if (lru->del_from_htab(lru->del_arg, node)) {\n\t\t\t__bpf_lru_node_move_to_free(l, node, free_list,\n\t\t\t\t\t\t    tgt_free_type);\n\t\t\tif (++nshrinked == tgt_nshrink)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (++i == lru->nr_scans)\n\t\t\tbreak;\n\t}\n\n\treturn nshrinked;\n}\n\n \nstatic void __bpf_lru_list_rotate(struct bpf_lru *lru, struct bpf_lru_list *l)\n{\n\tif (bpf_lru_list_inactive_low(l))\n\t\t__bpf_lru_list_rotate_active(lru, l);\n\n\t__bpf_lru_list_rotate_inactive(lru, l);\n}\n\n \nstatic unsigned int __bpf_lru_list_shrink(struct bpf_lru *lru,\n\t\t\t\t\t  struct bpf_lru_list *l,\n\t\t\t\t\t  unsigned int tgt_nshrink,\n\t\t\t\t\t  struct list_head *free_list,\n\t\t\t\t\t  enum bpf_lru_list_type tgt_free_type)\n\n{\n\tstruct bpf_lru_node *node, *tmp_node;\n\tstruct list_head *force_shrink_list;\n\tunsigned int nshrinked;\n\n\tnshrinked = __bpf_lru_list_shrink_inactive(lru, l, tgt_nshrink,\n\t\t\t\t\t\t   free_list, tgt_free_type);\n\tif (nshrinked)\n\t\treturn nshrinked;\n\n\t \n\tif (!list_empty(&l->lists[BPF_LRU_LIST_T_INACTIVE]))\n\t\tforce_shrink_list = &l->lists[BPF_LRU_LIST_T_INACTIVE];\n\telse\n\t\tforce_shrink_list = &l->lists[BPF_LRU_LIST_T_ACTIVE];\n\n\tlist_for_each_entry_safe_reverse(node, tmp_node, force_shrink_list,\n\t\t\t\t\t list) {\n\t\tif (lru->del_from_htab(lru->del_arg, node)) {\n\t\t\t__bpf_lru_node_move_to_free(l, node, free_list,\n\t\t\t\t\t\t    tgt_free_type);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void __local_list_flush(struct bpf_lru_list *l,\n\t\t\t       struct bpf_lru_locallist *loc_l)\n{\n\tstruct bpf_lru_node *node, *tmp_node;\n\n\tlist_for_each_entry_safe_reverse(node, tmp_node,\n\t\t\t\t\t local_pending_list(loc_l), list) {\n\t\tif (bpf_lru_node_is_ref(node))\n\t\t\t__bpf_lru_node_move_in(l, node, BPF_LRU_LIST_T_ACTIVE);\n\t\telse\n\t\t\t__bpf_lru_node_move_in(l, node,\n\t\t\t\t\t       BPF_LRU_LIST_T_INACTIVE);\n\t}\n}\n\nstatic void bpf_lru_list_push_free(struct bpf_lru_list *l,\n\t\t\t\t   struct bpf_lru_node *node)\n{\n\tunsigned long flags;\n\n\tif (WARN_ON_ONCE(IS_LOCAL_LIST_TYPE(node->type)))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&l->lock, flags);\n\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);\n\traw_spin_unlock_irqrestore(&l->lock, flags);\n}\n\nstatic void bpf_lru_list_pop_free_to_local(struct bpf_lru *lru,\n\t\t\t\t\t   struct bpf_lru_locallist *loc_l)\n{\n\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;\n\tstruct bpf_lru_node *node, *tmp_node;\n\tunsigned int nfree = 0;\n\n\traw_spin_lock(&l->lock);\n\n\t__local_list_flush(l, loc_l);\n\n\t__bpf_lru_list_rotate(lru, l);\n\n\tlist_for_each_entry_safe(node, tmp_node, &l->lists[BPF_LRU_LIST_T_FREE],\n\t\t\t\t list) {\n\t\t__bpf_lru_node_move_to_free(l, node, local_free_list(loc_l),\n\t\t\t\t\t    BPF_LRU_LOCAL_LIST_T_FREE);\n\t\tif (++nfree == LOCAL_FREE_TARGET)\n\t\t\tbreak;\n\t}\n\n\tif (nfree < LOCAL_FREE_TARGET)\n\t\t__bpf_lru_list_shrink(lru, l, LOCAL_FREE_TARGET - nfree,\n\t\t\t\t      local_free_list(loc_l),\n\t\t\t\t      BPF_LRU_LOCAL_LIST_T_FREE);\n\n\traw_spin_unlock(&l->lock);\n}\n\nstatic void __local_list_add_pending(struct bpf_lru *lru,\n\t\t\t\t     struct bpf_lru_locallist *loc_l,\n\t\t\t\t     int cpu,\n\t\t\t\t     struct bpf_lru_node *node,\n\t\t\t\t     u32 hash)\n{\n\t*(u32 *)((void *)node + lru->hash_offset) = hash;\n\tnode->cpu = cpu;\n\tnode->type = BPF_LRU_LOCAL_LIST_T_PENDING;\n\tbpf_lru_node_clear_ref(node);\n\tlist_add(&node->list, local_pending_list(loc_l));\n}\n\nstatic struct bpf_lru_node *\n__local_list_pop_free(struct bpf_lru_locallist *loc_l)\n{\n\tstruct bpf_lru_node *node;\n\n\tnode = list_first_entry_or_null(local_free_list(loc_l),\n\t\t\t\t\tstruct bpf_lru_node,\n\t\t\t\t\tlist);\n\tif (node)\n\t\tlist_del(&node->list);\n\n\treturn node;\n}\n\nstatic struct bpf_lru_node *\n__local_list_pop_pending(struct bpf_lru *lru, struct bpf_lru_locallist *loc_l)\n{\n\tstruct bpf_lru_node *node;\n\tbool force = false;\n\nignore_ref:\n\t \n\tlist_for_each_entry_reverse(node, local_pending_list(loc_l),\n\t\t\t\t    list) {\n\t\tif ((!bpf_lru_node_is_ref(node) || force) &&\n\t\t    lru->del_from_htab(lru->del_arg, node)) {\n\t\t\tlist_del(&node->list);\n\t\t\treturn node;\n\t\t}\n\t}\n\n\tif (!force) {\n\t\tforce = true;\n\t\tgoto ignore_ref;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct bpf_lru_node *bpf_percpu_lru_pop_free(struct bpf_lru *lru,\n\t\t\t\t\t\t    u32 hash)\n{\n\tstruct list_head *free_list;\n\tstruct bpf_lru_node *node = NULL;\n\tstruct bpf_lru_list *l;\n\tunsigned long flags;\n\tint cpu = raw_smp_processor_id();\n\n\tl = per_cpu_ptr(lru->percpu_lru, cpu);\n\n\traw_spin_lock_irqsave(&l->lock, flags);\n\n\t__bpf_lru_list_rotate(lru, l);\n\n\tfree_list = &l->lists[BPF_LRU_LIST_T_FREE];\n\tif (list_empty(free_list))\n\t\t__bpf_lru_list_shrink(lru, l, PERCPU_FREE_TARGET, free_list,\n\t\t\t\t      BPF_LRU_LIST_T_FREE);\n\n\tif (!list_empty(free_list)) {\n\t\tnode = list_first_entry(free_list, struct bpf_lru_node, list);\n\t\t*(u32 *)((void *)node + lru->hash_offset) = hash;\n\t\tbpf_lru_node_clear_ref(node);\n\t\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_INACTIVE);\n\t}\n\n\traw_spin_unlock_irqrestore(&l->lock, flags);\n\n\treturn node;\n}\n\nstatic struct bpf_lru_node *bpf_common_lru_pop_free(struct bpf_lru *lru,\n\t\t\t\t\t\t    u32 hash)\n{\n\tstruct bpf_lru_locallist *loc_l, *steal_loc_l;\n\tstruct bpf_common_lru *clru = &lru->common_lru;\n\tstruct bpf_lru_node *node;\n\tint steal, first_steal;\n\tunsigned long flags;\n\tint cpu = raw_smp_processor_id();\n\n\tloc_l = per_cpu_ptr(clru->local_list, cpu);\n\n\traw_spin_lock_irqsave(&loc_l->lock, flags);\n\n\tnode = __local_list_pop_free(loc_l);\n\tif (!node) {\n\t\tbpf_lru_list_pop_free_to_local(lru, loc_l);\n\t\tnode = __local_list_pop_free(loc_l);\n\t}\n\n\tif (node)\n\t\t__local_list_add_pending(lru, loc_l, cpu, node, hash);\n\n\traw_spin_unlock_irqrestore(&loc_l->lock, flags);\n\n\tif (node)\n\t\treturn node;\n\n\t \n\n\tfirst_steal = loc_l->next_steal;\n\tsteal = first_steal;\n\tdo {\n\t\tsteal_loc_l = per_cpu_ptr(clru->local_list, steal);\n\n\t\traw_spin_lock_irqsave(&steal_loc_l->lock, flags);\n\n\t\tnode = __local_list_pop_free(steal_loc_l);\n\t\tif (!node)\n\t\t\tnode = __local_list_pop_pending(lru, steal_loc_l);\n\n\t\traw_spin_unlock_irqrestore(&steal_loc_l->lock, flags);\n\n\t\tsteal = get_next_cpu(steal);\n\t} while (!node && steal != first_steal);\n\n\tloc_l->next_steal = steal;\n\n\tif (node) {\n\t\traw_spin_lock_irqsave(&loc_l->lock, flags);\n\t\t__local_list_add_pending(lru, loc_l, cpu, node, hash);\n\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);\n\t}\n\n\treturn node;\n}\n\nstruct bpf_lru_node *bpf_lru_pop_free(struct bpf_lru *lru, u32 hash)\n{\n\tif (lru->percpu)\n\t\treturn bpf_percpu_lru_pop_free(lru, hash);\n\telse\n\t\treturn bpf_common_lru_pop_free(lru, hash);\n}\n\nstatic void bpf_common_lru_push_free(struct bpf_lru *lru,\n\t\t\t\t     struct bpf_lru_node *node)\n{\n\tu8 node_type = READ_ONCE(node->type);\n\tunsigned long flags;\n\n\tif (WARN_ON_ONCE(node_type == BPF_LRU_LIST_T_FREE) ||\n\t    WARN_ON_ONCE(node_type == BPF_LRU_LOCAL_LIST_T_FREE))\n\t\treturn;\n\n\tif (node_type == BPF_LRU_LOCAL_LIST_T_PENDING) {\n\t\tstruct bpf_lru_locallist *loc_l;\n\n\t\tloc_l = per_cpu_ptr(lru->common_lru.local_list, node->cpu);\n\n\t\traw_spin_lock_irqsave(&loc_l->lock, flags);\n\n\t\tif (unlikely(node->type != BPF_LRU_LOCAL_LIST_T_PENDING)) {\n\t\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);\n\t\t\tgoto check_lru_list;\n\t\t}\n\n\t\tnode->type = BPF_LRU_LOCAL_LIST_T_FREE;\n\t\tbpf_lru_node_clear_ref(node);\n\t\tlist_move(&node->list, local_free_list(loc_l));\n\n\t\traw_spin_unlock_irqrestore(&loc_l->lock, flags);\n\t\treturn;\n\t}\n\ncheck_lru_list:\n\tbpf_lru_list_push_free(&lru->common_lru.lru_list, node);\n}\n\nstatic void bpf_percpu_lru_push_free(struct bpf_lru *lru,\n\t\t\t\t     struct bpf_lru_node *node)\n{\n\tstruct bpf_lru_list *l;\n\tunsigned long flags;\n\n\tl = per_cpu_ptr(lru->percpu_lru, node->cpu);\n\n\traw_spin_lock_irqsave(&l->lock, flags);\n\n\t__bpf_lru_node_move(l, node, BPF_LRU_LIST_T_FREE);\n\n\traw_spin_unlock_irqrestore(&l->lock, flags);\n}\n\nvoid bpf_lru_push_free(struct bpf_lru *lru, struct bpf_lru_node *node)\n{\n\tif (lru->percpu)\n\t\tbpf_percpu_lru_push_free(lru, node);\n\telse\n\t\tbpf_common_lru_push_free(lru, node);\n}\n\nstatic void bpf_common_lru_populate(struct bpf_lru *lru, void *buf,\n\t\t\t\t    u32 node_offset, u32 elem_size,\n\t\t\t\t    u32 nr_elems)\n{\n\tstruct bpf_lru_list *l = &lru->common_lru.lru_list;\n\tu32 i;\n\n\tfor (i = 0; i < nr_elems; i++) {\n\t\tstruct bpf_lru_node *node;\n\n\t\tnode = (struct bpf_lru_node *)(buf + node_offset);\n\t\tnode->type = BPF_LRU_LIST_T_FREE;\n\t\tbpf_lru_node_clear_ref(node);\n\t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);\n\t\tbuf += elem_size;\n\t}\n}\n\nstatic void bpf_percpu_lru_populate(struct bpf_lru *lru, void *buf,\n\t\t\t\t    u32 node_offset, u32 elem_size,\n\t\t\t\t    u32 nr_elems)\n{\n\tu32 i, pcpu_entries;\n\tint cpu;\n\tstruct bpf_lru_list *l;\n\n\tpcpu_entries = nr_elems / num_possible_cpus();\n\n\ti = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct bpf_lru_node *node;\n\n\t\tl = per_cpu_ptr(lru->percpu_lru, cpu);\nagain:\n\t\tnode = (struct bpf_lru_node *)(buf + node_offset);\n\t\tnode->cpu = cpu;\n\t\tnode->type = BPF_LRU_LIST_T_FREE;\n\t\tbpf_lru_node_clear_ref(node);\n\t\tlist_add(&node->list, &l->lists[BPF_LRU_LIST_T_FREE]);\n\t\ti++;\n\t\tbuf += elem_size;\n\t\tif (i == nr_elems)\n\t\t\tbreak;\n\t\tif (i % pcpu_entries)\n\t\t\tgoto again;\n\t}\n}\n\nvoid bpf_lru_populate(struct bpf_lru *lru, void *buf, u32 node_offset,\n\t\t      u32 elem_size, u32 nr_elems)\n{\n\tif (lru->percpu)\n\t\tbpf_percpu_lru_populate(lru, buf, node_offset, elem_size,\n\t\t\t\t\tnr_elems);\n\telse\n\t\tbpf_common_lru_populate(lru, buf, node_offset, elem_size,\n\t\t\t\t\tnr_elems);\n}\n\nstatic void bpf_lru_locallist_init(struct bpf_lru_locallist *loc_l, int cpu)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_BPF_LRU_LOCAL_LIST_T; i++)\n\t\tINIT_LIST_HEAD(&loc_l->lists[i]);\n\n\tloc_l->next_steal = cpu;\n\n\traw_spin_lock_init(&loc_l->lock);\n}\n\nstatic void bpf_lru_list_init(struct bpf_lru_list *l)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_BPF_LRU_LIST_T; i++)\n\t\tINIT_LIST_HEAD(&l->lists[i]);\n\n\tfor (i = 0; i < NR_BPF_LRU_LIST_COUNT; i++)\n\t\tl->counts[i] = 0;\n\n\tl->next_inactive_rotation = &l->lists[BPF_LRU_LIST_T_INACTIVE];\n\n\traw_spin_lock_init(&l->lock);\n}\n\nint bpf_lru_init(struct bpf_lru *lru, bool percpu, u32 hash_offset,\n\t\t del_from_htab_func del_from_htab, void *del_arg)\n{\n\tint cpu;\n\n\tif (percpu) {\n\t\tlru->percpu_lru = alloc_percpu(struct bpf_lru_list);\n\t\tif (!lru->percpu_lru)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct bpf_lru_list *l;\n\n\t\t\tl = per_cpu_ptr(lru->percpu_lru, cpu);\n\t\t\tbpf_lru_list_init(l);\n\t\t}\n\t\tlru->nr_scans = PERCPU_NR_SCANS;\n\t} else {\n\t\tstruct bpf_common_lru *clru = &lru->common_lru;\n\n\t\tclru->local_list = alloc_percpu(struct bpf_lru_locallist);\n\t\tif (!clru->local_list)\n\t\t\treturn -ENOMEM;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct bpf_lru_locallist *loc_l;\n\n\t\t\tloc_l = per_cpu_ptr(clru->local_list, cpu);\n\t\t\tbpf_lru_locallist_init(loc_l, cpu);\n\t\t}\n\n\t\tbpf_lru_list_init(&clru->lru_list);\n\t\tlru->nr_scans = LOCAL_NR_SCANS;\n\t}\n\n\tlru->percpu = percpu;\n\tlru->del_from_htab = del_from_htab;\n\tlru->del_arg = del_arg;\n\tlru->hash_offset = hash_offset;\n\n\treturn 0;\n}\n\nvoid bpf_lru_destroy(struct bpf_lru *lru)\n{\n\tif (lru->percpu)\n\t\tfree_percpu(lru->percpu_lru);\n\telse\n\t\tfree_percpu(lru->common_lru.local_list);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}