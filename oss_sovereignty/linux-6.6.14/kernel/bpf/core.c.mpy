{
  "module_name": "core.c",
  "hash_id": "c8ef7737f7667f84e8c6171210b23ae7876244aa3985b6646bb345db9b8b681b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/core.c",
  "human_readable_source": "\n \n\n#include <uapi/linux/btf.h>\n#include <linux/filter.h>\n#include <linux/skbuff.h>\n#include <linux/vmalloc.h>\n#include <linux/random.h>\n#include <linux/moduleloader.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/objtool.h>\n#include <linux/rbtree_latch.h>\n#include <linux/kallsyms.h>\n#include <linux/rcupdate.h>\n#include <linux/perf_event.h>\n#include <linux/extable.h>\n#include <linux/log2.h>\n#include <linux/bpf_verifier.h>\n#include <linux/nodemask.h>\n#include <linux/nospec.h>\n#include <linux/bpf_mem_alloc.h>\n#include <linux/memcontrol.h>\n\n#include <asm/barrier.h>\n#include <asm/unaligned.h>\n\n \n#define BPF_R0\tregs[BPF_REG_0]\n#define BPF_R1\tregs[BPF_REG_1]\n#define BPF_R2\tregs[BPF_REG_2]\n#define BPF_R3\tregs[BPF_REG_3]\n#define BPF_R4\tregs[BPF_REG_4]\n#define BPF_R5\tregs[BPF_REG_5]\n#define BPF_R6\tregs[BPF_REG_6]\n#define BPF_R7\tregs[BPF_REG_7]\n#define BPF_R8\tregs[BPF_REG_8]\n#define BPF_R9\tregs[BPF_REG_9]\n#define BPF_R10\tregs[BPF_REG_10]\n\n \n#define DST\tregs[insn->dst_reg]\n#define SRC\tregs[insn->src_reg]\n#define FP\tregs[BPF_REG_FP]\n#define AX\tregs[BPF_REG_AX]\n#define ARG1\tregs[BPF_REG_ARG1]\n#define CTX\tregs[BPF_REG_CTX]\n#define OFF\tinsn->off\n#define IMM\tinsn->imm\n\nstruct bpf_mem_alloc bpf_global_ma;\nbool bpf_global_ma_set;\n\n \nvoid *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb, int k, unsigned int size)\n{\n\tu8 *ptr = NULL;\n\n\tif (k >= SKF_NET_OFF) {\n\t\tptr = skb_network_header(skb) + k - SKF_NET_OFF;\n\t} else if (k >= SKF_LL_OFF) {\n\t\tif (unlikely(!skb_mac_header_was_set(skb)))\n\t\t\treturn NULL;\n\t\tptr = skb_mac_header(skb) + k - SKF_LL_OFF;\n\t}\n\tif (ptr >= skb->head && ptr + size <= skb_tail_pointer(skb))\n\t\treturn ptr;\n\n\treturn NULL;\n}\n\nstruct bpf_prog *bpf_prog_alloc_no_stats(unsigned int size, gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = bpf_memcg_flags(GFP_KERNEL | __GFP_ZERO | gfp_extra_flags);\n\tstruct bpf_prog_aux *aux;\n\tstruct bpf_prog *fp;\n\n\tsize = round_up(size, PAGE_SIZE);\n\tfp = __vmalloc(size, gfp_flags);\n\tif (fp == NULL)\n\t\treturn NULL;\n\n\taux = kzalloc(sizeof(*aux), bpf_memcg_flags(GFP_KERNEL | gfp_extra_flags));\n\tif (aux == NULL) {\n\t\tvfree(fp);\n\t\treturn NULL;\n\t}\n\tfp->active = alloc_percpu_gfp(int, bpf_memcg_flags(GFP_KERNEL | gfp_extra_flags));\n\tif (!fp->active) {\n\t\tvfree(fp);\n\t\tkfree(aux);\n\t\treturn NULL;\n\t}\n\n\tfp->pages = size / PAGE_SIZE;\n\tfp->aux = aux;\n\tfp->aux->prog = fp;\n\tfp->jit_requested = ebpf_jit_enabled();\n\tfp->blinding_requested = bpf_jit_blinding_enabled(fp);\n#ifdef CONFIG_CGROUP_BPF\n\taux->cgroup_atype = CGROUP_BPF_ATTACH_TYPE_INVALID;\n#endif\n\n\tINIT_LIST_HEAD_RCU(&fp->aux->ksym.lnode);\n\tmutex_init(&fp->aux->used_maps_mutex);\n\tmutex_init(&fp->aux->dst_mutex);\n\n\treturn fp;\n}\n\nstruct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = bpf_memcg_flags(GFP_KERNEL | __GFP_ZERO | gfp_extra_flags);\n\tstruct bpf_prog *prog;\n\tint cpu;\n\n\tprog = bpf_prog_alloc_no_stats(size, gfp_extra_flags);\n\tif (!prog)\n\t\treturn NULL;\n\n\tprog->stats = alloc_percpu_gfp(struct bpf_prog_stats, gfp_flags);\n\tif (!prog->stats) {\n\t\tfree_percpu(prog->active);\n\t\tkfree(prog->aux);\n\t\tvfree(prog);\n\t\treturn NULL;\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct bpf_prog_stats *pstats;\n\n\t\tpstats = per_cpu_ptr(prog->stats, cpu);\n\t\tu64_stats_init(&pstats->syncp);\n\t}\n\treturn prog;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_alloc);\n\nint bpf_prog_alloc_jited_linfo(struct bpf_prog *prog)\n{\n\tif (!prog->aux->nr_linfo || !prog->jit_requested)\n\t\treturn 0;\n\n\tprog->aux->jited_linfo = kvcalloc(prog->aux->nr_linfo,\n\t\t\t\t\t  sizeof(*prog->aux->jited_linfo),\n\t\t\t\t\t  bpf_memcg_flags(GFP_KERNEL | __GFP_NOWARN));\n\tif (!prog->aux->jited_linfo)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid bpf_prog_jit_attempt_done(struct bpf_prog *prog)\n{\n\tif (prog->aux->jited_linfo &&\n\t    (!prog->jited || !prog->aux->jited_linfo[0])) {\n\t\tkvfree(prog->aux->jited_linfo);\n\t\tprog->aux->jited_linfo = NULL;\n\t}\n\n\tkfree(prog->aux->kfunc_tab);\n\tprog->aux->kfunc_tab = NULL;\n}\n\n \nvoid bpf_prog_fill_jited_linfo(struct bpf_prog *prog,\n\t\t\t       const u32 *insn_to_jit_off)\n{\n\tu32 linfo_idx, insn_start, insn_end, nr_linfo, i;\n\tconst struct bpf_line_info *linfo;\n\tvoid **jited_linfo;\n\n\tif (!prog->aux->jited_linfo)\n\t\t \n\t\treturn;\n\n\tlinfo_idx = prog->aux->linfo_idx;\n\tlinfo = &prog->aux->linfo[linfo_idx];\n\tinsn_start = linfo[0].insn_off;\n\tinsn_end = insn_start + prog->len;\n\n\tjited_linfo = &prog->aux->jited_linfo[linfo_idx];\n\tjited_linfo[0] = prog->bpf_func;\n\n\tnr_linfo = prog->aux->nr_linfo - linfo_idx;\n\n\tfor (i = 1; i < nr_linfo && linfo[i].insn_off < insn_end; i++)\n\t\t \n\t\tjited_linfo[i] = prog->bpf_func +\n\t\t\tinsn_to_jit_off[linfo[i].insn_off - insn_start - 1];\n}\n\nstruct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,\n\t\t\t\t  gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = bpf_memcg_flags(GFP_KERNEL | __GFP_ZERO | gfp_extra_flags);\n\tstruct bpf_prog *fp;\n\tu32 pages;\n\n\tsize = round_up(size, PAGE_SIZE);\n\tpages = size / PAGE_SIZE;\n\tif (pages <= fp_old->pages)\n\t\treturn fp_old;\n\n\tfp = __vmalloc(size, gfp_flags);\n\tif (fp) {\n\t\tmemcpy(fp, fp_old, fp_old->pages * PAGE_SIZE);\n\t\tfp->pages = pages;\n\t\tfp->aux->prog = fp;\n\n\t\t \n\t\tfp_old->aux = NULL;\n\t\tfp_old->stats = NULL;\n\t\tfp_old->active = NULL;\n\t\t__bpf_prog_free(fp_old);\n\t}\n\n\treturn fp;\n}\n\nvoid __bpf_prog_free(struct bpf_prog *fp)\n{\n\tif (fp->aux) {\n\t\tmutex_destroy(&fp->aux->used_maps_mutex);\n\t\tmutex_destroy(&fp->aux->dst_mutex);\n\t\tkfree(fp->aux->poke_tab);\n\t\tkfree(fp->aux);\n\t}\n\tfree_percpu(fp->stats);\n\tfree_percpu(fp->active);\n\tvfree(fp);\n}\n\nint bpf_prog_calc_tag(struct bpf_prog *fp)\n{\n\tconst u32 bits_offset = SHA1_BLOCK_SIZE - sizeof(__be64);\n\tu32 raw_size = bpf_prog_tag_scratch_size(fp);\n\tu32 digest[SHA1_DIGEST_WORDS];\n\tu32 ws[SHA1_WORKSPACE_WORDS];\n\tu32 i, bsize, psize, blocks;\n\tstruct bpf_insn *dst;\n\tbool was_ld_map;\n\tu8 *raw, *todo;\n\t__be32 *result;\n\t__be64 *bits;\n\n\traw = vmalloc(raw_size);\n\tif (!raw)\n\t\treturn -ENOMEM;\n\n\tsha1_init(digest);\n\tmemset(ws, 0, sizeof(ws));\n\n\t \n\tdst = (void *)raw;\n\tfor (i = 0, was_ld_map = false; i < fp->len; i++) {\n\t\tdst[i] = fp->insnsi[i];\n\t\tif (!was_ld_map &&\n\t\t    dst[i].code == (BPF_LD | BPF_IMM | BPF_DW) &&\n\t\t    (dst[i].src_reg == BPF_PSEUDO_MAP_FD ||\n\t\t     dst[i].src_reg == BPF_PSEUDO_MAP_VALUE)) {\n\t\t\twas_ld_map = true;\n\t\t\tdst[i].imm = 0;\n\t\t} else if (was_ld_map &&\n\t\t\t   dst[i].code == 0 &&\n\t\t\t   dst[i].dst_reg == 0 &&\n\t\t\t   dst[i].src_reg == 0 &&\n\t\t\t   dst[i].off == 0) {\n\t\t\twas_ld_map = false;\n\t\t\tdst[i].imm = 0;\n\t\t} else {\n\t\t\twas_ld_map = false;\n\t\t}\n\t}\n\n\tpsize = bpf_prog_insn_size(fp);\n\tmemset(&raw[psize], 0, raw_size - psize);\n\traw[psize++] = 0x80;\n\n\tbsize  = round_up(psize, SHA1_BLOCK_SIZE);\n\tblocks = bsize / SHA1_BLOCK_SIZE;\n\ttodo   = raw;\n\tif (bsize - psize >= sizeof(__be64)) {\n\t\tbits = (__be64 *)(todo + bsize - sizeof(__be64));\n\t} else {\n\t\tbits = (__be64 *)(todo + bsize + bits_offset);\n\t\tblocks++;\n\t}\n\t*bits = cpu_to_be64((psize - 1) << 3);\n\n\twhile (blocks--) {\n\t\tsha1_transform(digest, todo, ws);\n\t\ttodo += SHA1_BLOCK_SIZE;\n\t}\n\n\tresult = (__force __be32 *)digest;\n\tfor (i = 0; i < SHA1_DIGEST_WORDS; i++)\n\t\tresult[i] = cpu_to_be32(digest[i]);\n\tmemcpy(fp->tag, result, sizeof(fp->tag));\n\n\tvfree(raw);\n\treturn 0;\n}\n\nstatic int bpf_adj_delta_to_imm(struct bpf_insn *insn, u32 pos, s32 end_old,\n\t\t\t\ts32 end_new, s32 curr, const bool probe_pass)\n{\n\tconst s64 imm_min = S32_MIN, imm_max = S32_MAX;\n\ts32 delta = end_new - end_old;\n\ts64 imm = insn->imm;\n\n\tif (curr < pos && curr + imm + 1 >= end_old)\n\t\timm += delta;\n\telse if (curr >= end_new && curr + imm + 1 < end_new)\n\t\timm -= delta;\n\tif (imm < imm_min || imm > imm_max)\n\t\treturn -ERANGE;\n\tif (!probe_pass)\n\t\tinsn->imm = imm;\n\treturn 0;\n}\n\nstatic int bpf_adj_delta_to_off(struct bpf_insn *insn, u32 pos, s32 end_old,\n\t\t\t\ts32 end_new, s32 curr, const bool probe_pass)\n{\n\ts64 off_min, off_max, off;\n\ts32 delta = end_new - end_old;\n\n\tif (insn->code == (BPF_JMP32 | BPF_JA)) {\n\t\toff = insn->imm;\n\t\toff_min = S32_MIN;\n\t\toff_max = S32_MAX;\n\t} else {\n\t\toff = insn->off;\n\t\toff_min = S16_MIN;\n\t\toff_max = S16_MAX;\n\t}\n\n\tif (curr < pos && curr + off + 1 >= end_old)\n\t\toff += delta;\n\telse if (curr >= end_new && curr + off + 1 < end_new)\n\t\toff -= delta;\n\tif (off < off_min || off > off_max)\n\t\treturn -ERANGE;\n\tif (!probe_pass) {\n\t\tif (insn->code == (BPF_JMP32 | BPF_JA))\n\t\t\tinsn->imm = off;\n\t\telse\n\t\t\tinsn->off = off;\n\t}\n\treturn 0;\n}\n\nstatic int bpf_adj_branches(struct bpf_prog *prog, u32 pos, s32 end_old,\n\t\t\t    s32 end_new, const bool probe_pass)\n{\n\tu32 i, insn_cnt = prog->len + (probe_pass ? end_new - end_old : 0);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tint ret = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tu8 code;\n\n\t\t \n\t\tif (probe_pass && i == pos) {\n\t\t\ti = end_new;\n\t\t\tinsn = prog->insnsi + end_old;\n\t\t}\n\t\tif (bpf_pseudo_func(insn)) {\n\t\t\tret = bpf_adj_delta_to_imm(insn, pos, end_old,\n\t\t\t\t\t\t   end_new, i, probe_pass);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tcontinue;\n\t\t}\n\t\tcode = insn->code;\n\t\tif ((BPF_CLASS(code) != BPF_JMP &&\n\t\t     BPF_CLASS(code) != BPF_JMP32) ||\n\t\t    BPF_OP(code) == BPF_EXIT)\n\t\t\tcontinue;\n\t\t \n\t\tif (BPF_OP(code) == BPF_CALL) {\n\t\t\tif (insn->src_reg != BPF_PSEUDO_CALL)\n\t\t\t\tcontinue;\n\t\t\tret = bpf_adj_delta_to_imm(insn, pos, end_old,\n\t\t\t\t\t\t   end_new, i, probe_pass);\n\t\t} else {\n\t\t\tret = bpf_adj_delta_to_off(insn, pos, end_old,\n\t\t\t\t\t\t   end_new, i, probe_pass);\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void bpf_adj_linfo(struct bpf_prog *prog, u32 off, u32 delta)\n{\n\tstruct bpf_line_info *linfo;\n\tu32 i, nr_linfo;\n\n\tnr_linfo = prog->aux->nr_linfo;\n\tif (!nr_linfo || !delta)\n\t\treturn;\n\n\tlinfo = prog->aux->linfo;\n\n\tfor (i = 0; i < nr_linfo; i++)\n\t\tif (off < linfo[i].insn_off)\n\t\t\tbreak;\n\n\t \n\tfor (; i < nr_linfo; i++)\n\t\tlinfo[i].insn_off += delta;\n}\n\nstruct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,\n\t\t\t\t       const struct bpf_insn *patch, u32 len)\n{\n\tu32 insn_adj_cnt, insn_rest, insn_delta = len - 1;\n\tconst u32 cnt_max = S16_MAX;\n\tstruct bpf_prog *prog_adj;\n\tint err;\n\n\t \n\tif (insn_delta == 0) {\n\t\tmemcpy(prog->insnsi + off, patch, sizeof(*patch));\n\t\treturn prog;\n\t}\n\n\tinsn_adj_cnt = prog->len + insn_delta;\n\n\t \n\tif (insn_adj_cnt > cnt_max &&\n\t    (err = bpf_adj_branches(prog, off, off + 1, off + len, true)))\n\t\treturn ERR_PTR(err);\n\n\t \n\tprog_adj = bpf_prog_realloc(prog, bpf_prog_size(insn_adj_cnt),\n\t\t\t\t    GFP_USER);\n\tif (!prog_adj)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tprog_adj->len = insn_adj_cnt;\n\n\t \n\tinsn_rest = insn_adj_cnt - off - len;\n\n\tmemmove(prog_adj->insnsi + off + len, prog_adj->insnsi + off + 1,\n\t\tsizeof(*patch) * insn_rest);\n\tmemcpy(prog_adj->insnsi + off, patch, sizeof(*patch) * len);\n\n\t \n\tBUG_ON(bpf_adj_branches(prog_adj, off, off + 1, off + len, false));\n\n\tbpf_adj_linfo(prog_adj, off, insn_delta);\n\n\treturn prog_adj;\n}\n\nint bpf_remove_insns(struct bpf_prog *prog, u32 off, u32 cnt)\n{\n\t \n\tmemmove(prog->insnsi + off, prog->insnsi + off + cnt,\n\t\tsizeof(struct bpf_insn) * (prog->len - off - cnt));\n\tprog->len -= cnt;\n\n\treturn WARN_ON_ONCE(bpf_adj_branches(prog, off, off + cnt, off, false));\n}\n\nstatic void bpf_prog_kallsyms_del_subprogs(struct bpf_prog *fp)\n{\n\tint i;\n\n\tfor (i = 0; i < fp->aux->func_cnt; i++)\n\t\tbpf_prog_kallsyms_del(fp->aux->func[i]);\n}\n\nvoid bpf_prog_kallsyms_del_all(struct bpf_prog *fp)\n{\n\tbpf_prog_kallsyms_del_subprogs(fp);\n\tbpf_prog_kallsyms_del(fp);\n}\n\n#ifdef CONFIG_BPF_JIT\n \nint bpf_jit_enable   __read_mostly = IS_BUILTIN(CONFIG_BPF_JIT_DEFAULT_ON);\nint bpf_jit_kallsyms __read_mostly = IS_BUILTIN(CONFIG_BPF_JIT_DEFAULT_ON);\nint bpf_jit_harden   __read_mostly;\nlong bpf_jit_limit   __read_mostly;\nlong bpf_jit_limit_max __read_mostly;\n\nstatic void\nbpf_prog_ksym_set_addr(struct bpf_prog *prog)\n{\n\tWARN_ON_ONCE(!bpf_prog_ebpf_jited(prog));\n\n\tprog->aux->ksym.start = (unsigned long) prog->bpf_func;\n\tprog->aux->ksym.end   = prog->aux->ksym.start + prog->jited_len;\n}\n\nstatic void\nbpf_prog_ksym_set_name(struct bpf_prog *prog)\n{\n\tchar *sym = prog->aux->ksym.name;\n\tconst char *end = sym + KSYM_NAME_LEN;\n\tconst struct btf_type *type;\n\tconst char *func_name;\n\n\tBUILD_BUG_ON(sizeof(\"bpf_prog_\") +\n\t\t     sizeof(prog->tag) * 2 +\n\t\t      \n\t\t     sizeof(prog->aux->name) > KSYM_NAME_LEN);\n\n\tsym += snprintf(sym, KSYM_NAME_LEN, \"bpf_prog_\");\n\tsym  = bin2hex(sym, prog->tag, sizeof(prog->tag));\n\n\t \n\tif (prog->aux->func_info_cnt) {\n\t\ttype = btf_type_by_id(prog->aux->btf,\n\t\t\t\t      prog->aux->func_info[prog->aux->func_idx].type_id);\n\t\tfunc_name = btf_name_by_offset(prog->aux->btf, type->name_off);\n\t\tsnprintf(sym, (size_t)(end - sym), \"_%s\", func_name);\n\t\treturn;\n\t}\n\n\tif (prog->aux->name[0])\n\t\tsnprintf(sym, (size_t)(end - sym), \"_%s\", prog->aux->name);\n\telse\n\t\t*sym = 0;\n}\n\nstatic unsigned long bpf_get_ksym_start(struct latch_tree_node *n)\n{\n\treturn container_of(n, struct bpf_ksym, tnode)->start;\n}\n\nstatic __always_inline bool bpf_tree_less(struct latch_tree_node *a,\n\t\t\t\t\t  struct latch_tree_node *b)\n{\n\treturn bpf_get_ksym_start(a) < bpf_get_ksym_start(b);\n}\n\nstatic __always_inline int bpf_tree_comp(void *key, struct latch_tree_node *n)\n{\n\tunsigned long val = (unsigned long)key;\n\tconst struct bpf_ksym *ksym;\n\n\tksym = container_of(n, struct bpf_ksym, tnode);\n\n\tif (val < ksym->start)\n\t\treturn -1;\n\t \n\tif (val > ksym->end)\n\t\treturn  1;\n\n\treturn 0;\n}\n\nstatic const struct latch_tree_ops bpf_tree_ops = {\n\t.less\t= bpf_tree_less,\n\t.comp\t= bpf_tree_comp,\n};\n\nstatic DEFINE_SPINLOCK(bpf_lock);\nstatic LIST_HEAD(bpf_kallsyms);\nstatic struct latch_tree_root bpf_tree __cacheline_aligned;\n\nvoid bpf_ksym_add(struct bpf_ksym *ksym)\n{\n\tspin_lock_bh(&bpf_lock);\n\tWARN_ON_ONCE(!list_empty(&ksym->lnode));\n\tlist_add_tail_rcu(&ksym->lnode, &bpf_kallsyms);\n\tlatch_tree_insert(&ksym->tnode, &bpf_tree, &bpf_tree_ops);\n\tspin_unlock_bh(&bpf_lock);\n}\n\nstatic void __bpf_ksym_del(struct bpf_ksym *ksym)\n{\n\tif (list_empty(&ksym->lnode))\n\t\treturn;\n\n\tlatch_tree_erase(&ksym->tnode, &bpf_tree, &bpf_tree_ops);\n\tlist_del_rcu(&ksym->lnode);\n}\n\nvoid bpf_ksym_del(struct bpf_ksym *ksym)\n{\n\tspin_lock_bh(&bpf_lock);\n\t__bpf_ksym_del(ksym);\n\tspin_unlock_bh(&bpf_lock);\n}\n\nstatic bool bpf_prog_kallsyms_candidate(const struct bpf_prog *fp)\n{\n\treturn fp->jited && !bpf_prog_was_classic(fp);\n}\n\nvoid bpf_prog_kallsyms_add(struct bpf_prog *fp)\n{\n\tif (!bpf_prog_kallsyms_candidate(fp) ||\n\t    !bpf_capable())\n\t\treturn;\n\n\tbpf_prog_ksym_set_addr(fp);\n\tbpf_prog_ksym_set_name(fp);\n\tfp->aux->ksym.prog = true;\n\n\tbpf_ksym_add(&fp->aux->ksym);\n}\n\nvoid bpf_prog_kallsyms_del(struct bpf_prog *fp)\n{\n\tif (!bpf_prog_kallsyms_candidate(fp))\n\t\treturn;\n\n\tbpf_ksym_del(&fp->aux->ksym);\n}\n\nstatic struct bpf_ksym *bpf_ksym_find(unsigned long addr)\n{\n\tstruct latch_tree_node *n;\n\n\tn = latch_tree_find((void *)addr, &bpf_tree, &bpf_tree_ops);\n\treturn n ? container_of(n, struct bpf_ksym, tnode) : NULL;\n}\n\nconst char *__bpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t\t\t unsigned long *off, char *sym)\n{\n\tstruct bpf_ksym *ksym;\n\tchar *ret = NULL;\n\n\trcu_read_lock();\n\tksym = bpf_ksym_find(addr);\n\tif (ksym) {\n\t\tunsigned long symbol_start = ksym->start;\n\t\tunsigned long symbol_end = ksym->end;\n\n\t\tstrncpy(sym, ksym->name, KSYM_NAME_LEN);\n\n\t\tret = sym;\n\t\tif (size)\n\t\t\t*size = symbol_end - symbol_start;\n\t\tif (off)\n\t\t\t*off  = addr - symbol_start;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nbool is_bpf_text_address(unsigned long addr)\n{\n\tbool ret;\n\n\trcu_read_lock();\n\tret = bpf_ksym_find(addr) != NULL;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic struct bpf_prog *bpf_prog_ksym_find(unsigned long addr)\n{\n\tstruct bpf_ksym *ksym = bpf_ksym_find(addr);\n\n\treturn ksym && ksym->prog ?\n\t       container_of(ksym, struct bpf_prog_aux, ksym)->prog :\n\t       NULL;\n}\n\nconst struct exception_table_entry *search_bpf_extables(unsigned long addr)\n{\n\tconst struct exception_table_entry *e = NULL;\n\tstruct bpf_prog *prog;\n\n\trcu_read_lock();\n\tprog = bpf_prog_ksym_find(addr);\n\tif (!prog)\n\t\tgoto out;\n\tif (!prog->aux->num_exentries)\n\t\tgoto out;\n\n\te = search_extable(prog->aux->extable, prog->aux->num_exentries, addr);\nout:\n\trcu_read_unlock();\n\treturn e;\n}\n\nint bpf_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t    char *sym)\n{\n\tstruct bpf_ksym *ksym;\n\tunsigned int it = 0;\n\tint ret = -ERANGE;\n\n\tif (!bpf_jit_kallsyms_enabled())\n\t\treturn ret;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(ksym, &bpf_kallsyms, lnode) {\n\t\tif (it++ != symnum)\n\t\t\tcontinue;\n\n\t\tstrncpy(sym, ksym->name, KSYM_NAME_LEN);\n\n\t\t*value = ksym->start;\n\t\t*type  = BPF_SYM_ELF_TYPE;\n\n\t\tret = 0;\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nint bpf_jit_add_poke_descriptor(struct bpf_prog *prog,\n\t\t\t\tstruct bpf_jit_poke_descriptor *poke)\n{\n\tstruct bpf_jit_poke_descriptor *tab = prog->aux->poke_tab;\n\tstatic const u32 poke_tab_max = 1024;\n\tu32 slot = prog->aux->size_poke_tab;\n\tu32 size = slot + 1;\n\n\tif (size > poke_tab_max)\n\t\treturn -ENOSPC;\n\tif (poke->tailcall_target || poke->tailcall_target_stable ||\n\t    poke->tailcall_bypass || poke->adj_off || poke->bypass_addr)\n\t\treturn -EINVAL;\n\n\tswitch (poke->reason) {\n\tcase BPF_POKE_REASON_TAIL_CALL:\n\t\tif (!poke->tail_call.map)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\ttab = krealloc(tab, size * sizeof(*poke), GFP_KERNEL);\n\tif (!tab)\n\t\treturn -ENOMEM;\n\n\tmemcpy(&tab[slot], poke, sizeof(*poke));\n\tprog->aux->size_poke_tab = size;\n\tprog->aux->poke_tab = tab;\n\n\treturn slot;\n}\n\n \n#define BPF_PROG_CHUNK_SHIFT\t6\n#define BPF_PROG_CHUNK_SIZE\t(1 << BPF_PROG_CHUNK_SHIFT)\n#define BPF_PROG_CHUNK_MASK\t(~(BPF_PROG_CHUNK_SIZE - 1))\n\nstruct bpf_prog_pack {\n\tstruct list_head list;\n\tvoid *ptr;\n\tunsigned long bitmap[];\n};\n\nvoid bpf_jit_fill_hole_with_zero(void *area, unsigned int size)\n{\n\tmemset(area, 0, size);\n}\n\n#define BPF_PROG_SIZE_TO_NBITS(size)\t(round_up(size, BPF_PROG_CHUNK_SIZE) / BPF_PROG_CHUNK_SIZE)\n\nstatic DEFINE_MUTEX(pack_mutex);\nstatic LIST_HEAD(pack_list);\n\n \n#ifdef PMD_SIZE\n#define BPF_PROG_PACK_SIZE (PMD_SIZE * num_possible_nodes())\n#else\n#define BPF_PROG_PACK_SIZE PAGE_SIZE\n#endif\n\n#define BPF_PROG_CHUNK_COUNT (BPF_PROG_PACK_SIZE / BPF_PROG_CHUNK_SIZE)\n\nstatic struct bpf_prog_pack *alloc_new_pack(bpf_jit_fill_hole_t bpf_fill_ill_insns)\n{\n\tstruct bpf_prog_pack *pack;\n\n\tpack = kzalloc(struct_size(pack, bitmap, BITS_TO_LONGS(BPF_PROG_CHUNK_COUNT)),\n\t\t       GFP_KERNEL);\n\tif (!pack)\n\t\treturn NULL;\n\tpack->ptr = bpf_jit_alloc_exec(BPF_PROG_PACK_SIZE);\n\tif (!pack->ptr) {\n\t\tkfree(pack);\n\t\treturn NULL;\n\t}\n\tbpf_fill_ill_insns(pack->ptr, BPF_PROG_PACK_SIZE);\n\tbitmap_zero(pack->bitmap, BPF_PROG_PACK_SIZE / BPF_PROG_CHUNK_SIZE);\n\tlist_add_tail(&pack->list, &pack_list);\n\n\tset_vm_flush_reset_perms(pack->ptr);\n\tset_memory_rox((unsigned long)pack->ptr, BPF_PROG_PACK_SIZE / PAGE_SIZE);\n\treturn pack;\n}\n\nvoid *bpf_prog_pack_alloc(u32 size, bpf_jit_fill_hole_t bpf_fill_ill_insns)\n{\n\tunsigned int nbits = BPF_PROG_SIZE_TO_NBITS(size);\n\tstruct bpf_prog_pack *pack;\n\tunsigned long pos;\n\tvoid *ptr = NULL;\n\n\tmutex_lock(&pack_mutex);\n\tif (size > BPF_PROG_PACK_SIZE) {\n\t\tsize = round_up(size, PAGE_SIZE);\n\t\tptr = bpf_jit_alloc_exec(size);\n\t\tif (ptr) {\n\t\t\tbpf_fill_ill_insns(ptr, size);\n\t\t\tset_vm_flush_reset_perms(ptr);\n\t\t\tset_memory_rox((unsigned long)ptr, size / PAGE_SIZE);\n\t\t}\n\t\tgoto out;\n\t}\n\tlist_for_each_entry(pack, &pack_list, list) {\n\t\tpos = bitmap_find_next_zero_area(pack->bitmap, BPF_PROG_CHUNK_COUNT, 0,\n\t\t\t\t\t\t nbits, 0);\n\t\tif (pos < BPF_PROG_CHUNK_COUNT)\n\t\t\tgoto found_free_area;\n\t}\n\n\tpack = alloc_new_pack(bpf_fill_ill_insns);\n\tif (!pack)\n\t\tgoto out;\n\n\tpos = 0;\n\nfound_free_area:\n\tbitmap_set(pack->bitmap, pos, nbits);\n\tptr = (void *)(pack->ptr) + (pos << BPF_PROG_CHUNK_SHIFT);\n\nout:\n\tmutex_unlock(&pack_mutex);\n\treturn ptr;\n}\n\nvoid bpf_prog_pack_free(struct bpf_binary_header *hdr)\n{\n\tstruct bpf_prog_pack *pack = NULL, *tmp;\n\tunsigned int nbits;\n\tunsigned long pos;\n\n\tmutex_lock(&pack_mutex);\n\tif (hdr->size > BPF_PROG_PACK_SIZE) {\n\t\tbpf_jit_free_exec(hdr);\n\t\tgoto out;\n\t}\n\n\tlist_for_each_entry(tmp, &pack_list, list) {\n\t\tif ((void *)hdr >= tmp->ptr && (tmp->ptr + BPF_PROG_PACK_SIZE) > (void *)hdr) {\n\t\t\tpack = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (WARN_ONCE(!pack, \"bpf_prog_pack bug\\n\"))\n\t\tgoto out;\n\n\tnbits = BPF_PROG_SIZE_TO_NBITS(hdr->size);\n\tpos = ((unsigned long)hdr - (unsigned long)pack->ptr) >> BPF_PROG_CHUNK_SHIFT;\n\n\tWARN_ONCE(bpf_arch_text_invalidate(hdr, hdr->size),\n\t\t  \"bpf_prog_pack bug: missing bpf_arch_text_invalidate?\\n\");\n\n\tbitmap_clear(pack->bitmap, pos, nbits);\n\tif (bitmap_find_next_zero_area(pack->bitmap, BPF_PROG_CHUNK_COUNT, 0,\n\t\t\t\t       BPF_PROG_CHUNK_COUNT, 0) == 0) {\n\t\tlist_del(&pack->list);\n\t\tbpf_jit_free_exec(pack->ptr);\n\t\tkfree(pack);\n\t}\nout:\n\tmutex_unlock(&pack_mutex);\n}\n\nstatic atomic_long_t bpf_jit_current;\n\n \nu64 __weak bpf_jit_alloc_exec_limit(void)\n{\n#if defined(MODULES_VADDR)\n\treturn MODULES_END - MODULES_VADDR;\n#else\n\treturn VMALLOC_END - VMALLOC_START;\n#endif\n}\n\nstatic int __init bpf_jit_charge_init(void)\n{\n\t \n\tbpf_jit_limit_max = bpf_jit_alloc_exec_limit();\n\tbpf_jit_limit = min_t(u64, round_up(bpf_jit_limit_max >> 1,\n\t\t\t\t\t    PAGE_SIZE), LONG_MAX);\n\treturn 0;\n}\npure_initcall(bpf_jit_charge_init);\n\nint bpf_jit_charge_modmem(u32 size)\n{\n\tif (atomic_long_add_return(size, &bpf_jit_current) > READ_ONCE(bpf_jit_limit)) {\n\t\tif (!bpf_capable()) {\n\t\t\tatomic_long_sub(size, &bpf_jit_current);\n\t\t\treturn -EPERM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nvoid bpf_jit_uncharge_modmem(u32 size)\n{\n\tatomic_long_sub(size, &bpf_jit_current);\n}\n\nvoid *__weak bpf_jit_alloc_exec(unsigned long size)\n{\n\treturn module_alloc(size);\n}\n\nvoid __weak bpf_jit_free_exec(void *addr)\n{\n\tmodule_memfree(addr);\n}\n\nstruct bpf_binary_header *\nbpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,\n\t\t     unsigned int alignment,\n\t\t     bpf_jit_fill_hole_t bpf_fill_ill_insns)\n{\n\tstruct bpf_binary_header *hdr;\n\tu32 size, hole, start;\n\n\tWARN_ON_ONCE(!is_power_of_2(alignment) ||\n\t\t     alignment > BPF_IMAGE_ALIGNMENT);\n\n\t \n\tsize = round_up(proglen + sizeof(*hdr) + 128, PAGE_SIZE);\n\n\tif (bpf_jit_charge_modmem(size))\n\t\treturn NULL;\n\thdr = bpf_jit_alloc_exec(size);\n\tif (!hdr) {\n\t\tbpf_jit_uncharge_modmem(size);\n\t\treturn NULL;\n\t}\n\n\t \n\tbpf_fill_ill_insns(hdr, size);\n\n\thdr->size = size;\n\thole = min_t(unsigned int, size - (proglen + sizeof(*hdr)),\n\t\t     PAGE_SIZE - sizeof(*hdr));\n\tstart = get_random_u32_below(hole) & ~(alignment - 1);\n\n\t \n\t*image_ptr = &hdr->image[start];\n\n\treturn hdr;\n}\n\nvoid bpf_jit_binary_free(struct bpf_binary_header *hdr)\n{\n\tu32 size = hdr->size;\n\n\tbpf_jit_free_exec(hdr);\n\tbpf_jit_uncharge_modmem(size);\n}\n\n \nstruct bpf_binary_header *\nbpf_jit_binary_pack_alloc(unsigned int proglen, u8 **image_ptr,\n\t\t\t  unsigned int alignment,\n\t\t\t  struct bpf_binary_header **rw_header,\n\t\t\t  u8 **rw_image,\n\t\t\t  bpf_jit_fill_hole_t bpf_fill_ill_insns)\n{\n\tstruct bpf_binary_header *ro_header;\n\tu32 size, hole, start;\n\n\tWARN_ON_ONCE(!is_power_of_2(alignment) ||\n\t\t     alignment > BPF_IMAGE_ALIGNMENT);\n\n\t \n\tsize = round_up(proglen + sizeof(*ro_header) + 16, BPF_PROG_CHUNK_SIZE);\n\n\tif (bpf_jit_charge_modmem(size))\n\t\treturn NULL;\n\tro_header = bpf_prog_pack_alloc(size, bpf_fill_ill_insns);\n\tif (!ro_header) {\n\t\tbpf_jit_uncharge_modmem(size);\n\t\treturn NULL;\n\t}\n\n\t*rw_header = kvmalloc(size, GFP_KERNEL);\n\tif (!*rw_header) {\n\t\tbpf_arch_text_copy(&ro_header->size, &size, sizeof(size));\n\t\tbpf_prog_pack_free(ro_header);\n\t\tbpf_jit_uncharge_modmem(size);\n\t\treturn NULL;\n\t}\n\n\t \n\tbpf_fill_ill_insns(*rw_header, size);\n\t(*rw_header)->size = size;\n\n\thole = min_t(unsigned int, size - (proglen + sizeof(*ro_header)),\n\t\t     BPF_PROG_CHUNK_SIZE - sizeof(*ro_header));\n\tstart = get_random_u32_below(hole) & ~(alignment - 1);\n\n\t*image_ptr = &ro_header->image[start];\n\t*rw_image = &(*rw_header)->image[start];\n\n\treturn ro_header;\n}\n\n \nint bpf_jit_binary_pack_finalize(struct bpf_prog *prog,\n\t\t\t\t struct bpf_binary_header *ro_header,\n\t\t\t\t struct bpf_binary_header *rw_header)\n{\n\tvoid *ptr;\n\n\tptr = bpf_arch_text_copy(ro_header, rw_header, rw_header->size);\n\n\tkvfree(rw_header);\n\n\tif (IS_ERR(ptr)) {\n\t\tbpf_prog_pack_free(ro_header);\n\t\treturn PTR_ERR(ptr);\n\t}\n\treturn 0;\n}\n\n \nvoid bpf_jit_binary_pack_free(struct bpf_binary_header *ro_header,\n\t\t\t      struct bpf_binary_header *rw_header)\n{\n\tu32 size = ro_header->size;\n\n\tbpf_prog_pack_free(ro_header);\n\tkvfree(rw_header);\n\tbpf_jit_uncharge_modmem(size);\n}\n\nstruct bpf_binary_header *\nbpf_jit_binary_pack_hdr(const struct bpf_prog *fp)\n{\n\tunsigned long real_start = (unsigned long)fp->bpf_func;\n\tunsigned long addr;\n\n\taddr = real_start & BPF_PROG_CHUNK_MASK;\n\treturn (void *)addr;\n}\n\nstatic inline struct bpf_binary_header *\nbpf_jit_binary_hdr(const struct bpf_prog *fp)\n{\n\tunsigned long real_start = (unsigned long)fp->bpf_func;\n\tunsigned long addr;\n\n\taddr = real_start & PAGE_MASK;\n\treturn (void *)addr;\n}\n\n \nvoid __weak bpf_jit_free(struct bpf_prog *fp)\n{\n\tif (fp->jited) {\n\t\tstruct bpf_binary_header *hdr = bpf_jit_binary_hdr(fp);\n\n\t\tbpf_jit_binary_free(hdr);\n\t\tWARN_ON_ONCE(!bpf_prog_kallsyms_verify_off(fp));\n\t}\n\n\tbpf_prog_unlock_free(fp);\n}\n\nint bpf_jit_get_func_addr(const struct bpf_prog *prog,\n\t\t\t  const struct bpf_insn *insn, bool extra_pass,\n\t\t\t  u64 *func_addr, bool *func_addr_fixed)\n{\n\ts16 off = insn->off;\n\ts32 imm = insn->imm;\n\tu8 *addr;\n\tint err;\n\n\t*func_addr_fixed = insn->src_reg != BPF_PSEUDO_CALL;\n\tif (!*func_addr_fixed) {\n\t\t \n\t\tif (!extra_pass)\n\t\t\taddr = NULL;\n\t\telse if (prog->aux->func &&\n\t\t\t off >= 0 && off < prog->aux->func_cnt)\n\t\t\taddr = (u8 *)prog->aux->func[off]->bpf_func;\n\t\telse\n\t\t\treturn -EINVAL;\n\t} else if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL &&\n\t\t   bpf_jit_supports_far_kfunc_call()) {\n\t\terr = bpf_get_kfunc_addr(prog, insn->imm, insn->off, &addr);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\t \n\t\taddr = (u8 *)__bpf_call_base + imm;\n\t}\n\n\t*func_addr = (unsigned long)addr;\n\treturn 0;\n}\n\nstatic int bpf_jit_blind_insn(const struct bpf_insn *from,\n\t\t\t      const struct bpf_insn *aux,\n\t\t\t      struct bpf_insn *to_buff,\n\t\t\t      bool emit_zext)\n{\n\tstruct bpf_insn *to = to_buff;\n\tu32 imm_rnd = get_random_u32();\n\ts16 off;\n\n\tBUILD_BUG_ON(BPF_REG_AX  + 1 != MAX_BPF_JIT_REG);\n\tBUILD_BUG_ON(MAX_BPF_REG + 1 != MAX_BPF_JIT_REG);\n\n\t \n\tif (from->dst_reg == BPF_REG_AX || from->src_reg == BPF_REG_AX)\n\t\tgoto out;\n\n\tif (from->imm == 0 &&\n\t    (from->code == (BPF_ALU   | BPF_MOV | BPF_K) ||\n\t     from->code == (BPF_ALU64 | BPF_MOV | BPF_K))) {\n\t\t*to++ = BPF_ALU64_REG(BPF_XOR, from->dst_reg, from->dst_reg);\n\t\tgoto out;\n\t}\n\n\tswitch (from->code) {\n\tcase BPF_ALU | BPF_ADD | BPF_K:\n\tcase BPF_ALU | BPF_SUB | BPF_K:\n\tcase BPF_ALU | BPF_AND | BPF_K:\n\tcase BPF_ALU | BPF_OR  | BPF_K:\n\tcase BPF_ALU | BPF_XOR | BPF_K:\n\tcase BPF_ALU | BPF_MUL | BPF_K:\n\tcase BPF_ALU | BPF_MOV | BPF_K:\n\tcase BPF_ALU | BPF_DIV | BPF_K:\n\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU32_REG_OFF(from->code, from->dst_reg, BPF_REG_AX, from->off);\n\t\tbreak;\n\n\tcase BPF_ALU64 | BPF_ADD | BPF_K:\n\tcase BPF_ALU64 | BPF_SUB | BPF_K:\n\tcase BPF_ALU64 | BPF_AND | BPF_K:\n\tcase BPF_ALU64 | BPF_OR  | BPF_K:\n\tcase BPF_ALU64 | BPF_XOR | BPF_K:\n\tcase BPF_ALU64 | BPF_MUL | BPF_K:\n\tcase BPF_ALU64 | BPF_MOV | BPF_K:\n\tcase BPF_ALU64 | BPF_DIV | BPF_K:\n\tcase BPF_ALU64 | BPF_MOD | BPF_K:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_REG_OFF(from->code, from->dst_reg, BPF_REG_AX, from->off);\n\t\tbreak;\n\n\tcase BPF_JMP | BPF_JEQ  | BPF_K:\n\tcase BPF_JMP | BPF_JNE  | BPF_K:\n\tcase BPF_JMP | BPF_JGT  | BPF_K:\n\tcase BPF_JMP | BPF_JLT  | BPF_K:\n\tcase BPF_JMP | BPF_JGE  | BPF_K:\n\tcase BPF_JMP | BPF_JLE  | BPF_K:\n\tcase BPF_JMP | BPF_JSGT | BPF_K:\n\tcase BPF_JMP | BPF_JSLT | BPF_K:\n\tcase BPF_JMP | BPF_JSGE | BPF_K:\n\tcase BPF_JMP | BPF_JSLE | BPF_K:\n\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\t \n\t\toff = from->off;\n\t\tif (off < 0)\n\t\t\toff -= 2;\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_JMP_REG(from->code, from->dst_reg, BPF_REG_AX, off);\n\t\tbreak;\n\n\tcase BPF_JMP32 | BPF_JEQ  | BPF_K:\n\tcase BPF_JMP32 | BPF_JNE  | BPF_K:\n\tcase BPF_JMP32 | BPF_JGT  | BPF_K:\n\tcase BPF_JMP32 | BPF_JLT  | BPF_K:\n\tcase BPF_JMP32 | BPF_JGE  | BPF_K:\n\tcase BPF_JMP32 | BPF_JLE  | BPF_K:\n\tcase BPF_JMP32 | BPF_JSGT | BPF_K:\n\tcase BPF_JMP32 | BPF_JSLT | BPF_K:\n\tcase BPF_JMP32 | BPF_JSGE | BPF_K:\n\tcase BPF_JMP32 | BPF_JSLE | BPF_K:\n\tcase BPF_JMP32 | BPF_JSET | BPF_K:\n\t\t \n\t\toff = from->off;\n\t\tif (off < 0)\n\t\t\toff -= 2;\n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_JMP32_REG(from->code, from->dst_reg, BPF_REG_AX,\n\t\t\t\t      off);\n\t\tbreak;\n\n\tcase BPF_LD | BPF_IMM | BPF_DW:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[1].imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);\n\t\t*to++ = BPF_ALU64_REG(BPF_MOV, aux[0].dst_reg, BPF_REG_AX);\n\t\tbreak;\n\tcase 0:  \n\t\t*to++ = BPF_ALU32_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ aux[0].imm);\n\t\t*to++ = BPF_ALU32_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\tif (emit_zext)\n\t\t\t*to++ = BPF_ZEXT_REG(BPF_REG_AX);\n\t\t*to++ = BPF_ALU64_REG(BPF_OR,  aux[0].dst_reg, BPF_REG_AX);\n\t\tbreak;\n\n\tcase BPF_ST | BPF_MEM | BPF_DW:\n\tcase BPF_ST | BPF_MEM | BPF_W:\n\tcase BPF_ST | BPF_MEM | BPF_H:\n\tcase BPF_ST | BPF_MEM | BPF_B:\n\t\t*to++ = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, imm_rnd ^ from->imm);\n\t\t*to++ = BPF_ALU64_IMM(BPF_XOR, BPF_REG_AX, imm_rnd);\n\t\t*to++ = BPF_STX_MEM(from->code, from->dst_reg, BPF_REG_AX, from->off);\n\t\tbreak;\n\t}\nout:\n\treturn to - to_buff;\n}\n\nstatic struct bpf_prog *bpf_prog_clone_create(struct bpf_prog *fp_other,\n\t\t\t\t\t      gfp_t gfp_extra_flags)\n{\n\tgfp_t gfp_flags = GFP_KERNEL | __GFP_ZERO | gfp_extra_flags;\n\tstruct bpf_prog *fp;\n\n\tfp = __vmalloc(fp_other->pages * PAGE_SIZE, gfp_flags);\n\tif (fp != NULL) {\n\t\t \n\t\tmemcpy(fp, fp_other, fp_other->pages * PAGE_SIZE);\n\t}\n\n\treturn fp;\n}\n\nstatic void bpf_prog_clone_free(struct bpf_prog *fp)\n{\n\t \n\tfp->aux = NULL;\n\tfp->stats = NULL;\n\tfp->active = NULL;\n\t__bpf_prog_free(fp);\n}\n\nvoid bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other)\n{\n\t \n\tfp->aux->prog = fp;\n\tbpf_prog_clone_free(fp_other);\n}\n\nstruct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *prog)\n{\n\tstruct bpf_insn insn_buff[16], aux[2];\n\tstruct bpf_prog *clone, *tmp;\n\tint insn_delta, insn_cnt;\n\tstruct bpf_insn *insn;\n\tint i, rewritten;\n\n\tif (!prog->blinding_requested || prog->blinded)\n\t\treturn prog;\n\n\tclone = bpf_prog_clone_create(prog, GFP_USER);\n\tif (!clone)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tinsn_cnt = clone->len;\n\tinsn = clone->insnsi;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (bpf_pseudo_func(insn)) {\n\t\t\t \n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW) &&\n\t\t    insn[1].code == 0)\n\t\t\tmemcpy(aux, insn, sizeof(aux));\n\n\t\trewritten = bpf_jit_blind_insn(insn, aux, insn_buff,\n\t\t\t\t\t\tclone->aux->verifier_zext);\n\t\tif (!rewritten)\n\t\t\tcontinue;\n\n\t\ttmp = bpf_patch_insn_single(clone, i, insn_buff, rewritten);\n\t\tif (IS_ERR(tmp)) {\n\t\t\t \n\t\t\tbpf_jit_prog_release_other(prog, clone);\n\t\t\treturn tmp;\n\t\t}\n\n\t\tclone = tmp;\n\t\tinsn_delta = rewritten - 1;\n\n\t\t \n\t\tinsn = clone->insnsi + i + insn_delta;\n\t\tinsn_cnt += insn_delta;\n\t\ti        += insn_delta;\n\t}\n\n\tclone->blinded = 1;\n\treturn clone;\n}\n#endif  \n\n \nnoinline u64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5)\n{\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__bpf_call_base);\n\n \n#define BPF_INSN_MAP(INSN_2, INSN_3)\t\t\\\n\t \t\t\\\n\t \t\t\t\\\n\tINSN_3(ALU, ADD,  X),\t\t\t\\\n\tINSN_3(ALU, SUB,  X),\t\t\t\\\n\tINSN_3(ALU, AND,  X),\t\t\t\\\n\tINSN_3(ALU, OR,   X),\t\t\t\\\n\tINSN_3(ALU, LSH,  X),\t\t\t\\\n\tINSN_3(ALU, RSH,  X),\t\t\t\\\n\tINSN_3(ALU, XOR,  X),\t\t\t\\\n\tINSN_3(ALU, MUL,  X),\t\t\t\\\n\tINSN_3(ALU, MOV,  X),\t\t\t\\\n\tINSN_3(ALU, ARSH, X),\t\t\t\\\n\tINSN_3(ALU, DIV,  X),\t\t\t\\\n\tINSN_3(ALU, MOD,  X),\t\t\t\\\n\tINSN_2(ALU, NEG),\t\t\t\\\n\tINSN_3(ALU, END, TO_BE),\t\t\\\n\tINSN_3(ALU, END, TO_LE),\t\t\\\n\t \t\t\\\n\tINSN_3(ALU, ADD,  K),\t\t\t\\\n\tINSN_3(ALU, SUB,  K),\t\t\t\\\n\tINSN_3(ALU, AND,  K),\t\t\t\\\n\tINSN_3(ALU, OR,   K),\t\t\t\\\n\tINSN_3(ALU, LSH,  K),\t\t\t\\\n\tINSN_3(ALU, RSH,  K),\t\t\t\\\n\tINSN_3(ALU, XOR,  K),\t\t\t\\\n\tINSN_3(ALU, MUL,  K),\t\t\t\\\n\tINSN_3(ALU, MOV,  K),\t\t\t\\\n\tINSN_3(ALU, ARSH, K),\t\t\t\\\n\tINSN_3(ALU, DIV,  K),\t\t\t\\\n\tINSN_3(ALU, MOD,  K),\t\t\t\\\n\t \t\t\\\n\t \t\t\t\\\n\tINSN_3(ALU64, ADD,  X),\t\t\t\\\n\tINSN_3(ALU64, SUB,  X),\t\t\t\\\n\tINSN_3(ALU64, AND,  X),\t\t\t\\\n\tINSN_3(ALU64, OR,   X),\t\t\t\\\n\tINSN_3(ALU64, LSH,  X),\t\t\t\\\n\tINSN_3(ALU64, RSH,  X),\t\t\t\\\n\tINSN_3(ALU64, XOR,  X),\t\t\t\\\n\tINSN_3(ALU64, MUL,  X),\t\t\t\\\n\tINSN_3(ALU64, MOV,  X),\t\t\t\\\n\tINSN_3(ALU64, ARSH, X),\t\t\t\\\n\tINSN_3(ALU64, DIV,  X),\t\t\t\\\n\tINSN_3(ALU64, MOD,  X),\t\t\t\\\n\tINSN_2(ALU64, NEG),\t\t\t\\\n\tINSN_3(ALU64, END, TO_LE),\t\t\\\n\t \t\t\\\n\tINSN_3(ALU64, ADD,  K),\t\t\t\\\n\tINSN_3(ALU64, SUB,  K),\t\t\t\\\n\tINSN_3(ALU64, AND,  K),\t\t\t\\\n\tINSN_3(ALU64, OR,   K),\t\t\t\\\n\tINSN_3(ALU64, LSH,  K),\t\t\t\\\n\tINSN_3(ALU64, RSH,  K),\t\t\t\\\n\tINSN_3(ALU64, XOR,  K),\t\t\t\\\n\tINSN_3(ALU64, MUL,  K),\t\t\t\\\n\tINSN_3(ALU64, MOV,  K),\t\t\t\\\n\tINSN_3(ALU64, ARSH, K),\t\t\t\\\n\tINSN_3(ALU64, DIV,  K),\t\t\t\\\n\tINSN_3(ALU64, MOD,  K),\t\t\t\\\n\t \t\t\t\\\n\tINSN_2(JMP, CALL),\t\t\t\\\n\t \t\t\t\\\n\tINSN_2(JMP, EXIT),\t\t\t\\\n\t \t\t\\\n\t \t\t\t\\\n\tINSN_3(JMP32, JEQ,  X),\t\t\t\\\n\tINSN_3(JMP32, JNE,  X),\t\t\t\\\n\tINSN_3(JMP32, JGT,  X),\t\t\t\\\n\tINSN_3(JMP32, JLT,  X),\t\t\t\\\n\tINSN_3(JMP32, JGE,  X),\t\t\t\\\n\tINSN_3(JMP32, JLE,  X),\t\t\t\\\n\tINSN_3(JMP32, JSGT, X),\t\t\t\\\n\tINSN_3(JMP32, JSLT, X),\t\t\t\\\n\tINSN_3(JMP32, JSGE, X),\t\t\t\\\n\tINSN_3(JMP32, JSLE, X),\t\t\t\\\n\tINSN_3(JMP32, JSET, X),\t\t\t\\\n\t \t\t\\\n\tINSN_3(JMP32, JEQ,  K),\t\t\t\\\n\tINSN_3(JMP32, JNE,  K),\t\t\t\\\n\tINSN_3(JMP32, JGT,  K),\t\t\t\\\n\tINSN_3(JMP32, JLT,  K),\t\t\t\\\n\tINSN_3(JMP32, JGE,  K),\t\t\t\\\n\tINSN_3(JMP32, JLE,  K),\t\t\t\\\n\tINSN_3(JMP32, JSGT, K),\t\t\t\\\n\tINSN_3(JMP32, JSLT, K),\t\t\t\\\n\tINSN_3(JMP32, JSGE, K),\t\t\t\\\n\tINSN_3(JMP32, JSLE, K),\t\t\t\\\n\tINSN_3(JMP32, JSET, K),\t\t\t\\\n\t \t\t\\\n\t \t\t\t\\\n\tINSN_3(JMP, JEQ,  X),\t\t\t\\\n\tINSN_3(JMP, JNE,  X),\t\t\t\\\n\tINSN_3(JMP, JGT,  X),\t\t\t\\\n\tINSN_3(JMP, JLT,  X),\t\t\t\\\n\tINSN_3(JMP, JGE,  X),\t\t\t\\\n\tINSN_3(JMP, JLE,  X),\t\t\t\\\n\tINSN_3(JMP, JSGT, X),\t\t\t\\\n\tINSN_3(JMP, JSLT, X),\t\t\t\\\n\tINSN_3(JMP, JSGE, X),\t\t\t\\\n\tINSN_3(JMP, JSLE, X),\t\t\t\\\n\tINSN_3(JMP, JSET, X),\t\t\t\\\n\t \t\t\\\n\tINSN_3(JMP, JEQ,  K),\t\t\t\\\n\tINSN_3(JMP, JNE,  K),\t\t\t\\\n\tINSN_3(JMP, JGT,  K),\t\t\t\\\n\tINSN_3(JMP, JLT,  K),\t\t\t\\\n\tINSN_3(JMP, JGE,  K),\t\t\t\\\n\tINSN_3(JMP, JLE,  K),\t\t\t\\\n\tINSN_3(JMP, JSGT, K),\t\t\t\\\n\tINSN_3(JMP, JSLT, K),\t\t\t\\\n\tINSN_3(JMP, JSGE, K),\t\t\t\\\n\tINSN_3(JMP, JSLE, K),\t\t\t\\\n\tINSN_3(JMP, JSET, K),\t\t\t\\\n\tINSN_2(JMP, JA),\t\t\t\\\n\tINSN_2(JMP32, JA),\t\t\t\\\n\t \t\t\\\n\t \t\t\t\\\n\tINSN_3(STX, MEM,  B),\t\t\t\\\n\tINSN_3(STX, MEM,  H),\t\t\t\\\n\tINSN_3(STX, MEM,  W),\t\t\t\\\n\tINSN_3(STX, MEM,  DW),\t\t\t\\\n\tINSN_3(STX, ATOMIC, W),\t\t\t\\\n\tINSN_3(STX, ATOMIC, DW),\t\t\\\n\t \t\t\\\n\tINSN_3(ST, MEM, B),\t\t\t\\\n\tINSN_3(ST, MEM, H),\t\t\t\\\n\tINSN_3(ST, MEM, W),\t\t\t\\\n\tINSN_3(ST, MEM, DW),\t\t\t\\\n\t \t\t\\\n\t \t\t\t\\\n\tINSN_3(LDX, MEM, B),\t\t\t\\\n\tINSN_3(LDX, MEM, H),\t\t\t\\\n\tINSN_3(LDX, MEM, W),\t\t\t\\\n\tINSN_3(LDX, MEM, DW),\t\t\t\\\n\tINSN_3(LDX, MEMSX, B),\t\t\t\\\n\tINSN_3(LDX, MEMSX, H),\t\t\t\\\n\tINSN_3(LDX, MEMSX, W),\t\t\t\\\n\t \t\t\\\n\tINSN_3(LD, IMM, DW)\n\nbool bpf_opcode_in_insntable(u8 code)\n{\n#define BPF_INSN_2_TBL(x, y)    [BPF_##x | BPF_##y] = true\n#define BPF_INSN_3_TBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = true\n\tstatic const bool public_insntable[256] = {\n\t\t[0 ... 255] = false,\n\t\t \n\t\tBPF_INSN_MAP(BPF_INSN_2_TBL, BPF_INSN_3_TBL),\n\t\t \n\t\t[BPF_LD | BPF_ABS | BPF_B] = true,\n\t\t[BPF_LD | BPF_ABS | BPF_H] = true,\n\t\t[BPF_LD | BPF_ABS | BPF_W] = true,\n\t\t[BPF_LD | BPF_IND | BPF_B] = true,\n\t\t[BPF_LD | BPF_IND | BPF_H] = true,\n\t\t[BPF_LD | BPF_IND | BPF_W] = true,\n\t};\n#undef BPF_INSN_3_TBL\n#undef BPF_INSN_2_TBL\n\treturn public_insntable[code];\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n \nstatic u64 ___bpf_prog_run(u64 *regs, const struct bpf_insn *insn)\n{\n#define BPF_INSN_2_LBL(x, y)    [BPF_##x | BPF_##y] = &&x##_##y\n#define BPF_INSN_3_LBL(x, y, z) [BPF_##x | BPF_##y | BPF_##z] = &&x##_##y##_##z\n\tstatic const void * const jumptable[256] __annotate_jump_table = {\n\t\t[0 ... 255] = &&default_label,\n\t\t \n\t\tBPF_INSN_MAP(BPF_INSN_2_LBL, BPF_INSN_3_LBL),\n\t\t \n\t\t[BPF_JMP | BPF_CALL_ARGS] = &&JMP_CALL_ARGS,\n\t\t[BPF_JMP | BPF_TAIL_CALL] = &&JMP_TAIL_CALL,\n\t\t[BPF_ST  | BPF_NOSPEC] = &&ST_NOSPEC,\n\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_B] = &&LDX_PROBE_MEM_B,\n\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_H] = &&LDX_PROBE_MEM_H,\n\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_W] = &&LDX_PROBE_MEM_W,\n\t\t[BPF_LDX | BPF_PROBE_MEM | BPF_DW] = &&LDX_PROBE_MEM_DW,\n\t\t[BPF_LDX | BPF_PROBE_MEMSX | BPF_B] = &&LDX_PROBE_MEMSX_B,\n\t\t[BPF_LDX | BPF_PROBE_MEMSX | BPF_H] = &&LDX_PROBE_MEMSX_H,\n\t\t[BPF_LDX | BPF_PROBE_MEMSX | BPF_W] = &&LDX_PROBE_MEMSX_W,\n\t};\n#undef BPF_INSN_3_LBL\n#undef BPF_INSN_2_LBL\n\tu32 tail_call_cnt = 0;\n\n#define CONT\t ({ insn++; goto select_insn; })\n#define CONT_JMP ({ insn++; goto select_insn; })\n\nselect_insn:\n\tgoto *jumptable[insn->code];\n\n\t \n\t \n#define SHT(OPCODE, OP)\t\t\t\t\t\\\n\tALU64_##OPCODE##_X:\t\t\t\t\\\n\t\tDST = DST OP (SRC & 63);\t\t\\\n\t\tCONT;\t\t\t\t\t\\\n\tALU_##OPCODE##_X:\t\t\t\t\\\n\t\tDST = (u32) DST OP ((u32) SRC & 31);\t\\\n\t\tCONT;\t\t\t\t\t\\\n\tALU64_##OPCODE##_K:\t\t\t\t\\\n\t\tDST = DST OP IMM;\t\t\t\\\n\t\tCONT;\t\t\t\t\t\\\n\tALU_##OPCODE##_K:\t\t\t\t\\\n\t\tDST = (u32) DST OP (u32) IMM;\t\t\\\n\t\tCONT;\n\t \n#define ALU(OPCODE, OP)\t\t\t\t\t\\\n\tALU64_##OPCODE##_X:\t\t\t\t\\\n\t\tDST = DST OP SRC;\t\t\t\\\n\t\tCONT;\t\t\t\t\t\\\n\tALU_##OPCODE##_X:\t\t\t\t\\\n\t\tDST = (u32) DST OP (u32) SRC;\t\t\\\n\t\tCONT;\t\t\t\t\t\\\n\tALU64_##OPCODE##_K:\t\t\t\t\\\n\t\tDST = DST OP IMM;\t\t\t\\\n\t\tCONT;\t\t\t\t\t\\\n\tALU_##OPCODE##_K:\t\t\t\t\\\n\t\tDST = (u32) DST OP (u32) IMM;\t\t\\\n\t\tCONT;\n\tALU(ADD,  +)\n\tALU(SUB,  -)\n\tALU(AND,  &)\n\tALU(OR,   |)\n\tALU(XOR,  ^)\n\tALU(MUL,  *)\n\tSHT(LSH, <<)\n\tSHT(RSH, >>)\n#undef SHT\n#undef ALU\n\tALU_NEG:\n\t\tDST = (u32) -DST;\n\t\tCONT;\n\tALU64_NEG:\n\t\tDST = -DST;\n\t\tCONT;\n\tALU_MOV_X:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tDST = (u32) SRC;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tDST = (u32)(s8) SRC;\n\t\t\tbreak;\n\t\tcase 16:\n\t\t\tDST = (u32)(s16) SRC;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_MOV_K:\n\t\tDST = (u32) IMM;\n\t\tCONT;\n\tALU64_MOV_X:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tDST = SRC;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tDST = (s8) SRC;\n\t\t\tbreak;\n\t\tcase 16:\n\t\t\tDST = (s16) SRC;\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (s32) SRC;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU64_MOV_K:\n\t\tDST = IMM;\n\t\tCONT;\n\tLD_IMM_DW:\n\t\tDST = (u64) (u32) insn[0].imm | ((u64) (u32) insn[1].imm) << 32;\n\t\tinsn++;\n\t\tCONT;\n\tALU_ARSH_X:\n\t\tDST = (u64) (u32) (((s32) DST) >> (SRC & 31));\n\t\tCONT;\n\tALU_ARSH_K:\n\t\tDST = (u64) (u32) (((s32) DST) >> IMM);\n\t\tCONT;\n\tALU64_ARSH_X:\n\t\t(*(s64 *) &DST) >>= (SRC & 63);\n\t\tCONT;\n\tALU64_ARSH_K:\n\t\t(*(s64 *) &DST) >>= IMM;\n\t\tCONT;\n\tALU64_MOD_X:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tdiv64_u64_rem(DST, SRC, &AX);\n\t\t\tDST = AX;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tAX = div64_s64(DST, SRC);\n\t\t\tDST = DST - AX * SRC;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_MOD_X:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tAX = (u32) DST;\n\t\t\tDST = do_div(AX, (u32) SRC);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tAX = abs((s32)DST);\n\t\t\tAX = do_div(AX, abs((s32)SRC));\n\t\t\tif ((s32)DST < 0)\n\t\t\t\tDST = (u32)-AX;\n\t\t\telse\n\t\t\t\tDST = (u32)AX;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU64_MOD_K:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tdiv64_u64_rem(DST, IMM, &AX);\n\t\t\tDST = AX;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tAX = div64_s64(DST, IMM);\n\t\t\tDST = DST - AX * IMM;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_MOD_K:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tAX = (u32) DST;\n\t\t\tDST = do_div(AX, (u32) IMM);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tAX = abs((s32)DST);\n\t\t\tAX = do_div(AX, abs((s32)IMM));\n\t\t\tif ((s32)DST < 0)\n\t\t\t\tDST = (u32)-AX;\n\t\t\telse\n\t\t\t\tDST = (u32)AX;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU64_DIV_X:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tDST = div64_u64(DST, SRC);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tDST = div64_s64(DST, SRC);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_DIV_X:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tAX = (u32) DST;\n\t\t\tdo_div(AX, (u32) SRC);\n\t\t\tDST = (u32) AX;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tAX = abs((s32)DST);\n\t\t\tdo_div(AX, abs((s32)SRC));\n\t\t\tif (((s32)DST < 0) == ((s32)SRC < 0))\n\t\t\t\tDST = (u32)AX;\n\t\t\telse\n\t\t\t\tDST = (u32)-AX;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU64_DIV_K:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tDST = div64_u64(DST, IMM);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tDST = div64_s64(DST, IMM);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_DIV_K:\n\t\tswitch (OFF) {\n\t\tcase 0:\n\t\t\tAX = (u32) DST;\n\t\t\tdo_div(AX, (u32) IMM);\n\t\t\tDST = (u32) AX;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tAX = abs((s32)DST);\n\t\t\tdo_div(AX, abs((s32)IMM));\n\t\t\tif (((s32)DST < 0) == ((s32)IMM < 0))\n\t\t\t\tDST = (u32)AX;\n\t\t\telse\n\t\t\t\tDST = (u32)-AX;\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_END_TO_BE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) cpu_to_be16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) cpu_to_be32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) cpu_to_be64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU_END_TO_LE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) cpu_to_le16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) cpu_to_le32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) cpu_to_le64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\tALU64_END_TO_LE:\n\t\tswitch (IMM) {\n\t\tcase 16:\n\t\t\tDST = (__force u16) __swab16(DST);\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tDST = (__force u32) __swab32(DST);\n\t\t\tbreak;\n\t\tcase 64:\n\t\t\tDST = (__force u64) __swab64(DST);\n\t\t\tbreak;\n\t\t}\n\t\tCONT;\n\n\t \n\tJMP_CALL:\n\t\t \n\t\tBPF_R0 = (__bpf_call_base + insn->imm)(BPF_R1, BPF_R2, BPF_R3,\n\t\t\t\t\t\t       BPF_R4, BPF_R5);\n\t\tCONT;\n\n\tJMP_CALL_ARGS:\n\t\tBPF_R0 = (__bpf_call_base_args + insn->imm)(BPF_R1, BPF_R2,\n\t\t\t\t\t\t\t    BPF_R3, BPF_R4,\n\t\t\t\t\t\t\t    BPF_R5,\n\t\t\t\t\t\t\t    insn + insn->off + 1);\n\t\tCONT;\n\n\tJMP_TAIL_CALL: {\n\t\tstruct bpf_map *map = (struct bpf_map *) (unsigned long) BPF_R2;\n\t\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\t\tstruct bpf_prog *prog;\n\t\tu32 index = BPF_R3;\n\n\t\tif (unlikely(index >= array->map.max_entries))\n\t\t\tgoto out;\n\n\t\tif (unlikely(tail_call_cnt >= MAX_TAIL_CALL_CNT))\n\t\t\tgoto out;\n\n\t\ttail_call_cnt++;\n\n\t\tprog = READ_ONCE(array->ptrs[index]);\n\t\tif (!prog)\n\t\t\tgoto out;\n\n\t\t \n\t\tinsn = prog->insnsi;\n\t\tgoto select_insn;\nout:\n\t\tCONT;\n\t}\n\tJMP_JA:\n\t\tinsn += insn->off;\n\t\tCONT;\n\tJMP32_JA:\n\t\tinsn += insn->imm;\n\t\tCONT;\n\tJMP_EXIT:\n\t\treturn BPF_R0;\n\t \n#define COND_JMP(SIGN, OPCODE, CMP_OP)\t\t\t\t\\\n\tJMP_##OPCODE##_X:\t\t\t\t\t\\\n\t\tif ((SIGN##64) DST CMP_OP (SIGN##64) SRC) {\t\\\n\t\t\tinsn += insn->off;\t\t\t\\\n\t\t\tCONT_JMP;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\tCONT;\t\t\t\t\t\t\\\n\tJMP32_##OPCODE##_X:\t\t\t\t\t\\\n\t\tif ((SIGN##32) DST CMP_OP (SIGN##32) SRC) {\t\\\n\t\t\tinsn += insn->off;\t\t\t\\\n\t\t\tCONT_JMP;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\tCONT;\t\t\t\t\t\t\\\n\tJMP_##OPCODE##_K:\t\t\t\t\t\\\n\t\tif ((SIGN##64) DST CMP_OP (SIGN##64) IMM) {\t\\\n\t\t\tinsn += insn->off;\t\t\t\\\n\t\t\tCONT_JMP;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\tCONT;\t\t\t\t\t\t\\\n\tJMP32_##OPCODE##_K:\t\t\t\t\t\\\n\t\tif ((SIGN##32) DST CMP_OP (SIGN##32) IMM) {\t\\\n\t\t\tinsn += insn->off;\t\t\t\\\n\t\t\tCONT_JMP;\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t\tCONT;\n\tCOND_JMP(u, JEQ, ==)\n\tCOND_JMP(u, JNE, !=)\n\tCOND_JMP(u, JGT, >)\n\tCOND_JMP(u, JLT, <)\n\tCOND_JMP(u, JGE, >=)\n\tCOND_JMP(u, JLE, <=)\n\tCOND_JMP(u, JSET, &)\n\tCOND_JMP(s, JSGT, >)\n\tCOND_JMP(s, JSLT, <)\n\tCOND_JMP(s, JSGE, >=)\n\tCOND_JMP(s, JSLE, <=)\n#undef COND_JMP\n\t \n\tST_NOSPEC:\n\t\t \n\t\tbarrier_nospec();\n\t\tCONT;\n#define LDST(SIZEOP, SIZE)\t\t\t\t\t\t\\\n\tSTX_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\t*(SIZE *)(unsigned long) (DST + insn->off) = SRC;\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tST_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\t*(SIZE *)(unsigned long) (DST + insn->off) = IMM;\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tLDX_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\tDST = *(SIZE *)(unsigned long) (SRC + insn->off);\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tLDX_PROBE_MEM_##SIZEOP:\t\t\t\t\t\t\\\n\t\tbpf_probe_read_kernel_common(&DST, sizeof(SIZE),\t\\\n\t\t\t      (const void *)(long) (SRC + insn->off));\t\\\n\t\tDST = *((SIZE *)&DST);\t\t\t\t\t\\\n\t\tCONT;\n\n\tLDST(B,   u8)\n\tLDST(H,  u16)\n\tLDST(W,  u32)\n\tLDST(DW, u64)\n#undef LDST\n\n#define LDSX(SIZEOP, SIZE)\t\t\t\t\t\t\\\n\tLDX_MEMSX_##SIZEOP:\t\t\t\t\t\t\\\n\t\tDST = *(SIZE *)(unsigned long) (SRC + insn->off);\t\\\n\t\tCONT;\t\t\t\t\t\t\t\\\n\tLDX_PROBE_MEMSX_##SIZEOP:\t\t\t\t\t\\\n\t\tbpf_probe_read_kernel_common(&DST, sizeof(SIZE),\t\t\\\n\t\t\t\t      (const void *)(long) (SRC + insn->off));\t\\\n\t\tDST = *((SIZE *)&DST);\t\t\t\t\t\\\n\t\tCONT;\n\n\tLDSX(B,   s8)\n\tLDSX(H,  s16)\n\tLDSX(W,  s32)\n#undef LDSX\n\n#define ATOMIC_ALU_OP(BOP, KOP)\t\t\t\t\t\t\\\n\t\tcase BOP:\t\t\t\t\t\t\\\n\t\t\tif (BPF_SIZE(insn->code) == BPF_W)\t\t\\\n\t\t\t\tatomic_##KOP((u32) SRC, (atomic_t *)(unsigned long) \\\n\t\t\t\t\t     (DST + insn->off));\t\\\n\t\t\telse\t\t\t\t\t\t\\\n\t\t\t\tatomic64_##KOP((u64) SRC, (atomic64_t *)(unsigned long) \\\n\t\t\t\t\t       (DST + insn->off));\t\\\n\t\t\tbreak;\t\t\t\t\t\t\\\n\t\tcase BOP | BPF_FETCH:\t\t\t\t\t\\\n\t\t\tif (BPF_SIZE(insn->code) == BPF_W)\t\t\\\n\t\t\t\tSRC = (u32) atomic_fetch_##KOP(\t\t\\\n\t\t\t\t\t(u32) SRC,\t\t\t\\\n\t\t\t\t\t(atomic_t *)(unsigned long) (DST + insn->off)); \\\n\t\t\telse\t\t\t\t\t\t\\\n\t\t\t\tSRC = (u64) atomic64_fetch_##KOP(\t\\\n\t\t\t\t\t(u64) SRC,\t\t\t\\\n\t\t\t\t\t(atomic64_t *)(unsigned long) (DST + insn->off)); \\\n\t\t\tbreak;\n\n\tSTX_ATOMIC_DW:\n\tSTX_ATOMIC_W:\n\t\tswitch (IMM) {\n\t\tATOMIC_ALU_OP(BPF_ADD, add)\n\t\tATOMIC_ALU_OP(BPF_AND, and)\n\t\tATOMIC_ALU_OP(BPF_OR, or)\n\t\tATOMIC_ALU_OP(BPF_XOR, xor)\n#undef ATOMIC_ALU_OP\n\n\t\tcase BPF_XCHG:\n\t\t\tif (BPF_SIZE(insn->code) == BPF_W)\n\t\t\t\tSRC = (u32) atomic_xchg(\n\t\t\t\t\t(atomic_t *)(unsigned long) (DST + insn->off),\n\t\t\t\t\t(u32) SRC);\n\t\t\telse\n\t\t\t\tSRC = (u64) atomic64_xchg(\n\t\t\t\t\t(atomic64_t *)(unsigned long) (DST + insn->off),\n\t\t\t\t\t(u64) SRC);\n\t\t\tbreak;\n\t\tcase BPF_CMPXCHG:\n\t\t\tif (BPF_SIZE(insn->code) == BPF_W)\n\t\t\t\tBPF_R0 = (u32) atomic_cmpxchg(\n\t\t\t\t\t(atomic_t *)(unsigned long) (DST + insn->off),\n\t\t\t\t\t(u32) BPF_R0, (u32) SRC);\n\t\t\telse\n\t\t\t\tBPF_R0 = (u64) atomic64_cmpxchg(\n\t\t\t\t\t(atomic64_t *)(unsigned long) (DST + insn->off),\n\t\t\t\t\t(u64) BPF_R0, (u64) SRC);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tgoto default_label;\n\t\t}\n\t\tCONT;\n\n\tdefault_label:\n\t\t \n\t\tpr_warn(\"BPF interpreter: unknown opcode %02x (imm: 0x%x)\\n\",\n\t\t\tinsn->code, insn->imm);\n\t\tBUG_ON(1);\n\t\treturn 0;\n}\n\n#define PROG_NAME(stack_size) __bpf_prog_run##stack_size\n#define DEFINE_BPF_PROG_RUN(stack_size) \\\nstatic unsigned int PROG_NAME(stack_size)(const void *ctx, const struct bpf_insn *insn) \\\n{ \\\n\tu64 stack[stack_size / sizeof(u64)]; \\\n\tu64 regs[MAX_BPF_EXT_REG] = {}; \\\n\\\n\tFP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)]; \\\n\tARG1 = (u64) (unsigned long) ctx; \\\n\treturn ___bpf_prog_run(regs, insn); \\\n}\n\n#define PROG_NAME_ARGS(stack_size) __bpf_prog_run_args##stack_size\n#define DEFINE_BPF_PROG_RUN_ARGS(stack_size) \\\nstatic u64 PROG_NAME_ARGS(stack_size)(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5, \\\n\t\t\t\t      const struct bpf_insn *insn) \\\n{ \\\n\tu64 stack[stack_size / sizeof(u64)]; \\\n\tu64 regs[MAX_BPF_EXT_REG]; \\\n\\\n\tFP = (u64) (unsigned long) &stack[ARRAY_SIZE(stack)]; \\\n\tBPF_R1 = r1; \\\n\tBPF_R2 = r2; \\\n\tBPF_R3 = r3; \\\n\tBPF_R4 = r4; \\\n\tBPF_R5 = r5; \\\n\treturn ___bpf_prog_run(regs, insn); \\\n}\n\n#define EVAL1(FN, X) FN(X)\n#define EVAL2(FN, X, Y...) FN(X) EVAL1(FN, Y)\n#define EVAL3(FN, X, Y...) FN(X) EVAL2(FN, Y)\n#define EVAL4(FN, X, Y...) FN(X) EVAL3(FN, Y)\n#define EVAL5(FN, X, Y...) FN(X) EVAL4(FN, Y)\n#define EVAL6(FN, X, Y...) FN(X) EVAL5(FN, Y)\n\nEVAL6(DEFINE_BPF_PROG_RUN, 32, 64, 96, 128, 160, 192);\nEVAL6(DEFINE_BPF_PROG_RUN, 224, 256, 288, 320, 352, 384);\nEVAL4(DEFINE_BPF_PROG_RUN, 416, 448, 480, 512);\n\nEVAL6(DEFINE_BPF_PROG_RUN_ARGS, 32, 64, 96, 128, 160, 192);\nEVAL6(DEFINE_BPF_PROG_RUN_ARGS, 224, 256, 288, 320, 352, 384);\nEVAL4(DEFINE_BPF_PROG_RUN_ARGS, 416, 448, 480, 512);\n\n#define PROG_NAME_LIST(stack_size) PROG_NAME(stack_size),\n\nstatic unsigned int (*interpreters[])(const void *ctx,\n\t\t\t\t      const struct bpf_insn *insn) = {\nEVAL6(PROG_NAME_LIST, 32, 64, 96, 128, 160, 192)\nEVAL6(PROG_NAME_LIST, 224, 256, 288, 320, 352, 384)\nEVAL4(PROG_NAME_LIST, 416, 448, 480, 512)\n};\n#undef PROG_NAME_LIST\n#define PROG_NAME_LIST(stack_size) PROG_NAME_ARGS(stack_size),\nstatic __maybe_unused\nu64 (*interpreters_args[])(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5,\n\t\t\t   const struct bpf_insn *insn) = {\nEVAL6(PROG_NAME_LIST, 32, 64, 96, 128, 160, 192)\nEVAL6(PROG_NAME_LIST, 224, 256, 288, 320, 352, 384)\nEVAL4(PROG_NAME_LIST, 416, 448, 480, 512)\n};\n#undef PROG_NAME_LIST\n\n#ifdef CONFIG_BPF_SYSCALL\nvoid bpf_patch_call_args(struct bpf_insn *insn, u32 stack_depth)\n{\n\tstack_depth = max_t(u32, stack_depth, 1);\n\tinsn->off = (s16) insn->imm;\n\tinsn->imm = interpreters_args[(round_up(stack_depth, 32) / 32) - 1] -\n\t\t__bpf_call_base_args;\n\tinsn->code = BPF_JMP | BPF_CALL_ARGS;\n}\n#endif\n#else\nstatic unsigned int __bpf_prog_ret0_warn(const void *ctx,\n\t\t\t\t\t const struct bpf_insn *insn)\n{\n\t \n\tWARN_ON_ONCE(1);\n\treturn 0;\n}\n#endif\n\nbool bpf_prog_map_compatible(struct bpf_map *map,\n\t\t\t     const struct bpf_prog *fp)\n{\n\tenum bpf_prog_type prog_type = resolve_prog_type(fp);\n\tbool ret;\n\n\tif (fp->kprobe_override)\n\t\treturn false;\n\n\t \n\tif (bpf_prog_is_dev_bound(fp->aux))\n\t\treturn false;\n\n\tspin_lock(&map->owner.lock);\n\tif (!map->owner.type) {\n\t\t \n\t\tmap->owner.type  = prog_type;\n\t\tmap->owner.jited = fp->jited;\n\t\tmap->owner.xdp_has_frags = fp->aux->xdp_has_frags;\n\t\tret = true;\n\t} else {\n\t\tret = map->owner.type  == prog_type &&\n\t\t      map->owner.jited == fp->jited &&\n\t\t      map->owner.xdp_has_frags == fp->aux->xdp_has_frags;\n\t}\n\tspin_unlock(&map->owner.lock);\n\n\treturn ret;\n}\n\nstatic int bpf_check_tail_call(const struct bpf_prog *fp)\n{\n\tstruct bpf_prog_aux *aux = fp->aux;\n\tint i, ret = 0;\n\n\tmutex_lock(&aux->used_maps_mutex);\n\tfor (i = 0; i < aux->used_map_cnt; i++) {\n\t\tstruct bpf_map *map = aux->used_maps[i];\n\n\t\tif (!map_type_contains_progs(map))\n\t\t\tcontinue;\n\n\t\tif (!bpf_prog_map_compatible(map, fp)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tmutex_unlock(&aux->used_maps_mutex);\n\treturn ret;\n}\n\nstatic void bpf_prog_select_func(struct bpf_prog *fp)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tu32 stack_depth = max_t(u32, fp->aux->stack_depth, 1);\n\n\tfp->bpf_func = interpreters[(round_up(stack_depth, 32) / 32) - 1];\n#else\n\tfp->bpf_func = __bpf_prog_ret0_warn;\n#endif\n}\n\n \nstruct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err)\n{\n\t \n\tbool jit_needed = false;\n\n\tif (fp->bpf_func)\n\t\tgoto finalize;\n\n\tif (IS_ENABLED(CONFIG_BPF_JIT_ALWAYS_ON) ||\n\t    bpf_prog_has_kfunc_call(fp))\n\t\tjit_needed = true;\n\n\tbpf_prog_select_func(fp);\n\n\t \n\tif (!bpf_prog_is_offloaded(fp->aux)) {\n\t\t*err = bpf_prog_alloc_jited_linfo(fp);\n\t\tif (*err)\n\t\t\treturn fp;\n\n\t\tfp = bpf_int_jit_compile(fp);\n\t\tbpf_prog_jit_attempt_done(fp);\n\t\tif (!fp->jited && jit_needed) {\n\t\t\t*err = -ENOTSUPP;\n\t\t\treturn fp;\n\t\t}\n\t} else {\n\t\t*err = bpf_prog_offload_compile(fp);\n\t\tif (*err)\n\t\t\treturn fp;\n\t}\n\nfinalize:\n\tbpf_prog_lock_ro(fp);\n\n\t \n\t*err = bpf_check_tail_call(fp);\n\n\treturn fp;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_select_runtime);\n\nstatic unsigned int __bpf_prog_ret1(const void *ctx,\n\t\t\t\t    const struct bpf_insn *insn)\n{\n\treturn 1;\n}\n\nstatic struct bpf_prog_dummy {\n\tstruct bpf_prog prog;\n} dummy_bpf_prog = {\n\t.prog = {\n\t\t.bpf_func = __bpf_prog_ret1,\n\t},\n};\n\nstruct bpf_empty_prog_array bpf_empty_prog_array = {\n\t.null_prog = NULL,\n};\nEXPORT_SYMBOL(bpf_empty_prog_array);\n\nstruct bpf_prog_array *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags)\n{\n\tif (prog_cnt)\n\t\treturn kzalloc(sizeof(struct bpf_prog_array) +\n\t\t\t       sizeof(struct bpf_prog_array_item) *\n\t\t\t       (prog_cnt + 1),\n\t\t\t       flags);\n\n\treturn &bpf_empty_prog_array.hdr;\n}\n\nvoid bpf_prog_array_free(struct bpf_prog_array *progs)\n{\n\tif (!progs || progs == &bpf_empty_prog_array.hdr)\n\t\treturn;\n\tkfree_rcu(progs, rcu);\n}\n\nstatic void __bpf_prog_array_free_sleepable_cb(struct rcu_head *rcu)\n{\n\tstruct bpf_prog_array *progs;\n\n\t \n\tprogs = container_of(rcu, struct bpf_prog_array, rcu);\n\tif (rcu_trace_implies_rcu_gp())\n\t\tkfree(progs);\n\telse\n\t\tkfree_rcu(progs, rcu);\n}\n\nvoid bpf_prog_array_free_sleepable(struct bpf_prog_array *progs)\n{\n\tif (!progs || progs == &bpf_empty_prog_array.hdr)\n\t\treturn;\n\tcall_rcu_tasks_trace(&progs->rcu, __bpf_prog_array_free_sleepable_cb);\n}\n\nint bpf_prog_array_length(struct bpf_prog_array *array)\n{\n\tstruct bpf_prog_array_item *item;\n\tu32 cnt = 0;\n\n\tfor (item = array->items; item->prog; item++)\n\t\tif (item->prog != &dummy_bpf_prog.prog)\n\t\t\tcnt++;\n\treturn cnt;\n}\n\nbool bpf_prog_array_is_empty(struct bpf_prog_array *array)\n{\n\tstruct bpf_prog_array_item *item;\n\n\tfor (item = array->items; item->prog; item++)\n\t\tif (item->prog != &dummy_bpf_prog.prog)\n\t\t\treturn false;\n\treturn true;\n}\n\nstatic bool bpf_prog_array_copy_core(struct bpf_prog_array *array,\n\t\t\t\t     u32 *prog_ids,\n\t\t\t\t     u32 request_cnt)\n{\n\tstruct bpf_prog_array_item *item;\n\tint i = 0;\n\n\tfor (item = array->items; item->prog; item++) {\n\t\tif (item->prog == &dummy_bpf_prog.prog)\n\t\t\tcontinue;\n\t\tprog_ids[i] = item->prog->aux->id;\n\t\tif (++i == request_cnt) {\n\t\t\titem++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn !!(item->prog);\n}\n\nint bpf_prog_array_copy_to_user(struct bpf_prog_array *array,\n\t\t\t\t__u32 __user *prog_ids, u32 cnt)\n{\n\tunsigned long err = 0;\n\tbool nospc;\n\tu32 *ids;\n\n\t \n\tids = kcalloc(cnt, sizeof(u32), GFP_USER | __GFP_NOWARN);\n\tif (!ids)\n\t\treturn -ENOMEM;\n\tnospc = bpf_prog_array_copy_core(array, ids, cnt);\n\terr = copy_to_user(prog_ids, ids, cnt * sizeof(u32));\n\tkfree(ids);\n\tif (err)\n\t\treturn -EFAULT;\n\tif (nospc)\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\nvoid bpf_prog_array_delete_safe(struct bpf_prog_array *array,\n\t\t\t\tstruct bpf_prog *old_prog)\n{\n\tstruct bpf_prog_array_item *item;\n\n\tfor (item = array->items; item->prog; item++)\n\t\tif (item->prog == old_prog) {\n\t\t\tWRITE_ONCE(item->prog, &dummy_bpf_prog.prog);\n\t\t\tbreak;\n\t\t}\n}\n\n \nint bpf_prog_array_delete_safe_at(struct bpf_prog_array *array, int index)\n{\n\treturn bpf_prog_array_update_at(array, index, &dummy_bpf_prog.prog);\n}\n\n \nint bpf_prog_array_update_at(struct bpf_prog_array *array, int index,\n\t\t\t     struct bpf_prog *prog)\n{\n\tstruct bpf_prog_array_item *item;\n\n\tif (unlikely(index < 0))\n\t\treturn -EINVAL;\n\n\tfor (item = array->items; item->prog; item++) {\n\t\tif (item->prog == &dummy_bpf_prog.prog)\n\t\t\tcontinue;\n\t\tif (!index) {\n\t\t\tWRITE_ONCE(item->prog, prog);\n\t\t\treturn 0;\n\t\t}\n\t\tindex--;\n\t}\n\treturn -ENOENT;\n}\n\nint bpf_prog_array_copy(struct bpf_prog_array *old_array,\n\t\t\tstruct bpf_prog *exclude_prog,\n\t\t\tstruct bpf_prog *include_prog,\n\t\t\tu64 bpf_cookie,\n\t\t\tstruct bpf_prog_array **new_array)\n{\n\tint new_prog_cnt, carry_prog_cnt = 0;\n\tstruct bpf_prog_array_item *existing, *new;\n\tstruct bpf_prog_array *array;\n\tbool found_exclude = false;\n\n\t \n\tif (old_array) {\n\t\texisting = old_array->items;\n\t\tfor (; existing->prog; existing++) {\n\t\t\tif (existing->prog == exclude_prog) {\n\t\t\t\tfound_exclude = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (existing->prog != &dummy_bpf_prog.prog)\n\t\t\t\tcarry_prog_cnt++;\n\t\t\tif (existing->prog == include_prog)\n\t\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\n\tif (exclude_prog && !found_exclude)\n\t\treturn -ENOENT;\n\n\t \n\tnew_prog_cnt = carry_prog_cnt;\n\tif (include_prog)\n\t\tnew_prog_cnt += 1;\n\n\t \n\tif (!new_prog_cnt) {\n\t\t*new_array = NULL;\n\t\treturn 0;\n\t}\n\n\t \n\tarray = bpf_prog_array_alloc(new_prog_cnt + 1, GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\tnew = array->items;\n\n\t \n\tif (carry_prog_cnt) {\n\t\texisting = old_array->items;\n\t\tfor (; existing->prog; existing++) {\n\t\t\tif (existing->prog == exclude_prog ||\n\t\t\t    existing->prog == &dummy_bpf_prog.prog)\n\t\t\t\tcontinue;\n\n\t\t\tnew->prog = existing->prog;\n\t\t\tnew->bpf_cookie = existing->bpf_cookie;\n\t\t\tnew++;\n\t\t}\n\t}\n\tif (include_prog) {\n\t\tnew->prog = include_prog;\n\t\tnew->bpf_cookie = bpf_cookie;\n\t\tnew++;\n\t}\n\tnew->prog = NULL;\n\t*new_array = array;\n\treturn 0;\n}\n\nint bpf_prog_array_copy_info(struct bpf_prog_array *array,\n\t\t\t     u32 *prog_ids, u32 request_cnt,\n\t\t\t     u32 *prog_cnt)\n{\n\tu32 cnt = 0;\n\n\tif (array)\n\t\tcnt = bpf_prog_array_length(array);\n\n\t*prog_cnt = cnt;\n\n\t \n\tif (!request_cnt || !cnt)\n\t\treturn 0;\n\n\t \n\treturn bpf_prog_array_copy_core(array, prog_ids, request_cnt) ? -ENOSPC\n\t\t\t\t\t\t\t\t     : 0;\n}\n\nvoid __bpf_free_used_maps(struct bpf_prog_aux *aux,\n\t\t\t  struct bpf_map **used_maps, u32 len)\n{\n\tstruct bpf_map *map;\n\tu32 i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tmap = used_maps[i];\n\t\tif (map->ops->map_poke_untrack)\n\t\t\tmap->ops->map_poke_untrack(map, aux);\n\t\tbpf_map_put(map);\n\t}\n}\n\nstatic void bpf_free_used_maps(struct bpf_prog_aux *aux)\n{\n\t__bpf_free_used_maps(aux, aux->used_maps, aux->used_map_cnt);\n\tkfree(aux->used_maps);\n}\n\nvoid __bpf_free_used_btfs(struct bpf_prog_aux *aux,\n\t\t\t  struct btf_mod_pair *used_btfs, u32 len)\n{\n#ifdef CONFIG_BPF_SYSCALL\n\tstruct btf_mod_pair *btf_mod;\n\tu32 i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tbtf_mod = &used_btfs[i];\n\t\tif (btf_mod->module)\n\t\t\tmodule_put(btf_mod->module);\n\t\tbtf_put(btf_mod->btf);\n\t}\n#endif\n}\n\nstatic void bpf_free_used_btfs(struct bpf_prog_aux *aux)\n{\n\t__bpf_free_used_btfs(aux, aux->used_btfs, aux->used_btf_cnt);\n\tkfree(aux->used_btfs);\n}\n\nstatic void bpf_prog_free_deferred(struct work_struct *work)\n{\n\tstruct bpf_prog_aux *aux;\n\tint i;\n\n\taux = container_of(work, struct bpf_prog_aux, work);\n#ifdef CONFIG_BPF_SYSCALL\n\tbpf_free_kfunc_btf_tab(aux->kfunc_btf_tab);\n#endif\n#ifdef CONFIG_CGROUP_BPF\n\tif (aux->cgroup_atype != CGROUP_BPF_ATTACH_TYPE_INVALID)\n\t\tbpf_cgroup_atype_put(aux->cgroup_atype);\n#endif\n\tbpf_free_used_maps(aux);\n\tbpf_free_used_btfs(aux);\n\tif (bpf_prog_is_dev_bound(aux))\n\t\tbpf_prog_dev_bound_destroy(aux->prog);\n#ifdef CONFIG_PERF_EVENTS\n\tif (aux->prog->has_callchain_buf)\n\t\tput_callchain_buffers();\n#endif\n\tif (aux->dst_trampoline)\n\t\tbpf_trampoline_put(aux->dst_trampoline);\n\tfor (i = 0; i < aux->func_cnt; i++) {\n\t\t \n\t\taux->func[i]->aux->poke_tab = NULL;\n\t\tbpf_jit_free(aux->func[i]);\n\t}\n\tif (aux->func_cnt) {\n\t\tkfree(aux->func);\n\t\tbpf_prog_unlock_free(aux->prog);\n\t} else {\n\t\tbpf_jit_free(aux->prog);\n\t}\n}\n\nvoid bpf_prog_free(struct bpf_prog *fp)\n{\n\tstruct bpf_prog_aux *aux = fp->aux;\n\n\tif (aux->dst_prog)\n\t\tbpf_prog_put(aux->dst_prog);\n\tINIT_WORK(&aux->work, bpf_prog_free_deferred);\n\tschedule_work(&aux->work);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_free);\n\n \nstatic DEFINE_PER_CPU(struct rnd_state, bpf_user_rnd_state);\n\nvoid bpf_user_rnd_init_once(void)\n{\n\tprandom_init_once(&bpf_user_rnd_state);\n}\n\nBPF_CALL_0(bpf_user_rnd_u32)\n{\n\t \n\tstruct rnd_state *state;\n\tu32 res;\n\n\tstate = &get_cpu_var(bpf_user_rnd_state);\n\tres = prandom_u32_state(state);\n\tput_cpu_var(bpf_user_rnd_state);\n\n\treturn res;\n}\n\nBPF_CALL_0(bpf_get_raw_cpu_id)\n{\n\treturn raw_smp_processor_id();\n}\n\n \nconst struct bpf_func_proto bpf_map_lookup_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_update_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_delete_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_push_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_pop_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_peek_elem_proto __weak;\nconst struct bpf_func_proto bpf_map_lookup_percpu_elem_proto __weak;\nconst struct bpf_func_proto bpf_spin_lock_proto __weak;\nconst struct bpf_func_proto bpf_spin_unlock_proto __weak;\nconst struct bpf_func_proto bpf_jiffies64_proto __weak;\n\nconst struct bpf_func_proto bpf_get_prandom_u32_proto __weak;\nconst struct bpf_func_proto bpf_get_smp_processor_id_proto __weak;\nconst struct bpf_func_proto bpf_get_numa_node_id_proto __weak;\nconst struct bpf_func_proto bpf_ktime_get_ns_proto __weak;\nconst struct bpf_func_proto bpf_ktime_get_boot_ns_proto __weak;\nconst struct bpf_func_proto bpf_ktime_get_coarse_ns_proto __weak;\nconst struct bpf_func_proto bpf_ktime_get_tai_ns_proto __weak;\n\nconst struct bpf_func_proto bpf_get_current_pid_tgid_proto __weak;\nconst struct bpf_func_proto bpf_get_current_uid_gid_proto __weak;\nconst struct bpf_func_proto bpf_get_current_comm_proto __weak;\nconst struct bpf_func_proto bpf_get_current_cgroup_id_proto __weak;\nconst struct bpf_func_proto bpf_get_current_ancestor_cgroup_id_proto __weak;\nconst struct bpf_func_proto bpf_get_local_storage_proto __weak;\nconst struct bpf_func_proto bpf_get_ns_current_pid_tgid_proto __weak;\nconst struct bpf_func_proto bpf_snprintf_btf_proto __weak;\nconst struct bpf_func_proto bpf_seq_printf_btf_proto __weak;\nconst struct bpf_func_proto bpf_set_retval_proto __weak;\nconst struct bpf_func_proto bpf_get_retval_proto __weak;\n\nconst struct bpf_func_proto * __weak bpf_get_trace_printk_proto(void)\n{\n\treturn NULL;\n}\n\nconst struct bpf_func_proto * __weak bpf_get_trace_vprintk_proto(void)\n{\n\treturn NULL;\n}\n\nu64 __weak\nbpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,\n\t\t void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy)\n{\n\treturn -ENOTSUPP;\n}\nEXPORT_SYMBOL_GPL(bpf_event_output);\n\n \nconst struct bpf_func_proto bpf_tail_call_proto = {\n\t.func\t\t= NULL,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_VOID,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\n \nstruct bpf_prog * __weak bpf_int_jit_compile(struct bpf_prog *prog)\n{\n\treturn prog;\n}\n\n \nvoid __weak bpf_jit_compile(struct bpf_prog *prog)\n{\n}\n\nbool __weak bpf_helper_changes_pkt_data(void *func)\n{\n\treturn false;\n}\n\n \nbool __weak bpf_jit_needs_zext(void)\n{\n\treturn false;\n}\n\n \nbool __weak bpf_jit_supports_subprog_tailcalls(void)\n{\n\treturn false;\n}\n\nbool __weak bpf_jit_supports_kfunc_call(void)\n{\n\treturn false;\n}\n\nbool __weak bpf_jit_supports_far_kfunc_call(void)\n{\n\treturn false;\n}\n\n \nint __weak skb_copy_bits(const struct sk_buff *skb, int offset, void *to,\n\t\t\t int len)\n{\n\treturn -EFAULT;\n}\n\nint __weak bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,\n\t\t\t      void *addr1, void *addr2)\n{\n\treturn -ENOTSUPP;\n}\n\nvoid * __weak bpf_arch_text_copy(void *dst, void *src, size_t len)\n{\n\treturn ERR_PTR(-ENOTSUPP);\n}\n\nint __weak bpf_arch_text_invalidate(void *dst, size_t len)\n{\n\treturn -ENOTSUPP;\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstatic int __init bpf_global_ma_init(void)\n{\n\tint ret;\n\n\tret = bpf_mem_alloc_init(&bpf_global_ma, 0, false);\n\tbpf_global_ma_set = !ret;\n\treturn ret;\n}\nlate_initcall(bpf_global_ma_init);\n#endif\n\nDEFINE_STATIC_KEY_FALSE(bpf_stats_enabled_key);\nEXPORT_SYMBOL(bpf_stats_enabled_key);\n\n \n#define CREATE_TRACE_POINTS\n#include <linux/bpf_trace.h>\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(xdp_exception);\nEXPORT_TRACEPOINT_SYMBOL_GPL(xdp_bulk_tx);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}