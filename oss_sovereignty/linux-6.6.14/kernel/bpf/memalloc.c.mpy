{
  "module_name": "memalloc.c",
  "hash_id": "570a707299d7c7e0f51d0a3356b4e153487e94d3cabea9b13dabeb31bbefc58d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/memalloc.c",
  "human_readable_source": "\n \n#include <linux/mm.h>\n#include <linux/llist.h>\n#include <linux/bpf.h>\n#include <linux/irq_work.h>\n#include <linux/bpf_mem_alloc.h>\n#include <linux/memcontrol.h>\n#include <asm/local.h>\n\n \n#define LLIST_NODE_SZ sizeof(struct llist_node)\n\n \nstatic u8 size_index[24] __ro_after_init = {\n\t3,\t \n\t3,\t \n\t4,\t \n\t4,\t \n\t5,\t \n\t5,\t \n\t5,\t \n\t5,\t \n\t1,\t \n\t1,\t \n\t1,\t \n\t1,\t \n\t6,\t \n\t6,\t \n\t6,\t \n\t6,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2,\t \n\t2\t \n};\n\nstatic int bpf_mem_cache_idx(size_t size)\n{\n\tif (!size || size > 4096)\n\t\treturn -1;\n\n\tif (size <= 192)\n\t\treturn size_index[(size - 1) / 8] - 1;\n\n\treturn fls(size - 1) - 2;\n}\n\n#define NUM_CACHES 11\n\nstruct bpf_mem_cache {\n\t \n\tstruct llist_head free_llist;\n\tlocal_t active;\n\n\t \n\tstruct llist_head free_llist_extra;\n\n\tstruct irq_work refill_work;\n\tstruct obj_cgroup *objcg;\n\tint unit_size;\n\t \n\tint free_cnt;\n\tint low_watermark, high_watermark, batch;\n\tint percpu_size;\n\tbool draining;\n\tstruct bpf_mem_cache *tgt;\n\n\t \n\tstruct llist_head free_by_rcu;\n\tstruct llist_node *free_by_rcu_tail;\n\tstruct llist_head waiting_for_gp;\n\tstruct llist_node *waiting_for_gp_tail;\n\tstruct rcu_head rcu;\n\tatomic_t call_rcu_in_progress;\n\tstruct llist_head free_llist_extra_rcu;\n\n\t \n\tstruct llist_head free_by_rcu_ttrace;\n\tstruct llist_head waiting_for_gp_ttrace;\n\tstruct rcu_head rcu_ttrace;\n\tatomic_t call_rcu_ttrace_in_progress;\n};\n\nstruct bpf_mem_caches {\n\tstruct bpf_mem_cache cache[NUM_CACHES];\n};\n\nstatic struct llist_node notrace *__llist_del_first(struct llist_head *head)\n{\n\tstruct llist_node *entry, *next;\n\n\tentry = head->first;\n\tif (!entry)\n\t\treturn NULL;\n\tnext = entry->next;\n\thead->first = next;\n\treturn entry;\n}\n\nstatic void *__alloc(struct bpf_mem_cache *c, int node, gfp_t flags)\n{\n\tif (c->percpu_size) {\n\t\tvoid **obj = kmalloc_node(c->percpu_size, flags, node);\n\t\tvoid *pptr = __alloc_percpu_gfp(c->unit_size, 8, flags);\n\n\t\tif (!obj || !pptr) {\n\t\t\tfree_percpu(pptr);\n\t\t\tkfree(obj);\n\t\t\treturn NULL;\n\t\t}\n\t\tobj[1] = pptr;\n\t\treturn obj;\n\t}\n\n\treturn kmalloc_node(c->unit_size, flags | __GFP_ZERO, node);\n}\n\nstatic struct mem_cgroup *get_memcg(const struct bpf_mem_cache *c)\n{\n#ifdef CONFIG_MEMCG_KMEM\n\tif (c->objcg)\n\t\treturn get_mem_cgroup_from_objcg(c->objcg);\n#endif\n\n#ifdef CONFIG_MEMCG\n\treturn root_mem_cgroup;\n#else\n\treturn NULL;\n#endif\n}\n\nstatic void inc_active(struct bpf_mem_cache *c, unsigned long *flags)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\t \n\t\tlocal_irq_save(*flags);\n\t \n\tWARN_ON_ONCE(local_inc_return(&c->active) != 1);\n}\n\nstatic void dec_active(struct bpf_mem_cache *c, unsigned long *flags)\n{\n\tlocal_dec(&c->active);\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tlocal_irq_restore(*flags);\n}\n\nstatic void add_obj_to_free_list(struct bpf_mem_cache *c, void *obj)\n{\n\tunsigned long flags;\n\n\tinc_active(c, &flags);\n\t__llist_add(obj, &c->free_llist);\n\tc->free_cnt++;\n\tdec_active(c, &flags);\n}\n\n \nstatic void alloc_bulk(struct bpf_mem_cache *c, int cnt, int node, bool atomic)\n{\n\tstruct mem_cgroup *memcg = NULL, *old_memcg;\n\tgfp_t gfp;\n\tvoid *obj;\n\tint i;\n\n\tgfp = __GFP_NOWARN | __GFP_ACCOUNT;\n\tgfp |= atomic ? GFP_NOWAIT : GFP_KERNEL;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\t \n\t\tobj = llist_del_first(&c->free_by_rcu_ttrace);\n\t\tif (!obj)\n\t\t\tbreak;\n\t\tadd_obj_to_free_list(c, obj);\n\t}\n\tif (i >= cnt)\n\t\treturn;\n\n\tfor (; i < cnt; i++) {\n\t\tobj = llist_del_first(&c->waiting_for_gp_ttrace);\n\t\tif (!obj)\n\t\t\tbreak;\n\t\tadd_obj_to_free_list(c, obj);\n\t}\n\tif (i >= cnt)\n\t\treturn;\n\n\tmemcg = get_memcg(c);\n\told_memcg = set_active_memcg(memcg);\n\tfor (; i < cnt; i++) {\n\t\t \n\t\tobj = __alloc(c, node, gfp);\n\t\tif (!obj)\n\t\t\tbreak;\n\t\tadd_obj_to_free_list(c, obj);\n\t}\n\tset_active_memcg(old_memcg);\n\tmem_cgroup_put(memcg);\n}\n\nstatic void free_one(void *obj, bool percpu)\n{\n\tif (percpu) {\n\t\tfree_percpu(((void **)obj)[1]);\n\t\tkfree(obj);\n\t\treturn;\n\t}\n\n\tkfree(obj);\n}\n\nstatic int free_all(struct llist_node *llnode, bool percpu)\n{\n\tstruct llist_node *pos, *t;\n\tint cnt = 0;\n\n\tllist_for_each_safe(pos, t, llnode) {\n\t\tfree_one(pos, percpu);\n\t\tcnt++;\n\t}\n\treturn cnt;\n}\n\nstatic void __free_rcu(struct rcu_head *head)\n{\n\tstruct bpf_mem_cache *c = container_of(head, struct bpf_mem_cache, rcu_ttrace);\n\n\tfree_all(llist_del_all(&c->waiting_for_gp_ttrace), !!c->percpu_size);\n\tatomic_set(&c->call_rcu_ttrace_in_progress, 0);\n}\n\nstatic void __free_rcu_tasks_trace(struct rcu_head *head)\n{\n\t \n\tif (rcu_trace_implies_rcu_gp())\n\t\t__free_rcu(head);\n\telse\n\t\tcall_rcu(head, __free_rcu);\n}\n\nstatic void enque_to_free(struct bpf_mem_cache *c, void *obj)\n{\n\tstruct llist_node *llnode = obj;\n\n\t \n\tllist_add(llnode, &c->free_by_rcu_ttrace);\n}\n\nstatic void do_call_rcu_ttrace(struct bpf_mem_cache *c)\n{\n\tstruct llist_node *llnode, *t;\n\n\tif (atomic_xchg(&c->call_rcu_ttrace_in_progress, 1)) {\n\t\tif (unlikely(READ_ONCE(c->draining))) {\n\t\t\tllnode = llist_del_all(&c->free_by_rcu_ttrace);\n\t\t\tfree_all(llnode, !!c->percpu_size);\n\t\t}\n\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));\n\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_by_rcu_ttrace))\n\t\tllist_add(llnode, &c->waiting_for_gp_ttrace);\n\n\tif (unlikely(READ_ONCE(c->draining))) {\n\t\t__free_rcu(&c->rcu_ttrace);\n\t\treturn;\n\t}\n\n\t \n\tcall_rcu_tasks_trace(&c->rcu_ttrace, __free_rcu_tasks_trace);\n}\n\nstatic void free_bulk(struct bpf_mem_cache *c)\n{\n\tstruct bpf_mem_cache *tgt = c->tgt;\n\tstruct llist_node *llnode, *t;\n\tunsigned long flags;\n\tint cnt;\n\n\tWARN_ON_ONCE(tgt->unit_size != c->unit_size);\n\n\tdo {\n\t\tinc_active(c, &flags);\n\t\tllnode = __llist_del_first(&c->free_llist);\n\t\tif (llnode)\n\t\t\tcnt = --c->free_cnt;\n\t\telse\n\t\t\tcnt = 0;\n\t\tdec_active(c, &flags);\n\t\tif (llnode)\n\t\t\tenque_to_free(tgt, llnode);\n\t} while (cnt > (c->high_watermark + c->low_watermark) / 2);\n\n\t \n\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra))\n\t\tenque_to_free(tgt, llnode);\n\tdo_call_rcu_ttrace(tgt);\n}\n\nstatic void __free_by_rcu(struct rcu_head *head)\n{\n\tstruct bpf_mem_cache *c = container_of(head, struct bpf_mem_cache, rcu);\n\tstruct bpf_mem_cache *tgt = c->tgt;\n\tstruct llist_node *llnode;\n\n\tllnode = llist_del_all(&c->waiting_for_gp);\n\tif (!llnode)\n\t\tgoto out;\n\n\tllist_add_batch(llnode, c->waiting_for_gp_tail, &tgt->free_by_rcu_ttrace);\n\n\t \n\tdo_call_rcu_ttrace(tgt);\nout:\n\tatomic_set(&c->call_rcu_in_progress, 0);\n}\n\nstatic void check_free_by_rcu(struct bpf_mem_cache *c)\n{\n\tstruct llist_node *llnode, *t;\n\tunsigned long flags;\n\n\t \n\tif (unlikely(!llist_empty(&c->free_llist_extra_rcu))) {\n\t\tinc_active(c, &flags);\n\t\tllist_for_each_safe(llnode, t, llist_del_all(&c->free_llist_extra_rcu))\n\t\t\tif (__llist_add(llnode, &c->free_by_rcu))\n\t\t\t\tc->free_by_rcu_tail = llnode;\n\t\tdec_active(c, &flags);\n\t}\n\n\tif (llist_empty(&c->free_by_rcu))\n\t\treturn;\n\n\tif (atomic_xchg(&c->call_rcu_in_progress, 1)) {\n\t\t \n\t\trcu_request_urgent_qs_task(current);\n\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));\n\n\tinc_active(c, &flags);\n\tWRITE_ONCE(c->waiting_for_gp.first, __llist_del_all(&c->free_by_rcu));\n\tc->waiting_for_gp_tail = c->free_by_rcu_tail;\n\tdec_active(c, &flags);\n\n\tif (unlikely(READ_ONCE(c->draining))) {\n\t\tfree_all(llist_del_all(&c->waiting_for_gp), !!c->percpu_size);\n\t\tatomic_set(&c->call_rcu_in_progress, 0);\n\t} else {\n\t\tcall_rcu_hurry(&c->rcu, __free_by_rcu);\n\t}\n}\n\nstatic void bpf_mem_refill(struct irq_work *work)\n{\n\tstruct bpf_mem_cache *c = container_of(work, struct bpf_mem_cache, refill_work);\n\tint cnt;\n\n\t \n\tcnt = c->free_cnt;\n\tif (cnt < c->low_watermark)\n\t\t \n\t\talloc_bulk(c, c->batch, NUMA_NO_NODE, true);\n\telse if (cnt > c->high_watermark)\n\t\tfree_bulk(c);\n\n\tcheck_free_by_rcu(c);\n}\n\nstatic void notrace irq_work_raise(struct bpf_mem_cache *c)\n{\n\tirq_work_queue(&c->refill_work);\n}\n\n \nstatic void init_refill_work(struct bpf_mem_cache *c)\n{\n\tinit_irq_work(&c->refill_work, bpf_mem_refill);\n\tif (c->unit_size <= 256) {\n\t\tc->low_watermark = 32;\n\t\tc->high_watermark = 96;\n\t} else {\n\t\t \n\t\tc->low_watermark = max(32 * 256 / c->unit_size, 1);\n\t\tc->high_watermark = max(96 * 256 / c->unit_size, 3);\n\t}\n\tc->batch = max((c->high_watermark - c->low_watermark) / 4 * 3, 1);\n}\n\nstatic void prefill_mem_cache(struct bpf_mem_cache *c, int cpu)\n{\n\t \n\talloc_bulk(c, c->unit_size <= 256 ? 4 : 1, cpu_to_node(cpu), false);\n}\n\n \nint bpf_mem_alloc_init(struct bpf_mem_alloc *ma, int size, bool percpu)\n{\n\tstatic u16 sizes[NUM_CACHES] = {96, 192, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096};\n\tstruct bpf_mem_caches *cc, __percpu *pcc;\n\tstruct bpf_mem_cache *c, __percpu *pc;\n\tstruct obj_cgroup *objcg = NULL;\n\tint cpu, i, unit_size, percpu_size = 0;\n\n\tma->percpu = percpu;\n\n\tif (size) {\n\t\tpc = __alloc_percpu_gfp(sizeof(*pc), 8, GFP_KERNEL);\n\t\tif (!pc)\n\t\t\treturn -ENOMEM;\n\n\t\tif (percpu)\n\t\t\t \n\t\t\tpercpu_size = LLIST_NODE_SZ + sizeof(void *);\n\t\telse\n\t\t\tsize += LLIST_NODE_SZ;  \n\t\tunit_size = size;\n\n#ifdef CONFIG_MEMCG_KMEM\n\t\tif (memcg_bpf_enabled())\n\t\t\tobjcg = get_obj_cgroup_from_current();\n#endif\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tc = per_cpu_ptr(pc, cpu);\n\t\t\tc->unit_size = unit_size;\n\t\t\tc->objcg = objcg;\n\t\t\tc->percpu_size = percpu_size;\n\t\t\tc->tgt = c;\n\t\t\tinit_refill_work(c);\n\t\t\tprefill_mem_cache(c, cpu);\n\t\t}\n\t\tma->cache = pc;\n\t\treturn 0;\n\t}\n\n\t \n\tif (WARN_ON_ONCE(percpu))\n\t\treturn -EINVAL;\n\n\tpcc = __alloc_percpu_gfp(sizeof(*cc), 8, GFP_KERNEL);\n\tif (!pcc)\n\t\treturn -ENOMEM;\n#ifdef CONFIG_MEMCG_KMEM\n\tobjcg = get_obj_cgroup_from_current();\n#endif\n\tfor_each_possible_cpu(cpu) {\n\t\tcc = per_cpu_ptr(pcc, cpu);\n\t\tfor (i = 0; i < NUM_CACHES; i++) {\n\t\t\tc = &cc->cache[i];\n\t\t\tc->unit_size = sizes[i];\n\t\t\tc->objcg = objcg;\n\t\t\tc->tgt = c;\n\n\t\t\tinit_refill_work(c);\n\t\t\tprefill_mem_cache(c, cpu);\n\t\t}\n\t}\n\n\tma->caches = pcc;\n\treturn 0;\n}\n\nstatic void drain_mem_cache(struct bpf_mem_cache *c)\n{\n\tbool percpu = !!c->percpu_size;\n\n\t \n\tfree_all(llist_del_all(&c->free_by_rcu_ttrace), percpu);\n\tfree_all(llist_del_all(&c->waiting_for_gp_ttrace), percpu);\n\tfree_all(__llist_del_all(&c->free_llist), percpu);\n\tfree_all(__llist_del_all(&c->free_llist_extra), percpu);\n\tfree_all(__llist_del_all(&c->free_by_rcu), percpu);\n\tfree_all(__llist_del_all(&c->free_llist_extra_rcu), percpu);\n\tfree_all(llist_del_all(&c->waiting_for_gp), percpu);\n}\n\nstatic void check_mem_cache(struct bpf_mem_cache *c)\n{\n\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu_ttrace));\n\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp_ttrace));\n\tWARN_ON_ONCE(!llist_empty(&c->free_llist));\n\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra));\n\tWARN_ON_ONCE(!llist_empty(&c->free_by_rcu));\n\tWARN_ON_ONCE(!llist_empty(&c->free_llist_extra_rcu));\n\tWARN_ON_ONCE(!llist_empty(&c->waiting_for_gp));\n}\n\nstatic void check_leaked_objs(struct bpf_mem_alloc *ma)\n{\n\tstruct bpf_mem_caches *cc;\n\tstruct bpf_mem_cache *c;\n\tint cpu, i;\n\n\tif (ma->cache) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tc = per_cpu_ptr(ma->cache, cpu);\n\t\t\tcheck_mem_cache(c);\n\t\t}\n\t}\n\tif (ma->caches) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tcc = per_cpu_ptr(ma->caches, cpu);\n\t\t\tfor (i = 0; i < NUM_CACHES; i++) {\n\t\t\t\tc = &cc->cache[i];\n\t\t\t\tcheck_mem_cache(c);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void free_mem_alloc_no_barrier(struct bpf_mem_alloc *ma)\n{\n\tcheck_leaked_objs(ma);\n\tfree_percpu(ma->cache);\n\tfree_percpu(ma->caches);\n\tma->cache = NULL;\n\tma->caches = NULL;\n}\n\nstatic void free_mem_alloc(struct bpf_mem_alloc *ma)\n{\n\t \n\trcu_barrier();  \n\trcu_barrier_tasks_trace();  \n\tif (!rcu_trace_implies_rcu_gp())\n\t\trcu_barrier();\n\tfree_mem_alloc_no_barrier(ma);\n}\n\nstatic void free_mem_alloc_deferred(struct work_struct *work)\n{\n\tstruct bpf_mem_alloc *ma = container_of(work, struct bpf_mem_alloc, work);\n\n\tfree_mem_alloc(ma);\n\tkfree(ma);\n}\n\nstatic void destroy_mem_alloc(struct bpf_mem_alloc *ma, int rcu_in_progress)\n{\n\tstruct bpf_mem_alloc *copy;\n\n\tif (!rcu_in_progress) {\n\t\t \n\t\tfree_mem_alloc_no_barrier(ma);\n\t\treturn;\n\t}\n\n\tcopy = kmemdup(ma, sizeof(*ma), GFP_KERNEL);\n\tif (!copy) {\n\t\t \n\t\tfree_mem_alloc(ma);\n\t\treturn;\n\t}\n\n\t \n\tmemset(ma, 0, sizeof(*ma));\n\tINIT_WORK(&copy->work, free_mem_alloc_deferred);\n\tqueue_work(system_unbound_wq, &copy->work);\n}\n\nvoid bpf_mem_alloc_destroy(struct bpf_mem_alloc *ma)\n{\n\tstruct bpf_mem_caches *cc;\n\tstruct bpf_mem_cache *c;\n\tint cpu, i, rcu_in_progress;\n\n\tif (ma->cache) {\n\t\trcu_in_progress = 0;\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tc = per_cpu_ptr(ma->cache, cpu);\n\t\t\tWRITE_ONCE(c->draining, true);\n\t\t\tirq_work_sync(&c->refill_work);\n\t\t\tdrain_mem_cache(c);\n\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);\n\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);\n\t\t}\n\t\t \n\t\tif (c->objcg)\n\t\t\tobj_cgroup_put(c->objcg);\n\t\tdestroy_mem_alloc(ma, rcu_in_progress);\n\t}\n\tif (ma->caches) {\n\t\trcu_in_progress = 0;\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tcc = per_cpu_ptr(ma->caches, cpu);\n\t\t\tfor (i = 0; i < NUM_CACHES; i++) {\n\t\t\t\tc = &cc->cache[i];\n\t\t\t\tWRITE_ONCE(c->draining, true);\n\t\t\t\tirq_work_sync(&c->refill_work);\n\t\t\t\tdrain_mem_cache(c);\n\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_ttrace_in_progress);\n\t\t\t\trcu_in_progress += atomic_read(&c->call_rcu_in_progress);\n\t\t\t}\n\t\t}\n\t\tif (c->objcg)\n\t\t\tobj_cgroup_put(c->objcg);\n\t\tdestroy_mem_alloc(ma, rcu_in_progress);\n\t}\n}\n\n \nstatic void notrace *unit_alloc(struct bpf_mem_cache *c)\n{\n\tstruct llist_node *llnode = NULL;\n\tunsigned long flags;\n\tint cnt = 0;\n\n\t \n\tlocal_irq_save(flags);\n\tif (local_inc_return(&c->active) == 1) {\n\t\tllnode = __llist_del_first(&c->free_llist);\n\t\tif (llnode) {\n\t\t\tcnt = --c->free_cnt;\n\t\t\t*(struct bpf_mem_cache **)llnode = c;\n\t\t}\n\t}\n\tlocal_dec(&c->active);\n\tlocal_irq_restore(flags);\n\n\tWARN_ON(cnt < 0);\n\n\tif (cnt < c->low_watermark)\n\t\tirq_work_raise(c);\n\treturn llnode;\n}\n\n \nstatic void notrace unit_free(struct bpf_mem_cache *c, void *ptr)\n{\n\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;\n\tunsigned long flags;\n\tint cnt = 0;\n\n\tBUILD_BUG_ON(LLIST_NODE_SZ > 8);\n\n\t \n\tc->tgt = *(struct bpf_mem_cache **)llnode;\n\n\tlocal_irq_save(flags);\n\tif (local_inc_return(&c->active) == 1) {\n\t\t__llist_add(llnode, &c->free_llist);\n\t\tcnt = ++c->free_cnt;\n\t} else {\n\t\t \n\t\tllist_add(llnode, &c->free_llist_extra);\n\t}\n\tlocal_dec(&c->active);\n\tlocal_irq_restore(flags);\n\n\tif (cnt > c->high_watermark)\n\t\t \n\t\tirq_work_raise(c);\n}\n\nstatic void notrace unit_free_rcu(struct bpf_mem_cache *c, void *ptr)\n{\n\tstruct llist_node *llnode = ptr - LLIST_NODE_SZ;\n\tunsigned long flags;\n\n\tc->tgt = *(struct bpf_mem_cache **)llnode;\n\n\tlocal_irq_save(flags);\n\tif (local_inc_return(&c->active) == 1) {\n\t\tif (__llist_add(llnode, &c->free_by_rcu))\n\t\t\tc->free_by_rcu_tail = llnode;\n\t} else {\n\t\tllist_add(llnode, &c->free_llist_extra_rcu);\n\t}\n\tlocal_dec(&c->active);\n\tlocal_irq_restore(flags);\n\n\tif (!atomic_read(&c->call_rcu_in_progress))\n\t\tirq_work_raise(c);\n}\n\n \nvoid notrace *bpf_mem_alloc(struct bpf_mem_alloc *ma, size_t size)\n{\n\tint idx;\n\tvoid *ret;\n\n\tif (!size)\n\t\treturn NULL;\n\n\tidx = bpf_mem_cache_idx(size + LLIST_NODE_SZ);\n\tif (idx < 0)\n\t\treturn NULL;\n\n\tret = unit_alloc(this_cpu_ptr(ma->caches)->cache + idx);\n\treturn !ret ? NULL : ret + LLIST_NODE_SZ;\n}\n\nvoid notrace bpf_mem_free(struct bpf_mem_alloc *ma, void *ptr)\n{\n\tstruct bpf_mem_cache *c;\n\tint idx;\n\n\tif (!ptr)\n\t\treturn;\n\n\tc = *(void **)(ptr - LLIST_NODE_SZ);\n\tidx = bpf_mem_cache_idx(c->unit_size);\n\tif (WARN_ON_ONCE(idx < 0))\n\t\treturn;\n\n\tunit_free(this_cpu_ptr(ma->caches)->cache + idx, ptr);\n}\n\nvoid notrace bpf_mem_free_rcu(struct bpf_mem_alloc *ma, void *ptr)\n{\n\tstruct bpf_mem_cache *c;\n\tint idx;\n\n\tif (!ptr)\n\t\treturn;\n\n\tc = *(void **)(ptr - LLIST_NODE_SZ);\n\tidx = bpf_mem_cache_idx(c->unit_size);\n\tif (WARN_ON_ONCE(idx < 0))\n\t\treturn;\n\n\tunit_free_rcu(this_cpu_ptr(ma->caches)->cache + idx, ptr);\n}\n\nvoid notrace *bpf_mem_cache_alloc(struct bpf_mem_alloc *ma)\n{\n\tvoid *ret;\n\n\tret = unit_alloc(this_cpu_ptr(ma->cache));\n\treturn !ret ? NULL : ret + LLIST_NODE_SZ;\n}\n\nvoid notrace bpf_mem_cache_free(struct bpf_mem_alloc *ma, void *ptr)\n{\n\tif (!ptr)\n\t\treturn;\n\n\tunit_free(this_cpu_ptr(ma->cache), ptr);\n}\n\nvoid notrace bpf_mem_cache_free_rcu(struct bpf_mem_alloc *ma, void *ptr)\n{\n\tif (!ptr)\n\t\treturn;\n\n\tunit_free_rcu(this_cpu_ptr(ma->cache), ptr);\n}\n\n \nvoid bpf_mem_cache_raw_free(void *ptr)\n{\n\tif (!ptr)\n\t\treturn;\n\n\tkfree(ptr - LLIST_NODE_SZ);\n}\n\n \nvoid notrace *bpf_mem_cache_alloc_flags(struct bpf_mem_alloc *ma, gfp_t flags)\n{\n\tstruct bpf_mem_cache *c;\n\tvoid *ret;\n\n\tc = this_cpu_ptr(ma->cache);\n\n\tret = unit_alloc(c);\n\tif (!ret && flags == GFP_KERNEL) {\n\t\tstruct mem_cgroup *memcg, *old_memcg;\n\n\t\tmemcg = get_memcg(c);\n\t\told_memcg = set_active_memcg(memcg);\n\t\tret = __alloc(c, NUMA_NO_NODE, GFP_KERNEL | __GFP_NOWARN | __GFP_ACCOUNT);\n\t\tif (ret)\n\t\t\t*(struct bpf_mem_cache **)ret = c;\n\t\tset_active_memcg(old_memcg);\n\t\tmem_cgroup_put(memcg);\n\t}\n\n\treturn !ret ? NULL : ret + LLIST_NODE_SZ;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}