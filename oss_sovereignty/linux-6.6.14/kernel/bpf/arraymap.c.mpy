{
  "module_name": "arraymap.c",
  "hash_id": "94d0e94d8f2288cf38c89320ff700189adb3c7245bac09d48f95dfb152135408",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/arraymap.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/err.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/filter.h>\n#include <linux/perf_event.h>\n#include <uapi/linux/btf.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/btf_ids.h>\n\n#include \"map_in_map.h\"\n\n#define ARRAY_CREATE_FLAG_MASK \\\n\t(BPF_F_NUMA_NODE | BPF_F_MMAPABLE | BPF_F_ACCESS_MASK | \\\n\t BPF_F_PRESERVE_ELEMS | BPF_F_INNER_MAP)\n\nstatic void bpf_array_free_percpu(struct bpf_array *array)\n{\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tfree_percpu(array->pptrs[i]);\n\t\tcond_resched();\n\t}\n}\n\nstatic int bpf_array_alloc_percpu(struct bpf_array *array)\n{\n\tvoid __percpu *ptr;\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tptr = bpf_map_alloc_percpu(&array->map, array->elem_size, 8,\n\t\t\t\t\t   GFP_USER | __GFP_NOWARN);\n\t\tif (!ptr) {\n\t\t\tbpf_array_free_percpu(array);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tarray->pptrs[i] = ptr;\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\n \nint array_map_alloc_check(union bpf_attr *attr)\n{\n\tbool percpu = attr->map_type == BPF_MAP_TYPE_PERCPU_ARRAY;\n\tint numa_node = bpf_map_attr_numa_node(attr);\n\n\t \n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    attr->value_size == 0 ||\n\t    attr->map_flags & ~ARRAY_CREATE_FLAG_MASK ||\n\t    !bpf_map_flags_access_ok(attr->map_flags) ||\n\t    (percpu && numa_node != NUMA_NO_NODE))\n\t\treturn -EINVAL;\n\n\tif (attr->map_type != BPF_MAP_TYPE_ARRAY &&\n\t    attr->map_flags & (BPF_F_MMAPABLE | BPF_F_INNER_MAP))\n\t\treturn -EINVAL;\n\n\tif (attr->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY &&\n\t    attr->map_flags & BPF_F_PRESERVE_ELEMS)\n\t\treturn -EINVAL;\n\n\t \n\tif (attr->value_size > INT_MAX)\n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nstatic struct bpf_map *array_map_alloc(union bpf_attr *attr)\n{\n\tbool percpu = attr->map_type == BPF_MAP_TYPE_PERCPU_ARRAY;\n\tint numa_node = bpf_map_attr_numa_node(attr);\n\tu32 elem_size, index_mask, max_entries;\n\tbool bypass_spec_v1 = bpf_bypass_spec_v1();\n\tu64 array_size, mask64;\n\tstruct bpf_array *array;\n\n\telem_size = round_up(attr->value_size, 8);\n\n\tmax_entries = attr->max_entries;\n\n\t \n\tmask64 = fls_long(max_entries - 1);\n\tmask64 = 1ULL << mask64;\n\tmask64 -= 1;\n\n\tindex_mask = mask64;\n\tif (!bypass_spec_v1) {\n\t\t \n\t\tmax_entries = index_mask + 1;\n\t\t \n\t\tif (max_entries < attr->max_entries)\n\t\t\treturn ERR_PTR(-E2BIG);\n\t}\n\n\tarray_size = sizeof(*array);\n\tif (percpu) {\n\t\tarray_size += (u64) max_entries * sizeof(void *);\n\t} else {\n\t\t \n\t\tif (attr->map_flags & BPF_F_MMAPABLE) {\n\t\t\tarray_size = PAGE_ALIGN(array_size);\n\t\t\tarray_size += PAGE_ALIGN((u64) max_entries * elem_size);\n\t\t} else {\n\t\t\tarray_size += (u64) max_entries * elem_size;\n\t\t}\n\t}\n\n\t \n\tif (attr->map_flags & BPF_F_MMAPABLE) {\n\t\tvoid *data;\n\n\t\t \n\t\tdata = bpf_map_area_mmapable_alloc(array_size, numa_node);\n\t\tif (!data)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tarray = data + PAGE_ALIGN(sizeof(struct bpf_array))\n\t\t\t- offsetof(struct bpf_array, value);\n\t} else {\n\t\tarray = bpf_map_area_alloc(array_size, numa_node);\n\t}\n\tif (!array)\n\t\treturn ERR_PTR(-ENOMEM);\n\tarray->index_mask = index_mask;\n\tarray->map.bypass_spec_v1 = bypass_spec_v1;\n\n\t \n\tbpf_map_init_from_attr(&array->map, attr);\n\tarray->elem_size = elem_size;\n\n\tif (percpu && bpf_array_alloc_percpu(array)) {\n\t\tbpf_map_area_free(array);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn &array->map;\n}\n\nstatic void *array_map_elem_ptr(struct bpf_array* array, u32 index)\n{\n\treturn array->value + (u64)array->elem_size * index;\n}\n\n \nstatic void *array_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\n\tif (unlikely(index >= array->map.max_entries))\n\t\treturn NULL;\n\n\treturn array->value + (u64)array->elem_size * (index & array->index_mask);\n}\n\nstatic int array_map_direct_value_addr(const struct bpf_map *map, u64 *imm,\n\t\t\t\t       u32 off)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\n\tif (map->max_entries != 1)\n\t\treturn -ENOTSUPP;\n\tif (off >= map->value_size)\n\t\treturn -EINVAL;\n\n\t*imm = (unsigned long)array->value;\n\treturn 0;\n}\n\nstatic int array_map_direct_value_meta(const struct bpf_map *map, u64 imm,\n\t\t\t\t       u32 *off)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu64 base = (unsigned long)array->value;\n\tu64 range = array->elem_size;\n\n\tif (map->max_entries != 1)\n\t\treturn -ENOTSUPP;\n\tif (imm < base || imm >= base + range)\n\t\treturn -ENOENT;\n\n\t*off = imm - base;\n\treturn 0;\n}\n\n \nstatic int array_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct bpf_insn *insn = insn_buf;\n\tu32 elem_size = array->elem_size;\n\tconst int ret = BPF_REG_0;\n\tconst int map_ptr = BPF_REG_1;\n\tconst int index = BPF_REG_2;\n\n\tif (map->map_flags & BPF_F_INNER_MAP)\n\t\treturn -EOPNOTSUPP;\n\n\t*insn++ = BPF_ALU64_IMM(BPF_ADD, map_ptr, offsetof(struct bpf_array, value));\n\t*insn++ = BPF_LDX_MEM(BPF_W, ret, index, 0);\n\tif (!map->bypass_spec_v1) {\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map->max_entries, 4);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, ret, array->index_mask);\n\t} else {\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map->max_entries, 3);\n\t}\n\n\tif (is_power_of_2(elem_size)) {\n\t\t*insn++ = BPF_ALU64_IMM(BPF_LSH, ret, ilog2(elem_size));\n\t} else {\n\t\t*insn++ = BPF_ALU64_IMM(BPF_MUL, ret, elem_size);\n\t}\n\t*insn++ = BPF_ALU64_REG(BPF_ADD, ret, map_ptr);\n\t*insn++ = BPF_JMP_IMM(BPF_JA, 0, 0, 1);\n\t*insn++ = BPF_MOV64_IMM(ret, 0);\n\treturn insn - insn_buf;\n}\n\n \nstatic void *percpu_array_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\n\tif (unlikely(index >= array->map.max_entries))\n\t\treturn NULL;\n\n\treturn this_cpu_ptr(array->pptrs[index & array->index_mask]);\n}\n\nstatic void *percpu_array_map_lookup_percpu_elem(struct bpf_map *map, void *key, u32 cpu)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\n\tif (cpu >= nr_cpu_ids)\n\t\treturn NULL;\n\n\tif (unlikely(index >= array->map.max_entries))\n\t\treturn NULL;\n\n\treturn per_cpu_ptr(array->pptrs[index & array->index_mask], cpu);\n}\n\nint bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\tvoid __percpu *pptr;\n\tint cpu, off = 0;\n\tu32 size;\n\n\tif (unlikely(index >= array->map.max_entries))\n\t\treturn -ENOENT;\n\n\t \n\tsize = array->elem_size;\n\trcu_read_lock();\n\tpptr = array->pptrs[index & array->index_mask];\n\tfor_each_possible_cpu(cpu) {\n\t\tcopy_map_value_long(map, value + off, per_cpu_ptr(pptr, cpu));\n\t\tcheck_and_init_map_value(map, value + off);\n\t\toff += size;\n\t}\n\trcu_read_unlock();\n\treturn 0;\n}\n\n \nstatic int array_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = key ? *(u32 *)key : U32_MAX;\n\tu32 *next = (u32 *)next_key;\n\n\tif (index >= array->map.max_entries) {\n\t\t*next = 0;\n\t\treturn 0;\n\t}\n\n\tif (index == array->map.max_entries - 1)\n\t\treturn -ENOENT;\n\n\t*next = index + 1;\n\treturn 0;\n}\n\n \nstatic long array_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t  u64 map_flags)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\tchar *val;\n\n\tif (unlikely((map_flags & ~BPF_F_LOCK) > BPF_EXIST))\n\t\t \n\t\treturn -EINVAL;\n\n\tif (unlikely(index >= array->map.max_entries))\n\t\t \n\t\treturn -E2BIG;\n\n\tif (unlikely(map_flags & BPF_NOEXIST))\n\t\t \n\t\treturn -EEXIST;\n\n\tif (unlikely((map_flags & BPF_F_LOCK) &&\n\t\t     !btf_record_has_field(map->record, BPF_SPIN_LOCK)))\n\t\treturn -EINVAL;\n\n\tif (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {\n\t\tval = this_cpu_ptr(array->pptrs[index & array->index_mask]);\n\t\tcopy_map_value(map, val, value);\n\t\tbpf_obj_free_fields(array->map.record, val);\n\t} else {\n\t\tval = array->value +\n\t\t\t(u64)array->elem_size * (index & array->index_mask);\n\t\tif (map_flags & BPF_F_LOCK)\n\t\t\tcopy_map_value_locked(map, val, value, false);\n\t\telse\n\t\t\tcopy_map_value(map, val, value);\n\t\tbpf_obj_free_fields(array->map.record, val);\n\t}\n\treturn 0;\n}\n\nint bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,\n\t\t\t    u64 map_flags)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\tvoid __percpu *pptr;\n\tint cpu, off = 0;\n\tu32 size;\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\t \n\t\treturn -EINVAL;\n\n\tif (unlikely(index >= array->map.max_entries))\n\t\t \n\t\treturn -E2BIG;\n\n\tif (unlikely(map_flags == BPF_NOEXIST))\n\t\t \n\t\treturn -EEXIST;\n\n\t \n\tsize = array->elem_size;\n\trcu_read_lock();\n\tpptr = array->pptrs[index & array->index_mask];\n\tfor_each_possible_cpu(cpu) {\n\t\tcopy_map_value_long(map, per_cpu_ptr(pptr, cpu), value + off);\n\t\tbpf_obj_free_fields(array->map.record, per_cpu_ptr(pptr, cpu));\n\t\toff += size;\n\t}\n\trcu_read_unlock();\n\treturn 0;\n}\n\n \nstatic long array_map_delete_elem(struct bpf_map *map, void *key)\n{\n\treturn -EINVAL;\n}\n\nstatic void *array_map_vmalloc_addr(struct bpf_array *array)\n{\n\treturn (void *)round_down((unsigned long)array, PAGE_SIZE);\n}\n\nstatic void array_map_free_timers(struct bpf_map *map)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tint i;\n\n\t \n\tif (!btf_record_has_field(map->record, BPF_TIMER))\n\t\treturn;\n\n\tfor (i = 0; i < array->map.max_entries; i++)\n\t\tbpf_obj_free_timer(map->record, array_map_elem_ptr(array, i));\n}\n\n \nstatic void array_map_free(struct bpf_map *map)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tint i;\n\n\tif (!IS_ERR_OR_NULL(map->record)) {\n\t\tif (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {\n\t\t\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\t\t\tvoid __percpu *pptr = array->pptrs[i & array->index_mask];\n\t\t\t\tint cpu;\n\n\t\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\t\tbpf_obj_free_fields(map->record, per_cpu_ptr(pptr, cpu));\n\t\t\t\t\tcond_resched();\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tfor (i = 0; i < array->map.max_entries; i++)\n\t\t\t\tbpf_obj_free_fields(map->record, array_map_elem_ptr(array, i));\n\t\t}\n\t}\n\n\tif (array->map.map_type == BPF_MAP_TYPE_PERCPU_ARRAY)\n\t\tbpf_array_free_percpu(array);\n\n\tif (array->map.map_flags & BPF_F_MMAPABLE)\n\t\tbpf_map_area_free(array_map_vmalloc_addr(array));\n\telse\n\t\tbpf_map_area_free(array);\n}\n\nstatic void array_map_seq_show_elem(struct bpf_map *map, void *key,\n\t\t\t\t    struct seq_file *m)\n{\n\tvoid *value;\n\n\trcu_read_lock();\n\n\tvalue = array_map_lookup_elem(map, key);\n\tif (!value) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (map->btf_key_type_id)\n\t\tseq_printf(m, \"%u: \", *(u32 *)key);\n\tbtf_type_seq_show(map->btf, map->btf_value_type_id, value, m);\n\tseq_puts(m, \"\\n\");\n\n\trcu_read_unlock();\n}\n\nstatic void percpu_array_map_seq_show_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t   struct seq_file *m)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 index = *(u32 *)key;\n\tvoid __percpu *pptr;\n\tint cpu;\n\n\trcu_read_lock();\n\n\tseq_printf(m, \"%u: {\\n\", *(u32 *)key);\n\tpptr = array->pptrs[index & array->index_mask];\n\tfor_each_possible_cpu(cpu) {\n\t\tseq_printf(m, \"\\tcpu%d: \", cpu);\n\t\tbtf_type_seq_show(map->btf, map->btf_value_type_id,\n\t\t\t\t  per_cpu_ptr(pptr, cpu), m);\n\t\tseq_puts(m, \"\\n\");\n\t}\n\tseq_puts(m, \"}\\n\");\n\n\trcu_read_unlock();\n}\n\nstatic int array_map_check_btf(const struct bpf_map *map,\n\t\t\t       const struct btf *btf,\n\t\t\t       const struct btf_type *key_type,\n\t\t\t       const struct btf_type *value_type)\n{\n\tu32 int_data;\n\n\t \n\tif (btf_type_is_void(key_type)) {\n\t\tif (map->map_type != BPF_MAP_TYPE_ARRAY ||\n\t\t    map->max_entries != 1)\n\t\t\treturn -EINVAL;\n\n\t\tif (BTF_INFO_KIND(value_type->info) != BTF_KIND_DATASEC)\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\t}\n\n\tif (BTF_INFO_KIND(key_type->info) != BTF_KIND_INT)\n\t\treturn -EINVAL;\n\n\tint_data = *(u32 *)(key_type + 1);\n\t \n\tif (BTF_INT_BITS(int_data) != 32 || BTF_INT_OFFSET(int_data))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int array_map_mmap(struct bpf_map *map, struct vm_area_struct *vma)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tpgoff_t pgoff = PAGE_ALIGN(sizeof(*array)) >> PAGE_SHIFT;\n\n\tif (!(map->map_flags & BPF_F_MMAPABLE))\n\t\treturn -EINVAL;\n\n\tif (vma->vm_pgoff * PAGE_SIZE + (vma->vm_end - vma->vm_start) >\n\t    PAGE_ALIGN((u64)array->map.max_entries * array->elem_size))\n\t\treturn -EINVAL;\n\n\treturn remap_vmalloc_range(vma, array_map_vmalloc_addr(array),\n\t\t\t\t   vma->vm_pgoff + pgoff);\n}\n\nstatic bool array_map_meta_equal(const struct bpf_map *meta0,\n\t\t\t\t const struct bpf_map *meta1)\n{\n\tif (!bpf_map_meta_equal(meta0, meta1))\n\t\treturn false;\n\treturn meta0->map_flags & BPF_F_INNER_MAP ? true :\n\t       meta0->max_entries == meta1->max_entries;\n}\n\nstruct bpf_iter_seq_array_map_info {\n\tstruct bpf_map *map;\n\tvoid *percpu_value_buf;\n\tu32 index;\n};\n\nstatic void *bpf_array_map_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct bpf_iter_seq_array_map_info *info = seq->private;\n\tstruct bpf_map *map = info->map;\n\tstruct bpf_array *array;\n\tu32 index;\n\n\tif (info->index >= map->max_entries)\n\t\treturn NULL;\n\n\tif (*pos == 0)\n\t\t++*pos;\n\tarray = container_of(map, struct bpf_array, map);\n\tindex = info->index & array->index_mask;\n\tif (info->percpu_value_buf)\n\t       return array->pptrs[index];\n\treturn array_map_elem_ptr(array, index);\n}\n\nstatic void *bpf_array_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct bpf_iter_seq_array_map_info *info = seq->private;\n\tstruct bpf_map *map = info->map;\n\tstruct bpf_array *array;\n\tu32 index;\n\n\t++*pos;\n\t++info->index;\n\tif (info->index >= map->max_entries)\n\t\treturn NULL;\n\n\tarray = container_of(map, struct bpf_array, map);\n\tindex = info->index & array->index_mask;\n\tif (info->percpu_value_buf)\n\t       return array->pptrs[index];\n\treturn array_map_elem_ptr(array, index);\n}\n\nstatic int __bpf_array_map_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct bpf_iter_seq_array_map_info *info = seq->private;\n\tstruct bpf_iter__bpf_map_elem ctx = {};\n\tstruct bpf_map *map = info->map;\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct bpf_iter_meta meta;\n\tstruct bpf_prog *prog;\n\tint off = 0, cpu = 0;\n\tvoid __percpu **pptr;\n\tu32 size;\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, v == NULL);\n\tif (!prog)\n\t\treturn 0;\n\n\tctx.meta = &meta;\n\tctx.map = info->map;\n\tif (v) {\n\t\tctx.key = &info->index;\n\n\t\tif (!info->percpu_value_buf) {\n\t\t\tctx.value = v;\n\t\t} else {\n\t\t\tpptr = v;\n\t\t\tsize = array->elem_size;\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tcopy_map_value_long(map, info->percpu_value_buf + off,\n\t\t\t\t\t\t    per_cpu_ptr(pptr, cpu));\n\t\t\t\tcheck_and_init_map_value(map, info->percpu_value_buf + off);\n\t\t\t\toff += size;\n\t\t\t}\n\t\t\tctx.value = info->percpu_value_buf;\n\t\t}\n\t}\n\n\treturn bpf_iter_run_prog(prog, &ctx);\n}\n\nstatic int bpf_array_map_seq_show(struct seq_file *seq, void *v)\n{\n\treturn __bpf_array_map_seq_show(seq, v);\n}\n\nstatic void bpf_array_map_seq_stop(struct seq_file *seq, void *v)\n{\n\tif (!v)\n\t\t(void)__bpf_array_map_seq_show(seq, NULL);\n}\n\nstatic int bpf_iter_init_array_map(void *priv_data,\n\t\t\t\t   struct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_iter_seq_array_map_info *seq_info = priv_data;\n\tstruct bpf_map *map = aux->map;\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tvoid *value_buf;\n\tu32 buf_size;\n\n\tif (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {\n\t\tbuf_size = array->elem_size * num_possible_cpus();\n\t\tvalue_buf = kmalloc(buf_size, GFP_USER | __GFP_NOWARN);\n\t\tif (!value_buf)\n\t\t\treturn -ENOMEM;\n\n\t\tseq_info->percpu_value_buf = value_buf;\n\t}\n\n\t \n\tbpf_map_inc_with_uref(map);\n\tseq_info->map = map;\n\treturn 0;\n}\n\nstatic void bpf_iter_fini_array_map(void *priv_data)\n{\n\tstruct bpf_iter_seq_array_map_info *seq_info = priv_data;\n\n\tbpf_map_put_with_uref(seq_info->map);\n\tkfree(seq_info->percpu_value_buf);\n}\n\nstatic const struct seq_operations bpf_array_map_seq_ops = {\n\t.start\t= bpf_array_map_seq_start,\n\t.next\t= bpf_array_map_seq_next,\n\t.stop\t= bpf_array_map_seq_stop,\n\t.show\t= bpf_array_map_seq_show,\n};\n\nstatic const struct bpf_iter_seq_info iter_seq_info = {\n\t.seq_ops\t\t= &bpf_array_map_seq_ops,\n\t.init_seq_private\t= bpf_iter_init_array_map,\n\t.fini_seq_private\t= bpf_iter_fini_array_map,\n\t.seq_priv_size\t\t= sizeof(struct bpf_iter_seq_array_map_info),\n};\n\nstatic long bpf_for_each_array_elem(struct bpf_map *map, bpf_callback_t callback_fn,\n\t\t\t\t    void *callback_ctx, u64 flags)\n{\n\tu32 i, key, num_elems = 0;\n\tstruct bpf_array *array;\n\tbool is_percpu;\n\tu64 ret = 0;\n\tvoid *val;\n\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\tis_percpu = map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY;\n\tarray = container_of(map, struct bpf_array, map);\n\tif (is_percpu)\n\t\tmigrate_disable();\n\tfor (i = 0; i < map->max_entries; i++) {\n\t\tif (is_percpu)\n\t\t\tval = this_cpu_ptr(array->pptrs[i]);\n\t\telse\n\t\t\tval = array_map_elem_ptr(array, i);\n\t\tnum_elems++;\n\t\tkey = i;\n\t\tret = callback_fn((u64)(long)map, (u64)(long)&key,\n\t\t\t\t  (u64)(long)val, (u64)(long)callback_ctx, 0);\n\t\t \n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (is_percpu)\n\t\tmigrate_enable();\n\treturn num_elems;\n}\n\nstatic u64 array_map_mem_usage(const struct bpf_map *map)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tbool percpu = map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY;\n\tu32 elem_size = array->elem_size;\n\tu64 entries = map->max_entries;\n\tu64 usage = sizeof(*array);\n\n\tif (percpu) {\n\t\tusage += entries * sizeof(void *);\n\t\tusage += entries * elem_size * num_possible_cpus();\n\t} else {\n\t\tif (map->map_flags & BPF_F_MMAPABLE) {\n\t\t\tusage = PAGE_ALIGN(usage);\n\t\t\tusage += PAGE_ALIGN(entries * elem_size);\n\t\t} else {\n\t\t\tusage += entries * elem_size;\n\t\t}\n\t}\n\treturn usage;\n}\n\nBTF_ID_LIST_SINGLE(array_map_btf_ids, struct, bpf_array)\nconst struct bpf_map_ops array_map_ops = {\n\t.map_meta_equal = array_map_meta_equal,\n\t.map_alloc_check = array_map_alloc_check,\n\t.map_alloc = array_map_alloc,\n\t.map_free = array_map_free,\n\t.map_get_next_key = array_map_get_next_key,\n\t.map_release_uref = array_map_free_timers,\n\t.map_lookup_elem = array_map_lookup_elem,\n\t.map_update_elem = array_map_update_elem,\n\t.map_delete_elem = array_map_delete_elem,\n\t.map_gen_lookup = array_map_gen_lookup,\n\t.map_direct_value_addr = array_map_direct_value_addr,\n\t.map_direct_value_meta = array_map_direct_value_meta,\n\t.map_mmap = array_map_mmap,\n\t.map_seq_show_elem = array_map_seq_show_elem,\n\t.map_check_btf = array_map_check_btf,\n\t.map_lookup_batch = generic_map_lookup_batch,\n\t.map_update_batch = generic_map_update_batch,\n\t.map_set_for_each_callback_args = map_set_for_each_callback_args,\n\t.map_for_each_callback = bpf_for_each_array_elem,\n\t.map_mem_usage = array_map_mem_usage,\n\t.map_btf_id = &array_map_btf_ids[0],\n\t.iter_seq_info = &iter_seq_info,\n};\n\nconst struct bpf_map_ops percpu_array_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = array_map_alloc_check,\n\t.map_alloc = array_map_alloc,\n\t.map_free = array_map_free,\n\t.map_get_next_key = array_map_get_next_key,\n\t.map_lookup_elem = percpu_array_map_lookup_elem,\n\t.map_update_elem = array_map_update_elem,\n\t.map_delete_elem = array_map_delete_elem,\n\t.map_lookup_percpu_elem = percpu_array_map_lookup_percpu_elem,\n\t.map_seq_show_elem = percpu_array_map_seq_show_elem,\n\t.map_check_btf = array_map_check_btf,\n\t.map_lookup_batch = generic_map_lookup_batch,\n\t.map_update_batch = generic_map_update_batch,\n\t.map_set_for_each_callback_args = map_set_for_each_callback_args,\n\t.map_for_each_callback = bpf_for_each_array_elem,\n\t.map_mem_usage = array_map_mem_usage,\n\t.map_btf_id = &array_map_btf_ids[0],\n\t.iter_seq_info = &iter_seq_info,\n};\n\nstatic int fd_array_map_alloc_check(union bpf_attr *attr)\n{\n\t \n\tif (attr->value_size != sizeof(u32))\n\t\treturn -EINVAL;\n\t \n\tif (attr->map_flags & (BPF_F_RDONLY_PROG | BPF_F_WRONLY_PROG))\n\t\treturn -EINVAL;\n\treturn array_map_alloc_check(attr);\n}\n\nstatic void fd_array_map_free(struct bpf_map *map)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tint i;\n\n\t \n\tfor (i = 0; i < array->map.max_entries; i++)\n\t\tBUG_ON(array->ptrs[i] != NULL);\n\n\tbpf_map_area_free(array);\n}\n\nstatic void *fd_array_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\n \nint bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value)\n{\n\tvoid **elem, *ptr;\n\tint ret =  0;\n\n\tif (!map->ops->map_fd_sys_lookup_elem)\n\t\treturn -ENOTSUPP;\n\n\trcu_read_lock();\n\telem = array_map_lookup_elem(map, key);\n\tif (elem && (ptr = READ_ONCE(*elem)))\n\t\t*value = map->ops->map_fd_sys_lookup_elem(ptr);\n\telse\n\t\tret = -ENOENT;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nint bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,\n\t\t\t\t void *key, void *value, u64 map_flags)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tvoid *new_ptr, *old_ptr;\n\tu32 index = *(u32 *)key, ufd;\n\n\tif (map_flags != BPF_ANY)\n\t\treturn -EINVAL;\n\n\tif (index >= array->map.max_entries)\n\t\treturn -E2BIG;\n\n\tufd = *(u32 *)value;\n\tnew_ptr = map->ops->map_fd_get_ptr(map, map_file, ufd);\n\tif (IS_ERR(new_ptr))\n\t\treturn PTR_ERR(new_ptr);\n\n\tif (map->ops->map_poke_run) {\n\t\tmutex_lock(&array->aux->poke_mutex);\n\t\told_ptr = xchg(array->ptrs + index, new_ptr);\n\t\tmap->ops->map_poke_run(map, index, old_ptr, new_ptr);\n\t\tmutex_unlock(&array->aux->poke_mutex);\n\t} else {\n\t\told_ptr = xchg(array->ptrs + index, new_ptr);\n\t}\n\n\tif (old_ptr)\n\t\tmap->ops->map_fd_put_ptr(map, old_ptr, true);\n\treturn 0;\n}\n\nstatic long fd_array_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tvoid *old_ptr;\n\tu32 index = *(u32 *)key;\n\n\tif (index >= array->map.max_entries)\n\t\treturn -E2BIG;\n\n\tif (map->ops->map_poke_run) {\n\t\tmutex_lock(&array->aux->poke_mutex);\n\t\told_ptr = xchg(array->ptrs + index, NULL);\n\t\tmap->ops->map_poke_run(map, index, old_ptr, NULL);\n\t\tmutex_unlock(&array->aux->poke_mutex);\n\t} else {\n\t\told_ptr = xchg(array->ptrs + index, NULL);\n\t}\n\n\tif (old_ptr) {\n\t\tmap->ops->map_fd_put_ptr(map, old_ptr, true);\n\t\treturn 0;\n\t} else {\n\t\treturn -ENOENT;\n\t}\n}\n\nstatic void *prog_fd_array_get_ptr(struct bpf_map *map,\n\t\t\t\t   struct file *map_file, int fd)\n{\n\tstruct bpf_prog *prog = bpf_prog_get(fd);\n\n\tif (IS_ERR(prog))\n\t\treturn prog;\n\n\tif (!bpf_prog_map_compatible(map, prog)) {\n\t\tbpf_prog_put(prog);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn prog;\n}\n\nstatic void prog_fd_array_put_ptr(struct bpf_map *map, void *ptr, bool need_defer)\n{\n\t \n\tbpf_prog_put(ptr);\n}\n\nstatic u32 prog_fd_array_sys_lookup_elem(void *ptr)\n{\n\treturn ((struct bpf_prog *)ptr)->aux->id;\n}\n\n \nstatic void bpf_fd_array_map_clear(struct bpf_map *map)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tint i;\n\n\tfor (i = 0; i < array->map.max_entries; i++)\n\t\tfd_array_map_delete_elem(map, &i);\n}\n\nstatic void prog_array_map_seq_show_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t struct seq_file *m)\n{\n\tvoid **elem, *ptr;\n\tu32 prog_id;\n\n\trcu_read_lock();\n\n\telem = array_map_lookup_elem(map, key);\n\tif (elem) {\n\t\tptr = READ_ONCE(*elem);\n\t\tif (ptr) {\n\t\t\tseq_printf(m, \"%u: \", *(u32 *)key);\n\t\t\tprog_id = prog_fd_array_sys_lookup_elem(ptr);\n\t\t\tbtf_type_seq_show(map->btf, map->btf_value_type_id,\n\t\t\t\t\t  &prog_id, m);\n\t\t\tseq_puts(m, \"\\n\");\n\t\t}\n\t}\n\n\trcu_read_unlock();\n}\n\nstruct prog_poke_elem {\n\tstruct list_head list;\n\tstruct bpf_prog_aux *aux;\n};\n\nstatic int prog_array_map_poke_track(struct bpf_map *map,\n\t\t\t\t     struct bpf_prog_aux *prog_aux)\n{\n\tstruct prog_poke_elem *elem;\n\tstruct bpf_array_aux *aux;\n\tint ret = 0;\n\n\taux = container_of(map, struct bpf_array, map)->aux;\n\tmutex_lock(&aux->poke_mutex);\n\tlist_for_each_entry(elem, &aux->poke_progs, list) {\n\t\tif (elem->aux == prog_aux)\n\t\t\tgoto out;\n\t}\n\n\telem = kmalloc(sizeof(*elem), GFP_KERNEL);\n\tif (!elem) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tINIT_LIST_HEAD(&elem->list);\n\t \n\telem->aux = prog_aux;\n\n\tlist_add_tail(&elem->list, &aux->poke_progs);\nout:\n\tmutex_unlock(&aux->poke_mutex);\n\treturn ret;\n}\n\nstatic void prog_array_map_poke_untrack(struct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog_aux *prog_aux)\n{\n\tstruct prog_poke_elem *elem, *tmp;\n\tstruct bpf_array_aux *aux;\n\n\taux = container_of(map, struct bpf_array, map)->aux;\n\tmutex_lock(&aux->poke_mutex);\n\tlist_for_each_entry_safe(elem, tmp, &aux->poke_progs, list) {\n\t\tif (elem->aux == prog_aux) {\n\t\t\tlist_del_init(&elem->list);\n\t\t\tkfree(elem);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&aux->poke_mutex);\n}\n\nvoid __weak bpf_arch_poke_desc_update(struct bpf_jit_poke_descriptor *poke,\n\t\t\t\t      struct bpf_prog *new, struct bpf_prog *old)\n{\n\tWARN_ON_ONCE(1);\n}\n\nstatic void prog_array_map_poke_run(struct bpf_map *map, u32 key,\n\t\t\t\t    struct bpf_prog *old,\n\t\t\t\t    struct bpf_prog *new)\n{\n\tstruct prog_poke_elem *elem;\n\tstruct bpf_array_aux *aux;\n\n\taux = container_of(map, struct bpf_array, map)->aux;\n\tWARN_ON_ONCE(!mutex_is_locked(&aux->poke_mutex));\n\n\tlist_for_each_entry(elem, &aux->poke_progs, list) {\n\t\tstruct bpf_jit_poke_descriptor *poke;\n\t\tint i;\n\n\t\tfor (i = 0; i < elem->aux->size_poke_tab; i++) {\n\t\t\tpoke = &elem->aux->poke_tab[i];\n\n\t\t\t \n\t\t\tif (!READ_ONCE(poke->tailcall_target_stable))\n\t\t\t\tcontinue;\n\t\t\tif (poke->reason != BPF_POKE_REASON_TAIL_CALL)\n\t\t\t\tcontinue;\n\t\t\tif (poke->tail_call.map != map ||\n\t\t\t    poke->tail_call.key != key)\n\t\t\t\tcontinue;\n\n\t\t\tbpf_arch_poke_desc_update(poke, new, old);\n\t\t}\n\t}\n}\n\nstatic void prog_array_map_clear_deferred(struct work_struct *work)\n{\n\tstruct bpf_map *map = container_of(work, struct bpf_array_aux,\n\t\t\t\t\t   work)->map;\n\tbpf_fd_array_map_clear(map);\n\tbpf_map_put(map);\n}\n\nstatic void prog_array_map_clear(struct bpf_map *map)\n{\n\tstruct bpf_array_aux *aux = container_of(map, struct bpf_array,\n\t\t\t\t\t\t map)->aux;\n\tbpf_map_inc(map);\n\tschedule_work(&aux->work);\n}\n\nstatic struct bpf_map *prog_array_map_alloc(union bpf_attr *attr)\n{\n\tstruct bpf_array_aux *aux;\n\tstruct bpf_map *map;\n\n\taux = kzalloc(sizeof(*aux), GFP_KERNEL_ACCOUNT);\n\tif (!aux)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_WORK(&aux->work, prog_array_map_clear_deferred);\n\tINIT_LIST_HEAD(&aux->poke_progs);\n\tmutex_init(&aux->poke_mutex);\n\n\tmap = array_map_alloc(attr);\n\tif (IS_ERR(map)) {\n\t\tkfree(aux);\n\t\treturn map;\n\t}\n\n\tcontainer_of(map, struct bpf_array, map)->aux = aux;\n\taux->map = map;\n\n\treturn map;\n}\n\nstatic void prog_array_map_free(struct bpf_map *map)\n{\n\tstruct prog_poke_elem *elem, *tmp;\n\tstruct bpf_array_aux *aux;\n\n\taux = container_of(map, struct bpf_array, map)->aux;\n\tlist_for_each_entry_safe(elem, tmp, &aux->poke_progs, list) {\n\t\tlist_del_init(&elem->list);\n\t\tkfree(elem);\n\t}\n\tkfree(aux);\n\tfd_array_map_free(map);\n}\n\n \nconst struct bpf_map_ops prog_array_map_ops = {\n\t.map_alloc_check = fd_array_map_alloc_check,\n\t.map_alloc = prog_array_map_alloc,\n\t.map_free = prog_array_map_free,\n\t.map_poke_track = prog_array_map_poke_track,\n\t.map_poke_untrack = prog_array_map_poke_untrack,\n\t.map_poke_run = prog_array_map_poke_run,\n\t.map_get_next_key = array_map_get_next_key,\n\t.map_lookup_elem = fd_array_map_lookup_elem,\n\t.map_delete_elem = fd_array_map_delete_elem,\n\t.map_fd_get_ptr = prog_fd_array_get_ptr,\n\t.map_fd_put_ptr = prog_fd_array_put_ptr,\n\t.map_fd_sys_lookup_elem = prog_fd_array_sys_lookup_elem,\n\t.map_release_uref = prog_array_map_clear,\n\t.map_seq_show_elem = prog_array_map_seq_show_elem,\n\t.map_mem_usage = array_map_mem_usage,\n\t.map_btf_id = &array_map_btf_ids[0],\n};\n\nstatic struct bpf_event_entry *bpf_event_entry_gen(struct file *perf_file,\n\t\t\t\t\t\t   struct file *map_file)\n{\n\tstruct bpf_event_entry *ee;\n\n\tee = kzalloc(sizeof(*ee), GFP_ATOMIC);\n\tif (ee) {\n\t\tee->event = perf_file->private_data;\n\t\tee->perf_file = perf_file;\n\t\tee->map_file = map_file;\n\t}\n\n\treturn ee;\n}\n\nstatic void __bpf_event_entry_free(struct rcu_head *rcu)\n{\n\tstruct bpf_event_entry *ee;\n\n\tee = container_of(rcu, struct bpf_event_entry, rcu);\n\tfput(ee->perf_file);\n\tkfree(ee);\n}\n\nstatic void bpf_event_entry_free_rcu(struct bpf_event_entry *ee)\n{\n\tcall_rcu(&ee->rcu, __bpf_event_entry_free);\n}\n\nstatic void *perf_event_fd_array_get_ptr(struct bpf_map *map,\n\t\t\t\t\t struct file *map_file, int fd)\n{\n\tstruct bpf_event_entry *ee;\n\tstruct perf_event *event;\n\tstruct file *perf_file;\n\tu64 value;\n\n\tperf_file = perf_event_get(fd);\n\tif (IS_ERR(perf_file))\n\t\treturn perf_file;\n\n\tee = ERR_PTR(-EOPNOTSUPP);\n\tevent = perf_file->private_data;\n\tif (perf_event_read_local(event, &value, NULL, NULL) == -EOPNOTSUPP)\n\t\tgoto err_out;\n\n\tee = bpf_event_entry_gen(perf_file, map_file);\n\tif (ee)\n\t\treturn ee;\n\tee = ERR_PTR(-ENOMEM);\nerr_out:\n\tfput(perf_file);\n\treturn ee;\n}\n\nstatic void perf_event_fd_array_put_ptr(struct bpf_map *map, void *ptr, bool need_defer)\n{\n\t \n\tbpf_event_entry_free_rcu(ptr);\n}\n\nstatic void perf_event_fd_array_release(struct bpf_map *map,\n\t\t\t\t\tstruct file *map_file)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct bpf_event_entry *ee;\n\tint i;\n\n\tif (map->map_flags & BPF_F_PRESERVE_ELEMS)\n\t\treturn;\n\n\trcu_read_lock();\n\tfor (i = 0; i < array->map.max_entries; i++) {\n\t\tee = READ_ONCE(array->ptrs[i]);\n\t\tif (ee && ee->map_file == map_file)\n\t\t\tfd_array_map_delete_elem(map, &i);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void perf_event_fd_array_map_free(struct bpf_map *map)\n{\n\tif (map->map_flags & BPF_F_PRESERVE_ELEMS)\n\t\tbpf_fd_array_map_clear(map);\n\tfd_array_map_free(map);\n}\n\nconst struct bpf_map_ops perf_event_array_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = fd_array_map_alloc_check,\n\t.map_alloc = array_map_alloc,\n\t.map_free = perf_event_fd_array_map_free,\n\t.map_get_next_key = array_map_get_next_key,\n\t.map_lookup_elem = fd_array_map_lookup_elem,\n\t.map_delete_elem = fd_array_map_delete_elem,\n\t.map_fd_get_ptr = perf_event_fd_array_get_ptr,\n\t.map_fd_put_ptr = perf_event_fd_array_put_ptr,\n\t.map_release = perf_event_fd_array_release,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = array_map_mem_usage,\n\t.map_btf_id = &array_map_btf_ids[0],\n};\n\n#ifdef CONFIG_CGROUPS\nstatic void *cgroup_fd_array_get_ptr(struct bpf_map *map,\n\t\t\t\t     struct file *map_file  ,\n\t\t\t\t     int fd)\n{\n\treturn cgroup_get_from_fd(fd);\n}\n\nstatic void cgroup_fd_array_put_ptr(struct bpf_map *map, void *ptr, bool need_defer)\n{\n\t \n\tcgroup_put(ptr);\n}\n\nstatic void cgroup_fd_array_free(struct bpf_map *map)\n{\n\tbpf_fd_array_map_clear(map);\n\tfd_array_map_free(map);\n}\n\nconst struct bpf_map_ops cgroup_array_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = fd_array_map_alloc_check,\n\t.map_alloc = array_map_alloc,\n\t.map_free = cgroup_fd_array_free,\n\t.map_get_next_key = array_map_get_next_key,\n\t.map_lookup_elem = fd_array_map_lookup_elem,\n\t.map_delete_elem = fd_array_map_delete_elem,\n\t.map_fd_get_ptr = cgroup_fd_array_get_ptr,\n\t.map_fd_put_ptr = cgroup_fd_array_put_ptr,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = array_map_mem_usage,\n\t.map_btf_id = &array_map_btf_ids[0],\n};\n#endif\n\nstatic struct bpf_map *array_of_map_alloc(union bpf_attr *attr)\n{\n\tstruct bpf_map *map, *inner_map_meta;\n\n\tinner_map_meta = bpf_map_meta_alloc(attr->inner_map_fd);\n\tif (IS_ERR(inner_map_meta))\n\t\treturn inner_map_meta;\n\n\tmap = array_map_alloc(attr);\n\tif (IS_ERR(map)) {\n\t\tbpf_map_meta_free(inner_map_meta);\n\t\treturn map;\n\t}\n\n\tmap->inner_map_meta = inner_map_meta;\n\n\treturn map;\n}\n\nstatic void array_of_map_free(struct bpf_map *map)\n{\n\t \n\tbpf_map_meta_free(map->inner_map_meta);\n\tbpf_fd_array_map_clear(map);\n\tfd_array_map_free(map);\n}\n\nstatic void *array_of_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_map **inner_map = array_map_lookup_elem(map, key);\n\n\tif (!inner_map)\n\t\treturn NULL;\n\n\treturn READ_ONCE(*inner_map);\n}\n\nstatic int array_of_map_gen_lookup(struct bpf_map *map,\n\t\t\t\t   struct bpf_insn *insn_buf)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tu32 elem_size = array->elem_size;\n\tstruct bpf_insn *insn = insn_buf;\n\tconst int ret = BPF_REG_0;\n\tconst int map_ptr = BPF_REG_1;\n\tconst int index = BPF_REG_2;\n\n\t*insn++ = BPF_ALU64_IMM(BPF_ADD, map_ptr, offsetof(struct bpf_array, value));\n\t*insn++ = BPF_LDX_MEM(BPF_W, ret, index, 0);\n\tif (!map->bypass_spec_v1) {\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map->max_entries, 6);\n\t\t*insn++ = BPF_ALU32_IMM(BPF_AND, ret, array->index_mask);\n\t} else {\n\t\t*insn++ = BPF_JMP_IMM(BPF_JGE, ret, map->max_entries, 5);\n\t}\n\tif (is_power_of_2(elem_size))\n\t\t*insn++ = BPF_ALU64_IMM(BPF_LSH, ret, ilog2(elem_size));\n\telse\n\t\t*insn++ = BPF_ALU64_IMM(BPF_MUL, ret, elem_size);\n\t*insn++ = BPF_ALU64_REG(BPF_ADD, ret, map_ptr);\n\t*insn++ = BPF_LDX_MEM(BPF_DW, ret, ret, 0);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 1);\n\t*insn++ = BPF_JMP_IMM(BPF_JA, 0, 0, 1);\n\t*insn++ = BPF_MOV64_IMM(ret, 0);\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_map_ops array_of_maps_map_ops = {\n\t.map_alloc_check = fd_array_map_alloc_check,\n\t.map_alloc = array_of_map_alloc,\n\t.map_free = array_of_map_free,\n\t.map_get_next_key = array_map_get_next_key,\n\t.map_lookup_elem = array_of_map_lookup_elem,\n\t.map_delete_elem = fd_array_map_delete_elem,\n\t.map_fd_get_ptr = bpf_map_fd_get_ptr,\n\t.map_fd_put_ptr = bpf_map_fd_put_ptr,\n\t.map_fd_sys_lookup_elem = bpf_map_fd_sys_lookup_elem,\n\t.map_gen_lookup = array_of_map_gen_lookup,\n\t.map_lookup_batch = generic_map_lookup_batch,\n\t.map_update_batch = generic_map_update_batch,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = array_map_mem_usage,\n\t.map_btf_id = &array_map_btf_ids[0],\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}