{
  "module_name": "devmap.c",
  "hash_id": "503add1384610603b85b8c98082287369bc1b1a33ab9c070c343063e41d1680c",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/devmap.c",
  "human_readable_source": "\n \n\n \n#include <linux/bpf.h>\n#include <net/xdp.h>\n#include <linux/filter.h>\n#include <trace/events/xdp.h>\n#include <linux/btf_ids.h>\n\n#define DEV_CREATE_FLAG_MASK \\\n\t(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY)\n\nstruct xdp_dev_bulk_queue {\n\tstruct xdp_frame *q[DEV_MAP_BULK_SIZE];\n\tstruct list_head flush_node;\n\tstruct net_device *dev;\n\tstruct net_device *dev_rx;\n\tstruct bpf_prog *xdp_prog;\n\tunsigned int count;\n};\n\nstruct bpf_dtab_netdev {\n\tstruct net_device *dev;  \n\tstruct hlist_node index_hlist;\n\tstruct bpf_prog *xdp_prog;\n\tstruct rcu_head rcu;\n\tunsigned int idx;\n\tstruct bpf_devmap_val val;\n};\n\nstruct bpf_dtab {\n\tstruct bpf_map map;\n\tstruct bpf_dtab_netdev __rcu **netdev_map;  \n\tstruct list_head list;\n\n\t \n\tstruct hlist_head *dev_index_head;\n\tspinlock_t index_lock;\n\tunsigned int items;\n\tu32 n_buckets;\n};\n\nstatic DEFINE_PER_CPU(struct list_head, dev_flush_list);\nstatic DEFINE_SPINLOCK(dev_map_lock);\nstatic LIST_HEAD(dev_map_list);\n\nstatic struct hlist_head *dev_map_create_hash(unsigned int entries,\n\t\t\t\t\t      int numa_node)\n{\n\tint i;\n\tstruct hlist_head *hash;\n\n\thash = bpf_map_area_alloc((u64) entries * sizeof(*hash), numa_node);\n\tif (hash != NULL)\n\t\tfor (i = 0; i < entries; i++)\n\t\t\tINIT_HLIST_HEAD(&hash[i]);\n\n\treturn hash;\n}\n\nstatic inline struct hlist_head *dev_map_index_hash(struct bpf_dtab *dtab,\n\t\t\t\t\t\t    int idx)\n{\n\treturn &dtab->dev_index_head[idx & (dtab->n_buckets - 1)];\n}\n\nstatic int dev_map_init_map(struct bpf_dtab *dtab, union bpf_attr *attr)\n{\n\tu32 valsize = attr->value_size;\n\n\t \n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    (valsize != offsetofend(struct bpf_devmap_val, ifindex) &&\n\t     valsize != offsetofend(struct bpf_devmap_val, bpf_prog.fd)) ||\n\t    attr->map_flags & ~DEV_CREATE_FLAG_MASK)\n\t\treturn -EINVAL;\n\n\t \n\tattr->map_flags |= BPF_F_RDONLY_PROG;\n\n\n\tbpf_map_init_from_attr(&dtab->map, attr);\n\n\tif (attr->map_type == BPF_MAP_TYPE_DEVMAP_HASH) {\n\t\tdtab->n_buckets = roundup_pow_of_two(dtab->map.max_entries);\n\n\t\tif (!dtab->n_buckets)  \n\t\t\treturn -EINVAL;\n\t}\n\n\tif (attr->map_type == BPF_MAP_TYPE_DEVMAP_HASH) {\n\t\tdtab->dev_index_head = dev_map_create_hash(dtab->n_buckets,\n\t\t\t\t\t\t\t   dtab->map.numa_node);\n\t\tif (!dtab->dev_index_head)\n\t\t\treturn -ENOMEM;\n\n\t\tspin_lock_init(&dtab->index_lock);\n\t} else {\n\t\tdtab->netdev_map = bpf_map_area_alloc((u64) dtab->map.max_entries *\n\t\t\t\t\t\t      sizeof(struct bpf_dtab_netdev *),\n\t\t\t\t\t\t      dtab->map.numa_node);\n\t\tif (!dtab->netdev_map)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic struct bpf_map *dev_map_alloc(union bpf_attr *attr)\n{\n\tstruct bpf_dtab *dtab;\n\tint err;\n\n\tdtab = bpf_map_area_alloc(sizeof(*dtab), NUMA_NO_NODE);\n\tif (!dtab)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = dev_map_init_map(dtab, attr);\n\tif (err) {\n\t\tbpf_map_area_free(dtab);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tspin_lock(&dev_map_lock);\n\tlist_add_tail_rcu(&dtab->list, &dev_map_list);\n\tspin_unlock(&dev_map_lock);\n\n\treturn &dtab->map;\n}\n\nstatic void dev_map_free(struct bpf_map *map)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tint i;\n\n\t \n\n\tspin_lock(&dev_map_lock);\n\tlist_del_rcu(&dtab->list);\n\tspin_unlock(&dev_map_lock);\n\n\tbpf_clear_redirect_map(map);\n\tsynchronize_rcu();\n\n\t \n\trcu_barrier();\n\n\tif (dtab->map.map_type == BPF_MAP_TYPE_DEVMAP_HASH) {\n\t\tfor (i = 0; i < dtab->n_buckets; i++) {\n\t\t\tstruct bpf_dtab_netdev *dev;\n\t\t\tstruct hlist_head *head;\n\t\t\tstruct hlist_node *next;\n\n\t\t\thead = dev_map_index_hash(dtab, i);\n\n\t\t\thlist_for_each_entry_safe(dev, next, head, index_hlist) {\n\t\t\t\thlist_del_rcu(&dev->index_hlist);\n\t\t\t\tif (dev->xdp_prog)\n\t\t\t\t\tbpf_prog_put(dev->xdp_prog);\n\t\t\t\tdev_put(dev->dev);\n\t\t\t\tkfree(dev);\n\t\t\t}\n\t\t}\n\n\t\tbpf_map_area_free(dtab->dev_index_head);\n\t} else {\n\t\tfor (i = 0; i < dtab->map.max_entries; i++) {\n\t\t\tstruct bpf_dtab_netdev *dev;\n\n\t\t\tdev = rcu_dereference_raw(dtab->netdev_map[i]);\n\t\t\tif (!dev)\n\t\t\t\tcontinue;\n\n\t\t\tif (dev->xdp_prog)\n\t\t\t\tbpf_prog_put(dev->xdp_prog);\n\t\t\tdev_put(dev->dev);\n\t\t\tkfree(dev);\n\t\t}\n\n\t\tbpf_map_area_free(dtab->netdev_map);\n\t}\n\n\tbpf_map_area_free(dtab);\n}\n\nstatic int dev_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tu32 index = key ? *(u32 *)key : U32_MAX;\n\tu32 *next = next_key;\n\n\tif (index >= dtab->map.max_entries) {\n\t\t*next = 0;\n\t\treturn 0;\n\t}\n\n\tif (index == dtab->map.max_entries - 1)\n\t\treturn -ENOENT;\n\t*next = index + 1;\n\treturn 0;\n}\n\n \nstatic void *__dev_map_hash_lookup_elem(struct bpf_map *map, u32 key)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct hlist_head *head = dev_map_index_hash(dtab, key);\n\tstruct bpf_dtab_netdev *dev;\n\n\thlist_for_each_entry_rcu(dev, head, index_hlist,\n\t\t\t\t lockdep_is_held(&dtab->index_lock))\n\t\tif (dev->idx == key)\n\t\t\treturn dev;\n\n\treturn NULL;\n}\n\nstatic int dev_map_hash_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\t    void *next_key)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tu32 idx, *next = next_key;\n\tstruct bpf_dtab_netdev *dev, *next_dev;\n\tstruct hlist_head *head;\n\tint i = 0;\n\n\tif (!key)\n\t\tgoto find_first;\n\n\tidx = *(u32 *)key;\n\n\tdev = __dev_map_hash_lookup_elem(map, idx);\n\tif (!dev)\n\t\tgoto find_first;\n\n\tnext_dev = hlist_entry_safe(rcu_dereference_raw(hlist_next_rcu(&dev->index_hlist)),\n\t\t\t\t    struct bpf_dtab_netdev, index_hlist);\n\n\tif (next_dev) {\n\t\t*next = next_dev->idx;\n\t\treturn 0;\n\t}\n\n\ti = idx & (dtab->n_buckets - 1);\n\ti++;\n\n find_first:\n\tfor (; i < dtab->n_buckets; i++) {\n\t\thead = dev_map_index_hash(dtab, i);\n\n\t\tnext_dev = hlist_entry_safe(rcu_dereference_raw(hlist_first_rcu(head)),\n\t\t\t\t\t    struct bpf_dtab_netdev,\n\t\t\t\t\t    index_hlist);\n\t\tif (next_dev) {\n\t\t\t*next = next_dev->idx;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic int dev_map_bpf_prog_run(struct bpf_prog *xdp_prog,\n\t\t\t\tstruct xdp_frame **frames, int n,\n\t\t\t\tstruct net_device *dev)\n{\n\tstruct xdp_txq_info txq = { .dev = dev };\n\tstruct xdp_buff xdp;\n\tint i, nframes = 0;\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tu32 act;\n\t\tint err;\n\n\t\txdp_convert_frame_to_buff(xdpf, &xdp);\n\t\txdp.txq = &txq;\n\n\t\tact = bpf_prog_run_xdp(xdp_prog, &xdp);\n\t\tswitch (act) {\n\t\tcase XDP_PASS:\n\t\t\terr = xdp_update_frame_from_buff(&xdp, xdpf);\n\t\t\tif (unlikely(err < 0))\n\t\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\t\telse\n\t\t\t\tframes[nframes++] = xdpf;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(NULL, xdp_prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_ABORTED:\n\t\t\ttrace_xdp_exception(dev, xdp_prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_DROP:\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn nframes;  \n}\n\nstatic void bq_xmit_all(struct xdp_dev_bulk_queue *bq, u32 flags)\n{\n\tstruct net_device *dev = bq->dev;\n\tunsigned int cnt = bq->count;\n\tint sent = 0, err = 0;\n\tint to_send = cnt;\n\tint i;\n\n\tif (unlikely(!cnt))\n\t\treturn;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tstruct xdp_frame *xdpf = bq->q[i];\n\n\t\tprefetch(xdpf);\n\t}\n\n\tif (bq->xdp_prog) {\n\t\tto_send = dev_map_bpf_prog_run(bq->xdp_prog, bq->q, cnt, dev);\n\t\tif (!to_send)\n\t\t\tgoto out;\n\t}\n\n\tsent = dev->netdev_ops->ndo_xdp_xmit(dev, to_send, bq->q, flags);\n\tif (sent < 0) {\n\t\t \n\t\terr = sent;\n\t\tsent = 0;\n\t}\n\n\t \n\tfor (i = sent; unlikely(i < to_send); i++)\n\t\txdp_return_frame_rx_napi(bq->q[i]);\n\nout:\n\tbq->count = 0;\n\ttrace_xdp_devmap_xmit(bq->dev_rx, dev, sent, cnt - sent, err);\n}\n\n \nvoid __dev_flush(void)\n{\n\tstruct list_head *flush_list = this_cpu_ptr(&dev_flush_list);\n\tstruct xdp_dev_bulk_queue *bq, *tmp;\n\n\tlist_for_each_entry_safe(bq, tmp, flush_list, flush_node) {\n\t\tbq_xmit_all(bq, XDP_XMIT_FLUSH);\n\t\tbq->dev_rx = NULL;\n\t\tbq->xdp_prog = NULL;\n\t\t__list_del_clearprev(&bq->flush_node);\n\t}\n}\n\n \nstatic void *__dev_map_lookup_elem(struct bpf_map *map, u32 key)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *obj;\n\n\tif (key >= map->max_entries)\n\t\treturn NULL;\n\n\tobj = rcu_dereference_check(dtab->netdev_map[key],\n\t\t\t\t    rcu_read_lock_bh_held());\n\treturn obj;\n}\n\n \nstatic void bq_enqueue(struct net_device *dev, struct xdp_frame *xdpf,\n\t\t       struct net_device *dev_rx, struct bpf_prog *xdp_prog)\n{\n\tstruct list_head *flush_list = this_cpu_ptr(&dev_flush_list);\n\tstruct xdp_dev_bulk_queue *bq = this_cpu_ptr(dev->xdp_bulkq);\n\n\tif (unlikely(bq->count == DEV_MAP_BULK_SIZE))\n\t\tbq_xmit_all(bq, 0);\n\n\t \n\tif (!bq->dev_rx) {\n\t\tbq->dev_rx = dev_rx;\n\t\tbq->xdp_prog = xdp_prog;\n\t\tlist_add(&bq->flush_node, flush_list);\n\t}\n\n\tbq->q[bq->count++] = xdpf;\n}\n\nstatic inline int __xdp_enqueue(struct net_device *dev, struct xdp_frame *xdpf,\n\t\t\t\tstruct net_device *dev_rx,\n\t\t\t\tstruct bpf_prog *xdp_prog)\n{\n\tint err;\n\n\tif (!(dev->xdp_features & NETDEV_XDP_ACT_NDO_XMIT))\n\t\treturn -EOPNOTSUPP;\n\n\tif (unlikely(!(dev->xdp_features & NETDEV_XDP_ACT_NDO_XMIT_SG) &&\n\t\t     xdp_frame_has_frags(xdpf)))\n\t\treturn -EOPNOTSUPP;\n\n\terr = xdp_ok_fwd_dev(dev, xdp_get_frame_len(xdpf));\n\tif (unlikely(err))\n\t\treturn err;\n\n\tbq_enqueue(dev, xdpf, dev_rx, xdp_prog);\n\treturn 0;\n}\n\nstatic u32 dev_map_bpf_prog_run_skb(struct sk_buff *skb, struct bpf_dtab_netdev *dst)\n{\n\tstruct xdp_txq_info txq = { .dev = dst->dev };\n\tstruct xdp_buff xdp;\n\tu32 act;\n\n\tif (!dst->xdp_prog)\n\t\treturn XDP_PASS;\n\n\t__skb_pull(skb, skb->mac_len);\n\txdp.txq = &txq;\n\n\tact = bpf_prog_run_generic_xdp(skb, &xdp, dst->xdp_prog);\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\t__skb_push(skb, skb->mac_len);\n\t\tbreak;\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(NULL, dst->xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(dst->dev, dst->xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\tkfree_skb(skb);\n\t\tbreak;\n\t}\n\n\treturn act;\n}\n\nint dev_xdp_enqueue(struct net_device *dev, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx)\n{\n\treturn __xdp_enqueue(dev, xdpf, dev_rx, NULL);\n}\n\nint dev_map_enqueue(struct bpf_dtab_netdev *dst, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx)\n{\n\tstruct net_device *dev = dst->dev;\n\n\treturn __xdp_enqueue(dev, xdpf, dev_rx, dst->xdp_prog);\n}\n\nstatic bool is_valid_dst(struct bpf_dtab_netdev *obj, struct xdp_frame *xdpf)\n{\n\tif (!obj)\n\t\treturn false;\n\n\tif (!(obj->dev->xdp_features & NETDEV_XDP_ACT_NDO_XMIT))\n\t\treturn false;\n\n\tif (unlikely(!(obj->dev->xdp_features & NETDEV_XDP_ACT_NDO_XMIT_SG) &&\n\t\t     xdp_frame_has_frags(xdpf)))\n\t\treturn false;\n\n\tif (xdp_ok_fwd_dev(obj->dev, xdp_get_frame_len(xdpf)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int dev_map_enqueue_clone(struct bpf_dtab_netdev *obj,\n\t\t\t\t struct net_device *dev_rx,\n\t\t\t\t struct xdp_frame *xdpf)\n{\n\tstruct xdp_frame *nxdpf;\n\n\tnxdpf = xdpf_clone(xdpf);\n\tif (!nxdpf)\n\t\treturn -ENOMEM;\n\n\tbq_enqueue(obj->dev, nxdpf, dev_rx, obj->xdp_prog);\n\n\treturn 0;\n}\n\nstatic inline bool is_ifindex_excluded(int *excluded, int num_excluded, int ifindex)\n{\n\twhile (num_excluded--) {\n\t\tif (ifindex == excluded[num_excluded])\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic int get_upper_ifindexes(struct net_device *dev, int *indexes)\n{\n\tstruct net_device *upper;\n\tstruct list_head *iter;\n\tint n = 0;\n\n\tnetdev_for_each_upper_dev_rcu(dev, upper, iter) {\n\t\tindexes[n++] = upper->ifindex;\n\t}\n\treturn n;\n}\n\nint dev_map_enqueue_multi(struct xdp_frame *xdpf, struct net_device *dev_rx,\n\t\t\t  struct bpf_map *map, bool exclude_ingress)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *dst, *last_dst = NULL;\n\tint excluded_devices[1+MAX_NEST_DEV];\n\tstruct hlist_head *head;\n\tint num_excluded = 0;\n\tunsigned int i;\n\tint err;\n\n\tif (exclude_ingress) {\n\t\tnum_excluded = get_upper_ifindexes(dev_rx, excluded_devices);\n\t\texcluded_devices[num_excluded++] = dev_rx->ifindex;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP) {\n\t\tfor (i = 0; i < map->max_entries; i++) {\n\t\t\tdst = rcu_dereference_check(dtab->netdev_map[i],\n\t\t\t\t\t\t    rcu_read_lock_bh_held());\n\t\t\tif (!is_valid_dst(dst, xdpf))\n\t\t\t\tcontinue;\n\n\t\t\tif (is_ifindex_excluded(excluded_devices, num_excluded, dst->dev->ifindex))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (!last_dst) {\n\t\t\t\tlast_dst = dst;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\terr = dev_map_enqueue_clone(last_dst, dev_rx, xdpf);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tlast_dst = dst;\n\t\t}\n\t} else {  \n\t\tfor (i = 0; i < dtab->n_buckets; i++) {\n\t\t\thead = dev_map_index_hash(dtab, i);\n\t\t\thlist_for_each_entry_rcu(dst, head, index_hlist,\n\t\t\t\t\t\t lockdep_is_held(&dtab->index_lock)) {\n\t\t\t\tif (!is_valid_dst(dst, xdpf))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (is_ifindex_excluded(excluded_devices, num_excluded,\n\t\t\t\t\t\t\tdst->dev->ifindex))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tif (!last_dst) {\n\t\t\t\t\tlast_dst = dst;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = dev_map_enqueue_clone(last_dst, dev_rx, xdpf);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tlast_dst = dst;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (last_dst)\n\t\tbq_enqueue(last_dst->dev, xdpf, dev_rx, last_dst->xdp_prog);\n\telse\n\t\txdp_return_frame_rx_napi(xdpf);  \n\n\treturn 0;\n}\n\nint dev_map_generic_redirect(struct bpf_dtab_netdev *dst, struct sk_buff *skb,\n\t\t\t     struct bpf_prog *xdp_prog)\n{\n\tint err;\n\n\terr = xdp_ok_fwd_dev(dst->dev, skb->len);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t \n\tif (dev_map_bpf_prog_run_skb(skb, dst) != XDP_PASS)\n\t\treturn 0;\n\n\tskb->dev = dst->dev;\n\tgeneric_xdp_tx(skb, xdp_prog);\n\n\treturn 0;\n}\n\nstatic int dev_map_redirect_clone(struct bpf_dtab_netdev *dst,\n\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t  struct bpf_prog *xdp_prog)\n{\n\tstruct sk_buff *nskb;\n\tint err;\n\n\tnskb = skb_clone(skb, GFP_ATOMIC);\n\tif (!nskb)\n\t\treturn -ENOMEM;\n\n\terr = dev_map_generic_redirect(dst, nskb, xdp_prog);\n\tif (unlikely(err)) {\n\t\tconsume_skb(nskb);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint dev_map_redirect_multi(struct net_device *dev, struct sk_buff *skb,\n\t\t\t   struct bpf_prog *xdp_prog, struct bpf_map *map,\n\t\t\t   bool exclude_ingress)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *dst, *last_dst = NULL;\n\tint excluded_devices[1+MAX_NEST_DEV];\n\tstruct hlist_head *head;\n\tstruct hlist_node *next;\n\tint num_excluded = 0;\n\tunsigned int i;\n\tint err;\n\n\tif (exclude_ingress) {\n\t\tnum_excluded = get_upper_ifindexes(dev, excluded_devices);\n\t\texcluded_devices[num_excluded++] = dev->ifindex;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP) {\n\t\tfor (i = 0; i < map->max_entries; i++) {\n\t\t\tdst = rcu_dereference_check(dtab->netdev_map[i],\n\t\t\t\t\t\t    rcu_read_lock_bh_held());\n\t\t\tif (!dst)\n\t\t\t\tcontinue;\n\n\t\t\tif (is_ifindex_excluded(excluded_devices, num_excluded, dst->dev->ifindex))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (!last_dst) {\n\t\t\t\tlast_dst = dst;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\terr = dev_map_redirect_clone(last_dst, skb, xdp_prog);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tlast_dst = dst;\n\n\t\t}\n\t} else {  \n\t\tfor (i = 0; i < dtab->n_buckets; i++) {\n\t\t\thead = dev_map_index_hash(dtab, i);\n\t\t\thlist_for_each_entry_safe(dst, next, head, index_hlist) {\n\t\t\t\tif (!dst)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (is_ifindex_excluded(excluded_devices, num_excluded,\n\t\t\t\t\t\t\tdst->dev->ifindex))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tif (!last_dst) {\n\t\t\t\t\tlast_dst = dst;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = dev_map_redirect_clone(last_dst, skb, xdp_prog);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tlast_dst = dst;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (last_dst)\n\t\treturn dev_map_generic_redirect(last_dst, skb, xdp_prog);\n\n\t \n\tconsume_skb(skb);\n\treturn 0;\n}\n\nstatic void *dev_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_dtab_netdev *obj = __dev_map_lookup_elem(map, *(u32 *)key);\n\n\treturn obj ? &obj->val : NULL;\n}\n\nstatic void *dev_map_hash_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_dtab_netdev *obj = __dev_map_hash_lookup_elem(map,\n\t\t\t\t\t\t\t\t*(u32 *)key);\n\treturn obj ? &obj->val : NULL;\n}\n\nstatic void __dev_map_entry_free(struct rcu_head *rcu)\n{\n\tstruct bpf_dtab_netdev *dev;\n\n\tdev = container_of(rcu, struct bpf_dtab_netdev, rcu);\n\tif (dev->xdp_prog)\n\t\tbpf_prog_put(dev->xdp_prog);\n\tdev_put(dev->dev);\n\tkfree(dev);\n}\n\nstatic long dev_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *old_dev;\n\tint k = *(u32 *)key;\n\n\tif (k >= map->max_entries)\n\t\treturn -EINVAL;\n\n\told_dev = unrcu_pointer(xchg(&dtab->netdev_map[k], NULL));\n\tif (old_dev) {\n\t\tcall_rcu(&old_dev->rcu, __dev_map_entry_free);\n\t\tatomic_dec((atomic_t *)&dtab->items);\n\t}\n\treturn 0;\n}\n\nstatic long dev_map_hash_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *old_dev;\n\tint k = *(u32 *)key;\n\tunsigned long flags;\n\tint ret = -ENOENT;\n\n\tspin_lock_irqsave(&dtab->index_lock, flags);\n\n\told_dev = __dev_map_hash_lookup_elem(map, k);\n\tif (old_dev) {\n\t\tdtab->items--;\n\t\thlist_del_init_rcu(&old_dev->index_hlist);\n\t\tcall_rcu(&old_dev->rcu, __dev_map_entry_free);\n\t\tret = 0;\n\t}\n\tspin_unlock_irqrestore(&dtab->index_lock, flags);\n\n\treturn ret;\n}\n\nstatic struct bpf_dtab_netdev *__dev_map_alloc_node(struct net *net,\n\t\t\t\t\t\t    struct bpf_dtab *dtab,\n\t\t\t\t\t\t    struct bpf_devmap_val *val,\n\t\t\t\t\t\t    unsigned int idx)\n{\n\tstruct bpf_prog *prog = NULL;\n\tstruct bpf_dtab_netdev *dev;\n\n\tdev = bpf_map_kmalloc_node(&dtab->map, sizeof(*dev),\n\t\t\t\t   GFP_NOWAIT | __GFP_NOWARN,\n\t\t\t\t   dtab->map.numa_node);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdev->dev = dev_get_by_index(net, val->ifindex);\n\tif (!dev->dev)\n\t\tgoto err_out;\n\n\tif (val->bpf_prog.fd > 0) {\n\t\tprog = bpf_prog_get_type_dev(val->bpf_prog.fd,\n\t\t\t\t\t     BPF_PROG_TYPE_XDP, false);\n\t\tif (IS_ERR(prog))\n\t\t\tgoto err_put_dev;\n\t\tif (prog->expected_attach_type != BPF_XDP_DEVMAP ||\n\t\t    !bpf_prog_map_compatible(&dtab->map, prog))\n\t\t\tgoto err_put_prog;\n\t}\n\n\tdev->idx = idx;\n\tif (prog) {\n\t\tdev->xdp_prog = prog;\n\t\tdev->val.bpf_prog.id = prog->aux->id;\n\t} else {\n\t\tdev->xdp_prog = NULL;\n\t\tdev->val.bpf_prog.id = 0;\n\t}\n\tdev->val.ifindex = val->ifindex;\n\n\treturn dev;\nerr_put_prog:\n\tbpf_prog_put(prog);\nerr_put_dev:\n\tdev_put(dev->dev);\nerr_out:\n\tkfree(dev);\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic long __dev_map_update_elem(struct net *net, struct bpf_map *map,\n\t\t\t\t  void *key, void *value, u64 map_flags)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *dev, *old_dev;\n\tstruct bpf_devmap_val val = {};\n\tu32 i = *(u32 *)key;\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\treturn -EINVAL;\n\tif (unlikely(i >= dtab->map.max_entries))\n\t\treturn -E2BIG;\n\tif (unlikely(map_flags == BPF_NOEXIST))\n\t\treturn -EEXIST;\n\n\t \n\tmemcpy(&val, value, map->value_size);\n\n\tif (!val.ifindex) {\n\t\tdev = NULL;\n\t\t \n\t\tif (val.bpf_prog.fd > 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tdev = __dev_map_alloc_node(net, dtab, &val, i);\n\t\tif (IS_ERR(dev))\n\t\t\treturn PTR_ERR(dev);\n\t}\n\n\t \n\told_dev = unrcu_pointer(xchg(&dtab->netdev_map[i], RCU_INITIALIZER(dev)));\n\tif (old_dev)\n\t\tcall_rcu(&old_dev->rcu, __dev_map_entry_free);\n\telse\n\t\tatomic_inc((atomic_t *)&dtab->items);\n\n\treturn 0;\n}\n\nstatic long dev_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\tu64 map_flags)\n{\n\treturn __dev_map_update_elem(current->nsproxy->net_ns,\n\t\t\t\t     map, key, value, map_flags);\n}\n\nstatic long __dev_map_hash_update_elem(struct net *net, struct bpf_map *map,\n\t\t\t\t       void *key, void *value, u64 map_flags)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tstruct bpf_dtab_netdev *dev, *old_dev;\n\tstruct bpf_devmap_val val = {};\n\tu32 idx = *(u32 *)key;\n\tunsigned long flags;\n\tint err = -EEXIST;\n\n\t \n\tmemcpy(&val, value, map->value_size);\n\n\tif (unlikely(map_flags > BPF_EXIST || !val.ifindex))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&dtab->index_lock, flags);\n\n\told_dev = __dev_map_hash_lookup_elem(map, idx);\n\tif (old_dev && (map_flags & BPF_NOEXIST))\n\t\tgoto out_err;\n\n\tdev = __dev_map_alloc_node(net, dtab, &val, idx);\n\tif (IS_ERR(dev)) {\n\t\terr = PTR_ERR(dev);\n\t\tgoto out_err;\n\t}\n\n\tif (old_dev) {\n\t\thlist_del_rcu(&old_dev->index_hlist);\n\t} else {\n\t\tif (dtab->items >= dtab->map.max_entries) {\n\t\t\tspin_unlock_irqrestore(&dtab->index_lock, flags);\n\t\t\tcall_rcu(&dev->rcu, __dev_map_entry_free);\n\t\t\treturn -E2BIG;\n\t\t}\n\t\tdtab->items++;\n\t}\n\n\thlist_add_head_rcu(&dev->index_hlist,\n\t\t\t   dev_map_index_hash(dtab, idx));\n\tspin_unlock_irqrestore(&dtab->index_lock, flags);\n\n\tif (old_dev)\n\t\tcall_rcu(&old_dev->rcu, __dev_map_entry_free);\n\n\treturn 0;\n\nout_err:\n\tspin_unlock_irqrestore(&dtab->index_lock, flags);\n\treturn err;\n}\n\nstatic long dev_map_hash_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t     u64 map_flags)\n{\n\treturn __dev_map_hash_update_elem(current->nsproxy->net_ns,\n\t\t\t\t\t map, key, value, map_flags);\n}\n\nstatic long dev_map_redirect(struct bpf_map *map, u64 ifindex, u64 flags)\n{\n\treturn __bpf_xdp_redirect_map(map, ifindex, flags,\n\t\t\t\t      BPF_F_BROADCAST | BPF_F_EXCLUDE_INGRESS,\n\t\t\t\t      __dev_map_lookup_elem);\n}\n\nstatic long dev_hash_map_redirect(struct bpf_map *map, u64 ifindex, u64 flags)\n{\n\treturn __bpf_xdp_redirect_map(map, ifindex, flags,\n\t\t\t\t      BPF_F_BROADCAST | BPF_F_EXCLUDE_INGRESS,\n\t\t\t\t      __dev_map_hash_lookup_elem);\n}\n\nstatic u64 dev_map_mem_usage(const struct bpf_map *map)\n{\n\tstruct bpf_dtab *dtab = container_of(map, struct bpf_dtab, map);\n\tu64 usage = sizeof(struct bpf_dtab);\n\n\tif (map->map_type == BPF_MAP_TYPE_DEVMAP_HASH)\n\t\tusage += (u64)dtab->n_buckets * sizeof(struct hlist_head);\n\telse\n\t\tusage += (u64)map->max_entries * sizeof(struct bpf_dtab_netdev *);\n\tusage += atomic_read((atomic_t *)&dtab->items) *\n\t\t\t (u64)sizeof(struct bpf_dtab_netdev);\n\treturn usage;\n}\n\nBTF_ID_LIST_SINGLE(dev_map_btf_ids, struct, bpf_dtab)\nconst struct bpf_map_ops dev_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc = dev_map_alloc,\n\t.map_free = dev_map_free,\n\t.map_get_next_key = dev_map_get_next_key,\n\t.map_lookup_elem = dev_map_lookup_elem,\n\t.map_update_elem = dev_map_update_elem,\n\t.map_delete_elem = dev_map_delete_elem,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = dev_map_mem_usage,\n\t.map_btf_id = &dev_map_btf_ids[0],\n\t.map_redirect = dev_map_redirect,\n};\n\nconst struct bpf_map_ops dev_map_hash_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc = dev_map_alloc,\n\t.map_free = dev_map_free,\n\t.map_get_next_key = dev_map_hash_get_next_key,\n\t.map_lookup_elem = dev_map_hash_lookup_elem,\n\t.map_update_elem = dev_map_hash_update_elem,\n\t.map_delete_elem = dev_map_hash_delete_elem,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = dev_map_mem_usage,\n\t.map_btf_id = &dev_map_btf_ids[0],\n\t.map_redirect = dev_hash_map_redirect,\n};\n\nstatic void dev_map_hash_remove_netdev(struct bpf_dtab *dtab,\n\t\t\t\t       struct net_device *netdev)\n{\n\tunsigned long flags;\n\tu32 i;\n\n\tspin_lock_irqsave(&dtab->index_lock, flags);\n\tfor (i = 0; i < dtab->n_buckets; i++) {\n\t\tstruct bpf_dtab_netdev *dev;\n\t\tstruct hlist_head *head;\n\t\tstruct hlist_node *next;\n\n\t\thead = dev_map_index_hash(dtab, i);\n\n\t\thlist_for_each_entry_safe(dev, next, head, index_hlist) {\n\t\t\tif (netdev != dev->dev)\n\t\t\t\tcontinue;\n\n\t\t\tdtab->items--;\n\t\t\thlist_del_rcu(&dev->index_hlist);\n\t\t\tcall_rcu(&dev->rcu, __dev_map_entry_free);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&dtab->index_lock, flags);\n}\n\nstatic int dev_map_notification(struct notifier_block *notifier,\n\t\t\t\tulong event, void *ptr)\n{\n\tstruct net_device *netdev = netdev_notifier_info_to_dev(ptr);\n\tstruct bpf_dtab *dtab;\n\tint i, cpu;\n\n\tswitch (event) {\n\tcase NETDEV_REGISTER:\n\t\tif (!netdev->netdev_ops->ndo_xdp_xmit || netdev->xdp_bulkq)\n\t\t\tbreak;\n\n\t\t \n\t\tnetdev->xdp_bulkq = alloc_percpu(struct xdp_dev_bulk_queue);\n\t\tif (!netdev->xdp_bulkq)\n\t\t\treturn NOTIFY_BAD;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tper_cpu_ptr(netdev->xdp_bulkq, cpu)->dev = netdev;\n\t\tbreak;\n\tcase NETDEV_UNREGISTER:\n\t\t \n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(dtab, &dev_map_list, list) {\n\t\t\tif (dtab->map.map_type == BPF_MAP_TYPE_DEVMAP_HASH) {\n\t\t\t\tdev_map_hash_remove_netdev(dtab, netdev);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tfor (i = 0; i < dtab->map.max_entries; i++) {\n\t\t\t\tstruct bpf_dtab_netdev *dev, *odev;\n\n\t\t\t\tdev = rcu_dereference(dtab->netdev_map[i]);\n\t\t\t\tif (!dev || netdev != dev->dev)\n\t\t\t\t\tcontinue;\n\t\t\t\todev = unrcu_pointer(cmpxchg(&dtab->netdev_map[i], RCU_INITIALIZER(dev), NULL));\n\t\t\t\tif (dev == odev) {\n\t\t\t\t\tcall_rcu(&dev->rcu,\n\t\t\t\t\t\t __dev_map_entry_free);\n\t\t\t\t\tatomic_dec((atomic_t *)&dtab->items);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block dev_map_notifier = {\n\t.notifier_call = dev_map_notification,\n};\n\nstatic int __init dev_map_init(void)\n{\n\tint cpu;\n\n\t \n\tBUILD_BUG_ON(offsetof(struct bpf_dtab_netdev, dev) !=\n\t\t     offsetof(struct _bpf_dtab_netdev, dev));\n\tregister_netdevice_notifier(&dev_map_notifier);\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_LIST_HEAD(&per_cpu(dev_flush_list, cpu));\n\treturn 0;\n}\n\nsubsys_initcall(dev_map_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}