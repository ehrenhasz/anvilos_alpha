{
  "module_name": "syscall.c",
  "hash_id": "673c0d7d667f57bd2508c13e6ace17b4c65fcf55c29d59ad35b8fff07ee232b5",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/syscall.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/bpf_trace.h>\n#include <linux/bpf_lirc.h>\n#include <linux/bpf_verifier.h>\n#include <linux/bsearch.h>\n#include <linux/btf.h>\n#include <linux/syscalls.h>\n#include <linux/slab.h>\n#include <linux/sched/signal.h>\n#include <linux/vmalloc.h>\n#include <linux/mmzone.h>\n#include <linux/anon_inodes.h>\n#include <linux/fdtable.h>\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/license.h>\n#include <linux/filter.h>\n#include <linux/kernel.h>\n#include <linux/idr.h>\n#include <linux/cred.h>\n#include <linux/timekeeping.h>\n#include <linux/ctype.h>\n#include <linux/nospec.h>\n#include <linux/audit.h>\n#include <uapi/linux/btf.h>\n#include <linux/pgtable.h>\n#include <linux/bpf_lsm.h>\n#include <linux/poll.h>\n#include <linux/sort.h>\n#include <linux/bpf-netns.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/memcontrol.h>\n#include <linux/trace_events.h>\n#include <net/netfilter/nf_bpf_link.h>\n\n#include <net/tcx.h>\n\n#define IS_FD_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY || \\\n\t\t\t  (map)->map_type == BPF_MAP_TYPE_CGROUP_ARRAY || \\\n\t\t\t  (map)->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)\n#define IS_FD_PROG_ARRAY(map) ((map)->map_type == BPF_MAP_TYPE_PROG_ARRAY)\n#define IS_FD_HASH(map) ((map)->map_type == BPF_MAP_TYPE_HASH_OF_MAPS)\n#define IS_FD_MAP(map) (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map) || \\\n\t\t\tIS_FD_HASH(map))\n\n#define BPF_OBJ_FLAG_MASK   (BPF_F_RDONLY | BPF_F_WRONLY)\n\nDEFINE_PER_CPU(int, bpf_prog_active);\nstatic DEFINE_IDR(prog_idr);\nstatic DEFINE_SPINLOCK(prog_idr_lock);\nstatic DEFINE_IDR(map_idr);\nstatic DEFINE_SPINLOCK(map_idr_lock);\nstatic DEFINE_IDR(link_idr);\nstatic DEFINE_SPINLOCK(link_idr_lock);\n\nint sysctl_unprivileged_bpf_disabled __read_mostly =\n\tIS_BUILTIN(CONFIG_BPF_UNPRIV_DEFAULT_OFF) ? 2 : 0;\n\nstatic const struct bpf_map_ops * const bpf_map_types[] = {\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)\n#define BPF_MAP_TYPE(_id, _ops) \\\n\t[_id] = &_ops,\n#define BPF_LINK_TYPE(_id, _name)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n};\n\n \nint bpf_check_uarg_tail_zero(bpfptr_t uaddr,\n\t\t\t     size_t expected_size,\n\t\t\t     size_t actual_size)\n{\n\tint res;\n\n\tif (unlikely(actual_size > PAGE_SIZE))\t \n\t\treturn -E2BIG;\n\n\tif (actual_size <= expected_size)\n\t\treturn 0;\n\n\tif (uaddr.is_kernel)\n\t\tres = memchr_inv(uaddr.kernel + expected_size, 0,\n\t\t\t\t actual_size - expected_size) == NULL;\n\telse\n\t\tres = check_zeroed_user(uaddr.user + expected_size,\n\t\t\t\t\tactual_size - expected_size);\n\tif (res < 0)\n\t\treturn res;\n\treturn res ? 0 : -E2BIG;\n}\n\nconst struct bpf_map_ops bpf_map_offload_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc = bpf_map_offload_map_alloc,\n\t.map_free = bpf_map_offload_map_free,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = bpf_map_offload_map_mem_usage,\n};\n\nstatic void bpf_map_write_active_inc(struct bpf_map *map)\n{\n\tatomic64_inc(&map->writecnt);\n}\n\nstatic void bpf_map_write_active_dec(struct bpf_map *map)\n{\n\tatomic64_dec(&map->writecnt);\n}\n\nbool bpf_map_write_active(const struct bpf_map *map)\n{\n\treturn atomic64_read(&map->writecnt) != 0;\n}\n\nstatic u32 bpf_map_value_size(const struct bpf_map *map)\n{\n\tif (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH ||\n\t    map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY ||\n\t    map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)\n\t\treturn round_up(map->value_size, 8) * num_possible_cpus();\n\telse if (IS_FD_MAP(map))\n\t\treturn sizeof(u32);\n\telse\n\t\treturn  map->value_size;\n}\n\nstatic void maybe_wait_bpf_programs(struct bpf_map *map)\n{\n\t \n\tif (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS ||\n\t    map->map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS)\n\t\tsynchronize_rcu();\n}\n\nstatic int bpf_map_update_value(struct bpf_map *map, struct file *map_file,\n\t\t\t\tvoid *key, void *value, __u64 flags)\n{\n\tint err;\n\n\t \n\tif (bpf_map_is_offloaded(map)) {\n\t\treturn bpf_map_offload_update_elem(map, key, value, flags);\n\t} else if (map->map_type == BPF_MAP_TYPE_CPUMAP ||\n\t\t   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\treturn map->ops->map_update_elem(map, key, value, flags);\n\t} else if (map->map_type == BPF_MAP_TYPE_SOCKHASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_SOCKMAP) {\n\t\treturn sock_map_update_elem_sys(map, key, value, flags);\n\t} else if (IS_FD_PROG_ARRAY(map)) {\n\t\treturn bpf_fd_array_map_update_elem(map, map_file, key, value,\n\t\t\t\t\t\t    flags);\n\t}\n\n\tbpf_disable_instrumentation();\n\tif (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n\t\terr = bpf_percpu_hash_update(map, key, value, flags);\n\t} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {\n\t\terr = bpf_percpu_array_update(map, key, value, flags);\n\t} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {\n\t\terr = bpf_percpu_cgroup_storage_update(map, key, value,\n\t\t\t\t\t\t       flags);\n\t} else if (IS_FD_ARRAY(map)) {\n\t\trcu_read_lock();\n\t\terr = bpf_fd_array_map_update_elem(map, map_file, key, value,\n\t\t\t\t\t\t   flags);\n\t\trcu_read_unlock();\n\t} else if (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {\n\t\trcu_read_lock();\n\t\terr = bpf_fd_htab_map_update_elem(map, map_file, key, value,\n\t\t\t\t\t\t  flags);\n\t\trcu_read_unlock();\n\t} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {\n\t\t \n\t\terr = bpf_fd_reuseport_array_update_elem(map, key, value,\n\t\t\t\t\t\t\t flags);\n\t} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t\t   map->map_type == BPF_MAP_TYPE_STACK ||\n\t\t   map->map_type == BPF_MAP_TYPE_BLOOM_FILTER) {\n\t\terr = map->ops->map_push_elem(map, value, flags);\n\t} else {\n\t\trcu_read_lock();\n\t\terr = map->ops->map_update_elem(map, key, value, flags);\n\t\trcu_read_unlock();\n\t}\n\tbpf_enable_instrumentation();\n\tmaybe_wait_bpf_programs(map);\n\n\treturn err;\n}\n\nstatic int bpf_map_copy_value(struct bpf_map *map, void *key, void *value,\n\t\t\t      __u64 flags)\n{\n\tvoid *ptr;\n\tint err;\n\n\tif (bpf_map_is_offloaded(map))\n\t\treturn bpf_map_offload_lookup_elem(map, key, value);\n\n\tbpf_disable_instrumentation();\n\tif (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n\t\terr = bpf_percpu_hash_copy(map, key, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_PERCPU_ARRAY) {\n\t\terr = bpf_percpu_array_copy(map, key, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE) {\n\t\terr = bpf_percpu_cgroup_storage_copy(map, key, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_STACK_TRACE) {\n\t\terr = bpf_stackmap_copy(map, key, value);\n\t} else if (IS_FD_ARRAY(map) || IS_FD_PROG_ARRAY(map)) {\n\t\terr = bpf_fd_array_map_lookup_elem(map, key, value);\n\t} else if (IS_FD_HASH(map)) {\n\t\terr = bpf_fd_htab_map_lookup_elem(map, key, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_REUSEPORT_SOCKARRAY) {\n\t\terr = bpf_fd_reuseport_array_lookup_elem(map, key, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t\t   map->map_type == BPF_MAP_TYPE_STACK ||\n\t\t   map->map_type == BPF_MAP_TYPE_BLOOM_FILTER) {\n\t\terr = map->ops->map_peek_elem(map, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\t \n\t\terr = bpf_struct_ops_map_sys_lookup_elem(map, key, value);\n\t} else {\n\t\trcu_read_lock();\n\t\tif (map->ops->map_lookup_elem_sys_only)\n\t\t\tptr = map->ops->map_lookup_elem_sys_only(map, key);\n\t\telse\n\t\t\tptr = map->ops->map_lookup_elem(map, key);\n\t\tif (IS_ERR(ptr)) {\n\t\t\terr = PTR_ERR(ptr);\n\t\t} else if (!ptr) {\n\t\t\terr = -ENOENT;\n\t\t} else {\n\t\t\terr = 0;\n\t\t\tif (flags & BPF_F_LOCK)\n\t\t\t\t \n\t\t\t\tcopy_map_value_locked(map, value, ptr, true);\n\t\t\telse\n\t\t\t\tcopy_map_value(map, value, ptr);\n\t\t\t \n\t\t\tcheck_and_init_map_value(map, value);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tbpf_enable_instrumentation();\n\tmaybe_wait_bpf_programs(map);\n\n\treturn err;\n}\n\n \nstatic void *__bpf_map_area_alloc(u64 size, int numa_node, bool mmapable)\n{\n\t \n\n\tgfp_t gfp = bpf_memcg_flags(__GFP_NOWARN | __GFP_ZERO);\n\tunsigned int flags = 0;\n\tunsigned long align = 1;\n\tvoid *area;\n\n\tif (size >= SIZE_MAX)\n\t\treturn NULL;\n\n\t \n\tif (mmapable) {\n\t\tBUG_ON(!PAGE_ALIGNED(size));\n\t\talign = SHMLBA;\n\t\tflags = VM_USERMAP;\n\t} else if (size <= (PAGE_SIZE << PAGE_ALLOC_COSTLY_ORDER)) {\n\t\tarea = kmalloc_node(size, gfp | GFP_USER | __GFP_NORETRY,\n\t\t\t\t    numa_node);\n\t\tif (area != NULL)\n\t\t\treturn area;\n\t}\n\n\treturn __vmalloc_node_range(size, align, VMALLOC_START, VMALLOC_END,\n\t\t\tgfp | GFP_KERNEL | __GFP_RETRY_MAYFAIL, PAGE_KERNEL,\n\t\t\tflags, numa_node, __builtin_return_address(0));\n}\n\nvoid *bpf_map_area_alloc(u64 size, int numa_node)\n{\n\treturn __bpf_map_area_alloc(size, numa_node, false);\n}\n\nvoid *bpf_map_area_mmapable_alloc(u64 size, int numa_node)\n{\n\treturn __bpf_map_area_alloc(size, numa_node, true);\n}\n\nvoid bpf_map_area_free(void *area)\n{\n\tkvfree(area);\n}\n\nstatic u32 bpf_map_flags_retain_permanent(u32 flags)\n{\n\t \n\treturn flags & ~(BPF_F_RDONLY | BPF_F_WRONLY);\n}\n\nvoid bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr)\n{\n\tmap->map_type = attr->map_type;\n\tmap->key_size = attr->key_size;\n\tmap->value_size = attr->value_size;\n\tmap->max_entries = attr->max_entries;\n\tmap->map_flags = bpf_map_flags_retain_permanent(attr->map_flags);\n\tmap->numa_node = bpf_map_attr_numa_node(attr);\n\tmap->map_extra = attr->map_extra;\n}\n\nstatic int bpf_map_alloc_id(struct bpf_map *map)\n{\n\tint id;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock_bh(&map_idr_lock);\n\tid = idr_alloc_cyclic(&map_idr, map, 1, INT_MAX, GFP_ATOMIC);\n\tif (id > 0)\n\t\tmap->id = id;\n\tspin_unlock_bh(&map_idr_lock);\n\tidr_preload_end();\n\n\tif (WARN_ON_ONCE(!id))\n\t\treturn -ENOSPC;\n\n\treturn id > 0 ? 0 : id;\n}\n\nvoid bpf_map_free_id(struct bpf_map *map)\n{\n\tunsigned long flags;\n\n\t \n\tif (!map->id)\n\t\treturn;\n\n\tspin_lock_irqsave(&map_idr_lock, flags);\n\n\tidr_remove(&map_idr, map->id);\n\tmap->id = 0;\n\n\tspin_unlock_irqrestore(&map_idr_lock, flags);\n}\n\n#ifdef CONFIG_MEMCG_KMEM\nstatic void bpf_map_save_memcg(struct bpf_map *map)\n{\n\t \n\tif (memcg_bpf_enabled())\n\t\tmap->objcg = get_obj_cgroup_from_current();\n}\n\nstatic void bpf_map_release_memcg(struct bpf_map *map)\n{\n\tif (map->objcg)\n\t\tobj_cgroup_put(map->objcg);\n}\n\nstatic struct mem_cgroup *bpf_map_get_memcg(const struct bpf_map *map)\n{\n\tif (map->objcg)\n\t\treturn get_mem_cgroup_from_objcg(map->objcg);\n\n\treturn root_mem_cgroup;\n}\n\nvoid *bpf_map_kmalloc_node(const struct bpf_map *map, size_t size, gfp_t flags,\n\t\t\t   int node)\n{\n\tstruct mem_cgroup *memcg, *old_memcg;\n\tvoid *ptr;\n\n\tmemcg = bpf_map_get_memcg(map);\n\told_memcg = set_active_memcg(memcg);\n\tptr = kmalloc_node(size, flags | __GFP_ACCOUNT, node);\n\tset_active_memcg(old_memcg);\n\tmem_cgroup_put(memcg);\n\n\treturn ptr;\n}\n\nvoid *bpf_map_kzalloc(const struct bpf_map *map, size_t size, gfp_t flags)\n{\n\tstruct mem_cgroup *memcg, *old_memcg;\n\tvoid *ptr;\n\n\tmemcg = bpf_map_get_memcg(map);\n\told_memcg = set_active_memcg(memcg);\n\tptr = kzalloc(size, flags | __GFP_ACCOUNT);\n\tset_active_memcg(old_memcg);\n\tmem_cgroup_put(memcg);\n\n\treturn ptr;\n}\n\nvoid *bpf_map_kvcalloc(struct bpf_map *map, size_t n, size_t size,\n\t\t       gfp_t flags)\n{\n\tstruct mem_cgroup *memcg, *old_memcg;\n\tvoid *ptr;\n\n\tmemcg = bpf_map_get_memcg(map);\n\told_memcg = set_active_memcg(memcg);\n\tptr = kvcalloc(n, size, flags | __GFP_ACCOUNT);\n\tset_active_memcg(old_memcg);\n\tmem_cgroup_put(memcg);\n\n\treturn ptr;\n}\n\nvoid __percpu *bpf_map_alloc_percpu(const struct bpf_map *map, size_t size,\n\t\t\t\t    size_t align, gfp_t flags)\n{\n\tstruct mem_cgroup *memcg, *old_memcg;\n\tvoid __percpu *ptr;\n\n\tmemcg = bpf_map_get_memcg(map);\n\told_memcg = set_active_memcg(memcg);\n\tptr = __alloc_percpu_gfp(size, align, flags | __GFP_ACCOUNT);\n\tset_active_memcg(old_memcg);\n\tmem_cgroup_put(memcg);\n\n\treturn ptr;\n}\n\n#else\nstatic void bpf_map_save_memcg(struct bpf_map *map)\n{\n}\n\nstatic void bpf_map_release_memcg(struct bpf_map *map)\n{\n}\n#endif\n\nstatic int btf_field_cmp(const void *a, const void *b)\n{\n\tconst struct btf_field *f1 = a, *f2 = b;\n\n\tif (f1->offset < f2->offset)\n\t\treturn -1;\n\telse if (f1->offset > f2->offset)\n\t\treturn 1;\n\treturn 0;\n}\n\nstruct btf_field *btf_record_find(const struct btf_record *rec, u32 offset,\n\t\t\t\t  u32 field_mask)\n{\n\tstruct btf_field *field;\n\n\tif (IS_ERR_OR_NULL(rec) || !(rec->field_mask & field_mask))\n\t\treturn NULL;\n\tfield = bsearch(&offset, rec->fields, rec->cnt, sizeof(rec->fields[0]), btf_field_cmp);\n\tif (!field || !(field->type & field_mask))\n\t\treturn NULL;\n\treturn field;\n}\n\nvoid btf_record_free(struct btf_record *rec)\n{\n\tint i;\n\n\tif (IS_ERR_OR_NULL(rec))\n\t\treturn;\n\tfor (i = 0; i < rec->cnt; i++) {\n\t\tswitch (rec->fields[i].type) {\n\t\tcase BPF_KPTR_UNREF:\n\t\tcase BPF_KPTR_REF:\n\t\t\tif (rec->fields[i].kptr.module)\n\t\t\t\tmodule_put(rec->fields[i].kptr.module);\n\t\t\tbtf_put(rec->fields[i].kptr.btf);\n\t\t\tbreak;\n\t\tcase BPF_LIST_HEAD:\n\t\tcase BPF_LIST_NODE:\n\t\tcase BPF_RB_ROOT:\n\t\tcase BPF_RB_NODE:\n\t\tcase BPF_SPIN_LOCK:\n\t\tcase BPF_TIMER:\n\t\tcase BPF_REFCOUNT:\n\t\t\t \n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tcontinue;\n\t\t}\n\t}\n\tkfree(rec);\n}\n\nvoid bpf_map_free_record(struct bpf_map *map)\n{\n\tbtf_record_free(map->record);\n\tmap->record = NULL;\n}\n\nstruct btf_record *btf_record_dup(const struct btf_record *rec)\n{\n\tconst struct btf_field *fields;\n\tstruct btf_record *new_rec;\n\tint ret, size, i;\n\n\tif (IS_ERR_OR_NULL(rec))\n\t\treturn NULL;\n\tsize = offsetof(struct btf_record, fields[rec->cnt]);\n\tnew_rec = kmemdup(rec, size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!new_rec)\n\t\treturn ERR_PTR(-ENOMEM);\n\t \n\tfields = rec->fields;\n\tnew_rec->cnt = 0;\n\tfor (i = 0; i < rec->cnt; i++) {\n\t\tswitch (fields[i].type) {\n\t\tcase BPF_KPTR_UNREF:\n\t\tcase BPF_KPTR_REF:\n\t\t\tbtf_get(fields[i].kptr.btf);\n\t\t\tif (fields[i].kptr.module && !try_module_get(fields[i].kptr.module)) {\n\t\t\t\tret = -ENXIO;\n\t\t\t\tgoto free;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BPF_LIST_HEAD:\n\t\tcase BPF_LIST_NODE:\n\t\tcase BPF_RB_ROOT:\n\t\tcase BPF_RB_NODE:\n\t\tcase BPF_SPIN_LOCK:\n\t\tcase BPF_TIMER:\n\t\tcase BPF_REFCOUNT:\n\t\t\t \n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EFAULT;\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tgoto free;\n\t\t}\n\t\tnew_rec->cnt++;\n\t}\n\treturn new_rec;\nfree:\n\tbtf_record_free(new_rec);\n\treturn ERR_PTR(ret);\n}\n\nbool btf_record_equal(const struct btf_record *rec_a, const struct btf_record *rec_b)\n{\n\tbool a_has_fields = !IS_ERR_OR_NULL(rec_a), b_has_fields = !IS_ERR_OR_NULL(rec_b);\n\tint size;\n\n\tif (!a_has_fields && !b_has_fields)\n\t\treturn true;\n\tif (a_has_fields != b_has_fields)\n\t\treturn false;\n\tif (rec_a->cnt != rec_b->cnt)\n\t\treturn false;\n\tsize = offsetof(struct btf_record, fields[rec_a->cnt]);\n\t \n\treturn !memcmp(rec_a, rec_b, size);\n}\n\nvoid bpf_obj_free_timer(const struct btf_record *rec, void *obj)\n{\n\tif (WARN_ON_ONCE(!btf_record_has_field(rec, BPF_TIMER)))\n\t\treturn;\n\tbpf_timer_cancel_and_free(obj + rec->timer_off);\n}\n\nextern void __bpf_obj_drop_impl(void *p, const struct btf_record *rec);\n\nvoid bpf_obj_free_fields(const struct btf_record *rec, void *obj)\n{\n\tconst struct btf_field *fields;\n\tint i;\n\n\tif (IS_ERR_OR_NULL(rec))\n\t\treturn;\n\tfields = rec->fields;\n\tfor (i = 0; i < rec->cnt; i++) {\n\t\tstruct btf_struct_meta *pointee_struct_meta;\n\t\tconst struct btf_field *field = &fields[i];\n\t\tvoid *field_ptr = obj + field->offset;\n\t\tvoid *xchgd_field;\n\n\t\tswitch (fields[i].type) {\n\t\tcase BPF_SPIN_LOCK:\n\t\t\tbreak;\n\t\tcase BPF_TIMER:\n\t\t\tbpf_timer_cancel_and_free(field_ptr);\n\t\t\tbreak;\n\t\tcase BPF_KPTR_UNREF:\n\t\t\tWRITE_ONCE(*(u64 *)field_ptr, 0);\n\t\t\tbreak;\n\t\tcase BPF_KPTR_REF:\n\t\t\txchgd_field = (void *)xchg((unsigned long *)field_ptr, 0);\n\t\t\tif (!xchgd_field)\n\t\t\t\tbreak;\n\n\t\t\tif (!btf_is_kernel(field->kptr.btf)) {\n\t\t\t\tpointee_struct_meta = btf_find_struct_meta(field->kptr.btf,\n\t\t\t\t\t\t\t\t\t   field->kptr.btf_id);\n\t\t\t\tmigrate_disable();\n\t\t\t\t__bpf_obj_drop_impl(xchgd_field, pointee_struct_meta ?\n\t\t\t\t\t\t\t\t pointee_struct_meta->record :\n\t\t\t\t\t\t\t\t NULL);\n\t\t\t\tmigrate_enable();\n\t\t\t} else {\n\t\t\t\tfield->kptr.dtor(xchgd_field);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BPF_LIST_HEAD:\n\t\t\tif (WARN_ON_ONCE(rec->spin_lock_off < 0))\n\t\t\t\tcontinue;\n\t\t\tbpf_list_head_free(field, field_ptr, obj + rec->spin_lock_off);\n\t\t\tbreak;\n\t\tcase BPF_RB_ROOT:\n\t\t\tif (WARN_ON_ONCE(rec->spin_lock_off < 0))\n\t\t\t\tcontinue;\n\t\t\tbpf_rb_root_free(field, field_ptr, obj + rec->spin_lock_off);\n\t\t\tbreak;\n\t\tcase BPF_LIST_NODE:\n\t\tcase BPF_RB_NODE:\n\t\tcase BPF_REFCOUNT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tcontinue;\n\t\t}\n\t}\n}\n\n \nstatic void bpf_map_free_deferred(struct work_struct *work)\n{\n\tstruct bpf_map *map = container_of(work, struct bpf_map, work);\n\tstruct btf_record *rec = map->record;\n\tstruct btf *btf = map->btf;\n\n\tsecurity_bpf_map_free(map);\n\tbpf_map_release_memcg(map);\n\t \n\tmap->ops->map_free(map);\n\t \n\tbtf_record_free(rec);\n\t \n\tbtf_put(btf);\n}\n\nstatic void bpf_map_put_uref(struct bpf_map *map)\n{\n\tif (atomic64_dec_and_test(&map->usercnt)) {\n\t\tif (map->ops->map_release_uref)\n\t\t\tmap->ops->map_release_uref(map);\n\t}\n}\n\nstatic void bpf_map_free_in_work(struct bpf_map *map)\n{\n\tINIT_WORK(&map->work, bpf_map_free_deferred);\n\t \n\tqueue_work(system_unbound_wq, &map->work);\n}\n\nstatic void bpf_map_free_rcu_gp(struct rcu_head *rcu)\n{\n\tbpf_map_free_in_work(container_of(rcu, struct bpf_map, rcu));\n}\n\nstatic void bpf_map_free_mult_rcu_gp(struct rcu_head *rcu)\n{\n\tif (rcu_trace_implies_rcu_gp())\n\t\tbpf_map_free_rcu_gp(rcu);\n\telse\n\t\tcall_rcu(rcu, bpf_map_free_rcu_gp);\n}\n\n \nvoid bpf_map_put(struct bpf_map *map)\n{\n\tif (atomic64_dec_and_test(&map->refcnt)) {\n\t\t \n\t\tbpf_map_free_id(map);\n\n\t\tif (READ_ONCE(map->free_after_mult_rcu_gp))\n\t\t\tcall_rcu_tasks_trace(&map->rcu, bpf_map_free_mult_rcu_gp);\n\t\telse\n\t\t\tbpf_map_free_in_work(map);\n\t}\n}\nEXPORT_SYMBOL_GPL(bpf_map_put);\n\nvoid bpf_map_put_with_uref(struct bpf_map *map)\n{\n\tbpf_map_put_uref(map);\n\tbpf_map_put(map);\n}\n\nstatic int bpf_map_release(struct inode *inode, struct file *filp)\n{\n\tstruct bpf_map *map = filp->private_data;\n\n\tif (map->ops->map_release)\n\t\tmap->ops->map_release(map, filp);\n\n\tbpf_map_put_with_uref(map);\n\treturn 0;\n}\n\nstatic fmode_t map_get_sys_perms(struct bpf_map *map, struct fd f)\n{\n\tfmode_t mode = f.file->f_mode;\n\n\t \n\tif (READ_ONCE(map->frozen))\n\t\tmode &= ~FMODE_CAN_WRITE;\n\treturn mode;\n}\n\n#ifdef CONFIG_PROC_FS\n \nstatic u64 bpf_map_memory_usage(const struct bpf_map *map)\n{\n\treturn map->ops->map_mem_usage(map);\n}\n\nstatic void bpf_map_show_fdinfo(struct seq_file *m, struct file *filp)\n{\n\tstruct bpf_map *map = filp->private_data;\n\tu32 type = 0, jited = 0;\n\n\tif (map_type_contains_progs(map)) {\n\t\tspin_lock(&map->owner.lock);\n\t\ttype  = map->owner.type;\n\t\tjited = map->owner.jited;\n\t\tspin_unlock(&map->owner.lock);\n\t}\n\n\tseq_printf(m,\n\t\t   \"map_type:\\t%u\\n\"\n\t\t   \"key_size:\\t%u\\n\"\n\t\t   \"value_size:\\t%u\\n\"\n\t\t   \"max_entries:\\t%u\\n\"\n\t\t   \"map_flags:\\t%#x\\n\"\n\t\t   \"map_extra:\\t%#llx\\n\"\n\t\t   \"memlock:\\t%llu\\n\"\n\t\t   \"map_id:\\t%u\\n\"\n\t\t   \"frozen:\\t%u\\n\",\n\t\t   map->map_type,\n\t\t   map->key_size,\n\t\t   map->value_size,\n\t\t   map->max_entries,\n\t\t   map->map_flags,\n\t\t   (unsigned long long)map->map_extra,\n\t\t   bpf_map_memory_usage(map),\n\t\t   map->id,\n\t\t   READ_ONCE(map->frozen));\n\tif (type) {\n\t\tseq_printf(m, \"owner_prog_type:\\t%u\\n\", type);\n\t\tseq_printf(m, \"owner_jited:\\t%u\\n\", jited);\n\t}\n}\n#endif\n\nstatic ssize_t bpf_dummy_read(struct file *filp, char __user *buf, size_t siz,\n\t\t\t      loff_t *ppos)\n{\n\t \n\treturn -EINVAL;\n}\n\nstatic ssize_t bpf_dummy_write(struct file *filp, const char __user *buf,\n\t\t\t       size_t siz, loff_t *ppos)\n{\n\t \n\treturn -EINVAL;\n}\n\n \nstatic void bpf_map_mmap_open(struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = vma->vm_file->private_data;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tbpf_map_write_active_inc(map);\n}\n\n \nstatic void bpf_map_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = vma->vm_file->private_data;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tbpf_map_write_active_dec(map);\n}\n\nstatic const struct vm_operations_struct bpf_map_default_vmops = {\n\t.open\t\t= bpf_map_mmap_open,\n\t.close\t\t= bpf_map_mmap_close,\n};\n\nstatic int bpf_map_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct bpf_map *map = filp->private_data;\n\tint err;\n\n\tif (!map->ops->map_mmap || !IS_ERR_OR_NULL(map->record))\n\t\treturn -ENOTSUPP;\n\n\tif (!(vma->vm_flags & VM_SHARED))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&map->freeze_mutex);\n\n\tif (vma->vm_flags & VM_WRITE) {\n\t\tif (map->frozen) {\n\t\t\terr = -EPERM;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (map->map_flags & BPF_F_RDONLY_PROG) {\n\t\t\terr = -EACCES;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tvma->vm_ops = &bpf_map_default_vmops;\n\tvma->vm_private_data = map;\n\tvm_flags_clear(vma, VM_MAYEXEC);\n\tif (!(vma->vm_flags & VM_WRITE))\n\t\t \n\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\n\terr = map->ops->map_mmap(map, vma);\n\tif (err)\n\t\tgoto out;\n\n\tif (vma->vm_flags & VM_MAYWRITE)\n\t\tbpf_map_write_active_inc(map);\nout:\n\tmutex_unlock(&map->freeze_mutex);\n\treturn err;\n}\n\nstatic __poll_t bpf_map_poll(struct file *filp, struct poll_table_struct *pts)\n{\n\tstruct bpf_map *map = filp->private_data;\n\n\tif (map->ops->map_poll)\n\t\treturn map->ops->map_poll(map, filp, pts);\n\n\treturn EPOLLERR;\n}\n\nconst struct file_operations bpf_map_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= bpf_map_show_fdinfo,\n#endif\n\t.release\t= bpf_map_release,\n\t.read\t\t= bpf_dummy_read,\n\t.write\t\t= bpf_dummy_write,\n\t.mmap\t\t= bpf_map_mmap,\n\t.poll\t\t= bpf_map_poll,\n};\n\nint bpf_map_new_fd(struct bpf_map *map, int flags)\n{\n\tint ret;\n\n\tret = security_bpf_map(map, OPEN_FMODE(flags));\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn anon_inode_getfd(\"bpf-map\", &bpf_map_fops, map,\n\t\t\t\tflags | O_CLOEXEC);\n}\n\nint bpf_get_file_flag(int flags)\n{\n\tif ((flags & BPF_F_RDONLY) && (flags & BPF_F_WRONLY))\n\t\treturn -EINVAL;\n\tif (flags & BPF_F_RDONLY)\n\t\treturn O_RDONLY;\n\tif (flags & BPF_F_WRONLY)\n\t\treturn O_WRONLY;\n\treturn O_RDWR;\n}\n\n \n#define CHECK_ATTR(CMD) \\\n\tmemchr_inv((void *) &attr->CMD##_LAST_FIELD + \\\n\t\t   sizeof(attr->CMD##_LAST_FIELD), 0, \\\n\t\t   sizeof(*attr) - \\\n\t\t   offsetof(union bpf_attr, CMD##_LAST_FIELD) - \\\n\t\t   sizeof(attr->CMD##_LAST_FIELD)) != NULL\n\n \nint bpf_obj_name_cpy(char *dst, const char *src, unsigned int size)\n{\n\tconst char *end = src + size;\n\tconst char *orig_src = src;\n\n\tmemset(dst, 0, size);\n\t \n\twhile (src < end && *src) {\n\t\tif (!isalnum(*src) &&\n\t\t    *src != '_' && *src != '.')\n\t\t\treturn -EINVAL;\n\t\t*dst++ = *src++;\n\t}\n\n\t \n\tif (src == end)\n\t\treturn -EINVAL;\n\n\treturn src - orig_src;\n}\n\nint map_check_no_btf(const struct bpf_map *map,\n\t\t     const struct btf *btf,\n\t\t     const struct btf_type *key_type,\n\t\t     const struct btf_type *value_type)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic int map_check_btf(struct bpf_map *map, const struct btf *btf,\n\t\t\t u32 btf_key_id, u32 btf_value_id)\n{\n\tconst struct btf_type *key_type, *value_type;\n\tu32 key_size, value_size;\n\tint ret = 0;\n\n\t \n\tif (btf_key_id) {\n\t\tkey_type = btf_type_id_size(btf, &btf_key_id, &key_size);\n\t\tif (!key_type || key_size != map->key_size)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tkey_type = btf_type_by_id(btf, 0);\n\t\tif (!map->ops->map_check_btf)\n\t\t\treturn -EINVAL;\n\t}\n\n\tvalue_type = btf_type_id_size(btf, &btf_value_id, &value_size);\n\tif (!value_type || value_size != map->value_size)\n\t\treturn -EINVAL;\n\n\tmap->record = btf_parse_fields(btf, value_type,\n\t\t\t\t       BPF_SPIN_LOCK | BPF_TIMER | BPF_KPTR | BPF_LIST_HEAD |\n\t\t\t\t       BPF_RB_ROOT | BPF_REFCOUNT,\n\t\t\t\t       map->value_size);\n\tif (!IS_ERR_OR_NULL(map->record)) {\n\t\tint i;\n\n\t\tif (!bpf_capable()) {\n\t\t\tret = -EPERM;\n\t\t\tgoto free_map_tab;\n\t\t}\n\t\tif (map->map_flags & (BPF_F_RDONLY_PROG | BPF_F_WRONLY_PROG)) {\n\t\t\tret = -EACCES;\n\t\t\tgoto free_map_tab;\n\t\t}\n\t\tfor (i = 0; i < sizeof(map->record->field_mask) * 8; i++) {\n\t\t\tswitch (map->record->field_mask & (1 << i)) {\n\t\t\tcase 0:\n\t\t\t\tcontinue;\n\t\t\tcase BPF_SPIN_LOCK:\n\t\t\t\tif (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_ARRAY &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_SK_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_INODE_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_TASK_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_CGRP_STORAGE) {\n\t\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\t\tgoto free_map_tab;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase BPF_TIMER:\n\t\t\t\tif (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_LRU_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_ARRAY) {\n\t\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\t\tgoto free_map_tab;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase BPF_KPTR_UNREF:\n\t\t\tcase BPF_KPTR_REF:\n\t\t\tcase BPF_REFCOUNT:\n\t\t\t\tif (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_LRU_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_LRU_PERCPU_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_ARRAY &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_ARRAY &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_SK_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_INODE_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_TASK_STORAGE &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_CGRP_STORAGE) {\n\t\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\t\tgoto free_map_tab;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase BPF_LIST_HEAD:\n\t\t\tcase BPF_RB_ROOT:\n\t\t\t\tif (map->map_type != BPF_MAP_TYPE_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_LRU_HASH &&\n\t\t\t\t    map->map_type != BPF_MAP_TYPE_ARRAY) {\n\t\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\t\tgoto free_map_tab;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tret = -EOPNOTSUPP;\n\t\t\t\tgoto free_map_tab;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = btf_check_and_fixup_fields(btf, map->record);\n\tif (ret < 0)\n\t\tgoto free_map_tab;\n\n\tif (map->ops->map_check_btf) {\n\t\tret = map->ops->map_check_btf(map, btf, key_type, value_type);\n\t\tif (ret < 0)\n\t\t\tgoto free_map_tab;\n\t}\n\n\treturn ret;\nfree_map_tab:\n\tbpf_map_free_record(map);\n\treturn ret;\n}\n\n#define BPF_MAP_CREATE_LAST_FIELD map_extra\n \nstatic int map_create(union bpf_attr *attr)\n{\n\tconst struct bpf_map_ops *ops;\n\tint numa_node = bpf_map_attr_numa_node(attr);\n\tu32 map_type = attr->map_type;\n\tstruct bpf_map *map;\n\tint f_flags;\n\tint err;\n\n\terr = CHECK_ATTR(BPF_MAP_CREATE);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tif (attr->btf_vmlinux_value_type_id) {\n\t\tif (attr->map_type != BPF_MAP_TYPE_STRUCT_OPS ||\n\t\t    attr->btf_key_type_id || attr->btf_value_type_id)\n\t\t\treturn -EINVAL;\n\t} else if (attr->btf_key_type_id && !attr->btf_value_type_id) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (attr->map_type != BPF_MAP_TYPE_BLOOM_FILTER &&\n\t    attr->map_extra != 0)\n\t\treturn -EINVAL;\n\n\tf_flags = bpf_get_file_flag(attr->map_flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\tif (numa_node != NUMA_NO_NODE &&\n\t    ((unsigned int)numa_node >= nr_node_ids ||\n\t     !node_online(numa_node)))\n\t\treturn -EINVAL;\n\n\t \n\tmap_type = attr->map_type;\n\tif (map_type >= ARRAY_SIZE(bpf_map_types))\n\t\treturn -EINVAL;\n\tmap_type = array_index_nospec(map_type, ARRAY_SIZE(bpf_map_types));\n\tops = bpf_map_types[map_type];\n\tif (!ops)\n\t\treturn -EINVAL;\n\n\tif (ops->map_alloc_check) {\n\t\terr = ops->map_alloc_check(attr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (attr->map_ifindex)\n\t\tops = &bpf_map_offload_ops;\n\tif (!ops->map_mem_usage)\n\t\treturn -EINVAL;\n\n\t \n\tif (sysctl_unprivileged_bpf_disabled && !bpf_capable())\n\t\treturn -EPERM;\n\n\t \n\tswitch (map_type) {\n\tcase BPF_MAP_TYPE_ARRAY:\n\tcase BPF_MAP_TYPE_PERCPU_ARRAY:\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH:\n\tcase BPF_MAP_TYPE_PERCPU_HASH:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\tcase BPF_MAP_TYPE_RINGBUF:\n\tcase BPF_MAP_TYPE_USER_RINGBUF:\n\tcase BPF_MAP_TYPE_CGROUP_STORAGE:\n\tcase BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:\n\t\t \n\t\tbreak;\n\tcase BPF_MAP_TYPE_SK_STORAGE:\n\tcase BPF_MAP_TYPE_INODE_STORAGE:\n\tcase BPF_MAP_TYPE_TASK_STORAGE:\n\tcase BPF_MAP_TYPE_CGRP_STORAGE:\n\tcase BPF_MAP_TYPE_BLOOM_FILTER:\n\tcase BPF_MAP_TYPE_LPM_TRIE:\n\tcase BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\tcase BPF_MAP_TYPE_QUEUE:\n\tcase BPF_MAP_TYPE_STACK:\n\tcase BPF_MAP_TYPE_LRU_HASH:\n\tcase BPF_MAP_TYPE_LRU_PERCPU_HASH:\n\tcase BPF_MAP_TYPE_STRUCT_OPS:\n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\tif (!bpf_capable())\n\t\t\treturn -EPERM;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\tcase BPF_MAP_TYPE_DEVMAP:\n\tcase BPF_MAP_TYPE_DEVMAP_HASH:\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn -EPERM;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unsupported map type %d\", map_type);\n\t\treturn -EPERM;\n\t}\n\n\tmap = ops->map_alloc(attr);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tmap->ops = ops;\n\tmap->map_type = map_type;\n\n\terr = bpf_obj_name_cpy(map->name, attr->map_name,\n\t\t\t       sizeof(attr->map_name));\n\tif (err < 0)\n\t\tgoto free_map;\n\n\tatomic64_set(&map->refcnt, 1);\n\tatomic64_set(&map->usercnt, 1);\n\tmutex_init(&map->freeze_mutex);\n\tspin_lock_init(&map->owner.lock);\n\n\tif (attr->btf_key_type_id || attr->btf_value_type_id ||\n\t     \n\t    attr->btf_vmlinux_value_type_id) {\n\t\tstruct btf *btf;\n\n\t\tbtf = btf_get_by_fd(attr->btf_fd);\n\t\tif (IS_ERR(btf)) {\n\t\t\terr = PTR_ERR(btf);\n\t\t\tgoto free_map;\n\t\t}\n\t\tif (btf_is_kernel(btf)) {\n\t\t\tbtf_put(btf);\n\t\t\terr = -EACCES;\n\t\t\tgoto free_map;\n\t\t}\n\t\tmap->btf = btf;\n\n\t\tif (attr->btf_value_type_id) {\n\t\t\terr = map_check_btf(map, btf, attr->btf_key_type_id,\n\t\t\t\t\t    attr->btf_value_type_id);\n\t\t\tif (err)\n\t\t\t\tgoto free_map;\n\t\t}\n\n\t\tmap->btf_key_type_id = attr->btf_key_type_id;\n\t\tmap->btf_value_type_id = attr->btf_value_type_id;\n\t\tmap->btf_vmlinux_value_type_id =\n\t\t\tattr->btf_vmlinux_value_type_id;\n\t}\n\n\terr = security_bpf_map_alloc(map);\n\tif (err)\n\t\tgoto free_map;\n\n\terr = bpf_map_alloc_id(map);\n\tif (err)\n\t\tgoto free_map_sec;\n\n\tbpf_map_save_memcg(map);\n\n\terr = bpf_map_new_fd(map, f_flags);\n\tif (err < 0) {\n\t\t \n\t\tbpf_map_put_with_uref(map);\n\t\treturn err;\n\t}\n\n\treturn err;\n\nfree_map_sec:\n\tsecurity_bpf_map_free(map);\nfree_map:\n\tbtf_put(map->btf);\n\tmap->ops->map_free(map);\n\treturn err;\n}\n\n \nstruct bpf_map *__bpf_map_get(struct fd f)\n{\n\tif (!f.file)\n\t\treturn ERR_PTR(-EBADF);\n\tif (f.file->f_op != &bpf_map_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn f.file->private_data;\n}\n\nvoid bpf_map_inc(struct bpf_map *map)\n{\n\tatomic64_inc(&map->refcnt);\n}\nEXPORT_SYMBOL_GPL(bpf_map_inc);\n\nvoid bpf_map_inc_with_uref(struct bpf_map *map)\n{\n\tatomic64_inc(&map->refcnt);\n\tatomic64_inc(&map->usercnt);\n}\nEXPORT_SYMBOL_GPL(bpf_map_inc_with_uref);\n\nstruct bpf_map *bpf_map_get(u32 ufd)\n{\n\tstruct fd f = fdget(ufd);\n\tstruct bpf_map *map;\n\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn map;\n\n\tbpf_map_inc(map);\n\tfdput(f);\n\n\treturn map;\n}\nEXPORT_SYMBOL(bpf_map_get);\n\nstruct bpf_map *bpf_map_get_with_uref(u32 ufd)\n{\n\tstruct fd f = fdget(ufd);\n\tstruct bpf_map *map;\n\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn map;\n\n\tbpf_map_inc_with_uref(map);\n\tfdput(f);\n\n\treturn map;\n}\n\n \nstruct bpf_map *__bpf_map_inc_not_zero(struct bpf_map *map, bool uref)\n{\n\tint refold;\n\n\trefold = atomic64_fetch_add_unless(&map->refcnt, 1, 0);\n\tif (!refold)\n\t\treturn ERR_PTR(-ENOENT);\n\tif (uref)\n\t\tatomic64_inc(&map->usercnt);\n\n\treturn map;\n}\n\nstruct bpf_map *bpf_map_inc_not_zero(struct bpf_map *map)\n{\n\tspin_lock_bh(&map_idr_lock);\n\tmap = __bpf_map_inc_not_zero(map, false);\n\tspin_unlock_bh(&map_idr_lock);\n\n\treturn map;\n}\nEXPORT_SYMBOL_GPL(bpf_map_inc_not_zero);\n\nint __weak bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic void *__bpf_copy_key(void __user *ukey, u64 key_size)\n{\n\tif (key_size)\n\t\treturn vmemdup_user(ukey, key_size);\n\n\tif (ukey)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn NULL;\n}\n\nstatic void *___bpf_copy_key(bpfptr_t ukey, u64 key_size)\n{\n\tif (key_size)\n\t\treturn kvmemdup_bpfptr(ukey, key_size);\n\n\tif (!bpfptr_is_null(ukey))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn NULL;\n}\n\n \n#define BPF_MAP_LOOKUP_ELEM_LAST_FIELD flags\n\nstatic int map_lookup_elem(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tvoid __user *uvalue = u64_to_user_ptr(attr->value);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_LOOKUP_ELEM))\n\t\treturn -EINVAL;\n\n\tif (attr->flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !btf_record_has_field(map->record, BPF_SPIN_LOCK)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = __bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\terr = -ENOMEM;\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value)\n\t\tgoto free_key;\n\n\tif (map->map_type == BPF_MAP_TYPE_BLOOM_FILTER) {\n\t\tif (copy_from_user(value, uvalue, value_size))\n\t\t\terr = -EFAULT;\n\t\telse\n\t\t\terr = bpf_map_copy_value(map, key, value, attr->flags);\n\t\tgoto free_value;\n\t}\n\n\terr = bpf_map_copy_value(map, key, value, attr->flags);\n\tif (err)\n\t\tgoto free_value;\n\n\terr = -EFAULT;\n\tif (copy_to_user(uvalue, value, value_size) != 0)\n\t\tgoto free_value;\n\n\terr = 0;\n\nfree_value:\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tfdput(f);\n\treturn err;\n}\n\n\n#define BPF_MAP_UPDATE_ELEM_LAST_FIELD flags\n\nstatic int map_update_elem(union bpf_attr *attr, bpfptr_t uattr)\n{\n\tbpfptr_t ukey = make_bpfptr(attr->key, uattr.is_kernel);\n\tbpfptr_t uvalue = make_bpfptr(attr->value, uattr.is_kernel);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_UPDATE_ELEM))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tbpf_map_write_active_inc(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !btf_record_has_field(map->record, BPF_SPIN_LOCK)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = ___bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\tvalue = kvmemdup_bpfptr(uvalue, value_size);\n\tif (IS_ERR(value)) {\n\t\terr = PTR_ERR(value);\n\t\tgoto free_key;\n\t}\n\n\terr = bpf_map_update_value(map, f.file, key, value, attr->flags);\n\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}\n\n#define BPF_MAP_DELETE_ELEM_LAST_FIELD key\n\nstatic int map_delete_elem(union bpf_attr *attr, bpfptr_t uattr)\n{\n\tbpfptr_t ukey = make_bpfptr(attr->key, uattr.is_kernel);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\tvoid *key;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_DELETE_ELEM))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tbpf_map_write_active_inc(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tkey = ___bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tif (bpf_map_is_offloaded(map)) {\n\t\terr = bpf_map_offload_delete_elem(map, key);\n\t\tgoto out;\n\t} else if (IS_FD_PROG_ARRAY(map) ||\n\t\t   map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\t \n\t\terr = map->ops->map_delete_elem(map, key);\n\t\tgoto out;\n\t}\n\n\tbpf_disable_instrumentation();\n\trcu_read_lock();\n\terr = map->ops->map_delete_elem(map, key);\n\trcu_read_unlock();\n\tbpf_enable_instrumentation();\n\tmaybe_wait_bpf_programs(map);\nout:\n\tkvfree(key);\nerr_put:\n\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}\n\n \n#define BPF_MAP_GET_NEXT_KEY_LAST_FIELD next_key\n\nstatic int map_get_next_key(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tvoid __user *unext_key = u64_to_user_ptr(attr->next_key);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *next_key;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_GET_NEXT_KEY))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (ukey) {\n\t\tkey = __bpf_copy_key(ukey, map->key_size);\n\t\tif (IS_ERR(key)) {\n\t\t\terr = PTR_ERR(key);\n\t\t\tgoto err_put;\n\t\t}\n\t} else {\n\t\tkey = NULL;\n\t}\n\n\terr = -ENOMEM;\n\tnext_key = kvmalloc(map->key_size, GFP_USER);\n\tif (!next_key)\n\t\tgoto free_key;\n\n\tif (bpf_map_is_offloaded(map)) {\n\t\terr = bpf_map_offload_get_next_key(map, key, next_key);\n\t\tgoto out;\n\t}\n\n\trcu_read_lock();\n\terr = map->ops->map_get_next_key(map, key, next_key);\n\trcu_read_unlock();\nout:\n\tif (err)\n\t\tgoto free_next_key;\n\n\terr = -EFAULT;\n\tif (copy_to_user(unext_key, next_key, map->key_size) != 0)\n\t\tgoto free_next_key;\n\n\terr = 0;\n\nfree_next_key:\n\tkvfree(next_key);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tfdput(f);\n\treturn err;\n}\n\nint generic_map_delete_batch(struct bpf_map *map,\n\t\t\t     const union bpf_attr *attr,\n\t\t\t     union bpf_attr __user *uattr)\n{\n\tvoid __user *keys = u64_to_user_ptr(attr->batch.keys);\n\tu32 cp, max_count;\n\tint err = 0;\n\tvoid *key;\n\n\tif (attr->batch.elem_flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tif ((attr->batch.elem_flags & BPF_F_LOCK) &&\n\t    !btf_record_has_field(map->record, BPF_SPIN_LOCK)) {\n\t\treturn -EINVAL;\n\t}\n\n\tmax_count = attr->batch.count;\n\tif (!max_count)\n\t\treturn 0;\n\n\tkey = kvmalloc(map->key_size, GFP_USER | __GFP_NOWARN);\n\tif (!key)\n\t\treturn -ENOMEM;\n\n\tfor (cp = 0; cp < max_count; cp++) {\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(key, keys + cp * map->key_size,\n\t\t\t\t   map->key_size))\n\t\t\tbreak;\n\n\t\tif (bpf_map_is_offloaded(map)) {\n\t\t\terr = bpf_map_offload_delete_elem(map, key);\n\t\t\tbreak;\n\t\t}\n\n\t\tbpf_disable_instrumentation();\n\t\trcu_read_lock();\n\t\terr = map->ops->map_delete_elem(map, key);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tif (err)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tif (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))\n\t\terr = -EFAULT;\n\n\tkvfree(key);\n\n\tmaybe_wait_bpf_programs(map);\n\treturn err;\n}\n\nint generic_map_update_batch(struct bpf_map *map, struct file *map_file,\n\t\t\t     const union bpf_attr *attr,\n\t\t\t     union bpf_attr __user *uattr)\n{\n\tvoid __user *values = u64_to_user_ptr(attr->batch.values);\n\tvoid __user *keys = u64_to_user_ptr(attr->batch.keys);\n\tu32 value_size, cp, max_count;\n\tvoid *key, *value;\n\tint err = 0;\n\n\tif (attr->batch.elem_flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tif ((attr->batch.elem_flags & BPF_F_LOCK) &&\n\t    !btf_record_has_field(map->record, BPF_SPIN_LOCK)) {\n\t\treturn -EINVAL;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\tmax_count = attr->batch.count;\n\tif (!max_count)\n\t\treturn 0;\n\n\tkey = kvmalloc(map->key_size, GFP_USER | __GFP_NOWARN);\n\tif (!key)\n\t\treturn -ENOMEM;\n\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value) {\n\t\tkvfree(key);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (cp = 0; cp < max_count; cp++) {\n\t\terr = -EFAULT;\n\t\tif (copy_from_user(key, keys + cp * map->key_size,\n\t\t    map->key_size) ||\n\t\t    copy_from_user(value, values + cp * value_size, value_size))\n\t\t\tbreak;\n\n\t\terr = bpf_map_update_value(map, map_file, key, value,\n\t\t\t\t\t   attr->batch.elem_flags);\n\n\t\tif (err)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\n\tif (copy_to_user(&uattr->batch.count, &cp, sizeof(cp)))\n\t\terr = -EFAULT;\n\n\tkvfree(value);\n\tkvfree(key);\n\treturn err;\n}\n\n#define MAP_LOOKUP_RETRIES 3\n\nint generic_map_lookup_batch(struct bpf_map *map,\n\t\t\t\t    const union bpf_attr *attr,\n\t\t\t\t    union bpf_attr __user *uattr)\n{\n\tvoid __user *uobatch = u64_to_user_ptr(attr->batch.out_batch);\n\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);\n\tvoid __user *values = u64_to_user_ptr(attr->batch.values);\n\tvoid __user *keys = u64_to_user_ptr(attr->batch.keys);\n\tvoid *buf, *buf_prevkey, *prev_key, *key, *value;\n\tint err, retry = MAP_LOOKUP_RETRIES;\n\tu32 value_size, cp, max_count;\n\n\tif (attr->batch.elem_flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tif ((attr->batch.elem_flags & BPF_F_LOCK) &&\n\t    !btf_record_has_field(map->record, BPF_SPIN_LOCK))\n\t\treturn -EINVAL;\n\n\tvalue_size = bpf_map_value_size(map);\n\n\tmax_count = attr->batch.count;\n\tif (!max_count)\n\t\treturn 0;\n\n\tif (put_user(0, &uattr->batch.count))\n\t\treturn -EFAULT;\n\n\tbuf_prevkey = kvmalloc(map->key_size, GFP_USER | __GFP_NOWARN);\n\tif (!buf_prevkey)\n\t\treturn -ENOMEM;\n\n\tbuf = kvmalloc(map->key_size + value_size, GFP_USER | __GFP_NOWARN);\n\tif (!buf) {\n\t\tkvfree(buf_prevkey);\n\t\treturn -ENOMEM;\n\t}\n\n\terr = -EFAULT;\n\tprev_key = NULL;\n\tif (ubatch && copy_from_user(buf_prevkey, ubatch, map->key_size))\n\t\tgoto free_buf;\n\tkey = buf;\n\tvalue = key + map->key_size;\n\tif (ubatch)\n\t\tprev_key = buf_prevkey;\n\n\tfor (cp = 0; cp < max_count;) {\n\t\trcu_read_lock();\n\t\terr = map->ops->map_get_next_key(map, prev_key, key);\n\t\trcu_read_unlock();\n\t\tif (err)\n\t\t\tbreak;\n\t\terr = bpf_map_copy_value(map, key, value,\n\t\t\t\t\t attr->batch.elem_flags);\n\n\t\tif (err == -ENOENT) {\n\t\t\tif (retry) {\n\t\t\t\tretry--;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\terr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto free_buf;\n\n\t\tif (copy_to_user(keys + cp * map->key_size, key,\n\t\t\t\t map->key_size)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto free_buf;\n\t\t}\n\t\tif (copy_to_user(values + cp * value_size, value, value_size)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto free_buf;\n\t\t}\n\n\t\tif (!prev_key)\n\t\t\tprev_key = buf_prevkey;\n\n\t\tswap(prev_key, key);\n\t\tretry = MAP_LOOKUP_RETRIES;\n\t\tcp++;\n\t\tcond_resched();\n\t}\n\n\tif (err == -EFAULT)\n\t\tgoto free_buf;\n\n\tif ((copy_to_user(&uattr->batch.count, &cp, sizeof(cp)) ||\n\t\t    (cp && copy_to_user(uobatch, prev_key, map->key_size))))\n\t\terr = -EFAULT;\n\nfree_buf:\n\tkvfree(buf_prevkey);\n\tkvfree(buf);\n\treturn err;\n}\n\n#define BPF_MAP_LOOKUP_AND_DELETE_ELEM_LAST_FIELD flags\n\nstatic int map_lookup_and_delete_elem(union bpf_attr *attr)\n{\n\tvoid __user *ukey = u64_to_user_ptr(attr->key);\n\tvoid __user *uvalue = u64_to_user_ptr(attr->value);\n\tint ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tvoid *key, *value;\n\tu32 value_size;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_MAP_LOOKUP_AND_DELETE_ELEM))\n\t\treturn -EINVAL;\n\n\tif (attr->flags & ~BPF_F_LOCK)\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tbpf_map_write_active_inc(map);\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_READ) ||\n\t    !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (attr->flags &&\n\t    (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t     map->map_type == BPF_MAP_TYPE_STACK)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tif ((attr->flags & BPF_F_LOCK) &&\n\t    !btf_record_has_field(map->record, BPF_SPIN_LOCK)) {\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tkey = __bpf_copy_key(ukey, map->key_size);\n\tif (IS_ERR(key)) {\n\t\terr = PTR_ERR(key);\n\t\tgoto err_put;\n\t}\n\n\tvalue_size = bpf_map_value_size(map);\n\n\terr = -ENOMEM;\n\tvalue = kvmalloc(value_size, GFP_USER | __GFP_NOWARN);\n\tif (!value)\n\t\tgoto free_key;\n\n\terr = -ENOTSUPP;\n\tif (map->map_type == BPF_MAP_TYPE_QUEUE ||\n\t    map->map_type == BPF_MAP_TYPE_STACK) {\n\t\terr = map->ops->map_pop_elem(map, value);\n\t} else if (map->map_type == BPF_MAP_TYPE_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_LRU_HASH ||\n\t\t   map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n\t\tif (!bpf_map_is_offloaded(map)) {\n\t\t\tbpf_disable_instrumentation();\n\t\t\trcu_read_lock();\n\t\t\terr = map->ops->map_lookup_and_delete_elem(map, key, value, attr->flags);\n\t\t\trcu_read_unlock();\n\t\t\tbpf_enable_instrumentation();\n\t\t}\n\t}\n\n\tif (err)\n\t\tgoto free_value;\n\n\tif (copy_to_user(uvalue, value, value_size) != 0) {\n\t\terr = -EFAULT;\n\t\tgoto free_value;\n\t}\n\n\terr = 0;\n\nfree_value:\n\tkvfree(value);\nfree_key:\n\tkvfree(key);\nerr_put:\n\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}\n\n#define BPF_MAP_FREEZE_LAST_FIELD map_fd\n\nstatic int map_freeze(const union bpf_attr *attr)\n{\n\tint err = 0, ufd = attr->map_fd;\n\tstruct bpf_map *map;\n\tstruct fd f;\n\n\tif (CHECK_ATTR(BPF_MAP_FREEZE))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tif (map->map_type == BPF_MAP_TYPE_STRUCT_OPS || !IS_ERR_OR_NULL(map->record)) {\n\t\tfdput(f);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tif (!(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\tfdput(f);\n\t\treturn -EPERM;\n\t}\n\n\tmutex_lock(&map->freeze_mutex);\n\tif (bpf_map_write_active(map)) {\n\t\terr = -EBUSY;\n\t\tgoto err_put;\n\t}\n\tif (READ_ONCE(map->frozen)) {\n\t\terr = -EBUSY;\n\t\tgoto err_put;\n\t}\n\n\tWRITE_ONCE(map->frozen, true);\nerr_put:\n\tmutex_unlock(&map->freeze_mutex);\n\tfdput(f);\n\treturn err;\n}\n\nstatic const struct bpf_prog_ops * const bpf_prog_types[] = {\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \\\n\t[_id] = & _name ## _prog_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#define BPF_LINK_TYPE(_id, _name)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n};\n\nstatic int find_prog_type(enum bpf_prog_type type, struct bpf_prog *prog)\n{\n\tconst struct bpf_prog_ops *ops;\n\n\tif (type >= ARRAY_SIZE(bpf_prog_types))\n\t\treturn -EINVAL;\n\ttype = array_index_nospec(type, ARRAY_SIZE(bpf_prog_types));\n\tops = bpf_prog_types[type];\n\tif (!ops)\n\t\treturn -EINVAL;\n\n\tif (!bpf_prog_is_offloaded(prog->aux))\n\t\tprog->aux->ops = ops;\n\telse\n\t\tprog->aux->ops = &bpf_offload_prog_ops;\n\tprog->type = type;\n\treturn 0;\n}\n\nenum bpf_audit {\n\tBPF_AUDIT_LOAD,\n\tBPF_AUDIT_UNLOAD,\n\tBPF_AUDIT_MAX,\n};\n\nstatic const char * const bpf_audit_str[BPF_AUDIT_MAX] = {\n\t[BPF_AUDIT_LOAD]   = \"LOAD\",\n\t[BPF_AUDIT_UNLOAD] = \"UNLOAD\",\n};\n\nstatic void bpf_audit_prog(const struct bpf_prog *prog, unsigned int op)\n{\n\tstruct audit_context *ctx = NULL;\n\tstruct audit_buffer *ab;\n\n\tif (WARN_ON_ONCE(op >= BPF_AUDIT_MAX))\n\t\treturn;\n\tif (audit_enabled == AUDIT_OFF)\n\t\treturn;\n\tif (!in_irq() && !irqs_disabled())\n\t\tctx = audit_context();\n\tab = audit_log_start(ctx, GFP_ATOMIC, AUDIT_BPF);\n\tif (unlikely(!ab))\n\t\treturn;\n\taudit_log_format(ab, \"prog-id=%u op=%s\",\n\t\t\t prog->aux->id, bpf_audit_str[op]);\n\taudit_log_end(ab);\n}\n\nstatic int bpf_prog_alloc_id(struct bpf_prog *prog)\n{\n\tint id;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock_bh(&prog_idr_lock);\n\tid = idr_alloc_cyclic(&prog_idr, prog, 1, INT_MAX, GFP_ATOMIC);\n\tif (id > 0)\n\t\tprog->aux->id = id;\n\tspin_unlock_bh(&prog_idr_lock);\n\tidr_preload_end();\n\n\t \n\tif (WARN_ON_ONCE(!id))\n\t\treturn -ENOSPC;\n\n\treturn id > 0 ? 0 : id;\n}\n\nvoid bpf_prog_free_id(struct bpf_prog *prog)\n{\n\tunsigned long flags;\n\n\t \n\tif (!prog->aux->id)\n\t\treturn;\n\n\tspin_lock_irqsave(&prog_idr_lock, flags);\n\tidr_remove(&prog_idr, prog->aux->id);\n\tprog->aux->id = 0;\n\tspin_unlock_irqrestore(&prog_idr_lock, flags);\n}\n\nstatic void __bpf_prog_put_rcu(struct rcu_head *rcu)\n{\n\tstruct bpf_prog_aux *aux = container_of(rcu, struct bpf_prog_aux, rcu);\n\n\tkvfree(aux->func_info);\n\tkfree(aux->func_info_aux);\n\tfree_uid(aux->user);\n\tsecurity_bpf_prog_free(aux);\n\tbpf_prog_free(aux->prog);\n}\n\nstatic void __bpf_prog_put_noref(struct bpf_prog *prog, bool deferred)\n{\n\tbpf_prog_kallsyms_del_all(prog);\n\tbtf_put(prog->aux->btf);\n\tmodule_put(prog->aux->mod);\n\tkvfree(prog->aux->jited_linfo);\n\tkvfree(prog->aux->linfo);\n\tkfree(prog->aux->kfunc_tab);\n\tif (prog->aux->attach_btf)\n\t\tbtf_put(prog->aux->attach_btf);\n\n\tif (deferred) {\n\t\tif (prog->aux->sleepable)\n\t\t\tcall_rcu_tasks_trace(&prog->aux->rcu, __bpf_prog_put_rcu);\n\t\telse\n\t\t\tcall_rcu(&prog->aux->rcu, __bpf_prog_put_rcu);\n\t} else {\n\t\t__bpf_prog_put_rcu(&prog->aux->rcu);\n\t}\n}\n\nstatic void bpf_prog_put_deferred(struct work_struct *work)\n{\n\tstruct bpf_prog_aux *aux;\n\tstruct bpf_prog *prog;\n\n\taux = container_of(work, struct bpf_prog_aux, work);\n\tprog = aux->prog;\n\tperf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_UNLOAD, 0);\n\tbpf_audit_prog(prog, BPF_AUDIT_UNLOAD);\n\tbpf_prog_free_id(prog);\n\t__bpf_prog_put_noref(prog, true);\n}\n\nstatic void __bpf_prog_put(struct bpf_prog *prog)\n{\n\tstruct bpf_prog_aux *aux = prog->aux;\n\n\tif (atomic64_dec_and_test(&aux->refcnt)) {\n\t\tif (in_irq() || irqs_disabled()) {\n\t\t\tINIT_WORK(&aux->work, bpf_prog_put_deferred);\n\t\t\tschedule_work(&aux->work);\n\t\t} else {\n\t\t\tbpf_prog_put_deferred(&aux->work);\n\t\t}\n\t}\n}\n\nvoid bpf_prog_put(struct bpf_prog *prog)\n{\n\t__bpf_prog_put(prog);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_put);\n\nstatic int bpf_prog_release(struct inode *inode, struct file *filp)\n{\n\tstruct bpf_prog *prog = filp->private_data;\n\n\tbpf_prog_put(prog);\n\treturn 0;\n}\n\nstruct bpf_prog_kstats {\n\tu64 nsecs;\n\tu64 cnt;\n\tu64 misses;\n};\n\nvoid notrace bpf_prog_inc_misses_counter(struct bpf_prog *prog)\n{\n\tstruct bpf_prog_stats *stats;\n\tunsigned int flags;\n\n\tstats = this_cpu_ptr(prog->stats);\n\tflags = u64_stats_update_begin_irqsave(&stats->syncp);\n\tu64_stats_inc(&stats->misses);\n\tu64_stats_update_end_irqrestore(&stats->syncp, flags);\n}\n\nstatic void bpf_prog_get_stats(const struct bpf_prog *prog,\n\t\t\t       struct bpf_prog_kstats *stats)\n{\n\tu64 nsecs = 0, cnt = 0, misses = 0;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tconst struct bpf_prog_stats *st;\n\t\tunsigned int start;\n\t\tu64 tnsecs, tcnt, tmisses;\n\n\t\tst = per_cpu_ptr(prog->stats, cpu);\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&st->syncp);\n\t\t\ttnsecs = u64_stats_read(&st->nsecs);\n\t\t\ttcnt = u64_stats_read(&st->cnt);\n\t\t\ttmisses = u64_stats_read(&st->misses);\n\t\t} while (u64_stats_fetch_retry(&st->syncp, start));\n\t\tnsecs += tnsecs;\n\t\tcnt += tcnt;\n\t\tmisses += tmisses;\n\t}\n\tstats->nsecs = nsecs;\n\tstats->cnt = cnt;\n\tstats->misses = misses;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void bpf_prog_show_fdinfo(struct seq_file *m, struct file *filp)\n{\n\tconst struct bpf_prog *prog = filp->private_data;\n\tchar prog_tag[sizeof(prog->tag) * 2 + 1] = { };\n\tstruct bpf_prog_kstats stats;\n\n\tbpf_prog_get_stats(prog, &stats);\n\tbin2hex(prog_tag, prog->tag, sizeof(prog->tag));\n\tseq_printf(m,\n\t\t   \"prog_type:\\t%u\\n\"\n\t\t   \"prog_jited:\\t%u\\n\"\n\t\t   \"prog_tag:\\t%s\\n\"\n\t\t   \"memlock:\\t%llu\\n\"\n\t\t   \"prog_id:\\t%u\\n\"\n\t\t   \"run_time_ns:\\t%llu\\n\"\n\t\t   \"run_cnt:\\t%llu\\n\"\n\t\t   \"recursion_misses:\\t%llu\\n\"\n\t\t   \"verified_insns:\\t%u\\n\",\n\t\t   prog->type,\n\t\t   prog->jited,\n\t\t   prog_tag,\n\t\t   prog->pages * 1ULL << PAGE_SHIFT,\n\t\t   prog->aux->id,\n\t\t   stats.nsecs,\n\t\t   stats.cnt,\n\t\t   stats.misses,\n\t\t   prog->aux->verified_insns);\n}\n#endif\n\nconst struct file_operations bpf_prog_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= bpf_prog_show_fdinfo,\n#endif\n\t.release\t= bpf_prog_release,\n\t.read\t\t= bpf_dummy_read,\n\t.write\t\t= bpf_dummy_write,\n};\n\nint bpf_prog_new_fd(struct bpf_prog *prog)\n{\n\tint ret;\n\n\tret = security_bpf_prog(prog);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn anon_inode_getfd(\"bpf-prog\", &bpf_prog_fops, prog,\n\t\t\t\tO_RDWR | O_CLOEXEC);\n}\n\nstatic struct bpf_prog *____bpf_prog_get(struct fd f)\n{\n\tif (!f.file)\n\t\treturn ERR_PTR(-EBADF);\n\tif (f.file->f_op != &bpf_prog_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn f.file->private_data;\n}\n\nvoid bpf_prog_add(struct bpf_prog *prog, int i)\n{\n\tatomic64_add(i, &prog->aux->refcnt);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_add);\n\nvoid bpf_prog_sub(struct bpf_prog *prog, int i)\n{\n\t \n\tWARN_ON(atomic64_sub_return(i, &prog->aux->refcnt) == 0);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_sub);\n\nvoid bpf_prog_inc(struct bpf_prog *prog)\n{\n\tatomic64_inc(&prog->aux->refcnt);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_inc);\n\n \nstruct bpf_prog *bpf_prog_inc_not_zero(struct bpf_prog *prog)\n{\n\tint refold;\n\n\trefold = atomic64_fetch_add_unless(&prog->aux->refcnt, 1, 0);\n\n\tif (!refold)\n\t\treturn ERR_PTR(-ENOENT);\n\n\treturn prog;\n}\nEXPORT_SYMBOL_GPL(bpf_prog_inc_not_zero);\n\nbool bpf_prog_get_ok(struct bpf_prog *prog,\n\t\t\t    enum bpf_prog_type *attach_type, bool attach_drv)\n{\n\t \n\tif (!attach_type)\n\t\treturn true;\n\n\tif (prog->type != *attach_type)\n\t\treturn false;\n\tif (bpf_prog_is_offloaded(prog->aux) && !attach_drv)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct bpf_prog *__bpf_prog_get(u32 ufd, enum bpf_prog_type *attach_type,\n\t\t\t\t       bool attach_drv)\n{\n\tstruct fd f = fdget(ufd);\n\tstruct bpf_prog *prog;\n\n\tprog = ____bpf_prog_get(f);\n\tif (IS_ERR(prog))\n\t\treturn prog;\n\tif (!bpf_prog_get_ok(prog, attach_type, attach_drv)) {\n\t\tprog = ERR_PTR(-EINVAL);\n\t\tgoto out;\n\t}\n\n\tbpf_prog_inc(prog);\nout:\n\tfdput(f);\n\treturn prog;\n}\n\nstruct bpf_prog *bpf_prog_get(u32 ufd)\n{\n\treturn __bpf_prog_get(ufd, NULL, false);\n}\n\nstruct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,\n\t\t\t\t       bool attach_drv)\n{\n\treturn __bpf_prog_get(ufd, &type, attach_drv);\n}\nEXPORT_SYMBOL_GPL(bpf_prog_get_type_dev);\n\n \nstatic void bpf_prog_load_fixup_attach_type(union bpf_attr *attr)\n{\n\tswitch (attr->prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\t\t \n\t\tif (!attr->expected_attach_type)\n\t\t\tattr->expected_attach_type =\n\t\t\t\tBPF_CGROUP_INET_SOCK_CREATE;\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\t\tif (!attr->expected_attach_type)\n\t\t\tattr->expected_attach_type =\n\t\t\t\tBPF_SK_REUSEPORT_SELECT;\n\t\tbreak;\n\t}\n}\n\nstatic int\nbpf_prog_load_check_attach(enum bpf_prog_type prog_type,\n\t\t\t   enum bpf_attach_type expected_attach_type,\n\t\t\t   struct btf *attach_btf, u32 btf_id,\n\t\t\t   struct bpf_prog *dst_prog)\n{\n\tif (btf_id) {\n\t\tif (btf_id > BTF_MAX_TYPE)\n\t\t\treturn -EINVAL;\n\n\t\tif (!attach_btf && !dst_prog)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (prog_type) {\n\t\tcase BPF_PROG_TYPE_TRACING:\n\t\tcase BPF_PROG_TYPE_LSM:\n\t\tcase BPF_PROG_TYPE_STRUCT_OPS:\n\t\tcase BPF_PROG_TYPE_EXT:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (attach_btf && (!btf_id || dst_prog))\n\t\treturn -EINVAL;\n\n\tif (dst_prog && prog_type != BPF_PROG_TYPE_TRACING &&\n\t    prog_type != BPF_PROG_TYPE_EXT)\n\t\treturn -EINVAL;\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\t\tswitch (expected_attach_type) {\n\t\tcase BPF_CGROUP_INET_SOCK_CREATE:\n\t\tcase BPF_CGROUP_INET_SOCK_RELEASE:\n\t\tcase BPF_CGROUP_INET4_POST_BIND:\n\t\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tswitch (expected_attach_type) {\n\t\tcase BPF_CGROUP_INET4_BIND:\n\t\tcase BPF_CGROUP_INET6_BIND:\n\t\tcase BPF_CGROUP_INET4_CONNECT:\n\t\tcase BPF_CGROUP_INET6_CONNECT:\n\t\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\t\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\t\tcase BPF_CGROUP_UDP4_SENDMSG:\n\t\tcase BPF_CGROUP_UDP6_SENDMSG:\n\t\tcase BPF_CGROUP_UDP4_RECVMSG:\n\t\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tswitch (expected_attach_type) {\n\t\tcase BPF_CGROUP_INET_INGRESS:\n\t\tcase BPF_CGROUP_INET_EGRESS:\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tswitch (expected_attach_type) {\n\t\tcase BPF_CGROUP_SETSOCKOPT:\n\t\tcase BPF_CGROUP_GETSOCKOPT:\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\tif (expected_attach_type == BPF_SK_LOOKUP)\n\t\t\treturn 0;\n\t\treturn -EINVAL;\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\t\tswitch (expected_attach_type) {\n\t\tcase BPF_SK_REUSEPORT_SELECT:\n\t\tcase BPF_SK_REUSEPORT_SELECT_OR_MIGRATE:\n\t\t\treturn 0;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\tcase BPF_PROG_TYPE_NETFILTER:\n\t\tif (expected_attach_type == BPF_NETFILTER)\n\t\t\treturn 0;\n\t\treturn -EINVAL;\n\tcase BPF_PROG_TYPE_SYSCALL:\n\tcase BPF_PROG_TYPE_EXT:\n\t\tif (expected_attach_type)\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic bool is_net_admin_prog_type(enum bpf_prog_type prog_type)\n{\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_LWT_SEG6LOCAL:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_EXT:  \n\tcase BPF_PROG_TYPE_NETFILTER:\n\t\treturn true;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\t \n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\t\t \n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool is_perfmon_prog_type(enum bpf_prog_type prog_type)\n{\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_KPROBE:\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE:\n\tcase BPF_PROG_TYPE_TRACING:\n\tcase BPF_PROG_TYPE_LSM:\n\tcase BPF_PROG_TYPE_STRUCT_OPS:  \n\tcase BPF_PROG_TYPE_EXT:  \n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \n#define\tBPF_PROG_LOAD_LAST_FIELD log_true_size\n\nstatic int bpf_prog_load(union bpf_attr *attr, bpfptr_t uattr, u32 uattr_size)\n{\n\tenum bpf_prog_type type = attr->prog_type;\n\tstruct bpf_prog *prog, *dst_prog = NULL;\n\tstruct btf *attach_btf = NULL;\n\tint err;\n\tchar license[128];\n\n\tif (CHECK_ATTR(BPF_PROG_LOAD))\n\t\treturn -EINVAL;\n\n\tif (attr->prog_flags & ~(BPF_F_STRICT_ALIGNMENT |\n\t\t\t\t BPF_F_ANY_ALIGNMENT |\n\t\t\t\t BPF_F_TEST_STATE_FREQ |\n\t\t\t\t BPF_F_SLEEPABLE |\n\t\t\t\t BPF_F_TEST_RND_HI32 |\n\t\t\t\t BPF_F_XDP_HAS_FRAGS |\n\t\t\t\t BPF_F_XDP_DEV_BOUND_ONLY))\n\t\treturn -EINVAL;\n\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) &&\n\t    (attr->prog_flags & BPF_F_ANY_ALIGNMENT) &&\n\t    !bpf_capable())\n\t\treturn -EPERM;\n\n\t \n\tif (sysctl_unprivileged_bpf_disabled && !bpf_capable())\n\t\treturn -EPERM;\n\n\tif (attr->insn_cnt == 0 ||\n\t    attr->insn_cnt > (bpf_capable() ? BPF_COMPLEXITY_LIMIT_INSNS : BPF_MAXINSNS))\n\t\treturn -E2BIG;\n\tif (type != BPF_PROG_TYPE_SOCKET_FILTER &&\n\t    type != BPF_PROG_TYPE_CGROUP_SKB &&\n\t    !bpf_capable())\n\t\treturn -EPERM;\n\n\tif (is_net_admin_prog_type(type) && !capable(CAP_NET_ADMIN) && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tif (is_perfmon_prog_type(type) && !perfmon_capable())\n\t\treturn -EPERM;\n\n\t \n\tif (attr->attach_prog_fd) {\n\t\tdst_prog = bpf_prog_get(attr->attach_prog_fd);\n\t\tif (IS_ERR(dst_prog)) {\n\t\t\tdst_prog = NULL;\n\t\t\tattach_btf = btf_get_by_fd(attr->attach_btf_obj_fd);\n\t\t\tif (IS_ERR(attach_btf))\n\t\t\t\treturn -EINVAL;\n\t\t\tif (!btf_is_kernel(attach_btf)) {\n\t\t\t\t \n\t\t\t\tbtf_put(attach_btf);\n\t\t\t\treturn -ENOTSUPP;\n\t\t\t}\n\t\t}\n\t} else if (attr->attach_btf_id) {\n\t\t \n\t\tattach_btf = bpf_get_btf_vmlinux();\n\t\tif (IS_ERR(attach_btf))\n\t\t\treturn PTR_ERR(attach_btf);\n\t\tif (!attach_btf)\n\t\t\treturn -EINVAL;\n\t\tbtf_get(attach_btf);\n\t}\n\n\tbpf_prog_load_fixup_attach_type(attr);\n\tif (bpf_prog_load_check_attach(type, attr->expected_attach_type,\n\t\t\t\t       attach_btf, attr->attach_btf_id,\n\t\t\t\t       dst_prog)) {\n\t\tif (dst_prog)\n\t\t\tbpf_prog_put(dst_prog);\n\t\tif (attach_btf)\n\t\t\tbtf_put(attach_btf);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tprog = bpf_prog_alloc(bpf_prog_size(attr->insn_cnt), GFP_USER);\n\tif (!prog) {\n\t\tif (dst_prog)\n\t\t\tbpf_prog_put(dst_prog);\n\t\tif (attach_btf)\n\t\t\tbtf_put(attach_btf);\n\t\treturn -ENOMEM;\n\t}\n\n\tprog->expected_attach_type = attr->expected_attach_type;\n\tprog->aux->attach_btf = attach_btf;\n\tprog->aux->attach_btf_id = attr->attach_btf_id;\n\tprog->aux->dst_prog = dst_prog;\n\tprog->aux->dev_bound = !!attr->prog_ifindex;\n\tprog->aux->sleepable = attr->prog_flags & BPF_F_SLEEPABLE;\n\tprog->aux->xdp_has_frags = attr->prog_flags & BPF_F_XDP_HAS_FRAGS;\n\n\terr = security_bpf_prog_alloc(prog->aux);\n\tif (err)\n\t\tgoto free_prog;\n\n\tprog->aux->user = get_current_user();\n\tprog->len = attr->insn_cnt;\n\n\terr = -EFAULT;\n\tif (copy_from_bpfptr(prog->insns,\n\t\t\t     make_bpfptr(attr->insns, uattr.is_kernel),\n\t\t\t     bpf_prog_insn_size(prog)) != 0)\n\t\tgoto free_prog_sec;\n\t \n\tif (strncpy_from_bpfptr(license,\n\t\t\t\tmake_bpfptr(attr->license, uattr.is_kernel),\n\t\t\t\tsizeof(license) - 1) < 0)\n\t\tgoto free_prog_sec;\n\tlicense[sizeof(license) - 1] = 0;\n\n\t \n\tprog->gpl_compatible = license_is_gpl_compatible(license) ? 1 : 0;\n\n\tprog->orig_prog = NULL;\n\tprog->jited = 0;\n\n\tatomic64_set(&prog->aux->refcnt, 1);\n\n\tif (bpf_prog_is_dev_bound(prog->aux)) {\n\t\terr = bpf_prog_dev_bound_init(prog, attr);\n\t\tif (err)\n\t\t\tgoto free_prog_sec;\n\t}\n\n\tif (type == BPF_PROG_TYPE_EXT && dst_prog &&\n\t    bpf_prog_is_dev_bound(dst_prog->aux)) {\n\t\terr = bpf_prog_dev_bound_inherit(prog, dst_prog);\n\t\tif (err)\n\t\t\tgoto free_prog_sec;\n\t}\n\n\t \n\terr = find_prog_type(type, prog);\n\tif (err < 0)\n\t\tgoto free_prog_sec;\n\n\tprog->aux->load_time = ktime_get_boottime_ns();\n\terr = bpf_obj_name_cpy(prog->aux->name, attr->prog_name,\n\t\t\t       sizeof(attr->prog_name));\n\tif (err < 0)\n\t\tgoto free_prog_sec;\n\n\t \n\terr = bpf_check(&prog, attr, uattr, uattr_size);\n\tif (err < 0)\n\t\tgoto free_used_maps;\n\n\tprog = bpf_prog_select_runtime(prog, &err);\n\tif (err < 0)\n\t\tgoto free_used_maps;\n\n\terr = bpf_prog_alloc_id(prog);\n\tif (err)\n\t\tgoto free_used_maps;\n\n\t \n\tbpf_prog_kallsyms_add(prog);\n\tperf_event_bpf_event(prog, PERF_BPF_EVENT_PROG_LOAD, 0);\n\tbpf_audit_prog(prog, BPF_AUDIT_LOAD);\n\n\terr = bpf_prog_new_fd(prog);\n\tif (err < 0)\n\t\tbpf_prog_put(prog);\n\treturn err;\n\nfree_used_maps:\n\t \n\t__bpf_prog_put_noref(prog, prog->aux->func_cnt);\n\treturn err;\nfree_prog_sec:\n\tfree_uid(prog->aux->user);\n\tsecurity_bpf_prog_free(prog->aux);\nfree_prog:\n\tif (prog->aux->attach_btf)\n\t\tbtf_put(prog->aux->attach_btf);\n\tbpf_prog_free(prog);\n\treturn err;\n}\n\n#define BPF_OBJ_LAST_FIELD path_fd\n\nstatic int bpf_obj_pin(const union bpf_attr *attr)\n{\n\tint path_fd;\n\n\tif (CHECK_ATTR(BPF_OBJ) || attr->file_flags & ~BPF_F_PATH_FD)\n\t\treturn -EINVAL;\n\n\t \n\tif (!(attr->file_flags & BPF_F_PATH_FD) && attr->path_fd)\n\t\treturn -EINVAL;\n\n\tpath_fd = attr->file_flags & BPF_F_PATH_FD ? attr->path_fd : AT_FDCWD;\n\treturn bpf_obj_pin_user(attr->bpf_fd, path_fd,\n\t\t\t\tu64_to_user_ptr(attr->pathname));\n}\n\nstatic int bpf_obj_get(const union bpf_attr *attr)\n{\n\tint path_fd;\n\n\tif (CHECK_ATTR(BPF_OBJ) || attr->bpf_fd != 0 ||\n\t    attr->file_flags & ~(BPF_OBJ_FLAG_MASK | BPF_F_PATH_FD))\n\t\treturn -EINVAL;\n\n\t \n\tif (!(attr->file_flags & BPF_F_PATH_FD) && attr->path_fd)\n\t\treturn -EINVAL;\n\n\tpath_fd = attr->file_flags & BPF_F_PATH_FD ? attr->path_fd : AT_FDCWD;\n\treturn bpf_obj_get_user(path_fd, u64_to_user_ptr(attr->pathname),\n\t\t\t\tattr->file_flags);\n}\n\nvoid bpf_link_init(struct bpf_link *link, enum bpf_link_type type,\n\t\t   const struct bpf_link_ops *ops, struct bpf_prog *prog)\n{\n\tatomic64_set(&link->refcnt, 1);\n\tlink->type = type;\n\tlink->id = 0;\n\tlink->ops = ops;\n\tlink->prog = prog;\n}\n\nstatic void bpf_link_free_id(int id)\n{\n\tif (!id)\n\t\treturn;\n\n\tspin_lock_bh(&link_idr_lock);\n\tidr_remove(&link_idr, id);\n\tspin_unlock_bh(&link_idr_lock);\n}\n\n \nvoid bpf_link_cleanup(struct bpf_link_primer *primer)\n{\n\tprimer->link->prog = NULL;\n\tbpf_link_free_id(primer->id);\n\tfput(primer->file);\n\tput_unused_fd(primer->fd);\n}\n\nvoid bpf_link_inc(struct bpf_link *link)\n{\n\tatomic64_inc(&link->refcnt);\n}\n\n \nstatic void bpf_link_free(struct bpf_link *link)\n{\n\tbpf_link_free_id(link->id);\n\tif (link->prog) {\n\t\t \n\t\tlink->ops->release(link);\n\t\tbpf_prog_put(link->prog);\n\t}\n\t \n\tlink->ops->dealloc(link);\n}\n\nstatic void bpf_link_put_deferred(struct work_struct *work)\n{\n\tstruct bpf_link *link = container_of(work, struct bpf_link, work);\n\n\tbpf_link_free(link);\n}\n\n \nvoid bpf_link_put(struct bpf_link *link)\n{\n\tif (!atomic64_dec_and_test(&link->refcnt))\n\t\treturn;\n\n\tINIT_WORK(&link->work, bpf_link_put_deferred);\n\tschedule_work(&link->work);\n}\nEXPORT_SYMBOL(bpf_link_put);\n\nstatic void bpf_link_put_direct(struct bpf_link *link)\n{\n\tif (!atomic64_dec_and_test(&link->refcnt))\n\t\treturn;\n\tbpf_link_free(link);\n}\n\nstatic int bpf_link_release(struct inode *inode, struct file *filp)\n{\n\tstruct bpf_link *link = filp->private_data;\n\n\tbpf_link_put_direct(link);\n\treturn 0;\n}\n\n#ifdef CONFIG_PROC_FS\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type)\n#define BPF_MAP_TYPE(_id, _ops)\n#define BPF_LINK_TYPE(_id, _name) [_id] = #_name,\nstatic const char *bpf_link_type_strs[] = {\n\t[BPF_LINK_TYPE_UNSPEC] = \"<invalid>\",\n#include <linux/bpf_types.h>\n};\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n\nstatic void bpf_link_show_fdinfo(struct seq_file *m, struct file *filp)\n{\n\tconst struct bpf_link *link = filp->private_data;\n\tconst struct bpf_prog *prog = link->prog;\n\tchar prog_tag[sizeof(prog->tag) * 2 + 1] = { };\n\n\tseq_printf(m,\n\t\t   \"link_type:\\t%s\\n\"\n\t\t   \"link_id:\\t%u\\n\",\n\t\t   bpf_link_type_strs[link->type],\n\t\t   link->id);\n\tif (prog) {\n\t\tbin2hex(prog_tag, prog->tag, sizeof(prog->tag));\n\t\tseq_printf(m,\n\t\t\t   \"prog_tag:\\t%s\\n\"\n\t\t\t   \"prog_id:\\t%u\\n\",\n\t\t\t   prog_tag,\n\t\t\t   prog->aux->id);\n\t}\n\tif (link->ops->show_fdinfo)\n\t\tlink->ops->show_fdinfo(link, m);\n}\n#endif\n\nstatic const struct file_operations bpf_link_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= bpf_link_show_fdinfo,\n#endif\n\t.release\t= bpf_link_release,\n\t.read\t\t= bpf_dummy_read,\n\t.write\t\t= bpf_dummy_write,\n};\n\nstatic int bpf_link_alloc_id(struct bpf_link *link)\n{\n\tint id;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock_bh(&link_idr_lock);\n\tid = idr_alloc_cyclic(&link_idr, link, 1, INT_MAX, GFP_ATOMIC);\n\tspin_unlock_bh(&link_idr_lock);\n\tidr_preload_end();\n\n\treturn id;\n}\n\n \nint bpf_link_prime(struct bpf_link *link, struct bpf_link_primer *primer)\n{\n\tstruct file *file;\n\tint fd, id;\n\n\tfd = get_unused_fd_flags(O_CLOEXEC);\n\tif (fd < 0)\n\t\treturn fd;\n\n\n\tid = bpf_link_alloc_id(link);\n\tif (id < 0) {\n\t\tput_unused_fd(fd);\n\t\treturn id;\n\t}\n\n\tfile = anon_inode_getfile(\"bpf_link\", &bpf_link_fops, link, O_CLOEXEC);\n\tif (IS_ERR(file)) {\n\t\tbpf_link_free_id(id);\n\t\tput_unused_fd(fd);\n\t\treturn PTR_ERR(file);\n\t}\n\n\tprimer->link = link;\n\tprimer->file = file;\n\tprimer->fd = fd;\n\tprimer->id = id;\n\treturn 0;\n}\n\nint bpf_link_settle(struct bpf_link_primer *primer)\n{\n\t \n\tspin_lock_bh(&link_idr_lock);\n\tprimer->link->id = primer->id;\n\tspin_unlock_bh(&link_idr_lock);\n\t \n\tfd_install(primer->fd, primer->file);\n\t \n\treturn primer->fd;\n}\n\nint bpf_link_new_fd(struct bpf_link *link)\n{\n\treturn anon_inode_getfd(\"bpf-link\", &bpf_link_fops, link, O_CLOEXEC);\n}\n\nstruct bpf_link *bpf_link_get_from_fd(u32 ufd)\n{\n\tstruct fd f = fdget(ufd);\n\tstruct bpf_link *link;\n\n\tif (!f.file)\n\t\treturn ERR_PTR(-EBADF);\n\tif (f.file->f_op != &bpf_link_fops) {\n\t\tfdput(f);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tlink = f.file->private_data;\n\tbpf_link_inc(link);\n\tfdput(f);\n\n\treturn link;\n}\nEXPORT_SYMBOL(bpf_link_get_from_fd);\n\nstatic void bpf_tracing_link_release(struct bpf_link *link)\n{\n\tstruct bpf_tracing_link *tr_link =\n\t\tcontainer_of(link, struct bpf_tracing_link, link.link);\n\n\tWARN_ON_ONCE(bpf_trampoline_unlink_prog(&tr_link->link,\n\t\t\t\t\t\ttr_link->trampoline));\n\n\tbpf_trampoline_put(tr_link->trampoline);\n\n\t \n\tif (tr_link->tgt_prog)\n\t\tbpf_prog_put(tr_link->tgt_prog);\n}\n\nstatic void bpf_tracing_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_tracing_link *tr_link =\n\t\tcontainer_of(link, struct bpf_tracing_link, link.link);\n\n\tkfree(tr_link);\n}\n\nstatic void bpf_tracing_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t\t struct seq_file *seq)\n{\n\tstruct bpf_tracing_link *tr_link =\n\t\tcontainer_of(link, struct bpf_tracing_link, link.link);\n\tu32 target_btf_id, target_obj_id;\n\n\tbpf_trampoline_unpack_key(tr_link->trampoline->key,\n\t\t\t\t  &target_obj_id, &target_btf_id);\n\tseq_printf(seq,\n\t\t   \"attach_type:\\t%d\\n\"\n\t\t   \"target_obj_id:\\t%u\\n\"\n\t\t   \"target_btf_id:\\t%u\\n\",\n\t\t   tr_link->attach_type,\n\t\t   target_obj_id,\n\t\t   target_btf_id);\n}\n\nstatic int bpf_tracing_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t\t   struct bpf_link_info *info)\n{\n\tstruct bpf_tracing_link *tr_link =\n\t\tcontainer_of(link, struct bpf_tracing_link, link.link);\n\n\tinfo->tracing.attach_type = tr_link->attach_type;\n\tbpf_trampoline_unpack_key(tr_link->trampoline->key,\n\t\t\t\t  &info->tracing.target_obj_id,\n\t\t\t\t  &info->tracing.target_btf_id);\n\n\treturn 0;\n}\n\nstatic const struct bpf_link_ops bpf_tracing_link_lops = {\n\t.release = bpf_tracing_link_release,\n\t.dealloc = bpf_tracing_link_dealloc,\n\t.show_fdinfo = bpf_tracing_link_show_fdinfo,\n\t.fill_link_info = bpf_tracing_link_fill_link_info,\n};\n\nstatic int bpf_tracing_prog_attach(struct bpf_prog *prog,\n\t\t\t\t   int tgt_prog_fd,\n\t\t\t\t   u32 btf_id,\n\t\t\t\t   u64 bpf_cookie)\n{\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_prog *tgt_prog = NULL;\n\tstruct bpf_trampoline *tr = NULL;\n\tstruct bpf_tracing_link *link;\n\tu64 key = 0;\n\tint err;\n\n\tswitch (prog->type) {\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tif (prog->expected_attach_type != BPF_TRACE_FENTRY &&\n\t\t    prog->expected_attach_type != BPF_TRACE_FEXIT &&\n\t\t    prog->expected_attach_type != BPF_MODIFY_RETURN) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_put_prog;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\tif (prog->expected_attach_type != 0) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_put_prog;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_LSM:\n\t\tif (prog->expected_attach_type != BPF_LSM_MAC) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_put_prog;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto out_put_prog;\n\t}\n\n\tif (!!tgt_prog_fd != !!btf_id) {\n\t\terr = -EINVAL;\n\t\tgoto out_put_prog;\n\t}\n\n\tif (tgt_prog_fd) {\n\t\t \n\t\tif (prog->type != BPF_PROG_TYPE_EXT) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_put_prog;\n\t\t}\n\n\t\ttgt_prog = bpf_prog_get(tgt_prog_fd);\n\t\tif (IS_ERR(tgt_prog)) {\n\t\t\terr = PTR_ERR(tgt_prog);\n\t\t\ttgt_prog = NULL;\n\t\t\tgoto out_put_prog;\n\t\t}\n\n\t\tkey = bpf_trampoline_compute_key(tgt_prog, NULL, btf_id);\n\t}\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_prog;\n\t}\n\tbpf_link_init(&link->link.link, BPF_LINK_TYPE_TRACING,\n\t\t      &bpf_tracing_link_lops, prog);\n\tlink->attach_type = prog->expected_attach_type;\n\tlink->link.cookie = bpf_cookie;\n\n\tmutex_lock(&prog->aux->dst_mutex);\n\n\t \n\tif (!prog->aux->dst_trampoline && !tgt_prog) {\n\t\t \n\t\tif (prog->type != BPF_PROG_TYPE_TRACING &&\n\t\t    prog->type != BPF_PROG_TYPE_LSM) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\t \n\t\tif (!prog->aux->attach_btf) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tbtf_id = prog->aux->attach_btf_id;\n\t\tkey = bpf_trampoline_compute_key(NULL, prog->aux->attach_btf, btf_id);\n\t}\n\n\tif (!prog->aux->dst_trampoline ||\n\t    (key && key != prog->aux->dst_trampoline->key)) {\n\t\t \n\t\tstruct bpf_attach_target_info tgt_info = {};\n\n\t\terr = bpf_check_attach_target(NULL, prog, tgt_prog, btf_id,\n\t\t\t\t\t      &tgt_info);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\n\t\tif (tgt_info.tgt_mod) {\n\t\t\tmodule_put(prog->aux->mod);\n\t\t\tprog->aux->mod = tgt_info.tgt_mod;\n\t\t}\n\n\t\ttr = bpf_trampoline_get(key, &tgt_info);\n\t\tif (!tr) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else {\n\t\t \n\t\ttr = prog->aux->dst_trampoline;\n\t\ttgt_prog = prog->aux->dst_prog;\n\t}\n\n\terr = bpf_link_prime(&link->link.link, &link_primer);\n\tif (err)\n\t\tgoto out_unlock;\n\n\terr = bpf_trampoline_link_prog(&link->link, tr);\n\tif (err) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\tlink = NULL;\n\t\tgoto out_unlock;\n\t}\n\n\tlink->tgt_prog = tgt_prog;\n\tlink->trampoline = tr;\n\n\t \n\tif (prog->aux->dst_prog &&\n\t    (tgt_prog_fd || tr != prog->aux->dst_trampoline))\n\t\t \n\t\tbpf_prog_put(prog->aux->dst_prog);\n\tif (prog->aux->dst_trampoline && tr != prog->aux->dst_trampoline)\n\t\t \n\t\tbpf_trampoline_put(prog->aux->dst_trampoline);\n\n\tprog->aux->dst_prog = NULL;\n\tprog->aux->dst_trampoline = NULL;\n\tmutex_unlock(&prog->aux->dst_mutex);\n\n\treturn bpf_link_settle(&link_primer);\nout_unlock:\n\tif (tr && tr != prog->aux->dst_trampoline)\n\t\tbpf_trampoline_put(tr);\n\tmutex_unlock(&prog->aux->dst_mutex);\n\tkfree(link);\nout_put_prog:\n\tif (tgt_prog_fd && tgt_prog)\n\t\tbpf_prog_put(tgt_prog);\n\treturn err;\n}\n\nstruct bpf_raw_tp_link {\n\tstruct bpf_link link;\n\tstruct bpf_raw_event_map *btp;\n};\n\nstatic void bpf_raw_tp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_raw_tp_link *raw_tp =\n\t\tcontainer_of(link, struct bpf_raw_tp_link, link);\n\n\tbpf_probe_unregister(raw_tp->btp, raw_tp->link.prog);\n\tbpf_put_raw_tracepoint(raw_tp->btp);\n}\n\nstatic void bpf_raw_tp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_raw_tp_link *raw_tp =\n\t\tcontainer_of(link, struct bpf_raw_tp_link, link);\n\n\tkfree(raw_tp);\n}\n\nstatic void bpf_raw_tp_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t\tstruct seq_file *seq)\n{\n\tstruct bpf_raw_tp_link *raw_tp_link =\n\t\tcontainer_of(link, struct bpf_raw_tp_link, link);\n\n\tseq_printf(seq,\n\t\t   \"tp_name:\\t%s\\n\",\n\t\t   raw_tp_link->btp->tp->name);\n}\n\nstatic int bpf_copy_to_user(char __user *ubuf, const char *buf, u32 ulen,\n\t\t\t    u32 len)\n{\n\tif (ulen >= len + 1) {\n\t\tif (copy_to_user(ubuf, buf, len + 1))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tchar zero = '\\0';\n\n\t\tif (copy_to_user(ubuf, buf, ulen - 1))\n\t\t\treturn -EFAULT;\n\t\tif (put_user(zero, ubuf + ulen - 1))\n\t\t\treturn -EFAULT;\n\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nstatic int bpf_raw_tp_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t\t  struct bpf_link_info *info)\n{\n\tstruct bpf_raw_tp_link *raw_tp_link =\n\t\tcontainer_of(link, struct bpf_raw_tp_link, link);\n\tchar __user *ubuf = u64_to_user_ptr(info->raw_tracepoint.tp_name);\n\tconst char *tp_name = raw_tp_link->btp->tp->name;\n\tu32 ulen = info->raw_tracepoint.tp_name_len;\n\tsize_t tp_len = strlen(tp_name);\n\n\tif (!ulen ^ !ubuf)\n\t\treturn -EINVAL;\n\n\tinfo->raw_tracepoint.tp_name_len = tp_len + 1;\n\n\tif (!ubuf)\n\t\treturn 0;\n\n\treturn bpf_copy_to_user(ubuf, tp_name, ulen, tp_len);\n}\n\nstatic const struct bpf_link_ops bpf_raw_tp_link_lops = {\n\t.release = bpf_raw_tp_link_release,\n\t.dealloc = bpf_raw_tp_link_dealloc,\n\t.show_fdinfo = bpf_raw_tp_link_show_fdinfo,\n\t.fill_link_info = bpf_raw_tp_link_fill_link_info,\n};\n\n#ifdef CONFIG_PERF_EVENTS\nstruct bpf_perf_link {\n\tstruct bpf_link link;\n\tstruct file *perf_file;\n};\n\nstatic void bpf_perf_link_release(struct bpf_link *link)\n{\n\tstruct bpf_perf_link *perf_link = container_of(link, struct bpf_perf_link, link);\n\tstruct perf_event *event = perf_link->perf_file->private_data;\n\n\tperf_event_free_bpf_prog(event);\n\tfput(perf_link->perf_file);\n}\n\nstatic void bpf_perf_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_perf_link *perf_link = container_of(link, struct bpf_perf_link, link);\n\n\tkfree(perf_link);\n}\n\nstatic int bpf_perf_link_fill_common(const struct perf_event *event,\n\t\t\t\t     char __user *uname, u32 ulen,\n\t\t\t\t     u64 *probe_offset, u64 *probe_addr,\n\t\t\t\t     u32 *fd_type)\n{\n\tconst char *buf;\n\tu32 prog_id;\n\tsize_t len;\n\tint err;\n\n\tif (!ulen ^ !uname)\n\t\treturn -EINVAL;\n\n\terr = bpf_get_perf_event_info(event, &prog_id, fd_type, &buf,\n\t\t\t\t      probe_offset, probe_addr);\n\tif (err)\n\t\treturn err;\n\tif (!uname)\n\t\treturn 0;\n\tif (buf) {\n\t\tlen = strlen(buf);\n\t\terr = bpf_copy_to_user(uname, buf, ulen, len);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\tchar zero = '\\0';\n\n\t\tif (put_user(zero, uname))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_KPROBE_EVENTS\nstatic int bpf_perf_link_fill_kprobe(const struct perf_event *event,\n\t\t\t\t     struct bpf_link_info *info)\n{\n\tchar __user *uname;\n\tu64 addr, offset;\n\tu32 ulen, type;\n\tint err;\n\n\tuname = u64_to_user_ptr(info->perf_event.kprobe.func_name);\n\tulen = info->perf_event.kprobe.name_len;\n\terr = bpf_perf_link_fill_common(event, uname, ulen, &offset, &addr,\n\t\t\t\t\t&type);\n\tif (err)\n\t\treturn err;\n\tif (type == BPF_FD_TYPE_KRETPROBE)\n\t\tinfo->perf_event.type = BPF_PERF_EVENT_KRETPROBE;\n\telse\n\t\tinfo->perf_event.type = BPF_PERF_EVENT_KPROBE;\n\n\tinfo->perf_event.kprobe.offset = offset;\n\tif (!kallsyms_show_value(current_cred()))\n\t\taddr = 0;\n\tinfo->perf_event.kprobe.addr = addr;\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_UPROBE_EVENTS\nstatic int bpf_perf_link_fill_uprobe(const struct perf_event *event,\n\t\t\t\t     struct bpf_link_info *info)\n{\n\tchar __user *uname;\n\tu64 addr, offset;\n\tu32 ulen, type;\n\tint err;\n\n\tuname = u64_to_user_ptr(info->perf_event.uprobe.file_name);\n\tulen = info->perf_event.uprobe.name_len;\n\terr = bpf_perf_link_fill_common(event, uname, ulen, &offset, &addr,\n\t\t\t\t\t&type);\n\tif (err)\n\t\treturn err;\n\n\tif (type == BPF_FD_TYPE_URETPROBE)\n\t\tinfo->perf_event.type = BPF_PERF_EVENT_URETPROBE;\n\telse\n\t\tinfo->perf_event.type = BPF_PERF_EVENT_UPROBE;\n\tinfo->perf_event.uprobe.offset = offset;\n\treturn 0;\n}\n#endif\n\nstatic int bpf_perf_link_fill_probe(const struct perf_event *event,\n\t\t\t\t    struct bpf_link_info *info)\n{\n#ifdef CONFIG_KPROBE_EVENTS\n\tif (event->tp_event->flags & TRACE_EVENT_FL_KPROBE)\n\t\treturn bpf_perf_link_fill_kprobe(event, info);\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\tif (event->tp_event->flags & TRACE_EVENT_FL_UPROBE)\n\t\treturn bpf_perf_link_fill_uprobe(event, info);\n#endif\n\treturn -EOPNOTSUPP;\n}\n\nstatic int bpf_perf_link_fill_tracepoint(const struct perf_event *event,\n\t\t\t\t\t struct bpf_link_info *info)\n{\n\tchar __user *uname;\n\tu32 ulen;\n\n\tuname = u64_to_user_ptr(info->perf_event.tracepoint.tp_name);\n\tulen = info->perf_event.tracepoint.name_len;\n\tinfo->perf_event.type = BPF_PERF_EVENT_TRACEPOINT;\n\treturn bpf_perf_link_fill_common(event, uname, ulen, NULL, NULL, NULL);\n}\n\nstatic int bpf_perf_link_fill_perf_event(const struct perf_event *event,\n\t\t\t\t\t struct bpf_link_info *info)\n{\n\tinfo->perf_event.event.type = event->attr.type;\n\tinfo->perf_event.event.config = event->attr.config;\n\tinfo->perf_event.type = BPF_PERF_EVENT_EVENT;\n\treturn 0;\n}\n\nstatic int bpf_perf_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t\tstruct bpf_link_info *info)\n{\n\tstruct bpf_perf_link *perf_link;\n\tconst struct perf_event *event;\n\n\tperf_link = container_of(link, struct bpf_perf_link, link);\n\tevent = perf_get_event(perf_link->perf_file);\n\tif (IS_ERR(event))\n\t\treturn PTR_ERR(event);\n\n\tswitch (event->prog->type) {\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\t\treturn bpf_perf_link_fill_perf_event(event, info);\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\t\treturn bpf_perf_link_fill_tracepoint(event, info);\n\tcase BPF_PROG_TYPE_KPROBE:\n\t\treturn bpf_perf_link_fill_probe(event, info);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic const struct bpf_link_ops bpf_perf_link_lops = {\n\t.release = bpf_perf_link_release,\n\t.dealloc = bpf_perf_link_dealloc,\n\t.fill_link_info = bpf_perf_link_fill_link_info,\n};\n\nstatic int bpf_perf_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_perf_link *link;\n\tstruct perf_event *event;\n\tstruct file *perf_file;\n\tint err;\n\n\tif (attr->link_create.flags)\n\t\treturn -EINVAL;\n\n\tperf_file = perf_event_get(attr->link_create.target_fd);\n\tif (IS_ERR(perf_file))\n\t\treturn PTR_ERR(perf_file);\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_file;\n\t}\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_PERF_EVENT, &bpf_perf_link_lops, prog);\n\tlink->perf_file = perf_file;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto out_put_file;\n\t}\n\n\tevent = perf_file->private_data;\n\terr = perf_event_set_bpf_prog(event, prog, attr->link_create.perf_event.bpf_cookie);\n\tif (err) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\tgoto out_put_file;\n\t}\n\t \n\tbpf_prog_inc(prog);\n\n\treturn bpf_link_settle(&link_primer);\n\nout_put_file:\n\tfput(perf_file);\n\treturn err;\n}\n#else\nstatic int bpf_perf_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif  \n\nstatic int bpf_raw_tp_link_attach(struct bpf_prog *prog,\n\t\t\t\t  const char __user *user_tp_name)\n{\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_raw_tp_link *link;\n\tstruct bpf_raw_event_map *btp;\n\tconst char *tp_name;\n\tchar buf[128];\n\tint err;\n\n\tswitch (prog->type) {\n\tcase BPF_PROG_TYPE_TRACING:\n\tcase BPF_PROG_TYPE_EXT:\n\tcase BPF_PROG_TYPE_LSM:\n\t\tif (user_tp_name)\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\tif (prog->type == BPF_PROG_TYPE_TRACING &&\n\t\t    prog->expected_attach_type == BPF_TRACE_RAW_TP) {\n\t\t\ttp_name = prog->aux->attach_func_name;\n\t\t\tbreak;\n\t\t}\n\t\treturn bpf_tracing_prog_attach(prog, 0, 0, 0);\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE:\n\t\tif (strncpy_from_user(buf, user_tp_name, sizeof(buf) - 1) < 0)\n\t\t\treturn -EFAULT;\n\t\tbuf[sizeof(buf) - 1] = 0;\n\t\ttp_name = buf;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tbtp = bpf_get_raw_tracepoint(tp_name);\n\tif (!btp)\n\t\treturn -ENOENT;\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_btp;\n\t}\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_RAW_TRACEPOINT,\n\t\t      &bpf_raw_tp_link_lops, prog);\n\tlink->btp = btp;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto out_put_btp;\n\t}\n\n\terr = bpf_probe_register(link->btp, prog);\n\tif (err) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\tgoto out_put_btp;\n\t}\n\n\treturn bpf_link_settle(&link_primer);\n\nout_put_btp:\n\tbpf_put_raw_tracepoint(btp);\n\treturn err;\n}\n\n#define BPF_RAW_TRACEPOINT_OPEN_LAST_FIELD raw_tracepoint.prog_fd\n\nstatic int bpf_raw_tracepoint_open(const union bpf_attr *attr)\n{\n\tstruct bpf_prog *prog;\n\tint fd;\n\n\tif (CHECK_ATTR(BPF_RAW_TRACEPOINT_OPEN))\n\t\treturn -EINVAL;\n\n\tprog = bpf_prog_get(attr->raw_tracepoint.prog_fd);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tfd = bpf_raw_tp_link_attach(prog, u64_to_user_ptr(attr->raw_tracepoint.name));\n\tif (fd < 0)\n\t\tbpf_prog_put(prog);\n\treturn fd;\n}\n\nstatic enum bpf_prog_type\nattach_type_to_prog_type(enum bpf_attach_type attach_type)\n{\n\tswitch (attach_type) {\n\tcase BPF_CGROUP_INET_INGRESS:\n\tcase BPF_CGROUP_INET_EGRESS:\n\t\treturn BPF_PROG_TYPE_CGROUP_SKB;\n\tcase BPF_CGROUP_INET_SOCK_CREATE:\n\tcase BPF_CGROUP_INET_SOCK_RELEASE:\n\tcase BPF_CGROUP_INET4_POST_BIND:\n\tcase BPF_CGROUP_INET6_POST_BIND:\n\t\treturn BPF_PROG_TYPE_CGROUP_SOCK;\n\tcase BPF_CGROUP_INET4_BIND:\n\tcase BPF_CGROUP_INET6_BIND:\n\tcase BPF_CGROUP_INET4_CONNECT:\n\tcase BPF_CGROUP_INET6_CONNECT:\n\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\tcase BPF_CGROUP_UDP4_SENDMSG:\n\tcase BPF_CGROUP_UDP6_SENDMSG:\n\tcase BPF_CGROUP_UDP4_RECVMSG:\n\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\treturn BPF_PROG_TYPE_CGROUP_SOCK_ADDR;\n\tcase BPF_CGROUP_SOCK_OPS:\n\t\treturn BPF_PROG_TYPE_SOCK_OPS;\n\tcase BPF_CGROUP_DEVICE:\n\t\treturn BPF_PROG_TYPE_CGROUP_DEVICE;\n\tcase BPF_SK_MSG_VERDICT:\n\t\treturn BPF_PROG_TYPE_SK_MSG;\n\tcase BPF_SK_SKB_STREAM_PARSER:\n\tcase BPF_SK_SKB_STREAM_VERDICT:\n\tcase BPF_SK_SKB_VERDICT:\n\t\treturn BPF_PROG_TYPE_SK_SKB;\n\tcase BPF_LIRC_MODE2:\n\t\treturn BPF_PROG_TYPE_LIRC_MODE2;\n\tcase BPF_FLOW_DISSECTOR:\n\t\treturn BPF_PROG_TYPE_FLOW_DISSECTOR;\n\tcase BPF_CGROUP_SYSCTL:\n\t\treturn BPF_PROG_TYPE_CGROUP_SYSCTL;\n\tcase BPF_CGROUP_GETSOCKOPT:\n\tcase BPF_CGROUP_SETSOCKOPT:\n\t\treturn BPF_PROG_TYPE_CGROUP_SOCKOPT;\n\tcase BPF_TRACE_ITER:\n\tcase BPF_TRACE_RAW_TP:\n\tcase BPF_TRACE_FENTRY:\n\tcase BPF_TRACE_FEXIT:\n\tcase BPF_MODIFY_RETURN:\n\t\treturn BPF_PROG_TYPE_TRACING;\n\tcase BPF_LSM_MAC:\n\t\treturn BPF_PROG_TYPE_LSM;\n\tcase BPF_SK_LOOKUP:\n\t\treturn BPF_PROG_TYPE_SK_LOOKUP;\n\tcase BPF_XDP:\n\t\treturn BPF_PROG_TYPE_XDP;\n\tcase BPF_LSM_CGROUP:\n\t\treturn BPF_PROG_TYPE_LSM;\n\tcase BPF_TCX_INGRESS:\n\tcase BPF_TCX_EGRESS:\n\t\treturn BPF_PROG_TYPE_SCHED_CLS;\n\tdefault:\n\t\treturn BPF_PROG_TYPE_UNSPEC;\n\t}\n}\n\nstatic int bpf_prog_attach_check_attach_type(const struct bpf_prog *prog,\n\t\t\t\t\t     enum bpf_attach_type attach_type)\n{\n\tenum bpf_prog_type ptype;\n\n\tswitch (prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\treturn attach_type == prog->expected_attach_type ? 0 : -EINVAL;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\t \n\t\t\treturn -EPERM;\n\t\treturn prog->enforce_expected_attach_type &&\n\t\t\tprog->expected_attach_type != attach_type ?\n\t\t\t-EINVAL : 0;\n\tcase BPF_PROG_TYPE_EXT:\n\t\treturn 0;\n\tcase BPF_PROG_TYPE_NETFILTER:\n\t\tif (attach_type != BPF_NETFILTER)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\t\tif (attach_type != BPF_PERF_EVENT)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tcase BPF_PROG_TYPE_KPROBE:\n\t\tif (prog->expected_attach_type == BPF_TRACE_KPROBE_MULTI &&\n\t\t    attach_type != BPF_TRACE_KPROBE_MULTI)\n\t\t\treturn -EINVAL;\n\t\tif (prog->expected_attach_type == BPF_TRACE_UPROBE_MULTI &&\n\t\t    attach_type != BPF_TRACE_UPROBE_MULTI)\n\t\t\treturn -EINVAL;\n\t\tif (attach_type != BPF_PERF_EVENT &&\n\t\t    attach_type != BPF_TRACE_KPROBE_MULTI &&\n\t\t    attach_type != BPF_TRACE_UPROBE_MULTI)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\t\tif (attach_type != BPF_TCX_INGRESS &&\n\t\t    attach_type != BPF_TCX_EGRESS)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\tdefault:\n\t\tptype = attach_type_to_prog_type(attach_type);\n\t\tif (ptype == BPF_PROG_TYPE_UNSPEC || ptype != prog->type)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n}\n\n#define BPF_PROG_ATTACH_LAST_FIELD expected_revision\n\n#define BPF_F_ATTACH_MASK_BASE\t\\\n\t(BPF_F_ALLOW_OVERRIDE |\t\\\n\t BPF_F_ALLOW_MULTI |\t\\\n\t BPF_F_REPLACE)\n\n#define BPF_F_ATTACH_MASK_MPROG\t\\\n\t(BPF_F_REPLACE |\t\\\n\t BPF_F_BEFORE |\t\t\\\n\t BPF_F_AFTER |\t\t\\\n\t BPF_F_ID |\t\t\\\n\t BPF_F_LINK)\n\nstatic int bpf_prog_attach(const union bpf_attr *attr)\n{\n\tenum bpf_prog_type ptype;\n\tstruct bpf_prog *prog;\n\tint ret;\n\n\tif (CHECK_ATTR(BPF_PROG_ATTACH))\n\t\treturn -EINVAL;\n\n\tptype = attach_type_to_prog_type(attr->attach_type);\n\tif (ptype == BPF_PROG_TYPE_UNSPEC)\n\t\treturn -EINVAL;\n\tif (bpf_mprog_supported(ptype)) {\n\t\tif (attr->attach_flags & ~BPF_F_ATTACH_MASK_MPROG)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (attr->attach_flags & ~BPF_F_ATTACH_MASK_BASE)\n\t\t\treturn -EINVAL;\n\t\tif (attr->relative_fd ||\n\t\t    attr->expected_revision)\n\t\t\treturn -EINVAL;\n\t}\n\n\tprog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tif (bpf_prog_attach_check_attach_type(prog, attr->attach_type)) {\n\t\tbpf_prog_put(prog);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (ptype) {\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\t\tret = sock_map_get_from_fd(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_LIRC_MODE2:\n\t\tret = lirc_prog_attach(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\t\tret = netns_bpf_prog_attach(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_LSM:\n\t\tif (ptype == BPF_PROG_TYPE_LSM &&\n\t\t    prog->expected_attach_type != BPF_LSM_CGROUP)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tret = cgroup_bpf_prog_attach(attr, ptype, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\t\tret = tcx_prog_attach(attr, prog);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tif (ret)\n\t\tbpf_prog_put(prog);\n\treturn ret;\n}\n\n#define BPF_PROG_DETACH_LAST_FIELD expected_revision\n\nstatic int bpf_prog_detach(const union bpf_attr *attr)\n{\n\tstruct bpf_prog *prog = NULL;\n\tenum bpf_prog_type ptype;\n\tint ret;\n\n\tif (CHECK_ATTR(BPF_PROG_DETACH))\n\t\treturn -EINVAL;\n\n\tptype = attach_type_to_prog_type(attr->attach_type);\n\tif (bpf_mprog_supported(ptype)) {\n\t\tif (ptype == BPF_PROG_TYPE_UNSPEC)\n\t\t\treturn -EINVAL;\n\t\tif (attr->attach_flags & ~BPF_F_ATTACH_MASK_MPROG)\n\t\t\treturn -EINVAL;\n\t\tif (attr->attach_bpf_fd) {\n\t\t\tprog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);\n\t\t\tif (IS_ERR(prog))\n\t\t\t\treturn PTR_ERR(prog);\n\t\t}\n\t} else if (attr->attach_flags ||\n\t\t   attr->relative_fd ||\n\t\t   attr->expected_revision) {\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (ptype) {\n\tcase BPF_PROG_TYPE_SK_MSG:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\t\tret = sock_map_prog_detach(attr, ptype);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_LIRC_MODE2:\n\t\tret = lirc_prog_detach(attr);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\t\tret = netns_bpf_prog_detach(attr, ptype);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_LSM:\n\t\tret = cgroup_bpf_prog_detach(attr, ptype);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\t\tret = tcx_prog_detach(attr, prog);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\tif (prog)\n\t\tbpf_prog_put(prog);\n\treturn ret;\n}\n\n#define BPF_PROG_QUERY_LAST_FIELD query.revision\n\nstatic int bpf_prog_query(const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\tif (CHECK_ATTR(BPF_PROG_QUERY))\n\t\treturn -EINVAL;\n\tif (attr->query.query_flags & ~BPF_F_QUERY_EFFECTIVE)\n\t\treturn -EINVAL;\n\n\tswitch (attr->query.attach_type) {\n\tcase BPF_CGROUP_INET_INGRESS:\n\tcase BPF_CGROUP_INET_EGRESS:\n\tcase BPF_CGROUP_INET_SOCK_CREATE:\n\tcase BPF_CGROUP_INET_SOCK_RELEASE:\n\tcase BPF_CGROUP_INET4_BIND:\n\tcase BPF_CGROUP_INET6_BIND:\n\tcase BPF_CGROUP_INET4_POST_BIND:\n\tcase BPF_CGROUP_INET6_POST_BIND:\n\tcase BPF_CGROUP_INET4_CONNECT:\n\tcase BPF_CGROUP_INET6_CONNECT:\n\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\tcase BPF_CGROUP_UDP4_SENDMSG:\n\tcase BPF_CGROUP_UDP6_SENDMSG:\n\tcase BPF_CGROUP_UDP4_RECVMSG:\n\tcase BPF_CGROUP_UDP6_RECVMSG:\n\tcase BPF_CGROUP_SOCK_OPS:\n\tcase BPF_CGROUP_DEVICE:\n\tcase BPF_CGROUP_SYSCTL:\n\tcase BPF_CGROUP_GETSOCKOPT:\n\tcase BPF_CGROUP_SETSOCKOPT:\n\tcase BPF_LSM_CGROUP:\n\t\treturn cgroup_bpf_prog_query(attr, uattr);\n\tcase BPF_LIRC_MODE2:\n\t\treturn lirc_prog_query(attr, uattr);\n\tcase BPF_FLOW_DISSECTOR:\n\tcase BPF_SK_LOOKUP:\n\t\treturn netns_bpf_prog_query(attr, uattr);\n\tcase BPF_SK_SKB_STREAM_PARSER:\n\tcase BPF_SK_SKB_STREAM_VERDICT:\n\tcase BPF_SK_MSG_VERDICT:\n\tcase BPF_SK_SKB_VERDICT:\n\t\treturn sock_map_bpf_prog_query(attr, uattr);\n\tcase BPF_TCX_INGRESS:\n\tcase BPF_TCX_EGRESS:\n\t\treturn tcx_prog_query(attr, uattr);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n#define BPF_PROG_TEST_RUN_LAST_FIELD test.batch_size\n\nstatic int bpf_prog_test_run(const union bpf_attr *attr,\n\t\t\t     union bpf_attr __user *uattr)\n{\n\tstruct bpf_prog *prog;\n\tint ret = -ENOTSUPP;\n\n\tif (CHECK_ATTR(BPF_PROG_TEST_RUN))\n\t\treturn -EINVAL;\n\n\tif ((attr->test.ctx_size_in && !attr->test.ctx_in) ||\n\t    (!attr->test.ctx_size_in && attr->test.ctx_in))\n\t\treturn -EINVAL;\n\n\tif ((attr->test.ctx_size_out && !attr->test.ctx_out) ||\n\t    (!attr->test.ctx_size_out && attr->test.ctx_out))\n\t\treturn -EINVAL;\n\n\tprog = bpf_prog_get(attr->test.prog_fd);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tif (prog->aux->ops->test_run)\n\t\tret = prog->aux->ops->test_run(prog, attr, uattr);\n\n\tbpf_prog_put(prog);\n\treturn ret;\n}\n\n#define BPF_OBJ_GET_NEXT_ID_LAST_FIELD next_id\n\nstatic int bpf_obj_get_next_id(const union bpf_attr *attr,\n\t\t\t       union bpf_attr __user *uattr,\n\t\t\t       struct idr *idr,\n\t\t\t       spinlock_t *lock)\n{\n\tu32 next_id = attr->start_id;\n\tint err = 0;\n\n\tif (CHECK_ATTR(BPF_OBJ_GET_NEXT_ID) || next_id >= INT_MAX)\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tnext_id++;\n\tspin_lock_bh(lock);\n\tif (!idr_get_next(idr, &next_id))\n\t\terr = -ENOENT;\n\tspin_unlock_bh(lock);\n\n\tif (!err)\n\t\terr = put_user(next_id, &uattr->next_id);\n\n\treturn err;\n}\n\nstruct bpf_map *bpf_map_get_curr_or_next(u32 *id)\n{\n\tstruct bpf_map *map;\n\n\tspin_lock_bh(&map_idr_lock);\nagain:\n\tmap = idr_get_next(&map_idr, id);\n\tif (map) {\n\t\tmap = __bpf_map_inc_not_zero(map, false);\n\t\tif (IS_ERR(map)) {\n\t\t\t(*id)++;\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&map_idr_lock);\n\n\treturn map;\n}\n\nstruct bpf_prog *bpf_prog_get_curr_or_next(u32 *id)\n{\n\tstruct bpf_prog *prog;\n\n\tspin_lock_bh(&prog_idr_lock);\nagain:\n\tprog = idr_get_next(&prog_idr, id);\n\tif (prog) {\n\t\tprog = bpf_prog_inc_not_zero(prog);\n\t\tif (IS_ERR(prog)) {\n\t\t\t(*id)++;\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&prog_idr_lock);\n\n\treturn prog;\n}\n\n#define BPF_PROG_GET_FD_BY_ID_LAST_FIELD prog_id\n\nstruct bpf_prog *bpf_prog_by_id(u32 id)\n{\n\tstruct bpf_prog *prog;\n\n\tif (!id)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tspin_lock_bh(&prog_idr_lock);\n\tprog = idr_find(&prog_idr, id);\n\tif (prog)\n\t\tprog = bpf_prog_inc_not_zero(prog);\n\telse\n\t\tprog = ERR_PTR(-ENOENT);\n\tspin_unlock_bh(&prog_idr_lock);\n\treturn prog;\n}\n\nstatic int bpf_prog_get_fd_by_id(const union bpf_attr *attr)\n{\n\tstruct bpf_prog *prog;\n\tu32 id = attr->prog_id;\n\tint fd;\n\n\tif (CHECK_ATTR(BPF_PROG_GET_FD_BY_ID))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tprog = bpf_prog_by_id(id);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tfd = bpf_prog_new_fd(prog);\n\tif (fd < 0)\n\t\tbpf_prog_put(prog);\n\n\treturn fd;\n}\n\n#define BPF_MAP_GET_FD_BY_ID_LAST_FIELD open_flags\n\nstatic int bpf_map_get_fd_by_id(const union bpf_attr *attr)\n{\n\tstruct bpf_map *map;\n\tu32 id = attr->map_id;\n\tint f_flags;\n\tint fd;\n\n\tif (CHECK_ATTR(BPF_MAP_GET_FD_BY_ID) ||\n\t    attr->open_flags & ~BPF_OBJ_FLAG_MASK)\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tf_flags = bpf_get_file_flag(attr->open_flags);\n\tif (f_flags < 0)\n\t\treturn f_flags;\n\n\tspin_lock_bh(&map_idr_lock);\n\tmap = idr_find(&map_idr, id);\n\tif (map)\n\t\tmap = __bpf_map_inc_not_zero(map, true);\n\telse\n\t\tmap = ERR_PTR(-ENOENT);\n\tspin_unlock_bh(&map_idr_lock);\n\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\n\tfd = bpf_map_new_fd(map, f_flags);\n\tif (fd < 0)\n\t\tbpf_map_put_with_uref(map);\n\n\treturn fd;\n}\n\nstatic const struct bpf_map *bpf_map_from_imm(const struct bpf_prog *prog,\n\t\t\t\t\t      unsigned long addr, u32 *off,\n\t\t\t\t\t      u32 *type)\n{\n\tconst struct bpf_map *map;\n\tint i;\n\n\tmutex_lock(&prog->aux->used_maps_mutex);\n\tfor (i = 0, *off = 0; i < prog->aux->used_map_cnt; i++) {\n\t\tmap = prog->aux->used_maps[i];\n\t\tif (map == (void *)addr) {\n\t\t\t*type = BPF_PSEUDO_MAP_FD;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!map->ops->map_direct_value_meta)\n\t\t\tcontinue;\n\t\tif (!map->ops->map_direct_value_meta(map, addr, off)) {\n\t\t\t*type = BPF_PSEUDO_MAP_VALUE;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tmap = NULL;\n\nout:\n\tmutex_unlock(&prog->aux->used_maps_mutex);\n\treturn map;\n}\n\nstatic struct bpf_insn *bpf_insn_prepare_dump(const struct bpf_prog *prog,\n\t\t\t\t\t      const struct cred *f_cred)\n{\n\tconst struct bpf_map *map;\n\tstruct bpf_insn *insns;\n\tu32 off, type;\n\tu64 imm;\n\tu8 code;\n\tint i;\n\n\tinsns = kmemdup(prog->insnsi, bpf_prog_insn_size(prog),\n\t\t\tGFP_USER);\n\tif (!insns)\n\t\treturn insns;\n\n\tfor (i = 0; i < prog->len; i++) {\n\t\tcode = insns[i].code;\n\n\t\tif (code == (BPF_JMP | BPF_TAIL_CALL)) {\n\t\t\tinsns[i].code = BPF_JMP | BPF_CALL;\n\t\t\tinsns[i].imm = BPF_FUNC_tail_call;\n\t\t\t \n\t\t}\n\t\tif (code == (BPF_JMP | BPF_CALL) ||\n\t\t    code == (BPF_JMP | BPF_CALL_ARGS)) {\n\t\t\tif (code == (BPF_JMP | BPF_CALL_ARGS))\n\t\t\t\tinsns[i].code = BPF_JMP | BPF_CALL;\n\t\t\tif (!bpf_dump_raw_ok(f_cred))\n\t\t\t\tinsns[i].imm = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (BPF_CLASS(code) == BPF_LDX && BPF_MODE(code) == BPF_PROBE_MEM) {\n\t\t\tinsns[i].code = BPF_LDX | BPF_SIZE(code) | BPF_MEM;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (code != (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tcontinue;\n\n\t\timm = ((u64)insns[i + 1].imm << 32) | (u32)insns[i].imm;\n\t\tmap = bpf_map_from_imm(prog, imm, &off, &type);\n\t\tif (map) {\n\t\t\tinsns[i].src_reg = type;\n\t\t\tinsns[i].imm = map->id;\n\t\t\tinsns[i + 1].imm = off;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\treturn insns;\n}\n\nstatic int set_info_rec_size(struct bpf_prog_info *info)\n{\n\t \n\n\tif ((info->nr_func_info || info->func_info_rec_size) &&\n\t    info->func_info_rec_size != sizeof(struct bpf_func_info))\n\t\treturn -EINVAL;\n\n\tif ((info->nr_line_info || info->line_info_rec_size) &&\n\t    info->line_info_rec_size != sizeof(struct bpf_line_info))\n\t\treturn -EINVAL;\n\n\tif ((info->nr_jited_line_info || info->jited_line_info_rec_size) &&\n\t    info->jited_line_info_rec_size != sizeof(__u64))\n\t\treturn -EINVAL;\n\n\tinfo->func_info_rec_size = sizeof(struct bpf_func_info);\n\tinfo->line_info_rec_size = sizeof(struct bpf_line_info);\n\tinfo->jited_line_info_rec_size = sizeof(__u64);\n\n\treturn 0;\n}\n\nstatic int bpf_prog_get_info_by_fd(struct file *file,\n\t\t\t\t   struct bpf_prog *prog,\n\t\t\t\t   const union bpf_attr *attr,\n\t\t\t\t   union bpf_attr __user *uattr)\n{\n\tstruct bpf_prog_info __user *uinfo = u64_to_user_ptr(attr->info.info);\n\tstruct btf *attach_btf = bpf_prog_get_target_btf(prog);\n\tstruct bpf_prog_info info;\n\tu32 info_len = attr->info.info_len;\n\tstruct bpf_prog_kstats stats;\n\tchar __user *uinsns;\n\tu32 ulen;\n\tint err;\n\n\terr = bpf_check_uarg_tail_zero(USER_BPFPTR(uinfo), sizeof(info), info_len);\n\tif (err)\n\t\treturn err;\n\tinfo_len = min_t(u32, sizeof(info), info_len);\n\n\tmemset(&info, 0, sizeof(info));\n\tif (copy_from_user(&info, uinfo, info_len))\n\t\treturn -EFAULT;\n\n\tinfo.type = prog->type;\n\tinfo.id = prog->aux->id;\n\tinfo.load_time = prog->aux->load_time;\n\tinfo.created_by_uid = from_kuid_munged(current_user_ns(),\n\t\t\t\t\t       prog->aux->user->uid);\n\tinfo.gpl_compatible = prog->gpl_compatible;\n\n\tmemcpy(info.tag, prog->tag, sizeof(prog->tag));\n\tmemcpy(info.name, prog->aux->name, sizeof(prog->aux->name));\n\n\tmutex_lock(&prog->aux->used_maps_mutex);\n\tulen = info.nr_map_ids;\n\tinfo.nr_map_ids = prog->aux->used_map_cnt;\n\tulen = min_t(u32, info.nr_map_ids, ulen);\n\tif (ulen) {\n\t\tu32 __user *user_map_ids = u64_to_user_ptr(info.map_ids);\n\t\tu32 i;\n\n\t\tfor (i = 0; i < ulen; i++)\n\t\t\tif (put_user(prog->aux->used_maps[i]->id,\n\t\t\t\t     &user_map_ids[i])) {\n\t\t\t\tmutex_unlock(&prog->aux->used_maps_mutex);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t}\n\tmutex_unlock(&prog->aux->used_maps_mutex);\n\n\terr = set_info_rec_size(&info);\n\tif (err)\n\t\treturn err;\n\n\tbpf_prog_get_stats(prog, &stats);\n\tinfo.run_time_ns = stats.nsecs;\n\tinfo.run_cnt = stats.cnt;\n\tinfo.recursion_misses = stats.misses;\n\n\tinfo.verified_insns = prog->aux->verified_insns;\n\n\tif (!bpf_capable()) {\n\t\tinfo.jited_prog_len = 0;\n\t\tinfo.xlated_prog_len = 0;\n\t\tinfo.nr_jited_ksyms = 0;\n\t\tinfo.nr_jited_func_lens = 0;\n\t\tinfo.nr_func_info = 0;\n\t\tinfo.nr_line_info = 0;\n\t\tinfo.nr_jited_line_info = 0;\n\t\tgoto done;\n\t}\n\n\tulen = info.xlated_prog_len;\n\tinfo.xlated_prog_len = bpf_prog_insn_size(prog);\n\tif (info.xlated_prog_len && ulen) {\n\t\tstruct bpf_insn *insns_sanitized;\n\t\tbool fault;\n\n\t\tif (prog->blinded && !bpf_dump_raw_ok(file->f_cred)) {\n\t\t\tinfo.xlated_prog_insns = 0;\n\t\t\tgoto done;\n\t\t}\n\t\tinsns_sanitized = bpf_insn_prepare_dump(prog, file->f_cred);\n\t\tif (!insns_sanitized)\n\t\t\treturn -ENOMEM;\n\t\tuinsns = u64_to_user_ptr(info.xlated_prog_insns);\n\t\tulen = min_t(u32, info.xlated_prog_len, ulen);\n\t\tfault = copy_to_user(uinsns, insns_sanitized, ulen);\n\t\tkfree(insns_sanitized);\n\t\tif (fault)\n\t\t\treturn -EFAULT;\n\t}\n\n\tif (bpf_prog_is_offloaded(prog->aux)) {\n\t\terr = bpf_prog_offload_info_fill(&info, prog);\n\t\tif (err)\n\t\t\treturn err;\n\t\tgoto done;\n\t}\n\n\t \n\tulen = info.jited_prog_len;\n\tif (prog->aux->func_cnt) {\n\t\tu32 i;\n\n\t\tinfo.jited_prog_len = 0;\n\t\tfor (i = 0; i < prog->aux->func_cnt; i++)\n\t\t\tinfo.jited_prog_len += prog->aux->func[i]->jited_len;\n\t} else {\n\t\tinfo.jited_prog_len = prog->jited_len;\n\t}\n\n\tif (info.jited_prog_len && ulen) {\n\t\tif (bpf_dump_raw_ok(file->f_cred)) {\n\t\t\tuinsns = u64_to_user_ptr(info.jited_prog_insns);\n\t\t\tulen = min_t(u32, info.jited_prog_len, ulen);\n\n\t\t\t \n\t\t\tif (prog->aux->func_cnt) {\n\t\t\t\tu32 len, free, i;\n\t\t\t\tu8 *img;\n\n\t\t\t\tfree = ulen;\n\t\t\t\tfor (i = 0; i < prog->aux->func_cnt; i++) {\n\t\t\t\t\tlen = prog->aux->func[i]->jited_len;\n\t\t\t\t\tlen = min_t(u32, len, free);\n\t\t\t\t\timg = (u8 *) prog->aux->func[i]->bpf_func;\n\t\t\t\t\tif (copy_to_user(uinsns, img, len))\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\t\tuinsns += len;\n\t\t\t\t\tfree -= len;\n\t\t\t\t\tif (!free)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (copy_to_user(uinsns, prog->bpf_func, ulen))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tinfo.jited_prog_insns = 0;\n\t\t}\n\t}\n\n\tulen = info.nr_jited_ksyms;\n\tinfo.nr_jited_ksyms = prog->aux->func_cnt ? : 1;\n\tif (ulen) {\n\t\tif (bpf_dump_raw_ok(file->f_cred)) {\n\t\t\tunsigned long ksym_addr;\n\t\t\tu64 __user *user_ksyms;\n\t\t\tu32 i;\n\n\t\t\t \n\t\t\tulen = min_t(u32, info.nr_jited_ksyms, ulen);\n\t\t\tuser_ksyms = u64_to_user_ptr(info.jited_ksyms);\n\t\t\tif (prog->aux->func_cnt) {\n\t\t\t\tfor (i = 0; i < ulen; i++) {\n\t\t\t\t\tksym_addr = (unsigned long)\n\t\t\t\t\t\tprog->aux->func[i]->bpf_func;\n\t\t\t\t\tif (put_user((u64) ksym_addr,\n\t\t\t\t\t\t     &user_ksyms[i]))\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tksym_addr = (unsigned long) prog->bpf_func;\n\t\t\t\tif (put_user((u64) ksym_addr, &user_ksyms[0]))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tinfo.jited_ksyms = 0;\n\t\t}\n\t}\n\n\tulen = info.nr_jited_func_lens;\n\tinfo.nr_jited_func_lens = prog->aux->func_cnt ? : 1;\n\tif (ulen) {\n\t\tif (bpf_dump_raw_ok(file->f_cred)) {\n\t\t\tu32 __user *user_lens;\n\t\t\tu32 func_len, i;\n\n\t\t\t \n\t\t\tulen = min_t(u32, info.nr_jited_func_lens, ulen);\n\t\t\tuser_lens = u64_to_user_ptr(info.jited_func_lens);\n\t\t\tif (prog->aux->func_cnt) {\n\t\t\t\tfor (i = 0; i < ulen; i++) {\n\t\t\t\t\tfunc_len =\n\t\t\t\t\t\tprog->aux->func[i]->jited_len;\n\t\t\t\t\tif (put_user(func_len, &user_lens[i]))\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfunc_len = prog->jited_len;\n\t\t\t\tif (put_user(func_len, &user_lens[0]))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tinfo.jited_func_lens = 0;\n\t\t}\n\t}\n\n\tif (prog->aux->btf)\n\t\tinfo.btf_id = btf_obj_id(prog->aux->btf);\n\tinfo.attach_btf_id = prog->aux->attach_btf_id;\n\tif (attach_btf)\n\t\tinfo.attach_btf_obj_id = btf_obj_id(attach_btf);\n\n\tulen = info.nr_func_info;\n\tinfo.nr_func_info = prog->aux->func_info_cnt;\n\tif (info.nr_func_info && ulen) {\n\t\tchar __user *user_finfo;\n\n\t\tuser_finfo = u64_to_user_ptr(info.func_info);\n\t\tulen = min_t(u32, info.nr_func_info, ulen);\n\t\tif (copy_to_user(user_finfo, prog->aux->func_info,\n\t\t\t\t info.func_info_rec_size * ulen))\n\t\t\treturn -EFAULT;\n\t}\n\n\tulen = info.nr_line_info;\n\tinfo.nr_line_info = prog->aux->nr_linfo;\n\tif (info.nr_line_info && ulen) {\n\t\t__u8 __user *user_linfo;\n\n\t\tuser_linfo = u64_to_user_ptr(info.line_info);\n\t\tulen = min_t(u32, info.nr_line_info, ulen);\n\t\tif (copy_to_user(user_linfo, prog->aux->linfo,\n\t\t\t\t info.line_info_rec_size * ulen))\n\t\t\treturn -EFAULT;\n\t}\n\n\tulen = info.nr_jited_line_info;\n\tif (prog->aux->jited_linfo)\n\t\tinfo.nr_jited_line_info = prog->aux->nr_linfo;\n\telse\n\t\tinfo.nr_jited_line_info = 0;\n\tif (info.nr_jited_line_info && ulen) {\n\t\tif (bpf_dump_raw_ok(file->f_cred)) {\n\t\t\tunsigned long line_addr;\n\t\t\t__u64 __user *user_linfo;\n\t\t\tu32 i;\n\n\t\t\tuser_linfo = u64_to_user_ptr(info.jited_line_info);\n\t\t\tulen = min_t(u32, info.nr_jited_line_info, ulen);\n\t\t\tfor (i = 0; i < ulen; i++) {\n\t\t\t\tline_addr = (unsigned long)prog->aux->jited_linfo[i];\n\t\t\t\tif (put_user((__u64)line_addr, &user_linfo[i]))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tinfo.jited_line_info = 0;\n\t\t}\n\t}\n\n\tulen = info.nr_prog_tags;\n\tinfo.nr_prog_tags = prog->aux->func_cnt ? : 1;\n\tif (ulen) {\n\t\t__u8 __user (*user_prog_tags)[BPF_TAG_SIZE];\n\t\tu32 i;\n\n\t\tuser_prog_tags = u64_to_user_ptr(info.prog_tags);\n\t\tulen = min_t(u32, info.nr_prog_tags, ulen);\n\t\tif (prog->aux->func_cnt) {\n\t\t\tfor (i = 0; i < ulen; i++) {\n\t\t\t\tif (copy_to_user(user_prog_tags[i],\n\t\t\t\t\t\t prog->aux->func[i]->tag,\n\t\t\t\t\t\t BPF_TAG_SIZE))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else {\n\t\t\tif (copy_to_user(user_prog_tags[0],\n\t\t\t\t\t prog->tag, BPF_TAG_SIZE))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\ndone:\n\tif (copy_to_user(uinfo, &info, info_len) ||\n\t    put_user(info_len, &uattr->info.info_len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int bpf_map_get_info_by_fd(struct file *file,\n\t\t\t\t  struct bpf_map *map,\n\t\t\t\t  const union bpf_attr *attr,\n\t\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct bpf_map_info __user *uinfo = u64_to_user_ptr(attr->info.info);\n\tstruct bpf_map_info info;\n\tu32 info_len = attr->info.info_len;\n\tint err;\n\n\terr = bpf_check_uarg_tail_zero(USER_BPFPTR(uinfo), sizeof(info), info_len);\n\tif (err)\n\t\treturn err;\n\tinfo_len = min_t(u32, sizeof(info), info_len);\n\n\tmemset(&info, 0, sizeof(info));\n\tinfo.type = map->map_type;\n\tinfo.id = map->id;\n\tinfo.key_size = map->key_size;\n\tinfo.value_size = map->value_size;\n\tinfo.max_entries = map->max_entries;\n\tinfo.map_flags = map->map_flags;\n\tinfo.map_extra = map->map_extra;\n\tmemcpy(info.name, map->name, sizeof(map->name));\n\n\tif (map->btf) {\n\t\tinfo.btf_id = btf_obj_id(map->btf);\n\t\tinfo.btf_key_type_id = map->btf_key_type_id;\n\t\tinfo.btf_value_type_id = map->btf_value_type_id;\n\t}\n\tinfo.btf_vmlinux_value_type_id = map->btf_vmlinux_value_type_id;\n\n\tif (bpf_map_is_offloaded(map)) {\n\t\terr = bpf_map_offload_info_fill(&info, map);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (copy_to_user(uinfo, &info, info_len) ||\n\t    put_user(info_len, &uattr->info.info_len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int bpf_btf_get_info_by_fd(struct file *file,\n\t\t\t\t  struct btf *btf,\n\t\t\t\t  const union bpf_attr *attr,\n\t\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct bpf_btf_info __user *uinfo = u64_to_user_ptr(attr->info.info);\n\tu32 info_len = attr->info.info_len;\n\tint err;\n\n\terr = bpf_check_uarg_tail_zero(USER_BPFPTR(uinfo), sizeof(*uinfo), info_len);\n\tif (err)\n\t\treturn err;\n\n\treturn btf_get_info_by_fd(btf, attr, uattr);\n}\n\nstatic int bpf_link_get_info_by_fd(struct file *file,\n\t\t\t\t  struct bpf_link *link,\n\t\t\t\t  const union bpf_attr *attr,\n\t\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct bpf_link_info __user *uinfo = u64_to_user_ptr(attr->info.info);\n\tstruct bpf_link_info info;\n\tu32 info_len = attr->info.info_len;\n\tint err;\n\n\terr = bpf_check_uarg_tail_zero(USER_BPFPTR(uinfo), sizeof(info), info_len);\n\tif (err)\n\t\treturn err;\n\tinfo_len = min_t(u32, sizeof(info), info_len);\n\n\tmemset(&info, 0, sizeof(info));\n\tif (copy_from_user(&info, uinfo, info_len))\n\t\treturn -EFAULT;\n\n\tinfo.type = link->type;\n\tinfo.id = link->id;\n\tif (link->prog)\n\t\tinfo.prog_id = link->prog->aux->id;\n\n\tif (link->ops->fill_link_info) {\n\t\terr = link->ops->fill_link_info(link, &info);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (copy_to_user(uinfo, &info, info_len) ||\n\t    put_user(info_len, &uattr->info.info_len))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n\n#define BPF_OBJ_GET_INFO_BY_FD_LAST_FIELD info.info\n\nstatic int bpf_obj_get_info_by_fd(const union bpf_attr *attr,\n\t\t\t\t  union bpf_attr __user *uattr)\n{\n\tint ufd = attr->info.bpf_fd;\n\tstruct fd f;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_OBJ_GET_INFO_BY_FD))\n\t\treturn -EINVAL;\n\n\tf = fdget(ufd);\n\tif (!f.file)\n\t\treturn -EBADFD;\n\n\tif (f.file->f_op == &bpf_prog_fops)\n\t\terr = bpf_prog_get_info_by_fd(f.file, f.file->private_data, attr,\n\t\t\t\t\t      uattr);\n\telse if (f.file->f_op == &bpf_map_fops)\n\t\terr = bpf_map_get_info_by_fd(f.file, f.file->private_data, attr,\n\t\t\t\t\t     uattr);\n\telse if (f.file->f_op == &btf_fops)\n\t\terr = bpf_btf_get_info_by_fd(f.file, f.file->private_data, attr, uattr);\n\telse if (f.file->f_op == &bpf_link_fops)\n\t\terr = bpf_link_get_info_by_fd(f.file, f.file->private_data,\n\t\t\t\t\t      attr, uattr);\n\telse\n\t\terr = -EINVAL;\n\n\tfdput(f);\n\treturn err;\n}\n\n#define BPF_BTF_LOAD_LAST_FIELD btf_log_true_size\n\nstatic int bpf_btf_load(const union bpf_attr *attr, bpfptr_t uattr, __u32 uattr_size)\n{\n\tif (CHECK_ATTR(BPF_BTF_LOAD))\n\t\treturn -EINVAL;\n\n\tif (!bpf_capable())\n\t\treturn -EPERM;\n\n\treturn btf_new_fd(attr, uattr, uattr_size);\n}\n\n#define BPF_BTF_GET_FD_BY_ID_LAST_FIELD btf_id\n\nstatic int bpf_btf_get_fd_by_id(const union bpf_attr *attr)\n{\n\tif (CHECK_ATTR(BPF_BTF_GET_FD_BY_ID))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\treturn btf_get_fd_by_id(attr->btf_id);\n}\n\nstatic int bpf_task_fd_query_copy(const union bpf_attr *attr,\n\t\t\t\t    union bpf_attr __user *uattr,\n\t\t\t\t    u32 prog_id, u32 fd_type,\n\t\t\t\t    const char *buf, u64 probe_offset,\n\t\t\t\t    u64 probe_addr)\n{\n\tchar __user *ubuf = u64_to_user_ptr(attr->task_fd_query.buf);\n\tu32 len = buf ? strlen(buf) : 0, input_len;\n\tint err = 0;\n\n\tif (put_user(len, &uattr->task_fd_query.buf_len))\n\t\treturn -EFAULT;\n\tinput_len = attr->task_fd_query.buf_len;\n\tif (input_len && ubuf) {\n\t\tif (!len) {\n\t\t\t \n\t\t\tchar zero = '\\0';\n\n\t\t\tif (put_user(zero, ubuf))\n\t\t\t\treturn -EFAULT;\n\t\t} else if (input_len >= len + 1) {\n\t\t\t \n\t\t\tif (copy_to_user(ubuf, buf, len + 1))\n\t\t\t\treturn -EFAULT;\n\t\t} else {\n\t\t\t \n\t\t\tchar zero = '\\0';\n\n\t\t\terr = -ENOSPC;\n\t\t\tif (copy_to_user(ubuf, buf, input_len - 1))\n\t\t\t\treturn -EFAULT;\n\t\t\tif (put_user(zero, ubuf + input_len - 1))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (put_user(prog_id, &uattr->task_fd_query.prog_id) ||\n\t    put_user(fd_type, &uattr->task_fd_query.fd_type) ||\n\t    put_user(probe_offset, &uattr->task_fd_query.probe_offset) ||\n\t    put_user(probe_addr, &uattr->task_fd_query.probe_addr))\n\t\treturn -EFAULT;\n\n\treturn err;\n}\n\n#define BPF_TASK_FD_QUERY_LAST_FIELD task_fd_query.probe_addr\n\nstatic int bpf_task_fd_query(const union bpf_attr *attr,\n\t\t\t     union bpf_attr __user *uattr)\n{\n\tpid_t pid = attr->task_fd_query.pid;\n\tu32 fd = attr->task_fd_query.fd;\n\tconst struct perf_event *event;\n\tstruct task_struct *task;\n\tstruct file *file;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_TASK_FD_QUERY))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (attr->task_fd_query.flags != 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\ttask = get_pid_task(find_vpid(pid), PIDTYPE_PID);\n\trcu_read_unlock();\n\tif (!task)\n\t\treturn -ENOENT;\n\n\terr = 0;\n\tfile = fget_task(task, fd);\n\tput_task_struct(task);\n\tif (!file)\n\t\treturn -EBADF;\n\n\tif (file->f_op == &bpf_link_fops) {\n\t\tstruct bpf_link *link = file->private_data;\n\n\t\tif (link->ops == &bpf_raw_tp_link_lops) {\n\t\t\tstruct bpf_raw_tp_link *raw_tp =\n\t\t\t\tcontainer_of(link, struct bpf_raw_tp_link, link);\n\t\t\tstruct bpf_raw_event_map *btp = raw_tp->btp;\n\n\t\t\terr = bpf_task_fd_query_copy(attr, uattr,\n\t\t\t\t\t\t     raw_tp->link.prog->aux->id,\n\t\t\t\t\t\t     BPF_FD_TYPE_RAW_TRACEPOINT,\n\t\t\t\t\t\t     btp->tp->name, 0, 0);\n\t\t\tgoto put_file;\n\t\t}\n\t\tgoto out_not_supp;\n\t}\n\n\tevent = perf_get_event(file);\n\tif (!IS_ERR(event)) {\n\t\tu64 probe_offset, probe_addr;\n\t\tu32 prog_id, fd_type;\n\t\tconst char *buf;\n\n\t\terr = bpf_get_perf_event_info(event, &prog_id, &fd_type,\n\t\t\t\t\t      &buf, &probe_offset,\n\t\t\t\t\t      &probe_addr);\n\t\tif (!err)\n\t\t\terr = bpf_task_fd_query_copy(attr, uattr, prog_id,\n\t\t\t\t\t\t     fd_type, buf,\n\t\t\t\t\t\t     probe_offset,\n\t\t\t\t\t\t     probe_addr);\n\t\tgoto put_file;\n\t}\n\nout_not_supp:\n\terr = -ENOTSUPP;\nput_file:\n\tfput(file);\n\treturn err;\n}\n\n#define BPF_MAP_BATCH_LAST_FIELD batch.flags\n\n#define BPF_DO_BATCH(fn, ...)\t\t\t\\\n\tdo {\t\t\t\t\t\\\n\t\tif (!fn) {\t\t\t\\\n\t\t\terr = -ENOTSUPP;\t\\\n\t\t\tgoto err_put;\t\t\\\n\t\t}\t\t\t\t\\\n\t\terr = fn(__VA_ARGS__);\t\t\\\n\t} while (0)\n\nstatic int bpf_map_do_batch(const union bpf_attr *attr,\n\t\t\t    union bpf_attr __user *uattr,\n\t\t\t    int cmd)\n{\n\tbool has_read  = cmd == BPF_MAP_LOOKUP_BATCH ||\n\t\t\t cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH;\n\tbool has_write = cmd != BPF_MAP_LOOKUP_BATCH;\n\tstruct bpf_map *map;\n\tint err, ufd;\n\tstruct fd f;\n\n\tif (CHECK_ATTR(BPF_MAP_BATCH))\n\t\treturn -EINVAL;\n\n\tufd = attr->batch.map_fd;\n\tf = fdget(ufd);\n\tmap = __bpf_map_get(f);\n\tif (IS_ERR(map))\n\t\treturn PTR_ERR(map);\n\tif (has_write)\n\t\tbpf_map_write_active_inc(map);\n\tif (has_read && !(map_get_sys_perms(map, f) & FMODE_CAN_READ)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\tif (has_write && !(map_get_sys_perms(map, f) & FMODE_CAN_WRITE)) {\n\t\terr = -EPERM;\n\t\tgoto err_put;\n\t}\n\n\tif (cmd == BPF_MAP_LOOKUP_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_lookup_batch, map, attr, uattr);\n\telse if (cmd == BPF_MAP_LOOKUP_AND_DELETE_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_lookup_and_delete_batch, map, attr, uattr);\n\telse if (cmd == BPF_MAP_UPDATE_BATCH)\n\t\tBPF_DO_BATCH(map->ops->map_update_batch, map, f.file, attr, uattr);\n\telse\n\t\tBPF_DO_BATCH(map->ops->map_delete_batch, map, attr, uattr);\nerr_put:\n\tif (has_write)\n\t\tbpf_map_write_active_dec(map);\n\tfdput(f);\n\treturn err;\n}\n\n#define BPF_LINK_CREATE_LAST_FIELD link_create.uprobe_multi.pid\nstatic int link_create(union bpf_attr *attr, bpfptr_t uattr)\n{\n\tstruct bpf_prog *prog;\n\tint ret;\n\n\tif (CHECK_ATTR(BPF_LINK_CREATE))\n\t\treturn -EINVAL;\n\n\tif (attr->link_create.attach_type == BPF_STRUCT_OPS)\n\t\treturn bpf_struct_ops_link_create(attr);\n\n\tprog = bpf_prog_get(attr->link_create.prog_fd);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tret = bpf_prog_attach_check_attach_type(prog,\n\t\t\t\t\t\tattr->link_create.attach_type);\n\tif (ret)\n\t\tgoto out;\n\n\tswitch (prog->type) {\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tret = cgroup_bpf_link_attach(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\tret = bpf_tracing_prog_attach(prog,\n\t\t\t\t\t      attr->link_create.target_fd,\n\t\t\t\t\t      attr->link_create.target_btf_id,\n\t\t\t\t\t      attr->link_create.tracing.cookie);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_LSM:\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tif (attr->link_create.attach_type != prog->expected_attach_type) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (prog->expected_attach_type == BPF_TRACE_RAW_TP)\n\t\t\tret = bpf_raw_tp_link_attach(prog, NULL);\n\t\telse if (prog->expected_attach_type == BPF_TRACE_ITER)\n\t\t\tret = bpf_iter_link_attach(attr, uattr, prog);\n\t\telse if (prog->expected_attach_type == BPF_LSM_CGROUP)\n\t\t\tret = cgroup_bpf_link_attach(attr, prog);\n\t\telse\n\t\t\tret = bpf_tracing_prog_attach(prog,\n\t\t\t\t\t\t      attr->link_create.target_fd,\n\t\t\t\t\t\t      attr->link_create.target_btf_id,\n\t\t\t\t\t\t      attr->link_create.tracing.cookie);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\tret = netns_bpf_link_create(attr, prog);\n\t\tbreak;\n#ifdef CONFIG_NET\n\tcase BPF_PROG_TYPE_XDP:\n\t\tret = bpf_xdp_link_attach(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\t\tret = tcx_link_attach(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_NETFILTER:\n\t\tret = bpf_nf_link_attach(attr, prog);\n\t\tbreak;\n#endif\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\t\tret = bpf_perf_link_attach(attr, prog);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_KPROBE:\n\t\tif (attr->link_create.attach_type == BPF_PERF_EVENT)\n\t\t\tret = bpf_perf_link_attach(attr, prog);\n\t\telse if (attr->link_create.attach_type == BPF_TRACE_KPROBE_MULTI)\n\t\t\tret = bpf_kprobe_multi_link_attach(attr, prog);\n\t\telse if (attr->link_create.attach_type == BPF_TRACE_UPROBE_MULTI)\n\t\t\tret = bpf_uprobe_multi_link_attach(attr, prog);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\nout:\n\tif (ret < 0)\n\t\tbpf_prog_put(prog);\n\treturn ret;\n}\n\nstatic int link_update_map(struct bpf_link *link, union bpf_attr *attr)\n{\n\tstruct bpf_map *new_map, *old_map = NULL;\n\tint ret;\n\n\tnew_map = bpf_map_get(attr->link_update.new_map_fd);\n\tif (IS_ERR(new_map))\n\t\treturn PTR_ERR(new_map);\n\n\tif (attr->link_update.flags & BPF_F_REPLACE) {\n\t\told_map = bpf_map_get(attr->link_update.old_map_fd);\n\t\tif (IS_ERR(old_map)) {\n\t\t\tret = PTR_ERR(old_map);\n\t\t\tgoto out_put;\n\t\t}\n\t} else if (attr->link_update.old_map_fd) {\n\t\tret = -EINVAL;\n\t\tgoto out_put;\n\t}\n\n\tret = link->ops->update_map(link, new_map, old_map);\n\n\tif (old_map)\n\t\tbpf_map_put(old_map);\nout_put:\n\tbpf_map_put(new_map);\n\treturn ret;\n}\n\n#define BPF_LINK_UPDATE_LAST_FIELD link_update.old_prog_fd\n\nstatic int link_update(union bpf_attr *attr)\n{\n\tstruct bpf_prog *old_prog = NULL, *new_prog;\n\tstruct bpf_link *link;\n\tu32 flags;\n\tint ret;\n\n\tif (CHECK_ATTR(BPF_LINK_UPDATE))\n\t\treturn -EINVAL;\n\n\tflags = attr->link_update.flags;\n\tif (flags & ~BPF_F_REPLACE)\n\t\treturn -EINVAL;\n\n\tlink = bpf_link_get_from_fd(attr->link_update.link_fd);\n\tif (IS_ERR(link))\n\t\treturn PTR_ERR(link);\n\n\tif (link->ops->update_map) {\n\t\tret = link_update_map(link, attr);\n\t\tgoto out_put_link;\n\t}\n\n\tnew_prog = bpf_prog_get(attr->link_update.new_prog_fd);\n\tif (IS_ERR(new_prog)) {\n\t\tret = PTR_ERR(new_prog);\n\t\tgoto out_put_link;\n\t}\n\n\tif (flags & BPF_F_REPLACE) {\n\t\told_prog = bpf_prog_get(attr->link_update.old_prog_fd);\n\t\tif (IS_ERR(old_prog)) {\n\t\t\tret = PTR_ERR(old_prog);\n\t\t\told_prog = NULL;\n\t\t\tgoto out_put_progs;\n\t\t}\n\t} else if (attr->link_update.old_prog_fd) {\n\t\tret = -EINVAL;\n\t\tgoto out_put_progs;\n\t}\n\n\tif (link->ops->update_prog)\n\t\tret = link->ops->update_prog(link, new_prog, old_prog);\n\telse\n\t\tret = -EINVAL;\n\nout_put_progs:\n\tif (old_prog)\n\t\tbpf_prog_put(old_prog);\n\tif (ret)\n\t\tbpf_prog_put(new_prog);\nout_put_link:\n\tbpf_link_put_direct(link);\n\treturn ret;\n}\n\n#define BPF_LINK_DETACH_LAST_FIELD link_detach.link_fd\n\nstatic int link_detach(union bpf_attr *attr)\n{\n\tstruct bpf_link *link;\n\tint ret;\n\n\tif (CHECK_ATTR(BPF_LINK_DETACH))\n\t\treturn -EINVAL;\n\n\tlink = bpf_link_get_from_fd(attr->link_detach.link_fd);\n\tif (IS_ERR(link))\n\t\treturn PTR_ERR(link);\n\n\tif (link->ops->detach)\n\t\tret = link->ops->detach(link);\n\telse\n\t\tret = -EOPNOTSUPP;\n\n\tbpf_link_put_direct(link);\n\treturn ret;\n}\n\nstatic struct bpf_link *bpf_link_inc_not_zero(struct bpf_link *link)\n{\n\treturn atomic64_fetch_add_unless(&link->refcnt, 1, 0) ? link : ERR_PTR(-ENOENT);\n}\n\nstruct bpf_link *bpf_link_by_id(u32 id)\n{\n\tstruct bpf_link *link;\n\n\tif (!id)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tspin_lock_bh(&link_idr_lock);\n\t \n\tlink = idr_find(&link_idr, id);\n\tif (link) {\n\t\tif (link->id)\n\t\t\tlink = bpf_link_inc_not_zero(link);\n\t\telse\n\t\t\tlink = ERR_PTR(-EAGAIN);\n\t} else {\n\t\tlink = ERR_PTR(-ENOENT);\n\t}\n\tspin_unlock_bh(&link_idr_lock);\n\treturn link;\n}\n\nstruct bpf_link *bpf_link_get_curr_or_next(u32 *id)\n{\n\tstruct bpf_link *link;\n\n\tspin_lock_bh(&link_idr_lock);\nagain:\n\tlink = idr_get_next(&link_idr, id);\n\tif (link) {\n\t\tlink = bpf_link_inc_not_zero(link);\n\t\tif (IS_ERR(link)) {\n\t\t\t(*id)++;\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock_bh(&link_idr_lock);\n\n\treturn link;\n}\n\n#define BPF_LINK_GET_FD_BY_ID_LAST_FIELD link_id\n\nstatic int bpf_link_get_fd_by_id(const union bpf_attr *attr)\n{\n\tstruct bpf_link *link;\n\tu32 id = attr->link_id;\n\tint fd;\n\n\tif (CHECK_ATTR(BPF_LINK_GET_FD_BY_ID))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tlink = bpf_link_by_id(id);\n\tif (IS_ERR(link))\n\t\treturn PTR_ERR(link);\n\n\tfd = bpf_link_new_fd(link);\n\tif (fd < 0)\n\t\tbpf_link_put_direct(link);\n\n\treturn fd;\n}\n\nDEFINE_MUTEX(bpf_stats_enabled_mutex);\n\nstatic int bpf_stats_release(struct inode *inode, struct file *file)\n{\n\tmutex_lock(&bpf_stats_enabled_mutex);\n\tstatic_key_slow_dec(&bpf_stats_enabled_key.key);\n\tmutex_unlock(&bpf_stats_enabled_mutex);\n\treturn 0;\n}\n\nstatic const struct file_operations bpf_stats_fops = {\n\t.release = bpf_stats_release,\n};\n\nstatic int bpf_enable_runtime_stats(void)\n{\n\tint fd;\n\n\tmutex_lock(&bpf_stats_enabled_mutex);\n\n\t \n\tif (static_key_count(&bpf_stats_enabled_key.key) > INT_MAX / 2) {\n\t\tmutex_unlock(&bpf_stats_enabled_mutex);\n\t\treturn -EBUSY;\n\t}\n\n\tfd = anon_inode_getfd(\"bpf-stats\", &bpf_stats_fops, NULL, O_CLOEXEC);\n\tif (fd >= 0)\n\t\tstatic_key_slow_inc(&bpf_stats_enabled_key.key);\n\n\tmutex_unlock(&bpf_stats_enabled_mutex);\n\treturn fd;\n}\n\n#define BPF_ENABLE_STATS_LAST_FIELD enable_stats.type\n\nstatic int bpf_enable_stats(union bpf_attr *attr)\n{\n\n\tif (CHECK_ATTR(BPF_ENABLE_STATS))\n\t\treturn -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tswitch (attr->enable_stats.type) {\n\tcase BPF_STATS_RUN_TIME:\n\t\treturn bpf_enable_runtime_stats();\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\n#define BPF_ITER_CREATE_LAST_FIELD iter_create.flags\n\nstatic int bpf_iter_create(union bpf_attr *attr)\n{\n\tstruct bpf_link *link;\n\tint err;\n\n\tif (CHECK_ATTR(BPF_ITER_CREATE))\n\t\treturn -EINVAL;\n\n\tif (attr->iter_create.flags)\n\t\treturn -EINVAL;\n\n\tlink = bpf_link_get_from_fd(attr->iter_create.link_fd);\n\tif (IS_ERR(link))\n\t\treturn PTR_ERR(link);\n\n\terr = bpf_iter_new_fd(link);\n\tbpf_link_put_direct(link);\n\n\treturn err;\n}\n\n#define BPF_PROG_BIND_MAP_LAST_FIELD prog_bind_map.flags\n\nstatic int bpf_prog_bind_map(union bpf_attr *attr)\n{\n\tstruct bpf_prog *prog;\n\tstruct bpf_map *map;\n\tstruct bpf_map **used_maps_old, **used_maps_new;\n\tint i, ret = 0;\n\n\tif (CHECK_ATTR(BPF_PROG_BIND_MAP))\n\t\treturn -EINVAL;\n\n\tif (attr->prog_bind_map.flags)\n\t\treturn -EINVAL;\n\n\tprog = bpf_prog_get(attr->prog_bind_map.prog_fd);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tmap = bpf_map_get(attr->prog_bind_map.map_fd);\n\tif (IS_ERR(map)) {\n\t\tret = PTR_ERR(map);\n\t\tgoto out_prog_put;\n\t}\n\n\tmutex_lock(&prog->aux->used_maps_mutex);\n\n\tused_maps_old = prog->aux->used_maps;\n\n\tfor (i = 0; i < prog->aux->used_map_cnt; i++)\n\t\tif (used_maps_old[i] == map) {\n\t\t\tbpf_map_put(map);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\tused_maps_new = kmalloc_array(prog->aux->used_map_cnt + 1,\n\t\t\t\t      sizeof(used_maps_new[0]),\n\t\t\t\t      GFP_KERNEL);\n\tif (!used_maps_new) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tmemcpy(used_maps_new, used_maps_old,\n\t       sizeof(used_maps_old[0]) * prog->aux->used_map_cnt);\n\tused_maps_new[prog->aux->used_map_cnt] = map;\n\n\tprog->aux->used_map_cnt++;\n\tprog->aux->used_maps = used_maps_new;\n\n\tkfree(used_maps_old);\n\nout_unlock:\n\tmutex_unlock(&prog->aux->used_maps_mutex);\n\n\tif (ret)\n\t\tbpf_map_put(map);\nout_prog_put:\n\tbpf_prog_put(prog);\n\treturn ret;\n}\n\nstatic int __sys_bpf(int cmd, bpfptr_t uattr, unsigned int size)\n{\n\tunion bpf_attr attr;\n\tint err;\n\n\terr = bpf_check_uarg_tail_zero(uattr, sizeof(attr), size);\n\tif (err)\n\t\treturn err;\n\tsize = min_t(u32, size, sizeof(attr));\n\n\t \n\tmemset(&attr, 0, sizeof(attr));\n\tif (copy_from_bpfptr(&attr, uattr, size) != 0)\n\t\treturn -EFAULT;\n\n\terr = security_bpf(cmd, &attr, size);\n\tif (err < 0)\n\t\treturn err;\n\n\tswitch (cmd) {\n\tcase BPF_MAP_CREATE:\n\t\terr = map_create(&attr);\n\t\tbreak;\n\tcase BPF_MAP_LOOKUP_ELEM:\n\t\terr = map_lookup_elem(&attr);\n\t\tbreak;\n\tcase BPF_MAP_UPDATE_ELEM:\n\t\terr = map_update_elem(&attr, uattr);\n\t\tbreak;\n\tcase BPF_MAP_DELETE_ELEM:\n\t\terr = map_delete_elem(&attr, uattr);\n\t\tbreak;\n\tcase BPF_MAP_GET_NEXT_KEY:\n\t\terr = map_get_next_key(&attr);\n\t\tbreak;\n\tcase BPF_MAP_FREEZE:\n\t\terr = map_freeze(&attr);\n\t\tbreak;\n\tcase BPF_PROG_LOAD:\n\t\terr = bpf_prog_load(&attr, uattr, size);\n\t\tbreak;\n\tcase BPF_OBJ_PIN:\n\t\terr = bpf_obj_pin(&attr);\n\t\tbreak;\n\tcase BPF_OBJ_GET:\n\t\terr = bpf_obj_get(&attr);\n\t\tbreak;\n\tcase BPF_PROG_ATTACH:\n\t\terr = bpf_prog_attach(&attr);\n\t\tbreak;\n\tcase BPF_PROG_DETACH:\n\t\terr = bpf_prog_detach(&attr);\n\t\tbreak;\n\tcase BPF_PROG_QUERY:\n\t\terr = bpf_prog_query(&attr, uattr.user);\n\t\tbreak;\n\tcase BPF_PROG_TEST_RUN:\n\t\terr = bpf_prog_test_run(&attr, uattr.user);\n\t\tbreak;\n\tcase BPF_PROG_GET_NEXT_ID:\n\t\terr = bpf_obj_get_next_id(&attr, uattr.user,\n\t\t\t\t\t  &prog_idr, &prog_idr_lock);\n\t\tbreak;\n\tcase BPF_MAP_GET_NEXT_ID:\n\t\terr = bpf_obj_get_next_id(&attr, uattr.user,\n\t\t\t\t\t  &map_idr, &map_idr_lock);\n\t\tbreak;\n\tcase BPF_BTF_GET_NEXT_ID:\n\t\terr = bpf_obj_get_next_id(&attr, uattr.user,\n\t\t\t\t\t  &btf_idr, &btf_idr_lock);\n\t\tbreak;\n\tcase BPF_PROG_GET_FD_BY_ID:\n\t\terr = bpf_prog_get_fd_by_id(&attr);\n\t\tbreak;\n\tcase BPF_MAP_GET_FD_BY_ID:\n\t\terr = bpf_map_get_fd_by_id(&attr);\n\t\tbreak;\n\tcase BPF_OBJ_GET_INFO_BY_FD:\n\t\terr = bpf_obj_get_info_by_fd(&attr, uattr.user);\n\t\tbreak;\n\tcase BPF_RAW_TRACEPOINT_OPEN:\n\t\terr = bpf_raw_tracepoint_open(&attr);\n\t\tbreak;\n\tcase BPF_BTF_LOAD:\n\t\terr = bpf_btf_load(&attr, uattr, size);\n\t\tbreak;\n\tcase BPF_BTF_GET_FD_BY_ID:\n\t\terr = bpf_btf_get_fd_by_id(&attr);\n\t\tbreak;\n\tcase BPF_TASK_FD_QUERY:\n\t\terr = bpf_task_fd_query(&attr, uattr.user);\n\t\tbreak;\n\tcase BPF_MAP_LOOKUP_AND_DELETE_ELEM:\n\t\terr = map_lookup_and_delete_elem(&attr);\n\t\tbreak;\n\tcase BPF_MAP_LOOKUP_BATCH:\n\t\terr = bpf_map_do_batch(&attr, uattr.user, BPF_MAP_LOOKUP_BATCH);\n\t\tbreak;\n\tcase BPF_MAP_LOOKUP_AND_DELETE_BATCH:\n\t\terr = bpf_map_do_batch(&attr, uattr.user,\n\t\t\t\t       BPF_MAP_LOOKUP_AND_DELETE_BATCH);\n\t\tbreak;\n\tcase BPF_MAP_UPDATE_BATCH:\n\t\terr = bpf_map_do_batch(&attr, uattr.user, BPF_MAP_UPDATE_BATCH);\n\t\tbreak;\n\tcase BPF_MAP_DELETE_BATCH:\n\t\terr = bpf_map_do_batch(&attr, uattr.user, BPF_MAP_DELETE_BATCH);\n\t\tbreak;\n\tcase BPF_LINK_CREATE:\n\t\terr = link_create(&attr, uattr);\n\t\tbreak;\n\tcase BPF_LINK_UPDATE:\n\t\terr = link_update(&attr);\n\t\tbreak;\n\tcase BPF_LINK_GET_FD_BY_ID:\n\t\terr = bpf_link_get_fd_by_id(&attr);\n\t\tbreak;\n\tcase BPF_LINK_GET_NEXT_ID:\n\t\terr = bpf_obj_get_next_id(&attr, uattr.user,\n\t\t\t\t\t  &link_idr, &link_idr_lock);\n\t\tbreak;\n\tcase BPF_ENABLE_STATS:\n\t\terr = bpf_enable_stats(&attr);\n\t\tbreak;\n\tcase BPF_ITER_CREATE:\n\t\terr = bpf_iter_create(&attr);\n\t\tbreak;\n\tcase BPF_LINK_DETACH:\n\t\terr = link_detach(&attr);\n\t\tbreak;\n\tcase BPF_PROG_BIND_MAP:\n\t\terr = bpf_prog_bind_map(&attr);\n\t\tbreak;\n\tdefault:\n\t\terr = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nSYSCALL_DEFINE3(bpf, int, cmd, union bpf_attr __user *, uattr, unsigned int, size)\n{\n\treturn __sys_bpf(cmd, USER_BPFPTR(uattr), size);\n}\n\nstatic bool syscall_prog_is_valid_access(int off, int size,\n\t\t\t\t\t enum bpf_access_type type,\n\t\t\t\t\t const struct bpf_prog *prog,\n\t\t\t\t\t struct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= U16_MAX)\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\treturn true;\n}\n\nBPF_CALL_3(bpf_sys_bpf, int, cmd, union bpf_attr *, attr, u32, attr_size)\n{\n\tswitch (cmd) {\n\tcase BPF_MAP_CREATE:\n\tcase BPF_MAP_DELETE_ELEM:\n\tcase BPF_MAP_UPDATE_ELEM:\n\tcase BPF_MAP_FREEZE:\n\tcase BPF_MAP_GET_FD_BY_ID:\n\tcase BPF_PROG_LOAD:\n\tcase BPF_BTF_LOAD:\n\tcase BPF_LINK_CREATE:\n\tcase BPF_RAW_TRACEPOINT_OPEN:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn __sys_bpf(cmd, KERNEL_BPFPTR(attr), attr_size);\n}\n\n\n \nint kern_sys_bpf(int cmd, union bpf_attr *attr, unsigned int size);\n\nint kern_sys_bpf(int cmd, union bpf_attr *attr, unsigned int size)\n{\n\tstruct bpf_prog * __maybe_unused prog;\n\tstruct bpf_tramp_run_ctx __maybe_unused run_ctx;\n\n\tswitch (cmd) {\n#ifdef CONFIG_BPF_JIT  \n\tcase BPF_PROG_TEST_RUN:\n\t\tif (attr->test.data_in || attr->test.data_out ||\n\t\t    attr->test.ctx_out || attr->test.duration ||\n\t\t    attr->test.repeat || attr->test.flags)\n\t\t\treturn -EINVAL;\n\n\t\tprog = bpf_prog_get_type(attr->test.prog_fd, BPF_PROG_TYPE_SYSCALL);\n\t\tif (IS_ERR(prog))\n\t\t\treturn PTR_ERR(prog);\n\n\t\tif (attr->test.ctx_size_in < prog->aux->max_ctx_offset ||\n\t\t    attr->test.ctx_size_in > U16_MAX) {\n\t\t\tbpf_prog_put(prog);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\trun_ctx.bpf_cookie = 0;\n\t\tif (!__bpf_prog_enter_sleepable_recur(prog, &run_ctx)) {\n\t\t\t \n\t\t\t__bpf_prog_exit_sleepable_recur(prog, 0, &run_ctx);\n\t\t\tbpf_prog_put(prog);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tattr->test.retval = bpf_prog_run(prog, (void *) (long) attr->test.ctx_in);\n\t\t__bpf_prog_exit_sleepable_recur(prog, 0  ,\n\t\t\t\t\t\t&run_ctx);\n\t\tbpf_prog_put(prog);\n\t\treturn 0;\n#endif\n\tdefault:\n\t\treturn ____bpf_sys_bpf(cmd, attr, size);\n\t}\n}\nEXPORT_SYMBOL(kern_sys_bpf);\n\nstatic const struct bpf_func_proto bpf_sys_bpf_proto = {\n\t.func\t\t= bpf_sys_bpf,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nconst struct bpf_func_proto * __weak\ntracing_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\treturn bpf_base_func_proto(func_id);\n}\n\nBPF_CALL_1(bpf_sys_close, u32, fd)\n{\n\t \n\treturn close_fd(fd);\n}\n\nstatic const struct bpf_func_proto bpf_sys_close_proto = {\n\t.func\t\t= bpf_sys_close,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_kallsyms_lookup_name, const char *, name, int, name_sz, int, flags, u64 *, res)\n{\n\tif (flags)\n\t\treturn -EINVAL;\n\n\tif (name_sz <= 1 || name[name_sz - 1])\n\t\treturn -EINVAL;\n\n\tif (!bpf_dump_raw_ok(current_cred()))\n\t\treturn -EPERM;\n\n\t*res = kallsyms_lookup_name(name);\n\treturn *res ? 0 : -ENOENT;\n}\n\nstatic const struct bpf_func_proto bpf_kallsyms_lookup_name_proto = {\n\t.func\t\t= bpf_kallsyms_lookup_name,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_LONG,\n};\n\nstatic const struct bpf_func_proto *\nsyscall_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_sys_bpf:\n\t\treturn !perfmon_capable() ? NULL : &bpf_sys_bpf_proto;\n\tcase BPF_FUNC_btf_find_by_name_kind:\n\t\treturn &bpf_btf_find_by_name_kind_proto;\n\tcase BPF_FUNC_sys_close:\n\t\treturn &bpf_sys_close_proto;\n\tcase BPF_FUNC_kallsyms_lookup_name:\n\t\treturn &bpf_kallsyms_lookup_name_proto;\n\tdefault:\n\t\treturn tracing_prog_func_proto(func_id, prog);\n\t}\n}\n\nconst struct bpf_verifier_ops bpf_syscall_verifier_ops = {\n\t.get_func_proto  = syscall_prog_func_proto,\n\t.is_valid_access = syscall_prog_is_valid_access,\n};\n\nconst struct bpf_prog_ops bpf_syscall_prog_ops = {\n\t.test_run = bpf_prog_test_run_syscall,\n};\n\n#ifdef CONFIG_SYSCTL\nstatic int bpf_stats_handler(struct ctl_table *table, int write,\n\t\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tstruct static_key *key = (struct static_key *)table->data;\n\tstatic int saved_val;\n\tint val, ret;\n\tstruct ctl_table tmp = {\n\t\t.data   = &val,\n\t\t.maxlen = sizeof(val),\n\t\t.mode   = table->mode,\n\t\t.extra1 = SYSCTL_ZERO,\n\t\t.extra2 = SYSCTL_ONE,\n\t};\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tmutex_lock(&bpf_stats_enabled_mutex);\n\tval = saved_val;\n\tret = proc_dointvec_minmax(&tmp, write, buffer, lenp, ppos);\n\tif (write && !ret && val != saved_val) {\n\t\tif (val)\n\t\t\tstatic_key_slow_inc(key);\n\t\telse\n\t\t\tstatic_key_slow_dec(key);\n\t\tsaved_val = val;\n\t}\n\tmutex_unlock(&bpf_stats_enabled_mutex);\n\treturn ret;\n}\n\nvoid __weak unpriv_ebpf_notify(int new_state)\n{\n}\n\nstatic int bpf_unpriv_handler(struct ctl_table *table, int write,\n\t\t\t      void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret, unpriv_enable = *(int *)table->data;\n\tbool locked_state = unpriv_enable == 1;\n\tstruct ctl_table tmp = *table;\n\n\tif (write && !capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\ttmp.data = &unpriv_enable;\n\tret = proc_dointvec_minmax(&tmp, write, buffer, lenp, ppos);\n\tif (write && !ret) {\n\t\tif (locked_state && unpriv_enable != 1)\n\t\t\treturn -EPERM;\n\t\t*(int *)table->data = unpriv_enable;\n\t}\n\n\tif (write)\n\t\tunpriv_ebpf_notify(unpriv_enable);\n\n\treturn ret;\n}\n\nstatic struct ctl_table bpf_syscall_table[] = {\n\t{\n\t\t.procname\t= \"unprivileged_bpf_disabled\",\n\t\t.data\t\t= &sysctl_unprivileged_bpf_disabled,\n\t\t.maxlen\t\t= sizeof(sysctl_unprivileged_bpf_disabled),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= bpf_unpriv_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_TWO,\n\t},\n\t{\n\t\t.procname\t= \"bpf_stats_enabled\",\n\t\t.data\t\t= &bpf_stats_enabled_key.key,\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= bpf_stats_handler,\n\t},\n\t{ }\n};\n\nstatic int __init bpf_syscall_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", bpf_syscall_table);\n\treturn 0;\n}\nlate_initcall(bpf_syscall_sysctl_init);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}