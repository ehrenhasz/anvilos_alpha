{
  "module_name": "trampoline.c",
  "hash_id": "778c7f82f10e8d1061fc9254863fc1fd63d59a03814fccd3acb2bacf64f0e2d0",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/trampoline.c",
  "human_readable_source": "\n \n#include <linux/hash.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/ftrace.h>\n#include <linux/rbtree_latch.h>\n#include <linux/perf_event.h>\n#include <linux/btf.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/rcupdate_wait.h>\n#include <linux/static_call.h>\n#include <linux/bpf_verifier.h>\n#include <linux/bpf_lsm.h>\n#include <linux/delay.h>\n\n \nconst struct bpf_verifier_ops bpf_extension_verifier_ops = {\n};\nconst struct bpf_prog_ops bpf_extension_prog_ops = {\n};\n\n \n#define TRAMPOLINE_HASH_BITS 10\n#define TRAMPOLINE_TABLE_SIZE (1 << TRAMPOLINE_HASH_BITS)\n\nstatic struct hlist_head trampoline_table[TRAMPOLINE_TABLE_SIZE];\n\n \nstatic DEFINE_MUTEX(trampoline_mutex);\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\nstatic int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mutex);\n\nstatic int bpf_tramp_ftrace_ops_func(struct ftrace_ops *ops, enum ftrace_ops_cmd cmd)\n{\n\tstruct bpf_trampoline *tr = ops->private;\n\tint ret = 0;\n\n\tif (cmd == FTRACE_OPS_CMD_ENABLE_SHARE_IPMODIFY_SELF) {\n\t\t \n\t\tlockdep_assert_held_once(&tr->mutex);\n\n\t\t \n\t\tif ((tr->flags & BPF_TRAMP_F_CALL_ORIG) &&\n\t\t    !(tr->flags & BPF_TRAMP_F_ORIG_STACK)) {\n\t\t\tif (WARN_ON_ONCE(tr->flags & BPF_TRAMP_F_SHARE_IPMODIFY))\n\t\t\t\treturn -EBUSY;\n\n\t\t\ttr->flags |= BPF_TRAMP_F_SHARE_IPMODIFY;\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t \n\tif (!mutex_trylock(&tr->mutex)) {\n\t\t \n\t\tmsleep(1);\n\t\treturn -EAGAIN;\n\t}\n\n\tswitch (cmd) {\n\tcase FTRACE_OPS_CMD_ENABLE_SHARE_IPMODIFY_PEER:\n\t\ttr->flags |= BPF_TRAMP_F_SHARE_IPMODIFY;\n\n\t\tif ((tr->flags & BPF_TRAMP_F_CALL_ORIG) &&\n\t\t    !(tr->flags & BPF_TRAMP_F_ORIG_STACK))\n\t\t\tret = bpf_trampoline_update(tr, false  );\n\t\tbreak;\n\tcase FTRACE_OPS_CMD_DISABLE_SHARE_IPMODIFY_PEER:\n\t\ttr->flags &= ~BPF_TRAMP_F_SHARE_IPMODIFY;\n\n\t\tif (tr->flags & BPF_TRAMP_F_ORIG_STACK)\n\t\t\tret = bpf_trampoline_update(tr, false  );\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&tr->mutex);\n\treturn ret;\n}\n#endif\n\nbool bpf_prog_has_trampoline(const struct bpf_prog *prog)\n{\n\tenum bpf_attach_type eatype = prog->expected_attach_type;\n\tenum bpf_prog_type ptype = prog->type;\n\n\treturn (ptype == BPF_PROG_TYPE_TRACING &&\n\t\t(eatype == BPF_TRACE_FENTRY || eatype == BPF_TRACE_FEXIT ||\n\t\t eatype == BPF_MODIFY_RETURN)) ||\n\t\t(ptype == BPF_PROG_TYPE_LSM && eatype == BPF_LSM_MAC);\n}\n\nvoid bpf_image_ksym_add(void *data, struct bpf_ksym *ksym)\n{\n\tksym->start = (unsigned long) data;\n\tksym->end = ksym->start + PAGE_SIZE;\n\tbpf_ksym_add(ksym);\n\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_BPF, ksym->start,\n\t\t\t   PAGE_SIZE, false, ksym->name);\n}\n\nvoid bpf_image_ksym_del(struct bpf_ksym *ksym)\n{\n\tbpf_ksym_del(ksym);\n\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_BPF, ksym->start,\n\t\t\t   PAGE_SIZE, true, ksym->name);\n}\n\nstatic struct bpf_trampoline *bpf_trampoline_lookup(u64 key)\n{\n\tstruct bpf_trampoline *tr;\n\tstruct hlist_head *head;\n\tint i;\n\n\tmutex_lock(&trampoline_mutex);\n\thead = &trampoline_table[hash_64(key, TRAMPOLINE_HASH_BITS)];\n\thlist_for_each_entry(tr, head, hlist) {\n\t\tif (tr->key == key) {\n\t\t\trefcount_inc(&tr->refcnt);\n\t\t\tgoto out;\n\t\t}\n\t}\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\tgoto out;\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\n\ttr->fops = kzalloc(sizeof(struct ftrace_ops), GFP_KERNEL);\n\tif (!tr->fops) {\n\t\tkfree(tr);\n\t\ttr = NULL;\n\t\tgoto out;\n\t}\n\ttr->fops->private = tr;\n\ttr->fops->ops_func = bpf_tramp_ftrace_ops_func;\n#endif\n\n\ttr->key = key;\n\tINIT_HLIST_NODE(&tr->hlist);\n\thlist_add_head(&tr->hlist, head);\n\trefcount_set(&tr->refcnt, 1);\n\tmutex_init(&tr->mutex);\n\tfor (i = 0; i < BPF_TRAMP_MAX; i++)\n\t\tINIT_HLIST_HEAD(&tr->progs_hlist[i]);\nout:\n\tmutex_unlock(&trampoline_mutex);\n\treturn tr;\n}\n\nstatic int unregister_fentry(struct bpf_trampoline *tr, void *old_addr)\n{\n\tvoid *ip = tr->func.addr;\n\tint ret;\n\n\tif (tr->func.ftrace_managed)\n\t\tret = unregister_ftrace_direct(tr->fops, (long)old_addr, false);\n\telse\n\t\tret = bpf_arch_text_poke(ip, BPF_MOD_CALL, old_addr, NULL);\n\n\treturn ret;\n}\n\nstatic int modify_fentry(struct bpf_trampoline *tr, void *old_addr, void *new_addr,\n\t\t\t bool lock_direct_mutex)\n{\n\tvoid *ip = tr->func.addr;\n\tint ret;\n\n\tif (tr->func.ftrace_managed) {\n\t\tif (lock_direct_mutex)\n\t\t\tret = modify_ftrace_direct(tr->fops, (long)new_addr);\n\t\telse\n\t\t\tret = modify_ftrace_direct_nolock(tr->fops, (long)new_addr);\n\t} else {\n\t\tret = bpf_arch_text_poke(ip, BPF_MOD_CALL, old_addr, new_addr);\n\t}\n\treturn ret;\n}\n\n \nstatic int register_fentry(struct bpf_trampoline *tr, void *new_addr)\n{\n\tvoid *ip = tr->func.addr;\n\tunsigned long faddr;\n\tint ret;\n\n\tfaddr = ftrace_location((unsigned long)ip);\n\tif (faddr) {\n\t\tif (!tr->fops)\n\t\t\treturn -ENOTSUPP;\n\t\ttr->func.ftrace_managed = true;\n\t}\n\n\tif (tr->func.ftrace_managed) {\n\t\tftrace_set_filter_ip(tr->fops, (unsigned long)ip, 0, 1);\n\t\tret = register_ftrace_direct(tr->fops, (long)new_addr);\n\t} else {\n\t\tret = bpf_arch_text_poke(ip, BPF_MOD_CALL, NULL, new_addr);\n\t}\n\n\treturn ret;\n}\n\nstatic struct bpf_tramp_links *\nbpf_trampoline_get_progs(const struct bpf_trampoline *tr, int *total, bool *ip_arg)\n{\n\tstruct bpf_tramp_link *link;\n\tstruct bpf_tramp_links *tlinks;\n\tstruct bpf_tramp_link **links;\n\tint kind;\n\n\t*total = 0;\n\ttlinks = kcalloc(BPF_TRAMP_MAX, sizeof(*tlinks), GFP_KERNEL);\n\tif (!tlinks)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (kind = 0; kind < BPF_TRAMP_MAX; kind++) {\n\t\ttlinks[kind].nr_links = tr->progs_cnt[kind];\n\t\t*total += tr->progs_cnt[kind];\n\t\tlinks = tlinks[kind].links;\n\n\t\thlist_for_each_entry(link, &tr->progs_hlist[kind], tramp_hlist) {\n\t\t\t*ip_arg |= link->link.prog->call_get_func_ip;\n\t\t\t*links++ = link;\n\t\t}\n\t}\n\treturn tlinks;\n}\n\nstatic void bpf_tramp_image_free(struct bpf_tramp_image *im)\n{\n\tbpf_image_ksym_del(&im->ksym);\n\tbpf_jit_free_exec(im->image);\n\tbpf_jit_uncharge_modmem(PAGE_SIZE);\n\tpercpu_ref_exit(&im->pcref);\n\tkfree_rcu(im, rcu);\n}\n\nstatic void __bpf_tramp_image_put_deferred(struct work_struct *work)\n{\n\tstruct bpf_tramp_image *im;\n\n\tim = container_of(work, struct bpf_tramp_image, work);\n\tbpf_tramp_image_free(im);\n}\n\n \nstatic void __bpf_tramp_image_put_rcu(struct rcu_head *rcu)\n{\n\tstruct bpf_tramp_image *im;\n\n\tim = container_of(rcu, struct bpf_tramp_image, rcu);\n\tINIT_WORK(&im->work, __bpf_tramp_image_put_deferred);\n\tschedule_work(&im->work);\n}\n\n \nstatic void __bpf_tramp_image_release(struct percpu_ref *pcref)\n{\n\tstruct bpf_tramp_image *im;\n\n\tim = container_of(pcref, struct bpf_tramp_image, pcref);\n\tcall_rcu_tasks(&im->rcu, __bpf_tramp_image_put_rcu);\n}\n\n \nstatic void __bpf_tramp_image_put_rcu_tasks(struct rcu_head *rcu)\n{\n\tstruct bpf_tramp_image *im;\n\n\tim = container_of(rcu, struct bpf_tramp_image, rcu);\n\tif (im->ip_after_call)\n\t\t \n\t\tpercpu_ref_kill(&im->pcref);\n\telse\n\t\t \n\t\tcall_rcu_tasks(&im->rcu, __bpf_tramp_image_put_rcu);\n}\n\nstatic void bpf_tramp_image_put(struct bpf_tramp_image *im)\n{\n\t \n\tif (im->ip_after_call) {\n\t\tint err = bpf_arch_text_poke(im->ip_after_call, BPF_MOD_JUMP,\n\t\t\t\t\t     NULL, im->ip_epilogue);\n\t\tWARN_ON(err);\n\t\tif (IS_ENABLED(CONFIG_PREEMPTION))\n\t\t\tcall_rcu_tasks(&im->rcu, __bpf_tramp_image_put_rcu_tasks);\n\t\telse\n\t\t\tpercpu_ref_kill(&im->pcref);\n\t\treturn;\n\t}\n\n\t \n\tcall_rcu_tasks_trace(&im->rcu, __bpf_tramp_image_put_rcu_tasks);\n}\n\nstatic struct bpf_tramp_image *bpf_tramp_image_alloc(u64 key)\n{\n\tstruct bpf_tramp_image *im;\n\tstruct bpf_ksym *ksym;\n\tvoid *image;\n\tint err = -ENOMEM;\n\n\tim = kzalloc(sizeof(*im), GFP_KERNEL);\n\tif (!im)\n\t\tgoto out;\n\n\terr = bpf_jit_charge_modmem(PAGE_SIZE);\n\tif (err)\n\t\tgoto out_free_im;\n\n\terr = -ENOMEM;\n\tim->image = image = bpf_jit_alloc_exec(PAGE_SIZE);\n\tif (!image)\n\t\tgoto out_uncharge;\n\tset_vm_flush_reset_perms(image);\n\n\terr = percpu_ref_init(&im->pcref, __bpf_tramp_image_release, 0, GFP_KERNEL);\n\tif (err)\n\t\tgoto out_free_image;\n\n\tksym = &im->ksym;\n\tINIT_LIST_HEAD_RCU(&ksym->lnode);\n\tsnprintf(ksym->name, KSYM_NAME_LEN, \"bpf_trampoline_%llu\", key);\n\tbpf_image_ksym_add(image, ksym);\n\treturn im;\n\nout_free_image:\n\tbpf_jit_free_exec(im->image);\nout_uncharge:\n\tbpf_jit_uncharge_modmem(PAGE_SIZE);\nout_free_im:\n\tkfree(im);\nout:\n\treturn ERR_PTR(err);\n}\n\nstatic int bpf_trampoline_update(struct bpf_trampoline *tr, bool lock_direct_mutex)\n{\n\tstruct bpf_tramp_image *im;\n\tstruct bpf_tramp_links *tlinks;\n\tu32 orig_flags = tr->flags;\n\tbool ip_arg = false;\n\tint err, total;\n\n\ttlinks = bpf_trampoline_get_progs(tr, &total, &ip_arg);\n\tif (IS_ERR(tlinks))\n\t\treturn PTR_ERR(tlinks);\n\n\tif (total == 0) {\n\t\terr = unregister_fentry(tr, tr->cur_image->image);\n\t\tbpf_tramp_image_put(tr->cur_image);\n\t\ttr->cur_image = NULL;\n\t\tgoto out;\n\t}\n\n\tim = bpf_tramp_image_alloc(tr->key);\n\tif (IS_ERR(im)) {\n\t\terr = PTR_ERR(im);\n\t\tgoto out;\n\t}\n\n\t \n\ttr->flags &= (BPF_TRAMP_F_SHARE_IPMODIFY | BPF_TRAMP_F_TAIL_CALL_CTX);\n\n\tif (tlinks[BPF_TRAMP_FEXIT].nr_links ||\n\t    tlinks[BPF_TRAMP_MODIFY_RETURN].nr_links) {\n\t\t \n\t\ttr->flags |= BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_SKIP_FRAME;\n\t} else {\n\t\ttr->flags |= BPF_TRAMP_F_RESTORE_REGS;\n\t}\n\n\tif (ip_arg)\n\t\ttr->flags |= BPF_TRAMP_F_IP_ARG;\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\nagain:\n\tif ((tr->flags & BPF_TRAMP_F_SHARE_IPMODIFY) &&\n\t    (tr->flags & BPF_TRAMP_F_CALL_ORIG))\n\t\ttr->flags |= BPF_TRAMP_F_ORIG_STACK;\n#endif\n\n\terr = arch_prepare_bpf_trampoline(im, im->image, im->image + PAGE_SIZE,\n\t\t\t\t\t  &tr->func.model, tr->flags, tlinks,\n\t\t\t\t\t  tr->func.addr);\n\tif (err < 0)\n\t\tgoto out_free;\n\n\tset_memory_rox((long)im->image, 1);\n\n\tWARN_ON(tr->cur_image && total == 0);\n\tif (tr->cur_image)\n\t\t \n\t\terr = modify_fentry(tr, tr->cur_image->image, im->image, lock_direct_mutex);\n\telse\n\t\t \n\t\terr = register_fentry(tr, im->image);\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\n\tif (err == -EAGAIN) {\n\t\t \n\t\t \n\t\ttr->fops->func = NULL;\n\t\ttr->fops->trampoline = 0;\n\n\t\t \n\t\tset_memory_nx((long)im->image, 1);\n\t\tset_memory_rw((long)im->image, 1);\n\t\tgoto again;\n\t}\n#endif\n\tif (err)\n\t\tgoto out_free;\n\n\tif (tr->cur_image)\n\t\tbpf_tramp_image_put(tr->cur_image);\n\ttr->cur_image = im;\nout:\n\t \n\tif (err)\n\t\ttr->flags = orig_flags;\n\tkfree(tlinks);\n\treturn err;\n\nout_free:\n\tbpf_tramp_image_free(im);\n\tgoto out;\n}\n\nstatic enum bpf_tramp_prog_type bpf_attach_type_to_tramp(struct bpf_prog *prog)\n{\n\tswitch (prog->expected_attach_type) {\n\tcase BPF_TRACE_FENTRY:\n\t\treturn BPF_TRAMP_FENTRY;\n\tcase BPF_MODIFY_RETURN:\n\t\treturn BPF_TRAMP_MODIFY_RETURN;\n\tcase BPF_TRACE_FEXIT:\n\t\treturn BPF_TRAMP_FEXIT;\n\tcase BPF_LSM_MAC:\n\t\tif (!prog->aux->attach_func_proto->type)\n\t\t\t \n\t\t\treturn BPF_TRAMP_FEXIT;\n\t\telse\n\t\t\treturn BPF_TRAMP_MODIFY_RETURN;\n\tdefault:\n\t\treturn BPF_TRAMP_REPLACE;\n\t}\n}\n\nstatic int __bpf_trampoline_link_prog(struct bpf_tramp_link *link, struct bpf_trampoline *tr)\n{\n\tenum bpf_tramp_prog_type kind;\n\tstruct bpf_tramp_link *link_exiting;\n\tint err = 0;\n\tint cnt = 0, i;\n\n\tkind = bpf_attach_type_to_tramp(link->link.prog);\n\tif (tr->extension_prog)\n\t\t \n\t\treturn -EBUSY;\n\n\tfor (i = 0; i < BPF_TRAMP_MAX; i++)\n\t\tcnt += tr->progs_cnt[i];\n\n\tif (kind == BPF_TRAMP_REPLACE) {\n\t\t \n\t\tif (cnt)\n\t\t\treturn -EBUSY;\n\t\ttr->extension_prog = link->link.prog;\n\t\treturn bpf_arch_text_poke(tr->func.addr, BPF_MOD_JUMP, NULL,\n\t\t\t\t\t  link->link.prog->bpf_func);\n\t}\n\tif (cnt >= BPF_MAX_TRAMP_LINKS)\n\t\treturn -E2BIG;\n\tif (!hlist_unhashed(&link->tramp_hlist))\n\t\t \n\t\treturn -EBUSY;\n\thlist_for_each_entry(link_exiting, &tr->progs_hlist[kind], tramp_hlist) {\n\t\tif (link_exiting->link.prog != link->link.prog)\n\t\t\tcontinue;\n\t\t \n\t\treturn -EBUSY;\n\t}\n\n\thlist_add_head(&link->tramp_hlist, &tr->progs_hlist[kind]);\n\ttr->progs_cnt[kind]++;\n\terr = bpf_trampoline_update(tr, true  );\n\tif (err) {\n\t\thlist_del_init(&link->tramp_hlist);\n\t\ttr->progs_cnt[kind]--;\n\t}\n\treturn err;\n}\n\nint bpf_trampoline_link_prog(struct bpf_tramp_link *link, struct bpf_trampoline *tr)\n{\n\tint err;\n\n\tmutex_lock(&tr->mutex);\n\terr = __bpf_trampoline_link_prog(link, tr);\n\tmutex_unlock(&tr->mutex);\n\treturn err;\n}\n\nstatic int __bpf_trampoline_unlink_prog(struct bpf_tramp_link *link, struct bpf_trampoline *tr)\n{\n\tenum bpf_tramp_prog_type kind;\n\tint err;\n\n\tkind = bpf_attach_type_to_tramp(link->link.prog);\n\tif (kind == BPF_TRAMP_REPLACE) {\n\t\tWARN_ON_ONCE(!tr->extension_prog);\n\t\terr = bpf_arch_text_poke(tr->func.addr, BPF_MOD_JUMP,\n\t\t\t\t\t tr->extension_prog->bpf_func, NULL);\n\t\ttr->extension_prog = NULL;\n\t\treturn err;\n\t}\n\thlist_del_init(&link->tramp_hlist);\n\ttr->progs_cnt[kind]--;\n\treturn bpf_trampoline_update(tr, true  );\n}\n\n \nint bpf_trampoline_unlink_prog(struct bpf_tramp_link *link, struct bpf_trampoline *tr)\n{\n\tint err;\n\n\tmutex_lock(&tr->mutex);\n\terr = __bpf_trampoline_unlink_prog(link, tr);\n\tmutex_unlock(&tr->mutex);\n\treturn err;\n}\n\n#if defined(CONFIG_CGROUP_BPF) && defined(CONFIG_BPF_LSM)\nstatic void bpf_shim_tramp_link_release(struct bpf_link *link)\n{\n\tstruct bpf_shim_tramp_link *shim_link =\n\t\tcontainer_of(link, struct bpf_shim_tramp_link, link.link);\n\n\t \n\tif (!shim_link->trampoline)\n\t\treturn;\n\n\tWARN_ON_ONCE(bpf_trampoline_unlink_prog(&shim_link->link, shim_link->trampoline));\n\tbpf_trampoline_put(shim_link->trampoline);\n}\n\nstatic void bpf_shim_tramp_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_shim_tramp_link *shim_link =\n\t\tcontainer_of(link, struct bpf_shim_tramp_link, link.link);\n\n\tkfree(shim_link);\n}\n\nstatic const struct bpf_link_ops bpf_shim_tramp_link_lops = {\n\t.release = bpf_shim_tramp_link_release,\n\t.dealloc = bpf_shim_tramp_link_dealloc,\n};\n\nstatic struct bpf_shim_tramp_link *cgroup_shim_alloc(const struct bpf_prog *prog,\n\t\t\t\t\t\t     bpf_func_t bpf_func,\n\t\t\t\t\t\t     int cgroup_atype)\n{\n\tstruct bpf_shim_tramp_link *shim_link = NULL;\n\tstruct bpf_prog *p;\n\n\tshim_link = kzalloc(sizeof(*shim_link), GFP_USER);\n\tif (!shim_link)\n\t\treturn NULL;\n\n\tp = bpf_prog_alloc(1, 0);\n\tif (!p) {\n\t\tkfree(shim_link);\n\t\treturn NULL;\n\t}\n\n\tp->jited = false;\n\tp->bpf_func = bpf_func;\n\n\tp->aux->cgroup_atype = cgroup_atype;\n\tp->aux->attach_func_proto = prog->aux->attach_func_proto;\n\tp->aux->attach_btf_id = prog->aux->attach_btf_id;\n\tp->aux->attach_btf = prog->aux->attach_btf;\n\tbtf_get(p->aux->attach_btf);\n\tp->type = BPF_PROG_TYPE_LSM;\n\tp->expected_attach_type = BPF_LSM_MAC;\n\tbpf_prog_inc(p);\n\tbpf_link_init(&shim_link->link.link, BPF_LINK_TYPE_UNSPEC,\n\t\t      &bpf_shim_tramp_link_lops, p);\n\tbpf_cgroup_atype_get(p->aux->attach_btf_id, cgroup_atype);\n\n\treturn shim_link;\n}\n\nstatic struct bpf_shim_tramp_link *cgroup_shim_find(struct bpf_trampoline *tr,\n\t\t\t\t\t\t    bpf_func_t bpf_func)\n{\n\tstruct bpf_tramp_link *link;\n\tint kind;\n\n\tfor (kind = 0; kind < BPF_TRAMP_MAX; kind++) {\n\t\thlist_for_each_entry(link, &tr->progs_hlist[kind], tramp_hlist) {\n\t\t\tstruct bpf_prog *p = link->link.prog;\n\n\t\t\tif (p->bpf_func == bpf_func)\n\t\t\t\treturn container_of(link, struct bpf_shim_tramp_link, link);\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nint bpf_trampoline_link_cgroup_shim(struct bpf_prog *prog,\n\t\t\t\t    int cgroup_atype)\n{\n\tstruct bpf_shim_tramp_link *shim_link = NULL;\n\tstruct bpf_attach_target_info tgt_info = {};\n\tstruct bpf_trampoline *tr;\n\tbpf_func_t bpf_func;\n\tu64 key;\n\tint err;\n\n\terr = bpf_check_attach_target(NULL, prog, NULL,\n\t\t\t\t      prog->aux->attach_btf_id,\n\t\t\t\t      &tgt_info);\n\tif (err)\n\t\treturn err;\n\n\tkey = bpf_trampoline_compute_key(NULL, prog->aux->attach_btf,\n\t\t\t\t\t prog->aux->attach_btf_id);\n\n\tbpf_lsm_find_cgroup_shim(prog, &bpf_func);\n\ttr = bpf_trampoline_get(key, &tgt_info);\n\tif (!tr)\n\t\treturn  -ENOMEM;\n\n\tmutex_lock(&tr->mutex);\n\n\tshim_link = cgroup_shim_find(tr, bpf_func);\n\tif (shim_link) {\n\t\t \n\t\tbpf_link_inc(&shim_link->link.link);\n\n\t\tmutex_unlock(&tr->mutex);\n\t\tbpf_trampoline_put(tr);  \n\t\treturn 0;\n\t}\n\n\t \n\n\tshim_link = cgroup_shim_alloc(prog, bpf_func, cgroup_atype);\n\tif (!shim_link) {\n\t\terr = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\terr = __bpf_trampoline_link_prog(&shim_link->link, tr);\n\tif (err)\n\t\tgoto err;\n\n\tshim_link->trampoline = tr;\n\t \n\n\tmutex_unlock(&tr->mutex);\n\n\treturn 0;\nerr:\n\tmutex_unlock(&tr->mutex);\n\n\tif (shim_link)\n\t\tbpf_link_put(&shim_link->link.link);\n\n\t \n\tbpf_trampoline_put(tr);  \n\n\treturn err;\n}\n\nvoid bpf_trampoline_unlink_cgroup_shim(struct bpf_prog *prog)\n{\n\tstruct bpf_shim_tramp_link *shim_link = NULL;\n\tstruct bpf_trampoline *tr;\n\tbpf_func_t bpf_func;\n\tu64 key;\n\n\tkey = bpf_trampoline_compute_key(NULL, prog->aux->attach_btf,\n\t\t\t\t\t prog->aux->attach_btf_id);\n\n\tbpf_lsm_find_cgroup_shim(prog, &bpf_func);\n\ttr = bpf_trampoline_lookup(key);\n\tif (WARN_ON_ONCE(!tr))\n\t\treturn;\n\n\tmutex_lock(&tr->mutex);\n\tshim_link = cgroup_shim_find(tr, bpf_func);\n\tmutex_unlock(&tr->mutex);\n\n\tif (shim_link)\n\t\tbpf_link_put(&shim_link->link.link);\n\n\tbpf_trampoline_put(tr);  \n}\n#endif\n\nstruct bpf_trampoline *bpf_trampoline_get(u64 key,\n\t\t\t\t\t  struct bpf_attach_target_info *tgt_info)\n{\n\tstruct bpf_trampoline *tr;\n\n\ttr = bpf_trampoline_lookup(key);\n\tif (!tr)\n\t\treturn NULL;\n\n\tmutex_lock(&tr->mutex);\n\tif (tr->func.addr)\n\t\tgoto out;\n\n\tmemcpy(&tr->func.model, &tgt_info->fmodel, sizeof(tgt_info->fmodel));\n\ttr->func.addr = (void *)tgt_info->tgt_addr;\nout:\n\tmutex_unlock(&tr->mutex);\n\treturn tr;\n}\n\nvoid bpf_trampoline_put(struct bpf_trampoline *tr)\n{\n\tint i;\n\n\tif (!tr)\n\t\treturn;\n\tmutex_lock(&trampoline_mutex);\n\tif (!refcount_dec_and_test(&tr->refcnt))\n\t\tgoto out;\n\tWARN_ON_ONCE(mutex_is_locked(&tr->mutex));\n\n\tfor (i = 0; i < BPF_TRAMP_MAX; i++)\n\t\tif (WARN_ON_ONCE(!hlist_empty(&tr->progs_hlist[i])))\n\t\t\tgoto out;\n\n\t \n\thlist_del(&tr->hlist);\n\tif (tr->fops) {\n\t\tftrace_free_filter(tr->fops);\n\t\tkfree(tr->fops);\n\t}\n\tkfree(tr);\nout:\n\tmutex_unlock(&trampoline_mutex);\n}\n\n#define NO_START_TIME 1\nstatic __always_inline u64 notrace bpf_prog_start_time(void)\n{\n\tu64 start = NO_START_TIME;\n\n\tif (static_branch_unlikely(&bpf_stats_enabled_key)) {\n\t\tstart = sched_clock();\n\t\tif (unlikely(!start))\n\t\t\tstart = NO_START_TIME;\n\t}\n\treturn start;\n}\n\n \nstatic u64 notrace __bpf_prog_enter_recur(struct bpf_prog *prog, struct bpf_tramp_run_ctx *run_ctx)\n\t__acquires(RCU)\n{\n\trcu_read_lock();\n\tmigrate_disable();\n\n\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);\n\n\tif (unlikely(this_cpu_inc_return(*(prog->active)) != 1)) {\n\t\tbpf_prog_inc_misses_counter(prog);\n\t\treturn 0;\n\t}\n\treturn bpf_prog_start_time();\n}\n\nstatic void notrace update_prog_stats(struct bpf_prog *prog,\n\t\t\t\t      u64 start)\n{\n\tstruct bpf_prog_stats *stats;\n\n\tif (static_branch_unlikely(&bpf_stats_enabled_key) &&\n\t     \n\t    start > NO_START_TIME) {\n\t\tunsigned long flags;\n\n\t\tstats = this_cpu_ptr(prog->stats);\n\t\tflags = u64_stats_update_begin_irqsave(&stats->syncp);\n\t\tu64_stats_inc(&stats->cnt);\n\t\tu64_stats_add(&stats->nsecs, sched_clock() - start);\n\t\tu64_stats_update_end_irqrestore(&stats->syncp, flags);\n\t}\n}\n\nstatic void notrace __bpf_prog_exit_recur(struct bpf_prog *prog, u64 start,\n\t\t\t\t\t  struct bpf_tramp_run_ctx *run_ctx)\n\t__releases(RCU)\n{\n\tbpf_reset_run_ctx(run_ctx->saved_run_ctx);\n\n\tupdate_prog_stats(prog, start);\n\tthis_cpu_dec(*(prog->active));\n\tmigrate_enable();\n\trcu_read_unlock();\n}\n\nstatic u64 notrace __bpf_prog_enter_lsm_cgroup(struct bpf_prog *prog,\n\t\t\t\t\t       struct bpf_tramp_run_ctx *run_ctx)\n\t__acquires(RCU)\n{\n\t \n\trcu_read_lock();\n\tmigrate_disable();\n\n\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);\n\n\treturn NO_START_TIME;\n}\n\nstatic void notrace __bpf_prog_exit_lsm_cgroup(struct bpf_prog *prog, u64 start,\n\t\t\t\t\t       struct bpf_tramp_run_ctx *run_ctx)\n\t__releases(RCU)\n{\n\tbpf_reset_run_ctx(run_ctx->saved_run_ctx);\n\n\tmigrate_enable();\n\trcu_read_unlock();\n}\n\nu64 notrace __bpf_prog_enter_sleepable_recur(struct bpf_prog *prog,\n\t\t\t\t\t     struct bpf_tramp_run_ctx *run_ctx)\n{\n\trcu_read_lock_trace();\n\tmigrate_disable();\n\tmight_fault();\n\n\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);\n\n\tif (unlikely(this_cpu_inc_return(*(prog->active)) != 1)) {\n\t\tbpf_prog_inc_misses_counter(prog);\n\t\treturn 0;\n\t}\n\treturn bpf_prog_start_time();\n}\n\nvoid notrace __bpf_prog_exit_sleepable_recur(struct bpf_prog *prog, u64 start,\n\t\t\t\t\t     struct bpf_tramp_run_ctx *run_ctx)\n{\n\tbpf_reset_run_ctx(run_ctx->saved_run_ctx);\n\n\tupdate_prog_stats(prog, start);\n\tthis_cpu_dec(*(prog->active));\n\tmigrate_enable();\n\trcu_read_unlock_trace();\n}\n\nstatic u64 notrace __bpf_prog_enter_sleepable(struct bpf_prog *prog,\n\t\t\t\t\t      struct bpf_tramp_run_ctx *run_ctx)\n{\n\trcu_read_lock_trace();\n\tmigrate_disable();\n\tmight_fault();\n\n\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);\n\n\treturn bpf_prog_start_time();\n}\n\nstatic void notrace __bpf_prog_exit_sleepable(struct bpf_prog *prog, u64 start,\n\t\t\t\t\t      struct bpf_tramp_run_ctx *run_ctx)\n{\n\tbpf_reset_run_ctx(run_ctx->saved_run_ctx);\n\n\tupdate_prog_stats(prog, start);\n\tmigrate_enable();\n\trcu_read_unlock_trace();\n}\n\nstatic u64 notrace __bpf_prog_enter(struct bpf_prog *prog,\n\t\t\t\t    struct bpf_tramp_run_ctx *run_ctx)\n\t__acquires(RCU)\n{\n\trcu_read_lock();\n\tmigrate_disable();\n\n\trun_ctx->saved_run_ctx = bpf_set_run_ctx(&run_ctx->run_ctx);\n\n\treturn bpf_prog_start_time();\n}\n\nstatic void notrace __bpf_prog_exit(struct bpf_prog *prog, u64 start,\n\t\t\t\t    struct bpf_tramp_run_ctx *run_ctx)\n\t__releases(RCU)\n{\n\tbpf_reset_run_ctx(run_ctx->saved_run_ctx);\n\n\tupdate_prog_stats(prog, start);\n\tmigrate_enable();\n\trcu_read_unlock();\n}\n\nvoid notrace __bpf_tramp_enter(struct bpf_tramp_image *tr)\n{\n\tpercpu_ref_get(&tr->pcref);\n}\n\nvoid notrace __bpf_tramp_exit(struct bpf_tramp_image *tr)\n{\n\tpercpu_ref_put(&tr->pcref);\n}\n\nbpf_trampoline_enter_t bpf_trampoline_enter(const struct bpf_prog *prog)\n{\n\tbool sleepable = prog->aux->sleepable;\n\n\tif (bpf_prog_check_recur(prog))\n\t\treturn sleepable ? __bpf_prog_enter_sleepable_recur :\n\t\t\t__bpf_prog_enter_recur;\n\n\tif (resolve_prog_type(prog) == BPF_PROG_TYPE_LSM &&\n\t    prog->expected_attach_type == BPF_LSM_CGROUP)\n\t\treturn __bpf_prog_enter_lsm_cgroup;\n\n\treturn sleepable ? __bpf_prog_enter_sleepable : __bpf_prog_enter;\n}\n\nbpf_trampoline_exit_t bpf_trampoline_exit(const struct bpf_prog *prog)\n{\n\tbool sleepable = prog->aux->sleepable;\n\n\tif (bpf_prog_check_recur(prog))\n\t\treturn sleepable ? __bpf_prog_exit_sleepable_recur :\n\t\t\t__bpf_prog_exit_recur;\n\n\tif (resolve_prog_type(prog) == BPF_PROG_TYPE_LSM &&\n\t    prog->expected_attach_type == BPF_LSM_CGROUP)\n\t\treturn __bpf_prog_exit_lsm_cgroup;\n\n\treturn sleepable ? __bpf_prog_exit_sleepable : __bpf_prog_exit;\n}\n\nint __weak\narch_prepare_bpf_trampoline(struct bpf_tramp_image *tr, void *image, void *image_end,\n\t\t\t    const struct btf_func_model *m, u32 flags,\n\t\t\t    struct bpf_tramp_links *tlinks,\n\t\t\t    void *orig_call)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic int __init init_trampolines(void)\n{\n\tint i;\n\n\tfor (i = 0; i < TRAMPOLINE_TABLE_SIZE; i++)\n\t\tINIT_HLIST_HEAD(&trampoline_table[i]);\n\treturn 0;\n}\nlate_initcall(init_trampolines);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}