{
  "module_name": "stackmap.c",
  "hash_id": "d1e002b96e8b22b1db3323ad7f4e84486b2e2ec390ad60b686b9c37a265bc81d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/stackmap.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/jhash.h>\n#include <linux/filter.h>\n#include <linux/kernel.h>\n#include <linux/stacktrace.h>\n#include <linux/perf_event.h>\n#include <linux/btf_ids.h>\n#include <linux/buildid.h>\n#include \"percpu_freelist.h\"\n#include \"mmap_unlock_work.h\"\n\n#define STACK_CREATE_FLAG_MASK\t\t\t\t\t\\\n\t(BPF_F_NUMA_NODE | BPF_F_RDONLY | BPF_F_WRONLY |\t\\\n\t BPF_F_STACK_BUILD_ID)\n\nstruct stack_map_bucket {\n\tstruct pcpu_freelist_node fnode;\n\tu32 hash;\n\tu32 nr;\n\tu64 data[];\n};\n\nstruct bpf_stack_map {\n\tstruct bpf_map map;\n\tvoid *elems;\n\tstruct pcpu_freelist freelist;\n\tu32 n_buckets;\n\tstruct stack_map_bucket *buckets[];\n};\n\nstatic inline bool stack_map_use_build_id(struct bpf_map *map)\n{\n\treturn (map->map_flags & BPF_F_STACK_BUILD_ID);\n}\n\nstatic inline int stack_map_data_size(struct bpf_map *map)\n{\n\treturn stack_map_use_build_id(map) ?\n\t\tsizeof(struct bpf_stack_build_id) : sizeof(u64);\n}\n\nstatic int prealloc_elems_and_freelist(struct bpf_stack_map *smap)\n{\n\tu64 elem_size = sizeof(struct stack_map_bucket) +\n\t\t\t(u64)smap->map.value_size;\n\tint err;\n\n\tsmap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,\n\t\t\t\t\t smap->map.numa_node);\n\tif (!smap->elems)\n\t\treturn -ENOMEM;\n\n\terr = pcpu_freelist_init(&smap->freelist);\n\tif (err)\n\t\tgoto free_elems;\n\n\tpcpu_freelist_populate(&smap->freelist, smap->elems, elem_size,\n\t\t\t       smap->map.max_entries);\n\treturn 0;\n\nfree_elems:\n\tbpf_map_area_free(smap->elems);\n\treturn err;\n}\n\n \nstatic struct bpf_map *stack_map_alloc(union bpf_attr *attr)\n{\n\tu32 value_size = attr->value_size;\n\tstruct bpf_stack_map *smap;\n\tu64 cost, n_buckets;\n\tint err;\n\n\tif (attr->map_flags & ~STACK_CREATE_FLAG_MASK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    value_size < 8 || value_size % 8)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tBUILD_BUG_ON(sizeof(struct bpf_stack_build_id) % sizeof(u64));\n\tif (attr->map_flags & BPF_F_STACK_BUILD_ID) {\n\t\tif (value_size % sizeof(struct bpf_stack_build_id) ||\n\t\t    value_size / sizeof(struct bpf_stack_build_id)\n\t\t    > sysctl_perf_event_max_stack)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t} else if (value_size / 8 > sysctl_perf_event_max_stack)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tn_buckets = roundup_pow_of_two(attr->max_entries);\n\tif (!n_buckets)\n\t\treturn ERR_PTR(-E2BIG);\n\n\tcost = n_buckets * sizeof(struct stack_map_bucket *) + sizeof(*smap);\n\tsmap = bpf_map_area_alloc(cost, bpf_map_attr_numa_node(attr));\n\tif (!smap)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&smap->map, attr);\n\tsmap->n_buckets = n_buckets;\n\n\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\tif (err)\n\t\tgoto free_smap;\n\n\terr = prealloc_elems_and_freelist(smap);\n\tif (err)\n\t\tgoto put_buffers;\n\n\treturn &smap->map;\n\nput_buffers:\n\tput_callchain_buffers();\nfree_smap:\n\tbpf_map_area_free(smap);\n\treturn ERR_PTR(err);\n}\n\nstatic void stack_map_get_build_id_offset(struct bpf_stack_build_id *id_offs,\n\t\t\t\t\t  u64 *ips, u32 trace_nr, bool user)\n{\n\tint i;\n\tstruct mmap_unlock_irq_work *work = NULL;\n\tbool irq_work_busy = bpf_mmap_unlock_get_irq_work(&work);\n\tstruct vm_area_struct *vma, *prev_vma = NULL;\n\tconst char *prev_build_id;\n\n\t \n\tif (!user || !current || !current->mm || irq_work_busy ||\n\t    !mmap_read_trylock(current->mm)) {\n\t\t \n\t\tfor (i = 0; i < trace_nr; i++) {\n\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;\n\t\t\tid_offs[i].ip = ips[i];\n\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < trace_nr; i++) {\n\t\tif (range_in_vma(prev_vma, ips[i], ips[i])) {\n\t\t\tvma = prev_vma;\n\t\t\tmemcpy(id_offs[i].build_id, prev_build_id,\n\t\t\t       BUILD_ID_SIZE_MAX);\n\t\t\tgoto build_id_valid;\n\t\t}\n\t\tvma = find_vma(current->mm, ips[i]);\n\t\tif (!vma || build_id_parse(vma, id_offs[i].build_id, NULL)) {\n\t\t\t \n\t\t\tid_offs[i].status = BPF_STACK_BUILD_ID_IP;\n\t\t\tid_offs[i].ip = ips[i];\n\t\t\tmemset(id_offs[i].build_id, 0, BUILD_ID_SIZE_MAX);\n\t\t\tcontinue;\n\t\t}\nbuild_id_valid:\n\t\tid_offs[i].offset = (vma->vm_pgoff << PAGE_SHIFT) + ips[i]\n\t\t\t- vma->vm_start;\n\t\tid_offs[i].status = BPF_STACK_BUILD_ID_VALID;\n\t\tprev_vma = vma;\n\t\tprev_build_id = id_offs[i].build_id;\n\t}\n\tbpf_mmap_unlock_mm(work, current->mm);\n}\n\nstatic struct perf_callchain_entry *\nget_callchain_entry_for_task(struct task_struct *task, u32 max_depth)\n{\n#ifdef CONFIG_STACKTRACE\n\tstruct perf_callchain_entry *entry;\n\tint rctx;\n\n\tentry = get_callchain_entry(&rctx);\n\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->nr = stack_trace_save_tsk(task, (unsigned long *)entry->ip,\n\t\t\t\t\t max_depth, 0);\n\n\t \n\tif (__BITS_PER_LONG != 64) {\n\t\tunsigned long *from = (unsigned long *) entry->ip;\n\t\tu64 *to = entry->ip;\n\t\tint i;\n\n\t\t \n\t\tfor (i = entry->nr - 1; i >= 0; i--)\n\t\t\tto[i] = (u64)(from[i]);\n\t}\n\n\tput_callchain_entry(rctx);\n\n\treturn entry;\n#else  \n\treturn NULL;\n#endif\n}\n\nstatic long __bpf_get_stackid(struct bpf_map *map,\n\t\t\t      struct perf_callchain_entry *trace, u64 flags)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *bucket, *new_bucket, *old_bucket;\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tu32 hash, id, trace_nr, trace_len;\n\tbool user = flags & BPF_F_USER_STACK;\n\tu64 *ips;\n\tbool hash_matches;\n\n\tif (trace->nr <= skip)\n\t\t \n\t\treturn -EFAULT;\n\n\ttrace_nr = trace->nr - skip;\n\ttrace_len = trace_nr * sizeof(u64);\n\tips = trace->ip + skip;\n\thash = jhash2((u32 *)ips, trace_len / sizeof(u32), 0);\n\tid = hash & (smap->n_buckets - 1);\n\tbucket = READ_ONCE(smap->buckets[id]);\n\n\thash_matches = bucket && bucket->hash == hash;\n\t \n\tif (hash_matches && flags & BPF_F_FAST_STACK_CMP)\n\t\treturn id;\n\n\tif (stack_map_use_build_id(map)) {\n\t\t \n\t\tnew_bucket = (struct stack_map_bucket *)\n\t\t\tpcpu_freelist_pop(&smap->freelist);\n\t\tif (unlikely(!new_bucket))\n\t\t\treturn -ENOMEM;\n\t\tnew_bucket->nr = trace_nr;\n\t\tstack_map_get_build_id_offset(\n\t\t\t(struct bpf_stack_build_id *)new_bucket->data,\n\t\t\tips, trace_nr, user);\n\t\ttrace_len = trace_nr * sizeof(struct bpf_stack_build_id);\n\t\tif (hash_matches && bucket->nr == trace_nr &&\n\t\t    memcmp(bucket->data, new_bucket->data, trace_len) == 0) {\n\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);\n\t\t\treturn id;\n\t\t}\n\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID)) {\n\t\t\tpcpu_freelist_push(&smap->freelist, &new_bucket->fnode);\n\t\t\treturn -EEXIST;\n\t\t}\n\t} else {\n\t\tif (hash_matches && bucket->nr == trace_nr &&\n\t\t    memcmp(bucket->data, ips, trace_len) == 0)\n\t\t\treturn id;\n\t\tif (bucket && !(flags & BPF_F_REUSE_STACKID))\n\t\t\treturn -EEXIST;\n\n\t\tnew_bucket = (struct stack_map_bucket *)\n\t\t\tpcpu_freelist_pop(&smap->freelist);\n\t\tif (unlikely(!new_bucket))\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(new_bucket->data, ips, trace_len);\n\t}\n\n\tnew_bucket->hash = hash;\n\tnew_bucket->nr = trace_nr;\n\n\told_bucket = xchg(&smap->buckets[id], new_bucket);\n\tif (old_bucket)\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\treturn id;\n}\n\nBPF_CALL_3(bpf_get_stackid, struct pt_regs *, regs, struct bpf_map *, map,\n\t   u64, flags)\n{\n\tu32 max_depth = map->value_size / stack_map_data_size(map);\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tbool user = flags & BPF_F_USER_STACK;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel = !user;\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))\n\t\treturn -EINVAL;\n\n\tmax_depth += skip;\n\tif (max_depth > sysctl_perf_event_max_stack)\n\t\tmax_depth = sysctl_perf_event_max_stack;\n\n\ttrace = get_perf_callchain(regs, 0, kernel, user, max_depth,\n\t\t\t\t   false, false);\n\n\tif (unlikely(!trace))\n\t\t \n\t\treturn -EFAULT;\n\n\treturn __bpf_get_stackid(map, trace, flags);\n}\n\nconst struct bpf_func_proto bpf_get_stackid_proto = {\n\t.func\t\t= bpf_get_stackid,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic __u64 count_kernel_ip(struct perf_callchain_entry *trace)\n{\n\t__u64 nr_kernel = 0;\n\n\twhile (nr_kernel < trace->nr) {\n\t\tif (trace->ip[nr_kernel] == PERF_CONTEXT_USER)\n\t\t\tbreak;\n\t\tnr_kernel++;\n\t}\n\treturn nr_kernel;\n}\n\nBPF_CALL_3(bpf_get_stackid_pe, struct bpf_perf_event_data_kern *, ctx,\n\t   struct bpf_map *, map, u64, flags)\n{\n\tstruct perf_event *event = ctx->event;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel, user;\n\t__u64 nr_kernel;\n\tint ret;\n\n\t \n\tif (!(event->attr.sample_type & PERF_SAMPLE_CALLCHAIN))\n\t\treturn bpf_get_stackid((unsigned long)(ctx->regs),\n\t\t\t\t       (unsigned long) map, flags, 0, 0);\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_FAST_STACK_CMP | BPF_F_REUSE_STACKID)))\n\t\treturn -EINVAL;\n\n\tuser = flags & BPF_F_USER_STACK;\n\tkernel = !user;\n\n\ttrace = ctx->data->callchain;\n\tif (unlikely(!trace))\n\t\treturn -EFAULT;\n\n\tnr_kernel = count_kernel_ip(trace);\n\n\tif (kernel) {\n\t\t__u64 nr = trace->nr;\n\n\t\ttrace->nr = nr_kernel;\n\t\tret = __bpf_get_stackid(map, trace, flags);\n\n\t\t \n\t\ttrace->nr = nr;\n\t} else {  \n\t\tu64 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\n\t\tskip += nr_kernel;\n\t\tif (skip > BPF_F_SKIP_FIELD_MASK)\n\t\t\treturn -EFAULT;\n\n\t\tflags = (flags & ~BPF_F_SKIP_FIELD_MASK) | skip;\n\t\tret = __bpf_get_stackid(map, trace, flags);\n\t}\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_get_stackid_proto_pe = {\n\t.func\t\t= bpf_get_stackid_pe,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic long __bpf_get_stack(struct pt_regs *regs, struct task_struct *task,\n\t\t\t    struct perf_callchain_entry *trace_in,\n\t\t\t    void *buf, u32 size, u64 flags)\n{\n\tu32 trace_nr, copy_len, elem_size, num_elem, max_depth;\n\tbool user_build_id = flags & BPF_F_USER_BUILD_ID;\n\tbool crosstask = task && task != current;\n\tu32 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\tbool user = flags & BPF_F_USER_STACK;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel = !user;\n\tint err = -EINVAL;\n\tu64 *ips;\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_USER_BUILD_ID)))\n\t\tgoto clear;\n\tif (kernel && user_build_id)\n\t\tgoto clear;\n\n\telem_size = (user && user_build_id) ? sizeof(struct bpf_stack_build_id)\n\t\t\t\t\t    : sizeof(u64);\n\tif (unlikely(size % elem_size))\n\t\tgoto clear;\n\n\t \n\tif (task && user && !user_mode(regs))\n\t\tgoto err_fault;\n\n\t \n\tif (crosstask && user) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto clear;\n\t}\n\n\tnum_elem = size / elem_size;\n\tmax_depth = num_elem + skip;\n\tif (sysctl_perf_event_max_stack < max_depth)\n\t\tmax_depth = sysctl_perf_event_max_stack;\n\n\tif (trace_in)\n\t\ttrace = trace_in;\n\telse if (kernel && task)\n\t\ttrace = get_callchain_entry_for_task(task, max_depth);\n\telse\n\t\ttrace = get_perf_callchain(regs, 0, kernel, user, max_depth,\n\t\t\t\t\t   crosstask, false);\n\tif (unlikely(!trace))\n\t\tgoto err_fault;\n\n\tif (trace->nr < skip)\n\t\tgoto err_fault;\n\n\ttrace_nr = trace->nr - skip;\n\ttrace_nr = (trace_nr <= num_elem) ? trace_nr : num_elem;\n\tcopy_len = trace_nr * elem_size;\n\n\tips = trace->ip + skip;\n\tif (user && user_build_id)\n\t\tstack_map_get_build_id_offset(buf, ips, trace_nr, user);\n\telse\n\t\tmemcpy(buf, ips, copy_len);\n\n\tif (size > copy_len)\n\t\tmemset(buf + copy_len, 0, size - copy_len);\n\treturn copy_len;\n\nerr_fault:\n\terr = -EFAULT;\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n}\n\nBPF_CALL_4(bpf_get_stack, struct pt_regs *, regs, void *, buf, u32, size,\n\t   u64, flags)\n{\n\treturn __bpf_get_stack(regs, NULL, NULL, buf, size, flags);\n}\n\nconst struct bpf_func_proto bpf_get_stack_proto = {\n\t.func\t\t= bpf_get_stack,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_task_stack, struct task_struct *, task, void *, buf,\n\t   u32, size, u64, flags)\n{\n\tstruct pt_regs *regs;\n\tlong res = -EINVAL;\n\n\tif (!try_get_task_stack(task))\n\t\treturn -EFAULT;\n\n\tregs = task_pt_regs(task);\n\tif (regs)\n\t\tres = __bpf_get_stack(regs, task, NULL, buf, size, flags);\n\tput_task_stack(task);\n\n\treturn res;\n}\n\nconst struct bpf_func_proto bpf_get_task_stack_proto = {\n\t.func\t\t= bpf_get_task_stack,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_tracing_ids[BTF_TRACING_TYPE_TASK],\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_stack_pe, struct bpf_perf_event_data_kern *, ctx,\n\t   void *, buf, u32, size, u64, flags)\n{\n\tstruct pt_regs *regs = (struct pt_regs *)(ctx->regs);\n\tstruct perf_event *event = ctx->event;\n\tstruct perf_callchain_entry *trace;\n\tbool kernel, user;\n\tint err = -EINVAL;\n\t__u64 nr_kernel;\n\n\tif (!(event->attr.sample_type & PERF_SAMPLE_CALLCHAIN))\n\t\treturn __bpf_get_stack(regs, NULL, NULL, buf, size, flags);\n\n\tif (unlikely(flags & ~(BPF_F_SKIP_FIELD_MASK | BPF_F_USER_STACK |\n\t\t\t       BPF_F_USER_BUILD_ID)))\n\t\tgoto clear;\n\n\tuser = flags & BPF_F_USER_STACK;\n\tkernel = !user;\n\n\terr = -EFAULT;\n\ttrace = ctx->data->callchain;\n\tif (unlikely(!trace))\n\t\tgoto clear;\n\n\tnr_kernel = count_kernel_ip(trace);\n\n\tif (kernel) {\n\t\t__u64 nr = trace->nr;\n\n\t\ttrace->nr = nr_kernel;\n\t\terr = __bpf_get_stack(regs, NULL, trace, buf, size, flags);\n\n\t\t \n\t\ttrace->nr = nr;\n\t} else {  \n\t\tu64 skip = flags & BPF_F_SKIP_FIELD_MASK;\n\n\t\tskip += nr_kernel;\n\t\tif (skip > BPF_F_SKIP_FIELD_MASK)\n\t\t\tgoto clear;\n\n\t\tflags = (flags & ~BPF_F_SKIP_FIELD_MASK) | skip;\n\t\terr = __bpf_get_stack(regs, NULL, trace, buf, size, flags);\n\t}\n\treturn err;\n\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n\n}\n\nconst struct bpf_func_proto bpf_get_stack_proto_pe = {\n\t.func\t\t= bpf_get_stack_pe,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\n \nstatic void *stack_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\n \nint bpf_stackmap_copy(struct bpf_map *map, void *key, void *value)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *bucket, *old_bucket;\n\tu32 id = *(u32 *)key, trace_len;\n\n\tif (unlikely(id >= smap->n_buckets))\n\t\treturn -ENOENT;\n\n\tbucket = xchg(&smap->buckets[id], NULL);\n\tif (!bucket)\n\t\treturn -ENOENT;\n\n\ttrace_len = bucket->nr * stack_map_data_size(map);\n\tmemcpy(value, bucket->data, trace_len);\n\tmemset(value + trace_len, 0, map->value_size - trace_len);\n\n\told_bucket = xchg(&smap->buckets[id], bucket);\n\tif (old_bucket)\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\treturn 0;\n}\n\nstatic int stack_map_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\t  void *next_key)\n{\n\tstruct bpf_stack_map *smap = container_of(map,\n\t\t\t\t\t\t  struct bpf_stack_map, map);\n\tu32 id;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (!key) {\n\t\tid = 0;\n\t} else {\n\t\tid = *(u32 *)key;\n\t\tif (id >= smap->n_buckets || !smap->buckets[id])\n\t\t\tid = 0;\n\t\telse\n\t\t\tid++;\n\t}\n\n\twhile (id < smap->n_buckets && !smap->buckets[id])\n\t\tid++;\n\n\tif (id >= smap->n_buckets)\n\t\treturn -ENOENT;\n\n\t*(u32 *)next_key = id;\n\treturn 0;\n}\n\nstatic long stack_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t  u64 map_flags)\n{\n\treturn -EINVAL;\n}\n\n \nstatic long stack_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tstruct stack_map_bucket *old_bucket;\n\tu32 id = *(u32 *)key;\n\n\tif (unlikely(id >= smap->n_buckets))\n\t\treturn -E2BIG;\n\n\told_bucket = xchg(&smap->buckets[id], NULL);\n\tif (old_bucket) {\n\t\tpcpu_freelist_push(&smap->freelist, &old_bucket->fnode);\n\t\treturn 0;\n\t} else {\n\t\treturn -ENOENT;\n\t}\n}\n\n \nstatic void stack_map_free(struct bpf_map *map)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\n\tbpf_map_area_free(smap->elems);\n\tpcpu_freelist_destroy(&smap->freelist);\n\tbpf_map_area_free(smap);\n\tput_callchain_buffers();\n}\n\nstatic u64 stack_map_mem_usage(const struct bpf_map *map)\n{\n\tstruct bpf_stack_map *smap = container_of(map, struct bpf_stack_map, map);\n\tu64 value_size = map->value_size;\n\tu64 n_buckets = smap->n_buckets;\n\tu64 enties = map->max_entries;\n\tu64 usage = sizeof(*smap);\n\n\tusage += n_buckets * sizeof(struct stack_map_bucket *);\n\tusage += enties * (sizeof(struct stack_map_bucket) + value_size);\n\treturn usage;\n}\n\nBTF_ID_LIST_SINGLE(stack_trace_map_btf_ids, struct, bpf_stack_map)\nconst struct bpf_map_ops stack_trace_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc = stack_map_alloc,\n\t.map_free = stack_map_free,\n\t.map_get_next_key = stack_map_get_next_key,\n\t.map_lookup_elem = stack_map_lookup_elem,\n\t.map_update_elem = stack_map_update_elem,\n\t.map_delete_elem = stack_map_delete_elem,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = stack_map_mem_usage,\n\t.map_btf_id = &stack_trace_map_btf_ids[0],\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}