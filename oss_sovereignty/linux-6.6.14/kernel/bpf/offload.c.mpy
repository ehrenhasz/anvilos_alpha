{
  "module_name": "offload.c",
  "hash_id": "d7f2f3046a701ae6b00e3110f27c85889b83875844216e0023488f28b16d9a2b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/offload.c",
  "human_readable_source": " \n\n#include <linux/bpf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/bug.h>\n#include <linux/kdev_t.h>\n#include <linux/list.h>\n#include <linux/lockdep.h>\n#include <linux/netdevice.h>\n#include <linux/printk.h>\n#include <linux/proc_ns.h>\n#include <linux/rhashtable.h>\n#include <linux/rtnetlink.h>\n#include <linux/rwsem.h>\n#include <net/xdp.h>\n\n \nstatic DECLARE_RWSEM(bpf_devs_lock);\n\nstruct bpf_offload_dev {\n\tconst struct bpf_prog_offload_ops *ops;\n\tstruct list_head netdevs;\n\tvoid *priv;\n};\n\nstruct bpf_offload_netdev {\n\tstruct rhash_head l;\n\tstruct net_device *netdev;\n\tstruct bpf_offload_dev *offdev;  \n\tstruct list_head progs;\n\tstruct list_head maps;\n\tstruct list_head offdev_netdevs;\n};\n\nstatic const struct rhashtable_params offdevs_params = {\n\t.nelem_hint\t\t= 4,\n\t.key_len\t\t= sizeof(struct net_device *),\n\t.key_offset\t\t= offsetof(struct bpf_offload_netdev, netdev),\n\t.head_offset\t\t= offsetof(struct bpf_offload_netdev, l),\n\t.automatic_shrinking\t= true,\n};\n\nstatic struct rhashtable offdevs;\n\nstatic int bpf_dev_offload_check(struct net_device *netdev)\n{\n\tif (!netdev)\n\t\treturn -EINVAL;\n\tif (!netdev->netdev_ops->ndo_bpf)\n\t\treturn -EOPNOTSUPP;\n\treturn 0;\n}\n\nstatic struct bpf_offload_netdev *\nbpf_offload_find_netdev(struct net_device *netdev)\n{\n\tlockdep_assert_held(&bpf_devs_lock);\n\n\treturn rhashtable_lookup_fast(&offdevs, &netdev, offdevs_params);\n}\n\nstatic int __bpf_offload_dev_netdev_register(struct bpf_offload_dev *offdev,\n\t\t\t\t\t     struct net_device *netdev)\n{\n\tstruct bpf_offload_netdev *ondev;\n\tint err;\n\n\tondev = kzalloc(sizeof(*ondev), GFP_KERNEL);\n\tif (!ondev)\n\t\treturn -ENOMEM;\n\n\tondev->netdev = netdev;\n\tondev->offdev = offdev;\n\tINIT_LIST_HEAD(&ondev->progs);\n\tINIT_LIST_HEAD(&ondev->maps);\n\n\terr = rhashtable_insert_fast(&offdevs, &ondev->l, offdevs_params);\n\tif (err) {\n\t\tnetdev_warn(netdev, \"failed to register for BPF offload\\n\");\n\t\tgoto err_free;\n\t}\n\n\tif (offdev)\n\t\tlist_add(&ondev->offdev_netdevs, &offdev->netdevs);\n\treturn 0;\n\nerr_free:\n\tkfree(ondev);\n\treturn err;\n}\n\nstatic void __bpf_prog_offload_destroy(struct bpf_prog *prog)\n{\n\tstruct bpf_prog_offload *offload = prog->aux->offload;\n\n\tif (offload->dev_state)\n\t\toffload->offdev->ops->destroy(prog);\n\n\tlist_del_init(&offload->offloads);\n\tkfree(offload);\n\tprog->aux->offload = NULL;\n}\n\nstatic int bpf_map_offload_ndo(struct bpf_offloaded_map *offmap,\n\t\t\t       enum bpf_netdev_command cmd)\n{\n\tstruct netdev_bpf data = {};\n\tstruct net_device *netdev;\n\n\tASSERT_RTNL();\n\n\tdata.command = cmd;\n\tdata.offmap = offmap;\n\t \n\tnetdev = offmap->netdev;\n\n\treturn netdev->netdev_ops->ndo_bpf(netdev, &data);\n}\n\nstatic void __bpf_map_offload_destroy(struct bpf_offloaded_map *offmap)\n{\n\tWARN_ON(bpf_map_offload_ndo(offmap, BPF_OFFLOAD_MAP_FREE));\n\t \n\tbpf_map_free_id(&offmap->map);\n\tlist_del_init(&offmap->offloads);\n\toffmap->netdev = NULL;\n}\n\nstatic void __bpf_offload_dev_netdev_unregister(struct bpf_offload_dev *offdev,\n\t\t\t\t\t\tstruct net_device *netdev)\n{\n\tstruct bpf_offload_netdev *ondev, *altdev = NULL;\n\tstruct bpf_offloaded_map *offmap, *mtmp;\n\tstruct bpf_prog_offload *offload, *ptmp;\n\n\tASSERT_RTNL();\n\n\tondev = rhashtable_lookup_fast(&offdevs, &netdev, offdevs_params);\n\tif (WARN_ON(!ondev))\n\t\treturn;\n\n\tWARN_ON(rhashtable_remove_fast(&offdevs, &ondev->l, offdevs_params));\n\n\t \n\tif (offdev) {\n\t\tlist_del(&ondev->offdev_netdevs);\n\t\taltdev = list_first_entry_or_null(&offdev->netdevs,\n\t\t\t\t\t\t  struct bpf_offload_netdev,\n\t\t\t\t\t\t  offdev_netdevs);\n\t}\n\n\tif (altdev) {\n\t\tlist_for_each_entry(offload, &ondev->progs, offloads)\n\t\t\toffload->netdev = altdev->netdev;\n\t\tlist_splice_init(&ondev->progs, &altdev->progs);\n\n\t\tlist_for_each_entry(offmap, &ondev->maps, offloads)\n\t\t\toffmap->netdev = altdev->netdev;\n\t\tlist_splice_init(&ondev->maps, &altdev->maps);\n\t} else {\n\t\tlist_for_each_entry_safe(offload, ptmp, &ondev->progs, offloads)\n\t\t\t__bpf_prog_offload_destroy(offload->prog);\n\t\tlist_for_each_entry_safe(offmap, mtmp, &ondev->maps, offloads)\n\t\t\t__bpf_map_offload_destroy(offmap);\n\t}\n\n\tWARN_ON(!list_empty(&ondev->progs));\n\tWARN_ON(!list_empty(&ondev->maps));\n\tkfree(ondev);\n}\n\nstatic int __bpf_prog_dev_bound_init(struct bpf_prog *prog, struct net_device *netdev)\n{\n\tstruct bpf_offload_netdev *ondev;\n\tstruct bpf_prog_offload *offload;\n\tint err;\n\n\toffload = kzalloc(sizeof(*offload), GFP_USER);\n\tif (!offload)\n\t\treturn -ENOMEM;\n\n\toffload->prog = prog;\n\toffload->netdev = netdev;\n\n\tondev = bpf_offload_find_netdev(offload->netdev);\n\t \n\tif (bpf_prog_is_offloaded(prog->aux) && (!ondev || !ondev->offdev)) {\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\tif (!ondev) {\n\t\t \n\t\terr = __bpf_offload_dev_netdev_register(NULL, offload->netdev);\n\t\tif (err)\n\t\t\tgoto err_free;\n\t\tondev = bpf_offload_find_netdev(offload->netdev);\n\t}\n\toffload->offdev = ondev->offdev;\n\tprog->aux->offload = offload;\n\tlist_add_tail(&offload->offloads, &ondev->progs);\n\n\treturn 0;\nerr_free:\n\tkfree(offload);\n\treturn err;\n}\n\nint bpf_prog_dev_bound_init(struct bpf_prog *prog, union bpf_attr *attr)\n{\n\tstruct net_device *netdev;\n\tint err;\n\n\tif (attr->prog_type != BPF_PROG_TYPE_SCHED_CLS &&\n\t    attr->prog_type != BPF_PROG_TYPE_XDP)\n\t\treturn -EINVAL;\n\n\tif (attr->prog_flags & ~BPF_F_XDP_DEV_BOUND_ONLY)\n\t\treturn -EINVAL;\n\n\tif (attr->prog_type == BPF_PROG_TYPE_SCHED_CLS &&\n\t    attr->prog_flags & BPF_F_XDP_DEV_BOUND_ONLY)\n\t\treturn -EINVAL;\n\n\tnetdev = dev_get_by_index(current->nsproxy->net_ns, attr->prog_ifindex);\n\tif (!netdev)\n\t\treturn -EINVAL;\n\n\terr = bpf_dev_offload_check(netdev);\n\tif (err)\n\t\tgoto out;\n\n\tprog->aux->offload_requested = !(attr->prog_flags & BPF_F_XDP_DEV_BOUND_ONLY);\n\n\tdown_write(&bpf_devs_lock);\n\terr = __bpf_prog_dev_bound_init(prog, netdev);\n\tup_write(&bpf_devs_lock);\n\nout:\n\tdev_put(netdev);\n\treturn err;\n}\n\nint bpf_prog_dev_bound_inherit(struct bpf_prog *new_prog, struct bpf_prog *old_prog)\n{\n\tint err;\n\n\tif (!bpf_prog_is_dev_bound(old_prog->aux))\n\t\treturn 0;\n\n\tif (bpf_prog_is_offloaded(old_prog->aux))\n\t\treturn -EINVAL;\n\n\tnew_prog->aux->dev_bound = old_prog->aux->dev_bound;\n\tnew_prog->aux->offload_requested = old_prog->aux->offload_requested;\n\n\tdown_write(&bpf_devs_lock);\n\tif (!old_prog->aux->offload) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = __bpf_prog_dev_bound_init(new_prog, old_prog->aux->offload->netdev);\n\nout:\n\tup_write(&bpf_devs_lock);\n\treturn err;\n}\n\nint bpf_prog_offload_verifier_prep(struct bpf_prog *prog)\n{\n\tstruct bpf_prog_offload *offload;\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\toffload = prog->aux->offload;\n\tif (offload) {\n\t\tret = offload->offdev->ops->prepare(prog);\n\t\toffload->dev_state = !ret;\n\t}\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nint bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,\n\t\t\t\t int insn_idx, int prev_insn_idx)\n{\n\tstruct bpf_prog_offload *offload;\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\toffload = env->prog->aux->offload;\n\tif (offload)\n\t\tret = offload->offdev->ops->insn_hook(env, insn_idx,\n\t\t\t\t\t\t      prev_insn_idx);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nint bpf_prog_offload_finalize(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_offload *offload;\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\toffload = env->prog->aux->offload;\n\tif (offload) {\n\t\tif (offload->offdev->ops->finalize)\n\t\t\tret = offload->offdev->ops->finalize(env);\n\t\telse\n\t\t\tret = 0;\n\t}\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nvoid\nbpf_prog_offload_replace_insn(struct bpf_verifier_env *env, u32 off,\n\t\t\t      struct bpf_insn *insn)\n{\n\tconst struct bpf_prog_offload_ops *ops;\n\tstruct bpf_prog_offload *offload;\n\tint ret = -EOPNOTSUPP;\n\n\tdown_read(&bpf_devs_lock);\n\toffload = env->prog->aux->offload;\n\tif (offload) {\n\t\tops = offload->offdev->ops;\n\t\tif (!offload->opt_failed && ops->replace_insn)\n\t\t\tret = ops->replace_insn(env, off, insn);\n\t\toffload->opt_failed |= ret;\n\t}\n\tup_read(&bpf_devs_lock);\n}\n\nvoid\nbpf_prog_offload_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt)\n{\n\tstruct bpf_prog_offload *offload;\n\tint ret = -EOPNOTSUPP;\n\n\tdown_read(&bpf_devs_lock);\n\toffload = env->prog->aux->offload;\n\tif (offload) {\n\t\tif (!offload->opt_failed && offload->offdev->ops->remove_insns)\n\t\t\tret = offload->offdev->ops->remove_insns(env, off, cnt);\n\t\toffload->opt_failed |= ret;\n\t}\n\tup_read(&bpf_devs_lock);\n}\n\nvoid bpf_prog_dev_bound_destroy(struct bpf_prog *prog)\n{\n\tstruct bpf_offload_netdev *ondev;\n\tstruct net_device *netdev;\n\n\trtnl_lock();\n\tdown_write(&bpf_devs_lock);\n\tif (prog->aux->offload) {\n\t\tlist_del_init(&prog->aux->offload->offloads);\n\n\t\tnetdev = prog->aux->offload->netdev;\n\t\t__bpf_prog_offload_destroy(prog);\n\n\t\tondev = bpf_offload_find_netdev(netdev);\n\t\tif (!ondev->offdev && list_empty(&ondev->progs))\n\t\t\t__bpf_offload_dev_netdev_unregister(NULL, netdev);\n\t}\n\tup_write(&bpf_devs_lock);\n\trtnl_unlock();\n}\n\nstatic int bpf_prog_offload_translate(struct bpf_prog *prog)\n{\n\tstruct bpf_prog_offload *offload;\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\toffload = prog->aux->offload;\n\tif (offload)\n\t\tret = offload->offdev->ops->translate(prog);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nstatic unsigned int bpf_prog_warn_on_exec(const void *ctx,\n\t\t\t\t\t  const struct bpf_insn *insn)\n{\n\tWARN(1, \"attempt to execute device eBPF program on the host!\");\n\treturn 0;\n}\n\nint bpf_prog_offload_compile(struct bpf_prog *prog)\n{\n\tprog->bpf_func = bpf_prog_warn_on_exec;\n\n\treturn bpf_prog_offload_translate(prog);\n}\n\nstruct ns_get_path_bpf_prog_args {\n\tstruct bpf_prog *prog;\n\tstruct bpf_prog_info *info;\n};\n\nstatic struct ns_common *bpf_prog_offload_info_fill_ns(void *private_data)\n{\n\tstruct ns_get_path_bpf_prog_args *args = private_data;\n\tstruct bpf_prog_aux *aux = args->prog->aux;\n\tstruct ns_common *ns;\n\tstruct net *net;\n\n\trtnl_lock();\n\tdown_read(&bpf_devs_lock);\n\n\tif (aux->offload) {\n\t\targs->info->ifindex = aux->offload->netdev->ifindex;\n\t\tnet = dev_net(aux->offload->netdev);\n\t\tget_net(net);\n\t\tns = &net->ns;\n\t} else {\n\t\targs->info->ifindex = 0;\n\t\tns = NULL;\n\t}\n\n\tup_read(&bpf_devs_lock);\n\trtnl_unlock();\n\n\treturn ns;\n}\n\nint bpf_prog_offload_info_fill(struct bpf_prog_info *info,\n\t\t\t       struct bpf_prog *prog)\n{\n\tstruct ns_get_path_bpf_prog_args args = {\n\t\t.prog\t= prog,\n\t\t.info\t= info,\n\t};\n\tstruct bpf_prog_aux *aux = prog->aux;\n\tstruct inode *ns_inode;\n\tstruct path ns_path;\n\tchar __user *uinsns;\n\tint res;\n\tu32 ulen;\n\n\tres = ns_get_path_cb(&ns_path, bpf_prog_offload_info_fill_ns, &args);\n\tif (res) {\n\t\tif (!info->ifindex)\n\t\t\treturn -ENODEV;\n\t\treturn res;\n\t}\n\n\tdown_read(&bpf_devs_lock);\n\n\tif (!aux->offload) {\n\t\tup_read(&bpf_devs_lock);\n\t\treturn -ENODEV;\n\t}\n\n\tulen = info->jited_prog_len;\n\tinfo->jited_prog_len = aux->offload->jited_len;\n\tif (info->jited_prog_len && ulen) {\n\t\tuinsns = u64_to_user_ptr(info->jited_prog_insns);\n\t\tulen = min_t(u32, info->jited_prog_len, ulen);\n\t\tif (copy_to_user(uinsns, aux->offload->jited_image, ulen)) {\n\t\t\tup_read(&bpf_devs_lock);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tup_read(&bpf_devs_lock);\n\n\tns_inode = ns_path.dentry->d_inode;\n\tinfo->netns_dev = new_encode_dev(ns_inode->i_sb->s_dev);\n\tinfo->netns_ino = ns_inode->i_ino;\n\tpath_put(&ns_path);\n\n\treturn 0;\n}\n\nconst struct bpf_prog_ops bpf_offload_prog_ops = {\n};\n\nstruct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr)\n{\n\tstruct net *net = current->nsproxy->net_ns;\n\tstruct bpf_offload_netdev *ondev;\n\tstruct bpf_offloaded_map *offmap;\n\tint err;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn ERR_PTR(-EPERM);\n\tif (attr->map_type != BPF_MAP_TYPE_ARRAY &&\n\t    attr->map_type != BPF_MAP_TYPE_HASH)\n\t\treturn ERR_PTR(-EINVAL);\n\n\toffmap = bpf_map_area_alloc(sizeof(*offmap), NUMA_NO_NODE);\n\tif (!offmap)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&offmap->map, attr);\n\n\trtnl_lock();\n\tdown_write(&bpf_devs_lock);\n\toffmap->netdev = __dev_get_by_index(net, attr->map_ifindex);\n\terr = bpf_dev_offload_check(offmap->netdev);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tondev = bpf_offload_find_netdev(offmap->netdev);\n\tif (!ondev) {\n\t\terr = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\terr = bpf_map_offload_ndo(offmap, BPF_OFFLOAD_MAP_ALLOC);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tlist_add_tail(&offmap->offloads, &ondev->maps);\n\tup_write(&bpf_devs_lock);\n\trtnl_unlock();\n\n\treturn &offmap->map;\n\nerr_unlock:\n\tup_write(&bpf_devs_lock);\n\trtnl_unlock();\n\tbpf_map_area_free(offmap);\n\treturn ERR_PTR(err);\n}\n\nvoid bpf_map_offload_map_free(struct bpf_map *map)\n{\n\tstruct bpf_offloaded_map *offmap = map_to_offmap(map);\n\n\trtnl_lock();\n\tdown_write(&bpf_devs_lock);\n\tif (offmap->netdev)\n\t\t__bpf_map_offload_destroy(offmap);\n\tup_write(&bpf_devs_lock);\n\trtnl_unlock();\n\n\tbpf_map_area_free(offmap);\n}\n\nu64 bpf_map_offload_map_mem_usage(const struct bpf_map *map)\n{\n\t \n\treturn sizeof(struct bpf_offloaded_map);\n}\n\nint bpf_map_offload_lookup_elem(struct bpf_map *map, void *key, void *value)\n{\n\tstruct bpf_offloaded_map *offmap = map_to_offmap(map);\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\tif (offmap->netdev)\n\t\tret = offmap->dev_ops->map_lookup_elem(offmap, key, value);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nint bpf_map_offload_update_elem(struct bpf_map *map,\n\t\t\t\tvoid *key, void *value, u64 flags)\n{\n\tstruct bpf_offloaded_map *offmap = map_to_offmap(map);\n\tint ret = -ENODEV;\n\n\tif (unlikely(flags > BPF_EXIST))\n\t\treturn -EINVAL;\n\n\tdown_read(&bpf_devs_lock);\n\tif (offmap->netdev)\n\t\tret = offmap->dev_ops->map_update_elem(offmap, key, value,\n\t\t\t\t\t\t       flags);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nint bpf_map_offload_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_offloaded_map *offmap = map_to_offmap(map);\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\tif (offmap->netdev)\n\t\tret = offmap->dev_ops->map_delete_elem(offmap, key);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nint bpf_map_offload_get_next_key(struct bpf_map *map, void *key, void *next_key)\n{\n\tstruct bpf_offloaded_map *offmap = map_to_offmap(map);\n\tint ret = -ENODEV;\n\n\tdown_read(&bpf_devs_lock);\n\tif (offmap->netdev)\n\t\tret = offmap->dev_ops->map_get_next_key(offmap, key, next_key);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nstruct ns_get_path_bpf_map_args {\n\tstruct bpf_offloaded_map *offmap;\n\tstruct bpf_map_info *info;\n};\n\nstatic struct ns_common *bpf_map_offload_info_fill_ns(void *private_data)\n{\n\tstruct ns_get_path_bpf_map_args *args = private_data;\n\tstruct ns_common *ns;\n\tstruct net *net;\n\n\trtnl_lock();\n\tdown_read(&bpf_devs_lock);\n\n\tif (args->offmap->netdev) {\n\t\targs->info->ifindex = args->offmap->netdev->ifindex;\n\t\tnet = dev_net(args->offmap->netdev);\n\t\tget_net(net);\n\t\tns = &net->ns;\n\t} else {\n\t\targs->info->ifindex = 0;\n\t\tns = NULL;\n\t}\n\n\tup_read(&bpf_devs_lock);\n\trtnl_unlock();\n\n\treturn ns;\n}\n\nint bpf_map_offload_info_fill(struct bpf_map_info *info, struct bpf_map *map)\n{\n\tstruct ns_get_path_bpf_map_args args = {\n\t\t.offmap\t= map_to_offmap(map),\n\t\t.info\t= info,\n\t};\n\tstruct inode *ns_inode;\n\tstruct path ns_path;\n\tint res;\n\n\tres = ns_get_path_cb(&ns_path, bpf_map_offload_info_fill_ns, &args);\n\tif (res) {\n\t\tif (!info->ifindex)\n\t\t\treturn -ENODEV;\n\t\treturn res;\n\t}\n\n\tns_inode = ns_path.dentry->d_inode;\n\tinfo->netns_dev = new_encode_dev(ns_inode->i_sb->s_dev);\n\tinfo->netns_ino = ns_inode->i_ino;\n\tpath_put(&ns_path);\n\n\treturn 0;\n}\n\nstatic bool __bpf_offload_dev_match(struct bpf_prog *prog,\n\t\t\t\t    struct net_device *netdev)\n{\n\tstruct bpf_offload_netdev *ondev1, *ondev2;\n\tstruct bpf_prog_offload *offload;\n\n\tif (!bpf_prog_is_dev_bound(prog->aux))\n\t\treturn false;\n\n\toffload = prog->aux->offload;\n\tif (!offload)\n\t\treturn false;\n\tif (offload->netdev == netdev)\n\t\treturn true;\n\n\tondev1 = bpf_offload_find_netdev(offload->netdev);\n\tondev2 = bpf_offload_find_netdev(netdev);\n\n\treturn ondev1 && ondev2 && ondev1->offdev == ondev2->offdev;\n}\n\nbool bpf_offload_dev_match(struct bpf_prog *prog, struct net_device *netdev)\n{\n\tbool ret;\n\n\tdown_read(&bpf_devs_lock);\n\tret = __bpf_offload_dev_match(prog, netdev);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(bpf_offload_dev_match);\n\nbool bpf_prog_dev_bound_match(const struct bpf_prog *lhs, const struct bpf_prog *rhs)\n{\n\tbool ret;\n\n\tif (bpf_prog_is_offloaded(lhs->aux) != bpf_prog_is_offloaded(rhs->aux))\n\t\treturn false;\n\n\tdown_read(&bpf_devs_lock);\n\tret = lhs->aux->offload && rhs->aux->offload &&\n\t      lhs->aux->offload->netdev &&\n\t      lhs->aux->offload->netdev == rhs->aux->offload->netdev;\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nbool bpf_offload_prog_map_match(struct bpf_prog *prog, struct bpf_map *map)\n{\n\tstruct bpf_offloaded_map *offmap;\n\tbool ret;\n\n\tif (!bpf_map_is_offloaded(map))\n\t\treturn bpf_map_offload_neutral(map);\n\toffmap = map_to_offmap(map);\n\n\tdown_read(&bpf_devs_lock);\n\tret = __bpf_offload_dev_match(prog, offmap->netdev);\n\tup_read(&bpf_devs_lock);\n\n\treturn ret;\n}\n\nint bpf_offload_dev_netdev_register(struct bpf_offload_dev *offdev,\n\t\t\t\t    struct net_device *netdev)\n{\n\tint err;\n\n\tdown_write(&bpf_devs_lock);\n\terr = __bpf_offload_dev_netdev_register(offdev, netdev);\n\tup_write(&bpf_devs_lock);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(bpf_offload_dev_netdev_register);\n\nvoid bpf_offload_dev_netdev_unregister(struct bpf_offload_dev *offdev,\n\t\t\t\t       struct net_device *netdev)\n{\n\tdown_write(&bpf_devs_lock);\n\t__bpf_offload_dev_netdev_unregister(offdev, netdev);\n\tup_write(&bpf_devs_lock);\n}\nEXPORT_SYMBOL_GPL(bpf_offload_dev_netdev_unregister);\n\nstruct bpf_offload_dev *\nbpf_offload_dev_create(const struct bpf_prog_offload_ops *ops, void *priv)\n{\n\tstruct bpf_offload_dev *offdev;\n\n\toffdev = kzalloc(sizeof(*offdev), GFP_KERNEL);\n\tif (!offdev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\toffdev->ops = ops;\n\toffdev->priv = priv;\n\tINIT_LIST_HEAD(&offdev->netdevs);\n\n\treturn offdev;\n}\nEXPORT_SYMBOL_GPL(bpf_offload_dev_create);\n\nvoid bpf_offload_dev_destroy(struct bpf_offload_dev *offdev)\n{\n\tWARN_ON(!list_empty(&offdev->netdevs));\n\tkfree(offdev);\n}\nEXPORT_SYMBOL_GPL(bpf_offload_dev_destroy);\n\nvoid *bpf_offload_dev_priv(struct bpf_offload_dev *offdev)\n{\n\treturn offdev->priv;\n}\nEXPORT_SYMBOL_GPL(bpf_offload_dev_priv);\n\nvoid bpf_dev_bound_netdev_unregister(struct net_device *dev)\n{\n\tstruct bpf_offload_netdev *ondev;\n\n\tASSERT_RTNL();\n\n\tdown_write(&bpf_devs_lock);\n\tondev = bpf_offload_find_netdev(dev);\n\tif (ondev && !ondev->offdev)\n\t\t__bpf_offload_dev_netdev_unregister(NULL, ondev->netdev);\n\tup_write(&bpf_devs_lock);\n}\n\nint bpf_dev_bound_kfunc_check(struct bpf_verifier_log *log,\n\t\t\t      struct bpf_prog_aux *prog_aux)\n{\n\tif (!bpf_prog_is_dev_bound(prog_aux)) {\n\t\tbpf_log(log, \"metadata kfuncs require device-bound program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (bpf_prog_is_offloaded(prog_aux)) {\n\t\tbpf_log(log, \"metadata kfuncs can't be offloaded\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid *bpf_dev_bound_resolve_kfunc(struct bpf_prog *prog, u32 func_id)\n{\n\tconst struct xdp_metadata_ops *ops;\n\tvoid *p = NULL;\n\n\t \n\tdown_read(&bpf_devs_lock);\n\tif (!prog->aux->offload)\n\t\tgoto out;\n\n\tops = prog->aux->offload->netdev->xdp_metadata_ops;\n\tif (!ops)\n\t\tgoto out;\n\n\tif (func_id == bpf_xdp_metadata_kfunc_id(XDP_METADATA_KFUNC_RX_TIMESTAMP))\n\t\tp = ops->xmo_rx_timestamp;\n\telse if (func_id == bpf_xdp_metadata_kfunc_id(XDP_METADATA_KFUNC_RX_HASH))\n\t\tp = ops->xmo_rx_hash;\nout:\n\tup_read(&bpf_devs_lock);\n\n\treturn p;\n}\n\nstatic int __init bpf_offload_init(void)\n{\n\treturn rhashtable_init(&offdevs, &offdevs_params);\n}\n\ncore_initcall(bpf_offload_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}