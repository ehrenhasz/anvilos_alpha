{
  "module_name": "helpers.c",
  "hash_id": "9f8900263a5f5596b8bb02a2a85f8479a0b837f804e6fabe9d9bb042cbe0c472",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/helpers.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/cgroup.h>\n#include <linux/rcupdate.h>\n#include <linux/random.h>\n#include <linux/smp.h>\n#include <linux/topology.h>\n#include <linux/ktime.h>\n#include <linux/sched.h>\n#include <linux/uidgid.h>\n#include <linux/filter.h>\n#include <linux/ctype.h>\n#include <linux/jiffies.h>\n#include <linux/pid_namespace.h>\n#include <linux/poison.h>\n#include <linux/proc_ns.h>\n#include <linux/sched/task.h>\n#include <linux/security.h>\n#include <linux/btf_ids.h>\n#include <linux/bpf_mem_alloc.h>\n\n#include \"../../lib/kstrtox.h\"\n\n \nBPF_CALL_2(bpf_map_lookup_elem, struct bpf_map *, map, void *, key)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\treturn (unsigned long) map->ops->map_lookup_elem(map, key);\n}\n\nconst struct bpf_func_proto bpf_map_lookup_elem_proto = {\n\t.func\t\t= bpf_map_lookup_elem,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_KEY,\n};\n\nBPF_CALL_4(bpf_map_update_elem, struct bpf_map *, map, void *, key,\n\t   void *, value, u64, flags)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\treturn map->ops->map_update_elem(map, key, value, flags);\n}\n\nconst struct bpf_func_proto bpf_map_update_elem_proto = {\n\t.func\t\t= bpf_map_update_elem,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_KEY,\n\t.arg3_type\t= ARG_PTR_TO_MAP_VALUE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_map_delete_elem, struct bpf_map *, map, void *, key)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\treturn map->ops->map_delete_elem(map, key);\n}\n\nconst struct bpf_func_proto bpf_map_delete_elem_proto = {\n\t.func\t\t= bpf_map_delete_elem,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_KEY,\n};\n\nBPF_CALL_3(bpf_map_push_elem, struct bpf_map *, map, void *, value, u64, flags)\n{\n\treturn map->ops->map_push_elem(map, value, flags);\n}\n\nconst struct bpf_func_proto bpf_map_push_elem_proto = {\n\t.func\t\t= bpf_map_push_elem,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_VALUE,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_2(bpf_map_pop_elem, struct bpf_map *, map, void *, value)\n{\n\treturn map->ops->map_pop_elem(map, value);\n}\n\nconst struct bpf_func_proto bpf_map_pop_elem_proto = {\n\t.func\t\t= bpf_map_pop_elem,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_VALUE | MEM_UNINIT,\n};\n\nBPF_CALL_2(bpf_map_peek_elem, struct bpf_map *, map, void *, value)\n{\n\treturn map->ops->map_peek_elem(map, value);\n}\n\nconst struct bpf_func_proto bpf_map_peek_elem_proto = {\n\t.func\t\t= bpf_map_peek_elem,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_VALUE | MEM_UNINIT,\n};\n\nBPF_CALL_3(bpf_map_lookup_percpu_elem, struct bpf_map *, map, void *, key, u32, cpu)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\treturn (unsigned long) map->ops->map_lookup_percpu_elem(map, key, cpu);\n}\n\nconst struct bpf_func_proto bpf_map_lookup_percpu_elem_proto = {\n\t.func\t\t= bpf_map_lookup_percpu_elem,\n\t.gpl_only\t= false,\n\t.pkt_access\t= true,\n\t.ret_type\t= RET_PTR_TO_MAP_VALUE_OR_NULL,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_PTR_TO_MAP_KEY,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nconst struct bpf_func_proto bpf_get_prandom_u32_proto = {\n\t.func\t\t= bpf_user_rnd_u32,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_get_smp_processor_id)\n{\n\treturn smp_processor_id();\n}\n\nconst struct bpf_func_proto bpf_get_smp_processor_id_proto = {\n\t.func\t\t= bpf_get_smp_processor_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_get_numa_node_id)\n{\n\treturn numa_node_id();\n}\n\nconst struct bpf_func_proto bpf_get_numa_node_id_proto = {\n\t.func\t\t= bpf_get_numa_node_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_ktime_get_ns)\n{\n\t \n\treturn ktime_get_mono_fast_ns();\n}\n\nconst struct bpf_func_proto bpf_ktime_get_ns_proto = {\n\t.func\t\t= bpf_ktime_get_ns,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_ktime_get_boot_ns)\n{\n\t \n\treturn ktime_get_boot_fast_ns();\n}\n\nconst struct bpf_func_proto bpf_ktime_get_boot_ns_proto = {\n\t.func\t\t= bpf_ktime_get_boot_ns,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_ktime_get_coarse_ns)\n{\n\treturn ktime_get_coarse_ns();\n}\n\nconst struct bpf_func_proto bpf_ktime_get_coarse_ns_proto = {\n\t.func\t\t= bpf_ktime_get_coarse_ns,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_ktime_get_tai_ns)\n{\n\t \n\treturn ktime_get_tai_fast_ns();\n}\n\nconst struct bpf_func_proto bpf_ktime_get_tai_ns_proto = {\n\t.func\t\t= bpf_ktime_get_tai_ns,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_get_current_pid_tgid)\n{\n\tstruct task_struct *task = current;\n\n\tif (unlikely(!task))\n\t\treturn -EINVAL;\n\n\treturn (u64) task->tgid << 32 | task->pid;\n}\n\nconst struct bpf_func_proto bpf_get_current_pid_tgid_proto = {\n\t.func\t\t= bpf_get_current_pid_tgid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_get_current_uid_gid)\n{\n\tstruct task_struct *task = current;\n\tkuid_t uid;\n\tkgid_t gid;\n\n\tif (unlikely(!task))\n\t\treturn -EINVAL;\n\n\tcurrent_uid_gid(&uid, &gid);\n\treturn (u64) from_kgid(&init_user_ns, gid) << 32 |\n\t\t     from_kuid(&init_user_ns, uid);\n}\n\nconst struct bpf_func_proto bpf_get_current_uid_gid_proto = {\n\t.func\t\t= bpf_get_current_uid_gid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_2(bpf_get_current_comm, char *, buf, u32, size)\n{\n\tstruct task_struct *task = current;\n\n\tif (unlikely(!task))\n\t\tgoto err_clear;\n\n\t \n\tstrscpy_pad(buf, task->comm, size);\n\treturn 0;\nerr_clear:\n\tmemset(buf, 0, size);\n\treturn -EINVAL;\n}\n\nconst struct bpf_func_proto bpf_get_current_comm_proto = {\n\t.func\t\t= bpf_get_current_comm,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE,\n};\n\n#if defined(CONFIG_QUEUED_SPINLOCKS) || defined(CONFIG_BPF_ARCH_SPINLOCK)\n\nstatic inline void __bpf_spin_lock(struct bpf_spin_lock *lock)\n{\n\tarch_spinlock_t *l = (void *)lock;\n\tunion {\n\t\t__u32 val;\n\t\tarch_spinlock_t lock;\n\t} u = { .lock = __ARCH_SPIN_LOCK_UNLOCKED };\n\n\tcompiletime_assert(u.val == 0, \"__ARCH_SPIN_LOCK_UNLOCKED not 0\");\n\tBUILD_BUG_ON(sizeof(*l) != sizeof(__u32));\n\tBUILD_BUG_ON(sizeof(*lock) != sizeof(__u32));\n\tpreempt_disable();\n\tarch_spin_lock(l);\n}\n\nstatic inline void __bpf_spin_unlock(struct bpf_spin_lock *lock)\n{\n\tarch_spinlock_t *l = (void *)lock;\n\n\tarch_spin_unlock(l);\n\tpreempt_enable();\n}\n\n#else\n\nstatic inline void __bpf_spin_lock(struct bpf_spin_lock *lock)\n{\n\tatomic_t *l = (void *)lock;\n\n\tBUILD_BUG_ON(sizeof(*l) != sizeof(*lock));\n\tdo {\n\t\tatomic_cond_read_relaxed(l, !VAL);\n\t} while (atomic_xchg(l, 1));\n}\n\nstatic inline void __bpf_spin_unlock(struct bpf_spin_lock *lock)\n{\n\tatomic_t *l = (void *)lock;\n\n\tatomic_set_release(l, 0);\n}\n\n#endif\n\nstatic DEFINE_PER_CPU(unsigned long, irqsave_flags);\n\nstatic inline void __bpf_spin_lock_irqsave(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__bpf_spin_lock(lock);\n\t__this_cpu_write(irqsave_flags, flags);\n}\n\nnotrace BPF_CALL_1(bpf_spin_lock, struct bpf_spin_lock *, lock)\n{\n\t__bpf_spin_lock_irqsave(lock);\n\treturn 0;\n}\n\nconst struct bpf_func_proto bpf_spin_lock_proto = {\n\t.func\t\t= bpf_spin_lock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_VOID,\n\t.arg1_type\t= ARG_PTR_TO_SPIN_LOCK,\n\t.arg1_btf_id    = BPF_PTR_POISON,\n};\n\nstatic inline void __bpf_spin_unlock_irqrestore(struct bpf_spin_lock *lock)\n{\n\tunsigned long flags;\n\n\tflags = __this_cpu_read(irqsave_flags);\n\t__bpf_spin_unlock(lock);\n\tlocal_irq_restore(flags);\n}\n\nnotrace BPF_CALL_1(bpf_spin_unlock, struct bpf_spin_lock *, lock)\n{\n\t__bpf_spin_unlock_irqrestore(lock);\n\treturn 0;\n}\n\nconst struct bpf_func_proto bpf_spin_unlock_proto = {\n\t.func\t\t= bpf_spin_unlock,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_VOID,\n\t.arg1_type\t= ARG_PTR_TO_SPIN_LOCK,\n\t.arg1_btf_id    = BPF_PTR_POISON,\n};\n\nvoid copy_map_value_locked(struct bpf_map *map, void *dst, void *src,\n\t\t\t   bool lock_src)\n{\n\tstruct bpf_spin_lock *lock;\n\n\tif (lock_src)\n\t\tlock = src + map->record->spin_lock_off;\n\telse\n\t\tlock = dst + map->record->spin_lock_off;\n\tpreempt_disable();\n\t__bpf_spin_lock_irqsave(lock);\n\tcopy_map_value(map, dst, src);\n\t__bpf_spin_unlock_irqrestore(lock);\n\tpreempt_enable();\n}\n\nBPF_CALL_0(bpf_jiffies64)\n{\n\treturn get_jiffies_64();\n}\n\nconst struct bpf_func_proto bpf_jiffies64_proto = {\n\t.func\t\t= bpf_jiffies64,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\n#ifdef CONFIG_CGROUPS\nBPF_CALL_0(bpf_get_current_cgroup_id)\n{\n\tstruct cgroup *cgrp;\n\tu64 cgrp_id;\n\n\trcu_read_lock();\n\tcgrp = task_dfl_cgroup(current);\n\tcgrp_id = cgroup_id(cgrp);\n\trcu_read_unlock();\n\n\treturn cgrp_id;\n}\n\nconst struct bpf_func_proto bpf_get_current_cgroup_id_proto = {\n\t.func\t\t= bpf_get_current_cgroup_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_1(bpf_get_current_ancestor_cgroup_id, int, ancestor_level)\n{\n\tstruct cgroup *cgrp;\n\tstruct cgroup *ancestor;\n\tu64 cgrp_id;\n\n\trcu_read_lock();\n\tcgrp = task_dfl_cgroup(current);\n\tancestor = cgroup_ancestor(cgrp, ancestor_level);\n\tcgrp_id = ancestor ? cgroup_id(ancestor) : 0;\n\trcu_read_unlock();\n\n\treturn cgrp_id;\n}\n\nconst struct bpf_func_proto bpf_get_current_ancestor_cgroup_id_proto = {\n\t.func\t\t= bpf_get_current_ancestor_cgroup_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n};\n#endif  \n\n#define BPF_STRTOX_BASE_MASK 0x1F\n\nstatic int __bpf_strtoull(const char *buf, size_t buf_len, u64 flags,\n\t\t\t  unsigned long long *res, bool *is_negative)\n{\n\tunsigned int base = flags & BPF_STRTOX_BASE_MASK;\n\tconst char *cur_buf = buf;\n\tsize_t cur_len = buf_len;\n\tunsigned int consumed;\n\tsize_t val_len;\n\tchar str[64];\n\n\tif (!buf || !buf_len || !res || !is_negative)\n\t\treturn -EINVAL;\n\n\tif (base != 0 && base != 8 && base != 10 && base != 16)\n\t\treturn -EINVAL;\n\n\tif (flags & ~BPF_STRTOX_BASE_MASK)\n\t\treturn -EINVAL;\n\n\twhile (cur_buf < buf + buf_len && isspace(*cur_buf))\n\t\t++cur_buf;\n\n\t*is_negative = (cur_buf < buf + buf_len && *cur_buf == '-');\n\tif (*is_negative)\n\t\t++cur_buf;\n\n\tconsumed = cur_buf - buf;\n\tcur_len -= consumed;\n\tif (!cur_len)\n\t\treturn -EINVAL;\n\n\tcur_len = min(cur_len, sizeof(str) - 1);\n\tmemcpy(str, cur_buf, cur_len);\n\tstr[cur_len] = '\\0';\n\tcur_buf = str;\n\n\tcur_buf = _parse_integer_fixup_radix(cur_buf, &base);\n\tval_len = _parse_integer(cur_buf, base, res);\n\n\tif (val_len & KSTRTOX_OVERFLOW)\n\t\treturn -ERANGE;\n\n\tif (val_len == 0)\n\t\treturn -EINVAL;\n\n\tcur_buf += val_len;\n\tconsumed += cur_buf - str;\n\n\treturn consumed;\n}\n\nstatic int __bpf_strtoll(const char *buf, size_t buf_len, u64 flags,\n\t\t\t long long *res)\n{\n\tunsigned long long _res;\n\tbool is_negative;\n\tint err;\n\n\terr = __bpf_strtoull(buf, buf_len, flags, &_res, &is_negative);\n\tif (err < 0)\n\t\treturn err;\n\tif (is_negative) {\n\t\tif ((long long)-_res > 0)\n\t\t\treturn -ERANGE;\n\t\t*res = -_res;\n\t} else {\n\t\tif ((long long)_res < 0)\n\t\t\treturn -ERANGE;\n\t\t*res = _res;\n\t}\n\treturn err;\n}\n\nBPF_CALL_4(bpf_strtol, const char *, buf, size_t, buf_len, u64, flags,\n\t   long *, res)\n{\n\tlong long _res;\n\tint err;\n\n\terr = __bpf_strtoll(buf, buf_len, flags, &_res);\n\tif (err < 0)\n\t\treturn err;\n\tif (_res != (long)_res)\n\t\treturn -ERANGE;\n\t*res = _res;\n\treturn err;\n}\n\nconst struct bpf_func_proto bpf_strtol_proto = {\n\t.func\t\t= bpf_strtol,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg2_type\t= ARG_CONST_SIZE,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_LONG,\n};\n\nBPF_CALL_4(bpf_strtoul, const char *, buf, size_t, buf_len, u64, flags,\n\t   unsigned long *, res)\n{\n\tunsigned long long _res;\n\tbool is_negative;\n\tint err;\n\n\terr = __bpf_strtoull(buf, buf_len, flags, &_res, &is_negative);\n\tif (err < 0)\n\t\treturn err;\n\tif (is_negative)\n\t\treturn -EINVAL;\n\tif (_res != (unsigned long)_res)\n\t\treturn -ERANGE;\n\t*res = _res;\n\treturn err;\n}\n\nconst struct bpf_func_proto bpf_strtoul_proto = {\n\t.func\t\t= bpf_strtoul,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg2_type\t= ARG_CONST_SIZE,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_LONG,\n};\n\nBPF_CALL_3(bpf_strncmp, const char *, s1, u32, s1_sz, const char *, s2)\n{\n\treturn strncmp(s1, s2, s1_sz);\n}\n\nstatic const struct bpf_func_proto bpf_strncmp_proto = {\n\t.func\t\t= bpf_strncmp,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg2_type\t= ARG_CONST_SIZE,\n\t.arg3_type\t= ARG_PTR_TO_CONST_STR,\n};\n\nBPF_CALL_4(bpf_get_ns_current_pid_tgid, u64, dev, u64, ino,\n\t   struct bpf_pidns_info *, nsdata, u32, size)\n{\n\tstruct task_struct *task = current;\n\tstruct pid_namespace *pidns;\n\tint err = -EINVAL;\n\n\tif (unlikely(size != sizeof(struct bpf_pidns_info)))\n\t\tgoto clear;\n\n\tif (unlikely((u64)(dev_t)dev != dev))\n\t\tgoto clear;\n\n\tif (unlikely(!task))\n\t\tgoto clear;\n\n\tpidns = task_active_pid_ns(task);\n\tif (unlikely(!pidns)) {\n\t\terr = -ENOENT;\n\t\tgoto clear;\n\t}\n\n\tif (!ns_match(&pidns->ns, (dev_t)dev, ino))\n\t\tgoto clear;\n\n\tnsdata->pid = task_pid_nr_ns(task, pidns);\n\tnsdata->tgid = task_tgid_nr_ns(task, pidns);\n\treturn 0;\nclear:\n\tmemset((void *)nsdata, 0, (size_t) size);\n\treturn err;\n}\n\nconst struct bpf_func_proto bpf_get_ns_current_pid_tgid_proto = {\n\t.func\t\t= bpf_get_ns_current_pid_tgid,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type      = ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type      = ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto bpf_get_raw_smp_processor_id_proto = {\n\t.func\t\t= bpf_get_raw_cpu_id,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_5(bpf_event_output_data, void *, ctx, struct bpf_map *, map,\n\t   u64, flags, void *, data, u64, size)\n{\n\tif (unlikely(flags & ~(BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\n\treturn bpf_event_output(map, flags, data, size, NULL, 0, NULL);\n}\n\nconst struct bpf_func_proto bpf_event_output_data_proto =  {\n\t.func\t\t= bpf_event_output_data,\n\t.gpl_only       = true,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_CONST_MAP_PTR,\n\t.arg3_type      = ARG_ANYTHING,\n\t.arg4_type      = ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type      = ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_3(bpf_copy_from_user, void *, dst, u32, size,\n\t   const void __user *, user_ptr)\n{\n\tint ret = copy_from_user(dst, user_ptr, size);\n\n\tif (unlikely(ret)) {\n\t\tmemset(dst, 0, size);\n\t\tret = -EFAULT;\n\t}\n\n\treturn ret;\n}\n\nconst struct bpf_func_proto bpf_copy_from_user_proto = {\n\t.func\t\t= bpf_copy_from_user,\n\t.gpl_only\t= false,\n\t.might_sleep\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_copy_from_user_task, void *, dst, u32, size,\n\t   const void __user *, user_ptr, struct task_struct *, tsk, u64, flags)\n{\n\tint ret;\n\n\t \n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!size))\n\t\treturn 0;\n\n\tret = access_process_vm(tsk, (unsigned long)user_ptr, dst, size, 0);\n\tif (ret == size)\n\t\treturn 0;\n\n\tmemset(dst, 0, size);\n\t \n\treturn ret < 0 ? ret : -EFAULT;\n}\n\nconst struct bpf_func_proto bpf_copy_from_user_task_proto = {\n\t.func\t\t= bpf_copy_from_user_task,\n\t.gpl_only\t= true,\n\t.might_sleep\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg4_btf_id\t= &btf_tracing_ids[BTF_TRACING_TYPE_TASK],\n\t.arg5_type\t= ARG_ANYTHING\n};\n\nBPF_CALL_2(bpf_per_cpu_ptr, const void *, ptr, u32, cpu)\n{\n\tif (cpu >= nr_cpu_ids)\n\t\treturn (unsigned long)NULL;\n\n\treturn (unsigned long)per_cpu_ptr((const void __percpu *)ptr, cpu);\n}\n\nconst struct bpf_func_proto bpf_per_cpu_ptr_proto = {\n\t.func\t\t= bpf_per_cpu_ptr,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_MEM_OR_BTF_ID | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg1_type\t= ARG_PTR_TO_PERCPU_BTF_ID,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_this_cpu_ptr, const void *, percpu_ptr)\n{\n\treturn (unsigned long)this_cpu_ptr((const void __percpu *)percpu_ptr);\n}\n\nconst struct bpf_func_proto bpf_this_cpu_ptr_proto = {\n\t.func\t\t= bpf_this_cpu_ptr,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_MEM_OR_BTF_ID | MEM_RDONLY,\n\t.arg1_type\t= ARG_PTR_TO_PERCPU_BTF_ID,\n};\n\nstatic int bpf_trace_copy_string(char *buf, void *unsafe_ptr, char fmt_ptype,\n\t\tsize_t bufsz)\n{\n\tvoid __user *user_ptr = (__force void __user *)unsafe_ptr;\n\n\tbuf[0] = 0;\n\n\tswitch (fmt_ptype) {\n\tcase 's':\n#ifdef CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE\n\t\tif ((unsigned long)unsafe_ptr < TASK_SIZE)\n\t\t\treturn strncpy_from_user_nofault(buf, user_ptr, bufsz);\n\t\tfallthrough;\n#endif\n\tcase 'k':\n\t\treturn strncpy_from_kernel_nofault(buf, unsafe_ptr, bufsz);\n\tcase 'u':\n\t\treturn strncpy_from_user_nofault(buf, user_ptr, bufsz);\n\t}\n\n\treturn -EINVAL;\n}\n\n \n#define MAX_BPRINTF_BIN_ARGS\t512\n\n \n#define MAX_BPRINTF_NEST_LEVEL\t3\nstruct bpf_bprintf_buffers {\n\tchar bin_args[MAX_BPRINTF_BIN_ARGS];\n\tchar buf[MAX_BPRINTF_BUF];\n};\n\nstatic DEFINE_PER_CPU(struct bpf_bprintf_buffers[MAX_BPRINTF_NEST_LEVEL], bpf_bprintf_bufs);\nstatic DEFINE_PER_CPU(int, bpf_bprintf_nest_level);\n\nstatic int try_get_buffers(struct bpf_bprintf_buffers **bufs)\n{\n\tint nest_level;\n\n\tpreempt_disable();\n\tnest_level = this_cpu_inc_return(bpf_bprintf_nest_level);\n\tif (WARN_ON_ONCE(nest_level > MAX_BPRINTF_NEST_LEVEL)) {\n\t\tthis_cpu_dec(bpf_bprintf_nest_level);\n\t\tpreempt_enable();\n\t\treturn -EBUSY;\n\t}\n\t*bufs = this_cpu_ptr(&bpf_bprintf_bufs[nest_level - 1]);\n\n\treturn 0;\n}\n\nvoid bpf_bprintf_cleanup(struct bpf_bprintf_data *data)\n{\n\tif (!data->bin_args && !data->buf)\n\t\treturn;\n\tif (WARN_ON_ONCE(this_cpu_read(bpf_bprintf_nest_level) == 0))\n\t\treturn;\n\tthis_cpu_dec(bpf_bprintf_nest_level);\n\tpreempt_enable();\n}\n\n \nint bpf_bprintf_prepare(char *fmt, u32 fmt_size, const u64 *raw_args,\n\t\t\tu32 num_args, struct bpf_bprintf_data *data)\n{\n\tbool get_buffers = (data->get_bin_args && num_args) || data->get_buf;\n\tchar *unsafe_ptr = NULL, *tmp_buf = NULL, *tmp_buf_end, *fmt_end;\n\tstruct bpf_bprintf_buffers *buffers = NULL;\n\tsize_t sizeof_cur_arg, sizeof_cur_ip;\n\tint err, i, num_spec = 0;\n\tu64 cur_arg;\n\tchar fmt_ptype, cur_ip[16], ip_spec[] = \"%pXX\";\n\n\tfmt_end = strnchr(fmt, fmt_size, 0);\n\tif (!fmt_end)\n\t\treturn -EINVAL;\n\tfmt_size = fmt_end - fmt;\n\n\tif (get_buffers && try_get_buffers(&buffers))\n\t\treturn -EBUSY;\n\n\tif (data->get_bin_args) {\n\t\tif (num_args)\n\t\t\ttmp_buf = buffers->bin_args;\n\t\ttmp_buf_end = tmp_buf + MAX_BPRINTF_BIN_ARGS;\n\t\tdata->bin_args = (u32 *)tmp_buf;\n\t}\n\n\tif (data->get_buf)\n\t\tdata->buf = buffers->buf;\n\n\tfor (i = 0; i < fmt_size; i++) {\n\t\tif ((!isprint(fmt[i]) && !isspace(fmt[i])) || !isascii(fmt[i])) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (fmt[i] != '%')\n\t\t\tcontinue;\n\n\t\tif (fmt[i + 1] == '%') {\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (num_spec >= num_args) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\ti++;\n\n\t\t \n\t\twhile (fmt[i] == '0' || fmt[i] == '+'  || fmt[i] == '-' ||\n\t\t       fmt[i] == ' ')\n\t\t\ti++;\n\t\tif (fmt[i] >= '1' && fmt[i] <= '9') {\n\t\t\ti++;\n\t\t\twhile (fmt[i] >= '0' && fmt[i] <= '9')\n\t\t\t\ti++;\n\t\t}\n\n\t\tif (fmt[i] == 'p') {\n\t\t\tsizeof_cur_arg = sizeof(long);\n\n\t\t\tif ((fmt[i + 1] == 'k' || fmt[i + 1] == 'u') &&\n\t\t\t    fmt[i + 2] == 's') {\n\t\t\t\tfmt_ptype = fmt[i + 1];\n\t\t\t\ti += 2;\n\t\t\t\tgoto fmt_str;\n\t\t\t}\n\n\t\t\tif (fmt[i + 1] == 0 || isspace(fmt[i + 1]) ||\n\t\t\t    ispunct(fmt[i + 1]) || fmt[i + 1] == 'K' ||\n\t\t\t    fmt[i + 1] == 'x' || fmt[i + 1] == 's' ||\n\t\t\t    fmt[i + 1] == 'S') {\n\t\t\t\t \n\t\t\t\tif (tmp_buf)\n\t\t\t\t\tcur_arg = raw_args[num_spec];\n\t\t\t\ti++;\n\t\t\t\tgoto nocopy_fmt;\n\t\t\t}\n\n\t\t\tif (fmt[i + 1] == 'B') {\n\t\t\t\tif (tmp_buf)  {\n\t\t\t\t\terr = snprintf(tmp_buf,\n\t\t\t\t\t\t       (tmp_buf_end - tmp_buf),\n\t\t\t\t\t\t       \"%pB\",\n\t\t\t\t\t\t       (void *)(long)raw_args[num_spec]);\n\t\t\t\t\ttmp_buf += (err + 1);\n\t\t\t\t}\n\n\t\t\t\ti++;\n\t\t\t\tnum_spec++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif ((fmt[i + 1] != 'i' && fmt[i + 1] != 'I') ||\n\t\t\t    (fmt[i + 2] != '4' && fmt[i + 2] != '6')) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\ti += 2;\n\t\t\tif (!tmp_buf)\n\t\t\t\tgoto nocopy_fmt;\n\n\t\t\tsizeof_cur_ip = (fmt[i] == '4') ? 4 : 16;\n\t\t\tif (tmp_buf_end - tmp_buf < sizeof_cur_ip) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tunsafe_ptr = (char *)(long)raw_args[num_spec];\n\t\t\terr = copy_from_kernel_nofault(cur_ip, unsafe_ptr,\n\t\t\t\t\t\t       sizeof_cur_ip);\n\t\t\tif (err < 0)\n\t\t\t\tmemset(cur_ip, 0, sizeof_cur_ip);\n\n\t\t\t \n\t\t\tip_spec[2] = fmt[i - 1];\n\t\t\tip_spec[3] = fmt[i];\n\t\t\terr = snprintf(tmp_buf, tmp_buf_end - tmp_buf,\n\t\t\t\t       ip_spec, &cur_ip);\n\n\t\t\ttmp_buf += err + 1;\n\t\t\tnum_spec++;\n\n\t\t\tcontinue;\n\t\t} else if (fmt[i] == 's') {\n\t\t\tfmt_ptype = fmt[i];\nfmt_str:\n\t\t\tif (fmt[i + 1] != 0 &&\n\t\t\t    !isspace(fmt[i + 1]) &&\n\t\t\t    !ispunct(fmt[i + 1])) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (!tmp_buf)\n\t\t\t\tgoto nocopy_fmt;\n\n\t\t\tif (tmp_buf_end == tmp_buf) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tunsafe_ptr = (char *)(long)raw_args[num_spec];\n\t\t\terr = bpf_trace_copy_string(tmp_buf, unsafe_ptr,\n\t\t\t\t\t\t    fmt_ptype,\n\t\t\t\t\t\t    tmp_buf_end - tmp_buf);\n\t\t\tif (err < 0) {\n\t\t\t\ttmp_buf[0] = '\\0';\n\t\t\t\terr = 1;\n\t\t\t}\n\n\t\t\ttmp_buf += err;\n\t\t\tnum_spec++;\n\n\t\t\tcontinue;\n\t\t} else if (fmt[i] == 'c') {\n\t\t\tif (!tmp_buf)\n\t\t\t\tgoto nocopy_fmt;\n\n\t\t\tif (tmp_buf_end == tmp_buf) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t*tmp_buf = raw_args[num_spec];\n\t\t\ttmp_buf++;\n\t\t\tnum_spec++;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tsizeof_cur_arg = sizeof(int);\n\n\t\tif (fmt[i] == 'l') {\n\t\t\tsizeof_cur_arg = sizeof(long);\n\t\t\ti++;\n\t\t}\n\t\tif (fmt[i] == 'l') {\n\t\t\tsizeof_cur_arg = sizeof(long long);\n\t\t\ti++;\n\t\t}\n\n\t\tif (fmt[i] != 'i' && fmt[i] != 'd' && fmt[i] != 'u' &&\n\t\t    fmt[i] != 'x' && fmt[i] != 'X') {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (tmp_buf)\n\t\t\tcur_arg = raw_args[num_spec];\nnocopy_fmt:\n\t\tif (tmp_buf) {\n\t\t\ttmp_buf = PTR_ALIGN(tmp_buf, sizeof(u32));\n\t\t\tif (tmp_buf_end - tmp_buf < sizeof_cur_arg) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (sizeof_cur_arg == 8) {\n\t\t\t\t*(u32 *)tmp_buf = *(u32 *)&cur_arg;\n\t\t\t\t*(u32 *)(tmp_buf + 4) = *((u32 *)&cur_arg + 1);\n\t\t\t} else {\n\t\t\t\t*(u32 *)tmp_buf = (u32)(long)cur_arg;\n\t\t\t}\n\t\t\ttmp_buf += sizeof_cur_arg;\n\t\t}\n\t\tnum_spec++;\n\t}\n\n\terr = 0;\nout:\n\tif (err)\n\t\tbpf_bprintf_cleanup(data);\n\treturn err;\n}\n\nBPF_CALL_5(bpf_snprintf, char *, str, u32, str_size, char *, fmt,\n\t   const void *, args, u32, data_len)\n{\n\tstruct bpf_bprintf_data data = {\n\t\t.get_bin_args\t= true,\n\t};\n\tint err, num_args;\n\n\tif (data_len % 8 || data_len > MAX_BPRINTF_VARARGS * 8 ||\n\t    (data_len && !args))\n\t\treturn -EINVAL;\n\tnum_args = data_len / 8;\n\n\t \n\terr = bpf_bprintf_prepare(fmt, UINT_MAX, args, num_args, &data);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = bstr_printf(str, str_size, fmt, data.bin_args);\n\n\tbpf_bprintf_cleanup(&data);\n\n\treturn err + 1;\n}\n\nconst struct bpf_func_proto bpf_snprintf_proto = {\n\t.func\t\t= bpf_snprintf,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM_OR_NULL,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_PTR_TO_CONST_STR,\n\t.arg4_type\t= ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\n \nstruct bpf_hrtimer {\n\tstruct hrtimer timer;\n\tstruct bpf_map *map;\n\tstruct bpf_prog *prog;\n\tvoid __rcu *callback_fn;\n\tvoid *value;\n};\n\n \nstruct bpf_timer_kern {\n\tstruct bpf_hrtimer *timer;\n\t \n\tstruct bpf_spin_lock lock;\n} __attribute__((aligned(8)));\n\nstatic DEFINE_PER_CPU(struct bpf_hrtimer *, hrtimer_running);\n\nstatic enum hrtimer_restart bpf_timer_cb(struct hrtimer *hrtimer)\n{\n\tstruct bpf_hrtimer *t = container_of(hrtimer, struct bpf_hrtimer, timer);\n\tstruct bpf_map *map = t->map;\n\tvoid *value = t->value;\n\tbpf_callback_t callback_fn;\n\tvoid *key;\n\tu32 idx;\n\n\tBTF_TYPE_EMIT(struct bpf_timer);\n\tcallback_fn = rcu_dereference_check(t->callback_fn, rcu_read_lock_bh_held());\n\tif (!callback_fn)\n\t\tgoto out;\n\n\t \n\tthis_cpu_write(hrtimer_running, t);\n\tif (map->map_type == BPF_MAP_TYPE_ARRAY) {\n\t\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\n\t\t \n\t\tidx = ((char *)value - array->value) / array->elem_size;\n\t\tkey = &idx;\n\t} else {  \n\t\tkey = value - round_up(map->key_size, 8);\n\t}\n\n\tcallback_fn((u64)(long)map, (u64)(long)key, (u64)(long)value, 0, 0);\n\t \n\n\tthis_cpu_write(hrtimer_running, NULL);\nout:\n\treturn HRTIMER_NORESTART;\n}\n\nBPF_CALL_3(bpf_timer_init, struct bpf_timer_kern *, timer, struct bpf_map *, map,\n\t   u64, flags)\n{\n\tclockid_t clockid = flags & (MAX_CLOCKS - 1);\n\tstruct bpf_hrtimer *t;\n\tint ret = 0;\n\n\tBUILD_BUG_ON(MAX_CLOCKS != 16);\n\tBUILD_BUG_ON(sizeof(struct bpf_timer_kern) > sizeof(struct bpf_timer));\n\tBUILD_BUG_ON(__alignof__(struct bpf_timer_kern) != __alignof__(struct bpf_timer));\n\n\tif (in_nmi())\n\t\treturn -EOPNOTSUPP;\n\n\tif (flags >= MAX_CLOCKS ||\n\t     \n\t    (clockid != CLOCK_MONOTONIC &&\n\t     clockid != CLOCK_REALTIME &&\n\t     clockid != CLOCK_BOOTTIME))\n\t\treturn -EINVAL;\n\t__bpf_spin_lock_irqsave(&timer->lock);\n\tt = timer->timer;\n\tif (t) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\t \n\tt = bpf_map_kmalloc_node(map, sizeof(*t), GFP_ATOMIC, map->numa_node);\n\tif (!t) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tt->value = (void *)timer - map->record->timer_off;\n\tt->map = map;\n\tt->prog = NULL;\n\trcu_assign_pointer(t->callback_fn, NULL);\n\thrtimer_init(&t->timer, clockid, HRTIMER_MODE_REL_SOFT);\n\tt->timer.function = bpf_timer_cb;\n\tWRITE_ONCE(timer->timer, t);\n\t \n\tsmp_mb();\n\tif (!atomic64_read(&map->usercnt)) {\n\t\t \n\t\tWRITE_ONCE(timer->timer, NULL);\n\t\tkfree(t);\n\t\tret = -EPERM;\n\t}\nout:\n\t__bpf_spin_unlock_irqrestore(&timer->lock);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_timer_init_proto = {\n\t.func\t\t= bpf_timer_init,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_TIMER,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_timer_set_callback, struct bpf_timer_kern *, timer, void *, callback_fn,\n\t   struct bpf_prog_aux *, aux)\n{\n\tstruct bpf_prog *prev, *prog = aux->prog;\n\tstruct bpf_hrtimer *t;\n\tint ret = 0;\n\n\tif (in_nmi())\n\t\treturn -EOPNOTSUPP;\n\t__bpf_spin_lock_irqsave(&timer->lock);\n\tt = timer->timer;\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (!atomic64_read(&t->map->usercnt)) {\n\t\t \n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\tprev = t->prog;\n\tif (prev != prog) {\n\t\t \n\t\tprog = bpf_prog_inc_not_zero(prog);\n\t\tif (IS_ERR(prog)) {\n\t\t\tret = PTR_ERR(prog);\n\t\t\tgoto out;\n\t\t}\n\t\tif (prev)\n\t\t\t \n\t\t\tbpf_prog_put(prev);\n\t\tt->prog = prog;\n\t}\n\trcu_assign_pointer(t->callback_fn, callback_fn);\nout:\n\t__bpf_spin_unlock_irqrestore(&timer->lock);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_timer_set_callback_proto = {\n\t.func\t\t= bpf_timer_set_callback,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_TIMER,\n\t.arg2_type\t= ARG_PTR_TO_FUNC,\n};\n\nBPF_CALL_3(bpf_timer_start, struct bpf_timer_kern *, timer, u64, nsecs, u64, flags)\n{\n\tstruct bpf_hrtimer *t;\n\tint ret = 0;\n\tenum hrtimer_mode mode;\n\n\tif (in_nmi())\n\t\treturn -EOPNOTSUPP;\n\tif (flags > BPF_F_TIMER_ABS)\n\t\treturn -EINVAL;\n\t__bpf_spin_lock_irqsave(&timer->lock);\n\tt = timer->timer;\n\tif (!t || !t->prog) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (flags & BPF_F_TIMER_ABS)\n\t\tmode = HRTIMER_MODE_ABS_SOFT;\n\telse\n\t\tmode = HRTIMER_MODE_REL_SOFT;\n\n\thrtimer_start(&t->timer, ns_to_ktime(nsecs), mode);\nout:\n\t__bpf_spin_unlock_irqrestore(&timer->lock);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_timer_start_proto = {\n\t.func\t\t= bpf_timer_start,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_TIMER,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic void drop_prog_refcnt(struct bpf_hrtimer *t)\n{\n\tstruct bpf_prog *prog = t->prog;\n\n\tif (prog) {\n\t\tbpf_prog_put(prog);\n\t\tt->prog = NULL;\n\t\trcu_assign_pointer(t->callback_fn, NULL);\n\t}\n}\n\nBPF_CALL_1(bpf_timer_cancel, struct bpf_timer_kern *, timer)\n{\n\tstruct bpf_hrtimer *t;\n\tint ret = 0;\n\n\tif (in_nmi())\n\t\treturn -EOPNOTSUPP;\n\t__bpf_spin_lock_irqsave(&timer->lock);\n\tt = timer->timer;\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (this_cpu_read(hrtimer_running) == t) {\n\t\t \n\t\tret = -EDEADLK;\n\t\tgoto out;\n\t}\n\tdrop_prog_refcnt(t);\nout:\n\t__bpf_spin_unlock_irqrestore(&timer->lock);\n\t \n\tret = ret ?: hrtimer_cancel(&t->timer);\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_timer_cancel_proto = {\n\t.func\t\t= bpf_timer_cancel,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_TIMER,\n};\n\n \nvoid bpf_timer_cancel_and_free(void *val)\n{\n\tstruct bpf_timer_kern *timer = val;\n\tstruct bpf_hrtimer *t;\n\n\t \n\tif (!READ_ONCE(timer->timer))\n\t\treturn;\n\n\t__bpf_spin_lock_irqsave(&timer->lock);\n\t \n\tt = timer->timer;\n\tif (!t)\n\t\tgoto out;\n\tdrop_prog_refcnt(t);\n\t \n\tWRITE_ONCE(timer->timer, NULL);\nout:\n\t__bpf_spin_unlock_irqrestore(&timer->lock);\n\tif (!t)\n\t\treturn;\n\t \n\tif (this_cpu_read(hrtimer_running) != t)\n\t\thrtimer_cancel(&t->timer);\n\tkfree(t);\n}\n\nBPF_CALL_2(bpf_kptr_xchg, void *, map_value, void *, ptr)\n{\n\tunsigned long *kptr = map_value;\n\n\treturn xchg(kptr, (unsigned long)ptr);\n}\n\n \nstatic const struct bpf_func_proto bpf_kptr_xchg_proto = {\n\t.func         = bpf_kptr_xchg,\n\t.gpl_only     = false,\n\t.ret_type     = RET_PTR_TO_BTF_ID_OR_NULL,\n\t.ret_btf_id   = BPF_PTR_POISON,\n\t.arg1_type    = ARG_PTR_TO_KPTR,\n\t.arg2_type    = ARG_PTR_TO_BTF_ID_OR_NULL | OBJ_RELEASE,\n\t.arg2_btf_id  = BPF_PTR_POISON,\n};\n\n \n#define DYNPTR_MAX_SIZE\t((1UL << 24) - 1)\n#define DYNPTR_TYPE_SHIFT\t28\n#define DYNPTR_SIZE_MASK\t0xFFFFFF\n#define DYNPTR_RDONLY_BIT\tBIT(31)\n\nstatic bool __bpf_dynptr_is_rdonly(const struct bpf_dynptr_kern *ptr)\n{\n\treturn ptr->size & DYNPTR_RDONLY_BIT;\n}\n\nvoid bpf_dynptr_set_rdonly(struct bpf_dynptr_kern *ptr)\n{\n\tptr->size |= DYNPTR_RDONLY_BIT;\n}\n\nstatic void bpf_dynptr_set_type(struct bpf_dynptr_kern *ptr, enum bpf_dynptr_type type)\n{\n\tptr->size |= type << DYNPTR_TYPE_SHIFT;\n}\n\nstatic enum bpf_dynptr_type bpf_dynptr_get_type(const struct bpf_dynptr_kern *ptr)\n{\n\treturn (ptr->size & ~(DYNPTR_RDONLY_BIT)) >> DYNPTR_TYPE_SHIFT;\n}\n\nu32 __bpf_dynptr_size(const struct bpf_dynptr_kern *ptr)\n{\n\treturn ptr->size & DYNPTR_SIZE_MASK;\n}\n\nstatic void bpf_dynptr_set_size(struct bpf_dynptr_kern *ptr, u32 new_size)\n{\n\tu32 metadata = ptr->size & ~DYNPTR_SIZE_MASK;\n\n\tptr->size = new_size | metadata;\n}\n\nint bpf_dynptr_check_size(u32 size)\n{\n\treturn size > DYNPTR_MAX_SIZE ? -E2BIG : 0;\n}\n\nvoid bpf_dynptr_init(struct bpf_dynptr_kern *ptr, void *data,\n\t\t     enum bpf_dynptr_type type, u32 offset, u32 size)\n{\n\tptr->data = data;\n\tptr->offset = offset;\n\tptr->size = size;\n\tbpf_dynptr_set_type(ptr, type);\n}\n\nvoid bpf_dynptr_set_null(struct bpf_dynptr_kern *ptr)\n{\n\tmemset(ptr, 0, sizeof(*ptr));\n}\n\nstatic int bpf_dynptr_check_off_len(const struct bpf_dynptr_kern *ptr, u32 offset, u32 len)\n{\n\tu32 size = __bpf_dynptr_size(ptr);\n\n\tif (len > size || offset > size - len)\n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nBPF_CALL_4(bpf_dynptr_from_mem, void *, data, u32, size, u64, flags, struct bpf_dynptr_kern *, ptr)\n{\n\tint err;\n\n\tBTF_TYPE_EMIT(struct bpf_dynptr);\n\n\terr = bpf_dynptr_check_size(size);\n\tif (err)\n\t\tgoto error;\n\n\t \n\tif (flags) {\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tbpf_dynptr_init(ptr, data, BPF_DYNPTR_TYPE_LOCAL, 0, size);\n\n\treturn 0;\n\nerror:\n\tbpf_dynptr_set_null(ptr);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_dynptr_from_mem_proto = {\n\t.func\t\t= bpf_dynptr_from_mem,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_DYNPTR | DYNPTR_TYPE_LOCAL | MEM_UNINIT,\n};\n\nBPF_CALL_5(bpf_dynptr_read, void *, dst, u32, len, const struct bpf_dynptr_kern *, src,\n\t   u32, offset, u64, flags)\n{\n\tenum bpf_dynptr_type type;\n\tint err;\n\n\tif (!src->data || flags)\n\t\treturn -EINVAL;\n\n\terr = bpf_dynptr_check_off_len(src, offset, len);\n\tif (err)\n\t\treturn err;\n\n\ttype = bpf_dynptr_get_type(src);\n\n\tswitch (type) {\n\tcase BPF_DYNPTR_TYPE_LOCAL:\n\tcase BPF_DYNPTR_TYPE_RINGBUF:\n\t\t \n\t\tmemmove(dst, src->data + src->offset + offset, len);\n\t\treturn 0;\n\tcase BPF_DYNPTR_TYPE_SKB:\n\t\treturn __bpf_skb_load_bytes(src->data, src->offset + offset, dst, len);\n\tcase BPF_DYNPTR_TYPE_XDP:\n\t\treturn __bpf_xdp_load_bytes(src->data, src->offset + offset, dst, len);\n\tdefault:\n\t\tWARN_ONCE(true, \"bpf_dynptr_read: unknown dynptr type %d\\n\", type);\n\t\treturn -EFAULT;\n\t}\n}\n\nstatic const struct bpf_func_proto bpf_dynptr_read_proto = {\n\t.func\t\t= bpf_dynptr_read,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_PTR_TO_DYNPTR | MEM_RDONLY,\n\t.arg4_type\t= ARG_ANYTHING,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_5(bpf_dynptr_write, const struct bpf_dynptr_kern *, dst, u32, offset, void *, src,\n\t   u32, len, u64, flags)\n{\n\tenum bpf_dynptr_type type;\n\tint err;\n\n\tif (!dst->data || __bpf_dynptr_is_rdonly(dst))\n\t\treturn -EINVAL;\n\n\terr = bpf_dynptr_check_off_len(dst, offset, len);\n\tif (err)\n\t\treturn err;\n\n\ttype = bpf_dynptr_get_type(dst);\n\n\tswitch (type) {\n\tcase BPF_DYNPTR_TYPE_LOCAL:\n\tcase BPF_DYNPTR_TYPE_RINGBUF:\n\t\tif (flags)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tmemmove(dst->data + dst->offset + offset, src, len);\n\t\treturn 0;\n\tcase BPF_DYNPTR_TYPE_SKB:\n\t\treturn __bpf_skb_store_bytes(dst->data, dst->offset + offset, src, len,\n\t\t\t\t\t     flags);\n\tcase BPF_DYNPTR_TYPE_XDP:\n\t\tif (flags)\n\t\t\treturn -EINVAL;\n\t\treturn __bpf_xdp_store_bytes(dst->data, dst->offset + offset, src, len);\n\tdefault:\n\t\tWARN_ONCE(true, \"bpf_dynptr_write: unknown dynptr type %d\\n\", type);\n\t\treturn -EFAULT;\n\t}\n}\n\nstatic const struct bpf_func_proto bpf_dynptr_write_proto = {\n\t.func\t\t= bpf_dynptr_write,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_DYNPTR | MEM_RDONLY,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_dynptr_data, const struct bpf_dynptr_kern *, ptr, u32, offset, u32, len)\n{\n\tenum bpf_dynptr_type type;\n\tint err;\n\n\tif (!ptr->data)\n\t\treturn 0;\n\n\terr = bpf_dynptr_check_off_len(ptr, offset, len);\n\tif (err)\n\t\treturn 0;\n\n\tif (__bpf_dynptr_is_rdonly(ptr))\n\t\treturn 0;\n\n\ttype = bpf_dynptr_get_type(ptr);\n\n\tswitch (type) {\n\tcase BPF_DYNPTR_TYPE_LOCAL:\n\tcase BPF_DYNPTR_TYPE_RINGBUF:\n\t\treturn (unsigned long)(ptr->data + ptr->offset + offset);\n\tcase BPF_DYNPTR_TYPE_SKB:\n\tcase BPF_DYNPTR_TYPE_XDP:\n\t\t \n\t\treturn 0;\n\tdefault:\n\t\tWARN_ONCE(true, \"bpf_dynptr_data: unknown dynptr type %d\\n\", type);\n\t\treturn 0;\n\t}\n}\n\nstatic const struct bpf_func_proto bpf_dynptr_data_proto = {\n\t.func\t\t= bpf_dynptr_data,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_DYNPTR_MEM_OR_NULL,\n\t.arg1_type\t= ARG_PTR_TO_DYNPTR | MEM_RDONLY,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_CONST_ALLOC_SIZE_OR_ZERO,\n};\n\nconst struct bpf_func_proto bpf_get_current_task_proto __weak;\nconst struct bpf_func_proto bpf_get_current_task_btf_proto __weak;\nconst struct bpf_func_proto bpf_probe_read_user_proto __weak;\nconst struct bpf_func_proto bpf_probe_read_user_str_proto __weak;\nconst struct bpf_func_proto bpf_probe_read_kernel_proto __weak;\nconst struct bpf_func_proto bpf_probe_read_kernel_str_proto __weak;\nconst struct bpf_func_proto bpf_task_pt_regs_proto __weak;\n\nconst struct bpf_func_proto *\nbpf_base_func_proto(enum bpf_func_id func_id)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_map_lookup_elem:\n\t\treturn &bpf_map_lookup_elem_proto;\n\tcase BPF_FUNC_map_update_elem:\n\t\treturn &bpf_map_update_elem_proto;\n\tcase BPF_FUNC_map_delete_elem:\n\t\treturn &bpf_map_delete_elem_proto;\n\tcase BPF_FUNC_map_push_elem:\n\t\treturn &bpf_map_push_elem_proto;\n\tcase BPF_FUNC_map_pop_elem:\n\t\treturn &bpf_map_pop_elem_proto;\n\tcase BPF_FUNC_map_peek_elem:\n\t\treturn &bpf_map_peek_elem_proto;\n\tcase BPF_FUNC_map_lookup_percpu_elem:\n\t\treturn &bpf_map_lookup_percpu_elem_proto;\n\tcase BPF_FUNC_get_prandom_u32:\n\t\treturn &bpf_get_prandom_u32_proto;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_raw_smp_processor_id_proto;\n\tcase BPF_FUNC_get_numa_node_id:\n\t\treturn &bpf_get_numa_node_id_proto;\n\tcase BPF_FUNC_tail_call:\n\t\treturn &bpf_tail_call_proto;\n\tcase BPF_FUNC_ktime_get_ns:\n\t\treturn &bpf_ktime_get_ns_proto;\n\tcase BPF_FUNC_ktime_get_boot_ns:\n\t\treturn &bpf_ktime_get_boot_ns_proto;\n\tcase BPF_FUNC_ktime_get_tai_ns:\n\t\treturn &bpf_ktime_get_tai_ns_proto;\n\tcase BPF_FUNC_ringbuf_output:\n\t\treturn &bpf_ringbuf_output_proto;\n\tcase BPF_FUNC_ringbuf_reserve:\n\t\treturn &bpf_ringbuf_reserve_proto;\n\tcase BPF_FUNC_ringbuf_submit:\n\t\treturn &bpf_ringbuf_submit_proto;\n\tcase BPF_FUNC_ringbuf_discard:\n\t\treturn &bpf_ringbuf_discard_proto;\n\tcase BPF_FUNC_ringbuf_query:\n\t\treturn &bpf_ringbuf_query_proto;\n\tcase BPF_FUNC_strncmp:\n\t\treturn &bpf_strncmp_proto;\n\tcase BPF_FUNC_strtol:\n\t\treturn &bpf_strtol_proto;\n\tcase BPF_FUNC_strtoul:\n\t\treturn &bpf_strtoul_proto;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (!bpf_capable())\n\t\treturn NULL;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_spin_lock:\n\t\treturn &bpf_spin_lock_proto;\n\tcase BPF_FUNC_spin_unlock:\n\t\treturn &bpf_spin_unlock_proto;\n\tcase BPF_FUNC_jiffies64:\n\t\treturn &bpf_jiffies64_proto;\n\tcase BPF_FUNC_per_cpu_ptr:\n\t\treturn &bpf_per_cpu_ptr_proto;\n\tcase BPF_FUNC_this_cpu_ptr:\n\t\treturn &bpf_this_cpu_ptr_proto;\n\tcase BPF_FUNC_timer_init:\n\t\treturn &bpf_timer_init_proto;\n\tcase BPF_FUNC_timer_set_callback:\n\t\treturn &bpf_timer_set_callback_proto;\n\tcase BPF_FUNC_timer_start:\n\t\treturn &bpf_timer_start_proto;\n\tcase BPF_FUNC_timer_cancel:\n\t\treturn &bpf_timer_cancel_proto;\n\tcase BPF_FUNC_kptr_xchg:\n\t\treturn &bpf_kptr_xchg_proto;\n\tcase BPF_FUNC_for_each_map_elem:\n\t\treturn &bpf_for_each_map_elem_proto;\n\tcase BPF_FUNC_loop:\n\t\treturn &bpf_loop_proto;\n\tcase BPF_FUNC_user_ringbuf_drain:\n\t\treturn &bpf_user_ringbuf_drain_proto;\n\tcase BPF_FUNC_ringbuf_reserve_dynptr:\n\t\treturn &bpf_ringbuf_reserve_dynptr_proto;\n\tcase BPF_FUNC_ringbuf_submit_dynptr:\n\t\treturn &bpf_ringbuf_submit_dynptr_proto;\n\tcase BPF_FUNC_ringbuf_discard_dynptr:\n\t\treturn &bpf_ringbuf_discard_dynptr_proto;\n\tcase BPF_FUNC_dynptr_from_mem:\n\t\treturn &bpf_dynptr_from_mem_proto;\n\tcase BPF_FUNC_dynptr_read:\n\t\treturn &bpf_dynptr_read_proto;\n\tcase BPF_FUNC_dynptr_write:\n\t\treturn &bpf_dynptr_write_proto;\n\tcase BPF_FUNC_dynptr_data:\n\t\treturn &bpf_dynptr_data_proto;\n#ifdef CONFIG_CGROUPS\n\tcase BPF_FUNC_cgrp_storage_get:\n\t\treturn &bpf_cgrp_storage_get_proto;\n\tcase BPF_FUNC_cgrp_storage_delete:\n\t\treturn &bpf_cgrp_storage_delete_proto;\n\tcase BPF_FUNC_get_current_cgroup_id:\n\t\treturn &bpf_get_current_cgroup_id_proto;\n\tcase BPF_FUNC_get_current_ancestor_cgroup_id:\n\t\treturn &bpf_get_current_ancestor_cgroup_id_proto;\n#endif\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (!perfmon_capable())\n\t\treturn NULL;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_trace_printk:\n\t\treturn bpf_get_trace_printk_proto();\n\tcase BPF_FUNC_get_current_task:\n\t\treturn &bpf_get_current_task_proto;\n\tcase BPF_FUNC_get_current_task_btf:\n\t\treturn &bpf_get_current_task_btf_proto;\n\tcase BPF_FUNC_probe_read_user:\n\t\treturn &bpf_probe_read_user_proto;\n\tcase BPF_FUNC_probe_read_kernel:\n\t\treturn security_locked_down(LOCKDOWN_BPF_READ_KERNEL) < 0 ?\n\t\t       NULL : &bpf_probe_read_kernel_proto;\n\tcase BPF_FUNC_probe_read_user_str:\n\t\treturn &bpf_probe_read_user_str_proto;\n\tcase BPF_FUNC_probe_read_kernel_str:\n\t\treturn security_locked_down(LOCKDOWN_BPF_READ_KERNEL) < 0 ?\n\t\t       NULL : &bpf_probe_read_kernel_str_proto;\n\tcase BPF_FUNC_snprintf_btf:\n\t\treturn &bpf_snprintf_btf_proto;\n\tcase BPF_FUNC_snprintf:\n\t\treturn &bpf_snprintf_proto;\n\tcase BPF_FUNC_task_pt_regs:\n\t\treturn &bpf_task_pt_regs_proto;\n\tcase BPF_FUNC_trace_vprintk:\n\t\treturn bpf_get_trace_vprintk_proto();\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nvoid __bpf_obj_drop_impl(void *p, const struct btf_record *rec);\n\nvoid bpf_list_head_free(const struct btf_field *field, void *list_head,\n\t\t\tstruct bpf_spin_lock *spin_lock)\n{\n\tstruct list_head *head = list_head, *orig_head = list_head;\n\n\tBUILD_BUG_ON(sizeof(struct list_head) > sizeof(struct bpf_list_head));\n\tBUILD_BUG_ON(__alignof__(struct list_head) > __alignof__(struct bpf_list_head));\n\n\t \n\t__bpf_spin_lock_irqsave(spin_lock);\n\tif (!head->next || list_empty(head))\n\t\tgoto unlock;\n\thead = head->next;\nunlock:\n\tINIT_LIST_HEAD(orig_head);\n\t__bpf_spin_unlock_irqrestore(spin_lock);\n\n\twhile (head != orig_head) {\n\t\tvoid *obj = head;\n\n\t\tobj -= field->graph_root.node_offset;\n\t\thead = head->next;\n\t\t \n\t\tmigrate_disable();\n\t\t__bpf_obj_drop_impl(obj, field->graph_root.value_rec);\n\t\tmigrate_enable();\n\t}\n}\n\n \n#define bpf_rbtree_postorder_for_each_entry_safe(pos, n, root) \\\n\tfor (pos = rb_first_postorder(root); \\\n\t    pos && ({ n = rb_next_postorder(pos); 1; }); \\\n\t    pos = n)\n\nvoid bpf_rb_root_free(const struct btf_field *field, void *rb_root,\n\t\t      struct bpf_spin_lock *spin_lock)\n{\n\tstruct rb_root_cached orig_root, *root = rb_root;\n\tstruct rb_node *pos, *n;\n\tvoid *obj;\n\n\tBUILD_BUG_ON(sizeof(struct rb_root_cached) > sizeof(struct bpf_rb_root));\n\tBUILD_BUG_ON(__alignof__(struct rb_root_cached) > __alignof__(struct bpf_rb_root));\n\n\t__bpf_spin_lock_irqsave(spin_lock);\n\torig_root = *root;\n\t*root = RB_ROOT_CACHED;\n\t__bpf_spin_unlock_irqrestore(spin_lock);\n\n\tbpf_rbtree_postorder_for_each_entry_safe(pos, n, &orig_root.rb_root) {\n\t\tobj = pos;\n\t\tobj -= field->graph_root.node_offset;\n\n\n\t\tmigrate_disable();\n\t\t__bpf_obj_drop_impl(obj, field->graph_root.value_rec);\n\t\tmigrate_enable();\n\t}\n}\n\n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"Global functions as their definitions will be in vmlinux BTF\");\n\n__bpf_kfunc void *bpf_obj_new_impl(u64 local_type_id__k, void *meta__ign)\n{\n\tstruct btf_struct_meta *meta = meta__ign;\n\tu64 size = local_type_id__k;\n\tvoid *p;\n\n\tp = bpf_mem_alloc(&bpf_global_ma, size);\n\tif (!p)\n\t\treturn NULL;\n\tif (meta)\n\t\tbpf_obj_init(meta->record, p);\n\treturn p;\n}\n\n \nvoid __bpf_obj_drop_impl(void *p, const struct btf_record *rec)\n{\n\tif (rec && rec->refcount_off >= 0 &&\n\t    !refcount_dec_and_test((refcount_t *)(p + rec->refcount_off))) {\n\t\t \n\t\treturn;\n\t}\n\n\tif (rec)\n\t\tbpf_obj_free_fields(rec, p);\n\n\tif (rec && rec->refcount_off >= 0)\n\t\tbpf_mem_free_rcu(&bpf_global_ma, p);\n\telse\n\t\tbpf_mem_free(&bpf_global_ma, p);\n}\n\n__bpf_kfunc void bpf_obj_drop_impl(void *p__alloc, void *meta__ign)\n{\n\tstruct btf_struct_meta *meta = meta__ign;\n\tvoid *p = p__alloc;\n\n\t__bpf_obj_drop_impl(p, meta ? meta->record : NULL);\n}\n\n__bpf_kfunc void *bpf_refcount_acquire_impl(void *p__refcounted_kptr, void *meta__ign)\n{\n\tstruct btf_struct_meta *meta = meta__ign;\n\tstruct bpf_refcount *ref;\n\n\t \n\tref = (struct bpf_refcount *)(p__refcounted_kptr + meta->record->refcount_off);\n\tif (!refcount_inc_not_zero((refcount_t *)ref))\n\t\treturn NULL;\n\n\t \n\treturn (void *)p__refcounted_kptr;\n}\n\nstatic int __bpf_list_add(struct bpf_list_node_kern *node,\n\t\t\t  struct bpf_list_head *head,\n\t\t\t  bool tail, struct btf_record *rec, u64 off)\n{\n\tstruct list_head *n = &node->list_head, *h = (void *)head;\n\n\t \n\tif (unlikely(!h->next))\n\t\tINIT_LIST_HEAD(h);\n\n\t \n\tif (cmpxchg(&node->owner, NULL, BPF_PTR_POISON)) {\n\t\t \n\t\t__bpf_obj_drop_impl((void *)n - off, rec);\n\t\treturn -EINVAL;\n\t}\n\n\ttail ? list_add_tail(n, h) : list_add(n, h);\n\tWRITE_ONCE(node->owner, head);\n\n\treturn 0;\n}\n\n__bpf_kfunc int bpf_list_push_front_impl(struct bpf_list_head *head,\n\t\t\t\t\t struct bpf_list_node *node,\n\t\t\t\t\t void *meta__ign, u64 off)\n{\n\tstruct bpf_list_node_kern *n = (void *)node;\n\tstruct btf_struct_meta *meta = meta__ign;\n\n\treturn __bpf_list_add(n, head, false, meta ? meta->record : NULL, off);\n}\n\n__bpf_kfunc int bpf_list_push_back_impl(struct bpf_list_head *head,\n\t\t\t\t\tstruct bpf_list_node *node,\n\t\t\t\t\tvoid *meta__ign, u64 off)\n{\n\tstruct bpf_list_node_kern *n = (void *)node;\n\tstruct btf_struct_meta *meta = meta__ign;\n\n\treturn __bpf_list_add(n, head, true, meta ? meta->record : NULL, off);\n}\n\nstatic struct bpf_list_node *__bpf_list_del(struct bpf_list_head *head, bool tail)\n{\n\tstruct list_head *n, *h = (void *)head;\n\tstruct bpf_list_node_kern *node;\n\n\t \n\tif (unlikely(!h->next))\n\t\tINIT_LIST_HEAD(h);\n\tif (list_empty(h))\n\t\treturn NULL;\n\n\tn = tail ? h->prev : h->next;\n\tnode = container_of(n, struct bpf_list_node_kern, list_head);\n\tif (WARN_ON_ONCE(READ_ONCE(node->owner) != head))\n\t\treturn NULL;\n\n\tlist_del_init(n);\n\tWRITE_ONCE(node->owner, NULL);\n\treturn (struct bpf_list_node *)n;\n}\n\n__bpf_kfunc struct bpf_list_node *bpf_list_pop_front(struct bpf_list_head *head)\n{\n\treturn __bpf_list_del(head, false);\n}\n\n__bpf_kfunc struct bpf_list_node *bpf_list_pop_back(struct bpf_list_head *head)\n{\n\treturn __bpf_list_del(head, true);\n}\n\n__bpf_kfunc struct bpf_rb_node *bpf_rbtree_remove(struct bpf_rb_root *root,\n\t\t\t\t\t\t  struct bpf_rb_node *node)\n{\n\tstruct bpf_rb_node_kern *node_internal = (struct bpf_rb_node_kern *)node;\n\tstruct rb_root_cached *r = (struct rb_root_cached *)root;\n\tstruct rb_node *n = &node_internal->rb_node;\n\n\t \n\tif (READ_ONCE(node_internal->owner) != root)\n\t\treturn NULL;\n\n\trb_erase_cached(n, r);\n\tRB_CLEAR_NODE(n);\n\tWRITE_ONCE(node_internal->owner, NULL);\n\treturn (struct bpf_rb_node *)n;\n}\n\n \nstatic int __bpf_rbtree_add(struct bpf_rb_root *root,\n\t\t\t    struct bpf_rb_node_kern *node,\n\t\t\t    void *less, struct btf_record *rec, u64 off)\n{\n\tstruct rb_node **link = &((struct rb_root_cached *)root)->rb_root.rb_node;\n\tstruct rb_node *parent = NULL, *n = &node->rb_node;\n\tbpf_callback_t cb = (bpf_callback_t)less;\n\tbool leftmost = true;\n\n\t \n\tif (cmpxchg(&node->owner, NULL, BPF_PTR_POISON)) {\n\t\t \n\t\t__bpf_obj_drop_impl((void *)n - off, rec);\n\t\treturn -EINVAL;\n\t}\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tif (cb((uintptr_t)node, (uintptr_t)parent, 0, 0, 0)) {\n\t\t\tlink = &parent->rb_left;\n\t\t} else {\n\t\t\tlink = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(n, parent, link);\n\trb_insert_color_cached(n, (struct rb_root_cached *)root, leftmost);\n\tWRITE_ONCE(node->owner, root);\n\treturn 0;\n}\n\n__bpf_kfunc int bpf_rbtree_add_impl(struct bpf_rb_root *root, struct bpf_rb_node *node,\n\t\t\t\t    bool (less)(struct bpf_rb_node *a, const struct bpf_rb_node *b),\n\t\t\t\t    void *meta__ign, u64 off)\n{\n\tstruct btf_struct_meta *meta = meta__ign;\n\tstruct bpf_rb_node_kern *n = (void *)node;\n\n\treturn __bpf_rbtree_add(root, n, (void *)less, meta ? meta->record : NULL, off);\n}\n\n__bpf_kfunc struct bpf_rb_node *bpf_rbtree_first(struct bpf_rb_root *root)\n{\n\tstruct rb_root_cached *r = (struct rb_root_cached *)root;\n\n\treturn (struct bpf_rb_node *)rb_first_cached(r);\n}\n\n \n__bpf_kfunc struct task_struct *bpf_task_acquire(struct task_struct *p)\n{\n\tif (refcount_inc_not_zero(&p->rcu_users))\n\t\treturn p;\n\treturn NULL;\n}\n\n \n__bpf_kfunc void bpf_task_release(struct task_struct *p)\n{\n\tput_task_struct_rcu_user(p);\n}\n\n#ifdef CONFIG_CGROUPS\n \n__bpf_kfunc struct cgroup *bpf_cgroup_acquire(struct cgroup *cgrp)\n{\n\treturn cgroup_tryget(cgrp) ? cgrp : NULL;\n}\n\n \n__bpf_kfunc void bpf_cgroup_release(struct cgroup *cgrp)\n{\n\tcgroup_put(cgrp);\n}\n\n \n__bpf_kfunc struct cgroup *bpf_cgroup_ancestor(struct cgroup *cgrp, int level)\n{\n\tstruct cgroup *ancestor;\n\n\tif (level > cgrp->level || level < 0)\n\t\treturn NULL;\n\n\t \n\tancestor = cgrp->ancestors[level];\n\tif (!cgroup_tryget(ancestor))\n\t\treturn NULL;\n\treturn ancestor;\n}\n\n \n__bpf_kfunc struct cgroup *bpf_cgroup_from_id(u64 cgid)\n{\n\tstruct cgroup *cgrp;\n\n\tcgrp = cgroup_get_from_id(cgid);\n\tif (IS_ERR(cgrp))\n\t\treturn NULL;\n\treturn cgrp;\n}\n\n \n__bpf_kfunc long bpf_task_under_cgroup(struct task_struct *task,\n\t\t\t\t       struct cgroup *ancestor)\n{\n\tlong ret;\n\n\trcu_read_lock();\n\tret = task_under_cgroup_hierarchy(task, ancestor);\n\trcu_read_unlock();\n\treturn ret;\n}\n#endif  \n\n \n__bpf_kfunc struct task_struct *bpf_task_from_pid(s32 pid)\n{\n\tstruct task_struct *p;\n\n\trcu_read_lock();\n\tp = find_task_by_pid_ns(pid, &init_pid_ns);\n\tif (p)\n\t\tp = bpf_task_acquire(p);\n\trcu_read_unlock();\n\n\treturn p;\n}\n\n \n__bpf_kfunc void *bpf_dynptr_slice(const struct bpf_dynptr_kern *ptr, u32 offset,\n\t\t\t\t   void *buffer__opt, u32 buffer__szk)\n{\n\tenum bpf_dynptr_type type;\n\tu32 len = buffer__szk;\n\tint err;\n\n\tif (!ptr->data)\n\t\treturn NULL;\n\n\terr = bpf_dynptr_check_off_len(ptr, offset, len);\n\tif (err)\n\t\treturn NULL;\n\n\ttype = bpf_dynptr_get_type(ptr);\n\n\tswitch (type) {\n\tcase BPF_DYNPTR_TYPE_LOCAL:\n\tcase BPF_DYNPTR_TYPE_RINGBUF:\n\t\treturn ptr->data + ptr->offset + offset;\n\tcase BPF_DYNPTR_TYPE_SKB:\n\t\tif (buffer__opt)\n\t\t\treturn skb_header_pointer(ptr->data, ptr->offset + offset, len, buffer__opt);\n\t\telse\n\t\t\treturn skb_pointer_if_linear(ptr->data, ptr->offset + offset, len);\n\tcase BPF_DYNPTR_TYPE_XDP:\n\t{\n\t\tvoid *xdp_ptr = bpf_xdp_pointer(ptr->data, ptr->offset + offset, len);\n\t\tif (!IS_ERR_OR_NULL(xdp_ptr))\n\t\t\treturn xdp_ptr;\n\n\t\tif (!buffer__opt)\n\t\t\treturn NULL;\n\t\tbpf_xdp_copy_buf(ptr->data, ptr->offset + offset, buffer__opt, len, false);\n\t\treturn buffer__opt;\n\t}\n\tdefault:\n\t\tWARN_ONCE(true, \"unknown dynptr type %d\\n\", type);\n\t\treturn NULL;\n\t}\n}\n\n \n__bpf_kfunc void *bpf_dynptr_slice_rdwr(const struct bpf_dynptr_kern *ptr, u32 offset,\n\t\t\t\t\tvoid *buffer__opt, u32 buffer__szk)\n{\n\tif (!ptr->data || __bpf_dynptr_is_rdonly(ptr))\n\t\treturn NULL;\n\n\t \n\treturn bpf_dynptr_slice(ptr, offset, buffer__opt, buffer__szk);\n}\n\n__bpf_kfunc int bpf_dynptr_adjust(struct bpf_dynptr_kern *ptr, u32 start, u32 end)\n{\n\tu32 size;\n\n\tif (!ptr->data || start > end)\n\t\treturn -EINVAL;\n\n\tsize = __bpf_dynptr_size(ptr);\n\n\tif (start > size || end > size)\n\t\treturn -ERANGE;\n\n\tptr->offset += start;\n\tbpf_dynptr_set_size(ptr, end - start);\n\n\treturn 0;\n}\n\n__bpf_kfunc bool bpf_dynptr_is_null(struct bpf_dynptr_kern *ptr)\n{\n\treturn !ptr->data;\n}\n\n__bpf_kfunc bool bpf_dynptr_is_rdonly(struct bpf_dynptr_kern *ptr)\n{\n\tif (!ptr->data)\n\t\treturn false;\n\n\treturn __bpf_dynptr_is_rdonly(ptr);\n}\n\n__bpf_kfunc __u32 bpf_dynptr_size(const struct bpf_dynptr_kern *ptr)\n{\n\tif (!ptr->data)\n\t\treturn -EINVAL;\n\n\treturn __bpf_dynptr_size(ptr);\n}\n\n__bpf_kfunc int bpf_dynptr_clone(struct bpf_dynptr_kern *ptr,\n\t\t\t\t struct bpf_dynptr_kern *clone__uninit)\n{\n\tif (!ptr->data) {\n\t\tbpf_dynptr_set_null(clone__uninit);\n\t\treturn -EINVAL;\n\t}\n\n\t*clone__uninit = *ptr;\n\n\treturn 0;\n}\n\n__bpf_kfunc void *bpf_cast_to_kern_ctx(void *obj)\n{\n\treturn obj;\n}\n\n__bpf_kfunc void *bpf_rdonly_cast(void *obj__ign, u32 btf_id__k)\n{\n\treturn obj__ign;\n}\n\n__bpf_kfunc void bpf_rcu_read_lock(void)\n{\n\trcu_read_lock();\n}\n\n__bpf_kfunc void bpf_rcu_read_unlock(void)\n{\n\trcu_read_unlock();\n}\n\n__diag_pop();\n\nBTF_SET8_START(generic_btf_ids)\n#ifdef CONFIG_KEXEC_CORE\nBTF_ID_FLAGS(func, crash_kexec, KF_DESTRUCTIVE)\n#endif\nBTF_ID_FLAGS(func, bpf_obj_new_impl, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_obj_drop_impl, KF_RELEASE)\nBTF_ID_FLAGS(func, bpf_refcount_acquire_impl, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_list_push_front_impl)\nBTF_ID_FLAGS(func, bpf_list_push_back_impl)\nBTF_ID_FLAGS(func, bpf_list_pop_front, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_list_pop_back, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_task_acquire, KF_ACQUIRE | KF_RCU | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_task_release, KF_RELEASE)\nBTF_ID_FLAGS(func, bpf_rbtree_remove, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_rbtree_add_impl)\nBTF_ID_FLAGS(func, bpf_rbtree_first, KF_RET_NULL)\n\n#ifdef CONFIG_CGROUPS\nBTF_ID_FLAGS(func, bpf_cgroup_acquire, KF_ACQUIRE | KF_RCU | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_cgroup_release, KF_RELEASE)\nBTF_ID_FLAGS(func, bpf_cgroup_ancestor, KF_ACQUIRE | KF_RCU | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_cgroup_from_id, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_task_under_cgroup, KF_RCU)\n#endif\nBTF_ID_FLAGS(func, bpf_task_from_pid, KF_ACQUIRE | KF_RET_NULL)\nBTF_SET8_END(generic_btf_ids)\n\nstatic const struct btf_kfunc_id_set generic_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &generic_btf_ids,\n};\n\n\nBTF_ID_LIST(generic_dtor_ids)\nBTF_ID(struct, task_struct)\nBTF_ID(func, bpf_task_release)\n#ifdef CONFIG_CGROUPS\nBTF_ID(struct, cgroup)\nBTF_ID(func, bpf_cgroup_release)\n#endif\n\nBTF_SET8_START(common_btf_ids)\nBTF_ID_FLAGS(func, bpf_cast_to_kern_ctx)\nBTF_ID_FLAGS(func, bpf_rdonly_cast)\nBTF_ID_FLAGS(func, bpf_rcu_read_lock)\nBTF_ID_FLAGS(func, bpf_rcu_read_unlock)\nBTF_ID_FLAGS(func, bpf_dynptr_slice, KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_dynptr_slice_rdwr, KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_iter_num_new, KF_ITER_NEW)\nBTF_ID_FLAGS(func, bpf_iter_num_next, KF_ITER_NEXT | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_iter_num_destroy, KF_ITER_DESTROY)\nBTF_ID_FLAGS(func, bpf_dynptr_adjust)\nBTF_ID_FLAGS(func, bpf_dynptr_is_null)\nBTF_ID_FLAGS(func, bpf_dynptr_is_rdonly)\nBTF_ID_FLAGS(func, bpf_dynptr_size)\nBTF_ID_FLAGS(func, bpf_dynptr_clone)\nBTF_SET8_END(common_btf_ids)\n\nstatic const struct btf_kfunc_id_set common_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set   = &common_btf_ids,\n};\n\nstatic int __init kfunc_init(void)\n{\n\tint ret;\n\tconst struct btf_id_dtor_kfunc generic_dtors[] = {\n\t\t{\n\t\t\t.btf_id       = generic_dtor_ids[0],\n\t\t\t.kfunc_btf_id = generic_dtor_ids[1]\n\t\t},\n#ifdef CONFIG_CGROUPS\n\t\t{\n\t\t\t.btf_id       = generic_dtor_ids[2],\n\t\t\t.kfunc_btf_id = generic_dtor_ids[3]\n\t\t},\n#endif\n\t};\n\n\tret = register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING, &generic_kfunc_set);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_SCHED_CLS, &generic_kfunc_set);\n\tret = ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_STRUCT_OPS, &generic_kfunc_set);\n\tret = ret ?: register_btf_id_dtor_kfuncs(generic_dtors,\n\t\t\t\t\t\t  ARRAY_SIZE(generic_dtors),\n\t\t\t\t\t\t  THIS_MODULE);\n\treturn ret ?: register_btf_kfunc_id_set(BPF_PROG_TYPE_UNSPEC, &common_kfunc_set);\n}\n\nlate_initcall(kfunc_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}