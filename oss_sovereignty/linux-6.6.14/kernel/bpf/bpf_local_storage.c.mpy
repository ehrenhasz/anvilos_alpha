{
  "module_name": "bpf_local_storage.c",
  "hash_id": "b59effa9afb7e80b98941f4f0d04a27c60acd948e6d828a3d92012aa5442ffe7",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/bpf_local_storage.c",
  "human_readable_source": "\n \n#include <linux/rculist.h>\n#include <linux/list.h>\n#include <linux/hash.h>\n#include <linux/types.h>\n#include <linux/spinlock.h>\n#include <linux/bpf.h>\n#include <linux/btf_ids.h>\n#include <linux/bpf_local_storage.h>\n#include <net/sock.h>\n#include <uapi/linux/sock_diag.h>\n#include <uapi/linux/btf.h>\n#include <linux/rcupdate.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/rcupdate_wait.h>\n\n#define BPF_LOCAL_STORAGE_CREATE_FLAG_MASK (BPF_F_NO_PREALLOC | BPF_F_CLONE)\n\nstatic struct bpf_local_storage_map_bucket *\nselect_bucket(struct bpf_local_storage_map *smap,\n\t      struct bpf_local_storage_elem *selem)\n{\n\treturn &smap->buckets[hash_ptr(selem, smap->bucket_log)];\n}\n\nstatic int mem_charge(struct bpf_local_storage_map *smap, void *owner, u32 size)\n{\n\tstruct bpf_map *map = &smap->map;\n\n\tif (!map->ops->map_local_storage_charge)\n\t\treturn 0;\n\n\treturn map->ops->map_local_storage_charge(smap, owner, size);\n}\n\nstatic void mem_uncharge(struct bpf_local_storage_map *smap, void *owner,\n\t\t\t u32 size)\n{\n\tstruct bpf_map *map = &smap->map;\n\n\tif (map->ops->map_local_storage_uncharge)\n\t\tmap->ops->map_local_storage_uncharge(smap, owner, size);\n}\n\nstatic struct bpf_local_storage __rcu **\nowner_storage(struct bpf_local_storage_map *smap, void *owner)\n{\n\tstruct bpf_map *map = &smap->map;\n\n\treturn map->ops->map_owner_storage_ptr(owner);\n}\n\nstatic bool selem_linked_to_storage_lockless(const struct bpf_local_storage_elem *selem)\n{\n\treturn !hlist_unhashed_lockless(&selem->snode);\n}\n\nstatic bool selem_linked_to_storage(const struct bpf_local_storage_elem *selem)\n{\n\treturn !hlist_unhashed(&selem->snode);\n}\n\nstatic bool selem_linked_to_map_lockless(const struct bpf_local_storage_elem *selem)\n{\n\treturn !hlist_unhashed_lockless(&selem->map_node);\n}\n\nstatic bool selem_linked_to_map(const struct bpf_local_storage_elem *selem)\n{\n\treturn !hlist_unhashed(&selem->map_node);\n}\n\nstruct bpf_local_storage_elem *\nbpf_selem_alloc(struct bpf_local_storage_map *smap, void *owner,\n\t\tvoid *value, bool charge_mem, gfp_t gfp_flags)\n{\n\tstruct bpf_local_storage_elem *selem;\n\n\tif (charge_mem && mem_charge(smap, owner, smap->elem_size))\n\t\treturn NULL;\n\n\tif (smap->bpf_ma) {\n\t\tmigrate_disable();\n\t\tselem = bpf_mem_cache_alloc_flags(&smap->selem_ma, gfp_flags);\n\t\tmigrate_enable();\n\t\tif (selem)\n\t\t\t \n\t\t\tmemset(SDATA(selem)->data, 0, smap->map.value_size);\n\t} else {\n\t\tselem = bpf_map_kzalloc(&smap->map, smap->elem_size,\n\t\t\t\t\tgfp_flags | __GFP_NOWARN);\n\t}\n\n\tif (selem) {\n\t\tif (value)\n\t\t\tcopy_map_value(&smap->map, SDATA(selem)->data, value);\n\t\t \n\t\treturn selem;\n\t}\n\n\tif (charge_mem)\n\t\tmem_uncharge(smap, owner, smap->elem_size);\n\n\treturn NULL;\n}\n\n \nstatic void __bpf_local_storage_free_trace_rcu(struct rcu_head *rcu)\n{\n\tstruct bpf_local_storage *local_storage;\n\n\t \n\tlocal_storage = container_of(rcu, struct bpf_local_storage, rcu);\n\tif (rcu_trace_implies_rcu_gp())\n\t\tkfree(local_storage);\n\telse\n\t\tkfree_rcu(local_storage, rcu);\n}\n\nstatic void bpf_local_storage_free_rcu(struct rcu_head *rcu)\n{\n\tstruct bpf_local_storage *local_storage;\n\n\tlocal_storage = container_of(rcu, struct bpf_local_storage, rcu);\n\tbpf_mem_cache_raw_free(local_storage);\n}\n\nstatic void bpf_local_storage_free_trace_rcu(struct rcu_head *rcu)\n{\n\tif (rcu_trace_implies_rcu_gp())\n\t\tbpf_local_storage_free_rcu(rcu);\n\telse\n\t\tcall_rcu(rcu, bpf_local_storage_free_rcu);\n}\n\n \nstatic void __bpf_local_storage_free(struct bpf_local_storage *local_storage,\n\t\t\t\t     bool vanilla_rcu)\n{\n\tif (vanilla_rcu)\n\t\tkfree_rcu(local_storage, rcu);\n\telse\n\t\tcall_rcu_tasks_trace(&local_storage->rcu,\n\t\t\t\t     __bpf_local_storage_free_trace_rcu);\n}\n\nstatic void bpf_local_storage_free(struct bpf_local_storage *local_storage,\n\t\t\t\t   struct bpf_local_storage_map *smap,\n\t\t\t\t   bool bpf_ma, bool reuse_now)\n{\n\tif (!local_storage)\n\t\treturn;\n\n\tif (!bpf_ma) {\n\t\t__bpf_local_storage_free(local_storage, reuse_now);\n\t\treturn;\n\t}\n\n\tif (!reuse_now) {\n\t\tcall_rcu_tasks_trace(&local_storage->rcu,\n\t\t\t\t     bpf_local_storage_free_trace_rcu);\n\t\treturn;\n\t}\n\n\tif (smap) {\n\t\tmigrate_disable();\n\t\tbpf_mem_cache_free(&smap->storage_ma, local_storage);\n\t\tmigrate_enable();\n\t} else {\n\t\t \n\t\tcall_rcu(&local_storage->rcu, bpf_local_storage_free_rcu);\n\t}\n}\n\n \nstatic void __bpf_selem_free_trace_rcu(struct rcu_head *rcu)\n{\n\tstruct bpf_local_storage_elem *selem;\n\n\tselem = container_of(rcu, struct bpf_local_storage_elem, rcu);\n\tif (rcu_trace_implies_rcu_gp())\n\t\tkfree(selem);\n\telse\n\t\tkfree_rcu(selem, rcu);\n}\n\n \nstatic void __bpf_selem_free(struct bpf_local_storage_elem *selem,\n\t\t\t     bool vanilla_rcu)\n{\n\tif (vanilla_rcu)\n\t\tkfree_rcu(selem, rcu);\n\telse\n\t\tcall_rcu_tasks_trace(&selem->rcu, __bpf_selem_free_trace_rcu);\n}\n\nstatic void bpf_selem_free_rcu(struct rcu_head *rcu)\n{\n\tstruct bpf_local_storage_elem *selem;\n\n\tselem = container_of(rcu, struct bpf_local_storage_elem, rcu);\n\tbpf_mem_cache_raw_free(selem);\n}\n\nstatic void bpf_selem_free_trace_rcu(struct rcu_head *rcu)\n{\n\tif (rcu_trace_implies_rcu_gp())\n\t\tbpf_selem_free_rcu(rcu);\n\telse\n\t\tcall_rcu(rcu, bpf_selem_free_rcu);\n}\n\nvoid bpf_selem_free(struct bpf_local_storage_elem *selem,\n\t\t    struct bpf_local_storage_map *smap,\n\t\t    bool reuse_now)\n{\n\tbpf_obj_free_fields(smap->map.record, SDATA(selem)->data);\n\n\tif (!smap->bpf_ma) {\n\t\t__bpf_selem_free(selem, reuse_now);\n\t\treturn;\n\t}\n\n\tif (!reuse_now) {\n\t\tcall_rcu_tasks_trace(&selem->rcu, bpf_selem_free_trace_rcu);\n\t} else {\n\t\t \n\t\tmigrate_disable();\n\t\tbpf_mem_cache_free(&smap->selem_ma, selem);\n\t\tmigrate_enable();\n\t}\n}\n\n \nstatic bool bpf_selem_unlink_storage_nolock(struct bpf_local_storage *local_storage,\n\t\t\t\t\t    struct bpf_local_storage_elem *selem,\n\t\t\t\t\t    bool uncharge_mem, bool reuse_now)\n{\n\tstruct bpf_local_storage_map *smap;\n\tbool free_local_storage;\n\tvoid *owner;\n\n\tsmap = rcu_dereference_check(SDATA(selem)->smap, bpf_rcu_lock_held());\n\towner = local_storage->owner;\n\n\t \n\tif (uncharge_mem)\n\t\tmem_uncharge(smap, owner, smap->elem_size);\n\n\tfree_local_storage = hlist_is_singular_node(&selem->snode,\n\t\t\t\t\t\t    &local_storage->list);\n\tif (free_local_storage) {\n\t\tmem_uncharge(smap, owner, sizeof(struct bpf_local_storage));\n\t\tlocal_storage->owner = NULL;\n\n\t\t \n\t\tRCU_INIT_POINTER(*owner_storage(smap, owner), NULL);\n\n\t\t \n\t}\n\thlist_del_init_rcu(&selem->snode);\n\tif (rcu_access_pointer(local_storage->cache[smap->cache_idx]) ==\n\t    SDATA(selem))\n\t\tRCU_INIT_POINTER(local_storage->cache[smap->cache_idx], NULL);\n\n\tbpf_selem_free(selem, smap, reuse_now);\n\n\tif (rcu_access_pointer(local_storage->smap) == smap)\n\t\tRCU_INIT_POINTER(local_storage->smap, NULL);\n\n\treturn free_local_storage;\n}\n\nstatic bool check_storage_bpf_ma(struct bpf_local_storage *local_storage,\n\t\t\t\t struct bpf_local_storage_map *storage_smap,\n\t\t\t\t struct bpf_local_storage_elem *selem)\n{\n\n\tstruct bpf_local_storage_map *selem_smap;\n\n\t \n\n\tif (storage_smap)\n\t\treturn storage_smap->bpf_ma;\n\n\tif (!selem) {\n\t\tstruct hlist_node *n;\n\n\t\tn = rcu_dereference_check(hlist_first_rcu(&local_storage->list),\n\t\t\t\t\t  bpf_rcu_lock_held());\n\t\tif (!n)\n\t\t\treturn false;\n\n\t\tselem = hlist_entry(n, struct bpf_local_storage_elem, snode);\n\t}\n\tselem_smap = rcu_dereference_check(SDATA(selem)->smap, bpf_rcu_lock_held());\n\n\treturn selem_smap->bpf_ma;\n}\n\nstatic void bpf_selem_unlink_storage(struct bpf_local_storage_elem *selem,\n\t\t\t\t     bool reuse_now)\n{\n\tstruct bpf_local_storage_map *storage_smap;\n\tstruct bpf_local_storage *local_storage;\n\tbool bpf_ma, free_local_storage = false;\n\tunsigned long flags;\n\n\tif (unlikely(!selem_linked_to_storage_lockless(selem)))\n\t\t \n\t\treturn;\n\n\tlocal_storage = rcu_dereference_check(selem->local_storage,\n\t\t\t\t\t      bpf_rcu_lock_held());\n\tstorage_smap = rcu_dereference_check(local_storage->smap,\n\t\t\t\t\t     bpf_rcu_lock_held());\n\tbpf_ma = check_storage_bpf_ma(local_storage, storage_smap, selem);\n\n\traw_spin_lock_irqsave(&local_storage->lock, flags);\n\tif (likely(selem_linked_to_storage(selem)))\n\t\tfree_local_storage = bpf_selem_unlink_storage_nolock(\n\t\t\tlocal_storage, selem, true, reuse_now);\n\traw_spin_unlock_irqrestore(&local_storage->lock, flags);\n\n\tif (free_local_storage)\n\t\tbpf_local_storage_free(local_storage, storage_smap, bpf_ma, reuse_now);\n}\n\nvoid bpf_selem_link_storage_nolock(struct bpf_local_storage *local_storage,\n\t\t\t\t   struct bpf_local_storage_elem *selem)\n{\n\tRCU_INIT_POINTER(selem->local_storage, local_storage);\n\thlist_add_head_rcu(&selem->snode, &local_storage->list);\n}\n\nstatic void bpf_selem_unlink_map(struct bpf_local_storage_elem *selem)\n{\n\tstruct bpf_local_storage_map *smap;\n\tstruct bpf_local_storage_map_bucket *b;\n\tunsigned long flags;\n\n\tif (unlikely(!selem_linked_to_map_lockless(selem)))\n\t\t \n\t\treturn;\n\n\tsmap = rcu_dereference_check(SDATA(selem)->smap, bpf_rcu_lock_held());\n\tb = select_bucket(smap, selem);\n\traw_spin_lock_irqsave(&b->lock, flags);\n\tif (likely(selem_linked_to_map(selem)))\n\t\thlist_del_init_rcu(&selem->map_node);\n\traw_spin_unlock_irqrestore(&b->lock, flags);\n}\n\nvoid bpf_selem_link_map(struct bpf_local_storage_map *smap,\n\t\t\tstruct bpf_local_storage_elem *selem)\n{\n\tstruct bpf_local_storage_map_bucket *b = select_bucket(smap, selem);\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&b->lock, flags);\n\tRCU_INIT_POINTER(SDATA(selem)->smap, smap);\n\thlist_add_head_rcu(&selem->map_node, &b->list);\n\traw_spin_unlock_irqrestore(&b->lock, flags);\n}\n\nvoid bpf_selem_unlink(struct bpf_local_storage_elem *selem, bool reuse_now)\n{\n\t \n\tbpf_selem_unlink_map(selem);\n\tbpf_selem_unlink_storage(selem, reuse_now);\n}\n\n \nstruct bpf_local_storage_data *\nbpf_local_storage_lookup(struct bpf_local_storage *local_storage,\n\t\t\t struct bpf_local_storage_map *smap,\n\t\t\t bool cacheit_lockit)\n{\n\tstruct bpf_local_storage_data *sdata;\n\tstruct bpf_local_storage_elem *selem;\n\n\t \n\tsdata = rcu_dereference_check(local_storage->cache[smap->cache_idx],\n\t\t\t\t      bpf_rcu_lock_held());\n\tif (sdata && rcu_access_pointer(sdata->smap) == smap)\n\t\treturn sdata;\n\n\t \n\thlist_for_each_entry_rcu(selem, &local_storage->list, snode,\n\t\t\t\t  rcu_read_lock_trace_held())\n\t\tif (rcu_access_pointer(SDATA(selem)->smap) == smap)\n\t\t\tbreak;\n\n\tif (!selem)\n\t\treturn NULL;\n\n\tsdata = SDATA(selem);\n\tif (cacheit_lockit) {\n\t\tunsigned long flags;\n\n\t\t \n\t\traw_spin_lock_irqsave(&local_storage->lock, flags);\n\t\tif (selem_linked_to_storage(selem))\n\t\t\trcu_assign_pointer(local_storage->cache[smap->cache_idx],\n\t\t\t\t\t   sdata);\n\t\traw_spin_unlock_irqrestore(&local_storage->lock, flags);\n\t}\n\n\treturn sdata;\n}\n\nstatic int check_flags(const struct bpf_local_storage_data *old_sdata,\n\t\t       u64 map_flags)\n{\n\tif (old_sdata && (map_flags & ~BPF_F_LOCK) == BPF_NOEXIST)\n\t\t \n\t\treturn -EEXIST;\n\n\tif (!old_sdata && (map_flags & ~BPF_F_LOCK) == BPF_EXIST)\n\t\t \n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nint bpf_local_storage_alloc(void *owner,\n\t\t\t    struct bpf_local_storage_map *smap,\n\t\t\t    struct bpf_local_storage_elem *first_selem,\n\t\t\t    gfp_t gfp_flags)\n{\n\tstruct bpf_local_storage *prev_storage, *storage;\n\tstruct bpf_local_storage **owner_storage_ptr;\n\tint err;\n\n\terr = mem_charge(smap, owner, sizeof(*storage));\n\tif (err)\n\t\treturn err;\n\n\tif (smap->bpf_ma) {\n\t\tmigrate_disable();\n\t\tstorage = bpf_mem_cache_alloc_flags(&smap->storage_ma, gfp_flags);\n\t\tmigrate_enable();\n\t} else {\n\t\tstorage = bpf_map_kzalloc(&smap->map, sizeof(*storage),\n\t\t\t\t\t  gfp_flags | __GFP_NOWARN);\n\t}\n\n\tif (!storage) {\n\t\terr = -ENOMEM;\n\t\tgoto uncharge;\n\t}\n\n\tRCU_INIT_POINTER(storage->smap, smap);\n\tINIT_HLIST_HEAD(&storage->list);\n\traw_spin_lock_init(&storage->lock);\n\tstorage->owner = owner;\n\n\tbpf_selem_link_storage_nolock(storage, first_selem);\n\tbpf_selem_link_map(smap, first_selem);\n\n\towner_storage_ptr =\n\t\t(struct bpf_local_storage **)owner_storage(smap, owner);\n\t \n\tprev_storage = cmpxchg(owner_storage_ptr, NULL, storage);\n\tif (unlikely(prev_storage)) {\n\t\tbpf_selem_unlink_map(first_selem);\n\t\terr = -EAGAIN;\n\t\tgoto uncharge;\n\n\t\t \n\t}\n\n\treturn 0;\n\nuncharge:\n\tbpf_local_storage_free(storage, smap, smap->bpf_ma, true);\n\tmem_uncharge(smap, owner, sizeof(*storage));\n\treturn err;\n}\n\n \nstruct bpf_local_storage_data *\nbpf_local_storage_update(void *owner, struct bpf_local_storage_map *smap,\n\t\t\t void *value, u64 map_flags, gfp_t gfp_flags)\n{\n\tstruct bpf_local_storage_data *old_sdata = NULL;\n\tstruct bpf_local_storage_elem *alloc_selem, *selem = NULL;\n\tstruct bpf_local_storage *local_storage;\n\tunsigned long flags;\n\tint err;\n\n\t \n\tif (unlikely((map_flags & ~BPF_F_LOCK) > BPF_EXIST) ||\n\t     \n\t    unlikely((map_flags & BPF_F_LOCK) &&\n\t\t     !btf_record_has_field(smap->map.record, BPF_SPIN_LOCK)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (gfp_flags == GFP_KERNEL && (map_flags & ~BPF_F_LOCK) != BPF_NOEXIST)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tlocal_storage = rcu_dereference_check(*owner_storage(smap, owner),\n\t\t\t\t\t      bpf_rcu_lock_held());\n\tif (!local_storage || hlist_empty(&local_storage->list)) {\n\t\t \n\t\terr = check_flags(NULL, map_flags);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\n\t\tselem = bpf_selem_alloc(smap, owner, value, true, gfp_flags);\n\t\tif (!selem)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\n\t\terr = bpf_local_storage_alloc(owner, smap, selem, gfp_flags);\n\t\tif (err) {\n\t\t\tbpf_selem_free(selem, smap, true);\n\t\t\tmem_uncharge(smap, owner, smap->elem_size);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\n\t\treturn SDATA(selem);\n\t}\n\n\tif ((map_flags & BPF_F_LOCK) && !(map_flags & BPF_NOEXIST)) {\n\t\t \n\t\told_sdata =\n\t\t\tbpf_local_storage_lookup(local_storage, smap, false);\n\t\terr = check_flags(old_sdata, map_flags);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t\tif (old_sdata && selem_linked_to_storage_lockless(SELEM(old_sdata))) {\n\t\t\tcopy_map_value_locked(&smap->map, old_sdata->data,\n\t\t\t\t\t      value, false);\n\t\t\treturn old_sdata;\n\t\t}\n\t}\n\n\t \n\talloc_selem = selem = bpf_selem_alloc(smap, owner, value, true, gfp_flags);\n\tif (!alloc_selem)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\traw_spin_lock_irqsave(&local_storage->lock, flags);\n\n\t \n\tif (unlikely(hlist_empty(&local_storage->list))) {\n\t\t \n\t\terr = -EAGAIN;\n\t\tgoto unlock;\n\t}\n\n\told_sdata = bpf_local_storage_lookup(local_storage, smap, false);\n\terr = check_flags(old_sdata, map_flags);\n\tif (err)\n\t\tgoto unlock;\n\n\tif (old_sdata && (map_flags & BPF_F_LOCK)) {\n\t\tcopy_map_value_locked(&smap->map, old_sdata->data, value,\n\t\t\t\t      false);\n\t\tselem = SELEM(old_sdata);\n\t\tgoto unlock;\n\t}\n\n\talloc_selem = NULL;\n\t \n\tbpf_selem_link_map(smap, selem);\n\n\t \n\tbpf_selem_link_storage_nolock(local_storage, selem);\n\n\t \n\tif (old_sdata) {\n\t\tbpf_selem_unlink_map(SELEM(old_sdata));\n\t\tbpf_selem_unlink_storage_nolock(local_storage, SELEM(old_sdata),\n\t\t\t\t\t\ttrue, false);\n\t}\n\nunlock:\n\traw_spin_unlock_irqrestore(&local_storage->lock, flags);\n\tif (alloc_selem) {\n\t\tmem_uncharge(smap, owner, smap->elem_size);\n\t\tbpf_selem_free(alloc_selem, smap, true);\n\t}\n\treturn err ? ERR_PTR(err) : SDATA(selem);\n}\n\nstatic u16 bpf_local_storage_cache_idx_get(struct bpf_local_storage_cache *cache)\n{\n\tu64 min_usage = U64_MAX;\n\tu16 i, res = 0;\n\n\tspin_lock(&cache->idx_lock);\n\n\tfor (i = 0; i < BPF_LOCAL_STORAGE_CACHE_SIZE; i++) {\n\t\tif (cache->idx_usage_counts[i] < min_usage) {\n\t\t\tmin_usage = cache->idx_usage_counts[i];\n\t\t\tres = i;\n\n\t\t\t \n\t\t\tif (!min_usage)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tcache->idx_usage_counts[res]++;\n\n\tspin_unlock(&cache->idx_lock);\n\n\treturn res;\n}\n\nstatic void bpf_local_storage_cache_idx_free(struct bpf_local_storage_cache *cache,\n\t\t\t\t\t     u16 idx)\n{\n\tspin_lock(&cache->idx_lock);\n\tcache->idx_usage_counts[idx]--;\n\tspin_unlock(&cache->idx_lock);\n}\n\nint bpf_local_storage_map_alloc_check(union bpf_attr *attr)\n{\n\tif (attr->map_flags & ~BPF_LOCAL_STORAGE_CREATE_FLAG_MASK ||\n\t    !(attr->map_flags & BPF_F_NO_PREALLOC) ||\n\t    attr->max_entries ||\n\t    attr->key_size != sizeof(int) || !attr->value_size ||\n\t     \n\t    !attr->btf_key_type_id || !attr->btf_value_type_id)\n\t\treturn -EINVAL;\n\n\tif (attr->value_size > BPF_LOCAL_STORAGE_MAX_VALUE_SIZE)\n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nint bpf_local_storage_map_check_btf(const struct bpf_map *map,\n\t\t\t\t    const struct btf *btf,\n\t\t\t\t    const struct btf_type *key_type,\n\t\t\t\t    const struct btf_type *value_type)\n{\n\tu32 int_data;\n\n\tif (BTF_INFO_KIND(key_type->info) != BTF_KIND_INT)\n\t\treturn -EINVAL;\n\n\tint_data = *(u32 *)(key_type + 1);\n\tif (BTF_INT_BITS(int_data) != 32 || BTF_INT_OFFSET(int_data))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nvoid bpf_local_storage_destroy(struct bpf_local_storage *local_storage)\n{\n\tstruct bpf_local_storage_map *storage_smap;\n\tstruct bpf_local_storage_elem *selem;\n\tbool bpf_ma, free_storage = false;\n\tstruct hlist_node *n;\n\tunsigned long flags;\n\n\tstorage_smap = rcu_dereference_check(local_storage->smap, bpf_rcu_lock_held());\n\tbpf_ma = check_storage_bpf_ma(local_storage, storage_smap, NULL);\n\n\t \n\traw_spin_lock_irqsave(&local_storage->lock, flags);\n\thlist_for_each_entry_safe(selem, n, &local_storage->list, snode) {\n\t\t \n\t\tbpf_selem_unlink_map(selem);\n\t\t \n\t\tfree_storage = bpf_selem_unlink_storage_nolock(\n\t\t\tlocal_storage, selem, true, true);\n\t}\n\traw_spin_unlock_irqrestore(&local_storage->lock, flags);\n\n\tif (free_storage)\n\t\tbpf_local_storage_free(local_storage, storage_smap, bpf_ma, true);\n}\n\nu64 bpf_local_storage_map_mem_usage(const struct bpf_map *map)\n{\n\tstruct bpf_local_storage_map *smap = (struct bpf_local_storage_map *)map;\n\tu64 usage = sizeof(*smap);\n\n\t \n\tusage += sizeof(*smap->buckets) * (1ULL << smap->bucket_log);\n\treturn usage;\n}\n\n \nstruct bpf_map *\nbpf_local_storage_map_alloc(union bpf_attr *attr,\n\t\t\t    struct bpf_local_storage_cache *cache,\n\t\t\t    bool bpf_ma)\n{\n\tstruct bpf_local_storage_map *smap;\n\tunsigned int i;\n\tu32 nbuckets;\n\tint err;\n\n\tsmap = bpf_map_area_alloc(sizeof(*smap), NUMA_NO_NODE);\n\tif (!smap)\n\t\treturn ERR_PTR(-ENOMEM);\n\tbpf_map_init_from_attr(&smap->map, attr);\n\n\tnbuckets = roundup_pow_of_two(num_possible_cpus());\n\t \n\tnbuckets = max_t(u32, 2, nbuckets);\n\tsmap->bucket_log = ilog2(nbuckets);\n\n\tsmap->buckets = bpf_map_kvcalloc(&smap->map, sizeof(*smap->buckets),\n\t\t\t\t\t nbuckets, GFP_USER | __GFP_NOWARN);\n\tif (!smap->buckets) {\n\t\terr = -ENOMEM;\n\t\tgoto free_smap;\n\t}\n\n\tfor (i = 0; i < nbuckets; i++) {\n\t\tINIT_HLIST_HEAD(&smap->buckets[i].list);\n\t\traw_spin_lock_init(&smap->buckets[i].lock);\n\t}\n\n\tsmap->elem_size = offsetof(struct bpf_local_storage_elem,\n\t\t\t\t   sdata.data[attr->value_size]);\n\n\tsmap->bpf_ma = bpf_ma;\n\tif (bpf_ma) {\n\t\terr = bpf_mem_alloc_init(&smap->selem_ma, smap->elem_size, false);\n\t\tif (err)\n\t\t\tgoto free_smap;\n\n\t\terr = bpf_mem_alloc_init(&smap->storage_ma, sizeof(struct bpf_local_storage), false);\n\t\tif (err) {\n\t\t\tbpf_mem_alloc_destroy(&smap->selem_ma);\n\t\t\tgoto free_smap;\n\t\t}\n\t}\n\n\tsmap->cache_idx = bpf_local_storage_cache_idx_get(cache);\n\treturn &smap->map;\n\nfree_smap:\n\tkvfree(smap->buckets);\n\tbpf_map_area_free(smap);\n\treturn ERR_PTR(err);\n}\n\nvoid bpf_local_storage_map_free(struct bpf_map *map,\n\t\t\t\tstruct bpf_local_storage_cache *cache,\n\t\t\t\tint __percpu *busy_counter)\n{\n\tstruct bpf_local_storage_map_bucket *b;\n\tstruct bpf_local_storage_elem *selem;\n\tstruct bpf_local_storage_map *smap;\n\tunsigned int i;\n\n\tsmap = (struct bpf_local_storage_map *)map;\n\tbpf_local_storage_cache_idx_free(cache, smap->cache_idx);\n\n\t \n\tsynchronize_rcu();\n\n\t \n\tfor (i = 0; i < (1U << smap->bucket_log); i++) {\n\t\tb = &smap->buckets[i];\n\n\t\trcu_read_lock();\n\t\t \n\t\twhile ((selem = hlist_entry_safe(\n\t\t\t\trcu_dereference_raw(hlist_first_rcu(&b->list)),\n\t\t\t\tstruct bpf_local_storage_elem, map_node))) {\n\t\t\tif (busy_counter) {\n\t\t\t\tmigrate_disable();\n\t\t\t\tthis_cpu_inc(*busy_counter);\n\t\t\t}\n\t\t\tbpf_selem_unlink(selem, true);\n\t\t\tif (busy_counter) {\n\t\t\t\tthis_cpu_dec(*busy_counter);\n\t\t\t\tmigrate_enable();\n\t\t\t}\n\t\t\tcond_resched_rcu();\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\tif (smap->bpf_ma) {\n\t\tbpf_mem_alloc_destroy(&smap->selem_ma);\n\t\tbpf_mem_alloc_destroy(&smap->storage_ma);\n\t}\n\tkvfree(smap->buckets);\n\tbpf_map_area_free(smap);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}