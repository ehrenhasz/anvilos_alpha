{
  "module_name": "hashtab.c",
  "hash_id": "e1f48b86c4628b00410b91f4a7a583b5d43c2514ba660549468130a5f217c34f",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/hashtab.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/jhash.h>\n#include <linux/filter.h>\n#include <linux/rculist_nulls.h>\n#include <linux/random.h>\n#include <uapi/linux/btf.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/btf_ids.h>\n#include \"percpu_freelist.h\"\n#include \"bpf_lru_list.h\"\n#include \"map_in_map.h\"\n#include <linux/bpf_mem_alloc.h>\n\n#define HTAB_CREATE_FLAG_MASK\t\t\t\t\t\t\\\n\t(BPF_F_NO_PREALLOC | BPF_F_NO_COMMON_LRU | BPF_F_NUMA_NODE |\t\\\n\t BPF_F_ACCESS_MASK | BPF_F_ZERO_SEED)\n\n#define BATCH_OPS(_name)\t\t\t\\\n\t.map_lookup_batch =\t\t\t\\\n\t_name##_map_lookup_batch,\t\t\\\n\t.map_lookup_and_delete_batch =\t\t\\\n\t_name##_map_lookup_and_delete_batch,\t\\\n\t.map_update_batch =\t\t\t\\\n\tgeneric_map_update_batch,\t\t\\\n\t.map_delete_batch =\t\t\t\\\n\tgeneric_map_delete_batch\n\n \nstruct bucket {\n\tstruct hlist_nulls_head head;\n\traw_spinlock_t raw_lock;\n};\n\n#define HASHTAB_MAP_LOCK_COUNT 8\n#define HASHTAB_MAP_LOCK_MASK (HASHTAB_MAP_LOCK_COUNT - 1)\n\nstruct bpf_htab {\n\tstruct bpf_map map;\n\tstruct bpf_mem_alloc ma;\n\tstruct bpf_mem_alloc pcpu_ma;\n\tstruct bucket *buckets;\n\tvoid *elems;\n\tunion {\n\t\tstruct pcpu_freelist freelist;\n\t\tstruct bpf_lru lru;\n\t};\n\tstruct htab_elem *__percpu *extra_elems;\n\t \n\tstruct percpu_counter pcount;\n\tatomic_t count;\n\tbool use_percpu_counter;\n\tu32 n_buckets;\t \n\tu32 elem_size;\t \n\tu32 hashrnd;\n\tstruct lock_class_key lockdep_key;\n\tint __percpu *map_locked[HASHTAB_MAP_LOCK_COUNT];\n};\n\n \nstruct htab_elem {\n\tunion {\n\t\tstruct hlist_nulls_node hash_node;\n\t\tstruct {\n\t\t\tvoid *padding;\n\t\t\tunion {\n\t\t\t\tstruct pcpu_freelist_node fnode;\n\t\t\t\tstruct htab_elem *batch_flink;\n\t\t\t};\n\t\t};\n\t};\n\tunion {\n\t\t \n\t\tvoid *ptr_to_pptr;\n\t\tstruct bpf_lru_node lru_node;\n\t};\n\tu32 hash;\n\tchar key[] __aligned(8);\n};\n\nstatic inline bool htab_is_prealloc(const struct bpf_htab *htab)\n{\n\treturn !(htab->map.map_flags & BPF_F_NO_PREALLOC);\n}\n\nstatic void htab_init_buckets(struct bpf_htab *htab)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < htab->n_buckets; i++) {\n\t\tINIT_HLIST_NULLS_HEAD(&htab->buckets[i].head, i);\n\t\traw_spin_lock_init(&htab->buckets[i].raw_lock);\n\t\tlockdep_set_class(&htab->buckets[i].raw_lock,\n\t\t\t\t\t  &htab->lockdep_key);\n\t\tcond_resched();\n\t}\n}\n\nstatic inline int htab_lock_bucket(const struct bpf_htab *htab,\n\t\t\t\t   struct bucket *b, u32 hash,\n\t\t\t\t   unsigned long *pflags)\n{\n\tunsigned long flags;\n\n\thash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);\n\n\tpreempt_disable();\n\tlocal_irq_save(flags);\n\tif (unlikely(__this_cpu_inc_return(*(htab->map_locked[hash])) != 1)) {\n\t\t__this_cpu_dec(*(htab->map_locked[hash]));\n\t\tlocal_irq_restore(flags);\n\t\tpreempt_enable();\n\t\treturn -EBUSY;\n\t}\n\n\traw_spin_lock(&b->raw_lock);\n\t*pflags = flags;\n\n\treturn 0;\n}\n\nstatic inline void htab_unlock_bucket(const struct bpf_htab *htab,\n\t\t\t\t      struct bucket *b, u32 hash,\n\t\t\t\t      unsigned long flags)\n{\n\thash = hash & min_t(u32, HASHTAB_MAP_LOCK_MASK, htab->n_buckets - 1);\n\traw_spin_unlock(&b->raw_lock);\n\t__this_cpu_dec(*(htab->map_locked[hash]));\n\tlocal_irq_restore(flags);\n\tpreempt_enable();\n}\n\nstatic bool htab_lru_map_delete_node(void *arg, struct bpf_lru_node *node);\n\nstatic bool htab_is_lru(const struct bpf_htab *htab)\n{\n\treturn htab->map.map_type == BPF_MAP_TYPE_LRU_HASH ||\n\t\thtab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;\n}\n\nstatic bool htab_is_percpu(const struct bpf_htab *htab)\n{\n\treturn htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t\thtab->map.map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH;\n}\n\nstatic inline void htab_elem_set_ptr(struct htab_elem *l, u32 key_size,\n\t\t\t\t     void __percpu *pptr)\n{\n\t*(void __percpu **)(l->key + key_size) = pptr;\n}\n\nstatic inline void __percpu *htab_elem_get_ptr(struct htab_elem *l, u32 key_size)\n{\n\treturn *(void __percpu **)(l->key + key_size);\n}\n\nstatic void *fd_htab_map_get_ptr(const struct bpf_map *map, struct htab_elem *l)\n{\n\treturn *(void **)(l->key + roundup(map->key_size, 8));\n}\n\nstatic struct htab_elem *get_htab_elem(struct bpf_htab *htab, int i)\n{\n\treturn (struct htab_elem *) (htab->elems + i * (u64)htab->elem_size);\n}\n\nstatic bool htab_has_extra_elems(struct bpf_htab *htab)\n{\n\treturn !htab_is_percpu(htab) && !htab_is_lru(htab);\n}\n\nstatic void htab_free_prealloced_timers(struct bpf_htab *htab)\n{\n\tu32 num_entries = htab->map.max_entries;\n\tint i;\n\n\tif (!btf_record_has_field(htab->map.record, BPF_TIMER))\n\t\treturn;\n\tif (htab_has_extra_elems(htab))\n\t\tnum_entries += num_possible_cpus();\n\n\tfor (i = 0; i < num_entries; i++) {\n\t\tstruct htab_elem *elem;\n\n\t\telem = get_htab_elem(htab, i);\n\t\tbpf_obj_free_timer(htab->map.record, elem->key + round_up(htab->map.key_size, 8));\n\t\tcond_resched();\n\t}\n}\n\nstatic void htab_free_prealloced_fields(struct bpf_htab *htab)\n{\n\tu32 num_entries = htab->map.max_entries;\n\tint i;\n\n\tif (IS_ERR_OR_NULL(htab->map.record))\n\t\treturn;\n\tif (htab_has_extra_elems(htab))\n\t\tnum_entries += num_possible_cpus();\n\tfor (i = 0; i < num_entries; i++) {\n\t\tstruct htab_elem *elem;\n\n\t\telem = get_htab_elem(htab, i);\n\t\tif (htab_is_percpu(htab)) {\n\t\t\tvoid __percpu *pptr = htab_elem_get_ptr(elem, htab->map.key_size);\n\t\t\tint cpu;\n\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tbpf_obj_free_fields(htab->map.record, per_cpu_ptr(pptr, cpu));\n\t\t\t\tcond_resched();\n\t\t\t}\n\t\t} else {\n\t\t\tbpf_obj_free_fields(htab->map.record, elem->key + round_up(htab->map.key_size, 8));\n\t\t\tcond_resched();\n\t\t}\n\t\tcond_resched();\n\t}\n}\n\nstatic void htab_free_elems(struct bpf_htab *htab)\n{\n\tint i;\n\n\tif (!htab_is_percpu(htab))\n\t\tgoto free_elems;\n\n\tfor (i = 0; i < htab->map.max_entries; i++) {\n\t\tvoid __percpu *pptr;\n\n\t\tpptr = htab_elem_get_ptr(get_htab_elem(htab, i),\n\t\t\t\t\t htab->map.key_size);\n\t\tfree_percpu(pptr);\n\t\tcond_resched();\n\t}\nfree_elems:\n\tbpf_map_area_free(htab->elems);\n}\n\n \nstatic struct htab_elem *prealloc_lru_pop(struct bpf_htab *htab, void *key,\n\t\t\t\t\t  u32 hash)\n{\n\tstruct bpf_lru_node *node = bpf_lru_pop_free(&htab->lru, hash);\n\tstruct htab_elem *l;\n\n\tif (node) {\n\t\tbpf_map_inc_elem_count(&htab->map);\n\t\tl = container_of(node, struct htab_elem, lru_node);\n\t\tmemcpy(l->key, key, htab->map.key_size);\n\t\treturn l;\n\t}\n\n\treturn NULL;\n}\n\nstatic int prealloc_init(struct bpf_htab *htab)\n{\n\tu32 num_entries = htab->map.max_entries;\n\tint err = -ENOMEM, i;\n\n\tif (htab_has_extra_elems(htab))\n\t\tnum_entries += num_possible_cpus();\n\n\thtab->elems = bpf_map_area_alloc((u64)htab->elem_size * num_entries,\n\t\t\t\t\t htab->map.numa_node);\n\tif (!htab->elems)\n\t\treturn -ENOMEM;\n\n\tif (!htab_is_percpu(htab))\n\t\tgoto skip_percpu_elems;\n\n\tfor (i = 0; i < num_entries; i++) {\n\t\tu32 size = round_up(htab->map.value_size, 8);\n\t\tvoid __percpu *pptr;\n\n\t\tpptr = bpf_map_alloc_percpu(&htab->map, size, 8,\n\t\t\t\t\t    GFP_USER | __GFP_NOWARN);\n\t\tif (!pptr)\n\t\t\tgoto free_elems;\n\t\thtab_elem_set_ptr(get_htab_elem(htab, i), htab->map.key_size,\n\t\t\t\t  pptr);\n\t\tcond_resched();\n\t}\n\nskip_percpu_elems:\n\tif (htab_is_lru(htab))\n\t\terr = bpf_lru_init(&htab->lru,\n\t\t\t\t   htab->map.map_flags & BPF_F_NO_COMMON_LRU,\n\t\t\t\t   offsetof(struct htab_elem, hash) -\n\t\t\t\t   offsetof(struct htab_elem, lru_node),\n\t\t\t\t   htab_lru_map_delete_node,\n\t\t\t\t   htab);\n\telse\n\t\terr = pcpu_freelist_init(&htab->freelist);\n\n\tif (err)\n\t\tgoto free_elems;\n\n\tif (htab_is_lru(htab))\n\t\tbpf_lru_populate(&htab->lru, htab->elems,\n\t\t\t\t offsetof(struct htab_elem, lru_node),\n\t\t\t\t htab->elem_size, num_entries);\n\telse\n\t\tpcpu_freelist_populate(&htab->freelist,\n\t\t\t\t       htab->elems + offsetof(struct htab_elem, fnode),\n\t\t\t\t       htab->elem_size, num_entries);\n\n\treturn 0;\n\nfree_elems:\n\thtab_free_elems(htab);\n\treturn err;\n}\n\nstatic void prealloc_destroy(struct bpf_htab *htab)\n{\n\thtab_free_elems(htab);\n\n\tif (htab_is_lru(htab))\n\t\tbpf_lru_destroy(&htab->lru);\n\telse\n\t\tpcpu_freelist_destroy(&htab->freelist);\n}\n\nstatic int alloc_extra_elems(struct bpf_htab *htab)\n{\n\tstruct htab_elem *__percpu *pptr, *l_new;\n\tstruct pcpu_freelist_node *l;\n\tint cpu;\n\n\tpptr = bpf_map_alloc_percpu(&htab->map, sizeof(struct htab_elem *), 8,\n\t\t\t\t    GFP_USER | __GFP_NOWARN);\n\tif (!pptr)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tl = pcpu_freelist_pop(&htab->freelist);\n\t\t \n\t\tl_new = container_of(l, struct htab_elem, fnode);\n\t\t*per_cpu_ptr(pptr, cpu) = l_new;\n\t}\n\thtab->extra_elems = pptr;\n\treturn 0;\n}\n\n \nstatic int htab_map_alloc_check(union bpf_attr *attr)\n{\n\tbool percpu = (attr->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t\t       attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);\n\tbool lru = (attr->map_type == BPF_MAP_TYPE_LRU_HASH ||\n\t\t    attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);\n\t \n\tbool percpu_lru = (attr->map_flags & BPF_F_NO_COMMON_LRU);\n\tbool prealloc = !(attr->map_flags & BPF_F_NO_PREALLOC);\n\tbool zero_seed = (attr->map_flags & BPF_F_ZERO_SEED);\n\tint numa_node = bpf_map_attr_numa_node(attr);\n\n\tBUILD_BUG_ON(offsetof(struct htab_elem, fnode.next) !=\n\t\t     offsetof(struct htab_elem, hash_node.pprev));\n\n\tif (zero_seed && !capable(CAP_SYS_ADMIN))\n\t\t \n\t\treturn -EPERM;\n\n\tif (attr->map_flags & ~HTAB_CREATE_FLAG_MASK ||\n\t    !bpf_map_flags_access_ok(attr->map_flags))\n\t\treturn -EINVAL;\n\n\tif (!lru && percpu_lru)\n\t\treturn -EINVAL;\n\n\tif (lru && !prealloc)\n\t\treturn -ENOTSUPP;\n\n\tif (numa_node != NUMA_NO_NODE && (percpu || percpu_lru))\n\t\treturn -EINVAL;\n\n\t \n\tif (attr->max_entries == 0 || attr->key_size == 0 ||\n\t    attr->value_size == 0)\n\t\treturn -EINVAL;\n\n\tif ((u64)attr->key_size + attr->value_size >= KMALLOC_MAX_SIZE -\n\t   sizeof(struct htab_elem))\n\t\t \n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nstatic struct bpf_map *htab_map_alloc(union bpf_attr *attr)\n{\n\tbool percpu = (attr->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t\t       attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);\n\tbool lru = (attr->map_type == BPF_MAP_TYPE_LRU_HASH ||\n\t\t    attr->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH);\n\t \n\tbool percpu_lru = (attr->map_flags & BPF_F_NO_COMMON_LRU);\n\tbool prealloc = !(attr->map_flags & BPF_F_NO_PREALLOC);\n\tstruct bpf_htab *htab;\n\tint err, i;\n\n\thtab = bpf_map_area_alloc(sizeof(*htab), NUMA_NO_NODE);\n\tif (!htab)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlockdep_register_key(&htab->lockdep_key);\n\n\tbpf_map_init_from_attr(&htab->map, attr);\n\n\tif (percpu_lru) {\n\t\t \n\t\thtab->map.max_entries = roundup(attr->max_entries,\n\t\t\t\t\t\tnum_possible_cpus());\n\t\tif (htab->map.max_entries < attr->max_entries)\n\t\t\thtab->map.max_entries = rounddown(attr->max_entries,\n\t\t\t\t\t\t\t  num_possible_cpus());\n\t}\n\n\t \n\thtab->n_buckets = roundup_pow_of_two(htab->map.max_entries);\n\n\thtab->elem_size = sizeof(struct htab_elem) +\n\t\t\t  round_up(htab->map.key_size, 8);\n\tif (percpu)\n\t\thtab->elem_size += sizeof(void *);\n\telse\n\t\thtab->elem_size += round_up(htab->map.value_size, 8);\n\n\terr = -E2BIG;\n\t \n\tif (htab->n_buckets == 0 ||\n\t    htab->n_buckets > U32_MAX / sizeof(struct bucket))\n\t\tgoto free_htab;\n\n\terr = bpf_map_init_elem_count(&htab->map);\n\tif (err)\n\t\tgoto free_htab;\n\n\terr = -ENOMEM;\n\thtab->buckets = bpf_map_area_alloc(htab->n_buckets *\n\t\t\t\t\t   sizeof(struct bucket),\n\t\t\t\t\t   htab->map.numa_node);\n\tif (!htab->buckets)\n\t\tgoto free_elem_count;\n\n\tfor (i = 0; i < HASHTAB_MAP_LOCK_COUNT; i++) {\n\t\thtab->map_locked[i] = bpf_map_alloc_percpu(&htab->map,\n\t\t\t\t\t\t\t   sizeof(int),\n\t\t\t\t\t\t\t   sizeof(int),\n\t\t\t\t\t\t\t   GFP_USER);\n\t\tif (!htab->map_locked[i])\n\t\t\tgoto free_map_locked;\n\t}\n\n\tif (htab->map.map_flags & BPF_F_ZERO_SEED)\n\t\thtab->hashrnd = 0;\n\telse\n\t\thtab->hashrnd = get_random_u32();\n\n\thtab_init_buckets(htab);\n\n \n#define PERCPU_COUNTER_BATCH 32\n\tif (attr->max_entries / 2 > num_online_cpus() * PERCPU_COUNTER_BATCH)\n\t\thtab->use_percpu_counter = true;\n\n\tif (htab->use_percpu_counter) {\n\t\terr = percpu_counter_init(&htab->pcount, 0, GFP_KERNEL);\n\t\tif (err)\n\t\t\tgoto free_map_locked;\n\t}\n\n\tif (prealloc) {\n\t\terr = prealloc_init(htab);\n\t\tif (err)\n\t\t\tgoto free_map_locked;\n\n\t\tif (!percpu && !lru) {\n\t\t\t \n\t\t\terr = alloc_extra_elems(htab);\n\t\t\tif (err)\n\t\t\t\tgoto free_prealloc;\n\t\t}\n\t} else {\n\t\terr = bpf_mem_alloc_init(&htab->ma, htab->elem_size, false);\n\t\tif (err)\n\t\t\tgoto free_map_locked;\n\t\tif (percpu) {\n\t\t\terr = bpf_mem_alloc_init(&htab->pcpu_ma,\n\t\t\t\t\t\t round_up(htab->map.value_size, 8), true);\n\t\t\tif (err)\n\t\t\t\tgoto free_map_locked;\n\t\t}\n\t}\n\n\treturn &htab->map;\n\nfree_prealloc:\n\tprealloc_destroy(htab);\nfree_map_locked:\n\tif (htab->use_percpu_counter)\n\t\tpercpu_counter_destroy(&htab->pcount);\n\tfor (i = 0; i < HASHTAB_MAP_LOCK_COUNT; i++)\n\t\tfree_percpu(htab->map_locked[i]);\n\tbpf_map_area_free(htab->buckets);\n\tbpf_mem_alloc_destroy(&htab->pcpu_ma);\n\tbpf_mem_alloc_destroy(&htab->ma);\nfree_elem_count:\n\tbpf_map_free_elem_count(&htab->map);\nfree_htab:\n\tlockdep_unregister_key(&htab->lockdep_key);\n\tbpf_map_area_free(htab);\n\treturn ERR_PTR(err);\n}\n\nstatic inline u32 htab_map_hash(const void *key, u32 key_len, u32 hashrnd)\n{\n\tif (likely(key_len % 4 == 0))\n\t\treturn jhash2(key, key_len / 4, hashrnd);\n\treturn jhash(key, key_len, hashrnd);\n}\n\nstatic inline struct bucket *__select_bucket(struct bpf_htab *htab, u32 hash)\n{\n\treturn &htab->buckets[hash & (htab->n_buckets - 1)];\n}\n\nstatic inline struct hlist_nulls_head *select_bucket(struct bpf_htab *htab, u32 hash)\n{\n\treturn &__select_bucket(htab, hash)->head;\n}\n\n \nstatic struct htab_elem *lookup_elem_raw(struct hlist_nulls_head *head, u32 hash,\n\t\t\t\t\t void *key, u32 key_size)\n{\n\tstruct hlist_nulls_node *n;\n\tstruct htab_elem *l;\n\n\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\n\t\tif (l->hash == hash && !memcmp(&l->key, key, key_size))\n\t\t\treturn l;\n\n\treturn NULL;\n}\n\n \nstatic struct htab_elem *lookup_nulls_elem_raw(struct hlist_nulls_head *head,\n\t\t\t\t\t       u32 hash, void *key,\n\t\t\t\t\t       u32 key_size, u32 n_buckets)\n{\n\tstruct hlist_nulls_node *n;\n\tstruct htab_elem *l;\n\nagain:\n\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\n\t\tif (l->hash == hash && !memcmp(&l->key, key, key_size))\n\t\t\treturn l;\n\n\tif (unlikely(get_nulls_value(n) != (hash & (n_buckets - 1))))\n\t\tgoto again;\n\n\treturn NULL;\n}\n\n \nstatic void *__htab_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_head *head;\n\tstruct htab_elem *l;\n\tu32 hash, key_size;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\n\thead = select_bucket(htab, hash);\n\n\tl = lookup_nulls_elem_raw(head, hash, key, key_size, htab->n_buckets);\n\n\treturn l;\n}\n\nstatic void *htab_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct htab_elem *l = __htab_map_lookup_elem(map, key);\n\n\tif (l)\n\t\treturn l->key + round_up(map->key_size, 8);\n\n\treturn NULL;\n}\n\n \nstatic int htab_map_gen_lookup(struct bpf_map *map, struct bpf_insn *insn_buf)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tconst int ret = BPF_REG_0;\n\n\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,\n\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 1);\n\t*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,\n\t\t\t\toffsetof(struct htab_elem, key) +\n\t\t\t\tround_up(map->key_size, 8));\n\treturn insn - insn_buf;\n}\n\nstatic __always_inline void *__htab_lru_map_lookup_elem(struct bpf_map *map,\n\t\t\t\t\t\t\tvoid *key, const bool mark)\n{\n\tstruct htab_elem *l = __htab_map_lookup_elem(map, key);\n\n\tif (l) {\n\t\tif (mark)\n\t\t\tbpf_lru_node_set_ref(&l->lru_node);\n\t\treturn l->key + round_up(map->key_size, 8);\n\t}\n\n\treturn NULL;\n}\n\nstatic void *htab_lru_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn __htab_lru_map_lookup_elem(map, key, true);\n}\n\nstatic void *htab_lru_map_lookup_elem_sys(struct bpf_map *map, void *key)\n{\n\treturn __htab_lru_map_lookup_elem(map, key, false);\n}\n\nstatic int htab_lru_map_gen_lookup(struct bpf_map *map,\n\t\t\t\t   struct bpf_insn *insn_buf)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tconst int ret = BPF_REG_0;\n\tconst int ref_reg = BPF_REG_1;\n\n\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,\n\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 4);\n\t*insn++ = BPF_LDX_MEM(BPF_B, ref_reg, ret,\n\t\t\t      offsetof(struct htab_elem, lru_node) +\n\t\t\t      offsetof(struct bpf_lru_node, ref));\n\t*insn++ = BPF_JMP_IMM(BPF_JNE, ref_reg, 0, 1);\n\t*insn++ = BPF_ST_MEM(BPF_B, ret,\n\t\t\t     offsetof(struct htab_elem, lru_node) +\n\t\t\t     offsetof(struct bpf_lru_node, ref),\n\t\t\t     1);\n\t*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,\n\t\t\t\toffsetof(struct htab_elem, key) +\n\t\t\t\tround_up(map->key_size, 8));\n\treturn insn - insn_buf;\n}\n\nstatic void check_and_free_fields(struct bpf_htab *htab,\n\t\t\t\t  struct htab_elem *elem)\n{\n\tif (htab_is_percpu(htab)) {\n\t\tvoid __percpu *pptr = htab_elem_get_ptr(elem, htab->map.key_size);\n\t\tint cpu;\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tbpf_obj_free_fields(htab->map.record, per_cpu_ptr(pptr, cpu));\n\t} else {\n\t\tvoid *map_value = elem->key + round_up(htab->map.key_size, 8);\n\n\t\tbpf_obj_free_fields(htab->map.record, map_value);\n\t}\n}\n\n \nstatic bool htab_lru_map_delete_node(void *arg, struct bpf_lru_node *node)\n{\n\tstruct bpf_htab *htab = arg;\n\tstruct htab_elem *l = NULL, *tgt_l;\n\tstruct hlist_nulls_head *head;\n\tstruct hlist_nulls_node *n;\n\tunsigned long flags;\n\tstruct bucket *b;\n\tint ret;\n\n\ttgt_l = container_of(node, struct htab_elem, lru_node);\n\tb = __select_bucket(htab, tgt_l->hash);\n\thead = &b->head;\n\n\tret = htab_lock_bucket(htab, b, tgt_l->hash, &flags);\n\tif (ret)\n\t\treturn false;\n\n\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\n\t\tif (l == tgt_l) {\n\t\t\thlist_nulls_del_rcu(&l->hash_node);\n\t\t\tcheck_and_free_fields(htab, l);\n\t\t\tbpf_map_dec_elem_count(&htab->map);\n\t\t\tbreak;\n\t\t}\n\n\thtab_unlock_bucket(htab, b, tgt_l->hash, flags);\n\n\treturn l == tgt_l;\n}\n\n \nstatic int htab_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_head *head;\n\tstruct htab_elem *l, *next_l;\n\tu32 hash, key_size;\n\tint i = 0;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tkey_size = map->key_size;\n\n\tif (!key)\n\t\tgoto find_first_elem;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\n\thead = select_bucket(htab, hash);\n\n\t \n\tl = lookup_nulls_elem_raw(head, hash, key, key_size, htab->n_buckets);\n\n\tif (!l)\n\t\tgoto find_first_elem;\n\n\t \n\tnext_l = hlist_nulls_entry_safe(rcu_dereference_raw(hlist_nulls_next_rcu(&l->hash_node)),\n\t\t\t\t  struct htab_elem, hash_node);\n\n\tif (next_l) {\n\t\t \n\t\tmemcpy(next_key, next_l->key, key_size);\n\t\treturn 0;\n\t}\n\n\t \n\ti = hash & (htab->n_buckets - 1);\n\ti++;\n\nfind_first_elem:\n\t \n\tfor (; i < htab->n_buckets; i++) {\n\t\thead = select_bucket(htab, i);\n\n\t\t \n\t\tnext_l = hlist_nulls_entry_safe(rcu_dereference_raw(hlist_nulls_first_rcu(head)),\n\t\t\t\t\t  struct htab_elem, hash_node);\n\t\tif (next_l) {\n\t\t\t \n\t\t\tmemcpy(next_key, next_l->key, key_size);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\treturn -ENOENT;\n}\n\nstatic void htab_elem_free(struct bpf_htab *htab, struct htab_elem *l)\n{\n\tcheck_and_free_fields(htab, l);\n\tif (htab->map.map_type == BPF_MAP_TYPE_PERCPU_HASH)\n\t\tbpf_mem_cache_free(&htab->pcpu_ma, l->ptr_to_pptr);\n\tbpf_mem_cache_free(&htab->ma, l);\n}\n\nstatic void htab_put_fd_value(struct bpf_htab *htab, struct htab_elem *l)\n{\n\tstruct bpf_map *map = &htab->map;\n\tvoid *ptr;\n\n\tif (map->ops->map_fd_put_ptr) {\n\t\tptr = fd_htab_map_get_ptr(map, l);\n\t\tmap->ops->map_fd_put_ptr(map, ptr, true);\n\t}\n}\n\nstatic bool is_map_full(struct bpf_htab *htab)\n{\n\tif (htab->use_percpu_counter)\n\t\treturn __percpu_counter_compare(&htab->pcount, htab->map.max_entries,\n\t\t\t\t\t\tPERCPU_COUNTER_BATCH) >= 0;\n\treturn atomic_read(&htab->count) >= htab->map.max_entries;\n}\n\nstatic void inc_elem_count(struct bpf_htab *htab)\n{\n\tbpf_map_inc_elem_count(&htab->map);\n\n\tif (htab->use_percpu_counter)\n\t\tpercpu_counter_add_batch(&htab->pcount, 1, PERCPU_COUNTER_BATCH);\n\telse\n\t\tatomic_inc(&htab->count);\n}\n\nstatic void dec_elem_count(struct bpf_htab *htab)\n{\n\tbpf_map_dec_elem_count(&htab->map);\n\n\tif (htab->use_percpu_counter)\n\t\tpercpu_counter_add_batch(&htab->pcount, -1, PERCPU_COUNTER_BATCH);\n\telse\n\t\tatomic_dec(&htab->count);\n}\n\n\nstatic void free_htab_elem(struct bpf_htab *htab, struct htab_elem *l)\n{\n\thtab_put_fd_value(htab, l);\n\n\tif (htab_is_prealloc(htab)) {\n\t\tbpf_map_dec_elem_count(&htab->map);\n\t\tcheck_and_free_fields(htab, l);\n\t\t__pcpu_freelist_push(&htab->freelist, &l->fnode);\n\t} else {\n\t\tdec_elem_count(htab);\n\t\thtab_elem_free(htab, l);\n\t}\n}\n\nstatic void pcpu_copy_value(struct bpf_htab *htab, void __percpu *pptr,\n\t\t\t    void *value, bool onallcpus)\n{\n\tif (!onallcpus) {\n\t\t \n\t\tcopy_map_value(&htab->map, this_cpu_ptr(pptr), value);\n\t} else {\n\t\tu32 size = round_up(htab->map.value_size, 8);\n\t\tint off = 0, cpu;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tcopy_map_value_long(&htab->map, per_cpu_ptr(pptr, cpu), value + off);\n\t\t\toff += size;\n\t\t}\n\t}\n}\n\nstatic void pcpu_init_value(struct bpf_htab *htab, void __percpu *pptr,\n\t\t\t    void *value, bool onallcpus)\n{\n\t \n\tif (!onallcpus) {\n\t\tint current_cpu = raw_smp_processor_id();\n\t\tint cpu;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tif (cpu == current_cpu)\n\t\t\t\tcopy_map_value_long(&htab->map, per_cpu_ptr(pptr, cpu), value);\n\t\t\telse  \n\t\t\t\tzero_map_value(&htab->map, per_cpu_ptr(pptr, cpu));\n\t\t}\n\t} else {\n\t\tpcpu_copy_value(htab, pptr, value, onallcpus);\n\t}\n}\n\nstatic bool fd_htab_map_needs_adjust(const struct bpf_htab *htab)\n{\n\treturn htab->map.map_type == BPF_MAP_TYPE_HASH_OF_MAPS &&\n\t       BITS_PER_LONG == 64;\n}\n\nstatic struct htab_elem *alloc_htab_elem(struct bpf_htab *htab, void *key,\n\t\t\t\t\t void *value, u32 key_size, u32 hash,\n\t\t\t\t\t bool percpu, bool onallcpus,\n\t\t\t\t\t struct htab_elem *old_elem)\n{\n\tu32 size = htab->map.value_size;\n\tbool prealloc = htab_is_prealloc(htab);\n\tstruct htab_elem *l_new, **pl_new;\n\tvoid __percpu *pptr;\n\n\tif (prealloc) {\n\t\tif (old_elem) {\n\t\t\t \n\t\t\tpl_new = this_cpu_ptr(htab->extra_elems);\n\t\t\tl_new = *pl_new;\n\t\t\thtab_put_fd_value(htab, old_elem);\n\t\t\t*pl_new = old_elem;\n\t\t} else {\n\t\t\tstruct pcpu_freelist_node *l;\n\n\t\t\tl = __pcpu_freelist_pop(&htab->freelist);\n\t\t\tif (!l)\n\t\t\t\treturn ERR_PTR(-E2BIG);\n\t\t\tl_new = container_of(l, struct htab_elem, fnode);\n\t\t\tbpf_map_inc_elem_count(&htab->map);\n\t\t}\n\t} else {\n\t\tif (is_map_full(htab))\n\t\t\tif (!old_elem)\n\t\t\t\t \n\t\t\t\treturn ERR_PTR(-E2BIG);\n\t\tinc_elem_count(htab);\n\t\tl_new = bpf_mem_cache_alloc(&htab->ma);\n\t\tif (!l_new) {\n\t\t\tl_new = ERR_PTR(-ENOMEM);\n\t\t\tgoto dec_count;\n\t\t}\n\t}\n\n\tmemcpy(l_new->key, key, key_size);\n\tif (percpu) {\n\t\tif (prealloc) {\n\t\t\tpptr = htab_elem_get_ptr(l_new, key_size);\n\t\t} else {\n\t\t\t \n\t\t\tpptr = bpf_mem_cache_alloc(&htab->pcpu_ma);\n\t\t\tif (!pptr) {\n\t\t\t\tbpf_mem_cache_free(&htab->ma, l_new);\n\t\t\t\tl_new = ERR_PTR(-ENOMEM);\n\t\t\t\tgoto dec_count;\n\t\t\t}\n\t\t\tl_new->ptr_to_pptr = pptr;\n\t\t\tpptr = *(void **)pptr;\n\t\t}\n\n\t\tpcpu_init_value(htab, pptr, value, onallcpus);\n\n\t\tif (!prealloc)\n\t\t\thtab_elem_set_ptr(l_new, key_size, pptr);\n\t} else if (fd_htab_map_needs_adjust(htab)) {\n\t\tsize = round_up(size, 8);\n\t\tmemcpy(l_new->key + round_up(key_size, 8), value, size);\n\t} else {\n\t\tcopy_map_value(&htab->map,\n\t\t\t       l_new->key + round_up(key_size, 8),\n\t\t\t       value);\n\t}\n\n\tl_new->hash = hash;\n\treturn l_new;\ndec_count:\n\tdec_elem_count(htab);\n\treturn l_new;\n}\n\nstatic int check_flags(struct bpf_htab *htab, struct htab_elem *l_old,\n\t\t       u64 map_flags)\n{\n\tif (l_old && (map_flags & ~BPF_F_LOCK) == BPF_NOEXIST)\n\t\t \n\t\treturn -EEXIST;\n\n\tif (!l_old && (map_flags & ~BPF_F_LOCK) == BPF_EXIST)\n\t\t \n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\n \nstatic long htab_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t u64 map_flags)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct htab_elem *l_new = NULL, *l_old;\n\tstruct hlist_nulls_head *head;\n\tunsigned long flags;\n\tstruct bucket *b;\n\tu32 key_size, hash;\n\tint ret;\n\n\tif (unlikely((map_flags & ~BPF_F_LOCK) > BPF_EXIST))\n\t\t \n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\tif (unlikely(map_flags & BPF_F_LOCK)) {\n\t\tif (unlikely(!btf_record_has_field(map->record, BPF_SPIN_LOCK)))\n\t\t\treturn -EINVAL;\n\t\t \n\t\tl_old = lookup_nulls_elem_raw(head, hash, key, key_size,\n\t\t\t\t\t      htab->n_buckets);\n\t\tret = check_flags(htab, l_old, map_flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (l_old) {\n\t\t\t \n\t\t\tcopy_map_value_locked(map,\n\t\t\t\t\t      l_old->key + round_up(key_size, 8),\n\t\t\t\t\t      value, false);\n\t\t\treturn 0;\n\t\t}\n\t\t \n\t}\n\n\tret = htab_lock_bucket(htab, b, hash, &flags);\n\tif (ret)\n\t\treturn ret;\n\n\tl_old = lookup_elem_raw(head, hash, key, key_size);\n\n\tret = check_flags(htab, l_old, map_flags);\n\tif (ret)\n\t\tgoto err;\n\n\tif (unlikely(l_old && (map_flags & BPF_F_LOCK))) {\n\t\t \n\t\tcopy_map_value_locked(map,\n\t\t\t\t      l_old->key + round_up(key_size, 8),\n\t\t\t\t      value, false);\n\t\tret = 0;\n\t\tgoto err;\n\t}\n\n\tl_new = alloc_htab_elem(htab, key, value, key_size, hash, false, false,\n\t\t\t\tl_old);\n\tif (IS_ERR(l_new)) {\n\t\t \n\t\tret = PTR_ERR(l_new);\n\t\tgoto err;\n\t}\n\n\t \n\thlist_nulls_add_head_rcu(&l_new->hash_node, head);\n\tif (l_old) {\n\t\thlist_nulls_del_rcu(&l_old->hash_node);\n\t\tif (!htab_is_prealloc(htab))\n\t\t\tfree_htab_elem(htab, l_old);\n\t\telse\n\t\t\tcheck_and_free_fields(htab, l_old);\n\t}\n\tret = 0;\nerr:\n\thtab_unlock_bucket(htab, b, hash, flags);\n\treturn ret;\n}\n\nstatic void htab_lru_push_free(struct bpf_htab *htab, struct htab_elem *elem)\n{\n\tcheck_and_free_fields(htab, elem);\n\tbpf_map_dec_elem_count(&htab->map);\n\tbpf_lru_push_free(&htab->lru, &elem->lru_node);\n}\n\nstatic long htab_lru_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\t     u64 map_flags)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct htab_elem *l_new, *l_old = NULL;\n\tstruct hlist_nulls_head *head;\n\tunsigned long flags;\n\tstruct bucket *b;\n\tu32 key_size, hash;\n\tint ret;\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\t \n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\t \n\tl_new = prealloc_lru_pop(htab, key, hash);\n\tif (!l_new)\n\t\treturn -ENOMEM;\n\tcopy_map_value(&htab->map,\n\t\t       l_new->key + round_up(map->key_size, 8), value);\n\n\tret = htab_lock_bucket(htab, b, hash, &flags);\n\tif (ret)\n\t\tgoto err_lock_bucket;\n\n\tl_old = lookup_elem_raw(head, hash, key, key_size);\n\n\tret = check_flags(htab, l_old, map_flags);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\thlist_nulls_add_head_rcu(&l_new->hash_node, head);\n\tif (l_old) {\n\t\tbpf_lru_node_set_ref(&l_new->lru_node);\n\t\thlist_nulls_del_rcu(&l_old->hash_node);\n\t}\n\tret = 0;\n\nerr:\n\thtab_unlock_bucket(htab, b, hash, flags);\n\nerr_lock_bucket:\n\tif (ret)\n\t\thtab_lru_push_free(htab, l_new);\n\telse if (l_old)\n\t\thtab_lru_push_free(htab, l_old);\n\n\treturn ret;\n}\n\nstatic long __htab_percpu_map_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t  void *value, u64 map_flags,\n\t\t\t\t\t  bool onallcpus)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct htab_elem *l_new = NULL, *l_old;\n\tstruct hlist_nulls_head *head;\n\tunsigned long flags;\n\tstruct bucket *b;\n\tu32 key_size, hash;\n\tint ret;\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\t \n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\tret = htab_lock_bucket(htab, b, hash, &flags);\n\tif (ret)\n\t\treturn ret;\n\n\tl_old = lookup_elem_raw(head, hash, key, key_size);\n\n\tret = check_flags(htab, l_old, map_flags);\n\tif (ret)\n\t\tgoto err;\n\n\tif (l_old) {\n\t\t \n\t\tpcpu_copy_value(htab, htab_elem_get_ptr(l_old, key_size),\n\t\t\t\tvalue, onallcpus);\n\t} else {\n\t\tl_new = alloc_htab_elem(htab, key, value, key_size,\n\t\t\t\t\thash, true, onallcpus, NULL);\n\t\tif (IS_ERR(l_new)) {\n\t\t\tret = PTR_ERR(l_new);\n\t\t\tgoto err;\n\t\t}\n\t\thlist_nulls_add_head_rcu(&l_new->hash_node, head);\n\t}\n\tret = 0;\nerr:\n\thtab_unlock_bucket(htab, b, hash, flags);\n\treturn ret;\n}\n\nstatic long __htab_lru_percpu_map_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t      void *value, u64 map_flags,\n\t\t\t\t\t      bool onallcpus)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct htab_elem *l_new = NULL, *l_old;\n\tstruct hlist_nulls_head *head;\n\tunsigned long flags;\n\tstruct bucket *b;\n\tu32 key_size, hash;\n\tint ret;\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\t \n\t\treturn -EINVAL;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\t \n\tif (map_flags != BPF_EXIST) {\n\t\tl_new = prealloc_lru_pop(htab, key, hash);\n\t\tif (!l_new)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tret = htab_lock_bucket(htab, b, hash, &flags);\n\tif (ret)\n\t\tgoto err_lock_bucket;\n\n\tl_old = lookup_elem_raw(head, hash, key, key_size);\n\n\tret = check_flags(htab, l_old, map_flags);\n\tif (ret)\n\t\tgoto err;\n\n\tif (l_old) {\n\t\tbpf_lru_node_set_ref(&l_old->lru_node);\n\n\t\t \n\t\tpcpu_copy_value(htab, htab_elem_get_ptr(l_old, key_size),\n\t\t\t\tvalue, onallcpus);\n\t} else {\n\t\tpcpu_init_value(htab, htab_elem_get_ptr(l_new, key_size),\n\t\t\t\tvalue, onallcpus);\n\t\thlist_nulls_add_head_rcu(&l_new->hash_node, head);\n\t\tl_new = NULL;\n\t}\n\tret = 0;\nerr:\n\thtab_unlock_bucket(htab, b, hash, flags);\nerr_lock_bucket:\n\tif (l_new) {\n\t\tbpf_map_dec_elem_count(&htab->map);\n\t\tbpf_lru_push_free(&htab->lru, &l_new->lru_node);\n\t}\n\treturn ret;\n}\n\nstatic long htab_percpu_map_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t\tvoid *value, u64 map_flags)\n{\n\treturn __htab_percpu_map_update_elem(map, key, value, map_flags, false);\n}\n\nstatic long htab_lru_percpu_map_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t    void *value, u64 map_flags)\n{\n\treturn __htab_lru_percpu_map_update_elem(map, key, value, map_flags,\n\t\t\t\t\t\t false);\n}\n\n \nstatic long htab_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_head *head;\n\tstruct bucket *b;\n\tstruct htab_elem *l;\n\tunsigned long flags;\n\tu32 hash, key_size;\n\tint ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\tret = htab_lock_bucket(htab, b, hash, &flags);\n\tif (ret)\n\t\treturn ret;\n\n\tl = lookup_elem_raw(head, hash, key, key_size);\n\n\tif (l) {\n\t\thlist_nulls_del_rcu(&l->hash_node);\n\t\tfree_htab_elem(htab, l);\n\t} else {\n\t\tret = -ENOENT;\n\t}\n\n\thtab_unlock_bucket(htab, b, hash, flags);\n\treturn ret;\n}\n\nstatic long htab_lru_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_head *head;\n\tstruct bucket *b;\n\tstruct htab_elem *l;\n\tunsigned long flags;\n\tu32 hash, key_size;\n\tint ret;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held() && !rcu_read_lock_trace_held() &&\n\t\t     !rcu_read_lock_bh_held());\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\tret = htab_lock_bucket(htab, b, hash, &flags);\n\tif (ret)\n\t\treturn ret;\n\n\tl = lookup_elem_raw(head, hash, key, key_size);\n\n\tif (l)\n\t\thlist_nulls_del_rcu(&l->hash_node);\n\telse\n\t\tret = -ENOENT;\n\n\thtab_unlock_bucket(htab, b, hash, flags);\n\tif (l)\n\t\thtab_lru_push_free(htab, l);\n\treturn ret;\n}\n\nstatic void delete_all_elements(struct bpf_htab *htab)\n{\n\tint i;\n\n\t \n\tmigrate_disable();\n\tfor (i = 0; i < htab->n_buckets; i++) {\n\t\tstruct hlist_nulls_head *head = select_bucket(htab, i);\n\t\tstruct hlist_nulls_node *n;\n\t\tstruct htab_elem *l;\n\n\t\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\n\t\t\thlist_nulls_del_rcu(&l->hash_node);\n\t\t\thtab_elem_free(htab, l);\n\t\t}\n\t}\n\tmigrate_enable();\n}\n\nstatic void htab_free_malloced_timers(struct bpf_htab *htab)\n{\n\tint i;\n\n\trcu_read_lock();\n\tfor (i = 0; i < htab->n_buckets; i++) {\n\t\tstruct hlist_nulls_head *head = select_bucket(htab, i);\n\t\tstruct hlist_nulls_node *n;\n\t\tstruct htab_elem *l;\n\n\t\thlist_nulls_for_each_entry(l, n, head, hash_node) {\n\t\t\t \n\t\t\tbpf_obj_free_timer(htab->map.record, l->key + round_up(htab->map.key_size, 8));\n\t\t}\n\t\tcond_resched_rcu();\n\t}\n\trcu_read_unlock();\n}\n\nstatic void htab_map_free_timers(struct bpf_map *map)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\n\t \n\tif (!btf_record_has_field(htab->map.record, BPF_TIMER))\n\t\treturn;\n\tif (!htab_is_prealloc(htab))\n\t\thtab_free_malloced_timers(htab);\n\telse\n\t\thtab_free_prealloced_timers(htab);\n}\n\n \nstatic void htab_map_free(struct bpf_map *map)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tint i;\n\n\t \n\n\t \n\tif (!htab_is_prealloc(htab)) {\n\t\tdelete_all_elements(htab);\n\t} else {\n\t\thtab_free_prealloced_fields(htab);\n\t\tprealloc_destroy(htab);\n\t}\n\n\tbpf_map_free_elem_count(map);\n\tfree_percpu(htab->extra_elems);\n\tbpf_map_area_free(htab->buckets);\n\tbpf_mem_alloc_destroy(&htab->pcpu_ma);\n\tbpf_mem_alloc_destroy(&htab->ma);\n\tif (htab->use_percpu_counter)\n\t\tpercpu_counter_destroy(&htab->pcount);\n\tfor (i = 0; i < HASHTAB_MAP_LOCK_COUNT; i++)\n\t\tfree_percpu(htab->map_locked[i]);\n\tlockdep_unregister_key(&htab->lockdep_key);\n\tbpf_map_area_free(htab);\n}\n\nstatic void htab_map_seq_show_elem(struct bpf_map *map, void *key,\n\t\t\t\t   struct seq_file *m)\n{\n\tvoid *value;\n\n\trcu_read_lock();\n\n\tvalue = htab_map_lookup_elem(map, key);\n\tif (!value) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tbtf_type_seq_show(map->btf, map->btf_key_type_id, key, m);\n\tseq_puts(m, \": \");\n\tbtf_type_seq_show(map->btf, map->btf_value_type_id, value, m);\n\tseq_puts(m, \"\\n\");\n\n\trcu_read_unlock();\n}\n\nstatic int __htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t     void *value, bool is_lru_map,\n\t\t\t\t\t     bool is_percpu, u64 flags)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_head *head;\n\tunsigned long bflags;\n\tstruct htab_elem *l;\n\tu32 hash, key_size;\n\tstruct bucket *b;\n\tint ret;\n\n\tkey_size = map->key_size;\n\n\thash = htab_map_hash(key, key_size, htab->hashrnd);\n\tb = __select_bucket(htab, hash);\n\thead = &b->head;\n\n\tret = htab_lock_bucket(htab, b, hash, &bflags);\n\tif (ret)\n\t\treturn ret;\n\n\tl = lookup_elem_raw(head, hash, key, key_size);\n\tif (!l) {\n\t\tret = -ENOENT;\n\t} else {\n\t\tif (is_percpu) {\n\t\t\tu32 roundup_value_size = round_up(map->value_size, 8);\n\t\t\tvoid __percpu *pptr;\n\t\t\tint off = 0, cpu;\n\n\t\t\tpptr = htab_elem_get_ptr(l, key_size);\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tcopy_map_value_long(&htab->map, value + off, per_cpu_ptr(pptr, cpu));\n\t\t\t\tcheck_and_init_map_value(&htab->map, value + off);\n\t\t\t\toff += roundup_value_size;\n\t\t\t}\n\t\t} else {\n\t\t\tu32 roundup_key_size = round_up(map->key_size, 8);\n\n\t\t\tif (flags & BPF_F_LOCK)\n\t\t\t\tcopy_map_value_locked(map, value, l->key +\n\t\t\t\t\t\t      roundup_key_size,\n\t\t\t\t\t\t      true);\n\t\t\telse\n\t\t\t\tcopy_map_value(map, value, l->key +\n\t\t\t\t\t       roundup_key_size);\n\t\t\t \n\t\t\tcheck_and_init_map_value(map, value);\n\t\t}\n\n\t\thlist_nulls_del_rcu(&l->hash_node);\n\t\tif (!is_lru_map)\n\t\t\tfree_htab_elem(htab, l);\n\t}\n\n\thtab_unlock_bucket(htab, b, hash, bflags);\n\n\tif (is_lru_map && l)\n\t\thtab_lru_push_free(htab, l);\n\n\treturn ret;\n}\n\nstatic int htab_map_lookup_and_delete_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t   void *value, u64 flags)\n{\n\treturn __htab_map_lookup_and_delete_elem(map, key, value, false, false,\n\t\t\t\t\t\t flags);\n}\n\nstatic int htab_percpu_map_lookup_and_delete_elem(struct bpf_map *map,\n\t\t\t\t\t\t  void *key, void *value,\n\t\t\t\t\t\t  u64 flags)\n{\n\treturn __htab_map_lookup_and_delete_elem(map, key, value, false, true,\n\t\t\t\t\t\t flags);\n}\n\nstatic int htab_lru_map_lookup_and_delete_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t       void *value, u64 flags)\n{\n\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, false,\n\t\t\t\t\t\t flags);\n}\n\nstatic int htab_lru_percpu_map_lookup_and_delete_elem(struct bpf_map *map,\n\t\t\t\t\t\t      void *key, void *value,\n\t\t\t\t\t\t      u64 flags)\n{\n\treturn __htab_map_lookup_and_delete_elem(map, key, value, true, true,\n\t\t\t\t\t\t flags);\n}\n\nstatic int\n__htab_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t   const union bpf_attr *attr,\n\t\t\t\t   union bpf_attr __user *uattr,\n\t\t\t\t   bool do_delete, bool is_lru_map,\n\t\t\t\t   bool is_percpu)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;\n\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;\n\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);\n\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);\n\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);\n\tu32 batch, max_count, size, bucket_size, map_id;\n\tstruct htab_elem *node_to_free = NULL;\n\tu64 elem_map_flags, map_flags;\n\tstruct hlist_nulls_head *head;\n\tstruct hlist_nulls_node *n;\n\tunsigned long flags = 0;\n\tbool locked = false;\n\tstruct htab_elem *l;\n\tstruct bucket *b;\n\tint ret = 0;\n\n\telem_map_flags = attr->batch.elem_flags;\n\tif ((elem_map_flags & ~BPF_F_LOCK) ||\n\t    ((elem_map_flags & BPF_F_LOCK) && !btf_record_has_field(map->record, BPF_SPIN_LOCK)))\n\t\treturn -EINVAL;\n\n\tmap_flags = attr->batch.flags;\n\tif (map_flags)\n\t\treturn -EINVAL;\n\n\tmax_count = attr->batch.count;\n\tif (!max_count)\n\t\treturn 0;\n\n\tif (put_user(0, &uattr->batch.count))\n\t\treturn -EFAULT;\n\n\tbatch = 0;\n\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))\n\t\treturn -EFAULT;\n\n\tif (batch >= htab->n_buckets)\n\t\treturn -ENOENT;\n\n\tkey_size = htab->map.key_size;\n\troundup_key_size = round_up(htab->map.key_size, 8);\n\tvalue_size = htab->map.value_size;\n\tsize = round_up(value_size, 8);\n\tif (is_percpu)\n\t\tvalue_size = size * num_possible_cpus();\n\ttotal = 0;\n\t \n\tbucket_size = 5;\n\nalloc:\n\t \n\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);\n\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);\n\tif (!keys || !values) {\n\t\tret = -ENOMEM;\n\t\tgoto after_loop;\n\t}\n\nagain:\n\tbpf_disable_instrumentation();\n\trcu_read_lock();\nagain_nocopy:\n\tdst_key = keys;\n\tdst_val = values;\n\tb = &htab->buckets[batch];\n\thead = &b->head;\n\t \n\tif (locked) {\n\t\tret = htab_lock_bucket(htab, b, batch, &flags);\n\t\tif (ret) {\n\t\t\trcu_read_unlock();\n\t\t\tbpf_enable_instrumentation();\n\t\t\tgoto after_loop;\n\t\t}\n\t}\n\n\tbucket_cnt = 0;\n\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\n\t\tbucket_cnt++;\n\n\tif (bucket_cnt && !locked) {\n\t\tlocked = true;\n\t\tgoto again_nocopy;\n\t}\n\n\tif (bucket_cnt > (max_count - total)) {\n\t\tif (total == 0)\n\t\t\tret = -ENOSPC;\n\t\t \n\t\thtab_unlock_bucket(htab, b, batch, flags);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tgoto after_loop;\n\t}\n\n\tif (bucket_cnt > bucket_size) {\n\t\tbucket_size = bucket_cnt;\n\t\t \n\t\thtab_unlock_bucket(htab, b, batch, flags);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tkvfree(keys);\n\t\tkvfree(values);\n\t\tgoto alloc;\n\t}\n\n\t \n\tif (!locked)\n\t\tgoto next_batch;\n\n\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\n\t\tmemcpy(dst_key, l->key, key_size);\n\n\t\tif (is_percpu) {\n\t\t\tint off = 0, cpu;\n\t\t\tvoid __percpu *pptr;\n\n\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tcopy_map_value_long(&htab->map, dst_val + off, per_cpu_ptr(pptr, cpu));\n\t\t\t\tcheck_and_init_map_value(&htab->map, dst_val + off);\n\t\t\t\toff += size;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue = l->key + roundup_key_size;\n\t\t\tif (map->map_type == BPF_MAP_TYPE_HASH_OF_MAPS) {\n\t\t\t\tstruct bpf_map **inner_map = value;\n\n\t\t\t\t  \n\t\t\t\tmap_id = map->ops->map_fd_sys_lookup_elem(*inner_map);\n\t\t\t\tvalue = &map_id;\n\t\t\t}\n\n\t\t\tif (elem_map_flags & BPF_F_LOCK)\n\t\t\t\tcopy_map_value_locked(map, dst_val, value,\n\t\t\t\t\t\t      true);\n\t\t\telse\n\t\t\t\tcopy_map_value(map, dst_val, value);\n\t\t\t \n\t\t\tcheck_and_init_map_value(map, dst_val);\n\t\t}\n\t\tif (do_delete) {\n\t\t\thlist_nulls_del_rcu(&l->hash_node);\n\n\t\t\t \n\t\t\tif (is_lru_map) {\n\t\t\t\tl->batch_flink = node_to_free;\n\t\t\t\tnode_to_free = l;\n\t\t\t} else {\n\t\t\t\tfree_htab_elem(htab, l);\n\t\t\t}\n\t\t}\n\t\tdst_key += key_size;\n\t\tdst_val += value_size;\n\t}\n\n\thtab_unlock_bucket(htab, b, batch, flags);\n\tlocked = false;\n\n\twhile (node_to_free) {\n\t\tl = node_to_free;\n\t\tnode_to_free = node_to_free->batch_flink;\n\t\thtab_lru_push_free(htab, l);\n\t}\n\nnext_batch:\n\t \n\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {\n\t\tbatch++;\n\t\tgoto again_nocopy;\n\t}\n\n\trcu_read_unlock();\n\tbpf_enable_instrumentation();\n\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,\n\t    key_size * bucket_cnt) ||\n\t    copy_to_user(uvalues + total * value_size, values,\n\t    value_size * bucket_cnt))) {\n\t\tret = -EFAULT;\n\t\tgoto after_loop;\n\t}\n\n\ttotal += bucket_cnt;\n\tbatch++;\n\tif (batch >= htab->n_buckets) {\n\t\tret = -ENOENT;\n\t\tgoto after_loop;\n\t}\n\tgoto again;\n\nafter_loop:\n\tif (ret == -EFAULT)\n\t\tgoto out;\n\n\t \n\tubatch = u64_to_user_ptr(attr->batch.out_batch);\n\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||\n\t    put_user(total, &uattr->batch.count))\n\t\tret = -EFAULT;\n\nout:\n\tkvfree(keys);\n\tkvfree(values);\n\treturn ret;\n}\n\nstatic int\nhtab_percpu_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,\n\t\t\t     union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,\n\t\t\t\t\t\t  false, true);\n}\n\nstatic int\nhtab_percpu_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t\tconst union bpf_attr *attr,\n\t\t\t\t\tunion bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,\n\t\t\t\t\t\t  false, true);\n}\n\nstatic int\nhtab_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,\n\t\t      union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,\n\t\t\t\t\t\t  false, false);\n}\n\nstatic int\nhtab_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t const union bpf_attr *attr,\n\t\t\t\t union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,\n\t\t\t\t\t\t  false, false);\n}\n\nstatic int\nhtab_lru_percpu_map_lookup_batch(struct bpf_map *map,\n\t\t\t\t const union bpf_attr *attr,\n\t\t\t\t union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,\n\t\t\t\t\t\t  true, true);\n}\n\nstatic int\nhtab_lru_percpu_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t\t    const union bpf_attr *attr,\n\t\t\t\t\t    union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,\n\t\t\t\t\t\t  true, true);\n}\n\nstatic int\nhtab_lru_map_lookup_batch(struct bpf_map *map, const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, false,\n\t\t\t\t\t\t  true, false);\n}\n\nstatic int\nhtab_lru_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t     const union bpf_attr *attr,\n\t\t\t\t     union bpf_attr __user *uattr)\n{\n\treturn __htab_map_lookup_and_delete_batch(map, attr, uattr, true,\n\t\t\t\t\t\t  true, false);\n}\n\nstruct bpf_iter_seq_hash_map_info {\n\tstruct bpf_map *map;\n\tstruct bpf_htab *htab;\n\tvoid *percpu_value_buf;  \n\tu32 bucket_id;\n\tu32 skip_elems;\n};\n\nstatic struct htab_elem *\nbpf_hash_map_seq_find_next(struct bpf_iter_seq_hash_map_info *info,\n\t\t\t   struct htab_elem *prev_elem)\n{\n\tconst struct bpf_htab *htab = info->htab;\n\tu32 skip_elems = info->skip_elems;\n\tu32 bucket_id = info->bucket_id;\n\tstruct hlist_nulls_head *head;\n\tstruct hlist_nulls_node *n;\n\tstruct htab_elem *elem;\n\tstruct bucket *b;\n\tu32 i, count;\n\n\tif (bucket_id >= htab->n_buckets)\n\t\treturn NULL;\n\n\t \n\tif (prev_elem) {\n\t\t \n\t\tn = rcu_dereference_raw(hlist_nulls_next_rcu(&prev_elem->hash_node));\n\t\telem = hlist_nulls_entry_safe(n, struct htab_elem, hash_node);\n\t\tif (elem)\n\t\t\treturn elem;\n\n\t\t \n\t\tb = &htab->buckets[bucket_id++];\n\t\trcu_read_unlock();\n\t\tskip_elems = 0;\n\t}\n\n\tfor (i = bucket_id; i < htab->n_buckets; i++) {\n\t\tb = &htab->buckets[i];\n\t\trcu_read_lock();\n\n\t\tcount = 0;\n\t\thead = &b->head;\n\t\thlist_nulls_for_each_entry_rcu(elem, n, head, hash_node) {\n\t\t\tif (count >= skip_elems) {\n\t\t\t\tinfo->bucket_id = i;\n\t\t\t\tinfo->skip_elems = count;\n\t\t\t\treturn elem;\n\t\t\t}\n\t\t\tcount++;\n\t\t}\n\n\t\trcu_read_unlock();\n\t\tskip_elems = 0;\n\t}\n\n\tinfo->bucket_id = i;\n\tinfo->skip_elems = 0;\n\treturn NULL;\n}\n\nstatic void *bpf_hash_map_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct bpf_iter_seq_hash_map_info *info = seq->private;\n\tstruct htab_elem *elem;\n\n\telem = bpf_hash_map_seq_find_next(info, NULL);\n\tif (!elem)\n\t\treturn NULL;\n\n\tif (*pos == 0)\n\t\t++*pos;\n\treturn elem;\n}\n\nstatic void *bpf_hash_map_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct bpf_iter_seq_hash_map_info *info = seq->private;\n\n\t++*pos;\n\t++info->skip_elems;\n\treturn bpf_hash_map_seq_find_next(info, v);\n}\n\nstatic int __bpf_hash_map_seq_show(struct seq_file *seq, struct htab_elem *elem)\n{\n\tstruct bpf_iter_seq_hash_map_info *info = seq->private;\n\tu32 roundup_key_size, roundup_value_size;\n\tstruct bpf_iter__bpf_map_elem ctx = {};\n\tstruct bpf_map *map = info->map;\n\tstruct bpf_iter_meta meta;\n\tint ret = 0, off = 0, cpu;\n\tstruct bpf_prog *prog;\n\tvoid __percpu *pptr;\n\n\tmeta.seq = seq;\n\tprog = bpf_iter_get_info(&meta, elem == NULL);\n\tif (prog) {\n\t\tctx.meta = &meta;\n\t\tctx.map = info->map;\n\t\tif (elem) {\n\t\t\troundup_key_size = round_up(map->key_size, 8);\n\t\t\tctx.key = elem->key;\n\t\t\tif (!info->percpu_value_buf) {\n\t\t\t\tctx.value = elem->key + roundup_key_size;\n\t\t\t} else {\n\t\t\t\troundup_value_size = round_up(map->value_size, 8);\n\t\t\t\tpptr = htab_elem_get_ptr(elem, map->key_size);\n\t\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\t\tcopy_map_value_long(map, info->percpu_value_buf + off,\n\t\t\t\t\t\t\t    per_cpu_ptr(pptr, cpu));\n\t\t\t\t\tcheck_and_init_map_value(map, info->percpu_value_buf + off);\n\t\t\t\t\toff += roundup_value_size;\n\t\t\t\t}\n\t\t\t\tctx.value = info->percpu_value_buf;\n\t\t\t}\n\t\t}\n\t\tret = bpf_iter_run_prog(prog, &ctx);\n\t}\n\n\treturn ret;\n}\n\nstatic int bpf_hash_map_seq_show(struct seq_file *seq, void *v)\n{\n\treturn __bpf_hash_map_seq_show(seq, v);\n}\n\nstatic void bpf_hash_map_seq_stop(struct seq_file *seq, void *v)\n{\n\tif (!v)\n\t\t(void)__bpf_hash_map_seq_show(seq, NULL);\n\telse\n\t\trcu_read_unlock();\n}\n\nstatic int bpf_iter_init_hash_map(void *priv_data,\n\t\t\t\t  struct bpf_iter_aux_info *aux)\n{\n\tstruct bpf_iter_seq_hash_map_info *seq_info = priv_data;\n\tstruct bpf_map *map = aux->map;\n\tvoid *value_buf;\n\tu32 buf_size;\n\n\tif (map->map_type == BPF_MAP_TYPE_PERCPU_HASH ||\n\t    map->map_type == BPF_MAP_TYPE_LRU_PERCPU_HASH) {\n\t\tbuf_size = round_up(map->value_size, 8) * num_possible_cpus();\n\t\tvalue_buf = kmalloc(buf_size, GFP_USER | __GFP_NOWARN);\n\t\tif (!value_buf)\n\t\t\treturn -ENOMEM;\n\n\t\tseq_info->percpu_value_buf = value_buf;\n\t}\n\n\tbpf_map_inc_with_uref(map);\n\tseq_info->map = map;\n\tseq_info->htab = container_of(map, struct bpf_htab, map);\n\treturn 0;\n}\n\nstatic void bpf_iter_fini_hash_map(void *priv_data)\n{\n\tstruct bpf_iter_seq_hash_map_info *seq_info = priv_data;\n\n\tbpf_map_put_with_uref(seq_info->map);\n\tkfree(seq_info->percpu_value_buf);\n}\n\nstatic const struct seq_operations bpf_hash_map_seq_ops = {\n\t.start\t= bpf_hash_map_seq_start,\n\t.next\t= bpf_hash_map_seq_next,\n\t.stop\t= bpf_hash_map_seq_stop,\n\t.show\t= bpf_hash_map_seq_show,\n};\n\nstatic const struct bpf_iter_seq_info iter_seq_info = {\n\t.seq_ops\t\t= &bpf_hash_map_seq_ops,\n\t.init_seq_private\t= bpf_iter_init_hash_map,\n\t.fini_seq_private\t= bpf_iter_fini_hash_map,\n\t.seq_priv_size\t\t= sizeof(struct bpf_iter_seq_hash_map_info),\n};\n\nstatic long bpf_for_each_hash_elem(struct bpf_map *map, bpf_callback_t callback_fn,\n\t\t\t\t   void *callback_ctx, u64 flags)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_head *head;\n\tstruct hlist_nulls_node *n;\n\tstruct htab_elem *elem;\n\tu32 roundup_key_size;\n\tint i, num_elems = 0;\n\tvoid __percpu *pptr;\n\tstruct bucket *b;\n\tvoid *key, *val;\n\tbool is_percpu;\n\tu64 ret = 0;\n\n\tif (flags != 0)\n\t\treturn -EINVAL;\n\n\tis_percpu = htab_is_percpu(htab);\n\n\troundup_key_size = round_up(map->key_size, 8);\n\t \n\tif (is_percpu)\n\t\tmigrate_disable();\n\tfor (i = 0; i < htab->n_buckets; i++) {\n\t\tb = &htab->buckets[i];\n\t\trcu_read_lock();\n\t\thead = &b->head;\n\t\thlist_nulls_for_each_entry_rcu(elem, n, head, hash_node) {\n\t\t\tkey = elem->key;\n\t\t\tif (is_percpu) {\n\t\t\t\t \n\t\t\t\tpptr = htab_elem_get_ptr(elem, map->key_size);\n\t\t\t\tval = this_cpu_ptr(pptr);\n\t\t\t} else {\n\t\t\t\tval = elem->key + roundup_key_size;\n\t\t\t}\n\t\t\tnum_elems++;\n\t\t\tret = callback_fn((u64)(long)map, (u64)(long)key,\n\t\t\t\t\t  (u64)(long)val, (u64)(long)callback_ctx, 0);\n\t\t\t \n\t\t\tif (ret) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\nout:\n\tif (is_percpu)\n\t\tmigrate_enable();\n\treturn num_elems;\n}\n\nstatic u64 htab_map_mem_usage(const struct bpf_map *map)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tu32 value_size = round_up(htab->map.value_size, 8);\n\tbool prealloc = htab_is_prealloc(htab);\n\tbool percpu = htab_is_percpu(htab);\n\tbool lru = htab_is_lru(htab);\n\tu64 num_entries;\n\tu64 usage = sizeof(struct bpf_htab);\n\n\tusage += sizeof(struct bucket) * htab->n_buckets;\n\tusage += sizeof(int) * num_possible_cpus() * HASHTAB_MAP_LOCK_COUNT;\n\tif (prealloc) {\n\t\tnum_entries = map->max_entries;\n\t\tif (htab_has_extra_elems(htab))\n\t\t\tnum_entries += num_possible_cpus();\n\n\t\tusage += htab->elem_size * num_entries;\n\n\t\tif (percpu)\n\t\t\tusage += value_size * num_possible_cpus() * num_entries;\n\t\telse if (!lru)\n\t\t\tusage += sizeof(struct htab_elem *) * num_possible_cpus();\n\t} else {\n#define LLIST_NODE_SZ sizeof(struct llist_node)\n\n\t\tnum_entries = htab->use_percpu_counter ?\n\t\t\t\t\t  percpu_counter_sum(&htab->pcount) :\n\t\t\t\t\t  atomic_read(&htab->count);\n\t\tusage += (htab->elem_size + LLIST_NODE_SZ) * num_entries;\n\t\tif (percpu) {\n\t\t\tusage += (LLIST_NODE_SZ + sizeof(void *)) * num_entries;\n\t\t\tusage += value_size * num_possible_cpus() * num_entries;\n\t\t}\n\t}\n\treturn usage;\n}\n\nBTF_ID_LIST_SINGLE(htab_map_btf_ids, struct, bpf_htab)\nconst struct bpf_map_ops htab_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = htab_map_alloc_check,\n\t.map_alloc = htab_map_alloc,\n\t.map_free = htab_map_free,\n\t.map_get_next_key = htab_map_get_next_key,\n\t.map_release_uref = htab_map_free_timers,\n\t.map_lookup_elem = htab_map_lookup_elem,\n\t.map_lookup_and_delete_elem = htab_map_lookup_and_delete_elem,\n\t.map_update_elem = htab_map_update_elem,\n\t.map_delete_elem = htab_map_delete_elem,\n\t.map_gen_lookup = htab_map_gen_lookup,\n\t.map_seq_show_elem = htab_map_seq_show_elem,\n\t.map_set_for_each_callback_args = map_set_for_each_callback_args,\n\t.map_for_each_callback = bpf_for_each_hash_elem,\n\t.map_mem_usage = htab_map_mem_usage,\n\tBATCH_OPS(htab),\n\t.map_btf_id = &htab_map_btf_ids[0],\n\t.iter_seq_info = &iter_seq_info,\n};\n\nconst struct bpf_map_ops htab_lru_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = htab_map_alloc_check,\n\t.map_alloc = htab_map_alloc,\n\t.map_free = htab_map_free,\n\t.map_get_next_key = htab_map_get_next_key,\n\t.map_release_uref = htab_map_free_timers,\n\t.map_lookup_elem = htab_lru_map_lookup_elem,\n\t.map_lookup_and_delete_elem = htab_lru_map_lookup_and_delete_elem,\n\t.map_lookup_elem_sys_only = htab_lru_map_lookup_elem_sys,\n\t.map_update_elem = htab_lru_map_update_elem,\n\t.map_delete_elem = htab_lru_map_delete_elem,\n\t.map_gen_lookup = htab_lru_map_gen_lookup,\n\t.map_seq_show_elem = htab_map_seq_show_elem,\n\t.map_set_for_each_callback_args = map_set_for_each_callback_args,\n\t.map_for_each_callback = bpf_for_each_hash_elem,\n\t.map_mem_usage = htab_map_mem_usage,\n\tBATCH_OPS(htab_lru),\n\t.map_btf_id = &htab_map_btf_ids[0],\n\t.iter_seq_info = &iter_seq_info,\n};\n\n \nstatic void *htab_percpu_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct htab_elem *l = __htab_map_lookup_elem(map, key);\n\n\tif (l)\n\t\treturn this_cpu_ptr(htab_elem_get_ptr(l, map->key_size));\n\telse\n\t\treturn NULL;\n}\n\nstatic void *htab_percpu_map_lookup_percpu_elem(struct bpf_map *map, void *key, u32 cpu)\n{\n\tstruct htab_elem *l;\n\n\tif (cpu >= nr_cpu_ids)\n\t\treturn NULL;\n\n\tl = __htab_map_lookup_elem(map, key);\n\tif (l)\n\t\treturn per_cpu_ptr(htab_elem_get_ptr(l, map->key_size), cpu);\n\telse\n\t\treturn NULL;\n}\n\nstatic void *htab_lru_percpu_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct htab_elem *l = __htab_map_lookup_elem(map, key);\n\n\tif (l) {\n\t\tbpf_lru_node_set_ref(&l->lru_node);\n\t\treturn this_cpu_ptr(htab_elem_get_ptr(l, map->key_size));\n\t}\n\n\treturn NULL;\n}\n\nstatic void *htab_lru_percpu_map_lookup_percpu_elem(struct bpf_map *map, void *key, u32 cpu)\n{\n\tstruct htab_elem *l;\n\n\tif (cpu >= nr_cpu_ids)\n\t\treturn NULL;\n\n\tl = __htab_map_lookup_elem(map, key);\n\tif (l) {\n\t\tbpf_lru_node_set_ref(&l->lru_node);\n\t\treturn per_cpu_ptr(htab_elem_get_ptr(l, map->key_size), cpu);\n\t}\n\n\treturn NULL;\n}\n\nint bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value)\n{\n\tstruct htab_elem *l;\n\tvoid __percpu *pptr;\n\tint ret = -ENOENT;\n\tint cpu, off = 0;\n\tu32 size;\n\n\t \n\tsize = round_up(map->value_size, 8);\n\trcu_read_lock();\n\tl = __htab_map_lookup_elem(map, key);\n\tif (!l)\n\t\tgoto out;\n\t \n\tpptr = htab_elem_get_ptr(l, map->key_size);\n\tfor_each_possible_cpu(cpu) {\n\t\tcopy_map_value_long(map, value + off, per_cpu_ptr(pptr, cpu));\n\t\tcheck_and_init_map_value(map, value + off);\n\t\toff += size;\n\t}\n\tret = 0;\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nint bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,\n\t\t\t   u64 map_flags)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tint ret;\n\n\trcu_read_lock();\n\tif (htab_is_lru(htab))\n\t\tret = __htab_lru_percpu_map_update_elem(map, key, value,\n\t\t\t\t\t\t\tmap_flags, true);\n\telse\n\t\tret = __htab_percpu_map_update_elem(map, key, value, map_flags,\n\t\t\t\t\t\t    true);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void htab_percpu_map_seq_show_elem(struct bpf_map *map, void *key,\n\t\t\t\t\t  struct seq_file *m)\n{\n\tstruct htab_elem *l;\n\tvoid __percpu *pptr;\n\tint cpu;\n\n\trcu_read_lock();\n\n\tl = __htab_map_lookup_elem(map, key);\n\tif (!l) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tbtf_type_seq_show(map->btf, map->btf_key_type_id, key, m);\n\tseq_puts(m, \": {\\n\");\n\tpptr = htab_elem_get_ptr(l, map->key_size);\n\tfor_each_possible_cpu(cpu) {\n\t\tseq_printf(m, \"\\tcpu%d: \", cpu);\n\t\tbtf_type_seq_show(map->btf, map->btf_value_type_id,\n\t\t\t\t  per_cpu_ptr(pptr, cpu), m);\n\t\tseq_puts(m, \"\\n\");\n\t}\n\tseq_puts(m, \"}\\n\");\n\n\trcu_read_unlock();\n}\n\nconst struct bpf_map_ops htab_percpu_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = htab_map_alloc_check,\n\t.map_alloc = htab_map_alloc,\n\t.map_free = htab_map_free,\n\t.map_get_next_key = htab_map_get_next_key,\n\t.map_lookup_elem = htab_percpu_map_lookup_elem,\n\t.map_lookup_and_delete_elem = htab_percpu_map_lookup_and_delete_elem,\n\t.map_update_elem = htab_percpu_map_update_elem,\n\t.map_delete_elem = htab_map_delete_elem,\n\t.map_lookup_percpu_elem = htab_percpu_map_lookup_percpu_elem,\n\t.map_seq_show_elem = htab_percpu_map_seq_show_elem,\n\t.map_set_for_each_callback_args = map_set_for_each_callback_args,\n\t.map_for_each_callback = bpf_for_each_hash_elem,\n\t.map_mem_usage = htab_map_mem_usage,\n\tBATCH_OPS(htab_percpu),\n\t.map_btf_id = &htab_map_btf_ids[0],\n\t.iter_seq_info = &iter_seq_info,\n};\n\nconst struct bpf_map_ops htab_lru_percpu_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = htab_map_alloc_check,\n\t.map_alloc = htab_map_alloc,\n\t.map_free = htab_map_free,\n\t.map_get_next_key = htab_map_get_next_key,\n\t.map_lookup_elem = htab_lru_percpu_map_lookup_elem,\n\t.map_lookup_and_delete_elem = htab_lru_percpu_map_lookup_and_delete_elem,\n\t.map_update_elem = htab_lru_percpu_map_update_elem,\n\t.map_delete_elem = htab_lru_map_delete_elem,\n\t.map_lookup_percpu_elem = htab_lru_percpu_map_lookup_percpu_elem,\n\t.map_seq_show_elem = htab_percpu_map_seq_show_elem,\n\t.map_set_for_each_callback_args = map_set_for_each_callback_args,\n\t.map_for_each_callback = bpf_for_each_hash_elem,\n\t.map_mem_usage = htab_map_mem_usage,\n\tBATCH_OPS(htab_lru_percpu),\n\t.map_btf_id = &htab_map_btf_ids[0],\n\t.iter_seq_info = &iter_seq_info,\n};\n\nstatic int fd_htab_map_alloc_check(union bpf_attr *attr)\n{\n\tif (attr->value_size != sizeof(u32))\n\t\treturn -EINVAL;\n\treturn htab_map_alloc_check(attr);\n}\n\nstatic void fd_htab_map_free(struct bpf_map *map)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tstruct hlist_nulls_node *n;\n\tstruct hlist_nulls_head *head;\n\tstruct htab_elem *l;\n\tint i;\n\n\tfor (i = 0; i < htab->n_buckets; i++) {\n\t\thead = select_bucket(htab, i);\n\n\t\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\n\t\t\tvoid *ptr = fd_htab_map_get_ptr(map, l);\n\n\t\t\tmap->ops->map_fd_put_ptr(map, ptr, false);\n\t\t}\n\t}\n\n\thtab_map_free(map);\n}\n\n \nint bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value)\n{\n\tvoid **ptr;\n\tint ret = 0;\n\n\tif (!map->ops->map_fd_sys_lookup_elem)\n\t\treturn -ENOTSUPP;\n\n\trcu_read_lock();\n\tptr = htab_map_lookup_elem(map, key);\n\tif (ptr)\n\t\t*value = map->ops->map_fd_sys_lookup_elem(READ_ONCE(*ptr));\n\telse\n\t\tret = -ENOENT;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nint bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,\n\t\t\t\tvoid *key, void *value, u64 map_flags)\n{\n\tvoid *ptr;\n\tint ret;\n\tu32 ufd = *(u32 *)value;\n\n\tptr = map->ops->map_fd_get_ptr(map, map_file, ufd);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tret = htab_map_update_elem(map, key, &ptr, map_flags);\n\tif (ret)\n\t\tmap->ops->map_fd_put_ptr(map, ptr, false);\n\n\treturn ret;\n}\n\nstatic struct bpf_map *htab_of_map_alloc(union bpf_attr *attr)\n{\n\tstruct bpf_map *map, *inner_map_meta;\n\n\tinner_map_meta = bpf_map_meta_alloc(attr->inner_map_fd);\n\tif (IS_ERR(inner_map_meta))\n\t\treturn inner_map_meta;\n\n\tmap = htab_map_alloc(attr);\n\tif (IS_ERR(map)) {\n\t\tbpf_map_meta_free(inner_map_meta);\n\t\treturn map;\n\t}\n\n\tmap->inner_map_meta = inner_map_meta;\n\n\treturn map;\n}\n\nstatic void *htab_of_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_map **inner_map  = htab_map_lookup_elem(map, key);\n\n\tif (!inner_map)\n\t\treturn NULL;\n\n\treturn READ_ONCE(*inner_map);\n}\n\nstatic int htab_of_map_gen_lookup(struct bpf_map *map,\n\t\t\t\t  struct bpf_insn *insn_buf)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tconst int ret = BPF_REG_0;\n\n\tBUILD_BUG_ON(!__same_type(&__htab_map_lookup_elem,\n\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t*insn++ = BPF_EMIT_CALL(__htab_map_lookup_elem);\n\t*insn++ = BPF_JMP_IMM(BPF_JEQ, ret, 0, 2);\n\t*insn++ = BPF_ALU64_IMM(BPF_ADD, ret,\n\t\t\t\toffsetof(struct htab_elem, key) +\n\t\t\t\tround_up(map->key_size, 8));\n\t*insn++ = BPF_LDX_MEM(BPF_DW, ret, ret, 0);\n\n\treturn insn - insn_buf;\n}\n\nstatic void htab_of_map_free(struct bpf_map *map)\n{\n\tbpf_map_meta_free(map->inner_map_meta);\n\tfd_htab_map_free(map);\n}\n\nconst struct bpf_map_ops htab_of_maps_map_ops = {\n\t.map_alloc_check = fd_htab_map_alloc_check,\n\t.map_alloc = htab_of_map_alloc,\n\t.map_free = htab_of_map_free,\n\t.map_get_next_key = htab_map_get_next_key,\n\t.map_lookup_elem = htab_of_map_lookup_elem,\n\t.map_delete_elem = htab_map_delete_elem,\n\t.map_fd_get_ptr = bpf_map_fd_get_ptr,\n\t.map_fd_put_ptr = bpf_map_fd_put_ptr,\n\t.map_fd_sys_lookup_elem = bpf_map_fd_sys_lookup_elem,\n\t.map_gen_lookup = htab_of_map_gen_lookup,\n\t.map_check_btf = map_check_no_btf,\n\t.map_mem_usage = htab_map_mem_usage,\n\tBATCH_OPS(htab),\n\t.map_btf_id = &htab_map_btf_ids[0],\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}