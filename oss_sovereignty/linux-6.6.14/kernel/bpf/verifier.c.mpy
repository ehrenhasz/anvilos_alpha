{
  "module_name": "verifier.c",
  "hash_id": "d28349df9ace4e75a1444692fe6028947661ab8bb1288bedf75a9c8a4f35f630",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/verifier.c",
  "human_readable_source": "\n \n#include <uapi/linux/btf.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/btf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/filter.h>\n#include <net/netlink.h>\n#include <linux/file.h>\n#include <linux/vmalloc.h>\n#include <linux/stringify.h>\n#include <linux/bsearch.h>\n#include <linux/sort.h>\n#include <linux/perf_event.h>\n#include <linux/ctype.h>\n#include <linux/error-injection.h>\n#include <linux/bpf_lsm.h>\n#include <linux/btf_ids.h>\n#include <linux/poison.h>\n#include <linux/module.h>\n#include <linux/cpumask.h>\n#include <net/xdp.h>\n\n#include \"disasm.h\"\n\nstatic const struct bpf_verifier_ops * const bpf_verifier_ops[] = {\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \\\n\t[_id] = & _name ## _verifier_ops,\n#define BPF_MAP_TYPE(_id, _ops)\n#define BPF_LINK_TYPE(_id, _name)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n};\n\n \n\n \nstruct bpf_verifier_stack_elem {\n\t \n\tstruct bpf_verifier_state st;\n\tint insn_idx;\n\tint prev_insn_idx;\n\tstruct bpf_verifier_stack_elem *next;\n\t \n\tu32 log_pos;\n};\n\n#define BPF_COMPLEXITY_LIMIT_JMP_SEQ\t8192\n#define BPF_COMPLEXITY_LIMIT_STATES\t64\n\n#define BPF_MAP_KEY_POISON\t(1ULL << 63)\n#define BPF_MAP_KEY_SEEN\t(1ULL << 62)\n\n#define BPF_MAP_PTR_UNPRIV\t1UL\n#define BPF_MAP_PTR_POISON\t((void *)((0xeB9FUL << 1) +\t\\\n\t\t\t\t\t  POISON_POINTER_DELTA))\n#define BPF_MAP_PTR(X)\t\t((struct bpf_map *)((X) & ~BPF_MAP_PTR_UNPRIV))\n\nstatic int acquire_reference_state(struct bpf_verifier_env *env, int insn_idx);\nstatic int release_reference(struct bpf_verifier_env *env, int ref_obj_id);\nstatic void invalidate_non_owning_refs(struct bpf_verifier_env *env);\nstatic bool in_rbtree_lock_required_cb(struct bpf_verifier_env *env);\nstatic int ref_set_non_owning(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *reg);\nstatic void specialize_kfunc(struct bpf_verifier_env *env,\n\t\t\t     u32 func_id, u16 offset, unsigned long *addr);\nstatic bool is_trusted_reg(const struct bpf_reg_state *reg);\n\nstatic bool bpf_map_ptr_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn BPF_MAP_PTR(aux->map_ptr_state) == BPF_MAP_PTR_POISON;\n}\n\nstatic bool bpf_map_ptr_unpriv(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_ptr_state & BPF_MAP_PTR_UNPRIV;\n}\n\nstatic void bpf_map_ptr_store(struct bpf_insn_aux_data *aux,\n\t\t\t      const struct bpf_map *map, bool unpriv)\n{\n\tBUILD_BUG_ON((unsigned long)BPF_MAP_PTR_POISON & BPF_MAP_PTR_UNPRIV);\n\tunpriv |= bpf_map_ptr_unpriv(aux);\n\taux->map_ptr_state = (unsigned long)map |\n\t\t\t     (unpriv ? BPF_MAP_PTR_UNPRIV : 0UL);\n}\n\nstatic bool bpf_map_key_poisoned(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_key_state & BPF_MAP_KEY_POISON;\n}\n\nstatic bool bpf_map_key_unseen(const struct bpf_insn_aux_data *aux)\n{\n\treturn !(aux->map_key_state & BPF_MAP_KEY_SEEN);\n}\n\nstatic u64 bpf_map_key_immediate(const struct bpf_insn_aux_data *aux)\n{\n\treturn aux->map_key_state & ~(BPF_MAP_KEY_SEEN | BPF_MAP_KEY_POISON);\n}\n\nstatic void bpf_map_key_store(struct bpf_insn_aux_data *aux, u64 state)\n{\n\tbool poisoned = bpf_map_key_poisoned(aux);\n\n\taux->map_key_state = state | BPF_MAP_KEY_SEEN |\n\t\t\t     (poisoned ? BPF_MAP_KEY_POISON : 0ULL);\n}\n\nstatic bool bpf_helper_call(const struct bpf_insn *insn)\n{\n\treturn insn->code == (BPF_JMP | BPF_CALL) &&\n\t       insn->src_reg == 0;\n}\n\nstatic bool bpf_pseudo_call(const struct bpf_insn *insn)\n{\n\treturn insn->code == (BPF_JMP | BPF_CALL) &&\n\t       insn->src_reg == BPF_PSEUDO_CALL;\n}\n\nstatic bool bpf_pseudo_kfunc_call(const struct bpf_insn *insn)\n{\n\treturn insn->code == (BPF_JMP | BPF_CALL) &&\n\t       insn->src_reg == BPF_PSEUDO_KFUNC_CALL;\n}\n\nstruct bpf_call_arg_meta {\n\tstruct bpf_map *map_ptr;\n\tbool raw_mode;\n\tbool pkt_access;\n\tu8 release_regno;\n\tint regno;\n\tint access_size;\n\tint mem_size;\n\tu64 msize_max_value;\n\tint ref_obj_id;\n\tint dynptr_id;\n\tint map_uid;\n\tint func_id;\n\tstruct btf *btf;\n\tu32 btf_id;\n\tstruct btf *ret_btf;\n\tu32 ret_btf_id;\n\tu32 subprogno;\n\tstruct btf_field *kptr_field;\n};\n\nstruct bpf_kfunc_call_arg_meta {\n\t \n\tstruct btf *btf;\n\tu32 func_id;\n\tu32 kfunc_flags;\n\tconst struct btf_type *func_proto;\n\tconst char *func_name;\n\t \n\tu32 ref_obj_id;\n\tu8 release_regno;\n\tbool r0_rdonly;\n\tu32 ret_btf_id;\n\tu64 r0_size;\n\tu32 subprogno;\n\tstruct {\n\t\tu64 value;\n\t\tbool found;\n\t} arg_constant;\n\n\t \n\tstruct btf *arg_btf;\n\tu32 arg_btf_id;\n\tbool arg_owning_ref;\n\n\tstruct {\n\t\tstruct btf_field *field;\n\t} arg_list_head;\n\tstruct {\n\t\tstruct btf_field *field;\n\t} arg_rbtree_root;\n\tstruct {\n\t\tenum bpf_dynptr_type type;\n\t\tu32 id;\n\t\tu32 ref_obj_id;\n\t} initialized_dynptr;\n\tstruct {\n\t\tu8 spi;\n\t\tu8 frameno;\n\t} iter;\n\tu64 mem_size;\n};\n\nstruct btf *btf_vmlinux;\n\nstatic DEFINE_MUTEX(bpf_verifier_lock);\n\nstatic const struct bpf_line_info *\nfind_linfo(const struct bpf_verifier_env *env, u32 insn_off)\n{\n\tconst struct bpf_line_info *linfo;\n\tconst struct bpf_prog *prog;\n\tu32 i, nr_linfo;\n\n\tprog = env->prog;\n\tnr_linfo = prog->aux->nr_linfo;\n\n\tif (!nr_linfo || insn_off >= prog->len)\n\t\treturn NULL;\n\n\tlinfo = prog->aux->linfo;\n\tfor (i = 1; i < nr_linfo; i++)\n\t\tif (insn_off < linfo[i].insn_off)\n\t\t\tbreak;\n\n\treturn &linfo[i - 1];\n}\n\n__printf(2, 3) static void verbose(void *private_data, const char *fmt, ...)\n{\n\tstruct bpf_verifier_env *env = private_data;\n\tva_list args;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tva_start(args, fmt);\n\tbpf_verifier_vlog(&env->log, fmt, args);\n\tva_end(args);\n}\n\nstatic const char *ltrim(const char *s)\n{\n\twhile (isspace(*s))\n\t\ts++;\n\n\treturn s;\n}\n\n__printf(3, 4) static void verbose_linfo(struct bpf_verifier_env *env,\n\t\t\t\t\t u32 insn_off,\n\t\t\t\t\t const char *prefix_fmt, ...)\n{\n\tconst struct bpf_line_info *linfo;\n\n\tif (!bpf_verifier_log_needed(&env->log))\n\t\treturn;\n\n\tlinfo = find_linfo(env, insn_off);\n\tif (!linfo || linfo == env->prev_linfo)\n\t\treturn;\n\n\tif (prefix_fmt) {\n\t\tva_list args;\n\n\t\tva_start(args, prefix_fmt);\n\t\tbpf_verifier_vlog(&env->log, prefix_fmt, args);\n\t\tva_end(args);\n\t}\n\n\tverbose(env, \"%s\\n\",\n\t\tltrim(btf_name_by_offset(env->prog->aux->btf,\n\t\t\t\t\t linfo->line_off)));\n\n\tenv->prev_linfo = linfo;\n}\n\nstatic void verbose_invalid_scalar(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *reg,\n\t\t\t\t   struct tnum *range, const char *ctx,\n\t\t\t\t   const char *reg_name)\n{\n\tchar tn_buf[48];\n\n\tverbose(env, \"At %s the register %s \", ctx, reg_name);\n\tif (!tnum_is_unknown(reg->var_off)) {\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"has value %s\", tn_buf);\n\t} else {\n\t\tverbose(env, \"has unknown scalar value\");\n\t}\n\ttnum_strn(tn_buf, sizeof(tn_buf), *range);\n\tverbose(env, \" should have been in %s\\n\", tn_buf);\n}\n\nstatic bool type_is_pkt_pointer(enum bpf_reg_type type)\n{\n\ttype = base_type(type);\n\treturn type == PTR_TO_PACKET ||\n\t       type == PTR_TO_PACKET_META;\n}\n\nstatic bool type_is_sk_pointer(enum bpf_reg_type type)\n{\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_SOCK_COMMON ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_XDP_SOCK;\n}\n\nstatic bool type_may_be_null(u32 type)\n{\n\treturn type & PTR_MAYBE_NULL;\n}\n\nstatic bool reg_not_null(const struct bpf_reg_state *reg)\n{\n\tenum bpf_reg_type type;\n\n\ttype = reg->type;\n\tif (type_may_be_null(type))\n\t\treturn false;\n\n\ttype = base_type(type);\n\treturn type == PTR_TO_SOCKET ||\n\t\ttype == PTR_TO_TCP_SOCK ||\n\t\ttype == PTR_TO_MAP_VALUE ||\n\t\ttype == PTR_TO_MAP_KEY ||\n\t\ttype == PTR_TO_SOCK_COMMON ||\n\t\t(type == PTR_TO_BTF_ID && is_trusted_reg(reg)) ||\n\t\ttype == PTR_TO_MEM;\n}\n\nstatic bool type_is_ptr_alloc_obj(u32 type)\n{\n\treturn base_type(type) == PTR_TO_BTF_ID && type_flag(type) & MEM_ALLOC;\n}\n\nstatic bool type_is_non_owning_ref(u32 type)\n{\n\treturn type_is_ptr_alloc_obj(type) && type_flag(type) & NON_OWN_REF;\n}\n\nstatic struct btf_record *reg_btf_record(const struct bpf_reg_state *reg)\n{\n\tstruct btf_record *rec = NULL;\n\tstruct btf_struct_meta *meta;\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\trec = reg->map_ptr->record;\n\t} else if (type_is_ptr_alloc_obj(reg->type)) {\n\t\tmeta = btf_find_struct_meta(reg->btf, reg->btf_id);\n\t\tif (meta)\n\t\t\trec = meta->record;\n\t}\n\treturn rec;\n}\n\nstatic bool subprog_is_global(const struct bpf_verifier_env *env, int subprog)\n{\n\tstruct bpf_func_info_aux *aux = env->prog->aux->func_info_aux;\n\n\treturn aux && aux[subprog].linkage == BTF_FUNC_GLOBAL;\n}\n\nstatic bool reg_may_point_to_spin_lock(const struct bpf_reg_state *reg)\n{\n\treturn btf_record_has_field(reg_btf_record(reg), BPF_SPIN_LOCK);\n}\n\nstatic bool type_is_rdonly_mem(u32 type)\n{\n\treturn type & MEM_RDONLY;\n}\n\nstatic bool is_acquire_function(enum bpf_func_id func_id,\n\t\t\t\tconst struct bpf_map *map)\n{\n\tenum bpf_map_type map_type = map ? map->map_type : BPF_MAP_TYPE_UNSPEC;\n\n\tif (func_id == BPF_FUNC_sk_lookup_tcp ||\n\t    func_id == BPF_FUNC_sk_lookup_udp ||\n\t    func_id == BPF_FUNC_skc_lookup_tcp ||\n\t    func_id == BPF_FUNC_ringbuf_reserve ||\n\t    func_id == BPF_FUNC_kptr_xchg)\n\t\treturn true;\n\n\tif (func_id == BPF_FUNC_map_lookup_elem &&\n\t    (map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t     map_type == BPF_MAP_TYPE_SOCKHASH))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool is_ptr_cast_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_tcp_sock ||\n\t\tfunc_id == BPF_FUNC_sk_fullsock ||\n\t\tfunc_id == BPF_FUNC_skc_to_tcp_sock ||\n\t\tfunc_id == BPF_FUNC_skc_to_tcp6_sock ||\n\t\tfunc_id == BPF_FUNC_skc_to_udp6_sock ||\n\t\tfunc_id == BPF_FUNC_skc_to_mptcp_sock ||\n\t\tfunc_id == BPF_FUNC_skc_to_tcp_timewait_sock ||\n\t\tfunc_id == BPF_FUNC_skc_to_tcp_request_sock;\n}\n\nstatic bool is_dynptr_ref_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_dynptr_data;\n}\n\nstatic bool is_callback_calling_kfunc(u32 btf_id);\n\nstatic bool is_callback_calling_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_for_each_map_elem ||\n\t       func_id == BPF_FUNC_timer_set_callback ||\n\t       func_id == BPF_FUNC_find_vma ||\n\t       func_id == BPF_FUNC_loop ||\n\t       func_id == BPF_FUNC_user_ringbuf_drain;\n}\n\nstatic bool is_async_callback_calling_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_timer_set_callback;\n}\n\nstatic bool is_storage_get_function(enum bpf_func_id func_id)\n{\n\treturn func_id == BPF_FUNC_sk_storage_get ||\n\t       func_id == BPF_FUNC_inode_storage_get ||\n\t       func_id == BPF_FUNC_task_storage_get ||\n\t       func_id == BPF_FUNC_cgrp_storage_get;\n}\n\nstatic bool helper_multiple_ref_obj_use(enum bpf_func_id func_id,\n\t\t\t\t\tconst struct bpf_map *map)\n{\n\tint ref_obj_uses = 0;\n\n\tif (is_ptr_cast_function(func_id))\n\t\tref_obj_uses++;\n\tif (is_acquire_function(func_id, map))\n\t\tref_obj_uses++;\n\tif (is_dynptr_ref_function(func_id))\n\t\tref_obj_uses++;\n\n\treturn ref_obj_uses > 1;\n}\n\nstatic bool is_cmpxchg_insn(const struct bpf_insn *insn)\n{\n\treturn BPF_CLASS(insn->code) == BPF_STX &&\n\t       BPF_MODE(insn->code) == BPF_ATOMIC &&\n\t       insn->imm == BPF_CMPXCHG;\n}\n\n \nstatic const char *reg_type_str(struct bpf_verifier_env *env,\n\t\t\t\tenum bpf_reg_type type)\n{\n\tchar postfix[16] = {0}, prefix[64] = {0};\n\tstatic const char * const str[] = {\n\t\t[NOT_INIT]\t\t= \"?\",\n\t\t[SCALAR_VALUE]\t\t= \"scalar\",\n\t\t[PTR_TO_CTX]\t\t= \"ctx\",\n\t\t[CONST_PTR_TO_MAP]\t= \"map_ptr\",\n\t\t[PTR_TO_MAP_VALUE]\t= \"map_value\",\n\t\t[PTR_TO_STACK]\t\t= \"fp\",\n\t\t[PTR_TO_PACKET]\t\t= \"pkt\",\n\t\t[PTR_TO_PACKET_META]\t= \"pkt_meta\",\n\t\t[PTR_TO_PACKET_END]\t= \"pkt_end\",\n\t\t[PTR_TO_FLOW_KEYS]\t= \"flow_keys\",\n\t\t[PTR_TO_SOCKET]\t\t= \"sock\",\n\t\t[PTR_TO_SOCK_COMMON]\t= \"sock_common\",\n\t\t[PTR_TO_TCP_SOCK]\t= \"tcp_sock\",\n\t\t[PTR_TO_TP_BUFFER]\t= \"tp_buffer\",\n\t\t[PTR_TO_XDP_SOCK]\t= \"xdp_sock\",\n\t\t[PTR_TO_BTF_ID]\t\t= \"ptr_\",\n\t\t[PTR_TO_MEM]\t\t= \"mem\",\n\t\t[PTR_TO_BUF]\t\t= \"buf\",\n\t\t[PTR_TO_FUNC]\t\t= \"func\",\n\t\t[PTR_TO_MAP_KEY]\t= \"map_key\",\n\t\t[CONST_PTR_TO_DYNPTR]\t= \"dynptr_ptr\",\n\t};\n\n\tif (type & PTR_MAYBE_NULL) {\n\t\tif (base_type(type) == PTR_TO_BTF_ID)\n\t\t\tstrncpy(postfix, \"or_null_\", 16);\n\t\telse\n\t\t\tstrncpy(postfix, \"_or_null\", 16);\n\t}\n\n\tsnprintf(prefix, sizeof(prefix), \"%s%s%s%s%s%s%s\",\n\t\t type & MEM_RDONLY ? \"rdonly_\" : \"\",\n\t\t type & MEM_RINGBUF ? \"ringbuf_\" : \"\",\n\t\t type & MEM_USER ? \"user_\" : \"\",\n\t\t type & MEM_PERCPU ? \"percpu_\" : \"\",\n\t\t type & MEM_RCU ? \"rcu_\" : \"\",\n\t\t type & PTR_UNTRUSTED ? \"untrusted_\" : \"\",\n\t\t type & PTR_TRUSTED ? \"trusted_\" : \"\"\n\t);\n\n\tsnprintf(env->tmp_str_buf, TMP_STR_BUF_LEN, \"%s%s%s\",\n\t\t prefix, str[base_type(type)], postfix);\n\treturn env->tmp_str_buf;\n}\n\nstatic char slot_type_char[] = {\n\t[STACK_INVALID]\t= '?',\n\t[STACK_SPILL]\t= 'r',\n\t[STACK_MISC]\t= 'm',\n\t[STACK_ZERO]\t= '0',\n\t[STACK_DYNPTR]\t= 'd',\n\t[STACK_ITER]\t= 'i',\n};\n\nstatic void print_liveness(struct bpf_verifier_env *env,\n\t\t\t   enum bpf_reg_liveness live)\n{\n\tif (live & (REG_LIVE_READ | REG_LIVE_WRITTEN | REG_LIVE_DONE))\n\t    verbose(env, \"_\");\n\tif (live & REG_LIVE_READ)\n\t\tverbose(env, \"r\");\n\tif (live & REG_LIVE_WRITTEN)\n\t\tverbose(env, \"w\");\n\tif (live & REG_LIVE_DONE)\n\t\tverbose(env, \"D\");\n}\n\nstatic int __get_spi(s32 off)\n{\n\treturn (-off - 1) / BPF_REG_SIZE;\n}\n\nstatic struct bpf_func_state *func(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[reg->frameno];\n}\n\nstatic bool is_spi_bounds_valid(struct bpf_func_state *state, int spi, int nr_slots)\n{\n       int allocated_slots = state->allocated_stack / BPF_REG_SIZE;\n\n        \n       return spi - nr_slots + 1 >= 0 && spi < allocated_slots;\n}\n\nstatic int stack_slot_obj_get_spi(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t\t          const char *obj_kind, int nr_slots)\n{\n\tint off, spi;\n\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tverbose(env, \"%s has to be at a constant offset\\n\", obj_kind);\n\t\treturn -EINVAL;\n\t}\n\n\toff = reg->off + reg->var_off.value;\n\tif (off % BPF_REG_SIZE) {\n\t\tverbose(env, \"cannot pass in %s at an offset=%d\\n\", obj_kind, off);\n\t\treturn -EINVAL;\n\t}\n\n\tspi = __get_spi(off);\n\tif (spi + 1 < nr_slots) {\n\t\tverbose(env, \"cannot pass in %s at an offset=%d\\n\", obj_kind, off);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!is_spi_bounds_valid(func(env, reg), spi, nr_slots))\n\t\treturn -ERANGE;\n\treturn spi;\n}\n\nstatic int dynptr_get_spi(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\treturn stack_slot_obj_get_spi(env, reg, \"dynptr\", BPF_DYNPTR_NR_SLOTS);\n}\n\nstatic int iter_get_spi(struct bpf_verifier_env *env, struct bpf_reg_state *reg, int nr_slots)\n{\n\treturn stack_slot_obj_get_spi(env, reg, \"iter\", nr_slots);\n}\n\nstatic const char *btf_type_name(const struct btf *btf, u32 id)\n{\n\treturn btf_name_by_offset(btf, btf_type_by_id(btf, id)->name_off);\n}\n\nstatic const char *dynptr_type_str(enum bpf_dynptr_type type)\n{\n\tswitch (type) {\n\tcase BPF_DYNPTR_TYPE_LOCAL:\n\t\treturn \"local\";\n\tcase BPF_DYNPTR_TYPE_RINGBUF:\n\t\treturn \"ringbuf\";\n\tcase BPF_DYNPTR_TYPE_SKB:\n\t\treturn \"skb\";\n\tcase BPF_DYNPTR_TYPE_XDP:\n\t\treturn \"xdp\";\n\tcase BPF_DYNPTR_TYPE_INVALID:\n\t\treturn \"<invalid>\";\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown dynptr type %d\\n\", type);\n\t\treturn \"<unknown>\";\n\t}\n}\n\nstatic const char *iter_type_str(const struct btf *btf, u32 btf_id)\n{\n\tif (!btf || btf_id == 0)\n\t\treturn \"<invalid>\";\n\n\t \n\treturn btf_type_name(btf, btf_id) + sizeof(ITER_PREFIX) - 1;\n}\n\nstatic const char *iter_state_str(enum bpf_iter_state state)\n{\n\tswitch (state) {\n\tcase BPF_ITER_STATE_ACTIVE:\n\t\treturn \"active\";\n\tcase BPF_ITER_STATE_DRAINED:\n\t\treturn \"drained\";\n\tcase BPF_ITER_STATE_INVALID:\n\t\treturn \"<invalid>\";\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown iter state %d\\n\", state);\n\t\treturn \"<unknown>\";\n\t}\n}\n\nstatic void mark_reg_scratched(struct bpf_verifier_env *env, u32 regno)\n{\n\tenv->scratched_regs |= 1U << regno;\n}\n\nstatic void mark_stack_slot_scratched(struct bpf_verifier_env *env, u32 spi)\n{\n\tenv->scratched_stack_slots |= 1ULL << spi;\n}\n\nstatic bool reg_scratched(const struct bpf_verifier_env *env, u32 regno)\n{\n\treturn (env->scratched_regs >> regno) & 1;\n}\n\nstatic bool stack_slot_scratched(const struct bpf_verifier_env *env, u64 regno)\n{\n\treturn (env->scratched_stack_slots >> regno) & 1;\n}\n\nstatic bool verifier_state_scratched(const struct bpf_verifier_env *env)\n{\n\treturn env->scratched_regs || env->scratched_stack_slots;\n}\n\nstatic void mark_verifier_state_clean(struct bpf_verifier_env *env)\n{\n\tenv->scratched_regs = 0U;\n\tenv->scratched_stack_slots = 0ULL;\n}\n\n \nstatic void mark_verifier_state_scratched(struct bpf_verifier_env *env)\n{\n\tenv->scratched_regs = ~0U;\n\tenv->scratched_stack_slots = ~0ULL;\n}\n\nstatic enum bpf_dynptr_type arg_to_dynptr_type(enum bpf_arg_type arg_type)\n{\n\tswitch (arg_type & DYNPTR_TYPE_FLAG_MASK) {\n\tcase DYNPTR_TYPE_LOCAL:\n\t\treturn BPF_DYNPTR_TYPE_LOCAL;\n\tcase DYNPTR_TYPE_RINGBUF:\n\t\treturn BPF_DYNPTR_TYPE_RINGBUF;\n\tcase DYNPTR_TYPE_SKB:\n\t\treturn BPF_DYNPTR_TYPE_SKB;\n\tcase DYNPTR_TYPE_XDP:\n\t\treturn BPF_DYNPTR_TYPE_XDP;\n\tdefault:\n\t\treturn BPF_DYNPTR_TYPE_INVALID;\n\t}\n}\n\nstatic enum bpf_type_flag get_dynptr_type_flag(enum bpf_dynptr_type type)\n{\n\tswitch (type) {\n\tcase BPF_DYNPTR_TYPE_LOCAL:\n\t\treturn DYNPTR_TYPE_LOCAL;\n\tcase BPF_DYNPTR_TYPE_RINGBUF:\n\t\treturn DYNPTR_TYPE_RINGBUF;\n\tcase BPF_DYNPTR_TYPE_SKB:\n\t\treturn DYNPTR_TYPE_SKB;\n\tcase BPF_DYNPTR_TYPE_XDP:\n\t\treturn DYNPTR_TYPE_XDP;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic bool dynptr_type_refcounted(enum bpf_dynptr_type type)\n{\n\treturn type == BPF_DYNPTR_TYPE_RINGBUF;\n}\n\nstatic void __mark_dynptr_reg(struct bpf_reg_state *reg,\n\t\t\t      enum bpf_dynptr_type type,\n\t\t\t      bool first_slot, int dynptr_id);\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg);\n\nstatic void mark_dynptr_stack_regs(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *sreg1,\n\t\t\t\t   struct bpf_reg_state *sreg2,\n\t\t\t\t   enum bpf_dynptr_type type)\n{\n\tint id = ++env->id_gen;\n\n\t__mark_dynptr_reg(sreg1, type, true, id);\n\t__mark_dynptr_reg(sreg2, type, false, id);\n}\n\nstatic void mark_dynptr_cb_reg(struct bpf_verifier_env *env,\n\t\t\t       struct bpf_reg_state *reg,\n\t\t\t       enum bpf_dynptr_type type)\n{\n\t__mark_dynptr_reg(reg, type, true, ++env->id_gen);\n}\n\nstatic int destroy_if_dynptr_stack_slot(struct bpf_verifier_env *env,\n\t\t\t\t        struct bpf_func_state *state, int spi);\n\nstatic int mark_stack_slots_dynptr(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t\t\t   enum bpf_arg_type arg_type, int insn_idx, int clone_ref_obj_id)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tenum bpf_dynptr_type type;\n\tint spi, i, err;\n\n\tspi = dynptr_get_spi(env, reg);\n\tif (spi < 0)\n\t\treturn spi;\n\n\t \n\terr = destroy_if_dynptr_stack_slot(env, state, spi);\n\tif (err)\n\t\treturn err;\n\terr = destroy_if_dynptr_stack_slot(env, state, spi - 1);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < BPF_REG_SIZE; i++) {\n\t\tstate->stack[spi].slot_type[i] = STACK_DYNPTR;\n\t\tstate->stack[spi - 1].slot_type[i] = STACK_DYNPTR;\n\t}\n\n\ttype = arg_to_dynptr_type(arg_type);\n\tif (type == BPF_DYNPTR_TYPE_INVALID)\n\t\treturn -EINVAL;\n\n\tmark_dynptr_stack_regs(env, &state->stack[spi].spilled_ptr,\n\t\t\t       &state->stack[spi - 1].spilled_ptr, type);\n\n\tif (dynptr_type_refcounted(type)) {\n\t\t \n\t\tint id;\n\n\t\tif (clone_ref_obj_id)\n\t\t\tid = clone_ref_obj_id;\n\t\telse\n\t\t\tid = acquire_reference_state(env, insn_idx);\n\n\t\tif (id < 0)\n\t\t\treturn id;\n\n\t\tstate->stack[spi].spilled_ptr.ref_obj_id = id;\n\t\tstate->stack[spi - 1].spilled_ptr.ref_obj_id = id;\n\t}\n\n\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\tstate->stack[spi - 1].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\treturn 0;\n}\n\nstatic void invalidate_dynptr(struct bpf_verifier_env *env, struct bpf_func_state *state, int spi)\n{\n\tint i;\n\n\tfor (i = 0; i < BPF_REG_SIZE; i++) {\n\t\tstate->stack[spi].slot_type[i] = STACK_INVALID;\n\t\tstate->stack[spi - 1].slot_type[i] = STACK_INVALID;\n\t}\n\n\t__mark_reg_not_init(env, &state->stack[spi].spilled_ptr);\n\t__mark_reg_not_init(env, &state->stack[spi - 1].spilled_ptr);\n\n\t \n\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\tstate->stack[spi - 1].spilled_ptr.live |= REG_LIVE_WRITTEN;\n}\n\nstatic int unmark_stack_slots_dynptr(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi, ref_obj_id, i;\n\n\tspi = dynptr_get_spi(env, reg);\n\tif (spi < 0)\n\t\treturn spi;\n\n\tif (!dynptr_type_refcounted(state->stack[spi].spilled_ptr.dynptr.type)) {\n\t\tinvalidate_dynptr(env, state, spi);\n\t\treturn 0;\n\t}\n\n\tref_obj_id = state->stack[spi].spilled_ptr.ref_obj_id;\n\n\t \n\n\t \n\tWARN_ON_ONCE(release_reference(env, ref_obj_id));\n\n\t \n\tfor (i = 1; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tif (state->stack[i].spilled_ptr.ref_obj_id != ref_obj_id)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (state->stack[i].slot_type[0] != STACK_DYNPTR) {\n\t\t\tverbose(env, \"verifier internal error: misconfigured ref_obj_id\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (state->stack[i].spilled_ptr.dynptr.first_slot)\n\t\t\tinvalidate_dynptr(env, state, i);\n\t}\n\n\treturn 0;\n}\n\nstatic void __mark_reg_unknown(const struct bpf_verifier_env *env,\n\t\t\t       struct bpf_reg_state *reg);\n\nstatic void mark_reg_invalid(const struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tif (!env->allow_ptr_leaks)\n\t\t__mark_reg_not_init(env, reg);\n\telse\n\t\t__mark_reg_unknown(env, reg);\n}\n\nstatic int destroy_if_dynptr_stack_slot(struct bpf_verifier_env *env,\n\t\t\t\t        struct bpf_func_state *state, int spi)\n{\n\tstruct bpf_func_state *fstate;\n\tstruct bpf_reg_state *dreg;\n\tint i, dynptr_id;\n\n\t \n\tif (state->stack[spi].slot_type[0] != STACK_DYNPTR)\n\t\treturn 0;\n\n\t \n\tif (!state->stack[spi].spilled_ptr.dynptr.first_slot)\n\t\tspi = spi + 1;\n\n\tif (dynptr_type_refcounted(state->stack[spi].spilled_ptr.dynptr.type)) {\n\t\tverbose(env, \"cannot overwrite referenced dynptr\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmark_stack_slot_scratched(env, spi);\n\tmark_stack_slot_scratched(env, spi - 1);\n\n\t \n\tfor (i = 0; i < BPF_REG_SIZE; i++) {\n\t\tstate->stack[spi].slot_type[i] = STACK_INVALID;\n\t\tstate->stack[spi - 1].slot_type[i] = STACK_INVALID;\n\t}\n\n\tdynptr_id = state->stack[spi].spilled_ptr.id;\n\t \n\tbpf_for_each_reg_in_vstate(env->cur_state, fstate, dreg, ({\n\t\t \n\t\tif (dreg->type != (PTR_TO_MEM | PTR_MAYBE_NULL) && dreg->type != PTR_TO_MEM)\n\t\t\tcontinue;\n\t\tif (dreg->dynptr_id == dynptr_id)\n\t\t\tmark_reg_invalid(env, dreg);\n\t}));\n\n\t \n\t__mark_reg_not_init(env, &state->stack[spi].spilled_ptr);\n\t__mark_reg_not_init(env, &state->stack[spi - 1].spilled_ptr);\n\n\t \n\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\tstate->stack[spi - 1].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\treturn 0;\n}\n\nstatic bool is_dynptr_reg_valid_uninit(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tint spi;\n\n\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\treturn false;\n\n\tspi = dynptr_get_spi(env, reg);\n\n\t \n\tif (spi < 0 && spi != -ERANGE)\n\t\treturn false;\n\n\t \n\treturn true;\n}\n\nstatic bool is_dynptr_reg_valid_init(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint i, spi;\n\n\t \n\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\treturn true;\n\n\tspi = dynptr_get_spi(env, reg);\n\tif (spi < 0)\n\t\treturn false;\n\tif (!state->stack[spi].spilled_ptr.dynptr.first_slot)\n\t\treturn false;\n\n\tfor (i = 0; i < BPF_REG_SIZE; i++) {\n\t\tif (state->stack[spi].slot_type[i] != STACK_DYNPTR ||\n\t\t    state->stack[spi - 1].slot_type[i] != STACK_DYNPTR)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool is_dynptr_type_expected(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_arg_type arg_type)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tenum bpf_dynptr_type dynptr_type;\n\tint spi;\n\n\t \n\tif (arg_type == ARG_PTR_TO_DYNPTR)\n\t\treturn true;\n\n\tdynptr_type = arg_to_dynptr_type(arg_type);\n\tif (reg->type == CONST_PTR_TO_DYNPTR) {\n\t\treturn reg->dynptr.type == dynptr_type;\n\t} else {\n\t\tspi = dynptr_get_spi(env, reg);\n\t\tif (spi < 0)\n\t\t\treturn false;\n\t\treturn state->stack[spi].spilled_ptr.dynptr.type == dynptr_type;\n\t}\n}\n\nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg);\n\nstatic int mark_stack_slots_iter(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_reg_state *reg, int insn_idx,\n\t\t\t\t struct btf *btf, u32 btf_id, int nr_slots)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi, i, j, id;\n\n\tspi = iter_get_spi(env, reg, nr_slots);\n\tif (spi < 0)\n\t\treturn spi;\n\n\tid = acquire_reference_state(env, insn_idx);\n\tif (id < 0)\n\t\treturn id;\n\n\tfor (i = 0; i < nr_slots; i++) {\n\t\tstruct bpf_stack_state *slot = &state->stack[spi - i];\n\t\tstruct bpf_reg_state *st = &slot->spilled_ptr;\n\n\t\t__mark_reg_known_zero(st);\n\t\tst->type = PTR_TO_STACK;  \n\t\tst->live |= REG_LIVE_WRITTEN;\n\t\tst->ref_obj_id = i == 0 ? id : 0;\n\t\tst->iter.btf = btf;\n\t\tst->iter.btf_id = btf_id;\n\t\tst->iter.state = BPF_ITER_STATE_ACTIVE;\n\t\tst->iter.depth = 0;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\tslot->slot_type[j] = STACK_ITER;\n\n\t\tmark_stack_slot_scratched(env, spi - i);\n\t}\n\n\treturn 0;\n}\n\nstatic int unmark_stack_slots_iter(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *reg, int nr_slots)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi, i, j;\n\n\tspi = iter_get_spi(env, reg, nr_slots);\n\tif (spi < 0)\n\t\treturn spi;\n\n\tfor (i = 0; i < nr_slots; i++) {\n\t\tstruct bpf_stack_state *slot = &state->stack[spi - i];\n\t\tstruct bpf_reg_state *st = &slot->spilled_ptr;\n\n\t\tif (i == 0)\n\t\t\tWARN_ON_ONCE(release_reference(env, st->ref_obj_id));\n\n\t\t__mark_reg_not_init(env, st);\n\n\t\t \n\t\tst->live |= REG_LIVE_WRITTEN;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\tslot->slot_type[j] = STACK_INVALID;\n\n\t\tmark_stack_slot_scratched(env, spi - i);\n\t}\n\n\treturn 0;\n}\n\nstatic bool is_iter_reg_valid_uninit(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_reg_state *reg, int nr_slots)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi, i, j;\n\n\t \n\tspi = iter_get_spi(env, reg, nr_slots);\n\tif (spi == -ERANGE)\n\t\treturn true;\n\tif (spi < 0)\n\t\treturn false;\n\n\tfor (i = 0; i < nr_slots; i++) {\n\t\tstruct bpf_stack_state *slot = &state->stack[spi - i];\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\tif (slot->slot_type[j] == STACK_ITER)\n\t\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool is_iter_reg_valid_init(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t\t\t   struct btf *btf, u32 btf_id, int nr_slots)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi, i, j;\n\n\tspi = iter_get_spi(env, reg, nr_slots);\n\tif (spi < 0)\n\t\treturn false;\n\n\tfor (i = 0; i < nr_slots; i++) {\n\t\tstruct bpf_stack_state *slot = &state->stack[spi - i];\n\t\tstruct bpf_reg_state *st = &slot->spilled_ptr;\n\n\t\t \n\t\tif (i == 0 && !st->ref_obj_id)\n\t\t\treturn false;\n\t\tif (i != 0 && st->ref_obj_id)\n\t\t\treturn false;\n\t\tif (st->iter.btf != btf || st->iter.btf_id != btf_id)\n\t\t\treturn false;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\tif (slot->slot_type[j] != STACK_ITER)\n\t\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic bool is_stack_slot_special(const struct bpf_stack_state *stack)\n{\n\tenum bpf_stack_slot_type type = stack->slot_type[BPF_REG_SIZE - 1];\n\n\tswitch (type) {\n\tcase STACK_SPILL:\n\tcase STACK_DYNPTR:\n\tcase STACK_ITER:\n\t\treturn true;\n\tcase STACK_INVALID:\n\tcase STACK_MISC:\n\tcase STACK_ZERO:\n\t\treturn false;\n\tdefault:\n\t\tWARN_ONCE(1, \"unknown stack slot type %d\\n\", type);\n\t\treturn true;\n\t}\n}\n\n \nstatic bool is_spilled_reg(const struct bpf_stack_state *stack)\n{\n\treturn stack->slot_type[BPF_REG_SIZE - 1] == STACK_SPILL;\n}\n\nstatic bool is_spilled_scalar_reg(const struct bpf_stack_state *stack)\n{\n\treturn stack->slot_type[BPF_REG_SIZE - 1] == STACK_SPILL &&\n\t       stack->spilled_ptr.type == SCALAR_VALUE;\n}\n\nstatic void scrub_spilled_slot(u8 *stype)\n{\n\tif (*stype != STACK_INVALID)\n\t\t*stype = STACK_MISC;\n}\n\nstatic void print_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_func_state *state,\n\t\t\t\t bool print_all)\n{\n\tconst struct bpf_reg_state *reg;\n\tenum bpf_reg_type t;\n\tint i;\n\n\tif (state->frameno)\n\t\tverbose(env, \" frame%d:\", state->frameno);\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\treg = &state->regs[i];\n\t\tt = reg->type;\n\t\tif (t == NOT_INIT)\n\t\t\tcontinue;\n\t\tif (!print_all && !reg_scratched(env, i))\n\t\t\tcontinue;\n\t\tverbose(env, \" R%d\", i);\n\t\tprint_liveness(env, reg->live);\n\t\tverbose(env, \"=\");\n\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\tverbose(env, \"P\");\n\t\tif ((t == SCALAR_VALUE || t == PTR_TO_STACK) &&\n\t\t    tnum_is_const(reg->var_off)) {\n\t\t\t \n\t\t\tverbose(env, \"%s\", t == SCALAR_VALUE ? \"\" : reg_type_str(env, t));\n\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t} else {\n\t\t\tconst char *sep = \"\";\n\n\t\t\tverbose(env, \"%s\", reg_type_str(env, t));\n\t\t\tif (base_type(t) == PTR_TO_BTF_ID)\n\t\t\t\tverbose(env, \"%s\", btf_type_name(reg->btf, reg->btf_id));\n\t\t\tverbose(env, \"(\");\n \n#define verbose_a(fmt, ...) ({ verbose(env, \"%s\" fmt, sep, __VA_ARGS__); sep = \",\"; })\n\n\t\t\tif (reg->id)\n\t\t\t\tverbose_a(\"id=%d\", reg->id);\n\t\t\tif (reg->ref_obj_id)\n\t\t\t\tverbose_a(\"ref_obj_id=%d\", reg->ref_obj_id);\n\t\t\tif (type_is_non_owning_ref(reg->type))\n\t\t\t\tverbose_a(\"%s\", \"non_own_ref\");\n\t\t\tif (t != SCALAR_VALUE)\n\t\t\t\tverbose_a(\"off=%d\", reg->off);\n\t\t\tif (type_is_pkt_pointer(t))\n\t\t\t\tverbose_a(\"r=%d\", reg->range);\n\t\t\telse if (base_type(t) == CONST_PTR_TO_MAP ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_KEY ||\n\t\t\t\t base_type(t) == PTR_TO_MAP_VALUE)\n\t\t\t\tverbose_a(\"ks=%d,vs=%d\",\n\t\t\t\t\t  reg->map_ptr->key_size,\n\t\t\t\t\t  reg->map_ptr->value_size);\n\t\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\t\t \n\t\t\t\tverbose_a(\"imm=%llx\", reg->var_off.value);\n\t\t\t} else {\n\t\t\t\tif (reg->smin_value != reg->umin_value &&\n\t\t\t\t    reg->smin_value != S64_MIN)\n\t\t\t\t\tverbose_a(\"smin=%lld\", (long long)reg->smin_value);\n\t\t\t\tif (reg->smax_value != reg->umax_value &&\n\t\t\t\t    reg->smax_value != S64_MAX)\n\t\t\t\t\tverbose_a(\"smax=%lld\", (long long)reg->smax_value);\n\t\t\t\tif (reg->umin_value != 0)\n\t\t\t\t\tverbose_a(\"umin=%llu\", (unsigned long long)reg->umin_value);\n\t\t\t\tif (reg->umax_value != U64_MAX)\n\t\t\t\t\tverbose_a(\"umax=%llu\", (unsigned long long)reg->umax_value);\n\t\t\t\tif (!tnum_is_unknown(reg->var_off)) {\n\t\t\t\t\tchar tn_buf[48];\n\n\t\t\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\t\t\tverbose_a(\"var_off=%s\", tn_buf);\n\t\t\t\t}\n\t\t\t\tif (reg->s32_min_value != reg->smin_value &&\n\t\t\t\t    reg->s32_min_value != S32_MIN)\n\t\t\t\t\tverbose_a(\"s32_min=%d\", (int)(reg->s32_min_value));\n\t\t\t\tif (reg->s32_max_value != reg->smax_value &&\n\t\t\t\t    reg->s32_max_value != S32_MAX)\n\t\t\t\t\tverbose_a(\"s32_max=%d\", (int)(reg->s32_max_value));\n\t\t\t\tif (reg->u32_min_value != reg->umin_value &&\n\t\t\t\t    reg->u32_min_value != U32_MIN)\n\t\t\t\t\tverbose_a(\"u32_min=%d\", (int)(reg->u32_min_value));\n\t\t\t\tif (reg->u32_max_value != reg->umax_value &&\n\t\t\t\t    reg->u32_max_value != U32_MAX)\n\t\t\t\t\tverbose_a(\"u32_max=%d\", (int)(reg->u32_max_value));\n\t\t\t}\n#undef verbose_a\n\n\t\t\tverbose(env, \")\");\n\t\t}\n\t}\n\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tchar types_buf[BPF_REG_SIZE + 1];\n\t\tbool valid = false;\n\t\tint j;\n\n\t\tfor (j = 0; j < BPF_REG_SIZE; j++) {\n\t\t\tif (state->stack[i].slot_type[j] != STACK_INVALID)\n\t\t\t\tvalid = true;\n\t\t\ttypes_buf[j] = slot_type_char[state->stack[i].slot_type[j]];\n\t\t}\n\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\t\tif (!valid)\n\t\t\tcontinue;\n\t\tif (!print_all && !stack_slot_scratched(env, i))\n\t\t\tcontinue;\n\t\tswitch (state->stack[i].slot_type[BPF_REG_SIZE - 1]) {\n\t\tcase STACK_SPILL:\n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tt = reg->type;\n\n\t\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\t\tprint_liveness(env, reg->live);\n\t\t\tverbose(env, \"=%s\", t == SCALAR_VALUE ? \"\" : reg_type_str(env, t));\n\t\t\tif (t == SCALAR_VALUE && reg->precise)\n\t\t\t\tverbose(env, \"P\");\n\t\t\tif (t == SCALAR_VALUE && tnum_is_const(reg->var_off))\n\t\t\t\tverbose(env, \"%lld\", reg->var_off.value + reg->off);\n\t\t\tbreak;\n\t\tcase STACK_DYNPTR:\n\t\t\ti += BPF_DYNPTR_NR_SLOTS - 1;\n\t\t\treg = &state->stack[i].spilled_ptr;\n\n\t\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\t\tprint_liveness(env, reg->live);\n\t\t\tverbose(env, \"=dynptr_%s\", dynptr_type_str(reg->dynptr.type));\n\t\t\tif (reg->ref_obj_id)\n\t\t\t\tverbose(env, \"(ref_id=%d)\", reg->ref_obj_id);\n\t\t\tbreak;\n\t\tcase STACK_ITER:\n\t\t\t \n\t\t\treg = &state->stack[i].spilled_ptr;\n\t\t\tif (!reg->ref_obj_id)\n\t\t\t\tcontinue;\n\n\t\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\t\tprint_liveness(env, reg->live);\n\t\t\tverbose(env, \"=iter_%s(ref_id=%d,state=%s,depth=%u)\",\n\t\t\t\titer_type_str(reg->iter.btf, reg->iter.btf_id),\n\t\t\t\treg->ref_obj_id, iter_state_str(reg->iter.state),\n\t\t\t\treg->iter.depth);\n\t\t\tbreak;\n\t\tcase STACK_MISC:\n\t\tcase STACK_ZERO:\n\t\tdefault:\n\t\t\treg = &state->stack[i].spilled_ptr;\n\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\ttypes_buf[j] = slot_type_char[state->stack[i].slot_type[j]];\n\t\t\ttypes_buf[BPF_REG_SIZE] = 0;\n\n\t\t\tverbose(env, \" fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\t\tprint_liveness(env, reg->live);\n\t\t\tverbose(env, \"=%s\", types_buf);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (state->acquired_refs && state->refs[0].id) {\n\t\tverbose(env, \" refs=%d\", state->refs[0].id);\n\t\tfor (i = 1; i < state->acquired_refs; i++)\n\t\t\tif (state->refs[i].id)\n\t\t\t\tverbose(env, \",%d\", state->refs[i].id);\n\t}\n\tif (state->in_callback_fn)\n\t\tverbose(env, \" cb\");\n\tif (state->in_async_callback_fn)\n\t\tverbose(env, \" async_cb\");\n\tverbose(env, \"\\n\");\n\tif (!print_all)\n\t\tmark_verifier_state_clean(env);\n}\n\nstatic inline u32 vlog_alignment(u32 pos)\n{\n\treturn round_up(max(pos + BPF_LOG_MIN_ALIGNMENT / 2, BPF_LOG_ALIGNMENT),\n\t\t\tBPF_LOG_MIN_ALIGNMENT) - pos - 1;\n}\n\nstatic void print_insn_state(struct bpf_verifier_env *env,\n\t\t\t     const struct bpf_func_state *state)\n{\n\tif (env->prev_log_pos && env->prev_log_pos == env->log.end_pos) {\n\t\t \n\t\tbpf_vlog_reset(&env->log, env->prev_log_pos - 1);\n\t\tverbose(env, \"%*c;\", vlog_alignment(env->prev_insn_print_pos), ' ');\n\t} else {\n\t\tverbose(env, \"%d:\", env->insn_idx);\n\t}\n\tprint_verifier_state(env, state, false);\n}\n\n \nstatic void *copy_array(void *dst, const void *src, size_t n, size_t size, gfp_t flags)\n{\n\tsize_t alloc_bytes;\n\tvoid *orig = dst;\n\tsize_t bytes;\n\n\tif (ZERO_OR_NULL_PTR(src))\n\t\tgoto out;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\n\talloc_bytes = max(ksize(orig), kmalloc_size_roundup(bytes));\n\tdst = krealloc(orig, alloc_bytes, flags);\n\tif (!dst) {\n\t\tkfree(orig);\n\t\treturn NULL;\n\t}\n\n\tmemcpy(dst, src, bytes);\nout:\n\treturn dst ? dst : ZERO_SIZE_PTR;\n}\n\n \nstatic void *realloc_array(void *arr, size_t old_n, size_t new_n, size_t size)\n{\n\tsize_t alloc_size;\n\tvoid *new_arr;\n\n\tif (!new_n || old_n == new_n)\n\t\tgoto out;\n\n\talloc_size = kmalloc_size_roundup(size_mul(new_n, size));\n\tnew_arr = krealloc(arr, alloc_size, GFP_KERNEL);\n\tif (!new_arr) {\n\t\tkfree(arr);\n\t\treturn NULL;\n\t}\n\tarr = new_arr;\n\n\tif (new_n > old_n)\n\t\tmemset(arr + old_n * size, 0, (new_n - old_n) * size);\n\nout:\n\treturn arr ? arr : ZERO_SIZE_PTR;\n}\n\nstatic int copy_reference_state(struct bpf_func_state *dst, const struct bpf_func_state *src)\n{\n\tdst->refs = copy_array(dst->refs, src->refs, src->acquired_refs,\n\t\t\t       sizeof(struct bpf_reference_state), GFP_KERNEL);\n\tif (!dst->refs)\n\t\treturn -ENOMEM;\n\n\tdst->acquired_refs = src->acquired_refs;\n\treturn 0;\n}\n\nstatic int copy_stack_state(struct bpf_func_state *dst, const struct bpf_func_state *src)\n{\n\tsize_t n = src->allocated_stack / BPF_REG_SIZE;\n\n\tdst->stack = copy_array(dst->stack, src->stack, n, sizeof(struct bpf_stack_state),\n\t\t\t\tGFP_KERNEL);\n\tif (!dst->stack)\n\t\treturn -ENOMEM;\n\n\tdst->allocated_stack = src->allocated_stack;\n\treturn 0;\n}\n\nstatic int resize_reference_state(struct bpf_func_state *state, size_t n)\n{\n\tstate->refs = realloc_array(state->refs, state->acquired_refs, n,\n\t\t\t\t    sizeof(struct bpf_reference_state));\n\tif (!state->refs)\n\t\treturn -ENOMEM;\n\n\tstate->acquired_refs = n;\n\treturn 0;\n}\n\n \nstatic int grow_stack_state(struct bpf_verifier_env *env, struct bpf_func_state *state, int size)\n{\n\tsize_t old_n = state->allocated_stack / BPF_REG_SIZE, n = size / BPF_REG_SIZE;\n\n\tif (old_n >= n)\n\t\treturn 0;\n\n\tstate->stack = realloc_array(state->stack, old_n, n, sizeof(struct bpf_stack_state));\n\tif (!state->stack)\n\t\treturn -ENOMEM;\n\n\tstate->allocated_stack = size;\n\n\t \n\tif (env->subprog_info[state->subprogno].stack_depth < size)\n\t\tenv->subprog_info[state->subprogno].stack_depth = size;\n\n\treturn 0;\n}\n\n \nstatic int acquire_reference_state(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tint new_ofs = state->acquired_refs;\n\tint id, err;\n\n\terr = resize_reference_state(state, state->acquired_refs + 1);\n\tif (err)\n\t\treturn err;\n\tid = ++env->id_gen;\n\tstate->refs[new_ofs].id = id;\n\tstate->refs[new_ofs].insn_idx = insn_idx;\n\tstate->refs[new_ofs].callback_ref = state->in_callback_fn ? state->frameno : 0;\n\n\treturn id;\n}\n\n \nstatic int release_reference_state(struct bpf_func_state *state, int ptr_id)\n{\n\tint i, last_idx;\n\n\tlast_idx = state->acquired_refs - 1;\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->refs[i].id == ptr_id) {\n\t\t\t \n\t\t\tif (state->in_callback_fn && state->refs[i].callback_ref != state->frameno)\n\t\t\t\treturn -EINVAL;\n\t\t\tif (last_idx && i != last_idx)\n\t\t\t\tmemcpy(&state->refs[i], &state->refs[last_idx],\n\t\t\t\t       sizeof(*state->refs));\n\t\t\tmemset(&state->refs[last_idx], 0, sizeof(*state->refs));\n\t\t\tstate->acquired_refs--;\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic void free_func_state(struct bpf_func_state *state)\n{\n\tif (!state)\n\t\treturn;\n\tkfree(state->refs);\n\tkfree(state->stack);\n\tkfree(state);\n}\n\nstatic void clear_jmp_history(struct bpf_verifier_state *state)\n{\n\tkfree(state->jmp_history);\n\tstate->jmp_history = NULL;\n\tstate->jmp_history_cnt = 0;\n}\n\nstatic void free_verifier_state(struct bpf_verifier_state *state,\n\t\t\t\tbool free_self)\n{\n\tint i;\n\n\tfor (i = 0; i <= state->curframe; i++) {\n\t\tfree_func_state(state->frame[i]);\n\t\tstate->frame[i] = NULL;\n\t}\n\tclear_jmp_history(state);\n\tif (free_self)\n\t\tkfree(state);\n}\n\n \nstatic int copy_func_state(struct bpf_func_state *dst,\n\t\t\t   const struct bpf_func_state *src)\n{\n\tint err;\n\n\tmemcpy(dst, src, offsetof(struct bpf_func_state, acquired_refs));\n\terr = copy_reference_state(dst, src);\n\tif (err)\n\t\treturn err;\n\treturn copy_stack_state(dst, src);\n}\n\nstatic int copy_verifier_state(struct bpf_verifier_state *dst_state,\n\t\t\t       const struct bpf_verifier_state *src)\n{\n\tstruct bpf_func_state *dst;\n\tint i, err;\n\n\tdst_state->jmp_history = copy_array(dst_state->jmp_history, src->jmp_history,\n\t\t\t\t\t    src->jmp_history_cnt, sizeof(struct bpf_idx_pair),\n\t\t\t\t\t    GFP_USER);\n\tif (!dst_state->jmp_history)\n\t\treturn -ENOMEM;\n\tdst_state->jmp_history_cnt = src->jmp_history_cnt;\n\n\t \n\tfor (i = src->curframe + 1; i <= dst_state->curframe; i++) {\n\t\tfree_func_state(dst_state->frame[i]);\n\t\tdst_state->frame[i] = NULL;\n\t}\n\tdst_state->speculative = src->speculative;\n\tdst_state->active_rcu_lock = src->active_rcu_lock;\n\tdst_state->curframe = src->curframe;\n\tdst_state->active_lock.ptr = src->active_lock.ptr;\n\tdst_state->active_lock.id = src->active_lock.id;\n\tdst_state->branches = src->branches;\n\tdst_state->parent = src->parent;\n\tdst_state->first_insn_idx = src->first_insn_idx;\n\tdst_state->last_insn_idx = src->last_insn_idx;\n\tfor (i = 0; i <= src->curframe; i++) {\n\t\tdst = dst_state->frame[i];\n\t\tif (!dst) {\n\t\t\tdst = kzalloc(sizeof(*dst), GFP_KERNEL);\n\t\t\tif (!dst)\n\t\t\t\treturn -ENOMEM;\n\t\t\tdst_state->frame[i] = dst;\n\t\t}\n\t\terr = copy_func_state(dst, src->frame[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic void update_branch_counts(struct bpf_verifier_env *env, struct bpf_verifier_state *st)\n{\n\twhile (st) {\n\t\tu32 br = --st->branches;\n\n\t\t \n\t\tWARN_ONCE((int)br < 0,\n\t\t\t  \"BUG update_branch_counts:branches_to_explore=%d\\n\",\n\t\t\t  br);\n\t\tif (br)\n\t\t\tbreak;\n\t\tst = st->parent;\n\t}\n}\n\nstatic int pop_stack(struct bpf_verifier_env *env, int *prev_insn_idx,\n\t\t     int *insn_idx, bool pop_log)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem, *head = env->head;\n\tint err;\n\n\tif (env->head == NULL)\n\t\treturn -ENOENT;\n\n\tif (cur) {\n\t\terr = copy_verifier_state(cur, &head->st);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tif (pop_log)\n\t\tbpf_vlog_reset(&env->log, head->log_pos);\n\tif (insn_idx)\n\t\t*insn_idx = head->insn_idx;\n\tif (prev_insn_idx)\n\t\t*prev_insn_idx = head->prev_insn_idx;\n\telem = head->next;\n\tfree_verifier_state(&head->st, false);\n\tkfree(head);\n\tenv->head = elem;\n\tenv->stack_size--;\n\treturn 0;\n}\n\nstatic struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,\n\t\t\t\t\t     int insn_idx, int prev_insn_idx,\n\t\t\t\t\t     bool speculative)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_verifier_stack_elem *elem;\n\tint err;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\telem->log_pos = env->log.end_pos;\n\tenv->head = elem;\n\tenv->stack_size++;\n\terr = copy_verifier_state(&elem->st, cur);\n\tif (err)\n\t\tgoto err;\n\telem->st.speculative |= speculative;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_JMP_SEQ) {\n\t\tverbose(env, \"The sequence of %d jumps is too complex.\\n\",\n\t\t\tenv->stack_size);\n\t\tgoto err;\n\t}\n\tif (elem->st.parent) {\n\t\t++elem->st.parent->branches;\n\t\t \n\t}\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t \n\twhile (!pop_stack(env, NULL, NULL, false));\n\treturn NULL;\n}\n\n#define CALLER_SAVED_REGS 6\nstatic const int caller_saved[CALLER_SAVED_REGS] = {\n\tBPF_REG_0, BPF_REG_1, BPF_REG_2, BPF_REG_3, BPF_REG_4, BPF_REG_5\n};\n\n \nstatic void ___mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\treg->var_off = tnum_const(imm);\n\treg->smin_value = (s64)imm;\n\treg->smax_value = (s64)imm;\n\treg->umin_value = imm;\n\treg->umax_value = imm;\n\n\treg->s32_min_value = (s32)imm;\n\treg->s32_max_value = (s32)imm;\n\treg->u32_min_value = (u32)imm;\n\treg->u32_max_value = (u32)imm;\n}\n\n \nstatic void __mark_reg_known(struct bpf_reg_state *reg, u64 imm)\n{\n\t \n\tmemset(((u8 *)reg) + sizeof(reg->type), 0,\n\t       offsetof(struct bpf_reg_state, var_off) - sizeof(reg->type));\n\treg->id = 0;\n\treg->ref_obj_id = 0;\n\t___mark_reg_known(reg, imm);\n}\n\nstatic void __mark_reg32_known(struct bpf_reg_state *reg, u64 imm)\n{\n\treg->var_off = tnum_const_subreg(reg->var_off, imm);\n\treg->s32_min_value = (s32)imm;\n\treg->s32_max_value = (s32)imm;\n\treg->u32_min_value = (u32)imm;\n\treg->u32_max_value = (u32)imm;\n}\n\n \nstatic void __mark_reg_known_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n}\n\nstatic void __mark_reg_const_zero(struct bpf_reg_state *reg)\n{\n\t__mark_reg_known(reg, 0);\n\treg->type = SCALAR_VALUE;\n}\n\nstatic void mark_reg_known_zero(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_known_zero(regs, %u)\\n\", regno);\n\t\t \n\t\tfor (regno = 0; regno < MAX_BPF_REG; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_known_zero(regs + regno);\n}\n\nstatic void __mark_dynptr_reg(struct bpf_reg_state *reg, enum bpf_dynptr_type type,\n\t\t\t      bool first_slot, int dynptr_id)\n{\n\t \n\t__mark_reg_known_zero(reg);\n\treg->type = CONST_PTR_TO_DYNPTR;\n\t \n\treg->id = dynptr_id;\n\treg->dynptr.type = type;\n\treg->dynptr.first_slot = first_slot;\n}\n\nstatic void mark_ptr_not_null_reg(struct bpf_reg_state *reg)\n{\n\tif (base_type(reg->type) == PTR_TO_MAP_VALUE) {\n\t\tconst struct bpf_map *map = reg->map_ptr;\n\n\t\tif (map->inner_map_meta) {\n\t\t\treg->type = CONST_PTR_TO_MAP;\n\t\t\treg->map_ptr = map->inner_map_meta;\n\t\t\t \n\t\t\tif (btf_record_has_field(map->inner_map_meta->record, BPF_TIMER))\n\t\t\t\treg->map_uid = reg->id;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_XSKMAP) {\n\t\t\treg->type = PTR_TO_XDP_SOCK;\n\t\t} else if (map->map_type == BPF_MAP_TYPE_SOCKMAP ||\n\t\t\t   map->map_type == BPF_MAP_TYPE_SOCKHASH) {\n\t\t\treg->type = PTR_TO_SOCKET;\n\t\t} else {\n\t\t\treg->type = PTR_TO_MAP_VALUE;\n\t\t}\n\t\treturn;\n\t}\n\n\treg->type &= ~PTR_MAYBE_NULL;\n}\n\nstatic void mark_reg_graph_node(struct bpf_reg_state *regs, u32 regno,\n\t\t\t\tstruct btf_field_graph_root *ds_head)\n{\n\t__mark_reg_known_zero(&regs[regno]);\n\tregs[regno].type = PTR_TO_BTF_ID | MEM_ALLOC;\n\tregs[regno].btf = ds_head->btf;\n\tregs[regno].btf_id = ds_head->value_btf_id;\n\tregs[regno].off = ds_head->node_offset;\n}\n\nstatic bool reg_is_pkt_pointer(const struct bpf_reg_state *reg)\n{\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool reg_is_pkt_pointer_any(const struct bpf_reg_state *reg)\n{\n\treturn reg_is_pkt_pointer(reg) ||\n\t       reg->type == PTR_TO_PACKET_END;\n}\n\nstatic bool reg_is_dynptr_slice_pkt(const struct bpf_reg_state *reg)\n{\n\treturn base_type(reg->type) == PTR_TO_MEM &&\n\t\t(reg->type & DYNPTR_TYPE_SKB || reg->type & DYNPTR_TYPE_XDP);\n}\n\n \nstatic bool reg_is_init_pkt_pointer(const struct bpf_reg_state *reg,\n\t\t\t\t    enum bpf_reg_type which)\n{\n\t \n\treturn reg->type == which &&\n\t       reg->id == 0 &&\n\t       reg->off == 0 &&\n\t       tnum_equals_const(reg->var_off, 0);\n}\n\n \nstatic void __mark_reg_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n\n\treg->s32_min_value = S32_MIN;\n\treg->s32_max_value = S32_MAX;\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void __mark_reg64_unbounded(struct bpf_reg_state *reg)\n{\n\treg->smin_value = S64_MIN;\n\treg->smax_value = S64_MAX;\n\treg->umin_value = 0;\n\treg->umax_value = U64_MAX;\n}\n\nstatic void __mark_reg32_unbounded(struct bpf_reg_state *reg)\n{\n\treg->s32_min_value = S32_MIN;\n\treg->s32_max_value = S32_MAX;\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void __update_reg32_bounds(struct bpf_reg_state *reg)\n{\n\tstruct tnum var32_off = tnum_subreg(reg->var_off);\n\n\t \n\treg->s32_min_value = max_t(s32, reg->s32_min_value,\n\t\t\tvar32_off.value | (var32_off.mask & S32_MIN));\n\t \n\treg->s32_max_value = min_t(s32, reg->s32_max_value,\n\t\t\tvar32_off.value | (var32_off.mask & S32_MAX));\n\treg->u32_min_value = max_t(u32, reg->u32_min_value, (u32)var32_off.value);\n\treg->u32_max_value = min(reg->u32_max_value,\n\t\t\t\t (u32)(var32_off.value | var32_off.mask));\n}\n\nstatic void __update_reg64_bounds(struct bpf_reg_state *reg)\n{\n\t \n\treg->smin_value = max_t(s64, reg->smin_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MIN));\n\t \n\treg->smax_value = min_t(s64, reg->smax_value,\n\t\t\t\treg->var_off.value | (reg->var_off.mask & S64_MAX));\n\treg->umin_value = max(reg->umin_value, reg->var_off.value);\n\treg->umax_value = min(reg->umax_value,\n\t\t\t      reg->var_off.value | reg->var_off.mask);\n}\n\nstatic void __update_reg_bounds(struct bpf_reg_state *reg)\n{\n\t__update_reg32_bounds(reg);\n\t__update_reg64_bounds(reg);\n}\n\n \nstatic void __reg32_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t \n\tif (reg->s32_min_value >= 0 || reg->s32_max_value < 0) {\n\t\treg->s32_min_value = reg->u32_min_value =\n\t\t\tmax_t(u32, reg->s32_min_value, reg->u32_min_value);\n\t\treg->s32_max_value = reg->u32_max_value =\n\t\t\tmin_t(u32, reg->s32_max_value, reg->u32_max_value);\n\t\treturn;\n\t}\n\t \n\tif ((s32)reg->u32_max_value >= 0) {\n\t\t \n\t\treg->s32_min_value = reg->u32_min_value;\n\t\treg->s32_max_value = reg->u32_max_value =\n\t\t\tmin_t(u32, reg->s32_max_value, reg->u32_max_value);\n\t} else if ((s32)reg->u32_min_value < 0) {\n\t\t \n\t\treg->s32_min_value = reg->u32_min_value =\n\t\t\tmax_t(u32, reg->s32_min_value, reg->u32_min_value);\n\t\treg->s32_max_value = reg->u32_max_value;\n\t}\n}\n\nstatic void __reg64_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t \n\tif (reg->smin_value >= 0 || reg->smax_value < 0) {\n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t\treturn;\n\t}\n\t \n\tif ((s64)reg->umax_value >= 0) {\n\t\t \n\t\treg->smin_value = reg->umin_value;\n\t\treg->smax_value = reg->umax_value = min_t(u64, reg->smax_value,\n\t\t\t\t\t\t\t  reg->umax_value);\n\t} else if ((s64)reg->umin_value < 0) {\n\t\t \n\t\treg->smin_value = reg->umin_value = max_t(u64, reg->smin_value,\n\t\t\t\t\t\t\t  reg->umin_value);\n\t\treg->smax_value = reg->umax_value;\n\t}\n}\n\nstatic void __reg_deduce_bounds(struct bpf_reg_state *reg)\n{\n\t__reg32_deduce_bounds(reg);\n\t__reg64_deduce_bounds(reg);\n}\n\n \nstatic void __reg_bound_offset(struct bpf_reg_state *reg)\n{\n\tstruct tnum var64_off = tnum_intersect(reg->var_off,\n\t\t\t\t\t       tnum_range(reg->umin_value,\n\t\t\t\t\t\t\t  reg->umax_value));\n\tstruct tnum var32_off = tnum_intersect(tnum_subreg(var64_off),\n\t\t\t\t\t       tnum_range(reg->u32_min_value,\n\t\t\t\t\t\t\t  reg->u32_max_value));\n\n\treg->var_off = tnum_or(tnum_clear_subreg(var64_off), var32_off);\n}\n\nstatic void reg_bounds_sync(struct bpf_reg_state *reg)\n{\n\t \n\t__update_reg_bounds(reg);\n\t \n\t__reg_deduce_bounds(reg);\n\t \n\t__reg_bound_offset(reg);\n\t \n\t__update_reg_bounds(reg);\n}\n\nstatic bool __reg32_bound_s64(s32 a)\n{\n\treturn a >= 0 && a <= S32_MAX;\n}\n\nstatic void __reg_assign_32_into_64(struct bpf_reg_state *reg)\n{\n\treg->umin_value = reg->u32_min_value;\n\treg->umax_value = reg->u32_max_value;\n\n\t \n\tif (__reg32_bound_s64(reg->s32_min_value) &&\n\t    __reg32_bound_s64(reg->s32_max_value)) {\n\t\treg->smin_value = reg->s32_min_value;\n\t\treg->smax_value = reg->s32_max_value;\n\t} else {\n\t\treg->smin_value = 0;\n\t\treg->smax_value = U32_MAX;\n\t}\n}\n\nstatic void __reg_combine_32_into_64(struct bpf_reg_state *reg)\n{\n\t \n\tif (tnum_equals_const(tnum_clear_subreg(reg->var_off), 0)) {\n\t\t__reg_assign_32_into_64(reg);\n\t} else {\n\t\t \n\t\t__mark_reg64_unbounded(reg);\n\t}\n\treg_bounds_sync(reg);\n}\n\nstatic bool __reg64_bound_s32(s64 a)\n{\n\treturn a >= S32_MIN && a <= S32_MAX;\n}\n\nstatic bool __reg64_bound_u32(u64 a)\n{\n\treturn a >= U32_MIN && a <= U32_MAX;\n}\n\nstatic void __reg_combine_64_into_32(struct bpf_reg_state *reg)\n{\n\t__mark_reg32_unbounded(reg);\n\tif (__reg64_bound_s32(reg->smin_value) && __reg64_bound_s32(reg->smax_value)) {\n\t\treg->s32_min_value = (s32)reg->smin_value;\n\t\treg->s32_max_value = (s32)reg->smax_value;\n\t}\n\tif (__reg64_bound_u32(reg->umin_value) && __reg64_bound_u32(reg->umax_value)) {\n\t\treg->u32_min_value = (u32)reg->umin_value;\n\t\treg->u32_max_value = (u32)reg->umax_value;\n\t}\n\treg_bounds_sync(reg);\n}\n\n \nstatic void __mark_reg_unknown(const struct bpf_verifier_env *env,\n\t\t\t       struct bpf_reg_state *reg)\n{\n\t \n\tmemset(reg, 0, offsetof(struct bpf_reg_state, var_off));\n\treg->type = SCALAR_VALUE;\n\treg->id = 0;\n\treg->ref_obj_id = 0;\n\treg->var_off = tnum_unknown;\n\treg->frameno = 0;\n\treg->precise = !env->bpf_capable;\n\t__mark_reg_unbounded(reg);\n}\n\nstatic void mark_reg_unknown(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_unknown(regs, %u)\\n\", regno);\n\t\t \n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_unknown(env, regs + regno);\n}\n\nstatic void __mark_reg_not_init(const struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg)\n{\n\t__mark_reg_unknown(env, reg);\n\treg->type = NOT_INIT;\n}\n\nstatic void mark_reg_not_init(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *regs, u32 regno)\n{\n\tif (WARN_ON(regno >= MAX_BPF_REG)) {\n\t\tverbose(env, \"mark_reg_not_init(regs, %u)\\n\", regno);\n\t\t \n\t\tfor (regno = 0; regno < BPF_REG_FP; regno++)\n\t\t\t__mark_reg_not_init(env, regs + regno);\n\t\treturn;\n\t}\n\t__mark_reg_not_init(env, regs + regno);\n}\n\nstatic void mark_btf_ld_reg(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_reg_state *regs, u32 regno,\n\t\t\t    enum bpf_reg_type reg_type,\n\t\t\t    struct btf *btf, u32 btf_id,\n\t\t\t    enum bpf_type_flag flag)\n{\n\tif (reg_type == SCALAR_VALUE) {\n\t\tmark_reg_unknown(env, regs, regno);\n\t\treturn;\n\t}\n\tmark_reg_known_zero(env, regs, regno);\n\tregs[regno].type = PTR_TO_BTF_ID | flag;\n\tregs[regno].btf = btf;\n\tregs[regno].btf_id = btf_id;\n}\n\n#define DEF_NOT_SUBREG\t(0)\nstatic void init_reg_state(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_func_state *state)\n{\n\tstruct bpf_reg_state *regs = state->regs;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++) {\n\t\tmark_reg_not_init(env, regs, i);\n\t\tregs[i].live = REG_LIVE_NONE;\n\t\tregs[i].parent = NULL;\n\t\tregs[i].subreg_def = DEF_NOT_SUBREG;\n\t}\n\n\t \n\tregs[BPF_REG_FP].type = PTR_TO_STACK;\n\tmark_reg_known_zero(env, regs, BPF_REG_FP);\n\tregs[BPF_REG_FP].frameno = state->frameno;\n}\n\n#define BPF_MAIN_FUNC (-1)\nstatic void init_func_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *state,\n\t\t\t    int callsite, int frameno, int subprogno)\n{\n\tstate->callsite = callsite;\n\tstate->frameno = frameno;\n\tstate->subprogno = subprogno;\n\tstate->callback_ret_range = tnum_range(0, 0);\n\tinit_reg_state(env, state);\n\tmark_verifier_state_scratched(env);\n}\n\n \nstatic struct bpf_verifier_state *push_async_cb(struct bpf_verifier_env *env,\n\t\t\t\t\t\tint insn_idx, int prev_insn_idx,\n\t\t\t\t\t\tint subprog)\n{\n\tstruct bpf_verifier_stack_elem *elem;\n\tstruct bpf_func_state *frame;\n\n\telem = kzalloc(sizeof(struct bpf_verifier_stack_elem), GFP_KERNEL);\n\tif (!elem)\n\t\tgoto err;\n\n\telem->insn_idx = insn_idx;\n\telem->prev_insn_idx = prev_insn_idx;\n\telem->next = env->head;\n\telem->log_pos = env->log.end_pos;\n\tenv->head = elem;\n\tenv->stack_size++;\n\tif (env->stack_size > BPF_COMPLEXITY_LIMIT_JMP_SEQ) {\n\t\tverbose(env,\n\t\t\t\"The sequence of %d jumps is too complex for async cb.\\n\",\n\t\t\tenv->stack_size);\n\t\tgoto err;\n\t}\n\t \n\telem->st.branches = 1;\n\tframe = kzalloc(sizeof(*frame), GFP_KERNEL);\n\tif (!frame)\n\t\tgoto err;\n\tinit_func_state(env, frame,\n\t\t\tBPF_MAIN_FUNC  ,\n\t\t\t0  ,\n\t\t\tsubprog  );\n\telem->st.frame[0] = frame;\n\treturn &elem->st;\nerr:\n\tfree_verifier_state(env->cur_state, true);\n\tenv->cur_state = NULL;\n\t \n\twhile (!pop_stack(env, NULL, NULL, false));\n\treturn NULL;\n}\n\n\nenum reg_arg_type {\n\tSRC_OP,\t\t \n\tDST_OP,\t\t \n\tDST_OP_NO_MARK\t \n};\n\nstatic int cmp_subprogs(const void *a, const void *b)\n{\n\treturn ((struct bpf_subprog_info *)a)->start -\n\t       ((struct bpf_subprog_info *)b)->start;\n}\n\nstatic int find_subprog(struct bpf_verifier_env *env, int off)\n{\n\tstruct bpf_subprog_info *p;\n\n\tp = bsearch(&off, env->subprog_info, env->subprog_cnt,\n\t\t    sizeof(env->subprog_info[0]), cmp_subprogs);\n\tif (!p)\n\t\treturn -ENOENT;\n\treturn p - env->subprog_info;\n\n}\n\nstatic int add_subprog(struct bpf_verifier_env *env, int off)\n{\n\tint insn_cnt = env->prog->len;\n\tint ret;\n\n\tif (off >= insn_cnt || off < 0) {\n\t\tverbose(env, \"call to invalid destination\\n\");\n\t\treturn -EINVAL;\n\t}\n\tret = find_subprog(env, off);\n\tif (ret >= 0)\n\t\treturn ret;\n\tif (env->subprog_cnt >= BPF_MAX_SUBPROGS) {\n\t\tverbose(env, \"too many subprograms\\n\");\n\t\treturn -E2BIG;\n\t}\n\t \n\tenv->subprog_info[env->subprog_cnt++].start = off;\n\tsort(env->subprog_info, env->subprog_cnt,\n\t     sizeof(env->subprog_info[0]), cmp_subprogs, NULL);\n\treturn env->subprog_cnt - 1;\n}\n\n#define MAX_KFUNC_DESCS 256\n#define MAX_KFUNC_BTFS\t256\n\nstruct bpf_kfunc_desc {\n\tstruct btf_func_model func_model;\n\tu32 func_id;\n\ts32 imm;\n\tu16 offset;\n\tunsigned long addr;\n};\n\nstruct bpf_kfunc_btf {\n\tstruct btf *btf;\n\tstruct module *module;\n\tu16 offset;\n};\n\nstruct bpf_kfunc_desc_tab {\n\t \n\tstruct bpf_kfunc_desc descs[MAX_KFUNC_DESCS];\n\tu32 nr_descs;\n};\n\nstruct bpf_kfunc_btf_tab {\n\tstruct bpf_kfunc_btf descs[MAX_KFUNC_BTFS];\n\tu32 nr_descs;\n};\n\nstatic int kfunc_desc_cmp_by_id_off(const void *a, const void *b)\n{\n\tconst struct bpf_kfunc_desc *d0 = a;\n\tconst struct bpf_kfunc_desc *d1 = b;\n\n\t \n\treturn d0->func_id - d1->func_id ?: d0->offset - d1->offset;\n}\n\nstatic int kfunc_btf_cmp_by_off(const void *a, const void *b)\n{\n\tconst struct bpf_kfunc_btf *d0 = a;\n\tconst struct bpf_kfunc_btf *d1 = b;\n\n\treturn d0->offset - d1->offset;\n}\n\nstatic const struct bpf_kfunc_desc *\nfind_kfunc_desc(const struct bpf_prog *prog, u32 func_id, u16 offset)\n{\n\tstruct bpf_kfunc_desc desc = {\n\t\t.func_id = func_id,\n\t\t.offset = offset,\n\t};\n\tstruct bpf_kfunc_desc_tab *tab;\n\n\ttab = prog->aux->kfunc_tab;\n\treturn bsearch(&desc, tab->descs, tab->nr_descs,\n\t\t       sizeof(tab->descs[0]), kfunc_desc_cmp_by_id_off);\n}\n\nint bpf_get_kfunc_addr(const struct bpf_prog *prog, u32 func_id,\n\t\t       u16 btf_fd_idx, u8 **func_addr)\n{\n\tconst struct bpf_kfunc_desc *desc;\n\n\tdesc = find_kfunc_desc(prog, func_id, btf_fd_idx);\n\tif (!desc)\n\t\treturn -EFAULT;\n\n\t*func_addr = (u8 *)desc->addr;\n\treturn 0;\n}\n\nstatic struct btf *__find_kfunc_desc_btf(struct bpf_verifier_env *env,\n\t\t\t\t\t s16 offset)\n{\n\tstruct bpf_kfunc_btf kf_btf = { .offset = offset };\n\tstruct bpf_kfunc_btf_tab *tab;\n\tstruct bpf_kfunc_btf *b;\n\tstruct module *mod;\n\tstruct btf *btf;\n\tint btf_fd;\n\n\ttab = env->prog->aux->kfunc_btf_tab;\n\tb = bsearch(&kf_btf, tab->descs, tab->nr_descs,\n\t\t    sizeof(tab->descs[0]), kfunc_btf_cmp_by_off);\n\tif (!b) {\n\t\tif (tab->nr_descs == MAX_KFUNC_BTFS) {\n\t\t\tverbose(env, \"too many different module BTFs\\n\");\n\t\t\treturn ERR_PTR(-E2BIG);\n\t\t}\n\n\t\tif (bpfptr_is_null(env->fd_array)) {\n\t\t\tverbose(env, \"kfunc offset > 0 without fd_array is invalid\\n\");\n\t\t\treturn ERR_PTR(-EPROTO);\n\t\t}\n\n\t\tif (copy_from_bpfptr_offset(&btf_fd, env->fd_array,\n\t\t\t\t\t    offset * sizeof(btf_fd),\n\t\t\t\t\t    sizeof(btf_fd)))\n\t\t\treturn ERR_PTR(-EFAULT);\n\n\t\tbtf = btf_get_by_fd(btf_fd);\n\t\tif (IS_ERR(btf)) {\n\t\t\tverbose(env, \"invalid module BTF fd specified\\n\");\n\t\t\treturn btf;\n\t\t}\n\n\t\tif (!btf_is_module(btf)) {\n\t\t\tverbose(env, \"BTF fd for kfunc is not a module BTF\\n\");\n\t\t\tbtf_put(btf);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tmod = btf_try_get_module(btf);\n\t\tif (!mod) {\n\t\t\tbtf_put(btf);\n\t\t\treturn ERR_PTR(-ENXIO);\n\t\t}\n\n\t\tb = &tab->descs[tab->nr_descs++];\n\t\tb->btf = btf;\n\t\tb->module = mod;\n\t\tb->offset = offset;\n\n\t\tsort(tab->descs, tab->nr_descs, sizeof(tab->descs[0]),\n\t\t     kfunc_btf_cmp_by_off, NULL);\n\t}\n\treturn b->btf;\n}\n\nvoid bpf_free_kfunc_btf_tab(struct bpf_kfunc_btf_tab *tab)\n{\n\tif (!tab)\n\t\treturn;\n\n\twhile (tab->nr_descs--) {\n\t\tmodule_put(tab->descs[tab->nr_descs].module);\n\t\tbtf_put(tab->descs[tab->nr_descs].btf);\n\t}\n\tkfree(tab);\n}\n\nstatic struct btf *find_kfunc_desc_btf(struct bpf_verifier_env *env, s16 offset)\n{\n\tif (offset) {\n\t\tif (offset < 0) {\n\t\t\t \n\t\t\tverbose(env, \"negative offset disallowed for kernel module function call\\n\");\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\treturn __find_kfunc_desc_btf(env, offset);\n\t}\n\treturn btf_vmlinux ?: ERR_PTR(-ENOENT);\n}\n\nstatic int add_kfunc_call(struct bpf_verifier_env *env, u32 func_id, s16 offset)\n{\n\tconst struct btf_type *func, *func_proto;\n\tstruct bpf_kfunc_btf_tab *btf_tab;\n\tstruct bpf_kfunc_desc_tab *tab;\n\tstruct bpf_prog_aux *prog_aux;\n\tstruct bpf_kfunc_desc *desc;\n\tconst char *func_name;\n\tstruct btf *desc_btf;\n\tunsigned long call_imm;\n\tunsigned long addr;\n\tint err;\n\n\tprog_aux = env->prog->aux;\n\ttab = prog_aux->kfunc_tab;\n\tbtf_tab = prog_aux->kfunc_btf_tab;\n\tif (!tab) {\n\t\tif (!btf_vmlinux) {\n\t\t\tverbose(env, \"calling kernel function is not supported without CONFIG_DEBUG_INFO_BTF\\n\");\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\n\t\tif (!env->prog->jit_requested) {\n\t\t\tverbose(env, \"JIT is required for calling kernel function\\n\");\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\n\t\tif (!bpf_jit_supports_kfunc_call()) {\n\t\t\tverbose(env, \"JIT does not support calling kernel function\\n\");\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\n\t\tif (!env->prog->gpl_compatible) {\n\t\t\tverbose(env, \"cannot call kernel function from non-GPL compatible program\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\ttab = kzalloc(sizeof(*tab), GFP_KERNEL);\n\t\tif (!tab)\n\t\t\treturn -ENOMEM;\n\t\tprog_aux->kfunc_tab = tab;\n\t}\n\n\t \n\tif (!func_id && !offset)\n\t\treturn 0;\n\n\tif (!btf_tab && offset) {\n\t\tbtf_tab = kzalloc(sizeof(*btf_tab), GFP_KERNEL);\n\t\tif (!btf_tab)\n\t\t\treturn -ENOMEM;\n\t\tprog_aux->kfunc_btf_tab = btf_tab;\n\t}\n\n\tdesc_btf = find_kfunc_desc_btf(env, offset);\n\tif (IS_ERR(desc_btf)) {\n\t\tverbose(env, \"failed to find BTF for kernel function\\n\");\n\t\treturn PTR_ERR(desc_btf);\n\t}\n\n\tif (find_kfunc_desc(env->prog, func_id, offset))\n\t\treturn 0;\n\n\tif (tab->nr_descs == MAX_KFUNC_DESCS) {\n\t\tverbose(env, \"too many different kernel function calls\\n\");\n\t\treturn -E2BIG;\n\t}\n\n\tfunc = btf_type_by_id(desc_btf, func_id);\n\tif (!func || !btf_type_is_func(func)) {\n\t\tverbose(env, \"kernel btf_id %u is not a function\\n\",\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\tfunc_proto = btf_type_by_id(desc_btf, func->type);\n\tif (!func_proto || !btf_type_is_func_proto(func_proto)) {\n\t\tverbose(env, \"kernel function btf_id %u does not have a valid func_proto\\n\",\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tfunc_name = btf_name_by_offset(desc_btf, func->name_off);\n\taddr = kallsyms_lookup_name(func_name);\n\tif (!addr) {\n\t\tverbose(env, \"cannot find address for kernel function %s\\n\",\n\t\t\tfunc_name);\n\t\treturn -EINVAL;\n\t}\n\tspecialize_kfunc(env, func_id, offset, &addr);\n\n\tif (bpf_jit_supports_far_kfunc_call()) {\n\t\tcall_imm = func_id;\n\t} else {\n\t\tcall_imm = BPF_CALL_IMM(addr);\n\t\t \n\t\tif ((unsigned long)(s32)call_imm != call_imm) {\n\t\t\tverbose(env, \"address of kernel function %s is out of range\\n\",\n\t\t\t\tfunc_name);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (bpf_dev_bound_kfunc_id(func_id)) {\n\t\terr = bpf_dev_bound_kfunc_check(&env->log, prog_aux);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tdesc = &tab->descs[tab->nr_descs++];\n\tdesc->func_id = func_id;\n\tdesc->imm = call_imm;\n\tdesc->offset = offset;\n\tdesc->addr = addr;\n\terr = btf_distill_func_proto(&env->log, desc_btf,\n\t\t\t\t     func_proto, func_name,\n\t\t\t\t     &desc->func_model);\n\tif (!err)\n\t\tsort(tab->descs, tab->nr_descs, sizeof(tab->descs[0]),\n\t\t     kfunc_desc_cmp_by_id_off, NULL);\n\treturn err;\n}\n\nstatic int kfunc_desc_cmp_by_imm_off(const void *a, const void *b)\n{\n\tconst struct bpf_kfunc_desc *d0 = a;\n\tconst struct bpf_kfunc_desc *d1 = b;\n\n\tif (d0->imm != d1->imm)\n\t\treturn d0->imm < d1->imm ? -1 : 1;\n\tif (d0->offset != d1->offset)\n\t\treturn d0->offset < d1->offset ? -1 : 1;\n\treturn 0;\n}\n\nstatic void sort_kfunc_descs_by_imm_off(struct bpf_prog *prog)\n{\n\tstruct bpf_kfunc_desc_tab *tab;\n\n\ttab = prog->aux->kfunc_tab;\n\tif (!tab)\n\t\treturn;\n\n\tsort(tab->descs, tab->nr_descs, sizeof(tab->descs[0]),\n\t     kfunc_desc_cmp_by_imm_off, NULL);\n}\n\nbool bpf_prog_has_kfunc_call(const struct bpf_prog *prog)\n{\n\treturn !!prog->aux->kfunc_tab;\n}\n\nconst struct btf_func_model *\nbpf_jit_find_kfunc_model(const struct bpf_prog *prog,\n\t\t\t const struct bpf_insn *insn)\n{\n\tconst struct bpf_kfunc_desc desc = {\n\t\t.imm = insn->imm,\n\t\t.offset = insn->off,\n\t};\n\tconst struct bpf_kfunc_desc *res;\n\tstruct bpf_kfunc_desc_tab *tab;\n\n\ttab = prog->aux->kfunc_tab;\n\tres = bsearch(&desc, tab->descs, tab->nr_descs,\n\t\t      sizeof(tab->descs[0]), kfunc_desc_cmp_by_imm_off);\n\n\treturn res ? &res->func_model : NULL;\n}\n\nstatic int add_subprog_and_kfunc(struct bpf_verifier_env *env)\n{\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint i, ret, insn_cnt = env->prog->len;\n\n\t \n\tret = add_subprog(env, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (!bpf_pseudo_func(insn) && !bpf_pseudo_call(insn) &&\n\t\t    !bpf_pseudo_kfunc_call(insn))\n\t\t\tcontinue;\n\n\t\tif (!env->bpf_capable) {\n\t\t\tverbose(env, \"loading/calling other bpf or kernel functions are allowed for CAP_BPF and CAP_SYS_ADMIN\\n\");\n\t\t\treturn -EPERM;\n\t\t}\n\n\t\tif (bpf_pseudo_func(insn) || bpf_pseudo_call(insn))\n\t\t\tret = add_subprog(env, i + insn->imm + 1);\n\t\telse\n\t\t\tret = add_kfunc_call(env, insn->imm, insn->off);\n\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t \n\tsubprog[env->subprog_cnt].start = insn_cnt;\n\n\tif (env->log.level & BPF_LOG_LEVEL2)\n\t\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\t\tverbose(env, \"func#%d @%d\\n\", i, subprog[i].start);\n\n\treturn 0;\n}\n\nstatic int check_subprogs(struct bpf_verifier_env *env)\n{\n\tint i, subprog_start, subprog_end, off, cur_subprog = 0;\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\n\t \n\tsubprog_start = subprog[cur_subprog].start;\n\tsubprog_end = subprog[cur_subprog + 1].start;\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tu8 code = insn[i].code;\n\n\t\tif (code == (BPF_JMP | BPF_CALL) &&\n\t\t    insn[i].src_reg == 0 &&\n\t\t    insn[i].imm == BPF_FUNC_tail_call)\n\t\t\tsubprog[cur_subprog].has_tail_call = true;\n\t\tif (BPF_CLASS(code) == BPF_LD &&\n\t\t    (BPF_MODE(code) == BPF_ABS || BPF_MODE(code) == BPF_IND))\n\t\t\tsubprog[cur_subprog].has_ld_abs = true;\n\t\tif (BPF_CLASS(code) != BPF_JMP && BPF_CLASS(code) != BPF_JMP32)\n\t\t\tgoto next;\n\t\tif (BPF_OP(code) == BPF_EXIT || BPF_OP(code) == BPF_CALL)\n\t\t\tgoto next;\n\t\tif (code == (BPF_JMP32 | BPF_JA))\n\t\t\toff = i + insn[i].imm + 1;\n\t\telse\n\t\t\toff = i + insn[i].off + 1;\n\t\tif (off < subprog_start || off >= subprog_end) {\n\t\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", i, off);\n\t\t\treturn -EINVAL;\n\t\t}\nnext:\n\t\tif (i == subprog_end - 1) {\n\t\t\t \n\t\t\tif (code != (BPF_JMP | BPF_EXIT) &&\n\t\t\t    code != (BPF_JMP32 | BPF_JA) &&\n\t\t\t    code != (BPF_JMP | BPF_JA)) {\n\t\t\t\tverbose(env, \"last insn is not an exit or jmp\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tsubprog_start = subprog_end;\n\t\t\tcur_subprog++;\n\t\t\tif (cur_subprog < env->subprog_cnt)\n\t\t\t\tsubprog_end = subprog[cur_subprog + 1].start;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int mark_reg_read(struct bpf_verifier_env *env,\n\t\t\t const struct bpf_reg_state *state,\n\t\t\t struct bpf_reg_state *parent, u8 flag)\n{\n\tbool writes = parent == state->parent;  \n\tint cnt = 0;\n\n\twhile (parent) {\n\t\t \n\t\tif (writes && state->live & REG_LIVE_WRITTEN)\n\t\t\tbreak;\n\t\tif (parent->live & REG_LIVE_DONE) {\n\t\t\tverbose(env, \"verifier BUG type %s var_off %lld off %d\\n\",\n\t\t\t\treg_type_str(env, parent->type),\n\t\t\t\tparent->var_off.value, parent->off);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t \n\t\tif ((parent->live & REG_LIVE_READ) == flag ||\n\t\t    parent->live & REG_LIVE_READ64)\n\t\t\t \n\t\t\tbreak;\n\t\t \n\t\tparent->live |= flag;\n\t\t \n\t\tif (flag == REG_LIVE_READ64)\n\t\t\tparent->live &= ~REG_LIVE_READ32;\n\t\tstate = parent;\n\t\tparent = state->parent;\n\t\twrites = true;\n\t\tcnt++;\n\t}\n\n\tif (env->longest_mark_read_walk < cnt)\n\t\tenv->longest_mark_read_walk = cnt;\n\treturn 0;\n}\n\nstatic int mark_dynptr_read(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi, ret;\n\n\t \n\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\treturn 0;\n\tspi = dynptr_get_spi(env, reg);\n\tif (spi < 0)\n\t\treturn spi;\n\t \n\tret = mark_reg_read(env, &state->stack[spi].spilled_ptr,\n\t\t\t    state->stack[spi].spilled_ptr.parent, REG_LIVE_READ64);\n\tif (ret)\n\t\treturn ret;\n\treturn mark_reg_read(env, &state->stack[spi - 1].spilled_ptr,\n\t\t\t     state->stack[spi - 1].spilled_ptr.parent, REG_LIVE_READ64);\n}\n\nstatic int mark_iter_read(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t\t  int spi, int nr_slots)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint err, i;\n\n\tfor (i = 0; i < nr_slots; i++) {\n\t\tstruct bpf_reg_state *st = &state->stack[spi - i].spilled_ptr;\n\n\t\terr = mark_reg_read(env, st, st->parent, REG_LIVE_READ64);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tmark_stack_slot_scratched(env, spi - i);\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool is_reg64(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t     u32 regno, struct bpf_reg_state *reg, enum reg_arg_type t)\n{\n\tu8 code, class, op;\n\n\tcode = insn->code;\n\tclass = BPF_CLASS(code);\n\top = BPF_OP(code);\n\tif (class == BPF_JMP) {\n\t\t \n\t\tif (op == BPF_EXIT)\n\t\t\treturn true;\n\t\tif (op == BPF_CALL) {\n\t\t\t \n\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\treturn false;\n\t\t\t \n\t\t\tif (t == SRC_OP)\n\t\t\t\treturn true;\n\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (class == BPF_ALU64 && op == BPF_END && (insn->imm == 16 || insn->imm == 32))\n\t\treturn false;\n\n\tif (class == BPF_ALU64 || class == BPF_JMP ||\n\t    (class == BPF_ALU && op == BPF_END && insn->imm == 64))\n\t\treturn true;\n\n\tif (class == BPF_ALU || class == BPF_JMP32)\n\t\treturn false;\n\n\tif (class == BPF_LDX) {\n\t\tif (t != SRC_OP)\n\t\t\treturn BPF_SIZE(code) == BPF_DW;\n\t\t \n\t\treturn true;\n\t}\n\n\tif (class == BPF_STX) {\n\t\t \n\t\tif (t == SRC_OP && reg->type != SCALAR_VALUE)\n\t\t\treturn true;\n\t\treturn BPF_SIZE(code) == BPF_DW;\n\t}\n\n\tif (class == BPF_LD) {\n\t\tu8 mode = BPF_MODE(code);\n\n\t\t \n\t\tif (mode == BPF_IMM)\n\t\t\treturn true;\n\n\t\t \n\t\tif (t != SRC_OP)\n\t\t\treturn  false;\n\n\t\t \n\t\tif (regno == BPF_REG_6)\n\t\t\treturn true;\n\n\t\t \n\t\treturn true;\n\t}\n\n\tif (class == BPF_ST)\n\t\t \n\t\treturn true;\n\n\t \n\treturn true;\n}\n\n \nstatic int insn_def_regno(const struct bpf_insn *insn)\n{\n\tswitch (BPF_CLASS(insn->code)) {\n\tcase BPF_JMP:\n\tcase BPF_JMP32:\n\tcase BPF_ST:\n\t\treturn -1;\n\tcase BPF_STX:\n\t\tif (BPF_MODE(insn->code) == BPF_ATOMIC &&\n\t\t    (insn->imm & BPF_FETCH)) {\n\t\t\tif (insn->imm == BPF_CMPXCHG)\n\t\t\t\treturn BPF_REG_0;\n\t\t\telse\n\t\t\t\treturn insn->src_reg;\n\t\t} else {\n\t\t\treturn -1;\n\t\t}\n\tdefault:\n\t\treturn insn->dst_reg;\n\t}\n}\n\n \nstatic bool insn_has_def32(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tint dst_reg = insn_def_regno(insn);\n\n\tif (dst_reg == -1)\n\t\treturn false;\n\n\treturn !is_reg64(env, insn, dst_reg, NULL, DST_OP);\n}\n\nstatic void mark_insn_zext(struct bpf_verifier_env *env,\n\t\t\t   struct bpf_reg_state *reg)\n{\n\ts32 def_idx = reg->subreg_def;\n\n\tif (def_idx == DEF_NOT_SUBREG)\n\t\treturn;\n\n\tenv->insn_aux_data[def_idx - 1].zext_dst = true;\n\t \n\treg->subreg_def = DEF_NOT_SUBREG;\n}\n\nstatic int check_reg_arg(struct bpf_verifier_env *env, u32 regno,\n\t\t\t enum reg_arg_type t)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_insn *insn = env->prog->insnsi + env->insn_idx;\n\tstruct bpf_reg_state *reg, *regs = state->regs;\n\tbool rw64;\n\n\tif (regno >= MAX_BPF_REG) {\n\t\tverbose(env, \"R%d is invalid\\n\", regno);\n\t\treturn -EINVAL;\n\t}\n\n\tmark_reg_scratched(env, regno);\n\n\treg = &regs[regno];\n\trw64 = is_reg64(env, insn, regno, reg, t);\n\tif (t == SRC_OP) {\n\t\t \n\t\tif (reg->type == NOT_INIT) {\n\t\t\tverbose(env, \"R%d !read_ok\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t \n\t\tif (regno == BPF_REG_FP)\n\t\t\treturn 0;\n\n\t\tif (rw64)\n\t\t\tmark_insn_zext(env, reg);\n\n\t\treturn mark_reg_read(env, reg, reg->parent,\n\t\t\t\t     rw64 ? REG_LIVE_READ64 : REG_LIVE_READ32);\n\t} else {\n\t\t \n\t\tif (regno == BPF_REG_FP) {\n\t\t\tverbose(env, \"frame pointer is read only\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\treg->live |= REG_LIVE_WRITTEN;\n\t\treg->subreg_def = rw64 ? DEF_NOT_SUBREG : env->insn_idx + 1;\n\t\tif (t == DST_OP)\n\t\t\tmark_reg_unknown(env, regs, regno);\n\t}\n\treturn 0;\n}\n\nstatic void mark_jmp_point(struct bpf_verifier_env *env, int idx)\n{\n\tenv->insn_aux_data[idx].jmp_point = true;\n}\n\nstatic bool is_jmp_point(struct bpf_verifier_env *env, int insn_idx)\n{\n\treturn env->insn_aux_data[insn_idx].jmp_point;\n}\n\n \nstatic int push_jmp_history(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_verifier_state *cur)\n{\n\tu32 cnt = cur->jmp_history_cnt;\n\tstruct bpf_idx_pair *p;\n\tsize_t alloc_size;\n\n\tif (!is_jmp_point(env, env->insn_idx))\n\t\treturn 0;\n\n\tcnt++;\n\talloc_size = kmalloc_size_roundup(size_mul(cnt, sizeof(*p)));\n\tp = krealloc(cur->jmp_history, alloc_size, GFP_USER);\n\tif (!p)\n\t\treturn -ENOMEM;\n\tp[cnt - 1].idx = env->insn_idx;\n\tp[cnt - 1].prev_idx = env->prev_insn_idx;\n\tcur->jmp_history = p;\n\tcur->jmp_history_cnt = cnt;\n\treturn 0;\n}\n\n \nstatic int get_prev_insn_idx(struct bpf_verifier_state *st, int i,\n\t\t\t     u32 *history)\n{\n\tu32 cnt = *history;\n\n\tif (i == st->first_insn_idx) {\n\t\tif (cnt == 0)\n\t\t\treturn -ENOENT;\n\t\tif (cnt == 1 && st->jmp_history[0].idx == i)\n\t\t\treturn -ENOENT;\n\t}\n\n\tif (cnt && st->jmp_history[cnt - 1].idx == i) {\n\t\ti = st->jmp_history[cnt - 1].prev_idx;\n\t\t(*history)--;\n\t} else {\n\t\ti--;\n\t}\n\treturn i;\n}\n\nstatic const char *disasm_kfunc_name(void *data, const struct bpf_insn *insn)\n{\n\tconst struct btf_type *func;\n\tstruct btf *desc_btf;\n\n\tif (insn->src_reg != BPF_PSEUDO_KFUNC_CALL)\n\t\treturn NULL;\n\n\tdesc_btf = find_kfunc_desc_btf(data, insn->off);\n\tif (IS_ERR(desc_btf))\n\t\treturn \"<error>\";\n\n\tfunc = btf_type_by_id(desc_btf, insn->imm);\n\treturn btf_name_by_offset(desc_btf, func->name_off);\n}\n\nstatic inline void bt_init(struct backtrack_state *bt, u32 frame)\n{\n\tbt->frame = frame;\n}\n\nstatic inline void bt_reset(struct backtrack_state *bt)\n{\n\tstruct bpf_verifier_env *env = bt->env;\n\n\tmemset(bt, 0, sizeof(*bt));\n\tbt->env = env;\n}\n\nstatic inline u32 bt_empty(struct backtrack_state *bt)\n{\n\tu64 mask = 0;\n\tint i;\n\n\tfor (i = 0; i <= bt->frame; i++)\n\t\tmask |= bt->reg_masks[i] | bt->stack_masks[i];\n\n\treturn mask == 0;\n}\n\nstatic inline int bt_subprog_enter(struct backtrack_state *bt)\n{\n\tif (bt->frame == MAX_CALL_FRAMES - 1) {\n\t\tverbose(bt->env, \"BUG subprog enter from frame %d\\n\", bt->frame);\n\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\treturn -EFAULT;\n\t}\n\tbt->frame++;\n\treturn 0;\n}\n\nstatic inline int bt_subprog_exit(struct backtrack_state *bt)\n{\n\tif (bt->frame == 0) {\n\t\tverbose(bt->env, \"BUG subprog exit from frame 0\\n\");\n\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\treturn -EFAULT;\n\t}\n\tbt->frame--;\n\treturn 0;\n}\n\nstatic inline void bt_set_frame_reg(struct backtrack_state *bt, u32 frame, u32 reg)\n{\n\tbt->reg_masks[frame] |= 1 << reg;\n}\n\nstatic inline void bt_clear_frame_reg(struct backtrack_state *bt, u32 frame, u32 reg)\n{\n\tbt->reg_masks[frame] &= ~(1 << reg);\n}\n\nstatic inline void bt_set_reg(struct backtrack_state *bt, u32 reg)\n{\n\tbt_set_frame_reg(bt, bt->frame, reg);\n}\n\nstatic inline void bt_clear_reg(struct backtrack_state *bt, u32 reg)\n{\n\tbt_clear_frame_reg(bt, bt->frame, reg);\n}\n\nstatic inline void bt_set_frame_slot(struct backtrack_state *bt, u32 frame, u32 slot)\n{\n\tbt->stack_masks[frame] |= 1ull << slot;\n}\n\nstatic inline void bt_clear_frame_slot(struct backtrack_state *bt, u32 frame, u32 slot)\n{\n\tbt->stack_masks[frame] &= ~(1ull << slot);\n}\n\nstatic inline void bt_set_slot(struct backtrack_state *bt, u32 slot)\n{\n\tbt_set_frame_slot(bt, bt->frame, slot);\n}\n\nstatic inline void bt_clear_slot(struct backtrack_state *bt, u32 slot)\n{\n\tbt_clear_frame_slot(bt, bt->frame, slot);\n}\n\nstatic inline u32 bt_frame_reg_mask(struct backtrack_state *bt, u32 frame)\n{\n\treturn bt->reg_masks[frame];\n}\n\nstatic inline u32 bt_reg_mask(struct backtrack_state *bt)\n{\n\treturn bt->reg_masks[bt->frame];\n}\n\nstatic inline u64 bt_frame_stack_mask(struct backtrack_state *bt, u32 frame)\n{\n\treturn bt->stack_masks[frame];\n}\n\nstatic inline u64 bt_stack_mask(struct backtrack_state *bt)\n{\n\treturn bt->stack_masks[bt->frame];\n}\n\nstatic inline bool bt_is_reg_set(struct backtrack_state *bt, u32 reg)\n{\n\treturn bt->reg_masks[bt->frame] & (1 << reg);\n}\n\nstatic inline bool bt_is_slot_set(struct backtrack_state *bt, u32 slot)\n{\n\treturn bt->stack_masks[bt->frame] & (1ull << slot);\n}\n\n \nstatic void fmt_reg_mask(char *buf, ssize_t buf_sz, u32 reg_mask)\n{\n\tDECLARE_BITMAP(mask, 64);\n\tbool first = true;\n\tint i, n;\n\n\tbuf[0] = '\\0';\n\n\tbitmap_from_u64(mask, reg_mask);\n\tfor_each_set_bit(i, mask, 32) {\n\t\tn = snprintf(buf, buf_sz, \"%sr%d\", first ? \"\" : \",\", i);\n\t\tfirst = false;\n\t\tbuf += n;\n\t\tbuf_sz -= n;\n\t\tif (buf_sz < 0)\n\t\t\tbreak;\n\t}\n}\n \nstatic void fmt_stack_mask(char *buf, ssize_t buf_sz, u64 stack_mask)\n{\n\tDECLARE_BITMAP(mask, 64);\n\tbool first = true;\n\tint i, n;\n\n\tbuf[0] = '\\0';\n\n\tbitmap_from_u64(mask, stack_mask);\n\tfor_each_set_bit(i, mask, 64) {\n\t\tn = snprintf(buf, buf_sz, \"%s%d\", first ? \"\" : \",\", -(i + 1) * 8);\n\t\tfirst = false;\n\t\tbuf += n;\n\t\tbuf_sz -= n;\n\t\tif (buf_sz < 0)\n\t\t\tbreak;\n\t}\n}\n\n \nstatic int backtrack_insn(struct bpf_verifier_env *env, int idx, int subseq_idx,\n\t\t\t  struct backtrack_state *bt)\n{\n\tconst struct bpf_insn_cbs cbs = {\n\t\t.cb_call\t= disasm_kfunc_name,\n\t\t.cb_print\t= verbose,\n\t\t.private_data\t= env,\n\t};\n\tstruct bpf_insn *insn = env->prog->insnsi + idx;\n\tu8 class = BPF_CLASS(insn->code);\n\tu8 opcode = BPF_OP(insn->code);\n\tu8 mode = BPF_MODE(insn->code);\n\tu32 dreg = insn->dst_reg;\n\tu32 sreg = insn->src_reg;\n\tu32 spi, i;\n\n\tif (insn->code == 0)\n\t\treturn 0;\n\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\tfmt_reg_mask(env->tmp_str_buf, TMP_STR_BUF_LEN, bt_reg_mask(bt));\n\t\tverbose(env, \"mark_precise: frame%d: regs=%s \",\n\t\t\tbt->frame, env->tmp_str_buf);\n\t\tfmt_stack_mask(env->tmp_str_buf, TMP_STR_BUF_LEN, bt_stack_mask(bt));\n\t\tverbose(env, \"stack=%s before \", env->tmp_str_buf);\n\t\tverbose(env, \"%d: \", idx);\n\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t}\n\n\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\tif (!bt_is_reg_set(bt, dreg))\n\t\t\treturn 0;\n\t\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\t\t \n\t\t\treturn 0;\n\t\t} else if (opcode == BPF_MOV) {\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\t \n\t\t\t\tbt_clear_reg(bt, dreg);\n\t\t\t\tbt_set_reg(bt, sreg);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tbt_clear_reg(bt, dreg);\n\t\t\t}\n\t\t} else {\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\t \n\t\t\t\tbt_set_reg(bt, sreg);\n\t\t\t}  \n\t\t}\n\t} else if (class == BPF_LDX) {\n\t\tif (!bt_is_reg_set(bt, dreg))\n\t\t\treturn 0;\n\t\tbt_clear_reg(bt, dreg);\n\n\t\t \n\t\tif (insn->src_reg != BPF_REG_FP)\n\t\t\treturn 0;\n\n\t\t \n\t\tspi = (-insn->off - 1) / BPF_REG_SIZE;\n\t\tif (spi >= 64) {\n\t\t\tverbose(env, \"BUG spi %d\\n\", spi);\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tbt_set_slot(bt, spi);\n\t} else if (class == BPF_STX || class == BPF_ST) {\n\t\tif (bt_is_reg_set(bt, dreg))\n\t\t\t \n\t\t\treturn -ENOTSUPP;\n\t\t \n\t\tif (insn->dst_reg != BPF_REG_FP)\n\t\t\treturn 0;\n\t\tspi = (-insn->off - 1) / BPF_REG_SIZE;\n\t\tif (spi >= 64) {\n\t\t\tverbose(env, \"BUG spi %d\\n\", spi);\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (!bt_is_slot_set(bt, spi))\n\t\t\treturn 0;\n\t\tbt_clear_slot(bt, spi);\n\t\tif (class == BPF_STX)\n\t\t\tbt_set_reg(bt, sreg);\n\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\tif (bpf_pseudo_call(insn)) {\n\t\t\tint subprog_insn_idx, subprog;\n\n\t\t\tsubprog_insn_idx = idx + insn->imm + 1;\n\t\t\tsubprog = find_subprog(env, subprog_insn_idx);\n\t\t\tif (subprog < 0)\n\t\t\t\treturn -EFAULT;\n\n\t\t\tif (subprog_is_global(env, subprog)) {\n\t\t\t\t \n\t\t\t\tWARN_ONCE(idx + 1 != subseq_idx, \"verifier backtracking bug\");\n\t\t\t\t \n\t\t\t\tif (bt_reg_mask(bt) & BPF_REGMASK_ARGS) {\n\t\t\t\t\tverbose(env, \"BUG regs %x\\n\", bt_reg_mask(bt));\n\t\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tbt_clear_reg(bt, BPF_REG_0);\n\t\t\t\treturn 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (bt_reg_mask(bt) & ~BPF_REGMASK_ARGS) {\n\t\t\t\t\tverbose(env, \"BUG regs %x\\n\", bt_reg_mask(bt));\n\t\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tif (bt_stack_mask(bt) != 0)\n\t\t\t\t\treturn -ENOTSUPP;\n\t\t\t\t \n\t\t\t\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++) {\n\t\t\t\t\tif (bt_is_reg_set(bt, i)) {\n\t\t\t\t\t\tbt_clear_reg(bt, i);\n\t\t\t\t\t\tbt_set_frame_reg(bt, bt->frame - 1, i);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (bt_subprog_exit(bt))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else if ((bpf_helper_call(insn) &&\n\t\t\t    is_callback_calling_function(insn->imm) &&\n\t\t\t    !is_async_callback_calling_function(insn->imm)) ||\n\t\t\t   (bpf_pseudo_kfunc_call(insn) && is_callback_calling_kfunc(insn->imm))) {\n\t\t\t \n\t\t\tif (bt_reg_mask(bt) & ~BPF_REGMASK_ARGS) {\n\t\t\t\tverbose(env, \"BUG regs %x\\n\", bt_reg_mask(bt));\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tif (bt_stack_mask(bt) != 0)\n\t\t\t\treturn -ENOTSUPP;\n\t\t\t \n\t\t\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++)\n\t\t\t\tbt_clear_reg(bt, i);\n\t\t\tif (bt_subprog_exit(bt))\n\t\t\t\treturn -EFAULT;\n\t\t\treturn 0;\n\t\t} else if (opcode == BPF_CALL) {\n\t\t\t \n\t\t\tif (insn->src_reg == BPF_PSEUDO_KFUNC_CALL && insn->imm == 0)\n\t\t\t\treturn -ENOTSUPP;\n\t\t\t \n\t\t\tbt_clear_reg(bt, BPF_REG_0);\n\t\t\tif (bt_reg_mask(bt) & BPF_REGMASK_ARGS) {\n\t\t\t\t \n\t\t\t\tverbose(env, \"BUG regs %x\\n\", bt_reg_mask(bt));\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else if (opcode == BPF_EXIT) {\n\t\t\tbool r0_precise;\n\n\t\t\tif (bt_reg_mask(bt) & BPF_REGMASK_ARGS) {\n\t\t\t\t \n\t\t\t\tverbose(env, \"BUG regs %x\\n\", bt_reg_mask(bt));\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\n\t\t\t \n\t\t\tr0_precise = subseq_idx - 1 >= 0 &&\n\t\t\t\t     bpf_pseudo_call(&env->prog->insnsi[subseq_idx - 1]) &&\n\t\t\t\t     bt_is_reg_set(bt, BPF_REG_0);\n\n\t\t\tbt_clear_reg(bt, BPF_REG_0);\n\t\t\tif (bt_subprog_enter(bt))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tif (r0_precise)\n\t\t\t\tbt_set_reg(bt, BPF_REG_0);\n\t\t\t \n\t\t\treturn 0;\n\t\t} else if (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (!bt_is_reg_set(bt, dreg) && !bt_is_reg_set(bt, sreg))\n\t\t\t\treturn 0;\n\t\t\t \n\t\t\tbt_set_reg(bt, dreg);\n\t\t\tbt_set_reg(bt, sreg);\n\t\t\t  \n\t\t}\n\t} else if (class == BPF_LD) {\n\t\tif (!bt_is_reg_set(bt, dreg))\n\t\t\treturn 0;\n\t\tbt_clear_reg(bt, dreg);\n\t\t \n\t\tif (mode == BPF_IND || mode == BPF_ABS)\n\t\t\t \n\t\t\treturn -ENOTSUPP;\n\t}\n\treturn 0;\n}\n\n \nstatic void mark_all_scalars_precise(struct bpf_verifier_env *env,\n\t\t\t\t     struct bpf_verifier_state *st)\n{\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tint i, j;\n\n\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\tverbose(env, \"mark_precise: frame%d: falling back to forcing all scalars precise\\n\",\n\t\t\tst->curframe);\n\t}\n\n\t \n\tfor (st = st->parent; st; st = st->parent) {\n\t\tfor (i = 0; i <= st->curframe; i++) {\n\t\t\tfunc = st->frame[i];\n\t\t\tfor (j = 0; j < BPF_REG_FP; j++) {\n\t\t\t\treg = &func->regs[j];\n\t\t\t\tif (reg->type != SCALAR_VALUE || reg->precise)\n\t\t\t\t\tcontinue;\n\t\t\t\treg->precise = true;\n\t\t\t\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\t\t\t\tverbose(env, \"force_precise: frame%d: forcing r%d to be precise\\n\",\n\t\t\t\t\t\ti, j);\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor (j = 0; j < func->allocated_stack / BPF_REG_SIZE; j++) {\n\t\t\t\tif (!is_spilled_reg(&func->stack[j]))\n\t\t\t\t\tcontinue;\n\t\t\t\treg = &func->stack[j].spilled_ptr;\n\t\t\t\tif (reg->type != SCALAR_VALUE || reg->precise)\n\t\t\t\t\tcontinue;\n\t\t\t\treg->precise = true;\n\t\t\t\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\t\t\t\tverbose(env, \"force_precise: frame%d: forcing fp%d to be precise\\n\",\n\t\t\t\t\t\ti, -(j + 1) * 8);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void mark_all_scalars_imprecise(struct bpf_verifier_env *env, struct bpf_verifier_state *st)\n{\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tint i, j;\n\n\tfor (i = 0; i <= st->curframe; i++) {\n\t\tfunc = st->frame[i];\n\t\tfor (j = 0; j < BPF_REG_FP; j++) {\n\t\t\treg = &func->regs[j];\n\t\t\tif (reg->type != SCALAR_VALUE)\n\t\t\t\tcontinue;\n\t\t\treg->precise = false;\n\t\t}\n\t\tfor (j = 0; j < func->allocated_stack / BPF_REG_SIZE; j++) {\n\t\t\tif (!is_spilled_reg(&func->stack[j]))\n\t\t\t\tcontinue;\n\t\t\treg = &func->stack[j].spilled_ptr;\n\t\t\tif (reg->type != SCALAR_VALUE)\n\t\t\t\tcontinue;\n\t\t\treg->precise = false;\n\t\t}\n\t}\n}\n\nstatic bool idset_contains(struct bpf_idset *s, u32 id)\n{\n\tu32 i;\n\n\tfor (i = 0; i < s->count; ++i)\n\t\tif (s->ids[i] == id)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic int idset_push(struct bpf_idset *s, u32 id)\n{\n\tif (WARN_ON_ONCE(s->count >= ARRAY_SIZE(s->ids)))\n\t\treturn -EFAULT;\n\ts->ids[s->count++] = id;\n\treturn 0;\n}\n\nstatic void idset_reset(struct bpf_idset *s)\n{\n\ts->count = 0;\n}\n\n \nstatic int mark_precise_scalar_ids(struct bpf_verifier_env *env, struct bpf_verifier_state *st)\n{\n\tstruct bpf_idset *precise_ids = &env->idset_scratch;\n\tstruct backtrack_state *bt = &env->bt;\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tDECLARE_BITMAP(mask, 64);\n\tint i, fr;\n\n\tidset_reset(precise_ids);\n\n\tfor (fr = bt->frame; fr >= 0; fr--) {\n\t\tfunc = st->frame[fr];\n\n\t\tbitmap_from_u64(mask, bt_frame_reg_mask(bt, fr));\n\t\tfor_each_set_bit(i, mask, 32) {\n\t\t\treg = &func->regs[i];\n\t\t\tif (!reg->id || reg->type != SCALAR_VALUE)\n\t\t\t\tcontinue;\n\t\t\tif (idset_push(precise_ids, reg->id))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tbitmap_from_u64(mask, bt_frame_stack_mask(bt, fr));\n\t\tfor_each_set_bit(i, mask, 64) {\n\t\t\tif (i >= func->allocated_stack / BPF_REG_SIZE)\n\t\t\t\tbreak;\n\t\t\tif (!is_spilled_scalar_reg(&func->stack[i]))\n\t\t\t\tcontinue;\n\t\t\treg = &func->stack[i].spilled_ptr;\n\t\t\tif (!reg->id)\n\t\t\t\tcontinue;\n\t\t\tif (idset_push(precise_ids, reg->id))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tfor (fr = 0; fr <= st->curframe; ++fr) {\n\t\tfunc = st->frame[fr];\n\n\t\tfor (i = BPF_REG_0; i < BPF_REG_10; ++i) {\n\t\t\treg = &func->regs[i];\n\t\t\tif (!reg->id)\n\t\t\t\tcontinue;\n\t\t\tif (!idset_contains(precise_ids, reg->id))\n\t\t\t\tcontinue;\n\t\t\tbt_set_frame_reg(bt, fr, i);\n\t\t}\n\t\tfor (i = 0; i < func->allocated_stack / BPF_REG_SIZE; ++i) {\n\t\t\tif (!is_spilled_scalar_reg(&func->stack[i]))\n\t\t\t\tcontinue;\n\t\t\treg = &func->stack[i].spilled_ptr;\n\t\t\tif (!reg->id)\n\t\t\t\tcontinue;\n\t\t\tif (!idset_contains(precise_ids, reg->id))\n\t\t\t\tcontinue;\n\t\t\tbt_set_frame_slot(bt, fr, i);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __mark_chain_precision(struct bpf_verifier_env *env, int regno)\n{\n\tstruct backtrack_state *bt = &env->bt;\n\tstruct bpf_verifier_state *st = env->cur_state;\n\tint first_idx = st->first_insn_idx;\n\tint last_idx = env->insn_idx;\n\tint subseq_idx = -1;\n\tstruct bpf_func_state *func;\n\tstruct bpf_reg_state *reg;\n\tbool skip_first = true;\n\tint i, fr, err;\n\n\tif (!env->bpf_capable)\n\t\treturn 0;\n\n\t \n\tbt_init(bt, env->cur_state->curframe);\n\n\t \n\tfunc = st->frame[bt->frame];\n\tif (regno >= 0) {\n\t\treg = &func->regs[regno];\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tWARN_ONCE(1, \"backtracing misuse\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tbt_set_reg(bt, regno);\n\t}\n\n\tif (bt_empty(bt))\n\t\treturn 0;\n\n\tfor (;;) {\n\t\tDECLARE_BITMAP(mask, 64);\n\t\tu32 history = st->jmp_history_cnt;\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\t\tverbose(env, \"mark_precise: frame%d: last_idx %d first_idx %d subseq_idx %d \\n\",\n\t\t\t\tbt->frame, last_idx, first_idx, subseq_idx);\n\t\t}\n\n\t\t \n\t\tif (mark_precise_scalar_ids(env, st))\n\t\t\treturn -EFAULT;\n\n\t\tif (last_idx < 0) {\n\t\t\t \n\t\t\tif (st->curframe == 0 &&\n\t\t\t    st->frame[0]->subprogno > 0 &&\n\t\t\t    st->frame[0]->callsite == BPF_MAIN_FUNC &&\n\t\t\t    bt_stack_mask(bt) == 0 &&\n\t\t\t    (bt_reg_mask(bt) & ~BPF_REGMASK_ARGS) == 0) {\n\t\t\t\tbitmap_from_u64(mask, bt_reg_mask(bt));\n\t\t\t\tfor_each_set_bit(i, mask, 32) {\n\t\t\t\t\treg = &st->frame[0]->regs[i];\n\t\t\t\t\tbt_clear_reg(bt, i);\n\t\t\t\t\tif (reg->type == SCALAR_VALUE)\n\t\t\t\t\t\treg->precise = true;\n\t\t\t\t}\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\tverbose(env, \"BUG backtracking func entry subprog %d reg_mask %x stack_mask %llx\\n\",\n\t\t\t\tst->frame[0]->subprogno, bt_reg_mask(bt), bt_stack_mask(bt));\n\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tfor (i = last_idx;;) {\n\t\t\tif (skip_first) {\n\t\t\t\terr = 0;\n\t\t\t\tskip_first = false;\n\t\t\t} else {\n\t\t\t\terr = backtrack_insn(env, i, subseq_idx, bt);\n\t\t\t}\n\t\t\tif (err == -ENOTSUPP) {\n\t\t\t\tmark_all_scalars_precise(env, env->cur_state);\n\t\t\t\tbt_reset(bt);\n\t\t\t\treturn 0;\n\t\t\t} else if (err) {\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\tif (bt_empty(bt))\n\t\t\t\t \n\t\t\t\treturn 0;\n\t\t\tsubseq_idx = i;\n\t\t\ti = get_prev_insn_idx(st, i, &history);\n\t\t\tif (i == -ENOENT)\n\t\t\t\tbreak;\n\t\t\tif (i >= env->prog->len) {\n\t\t\t\t \n\t\t\t\tverbose(env, \"BUG backtracking idx %d\\n\", i);\n\t\t\t\tWARN_ONCE(1, \"verifier backtracking bug\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t}\n\t\tst = st->parent;\n\t\tif (!st)\n\t\t\tbreak;\n\n\t\tfor (fr = bt->frame; fr >= 0; fr--) {\n\t\t\tfunc = st->frame[fr];\n\t\t\tbitmap_from_u64(mask, bt_frame_reg_mask(bt, fr));\n\t\t\tfor_each_set_bit(i, mask, 32) {\n\t\t\t\treg = &func->regs[i];\n\t\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\t\tbt_clear_frame_reg(bt, fr, i);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (reg->precise)\n\t\t\t\t\tbt_clear_frame_reg(bt, fr, i);\n\t\t\t\telse\n\t\t\t\t\treg->precise = true;\n\t\t\t}\n\n\t\t\tbitmap_from_u64(mask, bt_frame_stack_mask(bt, fr));\n\t\t\tfor_each_set_bit(i, mask, 64) {\n\t\t\t\tif (i >= func->allocated_stack / BPF_REG_SIZE) {\n\t\t\t\t\t \n\t\t\t\t\tmark_all_scalars_precise(env, env->cur_state);\n\t\t\t\t\tbt_reset(bt);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\t\t\tif (!is_spilled_scalar_reg(&func->stack[i])) {\n\t\t\t\t\tbt_clear_frame_slot(bt, fr, i);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\treg = &func->stack[i].spilled_ptr;\n\t\t\t\tif (reg->precise)\n\t\t\t\t\tbt_clear_frame_slot(bt, fr, i);\n\t\t\t\telse\n\t\t\t\t\treg->precise = true;\n\t\t\t}\n\t\t\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\t\t\tfmt_reg_mask(env->tmp_str_buf, TMP_STR_BUF_LEN,\n\t\t\t\t\t     bt_frame_reg_mask(bt, fr));\n\t\t\t\tverbose(env, \"mark_precise: frame%d: parent state regs=%s \",\n\t\t\t\t\tfr, env->tmp_str_buf);\n\t\t\t\tfmt_stack_mask(env->tmp_str_buf, TMP_STR_BUF_LEN,\n\t\t\t\t\t       bt_frame_stack_mask(bt, fr));\n\t\t\t\tverbose(env, \"stack=%s: \", env->tmp_str_buf);\n\t\t\t\tprint_verifier_state(env, func, true);\n\t\t\t}\n\t\t}\n\n\t\tif (bt_empty(bt))\n\t\t\treturn 0;\n\n\t\tsubseq_idx = first_idx;\n\t\tlast_idx = st->last_insn_idx;\n\t\tfirst_idx = st->first_insn_idx;\n\t}\n\n\t \n\tif (!bt_empty(bt)) {\n\t\tmark_all_scalars_precise(env, env->cur_state);\n\t\tbt_reset(bt);\n\t}\n\n\treturn 0;\n}\n\nint mark_chain_precision(struct bpf_verifier_env *env, int regno)\n{\n\treturn __mark_chain_precision(env, regno);\n}\n\n \nstatic int mark_chain_precision_batch(struct bpf_verifier_env *env)\n{\n\treturn __mark_chain_precision(env, -1);\n}\n\nstatic bool is_spillable_regtype(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_STACK:\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_FLOW_KEYS:\n\tcase CONST_PTR_TO_MAP:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BUF:\n\tcase PTR_TO_MEM:\n\tcase PTR_TO_FUNC:\n\tcase PTR_TO_MAP_KEY:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic bool register_is_null(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_equals_const(reg->var_off, 0);\n}\n\nstatic bool register_is_const(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && tnum_is_const(reg->var_off);\n}\n\nstatic bool __is_scalar_unbounded(struct bpf_reg_state *reg)\n{\n\treturn tnum_is_unknown(reg->var_off) &&\n\t       reg->smin_value == S64_MIN && reg->smax_value == S64_MAX &&\n\t       reg->umin_value == 0 && reg->umax_value == U64_MAX &&\n\t       reg->s32_min_value == S32_MIN && reg->s32_max_value == S32_MAX &&\n\t       reg->u32_min_value == 0 && reg->u32_max_value == U32_MAX;\n}\n\nstatic bool register_is_bounded(struct bpf_reg_state *reg)\n{\n\treturn reg->type == SCALAR_VALUE && !__is_scalar_unbounded(reg);\n}\n\nstatic bool __is_pointer_value(bool allow_ptr_leaks,\n\t\t\t       const struct bpf_reg_state *reg)\n{\n\tif (allow_ptr_leaks)\n\t\treturn false;\n\n\treturn reg->type != SCALAR_VALUE;\n}\n\n \nstatic void copy_register_state(struct bpf_reg_state *dst, const struct bpf_reg_state *src)\n{\n\tstruct bpf_reg_state *parent = dst->parent;\n\tenum bpf_reg_liveness live = dst->live;\n\n\t*dst = *src;\n\tdst->parent = parent;\n\tdst->live = live;\n}\n\nstatic void save_register_state(struct bpf_func_state *state,\n\t\t\t\tint spi, struct bpf_reg_state *reg,\n\t\t\t\tint size)\n{\n\tint i;\n\n\tcopy_register_state(&state->stack[spi].spilled_ptr, reg);\n\tif (size == BPF_REG_SIZE)\n\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\tfor (i = BPF_REG_SIZE; i > BPF_REG_SIZE - size; i--)\n\t\tstate->stack[spi].slot_type[i - 1] = STACK_SPILL;\n\n\t \n\tfor (; i; i--)\n\t\tscrub_spilled_slot(&state->stack[spi].slot_type[i - 1]);\n}\n\nstatic bool is_bpf_st_mem(struct bpf_insn *insn)\n{\n\treturn BPF_CLASS(insn->code) == BPF_ST && BPF_MODE(insn->code) == BPF_MEM;\n}\n\n \nstatic int check_stack_write_fixed_off(struct bpf_verifier_env *env,\n\t\t\t\t        \n\t\t\t\t       struct bpf_func_state *state,\n\t\t\t\t       int off, int size, int value_regno,\n\t\t\t\t       int insn_idx)\n{\n\tstruct bpf_func_state *cur;  \n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE, err;\n\tstruct bpf_insn *insn = &env->prog->insnsi[insn_idx];\n\tstruct bpf_reg_state *reg = NULL;\n\tu32 dst_reg = insn->dst_reg;\n\n\t \n\tif (!env->allow_ptr_leaks &&\n\t    is_spilled_reg(&state->stack[spi]) &&\n\t    size != BPF_REG_SIZE) {\n\t\tverbose(env, \"attempt to corrupt spilled pointer on stack\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tcur = env->cur_state->frame[env->cur_state->curframe];\n\tif (value_regno >= 0)\n\t\treg = &cur->regs[value_regno];\n\tif (!env->bypass_spec_v4) {\n\t\tbool sanitize = reg && is_spillable_regtype(reg->type);\n\n\t\tfor (i = 0; i < size; i++) {\n\t\t\tu8 type = state->stack[spi].slot_type[i];\n\n\t\t\tif (type != STACK_MISC && type != STACK_ZERO) {\n\t\t\t\tsanitize = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (sanitize)\n\t\t\tenv->insn_aux_data[insn_idx].sanitize_stack_spill = true;\n\t}\n\n\terr = destroy_if_dynptr_stack_slot(env, state, spi);\n\tif (err)\n\t\treturn err;\n\n\tmark_stack_slot_scratched(env, spi);\n\tif (reg && !(off % BPF_REG_SIZE) && register_is_bounded(reg) &&\n\t    !register_is_null(reg) && env->bpf_capable) {\n\t\tif (dst_reg != BPF_REG_FP) {\n\t\t\t \n\t\t\terr = mark_chain_precision(env, value_regno);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tsave_register_state(state, spi, reg, size);\n\t\t \n\t\tif (fls64(reg->umax_value) > BITS_PER_BYTE * size)\n\t\t\tstate->stack[spi].spilled_ptr.id = 0;\n\t} else if (!reg && !(off % BPF_REG_SIZE) && is_bpf_st_mem(insn) &&\n\t\t   insn->imm != 0 && env->bpf_capable) {\n\t\tstruct bpf_reg_state fake_reg = {};\n\n\t\t__mark_reg_known(&fake_reg, insn->imm);\n\t\tfake_reg.type = SCALAR_VALUE;\n\t\tsave_register_state(state, spi, &fake_reg, size);\n\t} else if (reg && is_spillable_regtype(reg->type)) {\n\t\t \n\t\tif (size != BPF_REG_SIZE) {\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\t\tverbose(env, \"invalid size of register spill\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (state != cur && reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"cannot spill pointers to stack into stack frame of the caller\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tsave_register_state(state, spi, reg, size);\n\t} else {\n\t\tu8 type = STACK_MISC;\n\n\t\t \n\t\tstate->stack[spi].spilled_ptr.type = NOT_INIT;\n\t\t \n\t\tif (is_stack_slot_special(&state->stack[spi]))\n\t\t\tfor (i = 0; i < BPF_REG_SIZE; i++)\n\t\t\t\tscrub_spilled_slot(&state->stack[spi].slot_type[i]);\n\n\t\t \n\t\tif (size == BPF_REG_SIZE)\n\t\t\tstate->stack[spi].spilled_ptr.live |= REG_LIVE_WRITTEN;\n\n\t\t \n\t\tif ((reg && register_is_null(reg)) ||\n\t\t    (!reg && is_bpf_st_mem(insn) && insn->imm == 0)) {\n\t\t\t \n\t\t\terr = mark_chain_precision(env, value_regno);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\ttype = STACK_ZERO;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < size; i++)\n\t\t\tstate->stack[spi].slot_type[(slot - i) % BPF_REG_SIZE] =\n\t\t\t\ttype;\n\t}\n\treturn 0;\n}\n\n \nstatic int check_stack_write_var_off(struct bpf_verifier_env *env,\n\t\t\t\t      \n\t\t\t\t     struct bpf_func_state *state,\n\t\t\t\t     int ptr_regno, int off, int size,\n\t\t\t\t     int value_regno, int insn_idx)\n{\n\tstruct bpf_func_state *cur;  \n\tint min_off, max_off;\n\tint i, err;\n\tstruct bpf_reg_state *ptr_reg = NULL, *value_reg = NULL;\n\tstruct bpf_insn *insn = &env->prog->insnsi[insn_idx];\n\tbool writing_zero = false;\n\t \n\tbool zero_used = false;\n\n\tcur = env->cur_state->frame[env->cur_state->curframe];\n\tptr_reg = &cur->regs[ptr_regno];\n\tmin_off = ptr_reg->smin_value + off;\n\tmax_off = ptr_reg->smax_value + off + size;\n\tif (value_regno >= 0)\n\t\tvalue_reg = &cur->regs[value_regno];\n\tif ((value_reg && register_is_null(value_reg)) ||\n\t    (!value_reg && is_bpf_st_mem(insn) && insn->imm == 0))\n\t\twriting_zero = true;\n\n\tfor (i = min_off; i < max_off; i++) {\n\t\tint spi;\n\n\t\tspi = __get_spi(i);\n\t\terr = destroy_if_dynptr_stack_slot(env, state, spi);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tfor (i = min_off; i < max_off; i++) {\n\t\tu8 new_type, *stype;\n\t\tint slot, spi;\n\n\t\tslot = -i - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tstype = &state->stack[spi].slot_type[slot % BPF_REG_SIZE];\n\t\tmark_stack_slot_scratched(env, spi);\n\n\t\tif (!env->allow_ptr_leaks && *stype != STACK_MISC && *stype != STACK_ZERO) {\n\t\t\t \n\t\t\tverbose(env, \"spilled ptr in range of var-offset stack write; insn %d, ptr off: %d\",\n\t\t\t\tinsn_idx, i);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tstate->stack[spi].spilled_ptr.type = NOT_INIT;\n\n\t\t \n\t\tnew_type = STACK_MISC;\n\t\tif (writing_zero && *stype == STACK_ZERO) {\n\t\t\tnew_type = STACK_ZERO;\n\t\t\tzero_used = true;\n\t\t}\n\t\t \n\t\tif (*stype == STACK_INVALID && !env->allow_uninit_stack) {\n\t\t\tverbose(env, \"uninit stack in range of var-offset write prohibited for !root; insn %d, off: %d\",\n\t\t\t\t\tinsn_idx, i);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*stype = new_type;\n\t}\n\tif (zero_used) {\n\t\t \n\t\terr = mark_chain_precision(env, value_regno);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\n \nstatic void mark_reg_stack_read(struct bpf_verifier_env *env,\n\t\t\t\t \n\t\t\t\tstruct bpf_func_state *ptr_state,\n\t\t\t\tint min_off, int max_off, int dst_regno)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tint i, slot, spi;\n\tu8 *stype;\n\tint zeros = 0;\n\n\tfor (i = min_off; i < max_off; i++) {\n\t\tslot = -i - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tmark_stack_slot_scratched(env, spi);\n\t\tstype = ptr_state->stack[spi].slot_type;\n\t\tif (stype[slot % BPF_REG_SIZE] != STACK_ZERO)\n\t\t\tbreak;\n\t\tzeros++;\n\t}\n\tif (zeros == max_off - min_off) {\n\t\t \n\t\t__mark_reg_const_zero(&state->regs[dst_regno]);\n\t\t \n\t\tstate->regs[dst_regno].precise = true;\n\t} else {\n\t\t \n\t\tmark_reg_unknown(env, state->regs, dst_regno);\n\t}\n\tstate->regs[dst_regno].live |= REG_LIVE_WRITTEN;\n}\n\n \nstatic int check_stack_read_fixed_off(struct bpf_verifier_env *env,\n\t\t\t\t       \n\t\t\t\t      struct bpf_func_state *reg_state,\n\t\t\t\t      int off, int size, int dst_regno)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tint i, slot = -off - 1, spi = slot / BPF_REG_SIZE;\n\tstruct bpf_reg_state *reg;\n\tu8 *stype, type;\n\n\tstype = reg_state->stack[spi].slot_type;\n\treg = &reg_state->stack[spi].spilled_ptr;\n\n\tmark_stack_slot_scratched(env, spi);\n\n\tif (is_spilled_reg(&reg_state->stack[spi])) {\n\t\tu8 spill_size = 1;\n\n\t\tfor (i = BPF_REG_SIZE - 1; i > 0 && stype[i - 1] == STACK_SPILL; i--)\n\t\t\tspill_size++;\n\n\t\tif (size != BPF_REG_SIZE || spill_size != BPF_REG_SIZE) {\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\t\tverbose(env, \"invalid size of register fill\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\t\tif (dst_regno < 0)\n\t\t\t\treturn 0;\n\n\t\t\tif (!(off % BPF_REG_SIZE) && size == spill_size) {\n\t\t\t\t \n\t\t\t\ts32 subreg_def = state->regs[dst_regno].subreg_def;\n\n\t\t\t\tcopy_register_state(&state->regs[dst_regno], reg);\n\t\t\t\tstate->regs[dst_regno].subreg_def = subreg_def;\n\t\t\t} else {\n\t\t\t\tfor (i = 0; i < size; i++) {\n\t\t\t\t\ttype = stype[(slot - i) % BPF_REG_SIZE];\n\t\t\t\t\tif (type == STACK_SPILL)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tif (type == STACK_MISC)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tif (type == STACK_INVALID && env->allow_uninit_stack)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\t\t\toff, i, size);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tmark_reg_unknown(env, state->regs, dst_regno);\n\t\t\t}\n\t\t\tstate->regs[dst_regno].live |= REG_LIVE_WRITTEN;\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (dst_regno >= 0) {\n\t\t\t \n\t\t\tcopy_register_state(&state->regs[dst_regno], reg);\n\t\t\t \n\t\t\tstate->regs[dst_regno].live |= REG_LIVE_WRITTEN;\n\t\t} else if (__is_pointer_value(env->allow_ptr_leaks, reg)) {\n\t\t\t \n\t\t\tverbose(env, \"leaking pointer from stack off %d\\n\",\n\t\t\t\toff);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t} else {\n\t\tfor (i = 0; i < size; i++) {\n\t\t\ttype = stype[(slot - i) % BPF_REG_SIZE];\n\t\t\tif (type == STACK_MISC)\n\t\t\t\tcontinue;\n\t\t\tif (type == STACK_ZERO)\n\t\t\t\tcontinue;\n\t\t\tif (type == STACK_INVALID && env->allow_uninit_stack)\n\t\t\t\tcontinue;\n\t\t\tverbose(env, \"invalid read from stack off %d+%d size %d\\n\",\n\t\t\t\toff, i, size);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\tif (dst_regno >= 0)\n\t\t\tmark_reg_stack_read(env, reg_state, off, off + size, dst_regno);\n\t}\n\treturn 0;\n}\n\nenum bpf_access_src {\n\tACCESS_DIRECT = 1,   \n\tACCESS_HELPER = 2,   \n};\n\nstatic int check_stack_range_initialized(struct bpf_verifier_env *env,\n\t\t\t\t\t int regno, int off, int access_size,\n\t\t\t\t\t bool zero_size_allowed,\n\t\t\t\t\t enum bpf_access_src type,\n\t\t\t\t\t struct bpf_call_arg_meta *meta);\n\nstatic struct bpf_reg_state *reg_state(struct bpf_verifier_env *env, int regno)\n{\n\treturn cur_regs(env) + regno;\n}\n\n \nstatic int check_stack_read_var_off(struct bpf_verifier_env *env,\n\t\t\t\t    int ptr_regno, int off, int size, int dst_regno)\n{\n\t \n\tstruct bpf_reg_state *reg = reg_state(env, ptr_regno);\n\tstruct bpf_func_state *ptr_state = func(env, reg);\n\tint err;\n\tint min_off, max_off;\n\n\t \n\terr = check_stack_range_initialized(env, ptr_regno, off, size,\n\t\t\t\t\t    false, ACCESS_DIRECT, NULL);\n\tif (err)\n\t\treturn err;\n\n\tmin_off = reg->smin_value + off;\n\tmax_off = reg->smax_value + off;\n\tmark_reg_stack_read(env, ptr_state, min_off, max_off + size, dst_regno);\n\treturn 0;\n}\n\n \nstatic int check_stack_read(struct bpf_verifier_env *env,\n\t\t\t    int ptr_regno, int off, int size,\n\t\t\t    int dst_regno)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, ptr_regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint err;\n\t \n\tbool var_off = !tnum_is_const(reg->var_off);\n\n\t \n\tif (dst_regno < 0 && var_off) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable offset stack pointer cannot be passed into helper function; var_off=%s off=%d size=%d\\n\",\n\t\t\ttn_buf, off, size);\n\t\treturn -EACCES;\n\t}\n\t \n\tif (!var_off) {\n\t\toff += reg->var_off.value;\n\t\terr = check_stack_read_fixed_off(env, state, off, size,\n\t\t\t\t\t\t dst_regno);\n\t} else {\n\t\t \n\t\terr = check_stack_read_var_off(env, ptr_regno, off, size,\n\t\t\t\t\t       dst_regno);\n\t}\n\treturn err;\n}\n\n\n \nstatic int check_stack_write(struct bpf_verifier_env *env,\n\t\t\t     int ptr_regno, int off, int size,\n\t\t\t     int value_regno, int insn_idx)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, ptr_regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint err;\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\toff += reg->var_off.value;\n\t\terr = check_stack_write_fixed_off(env, state, off, size,\n\t\t\t\t\t\t  value_regno, insn_idx);\n\t} else {\n\t\t \n\t\terr = check_stack_write_var_off(env, state,\n\t\t\t\t\t\tptr_regno, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t}\n\treturn err;\n}\n\nstatic int check_map_access_type(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int off, int size, enum bpf_access_type type)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_map *map = regs[regno].map_ptr;\n\tu32 cap = bpf_map_flags_to_cap(map);\n\n\tif (type == BPF_WRITE && !(cap & BPF_MAP_CAN_WRITE)) {\n\t\tverbose(env, \"write into map forbidden, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\n\tif (type == BPF_READ && !(cap & BPF_MAP_CAN_READ)) {\n\t\tverbose(env, \"read from map forbidden, value_size=%d off=%d size=%d\\n\",\n\t\t\tmap->value_size, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __check_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t      int off, int size, u32 mem_size,\n\t\t\t      bool zero_size_allowed)\n{\n\tbool size_ok = size > 0 || (size == 0 && zero_size_allowed);\n\tstruct bpf_reg_state *reg;\n\n\tif (off >= 0 && size_ok && (u64)off + size <= mem_size)\n\t\treturn 0;\n\n\treg = &cur_regs(env)[regno];\n\tswitch (reg->type) {\n\tcase PTR_TO_MAP_KEY:\n\t\tverbose(env, \"invalid access to map key, key_size=%d off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tverbose(env, \"invalid access to map value, value_size=%d off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t\tbreak;\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET_END:\n\t\tverbose(env, \"invalid access to packet, off=%d size=%d, R%d(id=%d,off=%d,r=%d)\\n\",\n\t\t\toff, size, regno, reg->id, off, mem_size);\n\t\tbreak;\n\tcase PTR_TO_MEM:\n\tdefault:\n\t\tverbose(env, \"invalid access to memory, mem_size=%u off=%d size=%d\\n\",\n\t\t\tmem_size, off, size);\n\t}\n\n\treturn -EACCES;\n}\n\n \nstatic int check_mem_region_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t   int off, int size, u32 mem_size,\n\t\t\t\t   bool zero_size_allowed)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tint err;\n\n\t \n\tif (reg->smin_value < 0 &&\n\t    (reg->smin_value == S64_MIN ||\n\t     (off + reg->smin_value != (s64)(s32)(off + reg->smin_value)) ||\n\t      reg->smin_value + off < 0)) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, reg->smin_value + off, size,\n\t\t\t\t mem_size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d min value is outside of the allowed memory range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\t \n\tif (reg->umax_value >= BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"R%d unbounded memory access, make sure to bounds check any such access\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = __check_mem_access(env, regno, reg->umax_value + off, size,\n\t\t\t\t mem_size, zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d max value is outside of the allowed memory range\\n\",\n\t\t\tregno);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int __check_ptr_off_reg(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg, int regno,\n\t\t\t       bool fixed_off_ok)\n{\n\t \n\n\tif (reg->off < 0) {\n\t\tverbose(env, \"negative offset %s ptr R%d off=%d disallowed\\n\",\n\t\t\treg_type_str(env, reg->type), regno, reg->off);\n\t\treturn -EACCES;\n\t}\n\n\tif (!fixed_off_ok && reg->off) {\n\t\tverbose(env, \"dereference of modified %s ptr R%d off=%d disallowed\\n\",\n\t\t\treg_type_str(env, reg->type), regno, reg->off);\n\t\treturn -EACCES;\n\t}\n\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"variable %s access var_off=%s disallowed\\n\",\n\t\t\treg_type_str(env, reg->type), tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nint check_ptr_off_reg(struct bpf_verifier_env *env,\n\t\t      const struct bpf_reg_state *reg, int regno)\n{\n\treturn __check_ptr_off_reg(env, reg, regno, false);\n}\n\nstatic int map_kptr_match_type(struct bpf_verifier_env *env,\n\t\t\t       struct btf_field *kptr_field,\n\t\t\t       struct bpf_reg_state *reg, u32 regno)\n{\n\tconst char *targ_name = btf_type_name(kptr_field->kptr.btf, kptr_field->kptr.btf_id);\n\tint perm_flags;\n\tconst char *reg_name = \"\";\n\n\tif (btf_is_kernel(reg->btf)) {\n\t\tperm_flags = PTR_MAYBE_NULL | PTR_TRUSTED | MEM_RCU;\n\n\t\t \n\t\tif (kptr_field->type == BPF_KPTR_UNREF)\n\t\t\tperm_flags |= PTR_UNTRUSTED;\n\t} else {\n\t\tperm_flags = PTR_MAYBE_NULL | MEM_ALLOC;\n\t}\n\n\tif (base_type(reg->type) != PTR_TO_BTF_ID || (type_flag(reg->type) & ~perm_flags))\n\t\tgoto bad_type;\n\n\t \n\treg_name = btf_type_name(reg->btf, reg->btf_id);\n\n\t \n\tif (__check_ptr_off_reg(env, reg, regno, true))\n\t\treturn -EACCES;\n\n\t \n\tif (!btf_struct_ids_match(&env->log, reg->btf, reg->btf_id, reg->off,\n\t\t\t\t  kptr_field->kptr.btf, kptr_field->kptr.btf_id,\n\t\t\t\t  kptr_field->type == BPF_KPTR_REF))\n\t\tgoto bad_type;\n\treturn 0;\nbad_type:\n\tverbose(env, \"invalid kptr access, R%d type=%s%s \", regno,\n\t\treg_type_str(env, reg->type), reg_name);\n\tverbose(env, \"expected=%s%s\", reg_type_str(env, PTR_TO_BTF_ID), targ_name);\n\tif (kptr_field->type == BPF_KPTR_UNREF)\n\t\tverbose(env, \" or %s%s\\n\", reg_type_str(env, PTR_TO_BTF_ID | PTR_UNTRUSTED),\n\t\t\ttarg_name);\n\telse\n\t\tverbose(env, \"\\n\");\n\treturn -EINVAL;\n}\n\n \nstatic bool in_rcu_cs(struct bpf_verifier_env *env)\n{\n\treturn env->cur_state->active_rcu_lock ||\n\t       env->cur_state->active_lock.ptr ||\n\t       !env->prog->aux->sleepable;\n}\n\n \nBTF_SET_START(rcu_protected_types)\nBTF_ID(struct, prog_test_ref_kfunc)\nBTF_ID(struct, cgroup)\nBTF_ID(struct, bpf_cpumask)\nBTF_ID(struct, task_struct)\nBTF_SET_END(rcu_protected_types)\n\nstatic bool rcu_protected_object(const struct btf *btf, u32 btf_id)\n{\n\tif (!btf_is_kernel(btf))\n\t\treturn false;\n\treturn btf_id_set_contains(&rcu_protected_types, btf_id);\n}\n\nstatic bool rcu_safe_kptr(const struct btf_field *field)\n{\n\tconst struct btf_field_kptr *kptr = &field->kptr;\n\n\treturn field->type == BPF_KPTR_REF && rcu_protected_object(kptr->btf, kptr->btf_id);\n}\n\nstatic int check_map_kptr_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t int value_regno, int insn_idx,\n\t\t\t\t struct btf_field *kptr_field)\n{\n\tstruct bpf_insn *insn = &env->prog->insnsi[insn_idx];\n\tint class = BPF_CLASS(insn->code);\n\tstruct bpf_reg_state *val_reg;\n\n\t \n\t \n\tif (BPF_MODE(insn->code) != BPF_MEM) {\n\t\tverbose(env, \"kptr in map can only be accessed using BPF_MEM instruction mode\\n\");\n\t\treturn -EACCES;\n\t}\n\n\t \n\tif (class != BPF_LDX && kptr_field->type == BPF_KPTR_REF) {\n\t\tverbose(env, \"store to referenced kptr disallowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (class == BPF_LDX) {\n\t\tval_reg = reg_state(env, value_regno);\n\t\t \n\t\tmark_btf_ld_reg(env, cur_regs(env), value_regno, PTR_TO_BTF_ID, kptr_field->kptr.btf,\n\t\t\t\tkptr_field->kptr.btf_id,\n\t\t\t\trcu_safe_kptr(kptr_field) && in_rcu_cs(env) ?\n\t\t\t\tPTR_MAYBE_NULL | MEM_RCU :\n\t\t\t\tPTR_MAYBE_NULL | PTR_UNTRUSTED);\n\t\t \n\t\tval_reg->id = ++env->id_gen;\n\t} else if (class == BPF_STX) {\n\t\tval_reg = reg_state(env, value_regno);\n\t\tif (!register_is_null(val_reg) &&\n\t\t    map_kptr_match_type(env, kptr_field, val_reg, value_regno))\n\t\t\treturn -EACCES;\n\t} else if (class == BPF_ST) {\n\t\tif (insn->imm) {\n\t\t\tverbose(env, \"BPF_ST imm must be 0 when storing to kptr at off=%u\\n\",\n\t\t\t\tkptr_field->offset);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tverbose(env, \"kptr in map can only be accessed using BPF_LDX/BPF_STX/BPF_ST\\n\");\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\n \nstatic int check_map_access(struct bpf_verifier_env *env, u32 regno,\n\t\t\t    int off, int size, bool zero_size_allowed,\n\t\t\t    enum bpf_access_src src)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regno];\n\tstruct bpf_map *map = reg->map_ptr;\n\tstruct btf_record *rec;\n\tint err, i;\n\n\terr = check_mem_region_access(env, regno, off, size, map->value_size,\n\t\t\t\t      zero_size_allowed);\n\tif (err)\n\t\treturn err;\n\n\tif (IS_ERR_OR_NULL(map->record))\n\t\treturn 0;\n\trec = map->record;\n\tfor (i = 0; i < rec->cnt; i++) {\n\t\tstruct btf_field *field = &rec->fields[i];\n\t\tu32 p = field->offset;\n\n\t\t \n\t\tif (reg->smin_value + off < p + btf_field_type_size(field->type) &&\n\t\t    p < reg->umax_value + off + size) {\n\t\t\tswitch (field->type) {\n\t\t\tcase BPF_KPTR_UNREF:\n\t\t\tcase BPF_KPTR_REF:\n\t\t\t\tif (src != ACCESS_DIRECT) {\n\t\t\t\t\tverbose(env, \"kptr cannot be accessed indirectly by helper\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\t\t\tverbose(env, \"kptr access cannot have variable offset\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tif (p != off + reg->var_off.value) {\n\t\t\t\t\tverbose(env, \"kptr access misaligned expected=%u off=%llu\\n\",\n\t\t\t\t\t\tp, off + reg->var_off.value);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tif (size != bpf_size_to_bytes(BPF_DW)) {\n\t\t\t\t\tverbose(env, \"kptr access size must be BPF_DW\\n\");\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tverbose(env, \"%s cannot be accessed directly by load/store\\n\",\n\t\t\t\t\tbtf_field_type_name(field->type));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n#define MAX_PACKET_OFF 0xffff\n\nstatic bool may_access_direct_pkt_data(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_call_arg_meta *meta,\n\t\t\t\t       enum bpf_access_type t)\n{\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\n\tswitch (prog_type) {\n\t \n\tcase BPF_PROG_TYPE_LWT_IN:\n\tcase BPF_PROG_TYPE_LWT_OUT:\n\tcase BPF_PROG_TYPE_LWT_SEG6LOCAL:\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (t == BPF_WRITE)\n\t\t\treturn false;\n\t\tfallthrough;\n\n\t \n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_LWT_XMIT:\n\tcase BPF_PROG_TYPE_SK_SKB:\n\tcase BPF_PROG_TYPE_SK_MSG:\n\t\tif (meta)\n\t\t\treturn meta->pkt_access;\n\n\t\tenv->seen_direct_write = true;\n\t\treturn true;\n\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tif (t == BPF_WRITE)\n\t\t\tenv->seen_direct_write = true;\n\n\t\treturn true;\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int check_packet_access(struct bpf_verifier_env *env, u32 regno, int off,\n\t\t\t       int size, bool zero_size_allowed)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tint err;\n\n\t \n\n\t \n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\terr = reg->range < 0 ? -EINVAL :\n\t      __check_mem_access(env, regno, off, size, reg->range,\n\t\t\t\t zero_size_allowed);\n\tif (err) {\n\t\tverbose(env, \"R%d offset is outside of the packet\\n\", regno);\n\t\treturn err;\n\t}\n\n\t \n\tenv->prog->aux->max_pkt_offset =\n\t\tmax_t(u32, env->prog->aux->max_pkt_offset,\n\t\t      off + reg->umax_value + size - 1);\n\n\treturn err;\n}\n\n \nstatic int check_ctx_access(struct bpf_verifier_env *env, int insn_idx, int off, int size,\n\t\t\t    enum bpf_access_type t, enum bpf_reg_type *reg_type,\n\t\t\t    struct btf **btf, u32 *btf_id)\n{\n\tstruct bpf_insn_access_aux info = {\n\t\t.reg_type = *reg_type,\n\t\t.log = &env->log,\n\t};\n\n\tif (env->ops->is_valid_access &&\n\t    env->ops->is_valid_access(off, size, t, env->prog, &info)) {\n\t\t \n\t\t*reg_type = info.reg_type;\n\n\t\tif (base_type(*reg_type) == PTR_TO_BTF_ID) {\n\t\t\t*btf = info.btf;\n\t\t\t*btf_id = info.btf_id;\n\t\t} else {\n\t\t\tenv->insn_aux_data[insn_idx].ctx_field_size = info.ctx_field_size;\n\t\t}\n\t\t \n\t\tif (env->prog->aux->max_ctx_offset < off + size)\n\t\t\tenv->prog->aux->max_ctx_offset = off + size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"invalid bpf_context access off=%d size=%d\\n\", off, size);\n\treturn -EACCES;\n}\n\nstatic int check_flow_keys_access(struct bpf_verifier_env *env, int off,\n\t\t\t\t  int size)\n{\n\tif (size < 0 || off < 0 ||\n\t    (u64)off + size > sizeof(struct bpf_flow_keys)) {\n\t\tverbose(env, \"invalid access to flow keys off=%d size=%d\\n\",\n\t\t\toff, size);\n\t\treturn -EACCES;\n\t}\n\treturn 0;\n}\n\nstatic int check_sock_access(struct bpf_verifier_env *env, int insn_idx,\n\t\t\t     u32 regno, int off, int size,\n\t\t\t     enum bpf_access_type t)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tstruct bpf_insn_access_aux info = {};\n\tbool valid;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned index or do a if (index >=0) check.\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tswitch (reg->type) {\n\tcase PTR_TO_SOCK_COMMON:\n\t\tvalid = bpf_sock_common_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tvalid = bpf_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tvalid = bpf_tcp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tvalid = bpf_xdp_sock_is_valid_access(off, size, t, &info);\n\t\tbreak;\n\tdefault:\n\t\tvalid = false;\n\t}\n\n\n\tif (valid) {\n\t\tenv->insn_aux_data[insn_idx].ctx_field_size =\n\t\t\tinfo.ctx_field_size;\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"R%d invalid %s access off=%d size=%d\\n\",\n\t\tregno, reg_type_str(env, reg->type), off, size);\n\n\treturn -EACCES;\n}\n\nstatic bool is_pointer_value(struct bpf_verifier_env *env, int regno)\n{\n\treturn __is_pointer_value(env->allow_ptr_leaks, reg_state(env, regno));\n}\n\nstatic bool is_ctx_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn reg->type == PTR_TO_CTX;\n}\n\nstatic bool is_sk_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_sk_pointer(reg->type);\n}\n\nstatic bool is_pkt_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\treturn type_is_pkt_pointer(reg->type);\n}\n\nstatic bool is_flow_key_reg(struct bpf_verifier_env *env, int regno)\n{\n\tconst struct bpf_reg_state *reg = reg_state(env, regno);\n\n\t \n\treturn reg->type == PTR_TO_FLOW_KEYS;\n}\n\nstatic u32 *reg2btf_ids[__BPF_REG_TYPE_MAX] = {\n#ifdef CONFIG_NET\n\t[PTR_TO_SOCKET] = &btf_sock_ids[BTF_SOCK_TYPE_SOCK],\n\t[PTR_TO_SOCK_COMMON] = &btf_sock_ids[BTF_SOCK_TYPE_SOCK_COMMON],\n\t[PTR_TO_TCP_SOCK] = &btf_sock_ids[BTF_SOCK_TYPE_TCP],\n#endif\n\t[CONST_PTR_TO_MAP] = btf_bpf_map_id,\n};\n\nstatic bool is_trusted_reg(const struct bpf_reg_state *reg)\n{\n\t \n\tif (reg->ref_obj_id)\n\t\treturn true;\n\n\t \n\tif (reg2btf_ids[base_type(reg->type)])\n\t\treturn true;\n\n\t \n\treturn type_flag(reg->type) & BPF_REG_TRUSTED_MODIFIERS &&\n\t       !bpf_type_has_unsafe_modifiers(reg->type);\n}\n\nstatic bool is_rcu_reg(const struct bpf_reg_state *reg)\n{\n\treturn reg->type & MEM_RCU;\n}\n\nstatic void clear_trusted_flags(enum bpf_type_flag *flag)\n{\n\t*flag &= ~(BPF_REG_TRUSTED_MODIFIERS | MEM_RCU);\n}\n\nstatic int check_pkt_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t\t   int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\tint ip_align;\n\n\t \n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\t \n\tip_align = 2;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(ip_align + reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"misaligned packet access off %d+%s+%d+%d size %d\\n\",\n\t\t\tip_align, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_generic_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t\t       const char *pointer_desc,\n\t\t\t\t       int off, int size, bool strict)\n{\n\tstruct tnum reg_off;\n\n\t \n\tif (!strict || size == 1)\n\t\treturn 0;\n\n\treg_off = tnum_add(reg->var_off, tnum_const(reg->off + off));\n\tif (!tnum_is_aligned(reg_off, size)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"misaligned %saccess off %s+%d+%d size %d\\n\",\n\t\t\tpointer_desc, tn_buf, reg->off, off, size);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_ptr_alignment(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg, int off,\n\t\t\t       int size, bool strict_alignment_once)\n{\n\tbool strict = env->strict_alignment || strict_alignment_once;\n\tconst char *pointer_desc = \"\";\n\n\tswitch (reg->type) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\t \n\t\treturn check_pkt_ptr_alignment(env, reg, off, size, strict);\n\tcase PTR_TO_FLOW_KEYS:\n\t\tpointer_desc = \"flow keys \";\n\t\tbreak;\n\tcase PTR_TO_MAP_KEY:\n\t\tpointer_desc = \"key \";\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tpointer_desc = \"value \";\n\t\tbreak;\n\tcase PTR_TO_CTX:\n\t\tpointer_desc = \"context \";\n\t\tbreak;\n\tcase PTR_TO_STACK:\n\t\tpointer_desc = \"stack \";\n\t\t \n\t\tstrict = true;\n\t\tbreak;\n\tcase PTR_TO_SOCKET:\n\t\tpointer_desc = \"sock \";\n\t\tbreak;\n\tcase PTR_TO_SOCK_COMMON:\n\t\tpointer_desc = \"sock_common \";\n\t\tbreak;\n\tcase PTR_TO_TCP_SOCK:\n\t\tpointer_desc = \"tcp_sock \";\n\t\tbreak;\n\tcase PTR_TO_XDP_SOCK:\n\t\tpointer_desc = \"xdp_sock \";\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn check_generic_ptr_alignment(env, reg, pointer_desc, off, size,\n\t\t\t\t\t   strict);\n}\n\n \nstatic int check_max_stack_depth_subprog(struct bpf_verifier_env *env, int idx)\n{\n\tstruct bpf_subprog_info *subprog = env->subprog_info;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint depth = 0, frame = 0, i, subprog_end;\n\tbool tail_call_reachable = false;\n\tint ret_insn[MAX_CALL_FRAMES];\n\tint ret_prog[MAX_CALL_FRAMES];\n\tint j;\n\n\ti = subprog[idx].start;\nprocess_func:\n\t \n\tif (idx && subprog[idx].has_tail_call && depth >= 256) {\n\t\tverbose(env,\n\t\t\t\"tail_calls are not allowed when call stack of previous frames is %d bytes. Too large\\n\",\n\t\t\tdepth);\n\t\treturn -EACCES;\n\t}\n\t \n\tdepth += round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tif (depth > MAX_BPF_STACK) {\n\t\tverbose(env, \"combined stack size of %d calls is %d. Too large\\n\",\n\t\t\tframe + 1, depth);\n\t\treturn -EACCES;\n\t}\ncontinue_func:\n\tsubprog_end = subprog[idx + 1].start;\n\tfor (; i < subprog_end; i++) {\n\t\tint next_insn, sidx;\n\n\t\tif (!bpf_pseudo_call(insn + i) && !bpf_pseudo_func(insn + i))\n\t\t\tcontinue;\n\t\t \n\t\tret_insn[frame] = i + 1;\n\t\tret_prog[frame] = idx;\n\n\t\t \n\t\tnext_insn = i + insn[i].imm + 1;\n\t\tsidx = find_subprog(env, next_insn);\n\t\tif (sidx < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  next_insn);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (subprog[sidx].is_async_cb) {\n\t\t\tif (subprog[sidx].has_tail_call) {\n\t\t\t\tverbose(env, \"verifier bug. subprog has tail_call and async cb\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\t \n\t\t\tif (!bpf_pseudo_call(insn + i))\n\t\t\t\tcontinue;\n\t\t}\n\t\ti = next_insn;\n\t\tidx = sidx;\n\n\t\tif (subprog[idx].has_tail_call)\n\t\t\ttail_call_reachable = true;\n\n\t\tframe++;\n\t\tif (frame >= MAX_CALL_FRAMES) {\n\t\t\tverbose(env, \"the call stack of %d frames is too deep !\\n\",\n\t\t\t\tframe);\n\t\t\treturn -E2BIG;\n\t\t}\n\t\tgoto process_func;\n\t}\n\t \n\tif (tail_call_reachable)\n\t\tfor (j = 0; j < frame; j++)\n\t\t\tsubprog[ret_prog[j]].tail_call_reachable = true;\n\tif (subprog[0].tail_call_reachable)\n\t\tenv->prog->aux->tail_call_reachable = true;\n\n\t \n\tif (frame == 0)\n\t\treturn 0;\n\tdepth -= round_up(max_t(u32, subprog[idx].stack_depth, 1), 32);\n\tframe--;\n\ti = ret_insn[frame];\n\tidx = ret_prog[frame];\n\tgoto continue_func;\n}\n\nstatic int check_max_stack_depth(struct bpf_verifier_env *env)\n{\n\tstruct bpf_subprog_info *si = env->subprog_info;\n\tint ret;\n\n\tfor (int i = 0; i < env->subprog_cnt; i++) {\n\t\tif (!i || si[i].is_async_cb) {\n\t\t\tret = check_max_stack_depth_subprog(env, i);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\tcontinue;\n\t}\n\treturn 0;\n}\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\nstatic int get_callee_stack_depth(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_insn *insn, int idx)\n{\n\tint start = idx + insn->imm + 1, subprog;\n\n\tsubprog = find_subprog(env, start);\n\tif (subprog < 0) {\n\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t  start);\n\t\treturn -EFAULT;\n\t}\n\treturn env->subprog_info[subprog].stack_depth;\n}\n#endif\n\nstatic int __check_buffer_access(struct bpf_verifier_env *env,\n\t\t\t\t const char *buf_info,\n\t\t\t\t const struct bpf_reg_state *reg,\n\t\t\t\t int regno, int off, int size)\n{\n\tif (off < 0) {\n\t\tverbose(env,\n\t\t\t\"R%d invalid %s buffer access: off=%d, size=%d\\n\",\n\t\t\tregno, buf_info, off, size);\n\t\treturn -EACCES;\n\t}\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"R%d invalid variable buffer offset: off=%d, var_off=%s\\n\",\n\t\t\tregno, off, tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_tp_buffer_access(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  int regno, int off, int size)\n{\n\tint err;\n\n\terr = __check_buffer_access(env, \"tracepoint\", reg, regno, off, size);\n\tif (err)\n\t\treturn err;\n\n\tif (off + size > env->prog->aux->max_tp_access)\n\t\tenv->prog->aux->max_tp_access = off + size;\n\n\treturn 0;\n}\n\nstatic int check_buffer_access(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_reg_state *reg,\n\t\t\t       int regno, int off, int size,\n\t\t\t       bool zero_size_allowed,\n\t\t\t       u32 *max_access)\n{\n\tconst char *buf_info = type_is_rdonly_mem(reg->type) ? \"rdonly\" : \"rdwr\";\n\tint err;\n\n\terr = __check_buffer_access(env, buf_info, reg, regno, off, size);\n\tif (err)\n\t\treturn err;\n\n\tif (off + size > *max_access)\n\t\t*max_access = off + size;\n\n\treturn 0;\n}\n\n \nstatic void zext_32_to_64(struct bpf_reg_state *reg)\n{\n\treg->var_off = tnum_subreg(reg->var_off);\n\t__reg_assign_32_into_64(reg);\n}\n\n \nstatic void coerce_reg_to_size(struct bpf_reg_state *reg, int size)\n{\n\tu64 mask;\n\n\t \n\treg->var_off = tnum_cast(reg->var_off, size);\n\n\t \n\tmask = ((u64)1 << (size * 8)) - 1;\n\tif ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {\n\t\treg->umin_value &= mask;\n\t\treg->umax_value &= mask;\n\t} else {\n\t\treg->umin_value = 0;\n\t\treg->umax_value = mask;\n\t}\n\treg->smin_value = reg->umin_value;\n\treg->smax_value = reg->umax_value;\n\n\t \n\tif (size >= 4)\n\t\treturn;\n\t__reg_combine_64_into_32(reg);\n}\n\nstatic void set_sext64_default_val(struct bpf_reg_state *reg, int size)\n{\n\tif (size == 1) {\n\t\treg->smin_value = reg->s32_min_value = S8_MIN;\n\t\treg->smax_value = reg->s32_max_value = S8_MAX;\n\t} else if (size == 2) {\n\t\treg->smin_value = reg->s32_min_value = S16_MIN;\n\t\treg->smax_value = reg->s32_max_value = S16_MAX;\n\t} else {\n\t\t \n\t\treg->smin_value = reg->s32_min_value = S32_MIN;\n\t\treg->smax_value = reg->s32_max_value = S32_MAX;\n\t}\n\treg->umin_value = reg->u32_min_value = 0;\n\treg->umax_value = U64_MAX;\n\treg->u32_max_value = U32_MAX;\n\treg->var_off = tnum_unknown;\n}\n\nstatic void coerce_reg_to_size_sx(struct bpf_reg_state *reg, int size)\n{\n\ts64 init_s64_max, init_s64_min, s64_max, s64_min, u64_cval;\n\tu64 top_smax_value, top_smin_value;\n\tu64 num_bits = size * 8;\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\tu64_cval = reg->var_off.value;\n\t\tif (size == 1)\n\t\t\treg->var_off = tnum_const((s8)u64_cval);\n\t\telse if (size == 2)\n\t\t\treg->var_off = tnum_const((s16)u64_cval);\n\t\telse\n\t\t\t \n\t\t\treg->var_off = tnum_const((s32)u64_cval);\n\n\t\tu64_cval = reg->var_off.value;\n\t\treg->smax_value = reg->smin_value = u64_cval;\n\t\treg->umax_value = reg->umin_value = u64_cval;\n\t\treg->s32_max_value = reg->s32_min_value = u64_cval;\n\t\treg->u32_max_value = reg->u32_min_value = u64_cval;\n\t\treturn;\n\t}\n\n\ttop_smax_value = ((u64)reg->smax_value >> num_bits) << num_bits;\n\ttop_smin_value = ((u64)reg->smin_value >> num_bits) << num_bits;\n\n\tif (top_smax_value != top_smin_value)\n\t\tgoto out;\n\n\t \n\tif (size == 1) {\n\t\tinit_s64_max = (s8)reg->smax_value;\n\t\tinit_s64_min = (s8)reg->smin_value;\n\t} else if (size == 2) {\n\t\tinit_s64_max = (s16)reg->smax_value;\n\t\tinit_s64_min = (s16)reg->smin_value;\n\t} else {\n\t\tinit_s64_max = (s32)reg->smax_value;\n\t\tinit_s64_min = (s32)reg->smin_value;\n\t}\n\n\ts64_max = max(init_s64_max, init_s64_min);\n\ts64_min = min(init_s64_max, init_s64_min);\n\n\t \n\tif ((s64_max >= 0) == (s64_min >= 0)) {\n\t\treg->smin_value = reg->s32_min_value = s64_min;\n\t\treg->smax_value = reg->s32_max_value = s64_max;\n\t\treg->umin_value = reg->u32_min_value = s64_min;\n\t\treg->umax_value = reg->u32_max_value = s64_max;\n\t\treg->var_off = tnum_range(s64_min, s64_max);\n\t\treturn;\n\t}\n\nout:\n\tset_sext64_default_val(reg, size);\n}\n\nstatic void set_sext32_default_val(struct bpf_reg_state *reg, int size)\n{\n\tif (size == 1) {\n\t\treg->s32_min_value = S8_MIN;\n\t\treg->s32_max_value = S8_MAX;\n\t} else {\n\t\t \n\t\treg->s32_min_value = S16_MIN;\n\t\treg->s32_max_value = S16_MAX;\n\t}\n\treg->u32_min_value = 0;\n\treg->u32_max_value = U32_MAX;\n}\n\nstatic void coerce_subreg_to_size_sx(struct bpf_reg_state *reg, int size)\n{\n\ts32 init_s32_max, init_s32_min, s32_max, s32_min, u32_val;\n\tu32 top_smax_value, top_smin_value;\n\tu32 num_bits = size * 8;\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\tu32_val = reg->var_off.value;\n\t\tif (size == 1)\n\t\t\treg->var_off = tnum_const((s8)u32_val);\n\t\telse\n\t\t\treg->var_off = tnum_const((s16)u32_val);\n\n\t\tu32_val = reg->var_off.value;\n\t\treg->s32_min_value = reg->s32_max_value = u32_val;\n\t\treg->u32_min_value = reg->u32_max_value = u32_val;\n\t\treturn;\n\t}\n\n\ttop_smax_value = ((u32)reg->s32_max_value >> num_bits) << num_bits;\n\ttop_smin_value = ((u32)reg->s32_min_value >> num_bits) << num_bits;\n\n\tif (top_smax_value != top_smin_value)\n\t\tgoto out;\n\n\t \n\tif (size == 1) {\n\t\tinit_s32_max = (s8)reg->s32_max_value;\n\t\tinit_s32_min = (s8)reg->s32_min_value;\n\t} else {\n\t\t \n\t\tinit_s32_max = (s16)reg->s32_max_value;\n\t\tinit_s32_min = (s16)reg->s32_min_value;\n\t}\n\ts32_max = max(init_s32_max, init_s32_min);\n\ts32_min = min(init_s32_max, init_s32_min);\n\n\tif ((s32_min >= 0) == (s32_max >= 0)) {\n\t\treg->s32_min_value = s32_min;\n\t\treg->s32_max_value = s32_max;\n\t\treg->u32_min_value = (u32)s32_min;\n\t\treg->u32_max_value = (u32)s32_max;\n\t\treturn;\n\t}\n\nout:\n\tset_sext32_default_val(reg, size);\n}\n\nstatic bool bpf_map_is_rdonly(const struct bpf_map *map)\n{\n\t \n\treturn (map->map_flags & BPF_F_RDONLY_PROG) &&\n\t       READ_ONCE(map->frozen) &&\n\t       !bpf_map_write_active(map);\n}\n\nstatic int bpf_map_direct_read(struct bpf_map *map, int off, int size, u64 *val,\n\t\t\t       bool is_ldsx)\n{\n\tvoid *ptr;\n\tu64 addr;\n\tint err;\n\n\terr = map->ops->map_direct_value_addr(map, &addr, off);\n\tif (err)\n\t\treturn err;\n\tptr = (void *)(long)addr + off;\n\n\tswitch (size) {\n\tcase sizeof(u8):\n\t\t*val = is_ldsx ? (s64)*(s8 *)ptr : (u64)*(u8 *)ptr;\n\t\tbreak;\n\tcase sizeof(u16):\n\t\t*val = is_ldsx ? (s64)*(s16 *)ptr : (u64)*(u16 *)ptr;\n\t\tbreak;\n\tcase sizeof(u32):\n\t\t*val = is_ldsx ? (s64)*(s32 *)ptr : (u64)*(u32 *)ptr;\n\t\tbreak;\n\tcase sizeof(u64):\n\t\t*val = *(u64 *)ptr;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n#define BTF_TYPE_SAFE_RCU(__type)  __PASTE(__type, __safe_rcu)\n#define BTF_TYPE_SAFE_RCU_OR_NULL(__type)  __PASTE(__type, __safe_rcu_or_null)\n#define BTF_TYPE_SAFE_TRUSTED(__type)  __PASTE(__type, __safe_trusted)\n\n \n\n \nBTF_TYPE_SAFE_RCU(struct task_struct) {\n\tconst cpumask_t *cpus_ptr;\n\tstruct css_set __rcu *cgroups;\n\tstruct task_struct __rcu *real_parent;\n\tstruct task_struct *group_leader;\n};\n\nBTF_TYPE_SAFE_RCU(struct cgroup) {\n\t \n\tstruct kernfs_node *kn;\n};\n\nBTF_TYPE_SAFE_RCU(struct css_set) {\n\tstruct cgroup *dfl_cgrp;\n};\n\n \nBTF_TYPE_SAFE_RCU_OR_NULL(struct mm_struct) {\n\tstruct file __rcu *exe_file;\n};\n\n \nBTF_TYPE_SAFE_RCU_OR_NULL(struct sk_buff) {\n\tstruct sock *sk;\n};\n\nBTF_TYPE_SAFE_RCU_OR_NULL(struct request_sock) {\n\tstruct sock *sk;\n};\n\n \nBTF_TYPE_SAFE_TRUSTED(struct bpf_iter_meta) {\n\tstruct seq_file *seq;\n};\n\nBTF_TYPE_SAFE_TRUSTED(struct bpf_iter__task) {\n\tstruct bpf_iter_meta *meta;\n\tstruct task_struct *task;\n};\n\nBTF_TYPE_SAFE_TRUSTED(struct linux_binprm) {\n\tstruct file *file;\n};\n\nBTF_TYPE_SAFE_TRUSTED(struct file) {\n\tstruct inode *f_inode;\n};\n\nBTF_TYPE_SAFE_TRUSTED(struct dentry) {\n\t \n\tstruct inode *d_inode;\n};\n\nBTF_TYPE_SAFE_TRUSTED(struct socket) {\n\tstruct sock *sk;\n};\n\nstatic bool type_is_rcu(struct bpf_verifier_env *env,\n\t\t\tstruct bpf_reg_state *reg,\n\t\t\tconst char *field_name, u32 btf_id)\n{\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_RCU(struct task_struct));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_RCU(struct cgroup));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_RCU(struct css_set));\n\n\treturn btf_nested_type_is_trusted(&env->log, reg, field_name, btf_id, \"__safe_rcu\");\n}\n\nstatic bool type_is_rcu_or_null(struct bpf_verifier_env *env,\n\t\t\t\tstruct bpf_reg_state *reg,\n\t\t\t\tconst char *field_name, u32 btf_id)\n{\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_RCU_OR_NULL(struct mm_struct));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_RCU_OR_NULL(struct sk_buff));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_RCU_OR_NULL(struct request_sock));\n\n\treturn btf_nested_type_is_trusted(&env->log, reg, field_name, btf_id, \"__safe_rcu_or_null\");\n}\n\nstatic bool type_is_trusted(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_reg_state *reg,\n\t\t\t    const char *field_name, u32 btf_id)\n{\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct bpf_iter_meta));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct bpf_iter__task));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct linux_binprm));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct file));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct dentry));\n\tBTF_TYPE_EMIT(BTF_TYPE_SAFE_TRUSTED(struct socket));\n\n\treturn btf_nested_type_is_trusted(&env->log, reg, field_name, btf_id, \"__safe_trusted\");\n}\n\nstatic int check_ptr_to_btf_access(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs,\n\t\t\t\t   int regno, int off, int size,\n\t\t\t\t   enum bpf_access_type atype,\n\t\t\t\t   int value_regno)\n{\n\tstruct bpf_reg_state *reg = regs + regno;\n\tconst struct btf_type *t = btf_type_by_id(reg->btf, reg->btf_id);\n\tconst char *tname = btf_name_by_offset(reg->btf, t->name_off);\n\tconst char *field_name = NULL;\n\tenum bpf_type_flag flag = 0;\n\tu32 btf_id = 0;\n\tint ret;\n\n\tif (!env->allow_ptr_leaks) {\n\t\tverbose(env,\n\t\t\t\"'struct %s' access is allowed only to CAP_PERFMON and CAP_SYS_ADMIN\\n\",\n\t\t\ttname);\n\t\treturn -EPERM;\n\t}\n\tif (!env->prog->gpl_compatible && btf_is_kernel(reg->btf)) {\n\t\tverbose(env,\n\t\t\t\"Cannot access kernel 'struct %s' from non-GPL compatible program\\n\",\n\t\t\ttname);\n\t\treturn -EINVAL;\n\t}\n\tif (off < 0) {\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s invalid negative access: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\tif (!tnum_is_const(reg->var_off) || reg->var_off.value) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s invalid variable offset: off=%d, var_off=%s\\n\",\n\t\t\tregno, tname, off, tn_buf);\n\t\treturn -EACCES;\n\t}\n\n\tif (reg->type & MEM_USER) {\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s access user memory: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\n\tif (reg->type & MEM_PERCPU) {\n\t\tverbose(env,\n\t\t\t\"R%d is ptr_%s access percpu memory: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\n\tif (env->ops->btf_struct_access && !type_is_alloc(reg->type) && atype == BPF_WRITE) {\n\t\tif (!btf_is_kernel(reg->btf)) {\n\t\t\tverbose(env, \"verifier internal error: reg->btf must be kernel btf\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tret = env->ops->btf_struct_access(&env->log, reg, off, size);\n\t} else {\n\t\t \n\t\tif (atype != BPF_READ && !type_is_ptr_alloc_obj(reg->type)) {\n\t\t\tverbose(env, \"only read is supported\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (type_is_alloc(reg->type) && !type_is_non_owning_ref(reg->type) &&\n\t\t    !reg->ref_obj_id) {\n\t\t\tverbose(env, \"verifier internal error: ref_obj_id for allocated object must be non-zero\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tret = btf_struct_access(&env->log, reg, off, size, atype, &btf_id, &flag, &field_name);\n\t}\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ret != PTR_TO_BTF_ID) {\n\t\t \n\n\t} else if (type_flag(reg->type) & PTR_UNTRUSTED) {\n\t\t \n\t\tflag = PTR_UNTRUSTED;\n\n\t} else if (is_trusted_reg(reg) || is_rcu_reg(reg)) {\n\t\t \n\t\tif (type_is_trusted(env, reg, field_name, btf_id)) {\n\t\t\tflag |= PTR_TRUSTED;\n\t\t} else if (in_rcu_cs(env) && !type_may_be_null(reg->type)) {\n\t\t\tif (type_is_rcu(env, reg, field_name, btf_id)) {\n\t\t\t\t \n\t\t\t\tflag |= MEM_RCU;\n\t\t\t} else if (flag & MEM_RCU ||\n\t\t\t\t   type_is_rcu_or_null(env, reg, field_name, btf_id)) {\n\t\t\t\t \n\t\t\t\tflag |= MEM_RCU | PTR_MAYBE_NULL;\n\n\t\t\t\t \n\t\t\t\tif (type_is_rcu_or_null(env, reg, field_name, btf_id) &&\n\t\t\t\t    flag & PTR_UNTRUSTED)\n\t\t\t\t\tflag &= ~PTR_UNTRUSTED;\n\t\t\t} else if (flag & (MEM_PERCPU | MEM_USER)) {\n\t\t\t\t \n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tclear_trusted_flags(&flag);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tflag = PTR_UNTRUSTED;\n\t\t}\n\t} else {\n\t\t \n\t\tclear_trusted_flags(&flag);\n\t}\n\n\tif (atype == BPF_READ && value_regno >= 0)\n\t\tmark_btf_ld_reg(env, regs, value_regno, ret, reg->btf, btf_id, flag);\n\n\treturn 0;\n}\n\nstatic int check_ptr_to_map_access(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs,\n\t\t\t\t   int regno, int off, int size,\n\t\t\t\t   enum bpf_access_type atype,\n\t\t\t\t   int value_regno)\n{\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_map *map = reg->map_ptr;\n\tstruct bpf_reg_state map_reg;\n\tenum bpf_type_flag flag = 0;\n\tconst struct btf_type *t;\n\tconst char *tname;\n\tu32 btf_id;\n\tint ret;\n\n\tif (!btf_vmlinux) {\n\t\tverbose(env, \"map_ptr access not supported without CONFIG_DEBUG_INFO_BTF\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\tif (!map->ops->map_btf_id || !*map->ops->map_btf_id) {\n\t\tverbose(env, \"map_ptr access not supported for map type %d\\n\",\n\t\t\tmap->map_type);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tt = btf_type_by_id(btf_vmlinux, *map->ops->map_btf_id);\n\ttname = btf_name_by_offset(btf_vmlinux, t->name_off);\n\n\tif (!env->allow_ptr_leaks) {\n\t\tverbose(env,\n\t\t\t\"'struct %s' access is allowed only to CAP_PERFMON and CAP_SYS_ADMIN\\n\",\n\t\t\ttname);\n\t\treturn -EPERM;\n\t}\n\n\tif (off < 0) {\n\t\tverbose(env, \"R%d is %s invalid negative access: off=%d\\n\",\n\t\t\tregno, tname, off);\n\t\treturn -EACCES;\n\t}\n\n\tif (atype != BPF_READ) {\n\t\tverbose(env, \"only read from %s is supported\\n\", tname);\n\t\treturn -EACCES;\n\t}\n\n\t \n\tmemset(&map_reg, 0, sizeof(map_reg));\n\tmark_btf_ld_reg(env, &map_reg, 0, PTR_TO_BTF_ID, btf_vmlinux, *map->ops->map_btf_id, 0);\n\tret = btf_struct_access(&env->log, &map_reg, off, size, atype, &btf_id, &flag, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (value_regno >= 0)\n\t\tmark_btf_ld_reg(env, regs, value_regno, ret, btf_vmlinux, btf_id, flag);\n\n\treturn 0;\n}\n\n \nstatic int check_stack_slot_within_bounds(struct bpf_verifier_env *env,\n                                          s64 off,\n                                          struct bpf_func_state *state,\n                                          enum bpf_access_type t)\n{\n\tint min_valid_off;\n\n\tif (t == BPF_WRITE || env->allow_uninit_stack)\n\t\tmin_valid_off = -MAX_BPF_STACK;\n\telse\n\t\tmin_valid_off = -state->allocated_stack;\n\n\tif (off < min_valid_off || off > -1)\n\t\treturn -EACCES;\n\treturn 0;\n}\n\n \nstatic int check_stack_access_within_bounds(\n\t\tstruct bpf_verifier_env *env,\n\t\tint regno, int off, int access_size,\n\t\tenum bpf_access_src src, enum bpf_access_type type)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tstruct bpf_func_state *state = func(env, reg);\n\ts64 min_off, max_off;\n\tint err;\n\tchar *err_extra;\n\n\tif (src == ACCESS_HELPER)\n\t\t \n\t\terr_extra = \" indirect access to\";\n\telse if (type == BPF_READ)\n\t\terr_extra = \" read from\";\n\telse\n\t\terr_extra = \" write to\";\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\tmin_off = (s64)reg->var_off.value + off;\n\t\tmax_off = min_off + access_size;\n\t} else {\n\t\tif (reg->smax_value >= BPF_MAX_VAR_OFF ||\n\t\t    reg->smin_value <= -BPF_MAX_VAR_OFF) {\n\t\t\tverbose(env, \"invalid unbounded variable-offset%s stack R%d\\n\",\n\t\t\t\terr_extra, regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmin_off = reg->smin_value + off;\n\t\tmax_off = reg->smax_value + off + access_size;\n\t}\n\n\terr = check_stack_slot_within_bounds(env, min_off, state, type);\n\tif (!err && max_off > 0)\n\t\terr = -EINVAL;  \n\n\tif (err) {\n\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"invalid%s stack R%d off=%d size=%d\\n\",\n\t\t\t\terr_extra, regno, off, access_size);\n\t\t} else {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"invalid variable-offset%s stack R%d var_off=%s size=%d\\n\",\n\t\t\t\terr_extra, regno, tn_buf, access_size);\n\t\t}\n\t\treturn err;\n\t}\n\n\treturn grow_stack_state(env, state, round_up(-min_off, BPF_REG_SIZE));\n}\n\n \nstatic int check_mem_access(struct bpf_verifier_env *env, int insn_idx, u32 regno,\n\t\t\t    int off, int bpf_size, enum bpf_access_type t,\n\t\t\t    int value_regno, bool strict_alignment_once, bool is_ldsx)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = regs + regno;\n\tint size, err = 0;\n\n\tsize = bpf_size_to_bytes(bpf_size);\n\tif (size < 0)\n\t\treturn size;\n\n\t \n\terr = check_ptr_alignment(env, reg, off, size, strict_alignment_once);\n\tif (err)\n\t\treturn err;\n\n\t \n\toff += reg->off;\n\n\tif (reg->type == PTR_TO_MAP_KEY) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"write to change key R%d not allowed\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->map_ptr->key_size, false);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_MAP_VALUE) {\n\t\tstruct btf_field *kptr_field = NULL;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into map\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_map_access_type(env, regno, off, size, t);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_map_access(env, regno, off, size, false, ACCESS_DIRECT);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\tkptr_field = btf_record_find(reg->map_ptr->record,\n\t\t\t\t\t\t     off + reg->var_off.value, BPF_KPTR);\n\t\tif (kptr_field) {\n\t\t\terr = check_map_kptr_access(env, regno, value_regno, insn_idx, kptr_field);\n\t\t} else if (t == BPF_READ && value_regno >= 0) {\n\t\t\tstruct bpf_map *map = reg->map_ptr;\n\n\t\t\t \n\t\t\tif (tnum_is_const(reg->var_off) &&\n\t\t\t    bpf_map_is_rdonly(map) &&\n\t\t\t    map->ops->map_direct_value_addr) {\n\t\t\t\tint map_off = off + reg->var_off.value;\n\t\t\t\tu64 val = 0;\n\n\t\t\t\terr = bpf_map_direct_read(map, map_off, size,\n\t\t\t\t\t\t\t  &val, is_ldsx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tregs[value_regno].type = SCALAR_VALUE;\n\t\t\t\t__mark_reg_known(&regs[value_regno], val);\n\t\t\t} else {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t}\n\t\t}\n\t} else if (base_type(reg->type) == PTR_TO_MEM) {\n\t\tbool rdonly_mem = type_is_rdonly_mem(reg->type);\n\n\t\tif (type_may_be_null(reg->type)) {\n\t\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (t == BPF_WRITE && rdonly_mem) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into mem\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_mem_region_access(env, regno, off, size,\n\t\t\t\t\t      reg->mem_size, false);\n\t\tif (!err && value_regno >= 0 && (t == BPF_READ || rdonly_mem))\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_CTX) {\n\t\tenum bpf_reg_type reg_type = SCALAR_VALUE;\n\t\tstruct btf *btf = NULL;\n\t\tu32 btf_id = 0;\n\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into ctx\\n\", value_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_ptr_off_reg(env, reg, regno);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = check_ctx_access(env, insn_idx, off, size, t, &reg_type, &btf,\n\t\t\t\t       &btf_id);\n\t\tif (err)\n\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\tif (!err && t == BPF_READ && value_regno >= 0) {\n\t\t\t \n\t\t\tif (reg_type == SCALAR_VALUE) {\n\t\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t\t\t} else {\n\t\t\t\tmark_reg_known_zero(env, regs,\n\t\t\t\t\t\t    value_regno);\n\t\t\t\tif (type_may_be_null(reg_type))\n\t\t\t\t\tregs[value_regno].id = ++env->id_gen;\n\t\t\t\t \n\t\t\t\tregs[value_regno].subreg_def = DEF_NOT_SUBREG;\n\t\t\t\tif (base_type(reg_type) == PTR_TO_BTF_ID) {\n\t\t\t\t\tregs[value_regno].btf = btf;\n\t\t\t\t\tregs[value_regno].btf_id = btf_id;\n\t\t\t\t}\n\t\t\t}\n\t\t\tregs[value_regno].type = reg_type;\n\t\t}\n\n\t} else if (reg->type == PTR_TO_STACK) {\n\t\t \n\t\terr = check_stack_access_within_bounds(env, regno, off, size, ACCESS_DIRECT, t);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (t == BPF_READ)\n\t\t\terr = check_stack_read(env, regno, off, size,\n\t\t\t\t\t       value_regno);\n\t\telse\n\t\t\terr = check_stack_write(env, regno, off, size,\n\t\t\t\t\t\tvalue_regno, insn_idx);\n\t} else if (reg_is_pkt_pointer(reg)) {\n\t\tif (t == BPF_WRITE && !may_access_direct_pkt_data(env, NULL, t)) {\n\t\t\tverbose(env, \"cannot write into packet\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into packet\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_packet_access(env, regno, off, size, false);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_FLOW_KEYS) {\n\t\tif (t == BPF_WRITE && value_regno >= 0 &&\n\t\t    is_pointer_value(env, value_regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into flow keys\\n\",\n\t\t\t\tvalue_regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_flow_keys_access(env, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (type_is_sk_pointer(reg->type)) {\n\t\tif (t == BPF_WRITE) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\tregno, reg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_sock_access(env, insn_idx, regno, off, size, t);\n\t\tif (!err && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (reg->type == PTR_TO_TP_BUFFER) {\n\t\terr = check_tp_buffer_access(env, reg, regno, off, size);\n\t\tif (!err && t == BPF_READ && value_regno >= 0)\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else if (base_type(reg->type) == PTR_TO_BTF_ID &&\n\t\t   !type_may_be_null(reg->type)) {\n\t\terr = check_ptr_to_btf_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (reg->type == CONST_PTR_TO_MAP) {\n\t\terr = check_ptr_to_map_access(env, regs, regno, off, size, t,\n\t\t\t\t\t      value_regno);\n\t} else if (base_type(reg->type) == PTR_TO_BUF) {\n\t\tbool rdonly_mem = type_is_rdonly_mem(reg->type);\n\t\tu32 *max_access;\n\n\t\tif (rdonly_mem) {\n\t\t\tif (t == BPF_WRITE) {\n\t\t\t\tverbose(env, \"R%d cannot write into %s\\n\",\n\t\t\t\t\tregno, reg_type_str(env, reg->type));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t\tmax_access = &env->prog->aux->max_rdonly_access;\n\t\t} else {\n\t\t\tmax_access = &env->prog->aux->max_rdwr_access;\n\t\t}\n\n\t\terr = check_buffer_access(env, reg, regno, off, size, false,\n\t\t\t\t\t  max_access);\n\n\t\tif (!err && value_regno >= 0 && (rdonly_mem || t == BPF_READ))\n\t\t\tmark_reg_unknown(env, regs, value_regno);\n\t} else {\n\t\tverbose(env, \"R%d invalid mem access '%s'\\n\", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tif (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&\n\t    regs[value_regno].type == SCALAR_VALUE) {\n\t\tif (!is_ldsx)\n\t\t\t \n\t\t\tcoerce_reg_to_size(&regs[value_regno], size);\n\t\telse\n\t\t\tcoerce_reg_to_size_sx(&regs[value_regno], size);\n\t}\n\treturn err;\n}\n\nstatic int check_atomic(struct bpf_verifier_env *env, int insn_idx, struct bpf_insn *insn)\n{\n\tint load_reg;\n\tint err;\n\n\tswitch (insn->imm) {\n\tcase BPF_ADD:\n\tcase BPF_ADD | BPF_FETCH:\n\tcase BPF_AND:\n\tcase BPF_AND | BPF_FETCH:\n\tcase BPF_OR:\n\tcase BPF_OR | BPF_FETCH:\n\tcase BPF_XOR:\n\tcase BPF_XOR | BPF_FETCH:\n\tcase BPF_XCHG:\n\tcase BPF_CMPXCHG:\n\t\tbreak;\n\tdefault:\n\t\tverbose(env, \"BPF_ATOMIC uses invalid atomic opcode %02x\\n\", insn->imm);\n\t\treturn -EINVAL;\n\t}\n\n\tif (BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid atomic operand size\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (insn->imm == BPF_CMPXCHG) {\n\t\t \n\t\tconst u32 aux_reg = BPF_REG_0;\n\n\t\terr = check_reg_arg(env, aux_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, aux_reg)) {\n\t\t\tverbose(env, \"R%d leaks addr into mem\\n\", aux_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\tif (is_pointer_value(env, insn->src_reg)) {\n\t\tverbose(env, \"R%d leaks addr into mem\\n\", insn->src_reg);\n\t\treturn -EACCES;\n\t}\n\n\tif (is_ctx_reg(env, insn->dst_reg) ||\n\t    is_pkt_reg(env, insn->dst_reg) ||\n\t    is_flow_key_reg(env, insn->dst_reg) ||\n\t    is_sk_reg(env, insn->dst_reg)) {\n\t\tverbose(env, \"BPF_ATOMIC stores into R%d %s is not allowed\\n\",\n\t\t\tinsn->dst_reg,\n\t\t\treg_type_str(env, reg_state(env, insn->dst_reg)->type));\n\t\treturn -EACCES;\n\t}\n\n\tif (insn->imm & BPF_FETCH) {\n\t\tif (insn->imm == BPF_CMPXCHG)\n\t\t\tload_reg = BPF_REG_0;\n\t\telse\n\t\t\tload_reg = insn->src_reg;\n\n\t\t \n\t\terr = check_reg_arg(env, load_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\t \n\t\tload_reg = -1;\n\t}\n\n\t \n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_READ, -1, true, false);\n\tif (!err && load_reg >= 0)\n\t\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t\t       BPF_SIZE(insn->code), BPF_READ, load_reg,\n\t\t\t\t       true, false);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = check_mem_access(env, insn_idx, insn->dst_reg, insn->off,\n\t\t\t       BPF_SIZE(insn->code), BPF_WRITE, -1, true, false);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n \nstatic int check_stack_range_initialized(\n\t\tstruct bpf_verifier_env *env, int regno, int off,\n\t\tint access_size, bool zero_size_allowed,\n\t\tenum bpf_access_src type, struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *reg = reg_state(env, regno);\n\tstruct bpf_func_state *state = func(env, reg);\n\tint err, min_off, max_off, i, j, slot, spi;\n\tchar *err_extra = type == ACCESS_HELPER ? \" indirect\" : \"\";\n\tenum bpf_access_type bounds_check_type;\n\t \n\tbool clobber = false;\n\n\tif (access_size == 0 && !zero_size_allowed) {\n\t\tverbose(env, \"invalid zero-sized read\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (type == ACCESS_HELPER) {\n\t\t \n\t\tbounds_check_type = BPF_WRITE;\n\t\tclobber = true;\n\t} else {\n\t\tbounds_check_type = BPF_READ;\n\t}\n\terr = check_stack_access_within_bounds(env, regno, off, access_size,\n\t\t\t\t\t       type, bounds_check_type);\n\tif (err)\n\t\treturn err;\n\n\n\tif (tnum_is_const(reg->var_off)) {\n\t\tmin_off = max_off = reg->var_off.value + off;\n\t} else {\n\t\t \n\t\tif (!env->bypass_spec_v1) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"R%d%s variable offset stack access prohibited for !root, var_off=%s\\n\",\n\t\t\t\tregno, err_extra, tn_buf);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t \n\t\tif (meta && meta->raw_mode)\n\t\t\tmeta = NULL;\n\n\t\tmin_off = reg->smin_value + off;\n\t\tmax_off = reg->smax_value + off;\n\t}\n\n\tif (meta && meta->raw_mode) {\n\t\t \n\t\tfor (i = min_off; i < max_off + access_size; i++) {\n\t\t\tint stack_off = -i - 1;\n\n\t\t\tspi = __get_spi(i);\n\t\t\t \n\t\t\tif (state->allocated_stack <= stack_off)\n\t\t\t\tcontinue;\n\t\t\tif (state->stack[spi].slot_type[stack_off % BPF_REG_SIZE] == STACK_DYNPTR) {\n\t\t\t\tverbose(env, \"potential write to dynptr at off=%d disallowed\\n\", i);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\t\tmeta->access_size = access_size;\n\t\tmeta->regno = regno;\n\t\treturn 0;\n\t}\n\n\tfor (i = min_off; i < max_off + access_size; i++) {\n\t\tu8 *stype;\n\n\t\tslot = -i - 1;\n\t\tspi = slot / BPF_REG_SIZE;\n\t\tif (state->allocated_stack <= slot) {\n\t\t\tverbose(env, \"verifier bug: allocated_stack too small\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tstype = &state->stack[spi].slot_type[slot % BPF_REG_SIZE];\n\t\tif (*stype == STACK_MISC)\n\t\t\tgoto mark;\n\t\tif ((*stype == STACK_ZERO) ||\n\t\t    (*stype == STACK_INVALID && env->allow_uninit_stack)) {\n\t\t\tif (clobber) {\n\t\t\t\t \n\t\t\t\t*stype = STACK_MISC;\n\t\t\t}\n\t\t\tgoto mark;\n\t\t}\n\n\t\tif (is_spilled_reg(&state->stack[spi]) &&\n\t\t    (state->stack[spi].spilled_ptr.type == SCALAR_VALUE ||\n\t\t     env->allow_ptr_leaks)) {\n\t\t\tif (clobber) {\n\t\t\t\t__mark_reg_unknown(env, &state->stack[spi].spilled_ptr);\n\t\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\t\tscrub_spilled_slot(&state->stack[spi].slot_type[j]);\n\t\t\t}\n\t\t\tgoto mark;\n\t\t}\n\n\t\tif (tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"invalid%s read from stack R%d off %d+%d size %d\\n\",\n\t\t\t\terr_extra, regno, min_off, i - min_off, access_size);\n\t\t} else {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\t\tverbose(env, \"invalid%s read from stack R%d var_off %s+%d size %d\\n\",\n\t\t\t\terr_extra, regno, tn_buf, i - min_off, access_size);\n\t\t}\n\t\treturn -EACCES;\nmark:\n\t\t \n\t\tmark_reg_read(env, &state->stack[spi].spilled_ptr,\n\t\t\t      state->stack[spi].spilled_ptr.parent,\n\t\t\t      REG_LIVE_READ64);\n\t\t \n\t}\n\treturn 0;\n}\n\nstatic int check_helper_mem_access(struct bpf_verifier_env *env, int regno,\n\t\t\t\t   int access_size, bool zero_size_allowed,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tu32 *max_access;\n\n\tswitch (base_type(reg->type)) {\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\t\treturn check_packet_access(env, regno, reg->off, access_size,\n\t\t\t\t\t   zero_size_allowed);\n\tcase PTR_TO_MAP_KEY:\n\t\tif (meta && meta->raw_mode) {\n\t\t\tverbose(env, \"R%d cannot write into %s\\n\", regno,\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn check_mem_region_access(env, regno, reg->off, access_size,\n\t\t\t\t\t       reg->map_ptr->key_size, false);\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access_type(env, regno, reg->off, access_size,\n\t\t\t\t\t  meta && meta->raw_mode ? BPF_WRITE :\n\t\t\t\t\t  BPF_READ))\n\t\t\treturn -EACCES;\n\t\treturn check_map_access(env, regno, reg->off, access_size,\n\t\t\t\t\tzero_size_allowed, ACCESS_HELPER);\n\tcase PTR_TO_MEM:\n\t\tif (type_is_rdonly_mem(reg->type)) {\n\t\t\tif (meta && meta->raw_mode) {\n\t\t\t\tverbose(env, \"R%d cannot write into %s\\n\", regno,\n\t\t\t\t\treg_type_str(env, reg->type));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\t\treturn check_mem_region_access(env, regno, reg->off,\n\t\t\t\t\t       access_size, reg->mem_size,\n\t\t\t\t\t       zero_size_allowed);\n\tcase PTR_TO_BUF:\n\t\tif (type_is_rdonly_mem(reg->type)) {\n\t\t\tif (meta && meta->raw_mode) {\n\t\t\t\tverbose(env, \"R%d cannot write into %s\\n\", regno,\n\t\t\t\t\treg_type_str(env, reg->type));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\tmax_access = &env->prog->aux->max_rdonly_access;\n\t\t} else {\n\t\t\tmax_access = &env->prog->aux->max_rdwr_access;\n\t\t}\n\t\treturn check_buffer_access(env, reg, regno, reg->off,\n\t\t\t\t\t   access_size, zero_size_allowed,\n\t\t\t\t\t   max_access);\n\tcase PTR_TO_STACK:\n\t\treturn check_stack_range_initialized(\n\t\t\t\tenv,\n\t\t\t\tregno, reg->off, access_size,\n\t\t\t\tzero_size_allowed, ACCESS_HELPER, meta);\n\tcase PTR_TO_BTF_ID:\n\t\treturn check_ptr_to_btf_access(env, regs, regno, reg->off,\n\t\t\t\t\t       access_size, BPF_READ, -1);\n\tcase PTR_TO_CTX:\n\t\t \n\t\tif (!env->ops->convert_ctx_access) {\n\t\t\tenum bpf_access_type atype = meta && meta->raw_mode ? BPF_WRITE : BPF_READ;\n\t\t\tint offset = access_size - 1;\n\n\t\t\t \n\t\t\tif (access_size == 0)\n\t\t\t\treturn zero_size_allowed ? 0 : -EACCES;\n\n\t\t\treturn check_mem_access(env, env->insn_idx, regno, offset, BPF_B,\n\t\t\t\t\t\tatype, -1, false, false);\n\t\t}\n\n\t\tfallthrough;\n\tdefault:  \n\t\t \n\t\tif (zero_size_allowed && access_size == 0 &&\n\t\t    register_is_null(reg))\n\t\t\treturn 0;\n\n\t\tverbose(env, \"R%d type=%s \", regno,\n\t\t\treg_type_str(env, reg->type));\n\t\tverbose(env, \"expected=%s\\n\", reg_type_str(env, PTR_TO_STACK));\n\t\treturn -EACCES;\n\t}\n}\n\nstatic int check_mem_size_reg(struct bpf_verifier_env *env,\n\t\t\t      struct bpf_reg_state *reg, u32 regno,\n\t\t\t      bool zero_size_allowed,\n\t\t\t      struct bpf_call_arg_meta *meta)\n{\n\tint err;\n\n\t \n\tmeta->msize_max_value = reg->umax_value;\n\n\t \n\tif (!tnum_is_const(reg->var_off))\n\t\t \n\t\tmeta = NULL;\n\n\tif (reg->smin_value < 0) {\n\t\tverbose(env, \"R%d min value is negative, either use unsigned or 'var &= const'\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\n\tif (reg->umin_value == 0) {\n\t\terr = check_helper_mem_access(env, regno - 1, 0,\n\t\t\t\t\t      zero_size_allowed,\n\t\t\t\t\t      meta);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (reg->umax_value >= BPF_MAX_VAR_SIZ) {\n\t\tverbose(env, \"R%d unbounded memory access, use 'var &= const' or 'if (var < const)'\\n\",\n\t\t\tregno);\n\t\treturn -EACCES;\n\t}\n\terr = check_helper_mem_access(env, regno - 1,\n\t\t\t\t      reg->umax_value,\n\t\t\t\t      zero_size_allowed, meta);\n\tif (!err)\n\t\terr = mark_chain_precision(env, regno);\n\treturn err;\n}\n\nint check_mem_reg(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t   u32 regno, u32 mem_size)\n{\n\tbool may_be_null = type_may_be_null(reg->type);\n\tstruct bpf_reg_state saved_reg;\n\tstruct bpf_call_arg_meta meta;\n\tint err;\n\n\tif (register_is_null(reg))\n\t\treturn 0;\n\n\tmemset(&meta, 0, sizeof(meta));\n\t \n\tif (may_be_null) {\n\t\tsaved_reg = *reg;\n\t\tmark_ptr_not_null_reg(reg);\n\t}\n\n\terr = check_helper_mem_access(env, regno, mem_size, true, &meta);\n\t \n\tmeta.raw_mode = true;\n\terr = err ?: check_helper_mem_access(env, regno, mem_size, true, &meta);\n\n\tif (may_be_null)\n\t\t*reg = saved_reg;\n\n\treturn err;\n}\n\nstatic int check_kfunc_mem_size_reg(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t\t\t    u32 regno)\n{\n\tstruct bpf_reg_state *mem_reg = &cur_regs(env)[regno - 1];\n\tbool may_be_null = type_may_be_null(mem_reg->type);\n\tstruct bpf_reg_state saved_reg;\n\tstruct bpf_call_arg_meta meta;\n\tint err;\n\n\tWARN_ON_ONCE(regno < BPF_REG_2 || regno > BPF_REG_5);\n\n\tmemset(&meta, 0, sizeof(meta));\n\n\tif (may_be_null) {\n\t\tsaved_reg = *mem_reg;\n\t\tmark_ptr_not_null_reg(mem_reg);\n\t}\n\n\terr = check_mem_size_reg(env, reg, regno, true, &meta);\n\t \n\tmeta.raw_mode = true;\n\terr = err ?: check_mem_size_reg(env, reg, regno, true, &meta);\n\n\tif (may_be_null)\n\t\t*mem_reg = saved_reg;\n\treturn err;\n}\n\n \nstatic int process_spin_lock(struct bpf_verifier_env *env, int regno,\n\t\t\t     bool is_lock)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tbool is_const = tnum_is_const(reg->var_off);\n\tu64 val = reg->var_off.value;\n\tstruct bpf_map *map = NULL;\n\tstruct btf *btf = NULL;\n\tstruct btf_record *rec;\n\n\tif (!is_const) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. bpf_spin_lock has to be at the constant offset\\n\",\n\t\t\tregno);\n\t\treturn -EINVAL;\n\t}\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tmap = reg->map_ptr;\n\t\tif (!map->btf) {\n\t\t\tverbose(env,\n\t\t\t\t\"map '%s' has to have BTF in order to use bpf_spin_lock\\n\",\n\t\t\t\tmap->name);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tbtf = reg->btf;\n\t}\n\n\trec = reg_btf_record(reg);\n\tif (!btf_record_has_field(rec, BPF_SPIN_LOCK)) {\n\t\tverbose(env, \"%s '%s' has no valid bpf_spin_lock\\n\", map ? \"map\" : \"local\",\n\t\t\tmap ? map->name : \"kptr\");\n\t\treturn -EINVAL;\n\t}\n\tif (rec->spin_lock_off != val + reg->off) {\n\t\tverbose(env, \"off %lld doesn't point to 'struct bpf_spin_lock' that is at %d\\n\",\n\t\t\tval + reg->off, rec->spin_lock_off);\n\t\treturn -EINVAL;\n\t}\n\tif (is_lock) {\n\t\tif (cur->active_lock.ptr) {\n\t\t\tverbose(env,\n\t\t\t\t\"Locking two bpf_spin_locks are not allowed\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (map)\n\t\t\tcur->active_lock.ptr = map;\n\t\telse\n\t\t\tcur->active_lock.ptr = btf;\n\t\tcur->active_lock.id = reg->id;\n\t} else {\n\t\tvoid *ptr;\n\n\t\tif (map)\n\t\t\tptr = map;\n\t\telse\n\t\t\tptr = btf;\n\n\t\tif (!cur->active_lock.ptr) {\n\t\t\tverbose(env, \"bpf_spin_unlock without taking a lock\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (cur->active_lock.ptr != ptr ||\n\t\t    cur->active_lock.id != reg->id) {\n\t\t\tverbose(env, \"bpf_spin_unlock of different lock\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinvalidate_non_owning_refs(env);\n\n\t\tcur->active_lock.ptr = NULL;\n\t\tcur->active_lock.id = 0;\n\t}\n\treturn 0;\n}\n\nstatic int process_timer_func(struct bpf_verifier_env *env, int regno,\n\t\t\t      struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tbool is_const = tnum_is_const(reg->var_off);\n\tstruct bpf_map *map = reg->map_ptr;\n\tu64 val = reg->var_off.value;\n\n\tif (!is_const) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. bpf_timer has to be at the constant offset\\n\",\n\t\t\tregno);\n\t\treturn -EINVAL;\n\t}\n\tif (!map->btf) {\n\t\tverbose(env, \"map '%s' has to have BTF in order to use bpf_timer\\n\",\n\t\t\tmap->name);\n\t\treturn -EINVAL;\n\t}\n\tif (!btf_record_has_field(map->record, BPF_TIMER)) {\n\t\tverbose(env, \"map '%s' has no valid bpf_timer\\n\", map->name);\n\t\treturn -EINVAL;\n\t}\n\tif (map->record->timer_off != val + reg->off) {\n\t\tverbose(env, \"off %lld doesn't point to 'struct bpf_timer' that is at %d\\n\",\n\t\t\tval + reg->off, map->record->timer_off);\n\t\treturn -EINVAL;\n\t}\n\tif (meta->map_ptr) {\n\t\tverbose(env, \"verifier bug. Two map pointers in a timer helper\\n\");\n\t\treturn -EFAULT;\n\t}\n\tmeta->map_uid = reg->map_uid;\n\tmeta->map_ptr = map;\n\treturn 0;\n}\n\nstatic int process_kptr_func(struct bpf_verifier_env *env, int regno,\n\t\t\t     struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tstruct bpf_map *map_ptr = reg->map_ptr;\n\tstruct btf_field *kptr_field;\n\tu32 kptr_off;\n\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. kptr has to be at the constant offset\\n\",\n\t\t\tregno);\n\t\treturn -EINVAL;\n\t}\n\tif (!map_ptr->btf) {\n\t\tverbose(env, \"map '%s' has to have BTF in order to use bpf_kptr_xchg\\n\",\n\t\t\tmap_ptr->name);\n\t\treturn -EINVAL;\n\t}\n\tif (!btf_record_has_field(map_ptr->record, BPF_KPTR)) {\n\t\tverbose(env, \"map '%s' has no valid kptr\\n\", map_ptr->name);\n\t\treturn -EINVAL;\n\t}\n\n\tmeta->map_ptr = map_ptr;\n\tkptr_off = reg->off + reg->var_off.value;\n\tkptr_field = btf_record_find(map_ptr->record, kptr_off, BPF_KPTR);\n\tif (!kptr_field) {\n\t\tverbose(env, \"off=%d doesn't point to kptr\\n\", kptr_off);\n\t\treturn -EACCES;\n\t}\n\tif (kptr_field->type != BPF_KPTR_REF) {\n\t\tverbose(env, \"off=%d kptr isn't referenced kptr\\n\", kptr_off);\n\t\treturn -EACCES;\n\t}\n\tmeta->kptr_field = kptr_field;\n\treturn 0;\n}\n\n \nstatic int process_dynptr_func(struct bpf_verifier_env *env, int regno, int insn_idx,\n\t\t\t       enum bpf_arg_type arg_type, int clone_ref_obj_id)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tint err;\n\n\t \n\tif ((arg_type & (MEM_UNINIT | MEM_RDONLY)) == (MEM_UNINIT | MEM_RDONLY)) {\n\t\tverbose(env, \"verifier internal error: misconfigured dynptr helper type flags\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tif (arg_type & MEM_UNINIT) {\n\t\tint i;\n\n\t\tif (!is_dynptr_reg_valid_uninit(env, reg)) {\n\t\t\tverbose(env, \"Dynptr has to be an uninitialized dynptr\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < BPF_DYNPTR_SIZE; i += 8) {\n\t\t\terr = check_mem_access(env, insn_idx, regno,\n\t\t\t\t\t       i, BPF_DW, BPF_WRITE, -1, false, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\terr = mark_stack_slots_dynptr(env, reg, arg_type, insn_idx, clone_ref_obj_id);\n\t} else   {\n\t\t \n\t\tif (reg->type == CONST_PTR_TO_DYNPTR && !(arg_type & MEM_RDONLY)) {\n\t\t\tverbose(env, \"cannot pass pointer to const bpf_dynptr, the helper mutates it\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!is_dynptr_reg_valid_init(env, reg)) {\n\t\t\tverbose(env,\n\t\t\t\t\"Expected an initialized dynptr as arg #%d\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (!is_dynptr_type_expected(env, reg, arg_type & ~MEM_RDONLY)) {\n\t\t\tverbose(env,\n\t\t\t\t\"Expected a dynptr of type %s as arg #%d\\n\",\n\t\t\t\tdynptr_type_str(arg_to_dynptr_type(arg_type)), regno);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\terr = mark_dynptr_read(env, reg);\n\t}\n\treturn err;\n}\n\nstatic u32 iter_ref_obj_id(struct bpf_verifier_env *env, struct bpf_reg_state *reg, int spi)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\n\treturn state->stack[spi].spilled_ptr.ref_obj_id;\n}\n\nstatic bool is_iter_kfunc(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & (KF_ITER_NEW | KF_ITER_NEXT | KF_ITER_DESTROY);\n}\n\nstatic bool is_iter_new_kfunc(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_ITER_NEW;\n}\n\nstatic bool is_iter_next_kfunc(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_ITER_NEXT;\n}\n\nstatic bool is_iter_destroy_kfunc(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_ITER_DESTROY;\n}\n\nstatic bool is_kfunc_arg_iter(struct bpf_kfunc_call_arg_meta *meta, int arg)\n{\n\t \n\treturn arg == 0 && is_iter_kfunc(meta);\n}\n\nstatic int process_iter_arg(struct bpf_verifier_env *env, int regno, int insn_idx,\n\t\t\t    struct bpf_kfunc_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tconst struct btf_type *t;\n\tconst struct btf_param *arg;\n\tint spi, err, i, nr_slots;\n\tu32 btf_id;\n\n\t \n\targ = &btf_params(meta->func_proto)[0];\n\tt = btf_type_skip_modifiers(meta->btf, arg->type, NULL);\t \n\tt = btf_type_skip_modifiers(meta->btf, t->type, &btf_id);\t \n\tnr_slots = t->size / BPF_REG_SIZE;\n\n\tif (is_iter_new_kfunc(meta)) {\n\t\t \n\t\tif (!is_iter_reg_valid_uninit(env, reg, nr_slots)) {\n\t\t\tverbose(env, \"expected uninitialized iter_%s as arg #%d\\n\",\n\t\t\t\titer_type_str(meta->btf, btf_id), regno);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor (i = 0; i < nr_slots * 8; i += BPF_REG_SIZE) {\n\t\t\terr = check_mem_access(env, insn_idx, regno,\n\t\t\t\t\t       i, BPF_DW, BPF_WRITE, -1, false, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\terr = mark_stack_slots_iter(env, reg, insn_idx, meta->btf, btf_id, nr_slots);\n\t\tif (err)\n\t\t\treturn err;\n\t} else {\n\t\t \n\t\tif (!is_iter_reg_valid_init(env, reg, meta->btf, btf_id, nr_slots)) {\n\t\t\tverbose(env, \"expected an initialized iter_%s as arg #%d\\n\",\n\t\t\t\titer_type_str(meta->btf, btf_id), regno);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tspi = iter_get_spi(env, reg, nr_slots);\n\t\tif (spi < 0)\n\t\t\treturn spi;\n\n\t\terr = mark_iter_read(env, reg, spi, nr_slots);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\tmeta->iter.spi = spi;\n\t\tmeta->iter.frameno = reg->frameno;\n\t\tmeta->ref_obj_id = iter_ref_obj_id(env, reg, spi);\n\n\t\tif (is_iter_destroy_kfunc(meta)) {\n\t\t\terr = unmark_stack_slots_iter(env, reg, nr_slots);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int process_iter_next_call(struct bpf_verifier_env *env, int insn_idx,\n\t\t\t\t  struct bpf_kfunc_call_arg_meta *meta)\n{\n\tstruct bpf_verifier_state *cur_st = env->cur_state, *queued_st;\n\tstruct bpf_func_state *cur_fr = cur_st->frame[cur_st->curframe], *queued_fr;\n\tstruct bpf_reg_state *cur_iter, *queued_iter;\n\tint iter_frameno = meta->iter.frameno;\n\tint iter_spi = meta->iter.spi;\n\n\tBTF_TYPE_EMIT(struct bpf_iter);\n\n\tcur_iter = &env->cur_state->frame[iter_frameno]->stack[iter_spi].spilled_ptr;\n\n\tif (cur_iter->iter.state != BPF_ITER_STATE_ACTIVE &&\n\t    cur_iter->iter.state != BPF_ITER_STATE_DRAINED) {\n\t\tverbose(env, \"verifier internal error: unexpected iterator state %d (%s)\\n\",\n\t\t\tcur_iter->iter.state, iter_state_str(cur_iter->iter.state));\n\t\treturn -EFAULT;\n\t}\n\n\tif (cur_iter->iter.state == BPF_ITER_STATE_ACTIVE) {\n\t\t \n\t\tqueued_st = push_stack(env, insn_idx + 1, insn_idx, false);\n\t\tif (!queued_st)\n\t\t\treturn -ENOMEM;\n\n\t\tqueued_iter = &queued_st->frame[iter_frameno]->stack[iter_spi].spilled_ptr;\n\t\tqueued_iter->iter.state = BPF_ITER_STATE_ACTIVE;\n\t\tqueued_iter->iter.depth++;\n\n\t\tqueued_fr = queued_st->frame[queued_st->curframe];\n\t\tmark_ptr_not_null_reg(&queued_fr->regs[BPF_REG_0]);\n\t}\n\n\t \n\t \n\tcur_iter->iter.state = BPF_ITER_STATE_DRAINED;\n\t__mark_reg_const_zero(&cur_fr->regs[BPF_REG_0]);\n\n\treturn 0;\n}\n\nstatic bool arg_type_is_mem_size(enum bpf_arg_type type)\n{\n\treturn type == ARG_CONST_SIZE ||\n\t       type == ARG_CONST_SIZE_OR_ZERO;\n}\n\nstatic bool arg_type_is_release(enum bpf_arg_type type)\n{\n\treturn type & OBJ_RELEASE;\n}\n\nstatic bool arg_type_is_dynptr(enum bpf_arg_type type)\n{\n\treturn base_type(type) == ARG_PTR_TO_DYNPTR;\n}\n\nstatic int int_ptr_type_to_size(enum bpf_arg_type type)\n{\n\tif (type == ARG_PTR_TO_INT)\n\t\treturn sizeof(u32);\n\telse if (type == ARG_PTR_TO_LONG)\n\t\treturn sizeof(u64);\n\n\treturn -EINVAL;\n}\n\nstatic int resolve_map_arg_type(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_call_arg_meta *meta,\n\t\t\t\t enum bpf_arg_type *arg_type)\n{\n\tif (!meta->map_ptr) {\n\t\t \n\t\tverbose(env, \"invalid map_ptr to access map->type\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tswitch (meta->map_ptr->map_type) {\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\tif (*arg_type == ARG_PTR_TO_MAP_VALUE) {\n\t\t\t*arg_type = ARG_PTR_TO_BTF_ID_SOCK_COMMON;\n\t\t} else {\n\t\t\tverbose(env, \"invalid arg_type for sockmap/sockhash\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_MAP_TYPE_BLOOM_FILTER:\n\t\tif (meta->func_id == BPF_FUNC_map_peek_elem)\n\t\t\t*arg_type = ARG_PTR_TO_MAP_VALUE;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstruct bpf_reg_types {\n\tconst enum bpf_reg_type types[10];\n\tu32 *btf_id;\n};\n\nstatic const struct bpf_reg_types sock_types = {\n\t.types = {\n\t\tPTR_TO_SOCK_COMMON,\n\t\tPTR_TO_SOCKET,\n\t\tPTR_TO_TCP_SOCK,\n\t\tPTR_TO_XDP_SOCK,\n\t},\n};\n\n#ifdef CONFIG_NET\nstatic const struct bpf_reg_types btf_id_sock_common_types = {\n\t.types = {\n\t\tPTR_TO_SOCK_COMMON,\n\t\tPTR_TO_SOCKET,\n\t\tPTR_TO_TCP_SOCK,\n\t\tPTR_TO_XDP_SOCK,\n\t\tPTR_TO_BTF_ID,\n\t\tPTR_TO_BTF_ID | PTR_TRUSTED,\n\t},\n\t.btf_id = &btf_sock_ids[BTF_SOCK_TYPE_SOCK_COMMON],\n};\n#endif\n\nstatic const struct bpf_reg_types mem_types = {\n\t.types = {\n\t\tPTR_TO_STACK,\n\t\tPTR_TO_PACKET,\n\t\tPTR_TO_PACKET_META,\n\t\tPTR_TO_MAP_KEY,\n\t\tPTR_TO_MAP_VALUE,\n\t\tPTR_TO_MEM,\n\t\tPTR_TO_MEM | MEM_RINGBUF,\n\t\tPTR_TO_BUF,\n\t\tPTR_TO_BTF_ID | PTR_TRUSTED,\n\t},\n};\n\nstatic const struct bpf_reg_types int_ptr_types = {\n\t.types = {\n\t\tPTR_TO_STACK,\n\t\tPTR_TO_PACKET,\n\t\tPTR_TO_PACKET_META,\n\t\tPTR_TO_MAP_KEY,\n\t\tPTR_TO_MAP_VALUE,\n\t},\n};\n\nstatic const struct bpf_reg_types spin_lock_types = {\n\t.types = {\n\t\tPTR_TO_MAP_VALUE,\n\t\tPTR_TO_BTF_ID | MEM_ALLOC,\n\t}\n};\n\nstatic const struct bpf_reg_types fullsock_types = { .types = { PTR_TO_SOCKET } };\nstatic const struct bpf_reg_types scalar_types = { .types = { SCALAR_VALUE } };\nstatic const struct bpf_reg_types context_types = { .types = { PTR_TO_CTX } };\nstatic const struct bpf_reg_types ringbuf_mem_types = { .types = { PTR_TO_MEM | MEM_RINGBUF } };\nstatic const struct bpf_reg_types const_map_ptr_types = { .types = { CONST_PTR_TO_MAP } };\nstatic const struct bpf_reg_types btf_ptr_types = {\n\t.types = {\n\t\tPTR_TO_BTF_ID,\n\t\tPTR_TO_BTF_ID | PTR_TRUSTED,\n\t\tPTR_TO_BTF_ID | MEM_RCU,\n\t},\n};\nstatic const struct bpf_reg_types percpu_btf_ptr_types = {\n\t.types = {\n\t\tPTR_TO_BTF_ID | MEM_PERCPU,\n\t\tPTR_TO_BTF_ID | MEM_PERCPU | PTR_TRUSTED,\n\t}\n};\nstatic const struct bpf_reg_types func_ptr_types = { .types = { PTR_TO_FUNC } };\nstatic const struct bpf_reg_types stack_ptr_types = { .types = { PTR_TO_STACK } };\nstatic const struct bpf_reg_types const_str_ptr_types = { .types = { PTR_TO_MAP_VALUE } };\nstatic const struct bpf_reg_types timer_types = { .types = { PTR_TO_MAP_VALUE } };\nstatic const struct bpf_reg_types kptr_types = { .types = { PTR_TO_MAP_VALUE } };\nstatic const struct bpf_reg_types dynptr_types = {\n\t.types = {\n\t\tPTR_TO_STACK,\n\t\tCONST_PTR_TO_DYNPTR,\n\t}\n};\n\nstatic const struct bpf_reg_types *compatible_reg_types[__BPF_ARG_TYPE_MAX] = {\n\t[ARG_PTR_TO_MAP_KEY]\t\t= &mem_types,\n\t[ARG_PTR_TO_MAP_VALUE]\t\t= &mem_types,\n\t[ARG_CONST_SIZE]\t\t= &scalar_types,\n\t[ARG_CONST_SIZE_OR_ZERO]\t= &scalar_types,\n\t[ARG_CONST_ALLOC_SIZE_OR_ZERO]\t= &scalar_types,\n\t[ARG_CONST_MAP_PTR]\t\t= &const_map_ptr_types,\n\t[ARG_PTR_TO_CTX]\t\t= &context_types,\n\t[ARG_PTR_TO_SOCK_COMMON]\t= &sock_types,\n#ifdef CONFIG_NET\n\t[ARG_PTR_TO_BTF_ID_SOCK_COMMON]\t= &btf_id_sock_common_types,\n#endif\n\t[ARG_PTR_TO_SOCKET]\t\t= &fullsock_types,\n\t[ARG_PTR_TO_BTF_ID]\t\t= &btf_ptr_types,\n\t[ARG_PTR_TO_SPIN_LOCK]\t\t= &spin_lock_types,\n\t[ARG_PTR_TO_MEM]\t\t= &mem_types,\n\t[ARG_PTR_TO_RINGBUF_MEM]\t= &ringbuf_mem_types,\n\t[ARG_PTR_TO_INT]\t\t= &int_ptr_types,\n\t[ARG_PTR_TO_LONG]\t\t= &int_ptr_types,\n\t[ARG_PTR_TO_PERCPU_BTF_ID]\t= &percpu_btf_ptr_types,\n\t[ARG_PTR_TO_FUNC]\t\t= &func_ptr_types,\n\t[ARG_PTR_TO_STACK]\t\t= &stack_ptr_types,\n\t[ARG_PTR_TO_CONST_STR]\t\t= &const_str_ptr_types,\n\t[ARG_PTR_TO_TIMER]\t\t= &timer_types,\n\t[ARG_PTR_TO_KPTR]\t\t= &kptr_types,\n\t[ARG_PTR_TO_DYNPTR]\t\t= &dynptr_types,\n};\n\nstatic int check_reg_type(struct bpf_verifier_env *env, u32 regno,\n\t\t\t  enum bpf_arg_type arg_type,\n\t\t\t  const u32 *arg_btf_id,\n\t\t\t  struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_reg_type expected, type = reg->type;\n\tconst struct bpf_reg_types *compatible;\n\tint i, j;\n\n\tcompatible = compatible_reg_types[base_type(arg_type)];\n\tif (!compatible) {\n\t\tverbose(env, \"verifier internal error: unsupported arg type %d\\n\", arg_type);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tif (arg_type & MEM_RDONLY)\n\t\ttype &= ~MEM_RDONLY;\n\tif (arg_type & PTR_MAYBE_NULL)\n\t\ttype &= ~PTR_MAYBE_NULL;\n\tif (base_type(arg_type) == ARG_PTR_TO_MEM)\n\t\ttype &= ~DYNPTR_TYPE_FLAG_MASK;\n\n\tif (meta->func_id == BPF_FUNC_kptr_xchg && type_is_alloc(type))\n\t\ttype &= ~MEM_ALLOC;\n\n\tfor (i = 0; i < ARRAY_SIZE(compatible->types); i++) {\n\t\texpected = compatible->types[i];\n\t\tif (expected == NOT_INIT)\n\t\t\tbreak;\n\n\t\tif (type == expected)\n\t\t\tgoto found;\n\t}\n\n\tverbose(env, \"R%d type=%s expected=\", regno, reg_type_str(env, reg->type));\n\tfor (j = 0; j + 1 < i; j++)\n\t\tverbose(env, \"%s, \", reg_type_str(env, compatible->types[j]));\n\tverbose(env, \"%s\\n\", reg_type_str(env, compatible->types[j]));\n\treturn -EACCES;\n\nfound:\n\tif (base_type(reg->type) != PTR_TO_BTF_ID)\n\t\treturn 0;\n\n\tif (compatible == &mem_types) {\n\t\tif (!(arg_type & MEM_RDONLY)) {\n\t\t\tverbose(env,\n\t\t\t\t\"%s() may write into memory pointed by R%d type=%s\\n\",\n\t\t\t\tfunc_id_name(meta->func_id),\n\t\t\t\tregno, reg_type_str(env, reg->type));\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch ((int)reg->type) {\n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID | PTR_TRUSTED:\n\tcase PTR_TO_BTF_ID | MEM_RCU:\n\tcase PTR_TO_BTF_ID | PTR_MAYBE_NULL:\n\tcase PTR_TO_BTF_ID | PTR_MAYBE_NULL | MEM_RCU:\n\t{\n\t\t \n\t\tbool strict_type_match = arg_type_is_release(arg_type) &&\n\t\t\t\t\t meta->func_id != BPF_FUNC_sk_release;\n\n\t\tif (type_may_be_null(reg->type) &&\n\t\t    (!type_may_be_null(arg_type) || arg_type_is_release(arg_type))) {\n\t\t\tverbose(env, \"Possibly NULL pointer passed to helper arg%d\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (!arg_btf_id) {\n\t\t\tif (!compatible->btf_id) {\n\t\t\t\tverbose(env, \"verifier internal error: missing arg compatible BTF ID\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\targ_btf_id = compatible->btf_id;\n\t\t}\n\n\t\tif (meta->func_id == BPF_FUNC_kptr_xchg) {\n\t\t\tif (map_kptr_match_type(env, meta->kptr_field, reg, regno))\n\t\t\t\treturn -EACCES;\n\t\t} else {\n\t\t\tif (arg_btf_id == BPF_PTR_POISON) {\n\t\t\t\tverbose(env, \"verifier internal error:\");\n\t\t\t\tverbose(env, \"R%d has non-overwritten BPF_PTR_POISON type\\n\",\n\t\t\t\t\tregno);\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\tif (!btf_struct_ids_match(&env->log, reg->btf, reg->btf_id, reg->off,\n\t\t\t\t\t\t  btf_vmlinux, *arg_btf_id,\n\t\t\t\t\t\t  strict_type_match)) {\n\t\t\t\tverbose(env, \"R%d is of type %s but %s is expected\\n\",\n\t\t\t\t\tregno, btf_type_name(reg->btf, reg->btf_id),\n\t\t\t\t\tbtf_type_name(btf_vmlinux, *arg_btf_id));\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\tcase PTR_TO_BTF_ID | MEM_ALLOC:\n\t\tif (meta->func_id != BPF_FUNC_spin_lock && meta->func_id != BPF_FUNC_spin_unlock &&\n\t\t    meta->func_id != BPF_FUNC_kptr_xchg) {\n\t\t\tverbose(env, \"verifier internal error: unimplemented handling of MEM_ALLOC\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (meta->func_id == BPF_FUNC_kptr_xchg) {\n\t\t\tif (map_kptr_match_type(env, meta->kptr_field, reg, regno))\n\t\t\t\treturn -EACCES;\n\t\t}\n\t\tbreak;\n\tcase PTR_TO_BTF_ID | MEM_PERCPU:\n\tcase PTR_TO_BTF_ID | MEM_PERCPU | PTR_TRUSTED:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tverbose(env, \"verifier internal error: invalid PTR_TO_BTF_ID register for type match\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\nstatic struct btf_field *\nreg_find_field_offset(const struct bpf_reg_state *reg, s32 off, u32 fields)\n{\n\tstruct btf_field *field;\n\tstruct btf_record *rec;\n\n\trec = reg_btf_record(reg);\n\tif (!rec)\n\t\treturn NULL;\n\n\tfield = btf_record_find(rec, off, fields);\n\tif (!field)\n\t\treturn NULL;\n\n\treturn field;\n}\n\nint check_func_arg_reg_off(struct bpf_verifier_env *env,\n\t\t\t   const struct bpf_reg_state *reg, int regno,\n\t\t\t   enum bpf_arg_type arg_type)\n{\n\tu32 type = reg->type;\n\n\t \n\tif (arg_type_is_release(arg_type)) {\n\t\t \n\t\tif (arg_type_is_dynptr(arg_type) && type == PTR_TO_STACK)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (reg->off) {\n\t\t\tverbose(env, \"R%d must have zero offset when passed to release func or trusted arg to kfunc\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn __check_ptr_off_reg(env, reg, regno, false);\n\t}\n\n\tswitch (type) {\n\t \n\tcase PTR_TO_STACK:\n\tcase PTR_TO_PACKET:\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MEM:\n\tcase PTR_TO_MEM | MEM_RDONLY:\n\tcase PTR_TO_MEM | MEM_RINGBUF:\n\tcase PTR_TO_BUF:\n\tcase PTR_TO_BUF | MEM_RDONLY:\n\tcase SCALAR_VALUE:\n\t\treturn 0;\n\t \n\tcase PTR_TO_BTF_ID:\n\tcase PTR_TO_BTF_ID | MEM_ALLOC:\n\tcase PTR_TO_BTF_ID | PTR_TRUSTED:\n\tcase PTR_TO_BTF_ID | MEM_RCU:\n\tcase PTR_TO_BTF_ID | MEM_ALLOC | NON_OWN_REF:\n\tcase PTR_TO_BTF_ID | MEM_ALLOC | NON_OWN_REF | MEM_RCU:\n\t\t \n\t\treturn __check_ptr_off_reg(env, reg, regno, true);\n\tdefault:\n\t\treturn __check_ptr_off_reg(env, reg, regno, false);\n\t}\n}\n\nstatic struct bpf_reg_state *get_dynptr_arg_reg(struct bpf_verifier_env *env,\n\t\t\t\t\t\tconst struct bpf_func_proto *fn,\n\t\t\t\t\t\tstruct bpf_reg_state *regs)\n{\n\tstruct bpf_reg_state *state = NULL;\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_FUNC_REG_ARGS; i++)\n\t\tif (arg_type_is_dynptr(fn->arg_type[i])) {\n\t\t\tif (state) {\n\t\t\t\tverbose(env, \"verifier internal error: multiple dynptr args\\n\");\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t\tstate = &regs[BPF_REG_1 + i];\n\t\t}\n\n\tif (!state)\n\t\tverbose(env, \"verifier internal error: no dynptr arg found\\n\");\n\n\treturn state;\n}\n\nstatic int dynptr_id(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi;\n\n\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\treturn reg->id;\n\tspi = dynptr_get_spi(env, reg);\n\tif (spi < 0)\n\t\treturn spi;\n\treturn state->stack[spi].spilled_ptr.id;\n}\n\nstatic int dynptr_ref_obj_id(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi;\n\n\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\treturn reg->ref_obj_id;\n\tspi = dynptr_get_spi(env, reg);\n\tif (spi < 0)\n\t\treturn spi;\n\treturn state->stack[spi].spilled_ptr.ref_obj_id;\n}\n\nstatic enum bpf_dynptr_type dynptr_get_type(struct bpf_verifier_env *env,\n\t\t\t\t\t    struct bpf_reg_state *reg)\n{\n\tstruct bpf_func_state *state = func(env, reg);\n\tint spi;\n\n\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\treturn reg->dynptr.type;\n\n\tspi = __get_spi(reg->off);\n\tif (spi < 0) {\n\t\tverbose(env, \"verifier internal error: invalid spi when querying dynptr type\\n\");\n\t\treturn BPF_DYNPTR_TYPE_INVALID;\n\t}\n\n\treturn state->stack[spi].spilled_ptr.dynptr.type;\n}\n\nstatic int check_func_arg(struct bpf_verifier_env *env, u32 arg,\n\t\t\t  struct bpf_call_arg_meta *meta,\n\t\t\t  const struct bpf_func_proto *fn,\n\t\t\t  int insn_idx)\n{\n\tu32 regno = BPF_REG_1 + arg;\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[regno];\n\tenum bpf_arg_type arg_type = fn->arg_type[arg];\n\tenum bpf_reg_type type = reg->type;\n\tu32 *arg_btf_id = NULL;\n\tint err = 0;\n\n\tif (arg_type == ARG_DONTCARE)\n\t\treturn 0;\n\n\terr = check_reg_arg(env, regno, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (arg_type == ARG_ANYTHING) {\n\t\tif (is_pointer_value(env, regno)) {\n\t\t\tverbose(env, \"R%d leaks addr into helper function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (type_is_pkt_pointer(type) &&\n\t    !may_access_direct_pkt_data(env, meta, BPF_READ)) {\n\t\tverbose(env, \"helper access to the packet is not allowed\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (base_type(arg_type) == ARG_PTR_TO_MAP_VALUE) {\n\t\terr = resolve_map_arg_type(env, meta, &arg_type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (register_is_null(reg) && type_may_be_null(arg_type))\n\t\t \n\t\tgoto skip_type_check;\n\n\t \n\tif (base_type(arg_type) == ARG_PTR_TO_BTF_ID ||\n\t    base_type(arg_type) == ARG_PTR_TO_SPIN_LOCK)\n\t\targ_btf_id = fn->arg_btf_id[arg];\n\n\terr = check_reg_type(env, regno, arg_type, arg_btf_id, meta);\n\tif (err)\n\t\treturn err;\n\n\terr = check_func_arg_reg_off(env, reg, regno, arg_type);\n\tif (err)\n\t\treturn err;\n\nskip_type_check:\n\tif (arg_type_is_release(arg_type)) {\n\t\tif (arg_type_is_dynptr(arg_type)) {\n\t\t\tstruct bpf_func_state *state = func(env, reg);\n\t\t\tint spi;\n\n\t\t\t \n\t\t\tif (reg->type == PTR_TO_STACK) {\n\t\t\t\tspi = dynptr_get_spi(env, reg);\n\t\t\t\tif (spi < 0 || !state->stack[spi].spilled_ptr.ref_obj_id) {\n\t\t\t\t\tverbose(env, \"arg %d is an unacquired reference\\n\", regno);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tverbose(env, \"cannot release unowned const bpf_dynptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else if (!reg->ref_obj_id && !register_is_null(reg)) {\n\t\t\tverbose(env, \"R%d must be referenced when passed to release function\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (meta->release_regno) {\n\t\t\tverbose(env, \"verifier internal error: more than one release argument\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tmeta->release_regno = regno;\n\t}\n\n\tif (reg->ref_obj_id) {\n\t\tif (meta->ref_obj_id) {\n\t\t\tverbose(env, \"verifier internal error: more than one arg with ref_obj_id R%d %u %u\\n\",\n\t\t\t\tregno, reg->ref_obj_id,\n\t\t\t\tmeta->ref_obj_id);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tmeta->ref_obj_id = reg->ref_obj_id;\n\t}\n\n\tswitch (base_type(arg_type)) {\n\tcase ARG_CONST_MAP_PTR:\n\t\t \n\t\tif (meta->map_ptr) {\n\t\t\t \n\t\t\tif (meta->map_ptr != reg->map_ptr ||\n\t\t\t    meta->map_uid != reg->map_uid) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"timer pointer in R1 map_uid=%d doesn't match map pointer in R2 map_uid=%d\\n\",\n\t\t\t\t\tmeta->map_uid, reg->map_uid);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\tmeta->map_ptr = reg->map_ptr;\n\t\tmeta->map_uid = reg->map_uid;\n\t\tbreak;\n\tcase ARG_PTR_TO_MAP_KEY:\n\t\t \n\t\tif (!meta->map_ptr) {\n\t\t\t \n\t\t\tverbose(env, \"invalid map_ptr to access map->key\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->key_size, false,\n\t\t\t\t\t      NULL);\n\t\tbreak;\n\tcase ARG_PTR_TO_MAP_VALUE:\n\t\tif (type_may_be_null(arg_type) && register_is_null(reg))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!meta->map_ptr) {\n\t\t\t \n\t\t\tverbose(env, \"invalid map_ptr to access map->value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->raw_mode = arg_type & MEM_UNINIT;\n\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t      meta->map_ptr->value_size, false,\n\t\t\t\t\t      meta);\n\t\tbreak;\n\tcase ARG_PTR_TO_PERCPU_BTF_ID:\n\t\tif (!reg->btf_id) {\n\t\t\tverbose(env, \"Helper has invalid btf_id in R%d\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->ret_btf = reg->btf;\n\t\tmeta->ret_btf_id = reg->btf_id;\n\t\tbreak;\n\tcase ARG_PTR_TO_SPIN_LOCK:\n\t\tif (in_rbtree_lock_required_cb(env)) {\n\t\t\tverbose(env, \"can't spin_{lock,unlock} in rbtree cb\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (meta->func_id == BPF_FUNC_spin_lock) {\n\t\t\terr = process_spin_lock(env, regno, true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else if (meta->func_id == BPF_FUNC_spin_unlock) {\n\t\t\terr = process_spin_lock(env, regno, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tverbose(env, \"verifier internal error\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tbreak;\n\tcase ARG_PTR_TO_TIMER:\n\t\terr = process_timer_func(env, regno, meta);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase ARG_PTR_TO_FUNC:\n\t\tmeta->subprogno = reg->subprogno;\n\t\tbreak;\n\tcase ARG_PTR_TO_MEM:\n\t\t \n\t\tmeta->raw_mode = arg_type & MEM_UNINIT;\n\t\tif (arg_type & MEM_FIXED_SIZE) {\n\t\t\terr = check_helper_mem_access(env, regno,\n\t\t\t\t\t\t      fn->arg_size[arg], false,\n\t\t\t\t\t\t      meta);\n\t\t}\n\t\tbreak;\n\tcase ARG_CONST_SIZE:\n\t\terr = check_mem_size_reg(env, reg, regno, false, meta);\n\t\tbreak;\n\tcase ARG_CONST_SIZE_OR_ZERO:\n\t\terr = check_mem_size_reg(env, reg, regno, true, meta);\n\t\tbreak;\n\tcase ARG_PTR_TO_DYNPTR:\n\t\terr = process_dynptr_func(env, regno, insn_idx, arg_type, 0);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase ARG_CONST_ALLOC_SIZE_OR_ZERO:\n\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"R%d is not a known constant'\\n\",\n\t\t\t\tregno);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tmeta->mem_size = reg->var_off.value;\n\t\terr = mark_chain_precision(env, regno);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tcase ARG_PTR_TO_INT:\n\tcase ARG_PTR_TO_LONG:\n\t{\n\t\tint size = int_ptr_type_to_size(arg_type);\n\n\t\terr = check_helper_mem_access(env, regno, size, false, meta);\n\t\tif (err)\n\t\t\treturn err;\n\t\terr = check_ptr_alignment(env, reg, 0, size, true);\n\t\tbreak;\n\t}\n\tcase ARG_PTR_TO_CONST_STR:\n\t{\n\t\tstruct bpf_map *map = reg->map_ptr;\n\t\tint map_off;\n\t\tu64 map_addr;\n\t\tchar *str_ptr;\n\n\t\tif (!bpf_map_is_rdonly(map)) {\n\t\t\tverbose(env, \"R%d does not point to a readonly map'\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\tverbose(env, \"R%d is not a constant address'\\n\", regno);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (!map->ops->map_direct_value_addr) {\n\t\t\tverbose(env, \"no direct value access support for this map type\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\terr = check_map_access(env, regno, reg->off,\n\t\t\t\t       map->value_size - reg->off, false,\n\t\t\t\t       ACCESS_HELPER);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tmap_off = reg->off + reg->var_off.value;\n\t\terr = map->ops->map_direct_value_addr(map, &map_addr, map_off);\n\t\tif (err) {\n\t\t\tverbose(env, \"direct value access on string failed\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\tstr_ptr = (char *)(long)(map_addr);\n\t\tif (!strnchr(str_ptr + map_off, map->value_size - map_off, 0)) {\n\t\t\tverbose(env, \"string is not zero-terminated\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\t}\n\tcase ARG_PTR_TO_KPTR:\n\t\terr = process_kptr_func(env, regno, meta);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic bool may_update_sockmap(struct bpf_verifier_env *env, int func_id)\n{\n\tenum bpf_attach_type eatype = env->prog->expected_attach_type;\n\tenum bpf_prog_type type = resolve_prog_type(env->prog);\n\n\tif (func_id != BPF_FUNC_map_update_elem)\n\t\treturn false;\n\n\t \n\tswitch (type) {\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tif (eatype == BPF_TRACE_ITER)\n\t\t\treturn true;\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\tcase BPF_PROG_TYPE_XDP:\n\tcase BPF_PROG_TYPE_SK_REUSEPORT:\n\tcase BPF_PROG_TYPE_FLOW_DISSECTOR:\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tverbose(env, \"cannot update sockmap in this context\\n\");\n\treturn false;\n}\n\nstatic bool allow_tail_call_in_subprogs(struct bpf_verifier_env *env)\n{\n\treturn env->prog->jit_requested &&\n\t       bpf_jit_supports_subprog_tailcalls();\n}\n\nstatic int check_map_func_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map, int func_id)\n{\n\tif (!map)\n\t\treturn 0;\n\n\t \n\tswitch (map->map_type) {\n\tcase BPF_MAP_TYPE_PROG_ARRAY:\n\t\tif (func_id != BPF_FUNC_tail_call)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_PERF_EVENT_ARRAY:\n\t\tif (func_id != BPF_FUNC_perf_event_read &&\n\t\t    func_id != BPF_FUNC_perf_event_output &&\n\t\t    func_id != BPF_FUNC_skb_output &&\n\t\t    func_id != BPF_FUNC_perf_event_read_value &&\n\t\t    func_id != BPF_FUNC_xdp_output)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_RINGBUF:\n\t\tif (func_id != BPF_FUNC_ringbuf_output &&\n\t\t    func_id != BPF_FUNC_ringbuf_reserve &&\n\t\t    func_id != BPF_FUNC_ringbuf_query &&\n\t\t    func_id != BPF_FUNC_ringbuf_reserve_dynptr &&\n\t\t    func_id != BPF_FUNC_ringbuf_submit_dynptr &&\n\t\t    func_id != BPF_FUNC_ringbuf_discard_dynptr)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_USER_RINGBUF:\n\t\tif (func_id != BPF_FUNC_user_ringbuf_drain)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_STACK_TRACE:\n\t\tif (func_id != BPF_FUNC_get_stackid)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_ARRAY:\n\t\tif (func_id != BPF_FUNC_skb_under_cgroup &&\n\t\t    func_id != BPF_FUNC_current_task_under_cgroup)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGROUP_STORAGE:\n\tcase BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE:\n\t\tif (func_id != BPF_FUNC_get_local_storage)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_DEVMAP:\n\tcase BPF_MAP_TYPE_DEVMAP_HASH:\n\t\tif (func_id != BPF_FUNC_redirect_map &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\t \n\tcase BPF_MAP_TYPE_CPUMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_XSKMAP:\n\t\tif (func_id != BPF_FUNC_redirect_map &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tif (func_id != BPF_FUNC_map_lookup_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKMAP:\n\t\tif (func_id != BPF_FUNC_sk_redirect_map &&\n\t\t    func_id != BPF_FUNC_sock_map_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_map &&\n\t\t    func_id != BPF_FUNC_sk_select_reuseport &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t\t    !may_update_sockmap(env, func_id))\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SOCKHASH:\n\t\tif (func_id != BPF_FUNC_sk_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sock_hash_update &&\n\t\t    func_id != BPF_FUNC_map_delete_elem &&\n\t\t    func_id != BPF_FUNC_msg_redirect_hash &&\n\t\t    func_id != BPF_FUNC_sk_select_reuseport &&\n\t\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t\t    !may_update_sockmap(env, func_id))\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_REUSEPORT_SOCKARRAY:\n\t\tif (func_id != BPF_FUNC_sk_select_reuseport)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_QUEUE:\n\tcase BPF_MAP_TYPE_STACK:\n\t\tif (func_id != BPF_FUNC_map_peek_elem &&\n\t\t    func_id != BPF_FUNC_map_pop_elem &&\n\t\t    func_id != BPF_FUNC_map_push_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_SK_STORAGE:\n\t\tif (func_id != BPF_FUNC_sk_storage_get &&\n\t\t    func_id != BPF_FUNC_sk_storage_delete &&\n\t\t    func_id != BPF_FUNC_kptr_xchg)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_INODE_STORAGE:\n\t\tif (func_id != BPF_FUNC_inode_storage_get &&\n\t\t    func_id != BPF_FUNC_inode_storage_delete &&\n\t\t    func_id != BPF_FUNC_kptr_xchg)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_TASK_STORAGE:\n\t\tif (func_id != BPF_FUNC_task_storage_get &&\n\t\t    func_id != BPF_FUNC_task_storage_delete &&\n\t\t    func_id != BPF_FUNC_kptr_xchg)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_CGRP_STORAGE:\n\t\tif (func_id != BPF_FUNC_cgrp_storage_get &&\n\t\t    func_id != BPF_FUNC_cgrp_storage_delete &&\n\t\t    func_id != BPF_FUNC_kptr_xchg)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_MAP_TYPE_BLOOM_FILTER:\n\t\tif (func_id != BPF_FUNC_map_peek_elem &&\n\t\t    func_id != BPF_FUNC_map_push_elem)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\tif (map->map_type != BPF_MAP_TYPE_PROG_ARRAY)\n\t\t\tgoto error;\n\t\tif (env->subprog_cnt > 1 && !allow_tail_call_in_subprogs(env)) {\n\t\t\tverbose(env, \"tail_calls are not allowed in non-JITed programs with bpf-to-bpf calls\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_perf_event_read:\n\tcase BPF_FUNC_perf_event_output:\n\tcase BPF_FUNC_perf_event_read_value:\n\tcase BPF_FUNC_skb_output:\n\tcase BPF_FUNC_xdp_output:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERF_EVENT_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_ringbuf_output:\n\tcase BPF_FUNC_ringbuf_reserve:\n\tcase BPF_FUNC_ringbuf_query:\n\tcase BPF_FUNC_ringbuf_reserve_dynptr:\n\tcase BPF_FUNC_ringbuf_submit_dynptr:\n\tcase BPF_FUNC_ringbuf_discard_dynptr:\n\t\tif (map->map_type != BPF_MAP_TYPE_RINGBUF)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_user_ringbuf_drain:\n\t\tif (map->map_type != BPF_MAP_TYPE_USER_RINGBUF)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_stackid:\n\t\tif (map->map_type != BPF_MAP_TYPE_STACK_TRACE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\tcase BPF_FUNC_skb_under_cgroup:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_ARRAY)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_redirect_map:\n\t\tif (map->map_type != BPF_MAP_TYPE_DEVMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_DEVMAP_HASH &&\n\t\t    map->map_type != BPF_MAP_TYPE_CPUMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_XSKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_map:\n\tcase BPF_FUNC_msg_redirect_map:\n\tcase BPF_FUNC_sock_map_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKMAP)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_redirect_hash:\n\tcase BPF_FUNC_msg_redirect_hash:\n\tcase BPF_FUNC_sock_hash_update:\n\t\tif (map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGROUP_STORAGE &&\n\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_select_reuseport:\n\t\tif (map->map_type != BPF_MAP_TYPE_REUSEPORT_SOCKARRAY &&\n\t\t    map->map_type != BPF_MAP_TYPE_SOCKMAP &&\n\t\t    map->map_type != BPF_MAP_TYPE_SOCKHASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_pop_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_QUEUE &&\n\t\t    map->map_type != BPF_MAP_TYPE_STACK)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_peek_elem:\n\tcase BPF_FUNC_map_push_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_QUEUE &&\n\t\t    map->map_type != BPF_MAP_TYPE_STACK &&\n\t\t    map->map_type != BPF_MAP_TYPE_BLOOM_FILTER)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_map_lookup_percpu_elem:\n\t\tif (map->map_type != BPF_MAP_TYPE_PERCPU_ARRAY &&\n\t\t    map->map_type != BPF_MAP_TYPE_PERCPU_HASH &&\n\t\t    map->map_type != BPF_MAP_TYPE_LRU_PERCPU_HASH)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_sk_storage_get:\n\tcase BPF_FUNC_sk_storage_delete:\n\t\tif (map->map_type != BPF_MAP_TYPE_SK_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_inode_storage_get:\n\tcase BPF_FUNC_inode_storage_delete:\n\t\tif (map->map_type != BPF_MAP_TYPE_INODE_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_task_storage_get:\n\tcase BPF_FUNC_task_storage_delete:\n\t\tif (map->map_type != BPF_MAP_TYPE_TASK_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tcase BPF_FUNC_cgrp_storage_get:\n\tcase BPF_FUNC_cgrp_storage_delete:\n\t\tif (map->map_type != BPF_MAP_TYPE_CGRP_STORAGE)\n\t\t\tgoto error;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\nerror:\n\tverbose(env, \"cannot pass map_type %d into func %s#%d\\n\",\n\t\tmap->map_type, func_id_name(func_id), func_id);\n\treturn -EINVAL;\n}\n\nstatic bool check_raw_mode_ok(const struct bpf_func_proto *fn)\n{\n\tint count = 0;\n\n\tif (fn->arg1_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg2_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg3_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg4_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\tif (fn->arg5_type == ARG_PTR_TO_UNINIT_MEM)\n\t\tcount++;\n\n\t \n\treturn count <= 1;\n}\n\nstatic bool check_args_pair_invalid(const struct bpf_func_proto *fn, int arg)\n{\n\tbool is_fixed = fn->arg_type[arg] & MEM_FIXED_SIZE;\n\tbool has_size = fn->arg_size[arg] != 0;\n\tbool is_next_size = false;\n\n\tif (arg + 1 < ARRAY_SIZE(fn->arg_type))\n\t\tis_next_size = arg_type_is_mem_size(fn->arg_type[arg + 1]);\n\n\tif (base_type(fn->arg_type[arg]) != ARG_PTR_TO_MEM)\n\t\treturn is_next_size;\n\n\treturn has_size == is_next_size || is_next_size == is_fixed;\n}\n\nstatic bool check_arg_pair_ok(const struct bpf_func_proto *fn)\n{\n\t \n\tif (arg_type_is_mem_size(fn->arg1_type) ||\n\t    check_args_pair_invalid(fn, 0) ||\n\t    check_args_pair_invalid(fn, 1) ||\n\t    check_args_pair_invalid(fn, 2) ||\n\t    check_args_pair_invalid(fn, 3) ||\n\t    check_args_pair_invalid(fn, 4))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool check_btf_id_ok(const struct bpf_func_proto *fn)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(fn->arg_type); i++) {\n\t\tif (base_type(fn->arg_type[i]) == ARG_PTR_TO_BTF_ID)\n\t\t\treturn !!fn->arg_btf_id[i];\n\t\tif (base_type(fn->arg_type[i]) == ARG_PTR_TO_SPIN_LOCK)\n\t\t\treturn fn->arg_btf_id[i] == BPF_PTR_POISON;\n\t\tif (base_type(fn->arg_type[i]) != ARG_PTR_TO_BTF_ID && fn->arg_btf_id[i] &&\n\t\t     \n\t\t    (base_type(fn->arg_type[i]) != ARG_PTR_TO_MEM ||\n\t\t     !(fn->arg_type[i] & MEM_FIXED_SIZE)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int check_func_proto(const struct bpf_func_proto *fn, int func_id)\n{\n\treturn check_raw_mode_ok(fn) &&\n\t       check_arg_pair_ok(fn) &&\n\t       check_btf_id_ok(fn) ? 0 : -EINVAL;\n}\n\n \nstatic void clear_all_pkt_pointers(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *state;\n\tstruct bpf_reg_state *reg;\n\n\tbpf_for_each_reg_in_vstate(env->cur_state, state, reg, ({\n\t\tif (reg_is_pkt_pointer_any(reg) || reg_is_dynptr_slice_pkt(reg))\n\t\t\tmark_reg_invalid(env, reg);\n\t}));\n}\n\nenum {\n\tAT_PKT_END = -1,\n\tBEYOND_PKT_END = -2,\n};\n\nstatic void mark_pkt_end(struct bpf_verifier_state *vstate, int regn, bool range_open)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *reg = &state->regs[regn];\n\n\tif (reg->type != PTR_TO_PACKET)\n\t\t \n\t\treturn;\n\n\t \n\tif (range_open)\n\t\treg->range = BEYOND_PKT_END;\n\telse\n\t\treg->range = AT_PKT_END;\n}\n\n \nstatic int release_reference(struct bpf_verifier_env *env,\n\t\t\t     int ref_obj_id)\n{\n\tstruct bpf_func_state *state;\n\tstruct bpf_reg_state *reg;\n\tint err;\n\n\terr = release_reference_state(cur_func(env), ref_obj_id);\n\tif (err)\n\t\treturn err;\n\n\tbpf_for_each_reg_in_vstate(env->cur_state, state, reg, ({\n\t\tif (reg->ref_obj_id == ref_obj_id)\n\t\t\tmark_reg_invalid(env, reg);\n\t}));\n\n\treturn 0;\n}\n\nstatic void invalidate_non_owning_refs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *unused;\n\tstruct bpf_reg_state *reg;\n\n\tbpf_for_each_reg_in_vstate(env->cur_state, unused, reg, ({\n\t\tif (type_is_non_owning_ref(reg->type))\n\t\t\tmark_reg_invalid(env, reg);\n\t}));\n}\n\nstatic void clear_caller_saved_regs(struct bpf_verifier_env *env,\n\t\t\t\t    struct bpf_reg_state *regs)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n}\n\ntypedef int (*set_callee_state_fn)(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *caller,\n\t\t\t\t   struct bpf_func_state *callee,\n\t\t\t\t   int insn_idx);\n\nstatic int set_callee_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *caller,\n\t\t\t    struct bpf_func_state *callee, int insn_idx);\n\nstatic int __check_func_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t     int *insn_idx, int subprog,\n\t\t\t     set_callee_state_fn set_callee_state_cb)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tint err;\n\n\tif (state->curframe + 1 >= MAX_CALL_FRAMES) {\n\t\tverbose(env, \"the call stack of %d frames is too deep\\n\",\n\t\t\tstate->curframe + 2);\n\t\treturn -E2BIG;\n\t}\n\n\tcaller = state->frame[state->curframe];\n\tif (state->frame[state->curframe + 1]) {\n\t\tverbose(env, \"verifier bug. Frame %d already allocated\\n\",\n\t\t\tstate->curframe + 1);\n\t\treturn -EFAULT;\n\t}\n\n\terr = btf_check_subprog_call(env, subprog, caller->regs);\n\tif (err == -EFAULT)\n\t\treturn err;\n\tif (subprog_is_global(env, subprog)) {\n\t\tif (err) {\n\t\t\tverbose(env, \"Caller passes invalid args into func#%d\\n\",\n\t\t\t\tsubprog);\n\t\t\treturn err;\n\t\t} else {\n\t\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"Func#%d is global and valid. Skipping.\\n\",\n\t\t\t\t\tsubprog);\n\t\t\tclear_caller_saved_regs(env, caller->regs);\n\n\t\t\t \n\t\t\tmark_reg_unknown(env, caller->regs, BPF_REG_0);\n\t\t\tcaller->regs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tif (set_callee_state_cb != set_callee_state) {\n\t\tif (bpf_pseudo_kfunc_call(insn) &&\n\t\t    !is_callback_calling_kfunc(insn->imm)) {\n\t\t\tverbose(env, \"verifier bug: kfunc %s#%d not marked as callback-calling\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t} else if (!bpf_pseudo_kfunc_call(insn) &&\n\t\t\t   !is_callback_calling_function(insn->imm)) {  \n\t\t\tverbose(env, \"verifier bug: helper %s#%d not marked as callback-calling\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (insn->code == (BPF_JMP | BPF_CALL) &&\n\t    insn->src_reg == 0 &&\n\t    insn->imm == BPF_FUNC_timer_set_callback) {\n\t\tstruct bpf_verifier_state *async_cb;\n\n\t\t \n\t\tenv->subprog_info[subprog].is_async_cb = true;\n\t\tasync_cb = push_async_cb(env, env->subprog_info[subprog].start,\n\t\t\t\t\t *insn_idx, subprog);\n\t\tif (!async_cb)\n\t\t\treturn -EFAULT;\n\t\tcallee = async_cb->frame[0];\n\t\tcallee->async_entry_cnt = caller->async_entry_cnt + 1;\n\n\t\t \n\t\terr = set_callee_state_cb(env, caller, callee, *insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tclear_caller_saved_regs(env, caller->regs);\n\t\tmark_reg_unknown(env, caller->regs, BPF_REG_0);\n\t\tcaller->regs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\t\t \n\t\treturn 0;\n\t}\n\n\tcallee = kzalloc(sizeof(*callee), GFP_KERNEL);\n\tif (!callee)\n\t\treturn -ENOMEM;\n\tstate->frame[state->curframe + 1] = callee;\n\n\t \n\tinit_func_state(env, callee,\n\t\t\t \n\t\t\t*insn_idx  ,\n\t\t\tstate->curframe + 1  ,\n\t\t\tsubprog  );\n\n\t \n\terr = copy_reference_state(callee, caller);\n\tif (err)\n\t\tgoto err_out;\n\n\terr = set_callee_state_cb(env, caller, callee, *insn_idx);\n\tif (err)\n\t\tgoto err_out;\n\n\tclear_caller_saved_regs(env, caller->regs);\n\n\t \n\tstate->curframe++;\n\n\t \n\t*insn_idx = env->subprog_info[subprog].start - 1;\n\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"caller:\\n\");\n\t\tprint_verifier_state(env, caller, true);\n\t\tverbose(env, \"callee:\\n\");\n\t\tprint_verifier_state(env, callee, true);\n\t}\n\treturn 0;\n\nerr_out:\n\tfree_func_state(callee);\n\tstate->frame[state->curframe + 1] = NULL;\n\treturn err;\n}\n\nint map_set_for_each_callback_args(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *caller,\n\t\t\t\t   struct bpf_func_state *callee)\n{\n\t \n\tcallee->regs[BPF_REG_1] = caller->regs[BPF_REG_1];\n\n\tcallee->regs[BPF_REG_2].type = PTR_TO_MAP_KEY;\n\t__mark_reg_known_zero(&callee->regs[BPF_REG_2]);\n\tcallee->regs[BPF_REG_2].map_ptr = caller->regs[BPF_REG_1].map_ptr;\n\n\tcallee->regs[BPF_REG_3].type = PTR_TO_MAP_VALUE;\n\t__mark_reg_known_zero(&callee->regs[BPF_REG_3]);\n\tcallee->regs[BPF_REG_3].map_ptr = caller->regs[BPF_REG_1].map_ptr;\n\n\t \n\tcallee->regs[BPF_REG_4] = caller->regs[BPF_REG_3];\n\n\t \n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_5]);\n\treturn 0;\n}\n\nstatic int set_callee_state(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_func_state *caller,\n\t\t\t    struct bpf_func_state *callee, int insn_idx)\n{\n\tint i;\n\n\t \n\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++)\n\t\tcallee->regs[i] = caller->regs[i];\n\treturn 0;\n}\n\nstatic int check_func_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t   int *insn_idx)\n{\n\tint subprog, target_insn;\n\n\ttarget_insn = *insn_idx + insn->imm + 1;\n\tsubprog = find_subprog(env, target_insn);\n\tif (subprog < 0) {\n\t\tverbose(env, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\ttarget_insn);\n\t\treturn -EFAULT;\n\t}\n\n\treturn __check_func_call(env, insn, insn_idx, subprog, set_callee_state);\n}\n\nstatic int set_map_elem_callback_state(struct bpf_verifier_env *env,\n\t\t\t\t       struct bpf_func_state *caller,\n\t\t\t\t       struct bpf_func_state *callee,\n\t\t\t\t       int insn_idx)\n{\n\tstruct bpf_insn_aux_data *insn_aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_map *map;\n\tint err;\n\n\tif (bpf_map_ptr_poisoned(insn_aux)) {\n\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmap = BPF_MAP_PTR(insn_aux->map_ptr_state);\n\tif (!map->ops->map_set_for_each_callback_args ||\n\t    !map->ops->map_for_each_callback) {\n\t\tverbose(env, \"callback function not allowed for map\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\terr = map->ops->map_set_for_each_callback_args(env, caller, callee);\n\tif (err)\n\t\treturn err;\n\n\tcallee->in_callback_fn = true;\n\tcallee->callback_ret_range = tnum_range(0, 1);\n\treturn 0;\n}\n\nstatic int set_loop_callback_state(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *caller,\n\t\t\t\t   struct bpf_func_state *callee,\n\t\t\t\t   int insn_idx)\n{\n\t \n\tcallee->regs[BPF_REG_1].type = SCALAR_VALUE;\n\tcallee->regs[BPF_REG_2] = caller->regs[BPF_REG_3];\n\n\t \n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_3]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_4]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_5]);\n\n\tcallee->in_callback_fn = true;\n\tcallee->callback_ret_range = tnum_range(0, 1);\n\treturn 0;\n}\n\nstatic int set_timer_callback_state(struct bpf_verifier_env *env,\n\t\t\t\t    struct bpf_func_state *caller,\n\t\t\t\t    struct bpf_func_state *callee,\n\t\t\t\t    int insn_idx)\n{\n\tstruct bpf_map *map_ptr = caller->regs[BPF_REG_1].map_ptr;\n\n\t \n\tcallee->regs[BPF_REG_1].type = CONST_PTR_TO_MAP;\n\t__mark_reg_known_zero(&callee->regs[BPF_REG_1]);\n\tcallee->regs[BPF_REG_1].map_ptr = map_ptr;\n\n\tcallee->regs[BPF_REG_2].type = PTR_TO_MAP_KEY;\n\t__mark_reg_known_zero(&callee->regs[BPF_REG_2]);\n\tcallee->regs[BPF_REG_2].map_ptr = map_ptr;\n\n\tcallee->regs[BPF_REG_3].type = PTR_TO_MAP_VALUE;\n\t__mark_reg_known_zero(&callee->regs[BPF_REG_3]);\n\tcallee->regs[BPF_REG_3].map_ptr = map_ptr;\n\n\t \n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_4]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_5]);\n\tcallee->in_async_callback_fn = true;\n\tcallee->callback_ret_range = tnum_range(0, 1);\n\treturn 0;\n}\n\nstatic int set_find_vma_callback_state(struct bpf_verifier_env *env,\n\t\t\t\t       struct bpf_func_state *caller,\n\t\t\t\t       struct bpf_func_state *callee,\n\t\t\t\t       int insn_idx)\n{\n\t \n\tcallee->regs[BPF_REG_1] = caller->regs[BPF_REG_1];\n\n\tcallee->regs[BPF_REG_2].type = PTR_TO_BTF_ID;\n\t__mark_reg_known_zero(&callee->regs[BPF_REG_2]);\n\tcallee->regs[BPF_REG_2].btf =  btf_vmlinux;\n\tcallee->regs[BPF_REG_2].btf_id = btf_tracing_ids[BTF_TRACING_TYPE_VMA],\n\n\t \n\tcallee->regs[BPF_REG_3] = caller->regs[BPF_REG_4];\n\n\t \n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_4]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_5]);\n\tcallee->in_callback_fn = true;\n\tcallee->callback_ret_range = tnum_range(0, 1);\n\treturn 0;\n}\n\nstatic int set_user_ringbuf_callback_state(struct bpf_verifier_env *env,\n\t\t\t\t\t   struct bpf_func_state *caller,\n\t\t\t\t\t   struct bpf_func_state *callee,\n\t\t\t\t\t   int insn_idx)\n{\n\t \n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_0]);\n\tmark_dynptr_cb_reg(env, &callee->regs[BPF_REG_1], BPF_DYNPTR_TYPE_LOCAL);\n\tcallee->regs[BPF_REG_2] = caller->regs[BPF_REG_3];\n\n\t \n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_3]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_4]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_5]);\n\n\tcallee->in_callback_fn = true;\n\tcallee->callback_ret_range = tnum_range(0, 1);\n\treturn 0;\n}\n\nstatic int set_rbtree_add_callback_state(struct bpf_verifier_env *env,\n\t\t\t\t\t struct bpf_func_state *caller,\n\t\t\t\t\t struct bpf_func_state *callee,\n\t\t\t\t\t int insn_idx)\n{\n\t \n\tstruct btf_field *field;\n\n\tfield = reg_find_field_offset(&caller->regs[BPF_REG_1], caller->regs[BPF_REG_1].off,\n\t\t\t\t      BPF_RB_ROOT);\n\tif (!field || !field->graph_root.value_btf_id)\n\t\treturn -EFAULT;\n\n\tmark_reg_graph_node(callee->regs, BPF_REG_1, &field->graph_root);\n\tref_set_non_owning(env, &callee->regs[BPF_REG_1]);\n\tmark_reg_graph_node(callee->regs, BPF_REG_2, &field->graph_root);\n\tref_set_non_owning(env, &callee->regs[BPF_REG_2]);\n\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_3]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_4]);\n\t__mark_reg_not_init(env, &callee->regs[BPF_REG_5]);\n\tcallee->in_callback_fn = true;\n\tcallee->callback_ret_range = tnum_range(0, 1);\n\treturn 0;\n}\n\nstatic bool is_rbtree_lock_required_kfunc(u32 btf_id);\n\n \nstatic bool in_rbtree_lock_required_cb(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tstruct bpf_func_state *callee;\n\tint kfunc_btf_id;\n\n\tif (!state->curframe)\n\t\treturn false;\n\n\tcallee = state->frame[state->curframe];\n\n\tif (!callee->in_callback_fn)\n\t\treturn false;\n\n\tkfunc_btf_id = insn[callee->callsite].imm;\n\treturn is_rbtree_lock_required_kfunc(kfunc_btf_id);\n}\n\nstatic int prepare_func_exit(struct bpf_verifier_env *env, int *insn_idx)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_func_state *caller, *callee;\n\tstruct bpf_reg_state *r0;\n\tint err;\n\n\tcallee = state->frame[state->curframe];\n\tr0 = &callee->regs[BPF_REG_0];\n\tif (r0->type == PTR_TO_STACK) {\n\t\t \n\t\tverbose(env, \"cannot return stack pointer to the caller\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tcaller = state->frame[state->curframe - 1];\n\tif (callee->in_callback_fn) {\n\t\t \n\t\tstruct tnum range = callee->callback_ret_range;\n\n\t\tif (r0->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"R0 not a scalar value\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t \n\t\terr = mark_reg_read(env, r0, r0->parent, REG_LIVE_READ64);\n\t\terr = err ?: mark_chain_precision(env, BPF_REG_0);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!tnum_in(range, r0->var_off)) {\n\t\t\tverbose_invalid_scalar(env, r0, &range, \"callback return\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\t \n\t\tcaller->regs[BPF_REG_0] = *r0;\n\t}\n\n\t \n\tif (!callee->in_callback_fn) {\n\t\t \n\t\terr = copy_reference_state(caller, callee);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t*insn_idx = callee->callsite + 1;\n\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\tverbose(env, \"returning from callee:\\n\");\n\t\tprint_verifier_state(env, callee, true);\n\t\tverbose(env, \"to caller at %d:\\n\", *insn_idx);\n\t\tprint_verifier_state(env, caller, true);\n\t}\n\t \n\tfree_func_state(callee);\n\tstate->frame[state->curframe--] = NULL;\n\treturn 0;\n}\n\nstatic void do_refine_retval_range(struct bpf_reg_state *regs, int ret_type,\n\t\t\t\t   int func_id,\n\t\t\t\t   struct bpf_call_arg_meta *meta)\n{\n\tstruct bpf_reg_state *ret_reg = &regs[BPF_REG_0];\n\n\tif (ret_type != RET_INTEGER)\n\t\treturn;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_get_stack:\n\tcase BPF_FUNC_get_task_stack:\n\tcase BPF_FUNC_probe_read_str:\n\tcase BPF_FUNC_probe_read_kernel_str:\n\tcase BPF_FUNC_probe_read_user_str:\n\t\tret_reg->smax_value = meta->msize_max_value;\n\t\tret_reg->s32_max_value = meta->msize_max_value;\n\t\tret_reg->smin_value = -MAX_ERRNO;\n\t\tret_reg->s32_min_value = -MAX_ERRNO;\n\t\treg_bounds_sync(ret_reg);\n\t\tbreak;\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\tret_reg->umax_value = nr_cpu_ids - 1;\n\t\tret_reg->u32_max_value = nr_cpu_ids - 1;\n\t\tret_reg->smax_value = nr_cpu_ids - 1;\n\t\tret_reg->s32_max_value = nr_cpu_ids - 1;\n\t\tret_reg->umin_value = 0;\n\t\tret_reg->u32_min_value = 0;\n\t\tret_reg->smin_value = 0;\n\t\tret_reg->s32_min_value = 0;\n\t\treg_bounds_sync(ret_reg);\n\t\tbreak;\n\t}\n}\n\nstatic int\nrecord_func_map(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_map *map = meta->map_ptr;\n\n\tif (func_id != BPF_FUNC_tail_call &&\n\t    func_id != BPF_FUNC_map_lookup_elem &&\n\t    func_id != BPF_FUNC_map_update_elem &&\n\t    func_id != BPF_FUNC_map_delete_elem &&\n\t    func_id != BPF_FUNC_map_push_elem &&\n\t    func_id != BPF_FUNC_map_pop_elem &&\n\t    func_id != BPF_FUNC_map_peek_elem &&\n\t    func_id != BPF_FUNC_for_each_map_elem &&\n\t    func_id != BPF_FUNC_redirect_map &&\n\t    func_id != BPF_FUNC_map_lookup_percpu_elem)\n\t\treturn 0;\n\n\tif (map == NULL) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((map->map_flags & BPF_F_RDONLY_PROG) &&\n\t    (func_id == BPF_FUNC_map_delete_elem ||\n\t     func_id == BPF_FUNC_map_update_elem ||\n\t     func_id == BPF_FUNC_map_push_elem ||\n\t     func_id == BPF_FUNC_map_pop_elem)) {\n\t\tverbose(env, \"write into map forbidden\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tif (!BPF_MAP_PTR(aux->map_ptr_state))\n\t\tbpf_map_ptr_store(aux, meta->map_ptr,\n\t\t\t\t  !meta->map_ptr->bypass_spec_v1);\n\telse if (BPF_MAP_PTR(aux->map_ptr_state) != meta->map_ptr)\n\t\tbpf_map_ptr_store(aux, BPF_MAP_PTR_POISON,\n\t\t\t\t  !meta->map_ptr->bypass_spec_v1);\n\treturn 0;\n}\n\nstatic int\nrecord_func_key(struct bpf_verifier_env *env, struct bpf_call_arg_meta *meta,\n\t\tint func_id, int insn_idx)\n{\n\tstruct bpf_insn_aux_data *aux = &env->insn_aux_data[insn_idx];\n\tstruct bpf_reg_state *regs = cur_regs(env), *reg;\n\tstruct bpf_map *map = meta->map_ptr;\n\tu64 val, max;\n\tint err;\n\n\tif (func_id != BPF_FUNC_tail_call)\n\t\treturn 0;\n\tif (!map || map->map_type != BPF_MAP_TYPE_PROG_ARRAY) {\n\t\tverbose(env, \"kernel subsystem misconfigured verifier\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treg = &regs[BPF_REG_3];\n\tval = reg->var_off.value;\n\tmax = map->max_entries;\n\n\tif (!(register_is_const(reg) && val < max)) {\n\t\tbpf_map_key_store(aux, BPF_MAP_KEY_POISON);\n\t\treturn 0;\n\t}\n\n\terr = mark_chain_precision(env, BPF_REG_3);\n\tif (err)\n\t\treturn err;\n\tif (bpf_map_key_unseen(aux))\n\t\tbpf_map_key_store(aux, val);\n\telse if (!bpf_map_key_poisoned(aux) &&\n\t\t  bpf_map_key_immediate(aux) != val)\n\t\tbpf_map_key_store(aux, BPF_MAP_KEY_POISON);\n\treturn 0;\n}\n\nstatic int check_reference_leak(struct bpf_verifier_env *env)\n{\n\tstruct bpf_func_state *state = cur_func(env);\n\tbool refs_lingering = false;\n\tint i;\n\n\tif (state->frameno && !state->in_callback_fn)\n\t\treturn 0;\n\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->in_callback_fn && state->refs[i].callback_ref != state->frameno)\n\t\t\tcontinue;\n\t\tverbose(env, \"Unreleased reference id=%d alloc_insn=%d\\n\",\n\t\t\tstate->refs[i].id, state->refs[i].insn_idx);\n\t\trefs_lingering = true;\n\t}\n\treturn refs_lingering ? -EINVAL : 0;\n}\n\nstatic int check_bpf_snprintf_call(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *regs)\n{\n\tstruct bpf_reg_state *fmt_reg = &regs[BPF_REG_3];\n\tstruct bpf_reg_state *data_len_reg = &regs[BPF_REG_5];\n\tstruct bpf_map *fmt_map = fmt_reg->map_ptr;\n\tstruct bpf_bprintf_data data = {};\n\tint err, fmt_map_off, num_args;\n\tu64 fmt_addr;\n\tchar *fmt;\n\n\t \n\tif (data_len_reg->var_off.value % 8)\n\t\treturn -EINVAL;\n\tnum_args = data_len_reg->var_off.value / 8;\n\n\t \n\tfmt_map_off = fmt_reg->off + fmt_reg->var_off.value;\n\terr = fmt_map->ops->map_direct_value_addr(fmt_map, &fmt_addr,\n\t\t\t\t\t\t  fmt_map_off);\n\tif (err) {\n\t\tverbose(env, \"verifier bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\tfmt = (char *)(long)fmt_addr + fmt_map_off;\n\n\t \n\terr = bpf_bprintf_prepare(fmt, UINT_MAX, NULL, num_args, &data);\n\tif (err < 0)\n\t\tverbose(env, \"Invalid format string\\n\");\n\n\treturn err;\n}\n\nstatic int check_get_func_ip(struct bpf_verifier_env *env)\n{\n\tenum bpf_prog_type type = resolve_prog_type(env->prog);\n\tint func_id = BPF_FUNC_get_func_ip;\n\n\tif (type == BPF_PROG_TYPE_TRACING) {\n\t\tif (!bpf_prog_has_trampoline(env->prog)) {\n\t\t\tverbose(env, \"func %s#%d supported only for fentry/fexit/fmod_ret programs\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\treturn 0;\n\t} else if (type == BPF_PROG_TYPE_KPROBE) {\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"func %s#%d not supported for program type %d\\n\",\n\t\tfunc_id_name(func_id), func_id, type);\n\treturn -ENOTSUPP;\n}\n\nstatic struct bpf_insn_aux_data *cur_aux(struct bpf_verifier_env *env)\n{\n\treturn &env->insn_aux_data[env->insn_idx];\n}\n\nstatic bool loop_flag_is_zero(struct bpf_verifier_env *env)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[BPF_REG_4];\n\tbool reg_is_null = register_is_null(reg);\n\n\tif (reg_is_null)\n\t\tmark_chain_precision(env, BPF_REG_4);\n\n\treturn reg_is_null;\n}\n\nstatic void update_loop_inline_state(struct bpf_verifier_env *env, u32 subprogno)\n{\n\tstruct bpf_loop_inline_state *state = &cur_aux(env)->loop_inline_state;\n\n\tif (!state->initialized) {\n\t\tstate->initialized = 1;\n\t\tstate->fit_for_inline = loop_flag_is_zero(env);\n\t\tstate->callback_subprogno = subprogno;\n\t\treturn;\n\t}\n\n\tif (!state->fit_for_inline)\n\t\treturn;\n\n\tstate->fit_for_inline = (loop_flag_is_zero(env) &&\n\t\t\t\t state->callback_subprogno == subprogno);\n}\n\nstatic int check_helper_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t     int *insn_idx_p)\n{\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tconst struct bpf_func_proto *fn = NULL;\n\tenum bpf_return_type ret_type;\n\tenum bpf_type_flag ret_flag;\n\tstruct bpf_reg_state *regs;\n\tstruct bpf_call_arg_meta meta;\n\tint insn_idx = *insn_idx_p;\n\tbool changes_data;\n\tint i, err, func_id;\n\n\t \n\tfunc_id = insn->imm;\n\tif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\n\t\tverbose(env, \"invalid func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->ops->get_func_proto)\n\t\tfn = env->ops->get_func_proto(func_id, env->prog);\n\tif (!fn) {\n\t\tverbose(env, \"unknown func %s#%d\\n\", func_id_name(func_id),\n\t\t\tfunc_id);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!env->prog->gpl_compatible && fn->gpl_only) {\n\t\tverbose(env, \"cannot call GPL-restricted function from non-GPL compatible program\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (fn->allowed && !fn->allowed(env->prog)) {\n\t\tverbose(env, \"helper call is not allowed in probe\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->prog->aux->sleepable && fn->might_sleep) {\n\t\tverbose(env, \"helper call might sleep in a non-sleepable prog\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tchanges_data = bpf_helper_changes_pkt_data(fn->func);\n\tif (changes_data && fn->arg1_type != ARG_PTR_TO_CTX) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d: r1 != ctx\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tmemset(&meta, 0, sizeof(meta));\n\tmeta.pkt_access = fn->pkt_access;\n\n\terr = check_func_proto(fn, func_id);\n\tif (err) {\n\t\tverbose(env, \"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn err;\n\t}\n\n\tif (env->cur_state->active_rcu_lock) {\n\t\tif (fn->might_sleep) {\n\t\t\tverbose(env, \"sleepable helper %s#%d in rcu_read_lock region\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (env->prog->aux->sleepable && is_storage_get_function(func_id))\n\t\t\tenv->insn_aux_data[insn_idx].storage_get_func_atomic = true;\n\t}\n\n\tmeta.func_id = func_id;\n\t \n\tfor (i = 0; i < MAX_BPF_FUNC_REG_ARGS; i++) {\n\t\terr = check_func_arg(env, i, &meta, fn, insn_idx);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = record_func_map(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\terr = record_func_key(env, &meta, func_id, insn_idx);\n\tif (err)\n\t\treturn err;\n\n\t \n\tfor (i = 0; i < meta.access_size; i++) {\n\t\terr = check_mem_access(env, insn_idx, meta.regno, i, BPF_B,\n\t\t\t\t       BPF_WRITE, -1, false, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tregs = cur_regs(env);\n\n\tif (meta.release_regno) {\n\t\terr = -EINVAL;\n\t\t \n\t\tif (arg_type_is_dynptr(fn->arg_type[meta.release_regno - BPF_REG_1])) {\n\t\t\tif (regs[meta.release_regno].type == CONST_PTR_TO_DYNPTR) {\n\t\t\t\tverbose(env, \"verifier internal error: CONST_PTR_TO_DYNPTR cannot be released\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\terr = unmark_stack_slots_dynptr(env, &regs[meta.release_regno]);\n\t\t} else if (meta.ref_obj_id) {\n\t\t\terr = release_reference(env, meta.ref_obj_id);\n\t\t} else if (register_is_null(&regs[meta.release_regno])) {\n\t\t\t \n\t\t\terr = 0;\n\t\t}\n\t\tif (err) {\n\t\t\tverbose(env, \"func %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_tail_call:\n\t\terr = check_reference_leak(env);\n\t\tif (err) {\n\t\t\tverbose(env, \"tail_call would lead to reference leak\\n\");\n\t\t\treturn err;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_get_local_storage:\n\t\t \n\t\tif (!register_is_null(&regs[BPF_REG_2])) {\n\t\t\tverbose(env, \"get_local_storage() doesn't support non-zero flags\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_for_each_map_elem:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_map_elem_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_timer_set_callback:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_timer_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_find_vma:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_find_vma_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_snprintf:\n\t\terr = check_bpf_snprintf_call(env, regs);\n\t\tbreak;\n\tcase BPF_FUNC_loop:\n\t\tupdate_loop_inline_state(env, meta.subprogno);\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_loop_callback_state);\n\t\tbreak;\n\tcase BPF_FUNC_dynptr_from_mem:\n\t\tif (regs[BPF_REG_1].type != PTR_TO_MAP_VALUE) {\n\t\t\tverbose(env, \"Unsupported reg type %s for bpf_dynptr_from_mem data\\n\",\n\t\t\t\treg_type_str(env, regs[BPF_REG_1].type));\n\t\t\treturn -EACCES;\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_set_retval:\n\t\tif (prog_type == BPF_PROG_TYPE_LSM &&\n\t\t    env->prog->expected_attach_type == BPF_LSM_CGROUP) {\n\t\t\tif (!env->prog->aux->attach_func_proto->type) {\n\t\t\t\t \n\t\t\t\tverbose(env, \"BPF_LSM_CGROUP that attach to void LSM hooks can't modify return value!\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase BPF_FUNC_dynptr_data:\n\t{\n\t\tstruct bpf_reg_state *reg;\n\t\tint id, ref_obj_id;\n\n\t\treg = get_dynptr_arg_reg(env, fn, regs);\n\t\tif (!reg)\n\t\t\treturn -EFAULT;\n\n\n\t\tif (meta.dynptr_id) {\n\t\t\tverbose(env, \"verifier internal error: meta.dynptr_id already set\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (meta.ref_obj_id) {\n\t\t\tverbose(env, \"verifier internal error: meta.ref_obj_id already set\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tid = dynptr_id(env, reg);\n\t\tif (id < 0) {\n\t\t\tverbose(env, \"verifier internal error: failed to obtain dynptr id\\n\");\n\t\t\treturn id;\n\t\t}\n\n\t\tref_obj_id = dynptr_ref_obj_id(env, reg);\n\t\tif (ref_obj_id < 0) {\n\t\t\tverbose(env, \"verifier internal error: failed to obtain dynptr ref_obj_id\\n\");\n\t\t\treturn ref_obj_id;\n\t\t}\n\n\t\tmeta.dynptr_id = id;\n\t\tmeta.ref_obj_id = ref_obj_id;\n\n\t\tbreak;\n\t}\n\tcase BPF_FUNC_dynptr_write:\n\t{\n\t\tenum bpf_dynptr_type dynptr_type;\n\t\tstruct bpf_reg_state *reg;\n\n\t\treg = get_dynptr_arg_reg(env, fn, regs);\n\t\tif (!reg)\n\t\t\treturn -EFAULT;\n\n\t\tdynptr_type = dynptr_get_type(env, reg);\n\t\tif (dynptr_type == BPF_DYNPTR_TYPE_INVALID)\n\t\t\treturn -EFAULT;\n\n\t\tif (dynptr_type == BPF_DYNPTR_TYPE_SKB)\n\t\t\t \n\t\t\tchanges_data = true;\n\n\t\tbreak;\n\t}\n\tcase BPF_FUNC_user_ringbuf_drain:\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_user_ringbuf_callback_state);\n\t\tbreak;\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\t \n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t \n\tregs[BPF_REG_0].subreg_def = DEF_NOT_SUBREG;\n\n\t \n\tret_type = fn->ret_type;\n\tret_flag = type_flag(ret_type);\n\n\tswitch (base_type(ret_type)) {\n\tcase RET_INTEGER:\n\t\t \n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t\tbreak;\n\tcase RET_VOID:\n\t\tregs[BPF_REG_0].type = NOT_INIT;\n\t\tbreak;\n\tcase RET_PTR_TO_MAP_VALUE:\n\t\t \n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t \n\t\tif (meta.map_ptr == NULL) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured verifier\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].map_ptr = meta.map_ptr;\n\t\tregs[BPF_REG_0].map_uid = meta.map_uid;\n\t\tregs[BPF_REG_0].type = PTR_TO_MAP_VALUE | ret_flag;\n\t\tif (!type_may_be_null(ret_type) &&\n\t\t    btf_record_has_field(meta.map_ptr->record, BPF_SPIN_LOCK)) {\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t\tbreak;\n\tcase RET_PTR_TO_SOCKET:\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCKET | ret_flag;\n\t\tbreak;\n\tcase RET_PTR_TO_SOCK_COMMON:\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_SOCK_COMMON | ret_flag;\n\t\tbreak;\n\tcase RET_PTR_TO_TCP_SOCK:\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_TCP_SOCK | ret_flag;\n\t\tbreak;\n\tcase RET_PTR_TO_MEM:\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_MEM | ret_flag;\n\t\tregs[BPF_REG_0].mem_size = meta.mem_size;\n\t\tbreak;\n\tcase RET_PTR_TO_MEM_OR_BTF_ID:\n\t{\n\t\tconst struct btf_type *t;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tt = btf_type_skip_modifiers(meta.ret_btf, meta.ret_btf_id, NULL);\n\t\tif (!btf_type_is_struct(t)) {\n\t\t\tu32 tsize;\n\t\t\tconst struct btf_type *ret;\n\t\t\tconst char *tname;\n\n\t\t\t \n\t\t\tret = btf_resolve_size(meta.ret_btf, t, &tsize);\n\t\t\tif (IS_ERR(ret)) {\n\t\t\t\ttname = btf_name_by_offset(meta.ret_btf, t->name_off);\n\t\t\t\tverbose(env, \"unable to resolve the size of type '%s': %ld\\n\",\n\t\t\t\t\ttname, PTR_ERR(ret));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MEM | ret_flag;\n\t\t\tregs[BPF_REG_0].mem_size = tsize;\n\t\t} else {\n\t\t\t \n\t\t\tret_flag &= ~MEM_RDONLY;\n\n\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | ret_flag;\n\t\t\tregs[BPF_REG_0].btf = meta.ret_btf;\n\t\t\tregs[BPF_REG_0].btf_id = meta.ret_btf_id;\n\t\t}\n\t\tbreak;\n\t}\n\tcase RET_PTR_TO_BTF_ID:\n\t{\n\t\tstruct btf *ret_btf;\n\t\tint ret_btf_id;\n\n\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | ret_flag;\n\t\tif (func_id == BPF_FUNC_kptr_xchg) {\n\t\t\tret_btf = meta.kptr_field->kptr.btf;\n\t\t\tret_btf_id = meta.kptr_field->kptr.btf_id;\n\t\t\tif (!btf_is_kernel(ret_btf))\n\t\t\t\tregs[BPF_REG_0].type |= MEM_ALLOC;\n\t\t} else {\n\t\t\tif (fn->ret_btf_id == BPF_PTR_POISON) {\n\t\t\t\tverbose(env, \"verifier internal error:\");\n\t\t\t\tverbose(env, \"func %s has non-overwritten BPF_PTR_POISON return type\\n\",\n\t\t\t\t\tfunc_id_name(func_id));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret_btf = btf_vmlinux;\n\t\t\tret_btf_id = *fn->ret_btf_id;\n\t\t}\n\t\tif (ret_btf_id == 0) {\n\t\t\tverbose(env, \"invalid return type %u of func %s#%d\\n\",\n\t\t\t\tbase_type(ret_type), func_id_name(func_id),\n\t\t\t\tfunc_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tregs[BPF_REG_0].btf = ret_btf;\n\t\tregs[BPF_REG_0].btf_id = ret_btf_id;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tverbose(env, \"unknown return type %u of func %s#%d\\n\",\n\t\t\tbase_type(ret_type), func_id_name(func_id), func_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (type_may_be_null(regs[BPF_REG_0].type))\n\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\n\tif (helper_multiple_ref_obj_use(func_id, meta.map_ptr)) {\n\t\tverbose(env, \"verifier internal error: func %s#%d sets ref_obj_id more than once\\n\",\n\t\t\tfunc_id_name(func_id), func_id);\n\t\treturn -EFAULT;\n\t}\n\n\tif (is_dynptr_ref_function(func_id))\n\t\tregs[BPF_REG_0].dynptr_id = meta.dynptr_id;\n\n\tif (is_ptr_cast_function(func_id) || is_dynptr_ref_function(func_id)) {\n\t\t \n\t\tregs[BPF_REG_0].ref_obj_id = meta.ref_obj_id;\n\t} else if (is_acquire_function(func_id, meta.map_ptr)) {\n\t\tint id = acquire_reference_state(env, insn_idx);\n\n\t\tif (id < 0)\n\t\t\treturn id;\n\t\t \n\t\tregs[BPF_REG_0].id = id;\n\t\t \n\t\tregs[BPF_REG_0].ref_obj_id = id;\n\t}\n\n\tdo_refine_retval_range(regs, fn->ret_type, func_id, &meta);\n\n\terr = check_map_func_compatibility(env, meta.map_ptr, func_id);\n\tif (err)\n\t\treturn err;\n\n\tif ((func_id == BPF_FUNC_get_stack ||\n\t     func_id == BPF_FUNC_get_task_stack) &&\n\t    !env->prog->has_callchain_buf) {\n\t\tconst char *err_str;\n\n#ifdef CONFIG_PERF_EVENTS\n\t\terr = get_callchain_buffers(sysctl_perf_event_max_stack);\n\t\terr_str = \"cannot get callchain buffer for func %s#%d\\n\";\n#else\n\t\terr = -ENOTSUPP;\n\t\terr_str = \"func %s#%d not supported without CONFIG_PERF_EVENTS\\n\";\n#endif\n\t\tif (err) {\n\t\t\tverbose(env, err_str, func_id_name(func_id), func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\tenv->prog->has_callchain_buf = true;\n\t}\n\n\tif (func_id == BPF_FUNC_get_stackid || func_id == BPF_FUNC_get_stack)\n\t\tenv->prog->call_get_stack = true;\n\n\tif (func_id == BPF_FUNC_get_func_ip) {\n\t\tif (check_get_func_ip(env))\n\t\t\treturn -ENOTSUPP;\n\t\tenv->prog->call_get_func_ip = true;\n\t}\n\n\tif (changes_data)\n\t\tclear_all_pkt_pointers(env);\n\treturn 0;\n}\n\n \nstatic void mark_btf_func_reg_size(struct bpf_verifier_env *env, u32 regno,\n\t\t\t\t   size_t reg_size)\n{\n\tstruct bpf_reg_state *reg = &cur_regs(env)[regno];\n\n\tif (regno == BPF_REG_0) {\n\t\t \n\t\treg->live |= REG_LIVE_WRITTEN;\n\t\treg->subreg_def = reg_size == sizeof(u64) ?\n\t\t\tDEF_NOT_SUBREG : env->insn_idx + 1;\n\t} else {\n\t\t \n\t\tif (reg_size == sizeof(u64)) {\n\t\t\tmark_insn_zext(env, reg);\n\t\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ64);\n\t\t} else {\n\t\t\tmark_reg_read(env, reg, reg->parent, REG_LIVE_READ32);\n\t\t}\n\t}\n}\n\nstatic bool is_kfunc_acquire(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_ACQUIRE;\n}\n\nstatic bool is_kfunc_release(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_RELEASE;\n}\n\nstatic bool is_kfunc_trusted_args(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn (meta->kfunc_flags & KF_TRUSTED_ARGS) || is_kfunc_release(meta);\n}\n\nstatic bool is_kfunc_sleepable(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_SLEEPABLE;\n}\n\nstatic bool is_kfunc_destructive(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_DESTRUCTIVE;\n}\n\nstatic bool is_kfunc_rcu(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->kfunc_flags & KF_RCU;\n}\n\nstatic bool __kfunc_param_match_suffix(const struct btf *btf,\n\t\t\t\t       const struct btf_param *arg,\n\t\t\t\t       const char *suffix)\n{\n\tint suffix_len = strlen(suffix), len;\n\tconst char *param_name;\n\n\t \n\tparam_name = btf_name_by_offset(btf, arg->name_off);\n\tif (str_is_empty(param_name))\n\t\treturn false;\n\tlen = strlen(param_name);\n\tif (len < suffix_len)\n\t\treturn false;\n\tparam_name += len - suffix_len;\n\treturn !strncmp(param_name, suffix, suffix_len);\n}\n\nstatic bool is_kfunc_arg_mem_size(const struct btf *btf,\n\t\t\t\t  const struct btf_param *arg,\n\t\t\t\t  const struct bpf_reg_state *reg)\n{\n\tconst struct btf_type *t;\n\n\tt = btf_type_skip_modifiers(btf, arg->type, NULL);\n\tif (!btf_type_is_scalar(t) || reg->type != SCALAR_VALUE)\n\t\treturn false;\n\n\treturn __kfunc_param_match_suffix(btf, arg, \"__sz\");\n}\n\nstatic bool is_kfunc_arg_const_mem_size(const struct btf *btf,\n\t\t\t\t\tconst struct btf_param *arg,\n\t\t\t\t\tconst struct bpf_reg_state *reg)\n{\n\tconst struct btf_type *t;\n\n\tt = btf_type_skip_modifiers(btf, arg->type, NULL);\n\tif (!btf_type_is_scalar(t) || reg->type != SCALAR_VALUE)\n\t\treturn false;\n\n\treturn __kfunc_param_match_suffix(btf, arg, \"__szk\");\n}\n\nstatic bool is_kfunc_arg_optional(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __kfunc_param_match_suffix(btf, arg, \"__opt\");\n}\n\nstatic bool is_kfunc_arg_constant(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __kfunc_param_match_suffix(btf, arg, \"__k\");\n}\n\nstatic bool is_kfunc_arg_ignore(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __kfunc_param_match_suffix(btf, arg, \"__ign\");\n}\n\nstatic bool is_kfunc_arg_alloc_obj(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __kfunc_param_match_suffix(btf, arg, \"__alloc\");\n}\n\nstatic bool is_kfunc_arg_uninit(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __kfunc_param_match_suffix(btf, arg, \"__uninit\");\n}\n\nstatic bool is_kfunc_arg_refcounted_kptr(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __kfunc_param_match_suffix(btf, arg, \"__refcounted_kptr\");\n}\n\nstatic bool is_kfunc_arg_scalar_with_name(const struct btf *btf,\n\t\t\t\t\t  const struct btf_param *arg,\n\t\t\t\t\t  const char *name)\n{\n\tint len, target_len = strlen(name);\n\tconst char *param_name;\n\n\tparam_name = btf_name_by_offset(btf, arg->name_off);\n\tif (str_is_empty(param_name))\n\t\treturn false;\n\tlen = strlen(param_name);\n\tif (len != target_len)\n\t\treturn false;\n\tif (strcmp(param_name, name))\n\t\treturn false;\n\n\treturn true;\n}\n\nenum {\n\tKF_ARG_DYNPTR_ID,\n\tKF_ARG_LIST_HEAD_ID,\n\tKF_ARG_LIST_NODE_ID,\n\tKF_ARG_RB_ROOT_ID,\n\tKF_ARG_RB_NODE_ID,\n};\n\nBTF_ID_LIST(kf_arg_btf_ids)\nBTF_ID(struct, bpf_dynptr_kern)\nBTF_ID(struct, bpf_list_head)\nBTF_ID(struct, bpf_list_node)\nBTF_ID(struct, bpf_rb_root)\nBTF_ID(struct, bpf_rb_node)\n\nstatic bool __is_kfunc_ptr_arg_type(const struct btf *btf,\n\t\t\t\t    const struct btf_param *arg, int type)\n{\n\tconst struct btf_type *t;\n\tu32 res_id;\n\n\tt = btf_type_skip_modifiers(btf, arg->type, NULL);\n\tif (!t)\n\t\treturn false;\n\tif (!btf_type_is_ptr(t))\n\t\treturn false;\n\tt = btf_type_skip_modifiers(btf, t->type, &res_id);\n\tif (!t)\n\t\treturn false;\n\treturn btf_types_are_same(btf, res_id, btf_vmlinux, kf_arg_btf_ids[type]);\n}\n\nstatic bool is_kfunc_arg_dynptr(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __is_kfunc_ptr_arg_type(btf, arg, KF_ARG_DYNPTR_ID);\n}\n\nstatic bool is_kfunc_arg_list_head(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __is_kfunc_ptr_arg_type(btf, arg, KF_ARG_LIST_HEAD_ID);\n}\n\nstatic bool is_kfunc_arg_list_node(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __is_kfunc_ptr_arg_type(btf, arg, KF_ARG_LIST_NODE_ID);\n}\n\nstatic bool is_kfunc_arg_rbtree_root(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __is_kfunc_ptr_arg_type(btf, arg, KF_ARG_RB_ROOT_ID);\n}\n\nstatic bool is_kfunc_arg_rbtree_node(const struct btf *btf, const struct btf_param *arg)\n{\n\treturn __is_kfunc_ptr_arg_type(btf, arg, KF_ARG_RB_NODE_ID);\n}\n\nstatic bool is_kfunc_arg_callback(struct bpf_verifier_env *env, const struct btf *btf,\n\t\t\t\t  const struct btf_param *arg)\n{\n\tconst struct btf_type *t;\n\n\tt = btf_type_resolve_func_ptr(btf, arg->type, NULL);\n\tif (!t)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool __btf_type_is_scalar_struct(struct bpf_verifier_env *env,\n\t\t\t\t\tconst struct btf *btf,\n\t\t\t\t\tconst struct btf_type *t, int rec)\n{\n\tconst struct btf_type *member_type;\n\tconst struct btf_member *member;\n\tu32 i;\n\n\tif (!btf_type_is_struct(t))\n\t\treturn false;\n\n\tfor_each_member(i, t, member) {\n\t\tconst struct btf_array *array;\n\n\t\tmember_type = btf_type_skip_modifiers(btf, member->type, NULL);\n\t\tif (btf_type_is_struct(member_type)) {\n\t\t\tif (rec >= 3) {\n\t\t\t\tverbose(env, \"max struct nesting depth exceeded\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tif (!__btf_type_is_scalar_struct(env, btf, member_type, rec + 1))\n\t\t\t\treturn false;\n\t\t\tcontinue;\n\t\t}\n\t\tif (btf_type_is_array(member_type)) {\n\t\t\tarray = btf_array(member_type);\n\t\t\tif (!array->nelems)\n\t\t\t\treturn false;\n\t\t\tmember_type = btf_type_skip_modifiers(btf, array->type, NULL);\n\t\t\tif (!btf_type_is_scalar(member_type))\n\t\t\t\treturn false;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!btf_type_is_scalar(member_type))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nenum kfunc_ptr_arg_type {\n\tKF_ARG_PTR_TO_CTX,\n\tKF_ARG_PTR_TO_ALLOC_BTF_ID,     \n\tKF_ARG_PTR_TO_REFCOUNTED_KPTR,  \n\tKF_ARG_PTR_TO_DYNPTR,\n\tKF_ARG_PTR_TO_ITER,\n\tKF_ARG_PTR_TO_LIST_HEAD,\n\tKF_ARG_PTR_TO_LIST_NODE,\n\tKF_ARG_PTR_TO_BTF_ID,\t        \n\tKF_ARG_PTR_TO_MEM,\n\tKF_ARG_PTR_TO_MEM_SIZE,\t        \n\tKF_ARG_PTR_TO_CALLBACK,\n\tKF_ARG_PTR_TO_RB_ROOT,\n\tKF_ARG_PTR_TO_RB_NODE,\n};\n\nenum special_kfunc_type {\n\tKF_bpf_obj_new_impl,\n\tKF_bpf_obj_drop_impl,\n\tKF_bpf_refcount_acquire_impl,\n\tKF_bpf_list_push_front_impl,\n\tKF_bpf_list_push_back_impl,\n\tKF_bpf_list_pop_front,\n\tKF_bpf_list_pop_back,\n\tKF_bpf_cast_to_kern_ctx,\n\tKF_bpf_rdonly_cast,\n\tKF_bpf_rcu_read_lock,\n\tKF_bpf_rcu_read_unlock,\n\tKF_bpf_rbtree_remove,\n\tKF_bpf_rbtree_add_impl,\n\tKF_bpf_rbtree_first,\n\tKF_bpf_dynptr_from_skb,\n\tKF_bpf_dynptr_from_xdp,\n\tKF_bpf_dynptr_slice,\n\tKF_bpf_dynptr_slice_rdwr,\n\tKF_bpf_dynptr_clone,\n};\n\nBTF_SET_START(special_kfunc_set)\nBTF_ID(func, bpf_obj_new_impl)\nBTF_ID(func, bpf_obj_drop_impl)\nBTF_ID(func, bpf_refcount_acquire_impl)\nBTF_ID(func, bpf_list_push_front_impl)\nBTF_ID(func, bpf_list_push_back_impl)\nBTF_ID(func, bpf_list_pop_front)\nBTF_ID(func, bpf_list_pop_back)\nBTF_ID(func, bpf_cast_to_kern_ctx)\nBTF_ID(func, bpf_rdonly_cast)\nBTF_ID(func, bpf_rbtree_remove)\nBTF_ID(func, bpf_rbtree_add_impl)\nBTF_ID(func, bpf_rbtree_first)\nBTF_ID(func, bpf_dynptr_from_skb)\nBTF_ID(func, bpf_dynptr_from_xdp)\nBTF_ID(func, bpf_dynptr_slice)\nBTF_ID(func, bpf_dynptr_slice_rdwr)\nBTF_ID(func, bpf_dynptr_clone)\nBTF_SET_END(special_kfunc_set)\n\nBTF_ID_LIST(special_kfunc_list)\nBTF_ID(func, bpf_obj_new_impl)\nBTF_ID(func, bpf_obj_drop_impl)\nBTF_ID(func, bpf_refcount_acquire_impl)\nBTF_ID(func, bpf_list_push_front_impl)\nBTF_ID(func, bpf_list_push_back_impl)\nBTF_ID(func, bpf_list_pop_front)\nBTF_ID(func, bpf_list_pop_back)\nBTF_ID(func, bpf_cast_to_kern_ctx)\nBTF_ID(func, bpf_rdonly_cast)\nBTF_ID(func, bpf_rcu_read_lock)\nBTF_ID(func, bpf_rcu_read_unlock)\nBTF_ID(func, bpf_rbtree_remove)\nBTF_ID(func, bpf_rbtree_add_impl)\nBTF_ID(func, bpf_rbtree_first)\nBTF_ID(func, bpf_dynptr_from_skb)\nBTF_ID(func, bpf_dynptr_from_xdp)\nBTF_ID(func, bpf_dynptr_slice)\nBTF_ID(func, bpf_dynptr_slice_rdwr)\nBTF_ID(func, bpf_dynptr_clone)\n\nstatic bool is_kfunc_ret_null(struct bpf_kfunc_call_arg_meta *meta)\n{\n\tif (meta->func_id == special_kfunc_list[KF_bpf_refcount_acquire_impl] &&\n\t    meta->arg_owning_ref) {\n\t\treturn false;\n\t}\n\n\treturn meta->kfunc_flags & KF_RET_NULL;\n}\n\nstatic bool is_kfunc_bpf_rcu_read_lock(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->func_id == special_kfunc_list[KF_bpf_rcu_read_lock];\n}\n\nstatic bool is_kfunc_bpf_rcu_read_unlock(struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn meta->func_id == special_kfunc_list[KF_bpf_rcu_read_unlock];\n}\n\nstatic enum kfunc_ptr_arg_type\nget_kfunc_ptr_arg_type(struct bpf_verifier_env *env,\n\t\t       struct bpf_kfunc_call_arg_meta *meta,\n\t\t       const struct btf_type *t, const struct btf_type *ref_t,\n\t\t       const char *ref_tname, const struct btf_param *args,\n\t\t       int argno, int nargs)\n{\n\tu32 regno = argno + 1;\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *reg = &regs[regno];\n\tbool arg_mem_size = false;\n\n\tif (meta->func_id == special_kfunc_list[KF_bpf_cast_to_kern_ctx])\n\t\treturn KF_ARG_PTR_TO_CTX;\n\n\t \n\tif (btf_get_prog_ctx_type(&env->log, meta->btf, t, resolve_prog_type(env->prog), argno))\n\t\treturn KF_ARG_PTR_TO_CTX;\n\n\tif (is_kfunc_arg_alloc_obj(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_ALLOC_BTF_ID;\n\n\tif (is_kfunc_arg_refcounted_kptr(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_REFCOUNTED_KPTR;\n\n\tif (is_kfunc_arg_dynptr(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_DYNPTR;\n\n\tif (is_kfunc_arg_iter(meta, argno))\n\t\treturn KF_ARG_PTR_TO_ITER;\n\n\tif (is_kfunc_arg_list_head(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_LIST_HEAD;\n\n\tif (is_kfunc_arg_list_node(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_LIST_NODE;\n\n\tif (is_kfunc_arg_rbtree_root(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_RB_ROOT;\n\n\tif (is_kfunc_arg_rbtree_node(meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_RB_NODE;\n\n\tif ((base_type(reg->type) == PTR_TO_BTF_ID || reg2btf_ids[base_type(reg->type)])) {\n\t\tif (!btf_type_is_struct(ref_t)) {\n\t\t\tverbose(env, \"kernel function %s args#%d pointer type %s %s is not supported\\n\",\n\t\t\t\tmeta->func_name, argno, btf_type_str(ref_t), ref_tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn KF_ARG_PTR_TO_BTF_ID;\n\t}\n\n\tif (is_kfunc_arg_callback(env, meta->btf, &args[argno]))\n\t\treturn KF_ARG_PTR_TO_CALLBACK;\n\n\n\tif (argno + 1 < nargs &&\n\t    (is_kfunc_arg_mem_size(meta->btf, &args[argno + 1], &regs[regno + 1]) ||\n\t     is_kfunc_arg_const_mem_size(meta->btf, &args[argno + 1], &regs[regno + 1])))\n\t\targ_mem_size = true;\n\n\t \n\tif (!btf_type_is_scalar(ref_t) && !__btf_type_is_scalar_struct(env, meta->btf, ref_t, 0) &&\n\t    (arg_mem_size ? !btf_type_is_void(ref_t) : 1)) {\n\t\tverbose(env, \"arg#%d pointer type %s %s must point to %sscalar, or struct with scalar\\n\",\n\t\t\targno, btf_type_str(ref_t), ref_tname, arg_mem_size ? \"void, \" : \"\");\n\t\treturn -EINVAL;\n\t}\n\treturn arg_mem_size ? KF_ARG_PTR_TO_MEM_SIZE : KF_ARG_PTR_TO_MEM;\n}\n\nstatic int process_kf_arg_ptr_to_btf_id(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_reg_state *reg,\n\t\t\t\t\tconst struct btf_type *ref_t,\n\t\t\t\t\tconst char *ref_tname, u32 ref_id,\n\t\t\t\t\tstruct bpf_kfunc_call_arg_meta *meta,\n\t\t\t\t\tint argno)\n{\n\tconst struct btf_type *reg_ref_t;\n\tbool strict_type_match = false;\n\tconst struct btf *reg_btf;\n\tconst char *reg_ref_tname;\n\tu32 reg_ref_id;\n\n\tif (base_type(reg->type) == PTR_TO_BTF_ID) {\n\t\treg_btf = reg->btf;\n\t\treg_ref_id = reg->btf_id;\n\t} else {\n\t\treg_btf = btf_vmlinux;\n\t\treg_ref_id = *reg2btf_ids[base_type(reg->type)];\n\t}\n\n\t \n\tif (is_kfunc_acquire(meta) ||\n\t    (is_kfunc_release(meta) && reg->ref_obj_id) ||\n\t    btf_type_ids_nocast_alias(&env->log, reg_btf, reg_ref_id, meta->btf, ref_id))\n\t\tstrict_type_match = true;\n\n\tWARN_ON_ONCE(is_kfunc_trusted_args(meta) && reg->off);\n\n\treg_ref_t = btf_type_skip_modifiers(reg_btf, reg_ref_id, &reg_ref_id);\n\treg_ref_tname = btf_name_by_offset(reg_btf, reg_ref_t->name_off);\n\tif (!btf_struct_ids_match(&env->log, reg_btf, reg_ref_id, reg->off, meta->btf, ref_id, strict_type_match)) {\n\t\tverbose(env, \"kernel function %s args#%d expected pointer to %s %s but R%d has a pointer to %s %s\\n\",\n\t\t\tmeta->func_name, argno, btf_type_str(ref_t), ref_tname, argno + 1,\n\t\t\tbtf_type_str(reg_ref_t), reg_ref_tname);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int ref_set_non_owning(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct btf_record *rec = reg_btf_record(reg);\n\n\tif (!state->active_lock.ptr) {\n\t\tverbose(env, \"verifier internal error: ref_set_non_owning w/o active lock\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (type_flag(reg->type) & NON_OWN_REF) {\n\t\tverbose(env, \"verifier internal error: NON_OWN_REF already set\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\treg->type |= NON_OWN_REF;\n\tif (rec->refcount_off >= 0)\n\t\treg->type |= MEM_RCU;\n\n\treturn 0;\n}\n\nstatic int ref_convert_owning_non_owning(struct bpf_verifier_env *env, u32 ref_obj_id)\n{\n\tstruct bpf_func_state *state, *unused;\n\tstruct bpf_reg_state *reg;\n\tint i;\n\n\tstate = cur_func(env);\n\n\tif (!ref_obj_id) {\n\t\tverbose(env, \"verifier internal error: ref_obj_id is zero for \"\n\t\t\t     \"owning -> non-owning conversion\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 0; i < state->acquired_refs; i++) {\n\t\tif (state->refs[i].id != ref_obj_id)\n\t\t\tcontinue;\n\n\t\t \n\t\tbpf_for_each_reg_in_vstate(env->cur_state, unused, reg, ({\n\t\t\tif (reg->ref_obj_id == ref_obj_id) {\n\t\t\t\treg->ref_obj_id = 0;\n\t\t\t\tref_set_non_owning(env, reg);\n\t\t\t}\n\t\t}));\n\t\treturn 0;\n\t}\n\n\tverbose(env, \"verifier internal error: ref state missing for ref_obj_id\\n\");\n\treturn -EFAULT;\n}\n\n \nstatic int check_reg_allocation_locked(struct bpf_verifier_env *env, struct bpf_reg_state *reg)\n{\n\tvoid *ptr;\n\tu32 id;\n\n\tswitch ((int)reg->type) {\n\tcase PTR_TO_MAP_VALUE:\n\t\tptr = reg->map_ptr;\n\t\tbreak;\n\tcase PTR_TO_BTF_ID | MEM_ALLOC:\n\t\tptr = reg->btf;\n\t\tbreak;\n\tdefault:\n\t\tverbose(env, \"verifier internal error: unknown reg type for lock check\\n\");\n\t\treturn -EFAULT;\n\t}\n\tid = reg->id;\n\n\tif (!env->cur_state->active_lock.ptr)\n\t\treturn -EINVAL;\n\tif (env->cur_state->active_lock.ptr != ptr ||\n\t    env->cur_state->active_lock.id != id) {\n\t\tverbose(env, \"held lock and object are not in the same allocation\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic bool is_bpf_list_api_kfunc(u32 btf_id)\n{\n\treturn btf_id == special_kfunc_list[KF_bpf_list_push_front_impl] ||\n\t       btf_id == special_kfunc_list[KF_bpf_list_push_back_impl] ||\n\t       btf_id == special_kfunc_list[KF_bpf_list_pop_front] ||\n\t       btf_id == special_kfunc_list[KF_bpf_list_pop_back];\n}\n\nstatic bool is_bpf_rbtree_api_kfunc(u32 btf_id)\n{\n\treturn btf_id == special_kfunc_list[KF_bpf_rbtree_add_impl] ||\n\t       btf_id == special_kfunc_list[KF_bpf_rbtree_remove] ||\n\t       btf_id == special_kfunc_list[KF_bpf_rbtree_first];\n}\n\nstatic bool is_bpf_graph_api_kfunc(u32 btf_id)\n{\n\treturn is_bpf_list_api_kfunc(btf_id) || is_bpf_rbtree_api_kfunc(btf_id) ||\n\t       btf_id == special_kfunc_list[KF_bpf_refcount_acquire_impl];\n}\n\nstatic bool is_callback_calling_kfunc(u32 btf_id)\n{\n\treturn btf_id == special_kfunc_list[KF_bpf_rbtree_add_impl];\n}\n\nstatic bool is_rbtree_lock_required_kfunc(u32 btf_id)\n{\n\treturn is_bpf_rbtree_api_kfunc(btf_id);\n}\n\nstatic bool check_kfunc_is_graph_root_api(struct bpf_verifier_env *env,\n\t\t\t\t\t  enum btf_field_type head_field_type,\n\t\t\t\t\t  u32 kfunc_btf_id)\n{\n\tbool ret;\n\n\tswitch (head_field_type) {\n\tcase BPF_LIST_HEAD:\n\t\tret = is_bpf_list_api_kfunc(kfunc_btf_id);\n\t\tbreak;\n\tcase BPF_RB_ROOT:\n\t\tret = is_bpf_rbtree_api_kfunc(kfunc_btf_id);\n\t\tbreak;\n\tdefault:\n\t\tverbose(env, \"verifier internal error: unexpected graph root argument type %s\\n\",\n\t\t\tbtf_field_type_name(head_field_type));\n\t\treturn false;\n\t}\n\n\tif (!ret)\n\t\tverbose(env, \"verifier internal error: %s head arg for unknown kfunc\\n\",\n\t\t\tbtf_field_type_name(head_field_type));\n\treturn ret;\n}\n\nstatic bool check_kfunc_is_graph_node_api(struct bpf_verifier_env *env,\n\t\t\t\t\t  enum btf_field_type node_field_type,\n\t\t\t\t\t  u32 kfunc_btf_id)\n{\n\tbool ret;\n\n\tswitch (node_field_type) {\n\tcase BPF_LIST_NODE:\n\t\tret = (kfunc_btf_id == special_kfunc_list[KF_bpf_list_push_front_impl] ||\n\t\t       kfunc_btf_id == special_kfunc_list[KF_bpf_list_push_back_impl]);\n\t\tbreak;\n\tcase BPF_RB_NODE:\n\t\tret = (kfunc_btf_id == special_kfunc_list[KF_bpf_rbtree_remove] ||\n\t\t       kfunc_btf_id == special_kfunc_list[KF_bpf_rbtree_add_impl]);\n\t\tbreak;\n\tdefault:\n\t\tverbose(env, \"verifier internal error: unexpected graph node argument type %s\\n\",\n\t\t\tbtf_field_type_name(node_field_type));\n\t\treturn false;\n\t}\n\n\tif (!ret)\n\t\tverbose(env, \"verifier internal error: %s node arg for unknown kfunc\\n\",\n\t\t\tbtf_field_type_name(node_field_type));\n\treturn ret;\n}\n\nstatic int\n__process_kf_arg_ptr_to_graph_root(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *reg, u32 regno,\n\t\t\t\t   struct bpf_kfunc_call_arg_meta *meta,\n\t\t\t\t   enum btf_field_type head_field_type,\n\t\t\t\t   struct btf_field **head_field)\n{\n\tconst char *head_type_name;\n\tstruct btf_field *field;\n\tstruct btf_record *rec;\n\tu32 head_off;\n\n\tif (meta->btf != btf_vmlinux) {\n\t\tverbose(env, \"verifier internal error: unexpected btf mismatch in kfunc call\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!check_kfunc_is_graph_root_api(env, head_field_type, meta->func_id))\n\t\treturn -EFAULT;\n\n\thead_type_name = btf_field_type_name(head_field_type);\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. %s has to be at the constant offset\\n\",\n\t\t\tregno, head_type_name);\n\t\treturn -EINVAL;\n\t}\n\n\trec = reg_btf_record(reg);\n\thead_off = reg->off + reg->var_off.value;\n\tfield = btf_record_find(rec, head_off, head_field_type);\n\tif (!field) {\n\t\tverbose(env, \"%s not found at offset=%u\\n\", head_type_name, head_off);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (check_reg_allocation_locked(env, reg)) {\n\t\tverbose(env, \"bpf_spin_lock at off=%d must be held for %s\\n\",\n\t\t\trec->spin_lock_off, head_type_name);\n\t\treturn -EINVAL;\n\t}\n\n\tif (*head_field) {\n\t\tverbose(env, \"verifier internal error: repeating %s arg\\n\", head_type_name);\n\t\treturn -EFAULT;\n\t}\n\t*head_field = field;\n\treturn 0;\n}\n\nstatic int process_kf_arg_ptr_to_list_head(struct bpf_verifier_env *env,\n\t\t\t\t\t   struct bpf_reg_state *reg, u32 regno,\n\t\t\t\t\t   struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn __process_kf_arg_ptr_to_graph_root(env, reg, regno, meta, BPF_LIST_HEAD,\n\t\t\t\t\t\t\t  &meta->arg_list_head.field);\n}\n\nstatic int process_kf_arg_ptr_to_rbtree_root(struct bpf_verifier_env *env,\n\t\t\t\t\t     struct bpf_reg_state *reg, u32 regno,\n\t\t\t\t\t     struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn __process_kf_arg_ptr_to_graph_root(env, reg, regno, meta, BPF_RB_ROOT,\n\t\t\t\t\t\t\t  &meta->arg_rbtree_root.field);\n}\n\nstatic int\n__process_kf_arg_ptr_to_graph_node(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_reg_state *reg, u32 regno,\n\t\t\t\t   struct bpf_kfunc_call_arg_meta *meta,\n\t\t\t\t   enum btf_field_type head_field_type,\n\t\t\t\t   enum btf_field_type node_field_type,\n\t\t\t\t   struct btf_field **node_field)\n{\n\tconst char *node_type_name;\n\tconst struct btf_type *et, *t;\n\tstruct btf_field *field;\n\tu32 node_off;\n\n\tif (meta->btf != btf_vmlinux) {\n\t\tverbose(env, \"verifier internal error: unexpected btf mismatch in kfunc call\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!check_kfunc_is_graph_node_api(env, node_field_type, meta->func_id))\n\t\treturn -EFAULT;\n\n\tnode_type_name = btf_field_type_name(node_field_type);\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tverbose(env,\n\t\t\t\"R%d doesn't have constant offset. %s has to be at the constant offset\\n\",\n\t\t\tregno, node_type_name);\n\t\treturn -EINVAL;\n\t}\n\n\tnode_off = reg->off + reg->var_off.value;\n\tfield = reg_find_field_offset(reg, node_off, node_field_type);\n\tif (!field || field->offset != node_off) {\n\t\tverbose(env, \"%s not found at offset=%u\\n\", node_type_name, node_off);\n\t\treturn -EINVAL;\n\t}\n\n\tfield = *node_field;\n\n\tet = btf_type_by_id(field->graph_root.btf, field->graph_root.value_btf_id);\n\tt = btf_type_by_id(reg->btf, reg->btf_id);\n\tif (!btf_struct_ids_match(&env->log, reg->btf, reg->btf_id, 0, field->graph_root.btf,\n\t\t\t\t  field->graph_root.value_btf_id, true)) {\n\t\tverbose(env, \"operation on %s expects arg#1 %s at offset=%d \"\n\t\t\t\"in struct %s, but arg is at offset=%d in struct %s\\n\",\n\t\t\tbtf_field_type_name(head_field_type),\n\t\t\tbtf_field_type_name(node_field_type),\n\t\t\tfield->graph_root.node_offset,\n\t\t\tbtf_name_by_offset(field->graph_root.btf, et->name_off),\n\t\t\tnode_off, btf_name_by_offset(reg->btf, t->name_off));\n\t\treturn -EINVAL;\n\t}\n\tmeta->arg_btf = reg->btf;\n\tmeta->arg_btf_id = reg->btf_id;\n\n\tif (node_off != field->graph_root.node_offset) {\n\t\tverbose(env, \"arg#1 offset=%d, but expected %s at offset=%d in struct %s\\n\",\n\t\t\tnode_off, btf_field_type_name(node_field_type),\n\t\t\tfield->graph_root.node_offset,\n\t\t\tbtf_name_by_offset(field->graph_root.btf, et->name_off));\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_kf_arg_ptr_to_list_node(struct bpf_verifier_env *env,\n\t\t\t\t\t   struct bpf_reg_state *reg, u32 regno,\n\t\t\t\t\t   struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn __process_kf_arg_ptr_to_graph_node(env, reg, regno, meta,\n\t\t\t\t\t\t  BPF_LIST_HEAD, BPF_LIST_NODE,\n\t\t\t\t\t\t  &meta->arg_list_head.field);\n}\n\nstatic int process_kf_arg_ptr_to_rbtree_node(struct bpf_verifier_env *env,\n\t\t\t\t\t     struct bpf_reg_state *reg, u32 regno,\n\t\t\t\t\t     struct bpf_kfunc_call_arg_meta *meta)\n{\n\treturn __process_kf_arg_ptr_to_graph_node(env, reg, regno, meta,\n\t\t\t\t\t\t  BPF_RB_ROOT, BPF_RB_NODE,\n\t\t\t\t\t\t  &meta->arg_rbtree_root.field);\n}\n\nstatic int check_kfunc_args(struct bpf_verifier_env *env, struct bpf_kfunc_call_arg_meta *meta,\n\t\t\t    int insn_idx)\n{\n\tconst char *func_name = meta->func_name, *ref_tname;\n\tconst struct btf *btf = meta->btf;\n\tconst struct btf_param *args;\n\tstruct btf_record *rec;\n\tu32 i, nargs;\n\tint ret;\n\n\targs = (const struct btf_param *)(meta->func_proto + 1);\n\tnargs = btf_type_vlen(meta->func_proto);\n\tif (nargs > MAX_BPF_FUNC_REG_ARGS) {\n\t\tverbose(env, \"Function %s has %d > %d args\\n\", func_name, nargs,\n\t\t\tMAX_BPF_FUNC_REG_ARGS);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfor (i = 0; i < nargs; i++) {\n\t\tstruct bpf_reg_state *regs = cur_regs(env), *reg = &regs[i + 1];\n\t\tconst struct btf_type *t, *ref_t, *resolve_ret;\n\t\tenum bpf_arg_type arg_type = ARG_DONTCARE;\n\t\tu32 regno = i + 1, ref_id, type_size;\n\t\tbool is_ret_buf_sz = false;\n\t\tint kf_arg_type;\n\n\t\tt = btf_type_skip_modifiers(btf, args[i].type, NULL);\n\n\t\tif (is_kfunc_arg_ignore(btf, &args[i]))\n\t\t\tcontinue;\n\n\t\tif (btf_type_is_scalar(t)) {\n\t\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\t\tverbose(env, \"R%d is not a scalar\\n\", regno);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (is_kfunc_arg_constant(meta->btf, &args[i])) {\n\t\t\t\tif (meta->arg_constant.found) {\n\t\t\t\t\tverbose(env, \"verifier internal error: only one constant argument permitted\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\t\t\tverbose(env, \"R%d must be a known constant\\n\", regno);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tret = mark_chain_precision(env, regno);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\treturn ret;\n\t\t\t\tmeta->arg_constant.found = true;\n\t\t\t\tmeta->arg_constant.value = reg->var_off.value;\n\t\t\t} else if (is_kfunc_arg_scalar_with_name(btf, &args[i], \"rdonly_buf_size\")) {\n\t\t\t\tmeta->r0_rdonly = true;\n\t\t\t\tis_ret_buf_sz = true;\n\t\t\t} else if (is_kfunc_arg_scalar_with_name(btf, &args[i], \"rdwr_buf_size\")) {\n\t\t\t\tis_ret_buf_sz = true;\n\t\t\t}\n\n\t\t\tif (is_ret_buf_sz) {\n\t\t\t\tif (meta->r0_size) {\n\t\t\t\t\tverbose(env, \"2 or more rdonly/rdwr_buf_size parameters for kfunc\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (!tnum_is_const(reg->var_off)) {\n\t\t\t\t\tverbose(env, \"R%d is not a const\\n\", regno);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tmeta->r0_size = reg->var_off.value;\n\t\t\t\tret = mark_chain_precision(env, regno);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!btf_type_is_ptr(t)) {\n\t\t\tverbose(env, \"Unrecognized arg#%d type %s\\n\", i, btf_type_str(t));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((is_kfunc_trusted_args(meta) || is_kfunc_rcu(meta)) &&\n\t\t    (register_is_null(reg) || type_may_be_null(reg->type))) {\n\t\t\tverbose(env, \"Possibly NULL pointer passed to trusted arg%d\\n\", i);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (reg->ref_obj_id) {\n\t\t\tif (is_kfunc_release(meta) && meta->ref_obj_id) {\n\t\t\t\tverbose(env, \"verifier internal error: more than one arg with ref_obj_id R%d %u %u\\n\",\n\t\t\t\t\tregno, reg->ref_obj_id,\n\t\t\t\t\tmeta->ref_obj_id);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tmeta->ref_obj_id = reg->ref_obj_id;\n\t\t\tif (is_kfunc_release(meta))\n\t\t\t\tmeta->release_regno = regno;\n\t\t}\n\n\t\tref_t = btf_type_skip_modifiers(btf, t->type, &ref_id);\n\t\tref_tname = btf_name_by_offset(btf, ref_t->name_off);\n\n\t\tkf_arg_type = get_kfunc_ptr_arg_type(env, meta, t, ref_t, ref_tname, args, i, nargs);\n\t\tif (kf_arg_type < 0)\n\t\t\treturn kf_arg_type;\n\n\t\tswitch (kf_arg_type) {\n\t\tcase KF_ARG_PTR_TO_ALLOC_BTF_ID:\n\t\tcase KF_ARG_PTR_TO_BTF_ID:\n\t\t\tif (!is_kfunc_trusted_args(meta) && !is_kfunc_rcu(meta))\n\t\t\t\tbreak;\n\n\t\t\tif (!is_trusted_reg(reg)) {\n\t\t\t\tif (!is_kfunc_rcu(meta)) {\n\t\t\t\t\tverbose(env, \"R%d must be referenced or trusted\\n\", regno);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (!is_rcu_reg(reg)) {\n\t\t\t\t\tverbose(env, \"R%d must be a rcu pointer\\n\", regno);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfallthrough;\n\t\tcase KF_ARG_PTR_TO_CTX:\n\t\t\t \n\t\t\targ_type |= OBJ_RELEASE;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_DYNPTR:\n\t\tcase KF_ARG_PTR_TO_ITER:\n\t\tcase KF_ARG_PTR_TO_LIST_HEAD:\n\t\tcase KF_ARG_PTR_TO_LIST_NODE:\n\t\tcase KF_ARG_PTR_TO_RB_ROOT:\n\t\tcase KF_ARG_PTR_TO_RB_NODE:\n\t\tcase KF_ARG_PTR_TO_MEM:\n\t\tcase KF_ARG_PTR_TO_MEM_SIZE:\n\t\tcase KF_ARG_PTR_TO_CALLBACK:\n\t\tcase KF_ARG_PTR_TO_REFCOUNTED_KPTR:\n\t\t\t \n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (is_kfunc_release(meta) && reg->ref_obj_id)\n\t\t\targ_type |= OBJ_RELEASE;\n\t\tret = check_func_arg_reg_off(env, reg, regno, arg_type);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tswitch (kf_arg_type) {\n\t\tcase KF_ARG_PTR_TO_CTX:\n\t\t\tif (reg->type != PTR_TO_CTX) {\n\t\t\t\tverbose(env, \"arg#%d expected pointer to ctx, but got %s\\n\", i, btf_type_str(t));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (meta->func_id == special_kfunc_list[KF_bpf_cast_to_kern_ctx]) {\n\t\t\t\tret = get_kern_ctx_btf_id(&env->log, resolve_prog_type(env->prog));\n\t\t\t\tif (ret < 0)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tmeta->ret_btf_id  = ret;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_ALLOC_BTF_ID:\n\t\t\tif (reg->type != (PTR_TO_BTF_ID | MEM_ALLOC)) {\n\t\t\t\tverbose(env, \"arg#%d expected pointer to allocated object\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!reg->ref_obj_id) {\n\t\t\t\tverbose(env, \"allocated object must be referenced\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (meta->btf == btf_vmlinux &&\n\t\t\t    meta->func_id == special_kfunc_list[KF_bpf_obj_drop_impl]) {\n\t\t\t\tmeta->arg_btf = reg->btf;\n\t\t\t\tmeta->arg_btf_id = reg->btf_id;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_DYNPTR:\n\t\t{\n\t\t\tenum bpf_arg_type dynptr_arg_type = ARG_PTR_TO_DYNPTR;\n\t\t\tint clone_ref_obj_id = 0;\n\n\t\t\tif (reg->type != PTR_TO_STACK &&\n\t\t\t    reg->type != CONST_PTR_TO_DYNPTR) {\n\t\t\t\tverbose(env, \"arg#%d expected pointer to stack or dynptr_ptr\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (reg->type == CONST_PTR_TO_DYNPTR)\n\t\t\t\tdynptr_arg_type |= MEM_RDONLY;\n\n\t\t\tif (is_kfunc_arg_uninit(btf, &args[i]))\n\t\t\t\tdynptr_arg_type |= MEM_UNINIT;\n\n\t\t\tif (meta->func_id == special_kfunc_list[KF_bpf_dynptr_from_skb]) {\n\t\t\t\tdynptr_arg_type |= DYNPTR_TYPE_SKB;\n\t\t\t} else if (meta->func_id == special_kfunc_list[KF_bpf_dynptr_from_xdp]) {\n\t\t\t\tdynptr_arg_type |= DYNPTR_TYPE_XDP;\n\t\t\t} else if (meta->func_id == special_kfunc_list[KF_bpf_dynptr_clone] &&\n\t\t\t\t   (dynptr_arg_type & MEM_UNINIT)) {\n\t\t\t\tenum bpf_dynptr_type parent_type = meta->initialized_dynptr.type;\n\n\t\t\t\tif (parent_type == BPF_DYNPTR_TYPE_INVALID) {\n\t\t\t\t\tverbose(env, \"verifier internal error: no dynptr type for parent of clone\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\n\t\t\t\tdynptr_arg_type |= (unsigned int)get_dynptr_type_flag(parent_type);\n\t\t\t\tclone_ref_obj_id = meta->initialized_dynptr.ref_obj_id;\n\t\t\t\tif (dynptr_type_refcounted(parent_type) && !clone_ref_obj_id) {\n\t\t\t\t\tverbose(env, \"verifier internal error: missing ref obj id for parent of clone\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = process_dynptr_func(env, regno, insn_idx, dynptr_arg_type, clone_ref_obj_id);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\n\t\t\tif (!(dynptr_arg_type & MEM_UNINIT)) {\n\t\t\t\tint id = dynptr_id(env, reg);\n\n\t\t\t\tif (id < 0) {\n\t\t\t\t\tverbose(env, \"verifier internal error: failed to obtain dynptr id\\n\");\n\t\t\t\t\treturn id;\n\t\t\t\t}\n\t\t\t\tmeta->initialized_dynptr.id = id;\n\t\t\t\tmeta->initialized_dynptr.type = dynptr_get_type(env, reg);\n\t\t\t\tmeta->initialized_dynptr.ref_obj_id = dynptr_ref_obj_id(env, reg);\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t\tcase KF_ARG_PTR_TO_ITER:\n\t\t\tret = process_iter_arg(env, regno, insn_idx, meta);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_LIST_HEAD:\n\t\t\tif (reg->type != PTR_TO_MAP_VALUE &&\n\t\t\t    reg->type != (PTR_TO_BTF_ID | MEM_ALLOC)) {\n\t\t\t\tverbose(env, \"arg#%d expected pointer to map value or allocated object\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (reg->type == (PTR_TO_BTF_ID | MEM_ALLOC) && !reg->ref_obj_id) {\n\t\t\t\tverbose(env, \"allocated object must be referenced\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = process_kf_arg_ptr_to_list_head(env, reg, regno, meta);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_RB_ROOT:\n\t\t\tif (reg->type != PTR_TO_MAP_VALUE &&\n\t\t\t    reg->type != (PTR_TO_BTF_ID | MEM_ALLOC)) {\n\t\t\t\tverbose(env, \"arg#%d expected pointer to map value or allocated object\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (reg->type == (PTR_TO_BTF_ID | MEM_ALLOC) && !reg->ref_obj_id) {\n\t\t\t\tverbose(env, \"allocated object must be referenced\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = process_kf_arg_ptr_to_rbtree_root(env, reg, regno, meta);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_LIST_NODE:\n\t\t\tif (reg->type != (PTR_TO_BTF_ID | MEM_ALLOC)) {\n\t\t\t\tverbose(env, \"arg#%d expected pointer to allocated object\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!reg->ref_obj_id) {\n\t\t\t\tverbose(env, \"allocated object must be referenced\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = process_kf_arg_ptr_to_list_node(env, reg, regno, meta);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_RB_NODE:\n\t\t\tif (meta->func_id == special_kfunc_list[KF_bpf_rbtree_remove]) {\n\t\t\t\tif (!type_is_non_owning_ref(reg->type) || reg->ref_obj_id) {\n\t\t\t\t\tverbose(env, \"rbtree_remove node input must be non-owning ref\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (in_rbtree_lock_required_cb(env)) {\n\t\t\t\t\tverbose(env, \"rbtree_remove not allowed in rbtree cb\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (reg->type != (PTR_TO_BTF_ID | MEM_ALLOC)) {\n\t\t\t\t\tverbose(env, \"arg#%d expected pointer to allocated object\\n\", i);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tif (!reg->ref_obj_id) {\n\t\t\t\t\tverbose(env, \"allocated object must be referenced\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tret = process_kf_arg_ptr_to_rbtree_node(env, reg, regno, meta);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_BTF_ID:\n\t\t\t \n\t\t\tif ((base_type(reg->type) != PTR_TO_BTF_ID ||\n\t\t\t     (bpf_type_has_unsafe_modifiers(reg->type) && !is_rcu_reg(reg))) &&\n\t\t\t    !reg2btf_ids[base_type(reg->type)]) {\n\t\t\t\tverbose(env, \"arg#%d is %s \", i, reg_type_str(env, reg->type));\n\t\t\t\tverbose(env, \"expected %s or socket\\n\",\n\t\t\t\t\treg_type_str(env, base_type(reg->type) |\n\t\t\t\t\t\t\t  (type_flag(reg->type) & BPF_REG_TRUSTED_MODIFIERS)));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = process_kf_arg_ptr_to_btf_id(env, reg, ref_t, ref_tname, ref_id, meta, i);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_MEM:\n\t\t\tresolve_ret = btf_resolve_size(btf, ref_t, &type_size);\n\t\t\tif (IS_ERR(resolve_ret)) {\n\t\t\t\tverbose(env, \"arg#%d reference type('%s %s') size cannot be determined: %ld\\n\",\n\t\t\t\t\ti, btf_type_str(ref_t), ref_tname, PTR_ERR(resolve_ret));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = check_mem_reg(env, reg, regno, type_size);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_MEM_SIZE:\n\t\t{\n\t\t\tstruct bpf_reg_state *buff_reg = &regs[regno];\n\t\t\tconst struct btf_param *buff_arg = &args[i];\n\t\t\tstruct bpf_reg_state *size_reg = &regs[regno + 1];\n\t\t\tconst struct btf_param *size_arg = &args[i + 1];\n\n\t\t\tif (!register_is_null(buff_reg) || !is_kfunc_arg_optional(meta->btf, buff_arg)) {\n\t\t\t\tret = check_kfunc_mem_size_reg(env, size_reg, regno + 1);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"arg#%d arg#%d memory, len pair leads to invalid memory access\\n\", i, i + 1);\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (is_kfunc_arg_const_mem_size(meta->btf, size_arg, size_reg)) {\n\t\t\t\tif (meta->arg_constant.found) {\n\t\t\t\t\tverbose(env, \"verifier internal error: only one constant argument permitted\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tif (!tnum_is_const(size_reg->var_off)) {\n\t\t\t\t\tverbose(env, \"R%d must be a known constant\\n\", regno + 1);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tmeta->arg_constant.found = true;\n\t\t\t\tmeta->arg_constant.value = size_reg->var_off.value;\n\t\t\t}\n\n\t\t\t \n\t\t\ti++;\n\t\t\tbreak;\n\t\t}\n\t\tcase KF_ARG_PTR_TO_CALLBACK:\n\t\t\tif (reg->type != PTR_TO_FUNC) {\n\t\t\t\tverbose(env, \"arg%d expected pointer to func\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tmeta->subprogno = reg->subprogno;\n\t\t\tbreak;\n\t\tcase KF_ARG_PTR_TO_REFCOUNTED_KPTR:\n\t\t\tif (!type_is_ptr_alloc_obj(reg->type)) {\n\t\t\t\tverbose(env, \"arg#%d is neither owning or non-owning ref\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!type_is_non_owning_ref(reg->type))\n\t\t\t\tmeta->arg_owning_ref = true;\n\n\t\t\trec = reg_btf_record(reg);\n\t\t\tif (!rec) {\n\t\t\t\tverbose(env, \"verifier internal error: Couldn't find btf_record\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\n\t\t\tif (rec->refcount_off < 0) {\n\t\t\t\tverbose(env, \"arg#%d doesn't point to a type with bpf_refcount field\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmeta->arg_btf = reg->btf;\n\t\t\tmeta->arg_btf_id = reg->btf_id;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (is_kfunc_release(meta) && !meta->release_regno) {\n\t\tverbose(env, \"release kernel function %s expects refcounted PTR_TO_BTF_ID\\n\",\n\t\t\tfunc_name);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int fetch_kfunc_meta(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    struct bpf_kfunc_call_arg_meta *meta,\n\t\t\t    const char **kfunc_name)\n{\n\tconst struct btf_type *func, *func_proto;\n\tu32 func_id, *kfunc_flags;\n\tconst char *func_name;\n\tstruct btf *desc_btf;\n\n\tif (kfunc_name)\n\t\t*kfunc_name = NULL;\n\n\tif (!insn->imm)\n\t\treturn -EINVAL;\n\n\tdesc_btf = find_kfunc_desc_btf(env, insn->off);\n\tif (IS_ERR(desc_btf))\n\t\treturn PTR_ERR(desc_btf);\n\n\tfunc_id = insn->imm;\n\tfunc = btf_type_by_id(desc_btf, func_id);\n\tfunc_name = btf_name_by_offset(desc_btf, func->name_off);\n\tif (kfunc_name)\n\t\t*kfunc_name = func_name;\n\tfunc_proto = btf_type_by_id(desc_btf, func->type);\n\n\tkfunc_flags = btf_kfunc_id_set_contains(desc_btf, func_id, env->prog);\n\tif (!kfunc_flags) {\n\t\treturn -EACCES;\n\t}\n\n\tmemset(meta, 0, sizeof(*meta));\n\tmeta->btf = desc_btf;\n\tmeta->func_id = func_id;\n\tmeta->kfunc_flags = *kfunc_flags;\n\tmeta->func_proto = func_proto;\n\tmeta->func_name = func_name;\n\n\treturn 0;\n}\n\nstatic int check_kfunc_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t    int *insn_idx_p)\n{\n\tconst struct btf_type *t, *ptr_type;\n\tu32 i, nargs, ptr_type_id, release_ref_obj_id;\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tconst char *func_name, *ptr_type_name;\n\tbool sleepable, rcu_lock, rcu_unlock;\n\tstruct bpf_kfunc_call_arg_meta meta;\n\tstruct bpf_insn_aux_data *insn_aux;\n\tint err, insn_idx = *insn_idx_p;\n\tconst struct btf_param *args;\n\tconst struct btf_type *ret_t;\n\tstruct btf *desc_btf;\n\n\t \n\tif (!insn->imm)\n\t\treturn 0;\n\n\terr = fetch_kfunc_meta(env, insn, &meta, &func_name);\n\tif (err == -EACCES && func_name)\n\t\tverbose(env, \"calling kernel function %s is not allowed\\n\", func_name);\n\tif (err)\n\t\treturn err;\n\tdesc_btf = meta.btf;\n\tinsn_aux = &env->insn_aux_data[insn_idx];\n\n\tinsn_aux->is_iter_next = is_iter_next_kfunc(&meta);\n\n\tif (is_kfunc_destructive(&meta) && !capable(CAP_SYS_BOOT)) {\n\t\tverbose(env, \"destructive kfunc calls require CAP_SYS_BOOT capability\\n\");\n\t\treturn -EACCES;\n\t}\n\n\tsleepable = is_kfunc_sleepable(&meta);\n\tif (sleepable && !env->prog->aux->sleepable) {\n\t\tverbose(env, \"program must be sleepable to call sleepable kfunc %s\\n\", func_name);\n\t\treturn -EACCES;\n\t}\n\n\trcu_lock = is_kfunc_bpf_rcu_read_lock(&meta);\n\trcu_unlock = is_kfunc_bpf_rcu_read_unlock(&meta);\n\n\tif (env->cur_state->active_rcu_lock) {\n\t\tstruct bpf_func_state *state;\n\t\tstruct bpf_reg_state *reg;\n\n\t\tif (in_rbtree_lock_required_cb(env) && (rcu_lock || rcu_unlock)) {\n\t\t\tverbose(env, \"Calling bpf_rcu_read_{lock,unlock} in unnecessary rbtree callback\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tif (rcu_lock) {\n\t\t\tverbose(env, \"nested rcu read lock (kernel function %s)\\n\", func_name);\n\t\t\treturn -EINVAL;\n\t\t} else if (rcu_unlock) {\n\t\t\tbpf_for_each_reg_in_vstate(env->cur_state, state, reg, ({\n\t\t\t\tif (reg->type & MEM_RCU) {\n\t\t\t\t\treg->type &= ~(MEM_RCU | PTR_MAYBE_NULL);\n\t\t\t\t\treg->type |= PTR_UNTRUSTED;\n\t\t\t\t}\n\t\t\t}));\n\t\t\tenv->cur_state->active_rcu_lock = false;\n\t\t} else if (sleepable) {\n\t\t\tverbose(env, \"kernel func %s is sleepable within rcu_read_lock region\\n\", func_name);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else if (rcu_lock) {\n\t\tenv->cur_state->active_rcu_lock = true;\n\t} else if (rcu_unlock) {\n\t\tverbose(env, \"unmatched rcu read unlock (kernel function %s)\\n\", func_name);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\terr = check_kfunc_args(env, &meta, insn_idx);\n\tif (err < 0)\n\t\treturn err;\n\t \n\tif (meta.release_regno) {\n\t\terr = release_reference(env, regs[meta.release_regno].ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"kfunc %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_name, meta.func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (meta.func_id == special_kfunc_list[KF_bpf_list_push_front_impl] ||\n\t    meta.func_id == special_kfunc_list[KF_bpf_list_push_back_impl] ||\n\t    meta.func_id == special_kfunc_list[KF_bpf_rbtree_add_impl]) {\n\t\trelease_ref_obj_id = regs[BPF_REG_2].ref_obj_id;\n\t\tinsn_aux->insert_off = regs[BPF_REG_2].off;\n\t\tinsn_aux->kptr_struct_meta = btf_find_struct_meta(meta.arg_btf, meta.arg_btf_id);\n\t\terr = ref_convert_owning_non_owning(env, release_ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"kfunc %s#%d conversion of owning ref to non-owning failed\\n\",\n\t\t\t\tfunc_name, meta.func_id);\n\t\t\treturn err;\n\t\t}\n\n\t\terr = release_reference(env, release_ref_obj_id);\n\t\tif (err) {\n\t\t\tverbose(env, \"kfunc %s#%d reference has not been acquired before\\n\",\n\t\t\t\tfunc_name, meta.func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (meta.func_id == special_kfunc_list[KF_bpf_rbtree_add_impl]) {\n\t\terr = __check_func_call(env, insn, insn_idx_p, meta.subprogno,\n\t\t\t\t\tset_rbtree_add_callback_state);\n\t\tif (err) {\n\t\t\tverbose(env, \"kfunc %s#%d failed callback verification\\n\",\n\t\t\t\tfunc_name, meta.func_id);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tfor (i = 0; i < CALLER_SAVED_REGS; i++)\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\n\t \n\tt = btf_type_skip_modifiers(desc_btf, meta.func_proto->type, NULL);\n\n\tif (is_kfunc_acquire(&meta) && !btf_type_is_struct_ptr(meta.btf, t)) {\n\t\t \n\t\tif (meta.btf != btf_vmlinux ||\n\t\t    (meta.func_id != special_kfunc_list[KF_bpf_obj_new_impl] &&\n\t\t     meta.func_id != special_kfunc_list[KF_bpf_refcount_acquire_impl])) {\n\t\t\tverbose(env, \"acquire kernel function does not return PTR_TO_BTF_ID\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (btf_type_is_scalar(t)) {\n\t\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t\tmark_btf_func_reg_size(env, BPF_REG_0, t->size);\n\t} else if (btf_type_is_ptr(t)) {\n\t\tptr_type = btf_type_skip_modifiers(desc_btf, t->type, &ptr_type_id);\n\n\t\tif (meta.btf == btf_vmlinux && btf_id_set_contains(&special_kfunc_set, meta.func_id)) {\n\t\t\tif (meta.func_id == special_kfunc_list[KF_bpf_obj_new_impl]) {\n\t\t\t\tstruct btf *ret_btf;\n\t\t\t\tu32 ret_btf_id;\n\n\t\t\t\tif (unlikely(!bpf_global_ma_set))\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tif (((u64)(u32)meta.arg_constant.value) != meta.arg_constant.value) {\n\t\t\t\t\tverbose(env, \"local type ID argument must be in range [0, U32_MAX]\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tret_btf = env->prog->aux->btf;\n\t\t\t\tret_btf_id = meta.arg_constant.value;\n\n\t\t\t\t \n\t\t\t\tif (!ret_btf) {\n\t\t\t\t\tverbose(env, \"bpf_obj_new requires prog BTF\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tret_t = btf_type_by_id(ret_btf, ret_btf_id);\n\t\t\t\tif (!ret_t || !__btf_type_is_struct(ret_t)) {\n\t\t\t\t\tverbose(env, \"bpf_obj_new type ID argument must be of a struct\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | MEM_ALLOC;\n\t\t\t\tregs[BPF_REG_0].btf = ret_btf;\n\t\t\t\tregs[BPF_REG_0].btf_id = ret_btf_id;\n\n\t\t\t\tinsn_aux->obj_new_size = ret_t->size;\n\t\t\t\tinsn_aux->kptr_struct_meta =\n\t\t\t\t\tbtf_find_struct_meta(ret_btf, ret_btf_id);\n\t\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_refcount_acquire_impl]) {\n\t\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | MEM_ALLOC;\n\t\t\t\tregs[BPF_REG_0].btf = meta.arg_btf;\n\t\t\t\tregs[BPF_REG_0].btf_id = meta.arg_btf_id;\n\n\t\t\t\tinsn_aux->kptr_struct_meta =\n\t\t\t\t\tbtf_find_struct_meta(meta.arg_btf,\n\t\t\t\t\t\t\t     meta.arg_btf_id);\n\t\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_list_pop_front] ||\n\t\t\t\t   meta.func_id == special_kfunc_list[KF_bpf_list_pop_back]) {\n\t\t\t\tstruct btf_field *field = meta.arg_list_head.field;\n\n\t\t\t\tmark_reg_graph_node(regs, BPF_REG_0, &field->graph_root);\n\t\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_rbtree_remove] ||\n\t\t\t\t   meta.func_id == special_kfunc_list[KF_bpf_rbtree_first]) {\n\t\t\t\tstruct btf_field *field = meta.arg_rbtree_root.field;\n\n\t\t\t\tmark_reg_graph_node(regs, BPF_REG_0, &field->graph_root);\n\t\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_cast_to_kern_ctx]) {\n\t\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | PTR_TRUSTED;\n\t\t\t\tregs[BPF_REG_0].btf = desc_btf;\n\t\t\t\tregs[BPF_REG_0].btf_id = meta.ret_btf_id;\n\t\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_rdonly_cast]) {\n\t\t\t\tret_t = btf_type_by_id(desc_btf, meta.arg_constant.value);\n\t\t\t\tif (!ret_t || !btf_type_is_struct(ret_t)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"kfunc bpf_rdonly_cast type ID argument must be of a struct\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID | PTR_UNTRUSTED;\n\t\t\t\tregs[BPF_REG_0].btf = desc_btf;\n\t\t\t\tregs[BPF_REG_0].btf_id = meta.arg_constant.value;\n\t\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_dynptr_slice] ||\n\t\t\t\t   meta.func_id == special_kfunc_list[KF_bpf_dynptr_slice_rdwr]) {\n\t\t\t\tenum bpf_type_flag type_flag = get_dynptr_type_flag(meta.initialized_dynptr.type);\n\n\t\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\n\t\t\t\tif (!meta.arg_constant.found) {\n\t\t\t\t\tverbose(env, \"verifier internal error: bpf_dynptr_slice(_rdwr) no constant size\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\n\t\t\t\tregs[BPF_REG_0].mem_size = meta.arg_constant.value;\n\n\t\t\t\t \n\t\t\t\tregs[BPF_REG_0].type = PTR_TO_MEM | type_flag;\n\n\t\t\t\tif (meta.func_id == special_kfunc_list[KF_bpf_dynptr_slice]) {\n\t\t\t\t\tregs[BPF_REG_0].type |= MEM_RDONLY;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tif (!may_access_direct_pkt_data(env, NULL, BPF_WRITE)) {\n\t\t\t\t\t\tverbose(env, \"the prog does not allow writes to packet data\\n\");\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (!meta.initialized_dynptr.id) {\n\t\t\t\t\tverbose(env, \"verifier internal error: no dynptr id\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tregs[BPF_REG_0].dynptr_id = meta.initialized_dynptr.id;\n\n\t\t\t\t \n\t\t\t} else {\n\t\t\t\tverbose(env, \"kernel function %s unhandled dynamic return type\\n\",\n\t\t\t\t\tmeta.func_name);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t} else if (!__btf_type_is_struct(ptr_type)) {\n\t\t\tif (!meta.r0_size) {\n\t\t\t\t__u32 sz;\n\n\t\t\t\tif (!IS_ERR(btf_resolve_size(desc_btf, ptr_type, &sz))) {\n\t\t\t\t\tmeta.r0_size = sz;\n\t\t\t\t\tmeta.r0_rdonly = true;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!meta.r0_size) {\n\t\t\t\tptr_type_name = btf_name_by_offset(desc_btf,\n\t\t\t\t\t\t\t\t   ptr_type->name_off);\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"kernel function %s returns pointer type %s %s is not supported\\n\",\n\t\t\t\t\tfunc_name,\n\t\t\t\t\tbtf_type_str(ptr_type),\n\t\t\t\t\tptr_type_name);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t\tregs[BPF_REG_0].type = PTR_TO_MEM;\n\t\t\tregs[BPF_REG_0].mem_size = meta.r0_size;\n\n\t\t\tif (meta.r0_rdonly)\n\t\t\t\tregs[BPF_REG_0].type |= MEM_RDONLY;\n\n\t\t\t \n\t\t\tif (meta.ref_obj_id)\n\t\t\t\tregs[BPF_REG_0].ref_obj_id = meta.ref_obj_id;\n\t\t} else {\n\t\t\tmark_reg_known_zero(env, regs, BPF_REG_0);\n\t\t\tregs[BPF_REG_0].btf = desc_btf;\n\t\t\tregs[BPF_REG_0].type = PTR_TO_BTF_ID;\n\t\t\tregs[BPF_REG_0].btf_id = ptr_type_id;\n\t\t}\n\n\t\tif (is_kfunc_ret_null(&meta)) {\n\t\t\tregs[BPF_REG_0].type |= PTR_MAYBE_NULL;\n\t\t\t \n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t\t}\n\t\tmark_btf_func_reg_size(env, BPF_REG_0, sizeof(void *));\n\t\tif (is_kfunc_acquire(&meta)) {\n\t\t\tint id = acquire_reference_state(env, insn_idx);\n\n\t\t\tif (id < 0)\n\t\t\t\treturn id;\n\t\t\tif (is_kfunc_ret_null(&meta))\n\t\t\t\tregs[BPF_REG_0].id = id;\n\t\t\tregs[BPF_REG_0].ref_obj_id = id;\n\t\t} else if (meta.func_id == special_kfunc_list[KF_bpf_rbtree_first]) {\n\t\t\tref_set_non_owning(env, &regs[BPF_REG_0]);\n\t\t}\n\n\t\tif (reg_may_point_to_spin_lock(&regs[BPF_REG_0]) && !regs[BPF_REG_0].id)\n\t\t\tregs[BPF_REG_0].id = ++env->id_gen;\n\t} else if (btf_type_is_void(t)) {\n\t\tif (meta.btf == btf_vmlinux && btf_id_set_contains(&special_kfunc_set, meta.func_id)) {\n\t\t\tif (meta.func_id == special_kfunc_list[KF_bpf_obj_drop_impl]) {\n\t\t\t\tinsn_aux->kptr_struct_meta =\n\t\t\t\t\tbtf_find_struct_meta(meta.arg_btf,\n\t\t\t\t\t\t\t     meta.arg_btf_id);\n\t\t\t}\n\t\t}\n\t}\n\n\tnargs = btf_type_vlen(meta.func_proto);\n\targs = (const struct btf_param *)(meta.func_proto + 1);\n\tfor (i = 0; i < nargs; i++) {\n\t\tu32 regno = i + 1;\n\n\t\tt = btf_type_skip_modifiers(desc_btf, args[i].type, NULL);\n\t\tif (btf_type_is_ptr(t))\n\t\t\tmark_btf_func_reg_size(env, regno, sizeof(void *));\n\t\telse\n\t\t\t \n\t\t\tmark_btf_func_reg_size(env, regno, t->size);\n\t}\n\n\tif (is_iter_next_kfunc(&meta)) {\n\t\terr = process_iter_next_call(env, insn_idx, &meta);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic bool signed_add_overflows(s64 a, s64 b)\n{\n\t \n\ts64 res = (s64)((u64)a + (u64)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_add32_overflows(s32 a, s32 b)\n{\n\t \n\ts32 res = (s32)((u32)a + (u32)b);\n\n\tif (b < 0)\n\t\treturn res > a;\n\treturn res < a;\n}\n\nstatic bool signed_sub_overflows(s64 a, s64 b)\n{\n\t \n\ts64 res = (s64)((u64)a - (u64)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool signed_sub32_overflows(s32 a, s32 b)\n{\n\t \n\ts32 res = (s32)((u32)a - (u32)b);\n\n\tif (b < 0)\n\t\treturn res < a;\n\treturn res > a;\n}\n\nstatic bool check_reg_sane_offset(struct bpf_verifier_env *env,\n\t\t\t\t  const struct bpf_reg_state *reg,\n\t\t\t\t  enum bpf_reg_type type)\n{\n\tbool known = tnum_is_const(reg->var_off);\n\ts64 val = reg->var_off.value;\n\ts64 smin = reg->smin_value;\n\n\tif (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {\n\t\tverbose(env, \"math between %s pointer and %lld is not allowed\\n\",\n\t\t\treg_type_str(env, type), val);\n\t\treturn false;\n\t}\n\n\tif (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"%s pointer offset %d is not allowed\\n\",\n\t\t\treg_type_str(env, type), reg->off);\n\t\treturn false;\n\t}\n\n\tif (smin == S64_MIN) {\n\t\tverbose(env, \"math between %s pointer and register with unbounded min value is not allowed\\n\",\n\t\t\treg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\tif (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {\n\t\tverbose(env, \"value %lld makes %s pointer be out of bounds\\n\",\n\t\t\tsmin, reg_type_str(env, type));\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nenum {\n\tREASON_BOUNDS\t= -1,\n\tREASON_TYPE\t= -2,\n\tREASON_PATHS\t= -3,\n\tREASON_LIMIT\t= -4,\n\tREASON_STACK\t= -5,\n};\n\nstatic int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,\n\t\t\t      u32 *alu_limit, bool mask_to_left)\n{\n\tu32 max = 0, ptr_limit = 0;\n\n\tswitch (ptr_reg->type) {\n\tcase PTR_TO_STACK:\n\t\t \n\t\tmax = MAX_BPF_STACK + mask_to_left;\n\t\tptr_limit = -(ptr_reg->var_off.value + ptr_reg->off);\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tmax = ptr_reg->map_ptr->value_size;\n\t\tptr_limit = (mask_to_left ?\n\t\t\t     ptr_reg->smin_value :\n\t\t\t     ptr_reg->umax_value) + ptr_reg->off;\n\t\tbreak;\n\tdefault:\n\t\treturn REASON_TYPE;\n\t}\n\n\tif (ptr_limit >= max)\n\t\treturn REASON_LIMIT;\n\t*alu_limit = ptr_limit;\n\treturn 0;\n}\n\nstatic bool can_skip_alu_sanitation(const struct bpf_verifier_env *env,\n\t\t\t\t    const struct bpf_insn *insn)\n{\n\treturn env->bypass_spec_v1 || BPF_SRC(insn->code) == BPF_K;\n}\n\nstatic int update_alu_sanitation_state(struct bpf_insn_aux_data *aux,\n\t\t\t\t       u32 alu_state, u32 alu_limit)\n{\n\t \n\tif (aux->alu_state &&\n\t    (aux->alu_state != alu_state ||\n\t     aux->alu_limit != alu_limit))\n\t\treturn REASON_PATHS;\n\n\t \n\taux->alu_state = alu_state;\n\taux->alu_limit = alu_limit;\n\treturn 0;\n}\n\nstatic int sanitize_val_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn)\n{\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\treturn update_alu_sanitation_state(aux, BPF_ALU_NON_POINTER, 0);\n}\n\nstatic bool sanitize_needed(u8 opcode)\n{\n\treturn opcode == BPF_ADD || opcode == BPF_SUB;\n}\n\nstruct bpf_sanitize_info {\n\tstruct bpf_insn_aux_data aux;\n\tbool mask_to_left;\n};\n\nstatic struct bpf_verifier_state *\nsanitize_speculative_path(struct bpf_verifier_env *env,\n\t\t\t  const struct bpf_insn *insn,\n\t\t\t  u32 next_idx, u32 curr_idx)\n{\n\tstruct bpf_verifier_state *branch;\n\tstruct bpf_reg_state *regs;\n\n\tbranch = push_stack(env, next_idx, curr_idx, true);\n\tif (branch && insn) {\n\t\tregs = branch->frame[branch->curframe]->regs;\n\t\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t} else if (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tmark_reg_unknown(env, regs, insn->src_reg);\n\t\t}\n\t}\n\treturn branch;\n}\n\nstatic int sanitize_ptr_alu(struct bpf_verifier_env *env,\n\t\t\t    struct bpf_insn *insn,\n\t\t\t    const struct bpf_reg_state *ptr_reg,\n\t\t\t    const struct bpf_reg_state *off_reg,\n\t\t\t    struct bpf_reg_state *dst_reg,\n\t\t\t    struct bpf_sanitize_info *info,\n\t\t\t    const bool commit_window)\n{\n\tstruct bpf_insn_aux_data *aux = commit_window ? cur_aux(env) : &info->aux;\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tbool off_is_imm = tnum_is_const(off_reg->var_off);\n\tbool off_is_neg = off_reg->smin_value < 0;\n\tbool ptr_is_dst_reg = ptr_reg == dst_reg;\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 alu_state, alu_limit;\n\tstruct bpf_reg_state tmp;\n\tbool ret;\n\tint err;\n\n\tif (can_skip_alu_sanitation(env, insn))\n\t\treturn 0;\n\n\t \n\tif (vstate->speculative)\n\t\tgoto do_sim;\n\n\tif (!commit_window) {\n\t\tif (!tnum_is_const(off_reg->var_off) &&\n\t\t    (off_reg->smin_value < 0) != (off_reg->smax_value < 0))\n\t\t\treturn REASON_BOUNDS;\n\n\t\tinfo->mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||\n\t\t\t\t     (opcode == BPF_SUB && !off_is_neg);\n\t}\n\n\terr = retrieve_ptr_limit(ptr_reg, &alu_limit, info->mask_to_left);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (commit_window) {\n\t\t \n\t\talu_state = info->aux.alu_state;\n\t\talu_limit = abs(info->aux.alu_limit - alu_limit);\n\t} else {\n\t\talu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;\n\t\talu_state |= off_is_imm ? BPF_ALU_IMMEDIATE : 0;\n\t\talu_state |= ptr_is_dst_reg ?\n\t\t\t     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;\n\n\t\t \n\t\tif (!off_is_imm)\n\t\t\tenv->explore_alu_limits = true;\n\t}\n\n\terr = update_alu_sanitation_state(aux, alu_state, alu_limit);\n\tif (err < 0)\n\t\treturn err;\ndo_sim:\n\t \n\tif (commit_window || off_is_imm)\n\t\treturn 0;\n\n\t \n\tif (!ptr_is_dst_reg) {\n\t\ttmp = *dst_reg;\n\t\tcopy_register_state(dst_reg, ptr_reg);\n\t}\n\tret = sanitize_speculative_path(env, NULL, env->insn_idx + 1,\n\t\t\t\t\tenv->insn_idx);\n\tif (!ptr_is_dst_reg && ret)\n\t\t*dst_reg = tmp;\n\treturn !ret ? REASON_STACK : 0;\n}\n\nstatic void sanitize_mark_insn_seen(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\n\t \n\tif (!vstate->speculative)\n\t\tenv->insn_aux_data[env->insn_idx].seen = env->pass_cnt;\n}\n\nstatic int sanitize_err(struct bpf_verifier_env *env,\n\t\t\tconst struct bpf_insn *insn, int reason,\n\t\t\tconst struct bpf_reg_state *off_reg,\n\t\t\tconst struct bpf_reg_state *dst_reg)\n{\n\tstatic const char *err = \"pointer arithmetic with it prohibited for !root\";\n\tconst char *op = BPF_OP(insn->code) == BPF_ADD ? \"add\" : \"sub\";\n\tu32 dst = insn->dst_reg, src = insn->src_reg;\n\n\tswitch (reason) {\n\tcase REASON_BOUNDS:\n\t\tverbose(env, \"R%d has unknown scalar with mixed signed bounds, %s\\n\",\n\t\t\toff_reg == dst_reg ? dst : src, err);\n\t\tbreak;\n\tcase REASON_TYPE:\n\t\tverbose(env, \"R%d has pointer with unsupported alu operation, %s\\n\",\n\t\t\toff_reg == dst_reg ? src : dst, err);\n\t\tbreak;\n\tcase REASON_PATHS:\n\t\tverbose(env, \"R%d tried to %s from different maps, paths or scalars, %s\\n\",\n\t\t\tdst, op, err);\n\t\tbreak;\n\tcase REASON_LIMIT:\n\t\tverbose(env, \"R%d tried to %s beyond pointer bounds, %s\\n\",\n\t\t\tdst, op, err);\n\t\tbreak;\n\tcase REASON_STACK:\n\t\tverbose(env, \"R%d could not be pushed for speculative verification, %s\\n\",\n\t\t\tdst, err);\n\t\tbreak;\n\tdefault:\n\t\tverbose(env, \"verifier internal error: unknown reason (%d)\\n\",\n\t\t\treason);\n\t\tbreak;\n\t}\n\n\treturn -EACCES;\n}\n\n \nstatic int check_stack_access_for_ptr_arithmetic(\n\t\t\t\tstruct bpf_verifier_env *env,\n\t\t\t\tint regno,\n\t\t\t\tconst struct bpf_reg_state *reg,\n\t\t\t\tint off)\n{\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg->var_off);\n\t\tverbose(env, \"R%d variable stack access prohibited for !root, var_off=%s off=%d\\n\",\n\t\t\tregno, tn_buf, off);\n\t\treturn -EACCES;\n\t}\n\n\tif (off >= 0 || off < -MAX_BPF_STACK) {\n\t\tverbose(env, \"R%d stack pointer arithmetic goes out of range, \"\n\t\t\t\"prohibited for !root; off=%d\\n\", regno, off);\n\t\treturn -EACCES;\n\t}\n\n\treturn 0;\n}\n\nstatic int sanitize_check_bounds(struct bpf_verifier_env *env,\n\t\t\t\t const struct bpf_insn *insn,\n\t\t\t\t const struct bpf_reg_state *dst_reg)\n{\n\tu32 dst = insn->dst_reg;\n\n\t \n\tif (env->bypass_spec_v1)\n\t\treturn 0;\n\n\tswitch (dst_reg->type) {\n\tcase PTR_TO_STACK:\n\t\tif (check_stack_access_for_ptr_arithmetic(env, dst, dst_reg,\n\t\t\t\t\tdst_reg->off + dst_reg->var_off.value))\n\t\t\treturn -EACCES;\n\t\tbreak;\n\tcase PTR_TO_MAP_VALUE:\n\t\tif (check_map_access(env, dst, dst_reg->off, 1, false, ACCESS_HELPER)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic of map value goes out of range, \"\n\t\t\t\t\"prohibited for !root\\n\", dst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int adjust_ptr_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn,\n\t\t\t\t   const struct bpf_reg_state *ptr_reg,\n\t\t\t\t   const struct bpf_reg_state *off_reg)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg;\n\tbool known = tnum_is_const(off_reg->var_off);\n\ts64 smin_val = off_reg->smin_value, smax_val = off_reg->smax_value,\n\t    smin_ptr = ptr_reg->smin_value, smax_ptr = ptr_reg->smax_value;\n\tu64 umin_val = off_reg->umin_value, umax_val = off_reg->umax_value,\n\t    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;\n\tstruct bpf_sanitize_info info = {};\n\tu8 opcode = BPF_OP(insn->code);\n\tu32 dst = insn->dst_reg;\n\tint ret;\n\n\tdst_reg = &regs[dst];\n\n\tif ((known && (smin_val != smax_val || umin_val != umax_val)) ||\n\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t \n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t \n\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\n\t\tverbose(env,\n\t\t\t\"R%d 32-bit pointer arithmetic prohibited\\n\",\n\t\t\tdst);\n\t\treturn -EACCES;\n\t}\n\n\tif (ptr_reg->type & PTR_MAYBE_NULL) {\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited, null-check it first\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\t}\n\n\tswitch (base_type(ptr_reg->type)) {\n\tcase PTR_TO_FLOW_KEYS:\n\t\tif (known)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase CONST_PTR_TO_MAP:\n\t\t \n\t\tif (known && smin_val == 0 && opcode == BPF_ADD)\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase PTR_TO_PACKET_END:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\t\tverbose(env, \"R%d pointer arithmetic on %s prohibited\\n\",\n\t\t\tdst, reg_type_str(env, ptr_reg->type));\n\t\treturn -EACCES;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tdst_reg->type = ptr_reg->type;\n\tdst_reg->id = ptr_reg->id;\n\n\tif (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||\n\t    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\n\t \n\t__mark_reg32_unbounded(dst_reg);\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, ptr_reg, off_reg, dst_reg,\n\t\t\t\t       &info, false);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\t \n\t\tif (known && (ptr_reg->off + smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off + smin_val))) {\n\t\t\t \n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->off = ptr_reg->off + smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (signed_add_overflows(smin_ptr, smin_val) ||\n\t\t    signed_add_overflows(smax_ptr, smax_val)) {\n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr + smin_val;\n\t\t\tdst_reg->smax_value = smax_ptr + smax_val;\n\t\t}\n\t\tif (umin_ptr + umin_val < umin_ptr ||\n\t\t    umax_ptr + umax_val < umax_ptr) {\n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\tdst_reg->umin_value = umin_ptr + umin_val;\n\t\t\tdst_reg->umax_value = umax_ptr + umax_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_add(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t \n\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tif (dst_reg == off_reg) {\n\t\t\t \n\t\t\tverbose(env, \"R%d tried to subtract pointer from scalar\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\t \n\t\tif (ptr_reg->type == PTR_TO_STACK) {\n\t\t\tverbose(env, \"R%d subtraction from stack pointer prohibited\\n\",\n\t\t\t\tdst);\n\t\t\treturn -EACCES;\n\t\t}\n\t\tif (known && (ptr_reg->off - smin_val ==\n\t\t\t      (s64)(s32)(ptr_reg->off - smin_val))) {\n\t\t\t \n\t\t\tdst_reg->smin_value = smin_ptr;\n\t\t\tdst_reg->smax_value = smax_ptr;\n\t\t\tdst_reg->umin_value = umin_ptr;\n\t\t\tdst_reg->umax_value = umax_ptr;\n\t\t\tdst_reg->var_off = ptr_reg->var_off;\n\t\t\tdst_reg->id = ptr_reg->id;\n\t\t\tdst_reg->off = ptr_reg->off - smin_val;\n\t\t\tdst_reg->raw = ptr_reg->raw;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (signed_sub_overflows(smin_ptr, smax_val) ||\n\t\t    signed_sub_overflows(smax_ptr, smin_val)) {\n\t\t\t \n\t\t\tdst_reg->smin_value = S64_MIN;\n\t\t\tdst_reg->smax_value = S64_MAX;\n\t\t} else {\n\t\t\tdst_reg->smin_value = smin_ptr - smax_val;\n\t\t\tdst_reg->smax_value = smax_ptr - smin_val;\n\t\t}\n\t\tif (umin_ptr < umax_val) {\n\t\t\t \n\t\t\tdst_reg->umin_value = 0;\n\t\t\tdst_reg->umax_value = U64_MAX;\n\t\t} else {\n\t\t\t \n\t\t\tdst_reg->umin_value = umin_ptr - umax_val;\n\t\t\tdst_reg->umax_value = umax_ptr - umin_val;\n\t\t}\n\t\tdst_reg->var_off = tnum_sub(ptr_reg->var_off, off_reg->var_off);\n\t\tdst_reg->off = ptr_reg->off;\n\t\tdst_reg->raw = ptr_reg->raw;\n\t\tif (reg_is_pkt_pointer(ptr_reg)) {\n\t\t\tdst_reg->id = ++env->id_gen;\n\t\t\t \n\t\t\tif (smin_val < 0)\n\t\t\t\tmemset(&dst_reg->raw, 0, sizeof(dst_reg->raw));\n\t\t}\n\t\tbreak;\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t \n\t\tverbose(env, \"R%d bitwise operator %s on pointer prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\tdefault:\n\t\t \n\t\tverbose(env, \"R%d pointer arithmetic with %s operator prohibited\\n\",\n\t\t\tdst, bpf_alu_string[opcode >> 4]);\n\t\treturn -EACCES;\n\t}\n\n\tif (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))\n\t\treturn -EINVAL;\n\treg_bounds_sync(dst_reg);\n\tif (sanitize_check_bounds(env, insn, dst_reg) < 0)\n\t\treturn -EACCES;\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_ptr_alu(env, insn, dst_reg, off_reg, dst_reg,\n\t\t\t\t       &info, true);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, off_reg, dst_reg);\n\t}\n\n\treturn 0;\n}\n\nstatic void scalar32_min_max_add(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\ts32 smax_val = src_reg->s32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (signed_add32_overflows(dst_reg->s32_min_value, smin_val) ||\n\t    signed_add32_overflows(dst_reg->s32_max_value, smax_val)) {\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value += smin_val;\n\t\tdst_reg->s32_max_value += smax_val;\n\t}\n\tif (dst_reg->u32_min_value + umin_val < umin_val ||\n\t    dst_reg->u32_max_value + umax_val < umax_val) {\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\tdst_reg->u32_min_value += umin_val;\n\t\tdst_reg->u32_max_value += umax_val;\n\t}\n}\n\nstatic void scalar_min_max_add(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (signed_add_overflows(dst_reg->smin_value, smin_val) ||\n\t    signed_add_overflows(dst_reg->smax_value, smax_val)) {\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value += smin_val;\n\t\tdst_reg->smax_value += smax_val;\n\t}\n\tif (dst_reg->umin_value + umin_val < umin_val ||\n\t    dst_reg->umax_value + umax_val < umax_val) {\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\tdst_reg->umin_value += umin_val;\n\t\tdst_reg->umax_value += umax_val;\n\t}\n}\n\nstatic void scalar32_min_max_sub(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\ts32 smax_val = src_reg->s32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (signed_sub32_overflows(dst_reg->s32_min_value, smax_val) ||\n\t    signed_sub32_overflows(dst_reg->s32_max_value, smin_val)) {\n\t\t \n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value -= smax_val;\n\t\tdst_reg->s32_max_value -= smin_val;\n\t}\n\tif (dst_reg->u32_min_value < umax_val) {\n\t\t \n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\t \n\t\tdst_reg->u32_min_value -= umax_val;\n\t\tdst_reg->u32_max_value -= umin_val;\n\t}\n}\n\nstatic void scalar_min_max_sub(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\ts64 smax_val = src_reg->smax_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (signed_sub_overflows(dst_reg->smin_value, smax_val) ||\n\t    signed_sub_overflows(dst_reg->smax_value, smin_val)) {\n\t\t \n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value -= smax_val;\n\t\tdst_reg->smax_value -= smin_val;\n\t}\n\tif (dst_reg->umin_value < umax_val) {\n\t\t \n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\t \n\t\tdst_reg->umin_value -= umax_val;\n\t\tdst_reg->umax_value -= umin_val;\n\t}\n}\n\nstatic void scalar32_min_max_mul(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (smin_val < 0 || dst_reg->s32_min_value < 0) {\n\t\t \n\t\t__mark_reg32_unbounded(dst_reg);\n\t\treturn;\n\t}\n\t \n\tif (umax_val > U16_MAX || dst_reg->u32_max_value > U16_MAX) {\n\t\t \n\t\t__mark_reg32_unbounded(dst_reg);\n\t\treturn;\n\t}\n\tdst_reg->u32_min_value *= umin_val;\n\tdst_reg->u32_max_value *= umax_val;\n\tif (dst_reg->u32_max_value > S32_MAX) {\n\t\t \n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}\n\nstatic void scalar_min_max_mul(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (smin_val < 0 || dst_reg->smin_value < 0) {\n\t\t \n\t\t__mark_reg64_unbounded(dst_reg);\n\t\treturn;\n\t}\n\t \n\tif (umax_val > U32_MAX || dst_reg->umax_value > U32_MAX) {\n\t\t \n\t\t__mark_reg64_unbounded(dst_reg);\n\t\treturn;\n\t}\n\tdst_reg->umin_value *= umin_val;\n\tdst_reg->umax_value *= umax_val;\n\tif (dst_reg->umax_value > S64_MAX) {\n\t\t \n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n}\n\nstatic void scalar32_min_max_and(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umax_val = src_reg->u32_max_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg32_known(dst_reg, var32_off.value);\n\t\treturn;\n\t}\n\n\t \n\tdst_reg->u32_min_value = var32_off.value;\n\tdst_reg->u32_max_value = min(dst_reg->u32_max_value, umax_val);\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t \n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t \n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}\n\nstatic void scalar_min_max_and(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umax_val = src_reg->umax_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t \n\tdst_reg->umin_value = dst_reg->var_off.value;\n\tdst_reg->umax_value = min(dst_reg->umax_value, umax_val);\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t \n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t \n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t \n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg32_known(dst_reg, var32_off.value);\n\t\treturn;\n\t}\n\n\t \n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t \n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t \n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}\n\nstatic void scalar_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t      struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t \n\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\tdst_reg->umax_value = dst_reg->var_off.value | dst_reg->var_off.mask;\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t \n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t \n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t \n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_xor(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg32_known(dst_reg, var32_off.value);\n\t\treturn;\n\t}\n\n\t \n\tdst_reg->u32_min_value = var32_off.value;\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\n\tif (dst_reg->s32_min_value >= 0 && smin_val >= 0) {\n\t\t \n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t} else {\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t}\n}\n\nstatic void scalar_min_max_xor(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\n\tif (src_known && dst_known) {\n\t\t \n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t \n\tdst_reg->umin_value = dst_reg->var_off.value;\n\tdst_reg->umax_value = dst_reg->var_off.value | dst_reg->var_off.mask;\n\n\tif (dst_reg->smin_value >= 0 && smin_val >= 0) {\n\t\t \n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t} else {\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t}\n\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void __scalar32_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t   u64 umin_val, u64 umax_val)\n{\n\t \n\tdst_reg->s32_min_value = S32_MIN;\n\tdst_reg->s32_max_value = S32_MAX;\n\t \n\tif (umax_val > 31 || dst_reg->u32_max_value > 1ULL << (31 - umax_val)) {\n\t\tdst_reg->u32_min_value = 0;\n\t\tdst_reg->u32_max_value = U32_MAX;\n\t} else {\n\t\tdst_reg->u32_min_value <<= umin_val;\n\t\tdst_reg->u32_max_value <<= umax_val;\n\t}\n}\n\nstatic void scalar32_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tu32 umax_val = src_reg->u32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\t \n\tstruct tnum subreg = tnum_subreg(dst_reg->var_off);\n\n\t__scalar32_min_max_lsh(dst_reg, umin_val, umax_val);\n\tdst_reg->var_off = tnum_subreg(tnum_lshift(subreg, umin_val));\n\t \n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void __scalar64_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t   u64 umin_val, u64 umax_val)\n{\n\t \n\tif (umin_val == 32 && umax_val == 32 && dst_reg->s32_max_value >= 0)\n\t\tdst_reg->smax_value = (s64)dst_reg->s32_max_value << 32;\n\telse\n\t\tdst_reg->smax_value = S64_MAX;\n\n\tif (umin_val == 32 && umax_val == 32 && dst_reg->s32_min_value >= 0)\n\t\tdst_reg->smin_value = (s64)dst_reg->s32_min_value << 32;\n\telse\n\t\tdst_reg->smin_value = S64_MIN;\n\n\t \n\tif (dst_reg->umax_value > 1ULL << (63 - umax_val)) {\n\t\tdst_reg->umin_value = 0;\n\t\tdst_reg->umax_value = U64_MAX;\n\t} else {\n\t\tdst_reg->umin_value <<= umin_val;\n\t\tdst_reg->umax_value <<= umax_val;\n\t}\n}\n\nstatic void scalar_min_max_lsh(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tu64 umax_val = src_reg->umax_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\t \n\t__scalar64_min_max_lsh(dst_reg, umin_val, umax_val);\n\t__scalar32_min_max_lsh(dst_reg, umin_val, umax_val);\n\n\tdst_reg->var_off = tnum_lshift(dst_reg->var_off, umin_val);\n\t \n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_rsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tstruct tnum subreg = tnum_subreg(dst_reg->var_off);\n\tu32 umax_val = src_reg->u32_max_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\t \n\tdst_reg->s32_min_value = S32_MIN;\n\tdst_reg->s32_max_value = S32_MAX;\n\n\tdst_reg->var_off = tnum_rshift(subreg, umin_val);\n\tdst_reg->u32_min_value >>= umax_val;\n\tdst_reg->u32_max_value >>= umin_val;\n\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void scalar_min_max_rsh(struct bpf_reg_state *dst_reg,\n\t\t\t       struct bpf_reg_state *src_reg)\n{\n\tu64 umax_val = src_reg->umax_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\t \n\tdst_reg->smin_value = S64_MIN;\n\tdst_reg->smax_value = S64_MAX;\n\tdst_reg->var_off = tnum_rshift(dst_reg->var_off, umin_val);\n\tdst_reg->umin_value >>= umax_val;\n\tdst_reg->umax_value >>= umin_val;\n\n\t \n\t__mark_reg32_unbounded(dst_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\nstatic void scalar32_min_max_arsh(struct bpf_reg_state *dst_reg,\n\t\t\t\t  struct bpf_reg_state *src_reg)\n{\n\tu64 umin_val = src_reg->u32_min_value;\n\n\t \n\tdst_reg->s32_min_value = (u32)(((s32)dst_reg->s32_min_value) >> umin_val);\n\tdst_reg->s32_max_value = (u32)(((s32)dst_reg->s32_max_value) >> umin_val);\n\n\tdst_reg->var_off = tnum_arshift(tnum_subreg(dst_reg->var_off), umin_val, 32);\n\n\t \n\tdst_reg->u32_min_value = 0;\n\tdst_reg->u32_max_value = U32_MAX;\n\n\t__mark_reg64_unbounded(dst_reg);\n\t__update_reg32_bounds(dst_reg);\n}\n\nstatic void scalar_min_max_arsh(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tu64 umin_val = src_reg->umin_value;\n\n\t \n\tdst_reg->smin_value >>= umin_val;\n\tdst_reg->smax_value >>= umin_val;\n\n\tdst_reg->var_off = tnum_arshift(dst_reg->var_off, umin_val, 64);\n\n\t \n\tdst_reg->umin_value = 0;\n\tdst_reg->umax_value = U64_MAX;\n\n\t \n\t__mark_reg32_unbounded(dst_reg);\n\t__update_reg_bounds(dst_reg);\n}\n\n \nstatic int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t      struct bpf_insn *insn,\n\t\t\t\t      struct bpf_reg_state *dst_reg,\n\t\t\t\t      struct bpf_reg_state src_reg)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tbool src_known;\n\ts64 smin_val, smax_val;\n\tu64 umin_val, umax_val;\n\ts32 s32_min_val, s32_max_val;\n\tu32 u32_min_val, u32_max_val;\n\tu64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;\n\tbool alu32 = (BPF_CLASS(insn->code) != BPF_ALU64);\n\tint ret;\n\n\tsmin_val = src_reg.smin_value;\n\tsmax_val = src_reg.smax_value;\n\tumin_val = src_reg.umin_value;\n\tumax_val = src_reg.umax_value;\n\n\ts32_min_val = src_reg.s32_min_value;\n\ts32_max_val = src_reg.s32_max_value;\n\tu32_min_val = src_reg.u32_min_value;\n\tu32_max_val = src_reg.u32_max_value;\n\n\tif (alu32) {\n\t\tsrc_known = tnum_subreg_is_const(src_reg.var_off);\n\t\tif ((src_known &&\n\t\t     (s32_min_val != s32_max_val || u32_min_val != u32_max_val)) ||\n\t\t    s32_min_val > s32_max_val || u32_min_val > u32_max_val) {\n\t\t\t \n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tsrc_known = tnum_is_const(src_reg.var_off);\n\t\tif ((src_known &&\n\t\t     (smin_val != smax_val || umin_val != umax_val)) ||\n\t\t    smin_val > smax_val || umin_val > umax_val) {\n\t\t\t \n\t\t\t__mark_reg_unknown(env, dst_reg);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (!src_known &&\n\t    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {\n\t\t__mark_reg_unknown(env, dst_reg);\n\t\treturn 0;\n\t}\n\n\tif (sanitize_needed(opcode)) {\n\t\tret = sanitize_val_alu(env, insn);\n\t\tif (ret < 0)\n\t\t\treturn sanitize_err(env, insn, ret, NULL, NULL);\n\t}\n\n\t \n\tswitch (opcode) {\n\tcase BPF_ADD:\n\t\tscalar32_min_max_add(dst_reg, &src_reg);\n\t\tscalar_min_max_add(dst_reg, &src_reg);\n\t\tdst_reg->var_off = tnum_add(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_SUB:\n\t\tscalar32_min_max_sub(dst_reg, &src_reg);\n\t\tscalar_min_max_sub(dst_reg, &src_reg);\n\t\tdst_reg->var_off = tnum_sub(dst_reg->var_off, src_reg.var_off);\n\t\tbreak;\n\tcase BPF_MUL:\n\t\tdst_reg->var_off = tnum_mul(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_mul(dst_reg, &src_reg);\n\t\tscalar_min_max_mul(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_AND:\n\t\tdst_reg->var_off = tnum_and(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_and(dst_reg, &src_reg);\n\t\tscalar_min_max_and(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_OR:\n\t\tdst_reg->var_off = tnum_or(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_or(dst_reg, &src_reg);\n\t\tscalar_min_max_or(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_XOR:\n\t\tdst_reg->var_off = tnum_xor(dst_reg->var_off, src_reg.var_off);\n\t\tscalar32_min_max_xor(dst_reg, &src_reg);\n\t\tscalar_min_max_xor(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_LSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t \n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_lsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_lsh(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_RSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t \n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_rsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_rsh(dst_reg, &src_reg);\n\t\tbreak;\n\tcase BPF_ARSH:\n\t\tif (umax_val >= insn_bitness) {\n\t\t\t \n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tbreak;\n\t\t}\n\t\tif (alu32)\n\t\t\tscalar32_min_max_arsh(dst_reg, &src_reg);\n\t\telse\n\t\t\tscalar_min_max_arsh(dst_reg, &src_reg);\n\t\tbreak;\n\tdefault:\n\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\tbreak;\n\t}\n\n\t \n\tif (alu32)\n\t\tzext_32_to_64(dst_reg);\n\treg_bounds_sync(dst_reg);\n\treturn 0;\n}\n\n \nstatic int adjust_reg_min_max_vals(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_insn *insn)\n{\n\tstruct bpf_verifier_state *vstate = env->cur_state;\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *dst_reg, *src_reg;\n\tstruct bpf_reg_state *ptr_reg = NULL, off_reg = {0};\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tsrc_reg = NULL;\n\tif (dst_reg->type != SCALAR_VALUE)\n\t\tptr_reg = dst_reg;\n\telse\n\t\t \n\t\tdst_reg->id = 0;\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (src_reg->type != SCALAR_VALUE) {\n\t\t\tif (dst_reg->type != SCALAR_VALUE) {\n\t\t\t\t \n\t\t\t\tif (opcode == BPF_SUB && env->allow_ptr_leaks) {\n\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\t\tverbose(env, \"R%d pointer %s pointer prohibited\\n\",\n\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\tbpf_alu_string[opcode >> 4]);\n\t\t\t\treturn -EACCES;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t\t       src_reg, dst_reg);\n\t\t\t}\n\t\t} else if (ptr_reg) {\n\t\t\t \n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       dst_reg, src_reg);\n\t\t} else if (dst_reg->precise) {\n\t\t\t \n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t} else {\n\t\t \n\t\toff_reg.type = SCALAR_VALUE;\n\t\t__mark_reg_known(&off_reg, insn->imm);\n\t\tsrc_reg = &off_reg;\n\t\tif (ptr_reg)  \n\t\t\treturn adjust_ptr_min_max_vals(env, insn,\n\t\t\t\t\t\t       ptr_reg, src_reg);\n\t}\n\n\t \n\tif (WARN_ON_ONCE(ptr_reg)) {\n\t\tprint_verifier_state(env, state, true);\n\t\tverbose(env, \"verifier internal error: unexpected ptr_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (WARN_ON(!src_reg)) {\n\t\tprint_verifier_state(env, state, true);\n\t\tverbose(env, \"verifier internal error: no src_reg\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn adjust_scalar_min_max_vals(env, insn, dst_reg, *src_reg);\n}\n\n \nstatic int check_alu_op(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tu8 opcode = BPF_OP(insn->code);\n\tint err;\n\n\tif (opcode == BPF_END || opcode == BPF_NEG) {\n\t\tif (opcode == BPF_NEG) {\n\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t    insn->off != 0 || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_NEG uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\n\t\t\t    (insn->imm != 16 && insn->imm != 32 && insn->imm != 64) ||\n\t\t\t    (BPF_CLASS(insn->code) == BPF_ALU64 &&\n\t\t\t     BPF_SRC(insn->code) != BPF_TO_LE)) {\n\t\t\t\tverbose(env, \"BPF_END uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (is_pointer_value(env, insn->dst_reg)) {\n\t\t\tverbose(env, \"R%d pointer arithmetic prohibited\\n\",\n\t\t\t\tinsn->dst_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\t \n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t} else if (opcode == BPF_MOV) {\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU) {\n\t\t\t\tif (insn->off != 0 && insn->off != 8 && insn->off != 16) {\n\t\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (insn->off != 0 && insn->off != 8 && insn->off != 16 &&\n\t\t\t\t    insn->off != 32) {\n\t\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\n\t\t\t\tverbose(env, \"BPF_MOV uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tstruct bpf_reg_state *src_reg = regs + insn->src_reg;\n\t\t\tstruct bpf_reg_state *dst_reg = regs + insn->dst_reg;\n\t\t\tbool need_id = src_reg->type == SCALAR_VALUE && !src_reg->id &&\n\t\t\t\t       !tnum_is_const(src_reg->var_off);\n\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\tif (insn->off == 0) {\n\t\t\t\t\t \n\t\t\t\t\tif (need_id)\n\t\t\t\t\t\t \n\t\t\t\t\t\tsrc_reg->id = ++env->id_gen;\n\t\t\t\t\tcopy_register_state(dst_reg, src_reg);\n\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t\tdst_reg->subreg_def = DEF_NOT_SUBREG;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\t\"R%d sign-extension part of pointer\\n\",\n\t\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\t\treturn -EACCES;\n\t\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\t\tbool no_sext;\n\n\t\t\t\t\t\tno_sext = src_reg->umax_value < (1ULL << (insn->off - 1));\n\t\t\t\t\t\tif (no_sext && need_id)\n\t\t\t\t\t\t\tsrc_reg->id = ++env->id_gen;\n\t\t\t\t\t\tcopy_register_state(dst_reg, src_reg);\n\t\t\t\t\t\tif (!no_sext)\n\t\t\t\t\t\t\tdst_reg->id = 0;\n\t\t\t\t\t\tcoerce_reg_to_size_sx(dst_reg, insn->off >> 3);\n\t\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t\t\tdst_reg->subreg_def = DEF_NOT_SUBREG;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (is_pointer_value(env, insn->src_reg)) {\n\t\t\t\t\tverbose(env,\n\t\t\t\t\t\t\"R%d partial copy of pointer\\n\",\n\t\t\t\t\t\tinsn->src_reg);\n\t\t\t\t\treturn -EACCES;\n\t\t\t\t} else if (src_reg->type == SCALAR_VALUE) {\n\t\t\t\t\tif (insn->off == 0) {\n\t\t\t\t\t\tbool is_src_reg_u32 = src_reg->umax_value <= U32_MAX;\n\n\t\t\t\t\t\tif (is_src_reg_u32 && need_id)\n\t\t\t\t\t\t\tsrc_reg->id = ++env->id_gen;\n\t\t\t\t\t\tcopy_register_state(dst_reg, src_reg);\n\t\t\t\t\t\t \n\t\t\t\t\t\tif (!is_src_reg_u32)\n\t\t\t\t\t\t\tdst_reg->id = 0;\n\t\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t\t\tdst_reg->subreg_def = env->insn_idx + 1;\n\t\t\t\t\t} else {\n\t\t\t\t\t\t \n\t\t\t\t\t\tbool no_sext = src_reg->umax_value < (1ULL << (insn->off - 1));\n\n\t\t\t\t\t\tif (no_sext && need_id)\n\t\t\t\t\t\t\tsrc_reg->id = ++env->id_gen;\n\t\t\t\t\t\tcopy_register_state(dst_reg, src_reg);\n\t\t\t\t\t\tif (!no_sext)\n\t\t\t\t\t\t\tdst_reg->id = 0;\n\t\t\t\t\t\tdst_reg->live |= REG_LIVE_WRITTEN;\n\t\t\t\t\t\tdst_reg->subreg_def = env->insn_idx + 1;\n\t\t\t\t\t\tcoerce_subreg_to_size_sx(dst_reg, insn->off >> 3);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tmark_reg_unknown(env, regs,\n\t\t\t\t\t\t\t insn->dst_reg);\n\t\t\t\t}\n\t\t\t\tzext_32_to_64(dst_reg);\n\t\t\t\treg_bounds_sync(dst_reg);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\t \n\t\t\tmark_reg_unknown(env, regs, insn->dst_reg);\n\t\t\tregs[insn->dst_reg].type = SCALAR_VALUE;\n\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU64) {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t insn->imm);\n\t\t\t} else {\n\t\t\t\t__mark_reg_known(regs + insn->dst_reg,\n\t\t\t\t\t\t (u32)insn->imm);\n\t\t\t}\n\t\t}\n\n\t} else if (opcode > BPF_END) {\n\t\tverbose(env, \"invalid BPF_ALU opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\n\t} else {\t \n\n\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\tif (insn->imm != 0 || insn->off > 1 ||\n\t\t\t    (insn->off == 1 && opcode != BPF_MOD && opcode != BPF_DIV)) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t \n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else {\n\t\t\tif (insn->src_reg != BPF_REG_0 || insn->off > 1 ||\n\t\t\t    (insn->off == 1 && opcode != BPF_MOD && opcode != BPF_DIV)) {\n\t\t\t\tverbose(env, \"BPF_ALU uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\n\t\t    BPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\n\t\t\tverbose(env, \"div by zero\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((opcode == BPF_LSH || opcode == BPF_RSH ||\n\t\t     opcode == BPF_ARSH) && BPF_SRC(insn->code) == BPF_K) {\n\t\t\tint size = BPF_CLASS(insn->code) == BPF_ALU64 ? 64 : 32;\n\n\t\t\tif (insn->imm < 0 || insn->imm >= size) {\n\t\t\t\tverbose(env, \"invalid shift %d\\n\", insn->imm);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\treturn adjust_reg_min_max_vals(env, insn);\n\t}\n\n\treturn 0;\n}\n\nstatic void find_good_pkt_pointers(struct bpf_verifier_state *vstate,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   enum bpf_reg_type type,\n\t\t\t\t   bool range_right_open)\n{\n\tstruct bpf_func_state *state;\n\tstruct bpf_reg_state *reg;\n\tint new_range;\n\n\tif (dst_reg->off < 0 ||\n\t    (dst_reg->off == 0 && range_right_open))\n\t\t \n\t\treturn;\n\n\tif (dst_reg->umax_value > MAX_PACKET_OFF ||\n\t    dst_reg->umax_value + dst_reg->off > MAX_PACKET_OFF)\n\t\t \n\t\treturn;\n\n\tnew_range = dst_reg->off;\n\tif (range_right_open)\n\t\tnew_range++;\n\n\t \n\n\t \n\tbpf_for_each_reg_in_vstate(vstate, state, reg, ({\n\t\tif (reg->type == type && reg->id == dst_reg->id)\n\t\t\t \n\t\t\treg->range = max(reg->range, new_range);\n\t}));\n}\n\nstatic int is_branch32_taken(struct bpf_reg_state *reg, u32 val, u8 opcode)\n{\n\tstruct tnum subreg = tnum_subreg(reg->var_off);\n\ts32 sval = (s32)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(subreg))\n\t\t\treturn !!tnum_equals_const(subreg, val);\n\t\telse if (val < reg->u32_min_value || val > reg->u32_max_value)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(subreg))\n\t\t\treturn !tnum_equals_const(subreg, val);\n\t\telse if (val < reg->u32_min_value || val > reg->u32_max_value)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~subreg.mask & subreg.value) & val)\n\t\t\treturn 1;\n\t\tif (!((subreg.mask | subreg.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->u32_min_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_max_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->s32_min_value > sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_max_value <= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->u32_max_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_min_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->s32_max_value < sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_min_value >= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->u32_min_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_max_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->s32_min_value >= sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_max_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->u32_max_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->u32_min_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->s32_max_value <= sval)\n\t\t\treturn 1;\n\t\telse if (reg->s32_min_value > sval)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n\nstatic int is_branch64_taken(struct bpf_reg_state *reg, u64 val, u8 opcode)\n{\n\ts64 sval = (s64)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !!tnum_equals_const(reg->var_off, val);\n\t\telse if (val < reg->umin_value || val > reg->umax_value)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (tnum_is_const(reg->var_off))\n\t\t\treturn !tnum_equals_const(reg->var_off, val);\n\t\telse if (val < reg->umin_value || val > reg->umax_value)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif ((~reg->var_off.mask & reg->var_off.value) & val)\n\t\t\treturn 1;\n\t\tif (!((reg->var_off.mask | reg->var_off.value) & val))\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGT:\n\t\tif (reg->umin_value > val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value <= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGT:\n\t\tif (reg->smin_value > sval)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value <= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif (reg->umax_value < val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value >= val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLT:\n\t\tif (reg->smax_value < sval)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value >= sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif (reg->umin_value >= val)\n\t\t\treturn 1;\n\t\telse if (reg->umax_value < val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSGE:\n\t\tif (reg->smin_value >= sval)\n\t\t\treturn 1;\n\t\telse if (reg->smax_value < sval)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif (reg->umax_value <= val)\n\t\t\treturn 1;\n\t\telse if (reg->umin_value > val)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase BPF_JSLE:\n\t\tif (reg->smax_value <= sval)\n\t\t\treturn 1;\n\t\telse if (reg->smin_value > sval)\n\t\t\treturn 0;\n\t\tbreak;\n\t}\n\n\treturn -1;\n}\n\n \nstatic int is_branch_taken(struct bpf_reg_state *reg, u64 val, u8 opcode,\n\t\t\t   bool is_jmp32)\n{\n\tif (__is_pointer_value(false, reg)) {\n\t\tif (!reg_not_null(reg))\n\t\t\treturn -1;\n\n\t\t \n\t\tif (val != 0)\n\t\t\treturn -1;\n\n\t\tswitch (opcode) {\n\t\tcase BPF_JEQ:\n\t\t\treturn 0;\n\t\tcase BPF_JNE:\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (is_jmp32)\n\t\treturn is_branch32_taken(reg, val, opcode);\n\treturn is_branch64_taken(reg, val, opcode);\n}\n\nstatic int flip_opcode(u32 opcode)\n{\n\t \n\tstatic const u8 opcode_flip[16] = {\n\t\t \n\t\t[BPF_JEQ  >> 4] = BPF_JEQ,\n\t\t[BPF_JNE  >> 4] = BPF_JNE,\n\t\t[BPF_JSET >> 4] = BPF_JSET,\n\t\t \n\t\t[BPF_JGE  >> 4] = BPF_JLE,\n\t\t[BPF_JGT  >> 4] = BPF_JLT,\n\t\t[BPF_JLE  >> 4] = BPF_JGE,\n\t\t[BPF_JLT  >> 4] = BPF_JGT,\n\t\t[BPF_JSGE >> 4] = BPF_JSLE,\n\t\t[BPF_JSGT >> 4] = BPF_JSLT,\n\t\t[BPF_JSLE >> 4] = BPF_JSGE,\n\t\t[BPF_JSLT >> 4] = BPF_JSGT\n\t};\n\treturn opcode_flip[opcode >> 4];\n}\n\nstatic int is_pkt_ptr_branch_taken(struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   u8 opcode)\n{\n\tstruct bpf_reg_state *pkt;\n\n\tif (src_reg->type == PTR_TO_PACKET_END) {\n\t\tpkt = dst_reg;\n\t} else if (dst_reg->type == PTR_TO_PACKET_END) {\n\t\tpkt = src_reg;\n\t\topcode = flip_opcode(opcode);\n\t} else {\n\t\treturn -1;\n\t}\n\n\tif (pkt->range >= 0)\n\t\treturn -1;\n\n\tswitch (opcode) {\n\tcase BPF_JLE:\n\t\t \n\t\tfallthrough;\n\tcase BPF_JGT:\n\t\t \n\t\tif (pkt->range == BEYOND_PKT_END)\n\t\t\t \n\t\t\treturn opcode == BPF_JGT;\n\t\tbreak;\n\tcase BPF_JLT:\n\t\t \n\t\tfallthrough;\n\tcase BPF_JGE:\n\t\t \n\t\tif (pkt->range == BEYOND_PKT_END || pkt->range == AT_PKT_END)\n\t\t\treturn opcode == BPF_JGE;\n\t\tbreak;\n\t}\n\treturn -1;\n}\n\n \nstatic void reg_set_min_max(struct bpf_reg_state *true_reg,\n\t\t\t    struct bpf_reg_state *false_reg,\n\t\t\t    u64 val, u32 val32,\n\t\t\t    u8 opcode, bool is_jmp32)\n{\n\tstruct tnum false_32off = tnum_subreg(false_reg->var_off);\n\tstruct tnum false_64off = false_reg->var_off;\n\tstruct tnum true_32off = tnum_subreg(true_reg->var_off);\n\tstruct tnum true_64off = true_reg->var_off;\n\ts64 sval = (s64)val;\n\ts32 sval32 = (s32)val32;\n\n\t \n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tswitch (opcode) {\n\t \n\tcase BPF_JEQ:\n\t\tif (is_jmp32) {\n\t\t\t__mark_reg32_known(true_reg, val32);\n\t\t\ttrue_32off = tnum_subreg(true_reg->var_off);\n\t\t} else {\n\t\t\t___mark_reg_known(true_reg, val);\n\t\t\ttrue_64off = true_reg->var_off;\n\t\t}\n\t\tbreak;\n\tcase BPF_JNE:\n\t\tif (is_jmp32) {\n\t\t\t__mark_reg32_known(false_reg, val32);\n\t\t\tfalse_32off = tnum_subreg(false_reg->var_off);\n\t\t} else {\n\t\t\t___mark_reg_known(false_reg, val);\n\t\t\tfalse_64off = false_reg->var_off;\n\t\t}\n\t\tbreak;\n\tcase BPF_JSET:\n\t\tif (is_jmp32) {\n\t\t\tfalse_32off = tnum_and(false_32off, tnum_const(~val32));\n\t\t\tif (is_power_of_2(val32))\n\t\t\t\ttrue_32off = tnum_or(true_32off,\n\t\t\t\t\t\t     tnum_const(val32));\n\t\t} else {\n\t\t\tfalse_64off = tnum_and(false_64off, tnum_const(~val));\n\t\t\tif (is_power_of_2(val))\n\t\t\t\ttrue_64off = tnum_or(true_64off,\n\t\t\t\t\t\t     tnum_const(val));\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\tu32 false_umax = opcode == BPF_JGT ? val32  : val32 - 1;\n\t\t\tu32 true_umin = opcode == BPF_JGT ? val32 + 1 : val32;\n\n\t\t\tfalse_reg->u32_max_value = min(false_reg->u32_max_value,\n\t\t\t\t\t\t       false_umax);\n\t\t\ttrue_reg->u32_min_value = max(true_reg->u32_min_value,\n\t\t\t\t\t\t      true_umin);\n\t\t} else {\n\t\t\tu64 false_umax = opcode == BPF_JGT ? val    : val - 1;\n\t\t\tu64 true_umin = opcode == BPF_JGT ? val + 1 : val;\n\n\t\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\ts32 false_smax = opcode == BPF_JSGT ? sval32    : sval32 - 1;\n\t\t\ts32 true_smin = opcode == BPF_JSGT ? sval32 + 1 : sval32;\n\n\t\t\tfalse_reg->s32_max_value = min(false_reg->s32_max_value, false_smax);\n\t\t\ttrue_reg->s32_min_value = max(true_reg->s32_min_value, true_smin);\n\t\t} else {\n\t\t\ts64 false_smax = opcode == BPF_JSGT ? sval    : sval - 1;\n\t\t\ts64 true_smin = opcode == BPF_JSGT ? sval + 1 : sval;\n\n\t\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\tu32 false_umin = opcode == BPF_JLT ? val32  : val32 + 1;\n\t\t\tu32 true_umax = opcode == BPF_JLT ? val32 - 1 : val32;\n\n\t\t\tfalse_reg->u32_min_value = max(false_reg->u32_min_value,\n\t\t\t\t\t\t       false_umin);\n\t\t\ttrue_reg->u32_max_value = min(true_reg->u32_max_value,\n\t\t\t\t\t\t      true_umax);\n\t\t} else {\n\t\t\tu64 false_umin = opcode == BPF_JLT ? val    : val + 1;\n\t\t\tu64 true_umax = opcode == BPF_JLT ? val - 1 : val;\n\n\t\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\tif (is_jmp32) {\n\t\t\ts32 false_smin = opcode == BPF_JSLT ? sval32    : sval32 + 1;\n\t\t\ts32 true_smax = opcode == BPF_JSLT ? sval32 - 1 : sval32;\n\n\t\t\tfalse_reg->s32_min_value = max(false_reg->s32_min_value, false_smin);\n\t\t\ttrue_reg->s32_max_value = min(true_reg->s32_max_value, true_smax);\n\t\t} else {\n\t\t\ts64 false_smin = opcode == BPF_JSLT ? sval    : sval + 1;\n\t\t\ts64 true_smax = opcode == BPF_JSLT ? sval - 1 : sval;\n\n\t\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn;\n\t}\n\n\tif (is_jmp32) {\n\t\tfalse_reg->var_off = tnum_or(tnum_clear_subreg(false_64off),\n\t\t\t\t\t     tnum_subreg(false_32off));\n\t\ttrue_reg->var_off = tnum_or(tnum_clear_subreg(true_64off),\n\t\t\t\t\t    tnum_subreg(true_32off));\n\t\t__reg_combine_32_into_64(false_reg);\n\t\t__reg_combine_32_into_64(true_reg);\n\t} else {\n\t\tfalse_reg->var_off = false_64off;\n\t\ttrue_reg->var_off = true_64off;\n\t\t__reg_combine_64_into_32(false_reg);\n\t\t__reg_combine_64_into_32(true_reg);\n\t}\n}\n\n \nstatic void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg,\n\t\t\t\tu64 val, u32 val32,\n\t\t\t\tu8 opcode, bool is_jmp32)\n{\n\topcode = flip_opcode(opcode);\n\t \n\tif (opcode)\n\t\treg_set_min_max(true_reg, false_reg, val, val32, opcode, is_jmp32);\n}\n\n \nstatic void __reg_combine_min_max(struct bpf_reg_state *src_reg,\n\t\t\t\t  struct bpf_reg_state *dst_reg)\n{\n\tsrc_reg->umin_value = dst_reg->umin_value = max(src_reg->umin_value,\n\t\t\t\t\t\t\tdst_reg->umin_value);\n\tsrc_reg->umax_value = dst_reg->umax_value = min(src_reg->umax_value,\n\t\t\t\t\t\t\tdst_reg->umax_value);\n\tsrc_reg->smin_value = dst_reg->smin_value = max(src_reg->smin_value,\n\t\t\t\t\t\t\tdst_reg->smin_value);\n\tsrc_reg->smax_value = dst_reg->smax_value = min(src_reg->smax_value,\n\t\t\t\t\t\t\tdst_reg->smax_value);\n\tsrc_reg->var_off = dst_reg->var_off = tnum_intersect(src_reg->var_off,\n\t\t\t\t\t\t\t     dst_reg->var_off);\n\treg_bounds_sync(src_reg);\n\treg_bounds_sync(dst_reg);\n}\n\nstatic void reg_combine_min_max(struct bpf_reg_state *true_src,\n\t\t\t\tstruct bpf_reg_state *true_dst,\n\t\t\t\tstruct bpf_reg_state *false_src,\n\t\t\t\tstruct bpf_reg_state *false_dst,\n\t\t\t\tu8 opcode)\n{\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\t\t__reg_combine_min_max(true_src, true_dst);\n\t\tbreak;\n\tcase BPF_JNE:\n\t\t__reg_combine_min_max(false_src, false_dst);\n\t\tbreak;\n\t}\n}\n\nstatic void mark_ptr_or_null_reg(struct bpf_func_state *state,\n\t\t\t\t struct bpf_reg_state *reg, u32 id,\n\t\t\t\t bool is_null)\n{\n\tif (type_may_be_null(reg->type) && reg->id == id &&\n\t    (is_rcu_reg(reg) || !WARN_ON_ONCE(!reg->id))) {\n\t\t \n\t\tif (WARN_ON_ONCE(reg->smin_value || reg->smax_value || !tnum_equals_const(reg->var_off, 0)))\n\t\t\treturn;\n\t\tif (!(type_is_ptr_alloc_obj(reg->type) || type_is_non_owning_ref(reg->type)) &&\n\t\t    WARN_ON_ONCE(reg->off))\n\t\t\treturn;\n\n\t\tif (is_null) {\n\t\t\treg->type = SCALAR_VALUE;\n\t\t\t \n\t\t\treg->id = 0;\n\t\t\treg->ref_obj_id = 0;\n\n\t\t\treturn;\n\t\t}\n\n\t\tmark_ptr_not_null_reg(reg);\n\n\t\tif (!reg_may_point_to_spin_lock(reg)) {\n\t\t\t \n\t\t\treg->id = 0;\n\t\t}\n\t}\n}\n\n \nstatic void mark_ptr_or_null_regs(struct bpf_verifier_state *vstate, u32 regno,\n\t\t\t\t  bool is_null)\n{\n\tstruct bpf_func_state *state = vstate->frame[vstate->curframe];\n\tstruct bpf_reg_state *regs = state->regs, *reg;\n\tu32 ref_obj_id = regs[regno].ref_obj_id;\n\tu32 id = regs[regno].id;\n\n\tif (ref_obj_id && ref_obj_id == id && is_null)\n\t\t \n\t\tWARN_ON_ONCE(release_reference_state(state, id));\n\n\tbpf_for_each_reg_in_vstate(vstate, state, reg, ({\n\t\tmark_ptr_or_null_reg(state, reg, id, is_null);\n\t}));\n}\n\nstatic bool try_match_pkt_pointers(const struct bpf_insn *insn,\n\t\t\t\t   struct bpf_reg_state *dst_reg,\n\t\t\t\t   struct bpf_reg_state *src_reg,\n\t\t\t\t   struct bpf_verifier_state *this_branch,\n\t\t\t\t   struct bpf_verifier_state *other_branch)\n{\n\tif (BPF_SRC(insn->code) != BPF_X)\n\t\treturn false;\n\n\t \n\tif (BPF_CLASS(insn->code) == BPF_JMP32)\n\t\treturn false;\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_JGT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t\tmark_pkt_end(other_branch, insn->dst_reg, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t\tmark_pkt_end(this_branch, insn->src_reg, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLT:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t\tmark_pkt_end(this_branch, insn->dst_reg, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t\tmark_pkt_end(other_branch, insn->src_reg, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JGE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(this_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, true);\n\t\t\tmark_pkt_end(other_branch, insn->dst_reg, false);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(other_branch, src_reg,\n\t\t\t\t\t       src_reg->type, false);\n\t\t\tmark_pkt_end(this_branch, insn->src_reg, true);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tcase BPF_JLE:\n\t\tif ((dst_reg->type == PTR_TO_PACKET &&\n\t\t     src_reg->type == PTR_TO_PACKET_END) ||\n\t\t    (dst_reg->type == PTR_TO_PACKET_META &&\n\t\t     reg_is_init_pkt_pointer(src_reg, PTR_TO_PACKET))) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(other_branch, dst_reg,\n\t\t\t\t\t       dst_reg->type, false);\n\t\t\tmark_pkt_end(this_branch, insn->dst_reg, true);\n\t\t} else if ((dst_reg->type == PTR_TO_PACKET_END &&\n\t\t\t    src_reg->type == PTR_TO_PACKET) ||\n\t\t\t   (reg_is_init_pkt_pointer(dst_reg, PTR_TO_PACKET) &&\n\t\t\t    src_reg->type == PTR_TO_PACKET_META)) {\n\t\t\t \n\t\t\tfind_good_pkt_pointers(this_branch, src_reg,\n\t\t\t\t\t       src_reg->type, true);\n\t\t\tmark_pkt_end(other_branch, insn->src_reg, false);\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void find_equal_scalars(struct bpf_verifier_state *vstate,\n\t\t\t       struct bpf_reg_state *known_reg)\n{\n\tstruct bpf_func_state *state;\n\tstruct bpf_reg_state *reg;\n\n\tbpf_for_each_reg_in_vstate(vstate, state, reg, ({\n\t\tif (reg->type == SCALAR_VALUE && reg->id == known_reg->id)\n\t\t\tcopy_register_state(reg, known_reg);\n\t}));\n}\n\nstatic int check_cond_jmp_op(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_insn *insn, int *insn_idx)\n{\n\tstruct bpf_verifier_state *this_branch = env->cur_state;\n\tstruct bpf_verifier_state *other_branch;\n\tstruct bpf_reg_state *regs = this_branch->frame[this_branch->curframe]->regs;\n\tstruct bpf_reg_state *dst_reg, *other_branch_regs, *src_reg = NULL;\n\tstruct bpf_reg_state *eq_branch_regs;\n\tu8 opcode = BPF_OP(insn->code);\n\tbool is_jmp32;\n\tint pred = -1;\n\tint err;\n\n\t \n\tif (opcode == BPF_JA || opcode > BPF_JSLE) {\n\t\tverbose(env, \"invalid BPF_JMP/JMP32 opcode %x\\n\", opcode);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tif (insn->imm != 0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tsrc_reg = &regs[insn->src_reg];\n\t\tif (!(reg_is_pkt_pointer_any(dst_reg) && reg_is_pkt_pointer_any(src_reg)) &&\n\t\t    is_pointer_value(env, insn->src_reg)) {\n\t\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\t\tinsn->src_reg);\n\t\t\treturn -EACCES;\n\t\t}\n\t} else {\n\t\tif (insn->src_reg != BPF_REG_0) {\n\t\t\tverbose(env, \"BPF_JMP/JMP32 uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tis_jmp32 = BPF_CLASS(insn->code) == BPF_JMP32;\n\n\tif (BPF_SRC(insn->code) == BPF_K) {\n\t\tpred = is_branch_taken(dst_reg, insn->imm, opcode, is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   is_jmp32 && tnum_is_const(tnum_subreg(src_reg->var_off))) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       tnum_subreg(src_reg->var_off).value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (src_reg->type == SCALAR_VALUE &&\n\t\t   !is_jmp32 && tnum_is_const(src_reg->var_off)) {\n\t\tpred = is_branch_taken(dst_reg,\n\t\t\t\t       src_reg->var_off.value,\n\t\t\t\t       opcode,\n\t\t\t\t       is_jmp32);\n\t} else if (dst_reg->type == SCALAR_VALUE &&\n\t\t   is_jmp32 && tnum_is_const(tnum_subreg(dst_reg->var_off))) {\n\t\tpred = is_branch_taken(src_reg,\n\t\t\t\t       tnum_subreg(dst_reg->var_off).value,\n\t\t\t\t       flip_opcode(opcode),\n\t\t\t\t       is_jmp32);\n\t} else if (dst_reg->type == SCALAR_VALUE &&\n\t\t   !is_jmp32 && tnum_is_const(dst_reg->var_off)) {\n\t\tpred = is_branch_taken(src_reg,\n\t\t\t\t       dst_reg->var_off.value,\n\t\t\t\t       flip_opcode(opcode),\n\t\t\t\t       is_jmp32);\n\t} else if (reg_is_pkt_pointer_any(dst_reg) &&\n\t\t   reg_is_pkt_pointer_any(src_reg) &&\n\t\t   !is_jmp32) {\n\t\tpred = is_pkt_ptr_branch_taken(dst_reg, src_reg, opcode);\n\t}\n\n\tif (pred >= 0) {\n\t\t \n\t\tif (!__is_pointer_value(false, dst_reg))\n\t\t\terr = mark_chain_precision(env, insn->dst_reg);\n\t\tif (BPF_SRC(insn->code) == BPF_X && !err &&\n\t\t    !__is_pointer_value(false, src_reg))\n\t\t\terr = mark_chain_precision(env, insn->src_reg);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (pred == 1) {\n\t\t \n\t\tif (!env->bypass_spec_v1 &&\n\t\t    !sanitize_speculative_path(env, insn, *insn_idx + 1,\n\t\t\t\t\t       *insn_idx))\n\t\t\treturn -EFAULT;\n\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\tprint_insn_state(env, this_branch->frame[this_branch->curframe]);\n\t\t*insn_idx += insn->off;\n\t\treturn 0;\n\t} else if (pred == 0) {\n\t\t \n\t\tif (!env->bypass_spec_v1 &&\n\t\t    !sanitize_speculative_path(env, insn,\n\t\t\t\t\t       *insn_idx + insn->off + 1,\n\t\t\t\t\t       *insn_idx))\n\t\t\treturn -EFAULT;\n\t\tif (env->log.level & BPF_LOG_LEVEL)\n\t\t\tprint_insn_state(env, this_branch->frame[this_branch->curframe]);\n\t\treturn 0;\n\t}\n\n\tother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,\n\t\t\t\t  false);\n\tif (!other_branch)\n\t\treturn -EFAULT;\n\tother_branch_regs = other_branch->frame[other_branch->curframe]->regs;\n\n\t \n\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\tstruct bpf_reg_state *src_reg = &regs[insn->src_reg];\n\n\t\tif (dst_reg->type == SCALAR_VALUE &&\n\t\t    src_reg->type == SCALAR_VALUE) {\n\t\t\tif (tnum_is_const(src_reg->var_off) ||\n\t\t\t    (is_jmp32 &&\n\t\t\t     tnum_is_const(tnum_subreg(src_reg->var_off))))\n\t\t\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\tdst_reg,\n\t\t\t\t\t\tsrc_reg->var_off.value,\n\t\t\t\t\t\ttnum_subreg(src_reg->var_off).value,\n\t\t\t\t\t\topcode, is_jmp32);\n\t\t\telse if (tnum_is_const(dst_reg->var_off) ||\n\t\t\t\t (is_jmp32 &&\n\t\t\t\t  tnum_is_const(tnum_subreg(dst_reg->var_off))))\n\t\t\t\treg_set_min_max_inv(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    src_reg,\n\t\t\t\t\t\t    dst_reg->var_off.value,\n\t\t\t\t\t\t    tnum_subreg(dst_reg->var_off).value,\n\t\t\t\t\t\t    opcode, is_jmp32);\n\t\t\telse if (!is_jmp32 &&\n\t\t\t\t (opcode == BPF_JEQ || opcode == BPF_JNE))\n\t\t\t\t \n\t\t\t\treg_combine_min_max(&other_branch_regs[insn->src_reg],\n\t\t\t\t\t\t    &other_branch_regs[insn->dst_reg],\n\t\t\t\t\t\t    src_reg, dst_reg, opcode);\n\t\t\tif (src_reg->id &&\n\t\t\t    !WARN_ON_ONCE(src_reg->id != other_branch_regs[insn->src_reg].id)) {\n\t\t\t\tfind_equal_scalars(this_branch, src_reg);\n\t\t\t\tfind_equal_scalars(other_branch, &other_branch_regs[insn->src_reg]);\n\t\t\t}\n\n\t\t}\n\t} else if (dst_reg->type == SCALAR_VALUE) {\n\t\treg_set_min_max(&other_branch_regs[insn->dst_reg],\n\t\t\t\t\tdst_reg, insn->imm, (u32)insn->imm,\n\t\t\t\t\topcode, is_jmp32);\n\t}\n\n\tif (dst_reg->type == SCALAR_VALUE && dst_reg->id &&\n\t    !WARN_ON_ONCE(dst_reg->id != other_branch_regs[insn->dst_reg].id)) {\n\t\tfind_equal_scalars(this_branch, dst_reg);\n\t\tfind_equal_scalars(other_branch, &other_branch_regs[insn->dst_reg]);\n\t}\n\n\t \n\tif (!is_jmp32 && BPF_SRC(insn->code) == BPF_X &&\n\t    __is_pointer_value(false, src_reg) && __is_pointer_value(false, dst_reg) &&\n\t    type_may_be_null(src_reg->type) != type_may_be_null(dst_reg->type) &&\n\t    base_type(src_reg->type) != PTR_TO_BTF_ID &&\n\t    base_type(dst_reg->type) != PTR_TO_BTF_ID) {\n\t\teq_branch_regs = NULL;\n\t\tswitch (opcode) {\n\t\tcase BPF_JEQ:\n\t\t\teq_branch_regs = other_branch_regs;\n\t\t\tbreak;\n\t\tcase BPF_JNE:\n\t\t\teq_branch_regs = regs;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tif (eq_branch_regs) {\n\t\t\tif (type_may_be_null(src_reg->type))\n\t\t\t\tmark_ptr_not_null_reg(&eq_branch_regs[insn->src_reg]);\n\t\t\telse\n\t\t\t\tmark_ptr_not_null_reg(&eq_branch_regs[insn->dst_reg]);\n\t\t}\n\t}\n\n\t \n\tif (!is_jmp32 && BPF_SRC(insn->code) == BPF_K &&\n\t    insn->imm == 0 && (opcode == BPF_JEQ || opcode == BPF_JNE) &&\n\t    type_may_be_null(dst_reg->type)) {\n\t\t \n\t\tmark_ptr_or_null_regs(this_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JNE);\n\t\tmark_ptr_or_null_regs(other_branch, insn->dst_reg,\n\t\t\t\t      opcode == BPF_JEQ);\n\t} else if (!try_match_pkt_pointers(insn, dst_reg, &regs[insn->src_reg],\n\t\t\t\t\t   this_branch, other_branch) &&\n\t\t   is_pointer_value(env, insn->dst_reg)) {\n\t\tverbose(env, \"R%d pointer comparison prohibited\\n\",\n\t\t\tinsn->dst_reg);\n\t\treturn -EACCES;\n\t}\n\tif (env->log.level & BPF_LOG_LEVEL)\n\t\tprint_insn_state(env, this_branch->frame[this_branch->curframe]);\n\treturn 0;\n}\n\n \nstatic int check_ld_imm(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_insn_aux_data *aux = cur_aux(env);\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstruct bpf_reg_state *dst_reg;\n\tstruct bpf_map *map;\n\tint err;\n\n\tif (BPF_SIZE(insn->code) != BPF_DW) {\n\t\tverbose(env, \"invalid BPF_LD_IMM insn\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (insn->off != 0) {\n\t\tverbose(env, \"BPF_LD_IMM64 uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = check_reg_arg(env, insn->dst_reg, DST_OP);\n\tif (err)\n\t\treturn err;\n\n\tdst_reg = &regs[insn->dst_reg];\n\tif (insn->src_reg == 0) {\n\t\tu64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;\n\n\t\tdst_reg->type = SCALAR_VALUE;\n\t\t__mark_reg_known(&regs[insn->dst_reg], imm);\n\t\treturn 0;\n\t}\n\n\t \n\tmark_reg_known_zero(env, regs, insn->dst_reg);\n\n\tif (insn->src_reg == BPF_PSEUDO_BTF_ID) {\n\t\tdst_reg->type = aux->btf_var.reg_type;\n\t\tswitch (base_type(dst_reg->type)) {\n\t\tcase PTR_TO_MEM:\n\t\t\tdst_reg->mem_size = aux->btf_var.mem_size;\n\t\t\tbreak;\n\t\tcase PTR_TO_BTF_ID:\n\t\t\tdst_reg->btf = aux->btf_var.btf;\n\t\t\tdst_reg->btf_id = aux->btf_var.btf_id;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (insn->src_reg == BPF_PSEUDO_FUNC) {\n\t\tstruct bpf_prog_aux *aux = env->prog->aux;\n\t\tu32 subprogno = find_subprog(env,\n\t\t\t\t\t     env->insn_idx + insn->imm + 1);\n\n\t\tif (!aux->func_info) {\n\t\t\tverbose(env, \"missing btf func_info\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (aux->func_info_aux[subprogno].linkage != BTF_FUNC_STATIC) {\n\t\t\tverbose(env, \"callback function not static\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tdst_reg->type = PTR_TO_FUNC;\n\t\tdst_reg->subprogno = subprogno;\n\t\treturn 0;\n\t}\n\n\tmap = env->used_maps[aux->map_index];\n\tdst_reg->map_ptr = map;\n\n\tif (insn->src_reg == BPF_PSEUDO_MAP_VALUE ||\n\t    insn->src_reg == BPF_PSEUDO_MAP_IDX_VALUE) {\n\t\tdst_reg->type = PTR_TO_MAP_VALUE;\n\t\tdst_reg->off = aux->map_off;\n\t\tWARN_ON_ONCE(map->max_entries != 1);\n\t\t \n\t} else if (insn->src_reg == BPF_PSEUDO_MAP_FD ||\n\t\t   insn->src_reg == BPF_PSEUDO_MAP_IDX) {\n\t\tdst_reg->type = CONST_PTR_TO_MAP;\n\t} else {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic bool may_access_skb(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_SOCKET_FILTER:\n\tcase BPF_PROG_TYPE_SCHED_CLS:\n\tcase BPF_PROG_TYPE_SCHED_ACT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic int check_ld_abs(struct bpf_verifier_env *env, struct bpf_insn *insn)\n{\n\tstruct bpf_reg_state *regs = cur_regs(env);\n\tstatic const int ctx_reg = BPF_REG_6;\n\tu8 mode = BPF_MODE(insn->code);\n\tint i, err;\n\n\tif (!may_access_skb(resolve_prog_type(env->prog))) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] instructions not allowed for this program type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!env->ops->gen_ld_abs) {\n\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\n\t    BPF_SIZE(insn->code) == BPF_DW ||\n\t    (mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] uses reserved fields\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\terr = check_reg_arg(env, ctx_reg, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = check_reference_leak(env);\n\tif (err) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be mixed with socket references\\n\");\n\t\treturn err;\n\t}\n\n\tif (env->cur_state->active_lock.ptr) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be used inside bpf_spin_lock-ed region\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (env->cur_state->active_rcu_lock) {\n\t\tverbose(env, \"BPF_LD_[ABS|IND] cannot be used inside bpf_rcu_read_lock-ed region\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (regs[ctx_reg].type != PTR_TO_CTX) {\n\t\tverbose(env,\n\t\t\t\"at the time of BPF_LD_ABS|IND R6 != pointer to skb\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (mode == BPF_IND) {\n\t\t \n\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = check_ptr_off_reg(env, &regs[ctx_reg], ctx_reg);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tfor (i = 0; i < CALLER_SAVED_REGS; i++) {\n\t\tmark_reg_not_init(env, regs, caller_saved[i]);\n\t\tcheck_reg_arg(env, caller_saved[i], DST_OP_NO_MARK);\n\t}\n\n\t \n\tmark_reg_unknown(env, regs, BPF_REG_0);\n\t \n\tregs[BPF_REG_0].subreg_def = env->insn_idx + 1;\n\treturn 0;\n}\n\nstatic int check_return_code(struct bpf_verifier_env *env)\n{\n\tstruct tnum enforce_attach_type_range = tnum_unknown;\n\tconst struct bpf_prog *prog = env->prog;\n\tstruct bpf_reg_state *reg;\n\tstruct tnum range = tnum_range(0, 1), const_0 = tnum_const(0);\n\tenum bpf_prog_type prog_type = resolve_prog_type(env->prog);\n\tint err;\n\tstruct bpf_func_state *frame = env->cur_state->frame[0];\n\tconst bool is_subprog = frame->subprogno;\n\n\t \n\tif (!is_subprog) {\n\t\tswitch (prog_type) {\n\t\tcase BPF_PROG_TYPE_LSM:\n\t\t\tif (prog->expected_attach_type == BPF_LSM_CGROUP)\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase BPF_PROG_TYPE_STRUCT_OPS:\n\t\t\tif (!prog->aux->attach_func_proto->type)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\terr = check_reg_arg(env, BPF_REG_0, SRC_OP);\n\tif (err)\n\t\treturn err;\n\n\tif (is_pointer_value(env, BPF_REG_0)) {\n\t\tverbose(env, \"R0 leaks addr as return value\\n\");\n\t\treturn -EACCES;\n\t}\n\n\treg = cur_regs(env) + BPF_REG_0;\n\n\tif (frame->in_async_callback_fn) {\n\t\t \n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"In async callback the register R0 is not a known value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!tnum_in(const_0, reg->var_off)) {\n\t\t\tverbose_invalid_scalar(env, reg, &const_0, \"async callback\", \"R0\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (is_subprog) {\n\t\tif (reg->type != SCALAR_VALUE) {\n\t\t\tverbose(env, \"At subprogram exit the register R0 is not a scalar value (%s)\\n\",\n\t\t\t\treg_type_str(env, reg->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tswitch (prog_type) {\n\tcase BPF_PROG_TYPE_CGROUP_SOCK_ADDR:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_UDP4_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_UDP6_RECVMSG ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETPEERNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET4_GETSOCKNAME ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_GETSOCKNAME)\n\t\t\trange = tnum_range(1, 1);\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET4_BIND ||\n\t\t    env->prog->expected_attach_type == BPF_CGROUP_INET6_BIND)\n\t\t\trange = tnum_range(0, 3);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SKB:\n\t\tif (env->prog->expected_attach_type == BPF_CGROUP_INET_EGRESS) {\n\t\t\trange = tnum_range(0, 3);\n\t\t\tenforce_attach_type_range = tnum_range(2, 3);\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_CGROUP_SOCK:\n\tcase BPF_PROG_TYPE_SOCK_OPS:\n\tcase BPF_PROG_TYPE_CGROUP_DEVICE:\n\tcase BPF_PROG_TYPE_CGROUP_SYSCTL:\n\tcase BPF_PROG_TYPE_CGROUP_SOCKOPT:\n\t\tbreak;\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\t\tif (!env->prog->aux->attach_btf_id)\n\t\t\treturn 0;\n\t\trange = tnum_const(0);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_TRACING:\n\t\tswitch (env->prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\t\trange = tnum_const(0);\n\t\t\tbreak;\n\t\tcase BPF_TRACE_RAW_TP:\n\t\tcase BPF_MODIFY_RETURN:\n\t\t\treturn 0;\n\t\tcase BPF_TRACE_ITER:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -ENOTSUPP;\n\t\t}\n\t\tbreak;\n\tcase BPF_PROG_TYPE_SK_LOOKUP:\n\t\trange = tnum_range(SK_DROP, SK_PASS);\n\t\tbreak;\n\n\tcase BPF_PROG_TYPE_LSM:\n\t\tif (env->prog->expected_attach_type != BPF_LSM_CGROUP) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t\tif (!env->prog->aux->attach_func_proto->type) {\n\t\t\t \n\t\t\trange = tnum_range(1, 1);\n\t\t}\n\t\tbreak;\n\n\tcase BPF_PROG_TYPE_NETFILTER:\n\t\trange = tnum_range(NF_DROP, NF_ACCEPT);\n\t\tbreak;\n\tcase BPF_PROG_TYPE_EXT:\n\t\t \n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (reg->type != SCALAR_VALUE) {\n\t\tverbose(env, \"At program exit the register R0 is not a known value (%s)\\n\",\n\t\t\treg_type_str(env, reg->type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_in(range, reg->var_off)) {\n\t\tverbose_invalid_scalar(env, reg, &range, \"program exit\", \"R0\");\n\t\tif (prog->expected_attach_type == BPF_LSM_CGROUP &&\n\t\t    prog_type == BPF_PROG_TYPE_LSM &&\n\t\t    !prog->aux->attach_func_proto->type)\n\t\t\tverbose(env, \"Note, BPF_LSM_CGROUP that attach to void LSM hooks can't modify return value!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!tnum_is_unknown(enforce_attach_type_range) &&\n\t    tnum_in(enforce_attach_type_range, reg->var_off))\n\t\tenv->prog->enforce_expected_attach_type = 1;\n\treturn 0;\n}\n\n \n\nenum {\n\tDISCOVERED = 0x10,\n\tEXPLORED = 0x20,\n\tFALLTHROUGH = 1,\n\tBRANCH = 2,\n};\n\nstatic u32 state_htab_size(struct bpf_verifier_env *env)\n{\n\treturn env->prog->len;\n}\n\nstatic struct bpf_verifier_state_list **explored_state(\n\t\t\t\t\tstruct bpf_verifier_env *env,\n\t\t\t\t\tint idx)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\tstruct bpf_func_state *state = cur->frame[cur->curframe];\n\n\treturn &env->explored_states[(idx ^ state->callsite) % state_htab_size(env)];\n}\n\nstatic void mark_prune_point(struct bpf_verifier_env *env, int idx)\n{\n\tenv->insn_aux_data[idx].prune_point = true;\n}\n\nstatic bool is_prune_point(struct bpf_verifier_env *env, int insn_idx)\n{\n\treturn env->insn_aux_data[insn_idx].prune_point;\n}\n\nstatic void mark_force_checkpoint(struct bpf_verifier_env *env, int idx)\n{\n\tenv->insn_aux_data[idx].force_checkpoint = true;\n}\n\nstatic bool is_force_checkpoint(struct bpf_verifier_env *env, int insn_idx)\n{\n\treturn env->insn_aux_data[insn_idx].force_checkpoint;\n}\n\n\nenum {\n\tDONE_EXPLORING = 0,\n\tKEEP_EXPLORING = 1,\n};\n\n \nstatic int push_insn(int t, int w, int e, struct bpf_verifier_env *env)\n{\n\tint *insn_stack = env->cfg.insn_stack;\n\tint *insn_state = env->cfg.insn_state;\n\n\tif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\n\t\treturn DONE_EXPLORING;\n\n\tif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\n\t\treturn DONE_EXPLORING;\n\n\tif (w < 0 || w >= env->prog->len) {\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose(env, \"jump out of range from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e == BRANCH) {\n\t\t \n\t\tmark_prune_point(env, w);\n\t\tmark_jmp_point(env, w);\n\t}\n\n\tif (insn_state[w] == 0) {\n\t\t \n\t\tinsn_state[t] = DISCOVERED | e;\n\t\tinsn_state[w] = DISCOVERED;\n\t\tif (env->cfg.cur_stack >= env->prog->len)\n\t\t\treturn -E2BIG;\n\t\tinsn_stack[env->cfg.cur_stack++] = w;\n\t\treturn KEEP_EXPLORING;\n\t} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\n\t\tif (env->bpf_capable)\n\t\t\treturn DONE_EXPLORING;\n\t\tverbose_linfo(env, t, \"%d: \", t);\n\t\tverbose_linfo(env, w, \"%d: \", w);\n\t\tverbose(env, \"back-edge from insn %d to %d\\n\", t, w);\n\t\treturn -EINVAL;\n\t} else if (insn_state[w] == EXPLORED) {\n\t\t \n\t\tinsn_state[t] = DISCOVERED | e;\n\t} else {\n\t\tverbose(env, \"insn state internal bug\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn DONE_EXPLORING;\n}\n\nstatic int visit_func_call_insn(int t, struct bpf_insn *insns,\n\t\t\t\tstruct bpf_verifier_env *env,\n\t\t\t\tbool visit_callee)\n{\n\tint ret, insn_sz;\n\n\tinsn_sz = bpf_is_ldimm64(&insns[t]) ? 2 : 1;\n\tret = push_insn(t, t + insn_sz, FALLTHROUGH, env);\n\tif (ret)\n\t\treturn ret;\n\n\tmark_prune_point(env, t + insn_sz);\n\t \n\tmark_jmp_point(env, t + insn_sz);\n\n\tif (visit_callee) {\n\t\tmark_prune_point(env, t);\n\t\tret = push_insn(t, t + insns[t].imm + 1, BRANCH, env);\n\t}\n\treturn ret;\n}\n\n \nstatic int visit_insn(int t, struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insns = env->prog->insnsi, *insn = &insns[t];\n\tint ret, off, insn_sz;\n\n\tif (bpf_pseudo_func(insn))\n\t\treturn visit_func_call_insn(t, insns, env, true);\n\n\t \n\tif (BPF_CLASS(insn->code) != BPF_JMP &&\n\t    BPF_CLASS(insn->code) != BPF_JMP32) {\n\t\tinsn_sz = bpf_is_ldimm64(insn) ? 2 : 1;\n\t\treturn push_insn(t, t + insn_sz, FALLTHROUGH, env);\n\t}\n\n\tswitch (BPF_OP(insn->code)) {\n\tcase BPF_EXIT:\n\t\treturn DONE_EXPLORING;\n\n\tcase BPF_CALL:\n\t\tif (insn->src_reg == 0 && insn->imm == BPF_FUNC_timer_set_callback)\n\t\t\t \n\t\t\tmark_prune_point(env, t);\n\t\tif (insn->src_reg == BPF_PSEUDO_KFUNC_CALL) {\n\t\t\tstruct bpf_kfunc_call_arg_meta meta;\n\n\t\t\tret = fetch_kfunc_meta(env, insn, &meta, NULL);\n\t\t\tif (ret == 0 && is_iter_next_kfunc(&meta)) {\n\t\t\t\tmark_prune_point(env, t);\n\t\t\t\t \n\t\t\t\tmark_force_checkpoint(env, t);\n\t\t\t}\n\t\t}\n\t\treturn visit_func_call_insn(t, insns, env, insn->src_reg == BPF_PSEUDO_CALL);\n\n\tcase BPF_JA:\n\t\tif (BPF_SRC(insn->code) != BPF_K)\n\t\t\treturn -EINVAL;\n\n\t\tif (BPF_CLASS(insn->code) == BPF_JMP)\n\t\t\toff = insn->off;\n\t\telse\n\t\t\toff = insn->imm;\n\n\t\t \n\t\tret = push_insn(t, t + off + 1, FALLTHROUGH, env);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tmark_prune_point(env, t + off + 1);\n\t\tmark_jmp_point(env, t + off + 1);\n\n\t\treturn ret;\n\n\tdefault:\n\t\t \n\t\tmark_prune_point(env, t);\n\n\t\tret = push_insn(t, t + 1, FALLTHROUGH, env);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\treturn push_insn(t, t + insn->off + 1, BRANCH, env);\n\t}\n}\n\n \nstatic int check_cfg(struct bpf_verifier_env *env)\n{\n\tint insn_cnt = env->prog->len;\n\tint *insn_stack, *insn_state;\n\tint ret = 0;\n\tint i;\n\n\tinsn_state = env->cfg.insn_state = kvcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_state)\n\t\treturn -ENOMEM;\n\n\tinsn_stack = env->cfg.insn_stack = kvcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\n\tif (!insn_stack) {\n\t\tkvfree(insn_state);\n\t\treturn -ENOMEM;\n\t}\n\n\tinsn_state[0] = DISCOVERED;  \n\tinsn_stack[0] = 0;  \n\tenv->cfg.cur_stack = 1;\n\n\twhile (env->cfg.cur_stack > 0) {\n\t\tint t = insn_stack[env->cfg.cur_stack - 1];\n\n\t\tret = visit_insn(t, env);\n\t\tswitch (ret) {\n\t\tcase DONE_EXPLORING:\n\t\t\tinsn_state[t] = EXPLORED;\n\t\t\tenv->cfg.cur_stack--;\n\t\t\tbreak;\n\t\tcase KEEP_EXPLORING:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (ret > 0) {\n\t\t\t\tverbose(env, \"visit_insn internal bug\\n\");\n\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\t}\n\n\tif (env->cfg.cur_stack < 0) {\n\t\tverbose(env, \"pop stack internal bug\\n\");\n\t\tret = -EFAULT;\n\t\tgoto err_free;\n\t}\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tstruct bpf_insn *insn = &env->prog->insnsi[i];\n\n\t\tif (insn_state[i] != EXPLORED) {\n\t\t\tverbose(env, \"unreachable insn %d\\n\", i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\t\tif (bpf_is_ldimm64(insn)) {\n\t\t\tif (insn_state[i + 1] != 0) {\n\t\t\t\tverbose(env, \"jump into the middle of ldimm64 insn %d\\n\", i);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t\ti++;  \n\t\t}\n\t}\n\tret = 0;  \n\nerr_free:\n\tkvfree(insn_state);\n\tkvfree(insn_stack);\n\tenv->cfg.insn_state = env->cfg.insn_stack = NULL;\n\treturn ret;\n}\n\nstatic int check_abnormal_return(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tfor (i = 1; i < env->subprog_cnt; i++) {\n\t\tif (env->subprog_info[i].has_ld_abs) {\n\t\t\tverbose(env, \"LD_ABS is not allowed in subprogs without BTF\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (env->subprog_info[i].has_tail_call) {\n\t\t\tverbose(env, \"tail_call is not allowed in subprogs without BTF\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \n#define MIN_BPF_FUNCINFO_SIZE\t8\n#define MAX_FUNCINFO_REC_SIZE\t252\n\nstatic int check_btf_func(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  bpfptr_t uattr)\n{\n\tconst struct btf_type *type, *func_proto, *ret_type;\n\tu32 i, nfuncs, urec_size, min_size;\n\tu32 krec_size = sizeof(struct bpf_func_info);\n\tstruct bpf_func_info *krecord;\n\tstruct bpf_func_info_aux *info_aux = NULL;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tbpfptr_t urecord;\n\tu32 prev_offset = 0;\n\tbool scalar_return;\n\tint ret = -ENOMEM;\n\n\tnfuncs = attr->func_info_cnt;\n\tif (!nfuncs) {\n\t\tif (check_abnormal_return(env))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\tif (nfuncs != env->subprog_cnt) {\n\t\tverbose(env, \"number of funcs in func_info doesn't match number of subprogs\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\turec_size = attr->func_info_rec_size;\n\tif (urec_size < MIN_BPF_FUNCINFO_SIZE ||\n\t    urec_size > MAX_FUNCINFO_REC_SIZE ||\n\t    urec_size % sizeof(u32)) {\n\t\tverbose(env, \"invalid func info rec size %u\\n\", urec_size);\n\t\treturn -EINVAL;\n\t}\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\turecord = make_bpfptr(attr->func_info, uattr.is_kernel);\n\tmin_size = min_t(u32, krec_size, urec_size);\n\n\tkrecord = kvcalloc(nfuncs, krec_size, GFP_KERNEL | __GFP_NOWARN);\n\tif (!krecord)\n\t\treturn -ENOMEM;\n\tinfo_aux = kcalloc(nfuncs, sizeof(*info_aux), GFP_KERNEL | __GFP_NOWARN);\n\tif (!info_aux)\n\t\tgoto err_free;\n\n\tfor (i = 0; i < nfuncs; i++) {\n\t\tret = bpf_check_uarg_tail_zero(urecord, krec_size, urec_size);\n\t\tif (ret) {\n\t\t\tif (ret == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in func info\");\n\t\t\t\t \n\t\t\t\tif (copy_to_bpfptr_offset(uattr,\n\t\t\t\t\t\t\t  offsetof(union bpf_attr, func_info_rec_size),\n\t\t\t\t\t\t\t  &min_size, sizeof(min_size)))\n\t\t\t\t\tret = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_bpfptr(&krecord[i], urecord, min_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t \n\t\tret = -EINVAL;\n\t\tif (i == 0) {\n\t\t\tif (krecord[i].insn_off) {\n\t\t\t\tverbose(env,\n\t\t\t\t\t\"nonzero insn_off %u for the first func info record\",\n\t\t\t\t\tkrecord[i].insn_off);\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t} else if (krecord[i].insn_off <= prev_offset) {\n\t\t\tverbose(env,\n\t\t\t\t\"same or smaller insn offset (%u) than previous func info record (%u)\",\n\t\t\t\tkrecord[i].insn_off, prev_offset);\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (env->subprog_info[i].start != krecord[i].insn_off) {\n\t\t\tverbose(env, \"func_info BTF section doesn't match subprog layout in BPF program\\n\");\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t \n\t\ttype = btf_type_by_id(btf, krecord[i].type_id);\n\t\tif (!type || !btf_type_is_func(type)) {\n\t\t\tverbose(env, \"invalid type id %d in func info\",\n\t\t\t\tkrecord[i].type_id);\n\t\t\tgoto err_free;\n\t\t}\n\t\tinfo_aux[i].linkage = BTF_INFO_VLEN(type->info);\n\n\t\tfunc_proto = btf_type_by_id(btf, type->type);\n\t\tif (unlikely(!func_proto || !btf_type_is_func_proto(func_proto)))\n\t\t\t \n\t\t\tgoto err_free;\n\t\tret_type = btf_type_skip_modifiers(btf, func_proto->type, NULL);\n\t\tscalar_return =\n\t\t\tbtf_type_is_small_int(ret_type) || btf_is_any_enum(ret_type);\n\t\tif (i && !scalar_return && env->subprog_info[i].has_ld_abs) {\n\t\t\tverbose(env, \"LD_ABS is only allowed in functions that return 'int'.\\n\");\n\t\t\tgoto err_free;\n\t\t}\n\t\tif (i && !scalar_return && env->subprog_info[i].has_tail_call) {\n\t\t\tverbose(env, \"tail_call is only allowed in functions that return 'int'.\\n\");\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tprev_offset = krecord[i].insn_off;\n\t\tbpfptr_add(&urecord, urec_size);\n\t}\n\n\tprog->aux->func_info = krecord;\n\tprog->aux->func_info_cnt = nfuncs;\n\tprog->aux->func_info_aux = info_aux;\n\treturn 0;\n\nerr_free:\n\tkvfree(krecord);\n\tkfree(info_aux);\n\treturn ret;\n}\n\nstatic void adjust_btf_func(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_aux *aux = env->prog->aux;\n\tint i;\n\n\tif (!aux->func_info)\n\t\treturn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\taux->func_info[i].insn_off = env->subprog_info[i].start;\n}\n\n#define MIN_BPF_LINEINFO_SIZE\toffsetofend(struct bpf_line_info, line_col)\n#define MAX_LINEINFO_REC_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_btf_line(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  bpfptr_t uattr)\n{\n\tu32 i, s, nr_linfo, ncopy, expected_size, rec_size, prev_offset = 0;\n\tstruct bpf_subprog_info *sub;\n\tstruct bpf_line_info *linfo;\n\tstruct bpf_prog *prog;\n\tconst struct btf *btf;\n\tbpfptr_t ulinfo;\n\tint err;\n\n\tnr_linfo = attr->line_info_cnt;\n\tif (!nr_linfo)\n\t\treturn 0;\n\tif (nr_linfo > INT_MAX / sizeof(struct bpf_line_info))\n\t\treturn -EINVAL;\n\n\trec_size = attr->line_info_rec_size;\n\tif (rec_size < MIN_BPF_LINEINFO_SIZE ||\n\t    rec_size > MAX_LINEINFO_REC_SIZE ||\n\t    rec_size & (sizeof(u32) - 1))\n\t\treturn -EINVAL;\n\n\t \n\tlinfo = kvcalloc(nr_linfo, sizeof(struct bpf_line_info),\n\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\tif (!linfo)\n\t\treturn -ENOMEM;\n\n\tprog = env->prog;\n\tbtf = prog->aux->btf;\n\n\ts = 0;\n\tsub = env->subprog_info;\n\tulinfo = make_bpfptr(attr->line_info, uattr.is_kernel);\n\texpected_size = sizeof(struct bpf_line_info);\n\tncopy = min_t(u32, expected_size, rec_size);\n\tfor (i = 0; i < nr_linfo; i++) {\n\t\terr = bpf_check_uarg_tail_zero(ulinfo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in line_info\");\n\t\t\t\tif (copy_to_bpfptr_offset(uattr,\n\t\t\t\t\t\t\t  offsetof(union bpf_attr, line_info_rec_size),\n\t\t\t\t\t\t\t  &expected_size, sizeof(expected_size)))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (copy_from_bpfptr(&linfo[i], ulinfo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\t \n\t\tif ((i && linfo[i].insn_off <= prev_offset) ||\n\t\t    linfo[i].insn_off >= prog->len) {\n\t\t\tverbose(env, \"Invalid line_info[%u].insn_off:%u (prev_offset:%u prog->len:%u)\\n\",\n\t\t\t\ti, linfo[i].insn_off, prev_offset,\n\t\t\t\tprog->len);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!prog->insnsi[linfo[i].insn_off].code) {\n\t\t\tverbose(env,\n\t\t\t\t\"Invalid insn code at line_info[%u].insn_off\\n\",\n\t\t\t\ti);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (!btf_name_by_offset(btf, linfo[i].line_off) ||\n\t\t    !btf_name_by_offset(btf, linfo[i].file_name_off)) {\n\t\t\tverbose(env, \"Invalid line_info[%u].line_off or .file_name_off\\n\", i);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_free;\n\t\t}\n\n\t\tif (s != env->subprog_cnt) {\n\t\t\tif (linfo[i].insn_off == sub[s].start) {\n\t\t\t\tsub[s].linfo_idx = i;\n\t\t\t\ts++;\n\t\t\t} else if (sub[s].start < linfo[i].insn_off) {\n\t\t\t\tverbose(env, \"missing bpf_line_info for func#%u\\n\", s);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\n\t\tprev_offset = linfo[i].insn_off;\n\t\tbpfptr_add(&ulinfo, rec_size);\n\t}\n\n\tif (s != env->subprog_cnt) {\n\t\tverbose(env, \"missing bpf_line_info for %u funcs starting from func#%u\\n\",\n\t\t\tenv->subprog_cnt - s, s);\n\t\terr = -EINVAL;\n\t\tgoto err_free;\n\t}\n\n\tprog->aux->linfo = linfo;\n\tprog->aux->nr_linfo = nr_linfo;\n\n\treturn 0;\n\nerr_free:\n\tkvfree(linfo);\n\treturn err;\n}\n\n#define MIN_CORE_RELO_SIZE\tsizeof(struct bpf_core_relo)\n#define MAX_CORE_RELO_SIZE\tMAX_FUNCINFO_REC_SIZE\n\nstatic int check_core_relo(struct bpf_verifier_env *env,\n\t\t\t   const union bpf_attr *attr,\n\t\t\t   bpfptr_t uattr)\n{\n\tu32 i, nr_core_relo, ncopy, expected_size, rec_size;\n\tstruct bpf_core_relo core_relo = {};\n\tstruct bpf_prog *prog = env->prog;\n\tconst struct btf *btf = prog->aux->btf;\n\tstruct bpf_core_ctx ctx = {\n\t\t.log = &env->log,\n\t\t.btf = btf,\n\t};\n\tbpfptr_t u_core_relo;\n\tint err;\n\n\tnr_core_relo = attr->core_relo_cnt;\n\tif (!nr_core_relo)\n\t\treturn 0;\n\tif (nr_core_relo > INT_MAX / sizeof(struct bpf_core_relo))\n\t\treturn -EINVAL;\n\n\trec_size = attr->core_relo_rec_size;\n\tif (rec_size < MIN_CORE_RELO_SIZE ||\n\t    rec_size > MAX_CORE_RELO_SIZE ||\n\t    rec_size % sizeof(u32))\n\t\treturn -EINVAL;\n\n\tu_core_relo = make_bpfptr(attr->core_relos, uattr.is_kernel);\n\texpected_size = sizeof(struct bpf_core_relo);\n\tncopy = min_t(u32, expected_size, rec_size);\n\n\t \n\tfor (i = 0; i < nr_core_relo; i++) {\n\t\t \n\t\terr = bpf_check_uarg_tail_zero(u_core_relo, expected_size, rec_size);\n\t\tif (err) {\n\t\t\tif (err == -E2BIG) {\n\t\t\t\tverbose(env, \"nonzero tailing record in core_relo\");\n\t\t\t\tif (copy_to_bpfptr_offset(uattr,\n\t\t\t\t\t\t\t  offsetof(union bpf_attr, core_relo_rec_size),\n\t\t\t\t\t\t\t  &expected_size, sizeof(expected_size)))\n\t\t\t\t\terr = -EFAULT;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tif (copy_from_bpfptr(&core_relo, u_core_relo, ncopy)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (core_relo.insn_off % 8 || core_relo.insn_off / 8 >= prog->len) {\n\t\t\tverbose(env, \"Invalid core_relo[%u].insn_off:%u prog->len:%u\\n\",\n\t\t\t\ti, core_relo.insn_off, prog->len);\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = bpf_core_apply(&ctx, &core_relo, i,\n\t\t\t\t     &prog->insnsi[core_relo.insn_off / 8]);\n\t\tif (err)\n\t\t\tbreak;\n\t\tbpfptr_add(&u_core_relo, rec_size);\n\t}\n\treturn err;\n}\n\nstatic int check_btf_info(struct bpf_verifier_env *env,\n\t\t\t  const union bpf_attr *attr,\n\t\t\t  bpfptr_t uattr)\n{\n\tstruct btf *btf;\n\tint err;\n\n\tif (!attr->func_info_cnt && !attr->line_info_cnt) {\n\t\tif (check_abnormal_return(env))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\tbtf = btf_get_by_fd(attr->prog_btf_fd);\n\tif (IS_ERR(btf))\n\t\treturn PTR_ERR(btf);\n\tif (btf_is_kernel(btf)) {\n\t\tbtf_put(btf);\n\t\treturn -EACCES;\n\t}\n\tenv->prog->aux->btf = btf;\n\n\terr = check_btf_func(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_btf_line(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\terr = check_core_relo(env, attr, uattr);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\n \nstatic bool range_within(struct bpf_reg_state *old,\n\t\t\t struct bpf_reg_state *cur)\n{\n\treturn old->umin_value <= cur->umin_value &&\n\t       old->umax_value >= cur->umax_value &&\n\t       old->smin_value <= cur->smin_value &&\n\t       old->smax_value >= cur->smax_value &&\n\t       old->u32_min_value <= cur->u32_min_value &&\n\t       old->u32_max_value >= cur->u32_max_value &&\n\t       old->s32_min_value <= cur->s32_min_value &&\n\t       old->s32_max_value >= cur->s32_max_value;\n}\n\n \nstatic bool check_ids(u32 old_id, u32 cur_id, struct bpf_idmap *idmap)\n{\n\tstruct bpf_id_pair *map = idmap->map;\n\tunsigned int i;\n\n\t \n\tif (!!old_id != !!cur_id)\n\t\treturn false;\n\n\tif (old_id == 0)  \n\t\treturn true;\n\n\tfor (i = 0; i < BPF_ID_MAP_SIZE; i++) {\n\t\tif (!map[i].old) {\n\t\t\t \n\t\t\tmap[i].old = old_id;\n\t\t\tmap[i].cur = cur_id;\n\t\t\treturn true;\n\t\t}\n\t\tif (map[i].old == old_id)\n\t\t\treturn map[i].cur == cur_id;\n\t\tif (map[i].cur == cur_id)\n\t\t\treturn false;\n\t}\n\t \n\tWARN_ON_ONCE(1);\n\treturn false;\n}\n\n \nstatic bool check_scalar_ids(u32 old_id, u32 cur_id, struct bpf_idmap *idmap)\n{\n\told_id = old_id ? old_id : ++idmap->tmp_id_gen;\n\tcur_id = cur_id ? cur_id : ++idmap->tmp_id_gen;\n\n\treturn check_ids(old_id, cur_id, idmap);\n}\n\nstatic void clean_func_state(struct bpf_verifier_env *env,\n\t\t\t     struct bpf_func_state *st)\n{\n\tenum bpf_reg_liveness live;\n\tint i, j;\n\n\tfor (i = 0; i < BPF_REG_FP; i++) {\n\t\tlive = st->regs[i].live;\n\t\t \n\t\tst->regs[i].live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ))\n\t\t\t \n\t\t\t__mark_reg_not_init(env, &st->regs[i]);\n\t}\n\n\tfor (i = 0; i < st->allocated_stack / BPF_REG_SIZE; i++) {\n\t\tlive = st->stack[i].spilled_ptr.live;\n\t\t \n\t\tst->stack[i].spilled_ptr.live |= REG_LIVE_DONE;\n\t\tif (!(live & REG_LIVE_READ)) {\n\t\t\t__mark_reg_not_init(env, &st->stack[i].spilled_ptr);\n\t\t\tfor (j = 0; j < BPF_REG_SIZE; j++)\n\t\t\t\tst->stack[i].slot_type[j] = STACK_INVALID;\n\t\t}\n\t}\n}\n\nstatic void clean_verifier_state(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_verifier_state *st)\n{\n\tint i;\n\n\tif (st->frame[0]->regs[0].live & REG_LIVE_DONE)\n\t\t \n\t\treturn;\n\n\tfor (i = 0; i <= st->curframe; i++)\n\t\tclean_func_state(env, st->frame[i]);\n}\n\n \nstatic void clean_live_states(struct bpf_verifier_env *env, int insn,\n\t\t\t      struct bpf_verifier_state *cur)\n{\n\tstruct bpf_verifier_state_list *sl;\n\tint i;\n\n\tsl = *explored_state(env, insn);\n\twhile (sl) {\n\t\tif (sl->state.branches)\n\t\t\tgoto next;\n\t\tif (sl->state.insn_idx != insn ||\n\t\t    sl->state.curframe != cur->curframe)\n\t\t\tgoto next;\n\t\tfor (i = 0; i <= cur->curframe; i++)\n\t\t\tif (sl->state.frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\t\tgoto next;\n\t\tclean_verifier_state(env, &sl->state);\nnext:\n\t\tsl = sl->next;\n\t}\n}\n\nstatic bool regs_exact(const struct bpf_reg_state *rold,\n\t\t       const struct bpf_reg_state *rcur,\n\t\t       struct bpf_idmap *idmap)\n{\n\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t       check_ids(rold->id, rcur->id, idmap) &&\n\t       check_ids(rold->ref_obj_id, rcur->ref_obj_id, idmap);\n}\n\n \nstatic bool regsafe(struct bpf_verifier_env *env, struct bpf_reg_state *rold,\n\t\t    struct bpf_reg_state *rcur, struct bpf_idmap *idmap)\n{\n\tif (!(rold->live & REG_LIVE_READ))\n\t\t \n\t\treturn true;\n\tif (rold->type == NOT_INIT)\n\t\t \n\t\treturn true;\n\tif (rcur->type == NOT_INIT)\n\t\treturn false;\n\n\t \n\tif (rold->type != rcur->type)\n\t\treturn false;\n\n\tswitch (base_type(rold->type)) {\n\tcase SCALAR_VALUE:\n\t\tif (env->explore_alu_limits) {\n\t\t\t \n\t\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, id)) == 0 &&\n\t\t\t       check_scalar_ids(rold->id, rcur->id, idmap);\n\t\t}\n\t\tif (!rold->precise)\n\t\t\treturn true;\n\t\t \n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off) &&\n\t\t       check_scalar_ids(rold->id, rcur->id, idmap);\n\tcase PTR_TO_MAP_KEY:\n\tcase PTR_TO_MAP_VALUE:\n\tcase PTR_TO_MEM:\n\tcase PTR_TO_BUF:\n\tcase PTR_TO_TP_BUFFER:\n\t\t \n\t\treturn memcmp(rold, rcur, offsetof(struct bpf_reg_state, var_off)) == 0 &&\n\t\t       range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off) &&\n\t\t       check_ids(rold->id, rcur->id, idmap) &&\n\t\t       check_ids(rold->ref_obj_id, rcur->ref_obj_id, idmap);\n\tcase PTR_TO_PACKET_META:\n\tcase PTR_TO_PACKET:\n\t\t \n\t\tif (rold->range > rcur->range)\n\t\t\treturn false;\n\t\t \n\t\tif (rold->off != rcur->off)\n\t\t\treturn false;\n\t\t \n\t\tif (!check_ids(rold->id, rcur->id, idmap))\n\t\t\treturn false;\n\t\t \n\t\treturn range_within(rold, rcur) &&\n\t\t       tnum_in(rold->var_off, rcur->var_off);\n\tcase PTR_TO_STACK:\n\t\t \n\t\treturn regs_exact(rold, rcur, idmap) && rold->frameno == rcur->frameno;\n\tdefault:\n\t\treturn regs_exact(rold, rcur, idmap);\n\t}\n}\n\nstatic bool stacksafe(struct bpf_verifier_env *env, struct bpf_func_state *old,\n\t\t      struct bpf_func_state *cur, struct bpf_idmap *idmap)\n{\n\tint i, spi;\n\n\t \n\tfor (i = 0; i < old->allocated_stack; i++) {\n\t\tstruct bpf_reg_state *old_reg, *cur_reg;\n\n\t\tspi = i / BPF_REG_SIZE;\n\n\t\tif (!(old->stack[spi].spilled_ptr.live & REG_LIVE_READ)) {\n\t\t\ti += BPF_REG_SIZE - 1;\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_INVALID)\n\t\t\tcontinue;\n\n\t\tif (env->allow_uninit_stack &&\n\t\t    old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (i >= cur->allocated_stack)\n\t\t\treturn false;\n\n\t\t \n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_MISC &&\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\t\tif (old->stack[spi].slot_type[i % BPF_REG_SIZE] !=\n\t\t    cur->stack[spi].slot_type[i % BPF_REG_SIZE])\n\t\t\t \n\t\t\treturn false;\n\t\tif (i % BPF_REG_SIZE != BPF_REG_SIZE - 1)\n\t\t\tcontinue;\n\t\t \n\t\tswitch (old->stack[spi].slot_type[BPF_REG_SIZE - 1]) {\n\t\tcase STACK_SPILL:\n\t\t\t \n\t\t\tif (!regsafe(env, &old->stack[spi].spilled_ptr,\n\t\t\t\t     &cur->stack[spi].spilled_ptr, idmap))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase STACK_DYNPTR:\n\t\t\told_reg = &old->stack[spi].spilled_ptr;\n\t\t\tcur_reg = &cur->stack[spi].spilled_ptr;\n\t\t\tif (old_reg->dynptr.type != cur_reg->dynptr.type ||\n\t\t\t    old_reg->dynptr.first_slot != cur_reg->dynptr.first_slot ||\n\t\t\t    !check_ids(old_reg->ref_obj_id, cur_reg->ref_obj_id, idmap))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase STACK_ITER:\n\t\t\told_reg = &old->stack[spi].spilled_ptr;\n\t\t\tcur_reg = &cur->stack[spi].spilled_ptr;\n\t\t\t \n\t\t\tif (old_reg->iter.btf != cur_reg->iter.btf ||\n\t\t\t    old_reg->iter.btf_id != cur_reg->iter.btf_id ||\n\t\t\t    old_reg->iter.state != cur_reg->iter.state ||\n\t\t\t     \n\t\t\t    !check_ids(old_reg->ref_obj_id, cur_reg->ref_obj_id, idmap))\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase STACK_MISC:\n\t\tcase STACK_ZERO:\n\t\tcase STACK_INVALID:\n\t\t\tcontinue;\n\t\t \n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\n\nstatic bool refsafe(struct bpf_func_state *old, struct bpf_func_state *cur,\n\t\t    struct bpf_idmap *idmap)\n{\n\tint i;\n\n\tif (old->acquired_refs != cur->acquired_refs)\n\t\treturn false;\n\n\tfor (i = 0; i < old->acquired_refs; i++) {\n\t\tif (!check_ids(old->refs[i].id, cur->refs[i].id, idmap))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic bool func_states_equal(struct bpf_verifier_env *env, struct bpf_func_state *old,\n\t\t\t      struct bpf_func_state *cur)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (!regsafe(env, &old->regs[i], &cur->regs[i],\n\t\t\t     &env->idmap_scratch))\n\t\t\treturn false;\n\n\tif (!stacksafe(env, old, cur, &env->idmap_scratch))\n\t\treturn false;\n\n\tif (!refsafe(old, cur, &env->idmap_scratch))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool states_equal(struct bpf_verifier_env *env,\n\t\t\t struct bpf_verifier_state *old,\n\t\t\t struct bpf_verifier_state *cur)\n{\n\tint i;\n\n\tif (old->curframe != cur->curframe)\n\t\treturn false;\n\n\tenv->idmap_scratch.tmp_id_gen = env->id_gen;\n\tmemset(&env->idmap_scratch.map, 0, sizeof(env->idmap_scratch.map));\n\n\t \n\tif (old->speculative && !cur->speculative)\n\t\treturn false;\n\n\tif (old->active_lock.ptr != cur->active_lock.ptr)\n\t\treturn false;\n\n\t \n\tif (!!old->active_lock.id != !!cur->active_lock.id)\n\t\treturn false;\n\n\tif (old->active_lock.id &&\n\t    !check_ids(old->active_lock.id, cur->active_lock.id, &env->idmap_scratch))\n\t\treturn false;\n\n\tif (old->active_rcu_lock != cur->active_rcu_lock)\n\t\treturn false;\n\n\t \n\tfor (i = 0; i <= old->curframe; i++) {\n\t\tif (old->frame[i]->callsite != cur->frame[i]->callsite)\n\t\t\treturn false;\n\t\tif (!func_states_equal(env, old->frame[i], cur->frame[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic int propagate_liveness_reg(struct bpf_verifier_env *env,\n\t\t\t\t  struct bpf_reg_state *reg,\n\t\t\t\t  struct bpf_reg_state *parent_reg)\n{\n\tu8 parent_flag = parent_reg->live & REG_LIVE_READ;\n\tu8 flag = reg->live & REG_LIVE_READ;\n\tint err;\n\n\t \n\tif (parent_flag == REG_LIVE_READ64 ||\n\t     \n\t    !flag ||\n\t     \n\t    parent_flag == flag)\n\t\treturn 0;\n\n\terr = mark_reg_read(env, reg, parent_reg, flag);\n\tif (err)\n\t\treturn err;\n\n\treturn flag;\n}\n\n \nstatic int propagate_liveness(struct bpf_verifier_env *env,\n\t\t\t      const struct bpf_verifier_state *vstate,\n\t\t\t      struct bpf_verifier_state *vparent)\n{\n\tstruct bpf_reg_state *state_reg, *parent_reg;\n\tstruct bpf_func_state *state, *parent;\n\tint i, frame, err = 0;\n\n\tif (vparent->curframe != vstate->curframe) {\n\t\tWARN(1, \"propagate_live: parent frame %d current frame %d\\n\",\n\t\t     vparent->curframe, vstate->curframe);\n\t\treturn -EFAULT;\n\t}\n\t \n\tBUILD_BUG_ON(BPF_REG_FP + 1 != MAX_BPF_REG);\n\tfor (frame = 0; frame <= vstate->curframe; frame++) {\n\t\tparent = vparent->frame[frame];\n\t\tstate = vstate->frame[frame];\n\t\tparent_reg = parent->regs;\n\t\tstate_reg = state->regs;\n\t\t \n\t\tfor (i = frame < vstate->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++) {\n\t\t\terr = propagate_liveness_reg(env, &state_reg[i],\n\t\t\t\t\t\t     &parent_reg[i]);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\tif (err == REG_LIVE_READ64)\n\t\t\t\tmark_insn_zext(env, &parent_reg[i]);\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE &&\n\t\t\t    i < parent->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tparent_reg = &parent->stack[i].spilled_ptr;\n\t\t\tstate_reg = &state->stack[i].spilled_ptr;\n\t\t\terr = propagate_liveness_reg(env, state_reg,\n\t\t\t\t\t\t     parent_reg);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int propagate_precision(struct bpf_verifier_env *env,\n\t\t\t       const struct bpf_verifier_state *old)\n{\n\tstruct bpf_reg_state *state_reg;\n\tstruct bpf_func_state *state;\n\tint i, err = 0, fr;\n\tbool first;\n\n\tfor (fr = old->curframe; fr >= 0; fr--) {\n\t\tstate = old->frame[fr];\n\t\tstate_reg = state->regs;\n\t\tfirst = true;\n\t\tfor (i = 0; i < BPF_REG_FP; i++, state_reg++) {\n\t\t\tif (state_reg->type != SCALAR_VALUE ||\n\t\t\t    !state_reg->precise ||\n\t\t\t    !(state_reg->live & REG_LIVE_READ))\n\t\t\t\tcontinue;\n\t\t\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\t\t\tif (first)\n\t\t\t\t\tverbose(env, \"frame %d: propagating r%d\", fr, i);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \",r%d\", i);\n\t\t\t}\n\t\t\tbt_set_frame_reg(&env->bt, fr, i);\n\t\t\tfirst = false;\n\t\t}\n\n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tif (!is_spilled_reg(&state->stack[i]))\n\t\t\t\tcontinue;\n\t\t\tstate_reg = &state->stack[i].spilled_ptr;\n\t\t\tif (state_reg->type != SCALAR_VALUE ||\n\t\t\t    !state_reg->precise ||\n\t\t\t    !(state_reg->live & REG_LIVE_READ))\n\t\t\t\tcontinue;\n\t\t\tif (env->log.level & BPF_LOG_LEVEL2) {\n\t\t\t\tif (first)\n\t\t\t\t\tverbose(env, \"frame %d: propagating fp%d\",\n\t\t\t\t\t\tfr, (-i - 1) * BPF_REG_SIZE);\n\t\t\t\telse\n\t\t\t\t\tverbose(env, \",fp%d\", (-i - 1) * BPF_REG_SIZE);\n\t\t\t}\n\t\t\tbt_set_frame_slot(&env->bt, fr, i);\n\t\t\tfirst = false;\n\t\t}\n\t\tif (!first)\n\t\t\tverbose(env, \"\\n\");\n\t}\n\n\terr = mark_chain_precision_batch(env);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic bool states_maybe_looping(struct bpf_verifier_state *old,\n\t\t\t\t struct bpf_verifier_state *cur)\n{\n\tstruct bpf_func_state *fold, *fcur;\n\tint i, fr = cur->curframe;\n\n\tif (old->curframe != fr)\n\t\treturn false;\n\n\tfold = old->frame[fr];\n\tfcur = cur->frame[fr];\n\tfor (i = 0; i < MAX_BPF_REG; i++)\n\t\tif (memcmp(&fold->regs[i], &fcur->regs[i],\n\t\t\t   offsetof(struct bpf_reg_state, parent)))\n\t\t\treturn false;\n\treturn true;\n}\n\nstatic bool is_iter_next_insn(struct bpf_verifier_env *env, int insn_idx)\n{\n\treturn env->insn_aux_data[insn_idx].is_iter_next;\n}\n\n \nstatic bool iter_active_depths_differ(struct bpf_verifier_state *old, struct bpf_verifier_state *cur)\n{\n\tstruct bpf_reg_state *slot, *cur_slot;\n\tstruct bpf_func_state *state;\n\tint i, fr;\n\n\tfor (fr = old->curframe; fr >= 0; fr--) {\n\t\tstate = old->frame[fr];\n\t\tfor (i = 0; i < state->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tif (state->stack[i].slot_type[0] != STACK_ITER)\n\t\t\t\tcontinue;\n\n\t\t\tslot = &state->stack[i].spilled_ptr;\n\t\t\tif (slot->iter.state != BPF_ITER_STATE_ACTIVE)\n\t\t\t\tcontinue;\n\n\t\t\tcur_slot = &cur->frame[fr]->stack[i].spilled_ptr;\n\t\t\tif (cur_slot->iter.depth != slot->iter.depth)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic int is_state_visited(struct bpf_verifier_env *env, int insn_idx)\n{\n\tstruct bpf_verifier_state_list *new_sl;\n\tstruct bpf_verifier_state_list *sl, **pprev;\n\tstruct bpf_verifier_state *cur = env->cur_state, *new;\n\tint i, j, err, states_cnt = 0;\n\tbool force_new_state = env->test_state_freq || is_force_checkpoint(env, insn_idx);\n\tbool add_new_state = force_new_state;\n\n\t \n\tif (env->jmps_processed - env->prev_jmps_processed >= 2 &&\n\t    env->insn_processed - env->prev_insn_processed >= 8)\n\t\tadd_new_state = true;\n\n\tpprev = explored_state(env, insn_idx);\n\tsl = *pprev;\n\n\tclean_live_states(env, insn_idx, cur);\n\n\twhile (sl) {\n\t\tstates_cnt++;\n\t\tif (sl->state.insn_idx != insn_idx)\n\t\t\tgoto next;\n\n\t\tif (sl->state.branches) {\n\t\t\tstruct bpf_func_state *frame = sl->state.frame[sl->state.curframe];\n\n\t\t\tif (frame->in_async_callback_fn &&\n\t\t\t    frame->async_entry_cnt != cur->frame[cur->curframe]->async_entry_cnt) {\n\t\t\t\t \n\t\t\t\tgoto skip_inf_loop_check;\n\t\t\t}\n\t\t\t \n\t\t\tif (is_iter_next_insn(env, insn_idx)) {\n\t\t\t\tif (states_equal(env, &sl->state, cur)) {\n\t\t\t\t\tstruct bpf_func_state *cur_frame;\n\t\t\t\t\tstruct bpf_reg_state *iter_state, *iter_reg;\n\t\t\t\t\tint spi;\n\n\t\t\t\t\tcur_frame = cur->frame[cur->curframe];\n\t\t\t\t\t \n\t\t\t\t\titer_reg = &cur_frame->regs[BPF_REG_1];\n\t\t\t\t\t \n\t\t\t\t\tspi = __get_spi(iter_reg->off + iter_reg->var_off.value);\n\t\t\t\t\titer_state = &func(env, iter_reg)->stack[spi].spilled_ptr;\n\t\t\t\t\tif (iter_state->iter.state == BPF_ITER_STATE_ACTIVE)\n\t\t\t\t\t\tgoto hit;\n\t\t\t\t}\n\t\t\t\tgoto skip_inf_loop_check;\n\t\t\t}\n\t\t\t \n\t\t\tif (states_maybe_looping(&sl->state, cur) &&\n\t\t\t    states_equal(env, &sl->state, cur) &&\n\t\t\t    !iter_active_depths_differ(&sl->state, cur)) {\n\t\t\t\tverbose_linfo(env, insn_idx, \"; \");\n\t\t\t\tverbose(env, \"infinite loop detected at insn %d\\n\", insn_idx);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t \nskip_inf_loop_check:\n\t\t\tif (!force_new_state &&\n\t\t\t    env->jmps_processed - env->prev_jmps_processed < 20 &&\n\t\t\t    env->insn_processed - env->prev_insn_processed < 100)\n\t\t\t\tadd_new_state = false;\n\t\t\tgoto miss;\n\t\t}\n\t\tif (states_equal(env, &sl->state, cur)) {\nhit:\n\t\t\tsl->hit_cnt++;\n\t\t\t \n\t\t\terr = propagate_liveness(env, &sl->state, cur);\n\n\t\t\t \n\t\t\terr = err ? : push_jmp_history(env, cur);\n\t\t\terr = err ? : propagate_precision(env, &sl->state);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\treturn 1;\n\t\t}\nmiss:\n\t\t \n\t\tif (add_new_state)\n\t\t\tsl->miss_cnt++;\n\t\t \n\t\tif (sl->miss_cnt > sl->hit_cnt * 3 + 3) {\n\t\t\t \n\t\t\t*pprev = sl->next;\n\t\t\tif (sl->state.frame[0]->regs[0].live & REG_LIVE_DONE) {\n\t\t\t\tu32 br = sl->state.branches;\n\n\t\t\t\tWARN_ONCE(br,\n\t\t\t\t\t  \"BUG live_done but branches_to_explore %d\\n\",\n\t\t\t\t\t  br);\n\t\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\t\tkfree(sl);\n\t\t\t\tenv->peak_states--;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tsl->next = env->free_list;\n\t\t\t\tenv->free_list = sl;\n\t\t\t}\n\t\t\tsl = *pprev;\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tpprev = &sl->next;\n\t\tsl = *pprev;\n\t}\n\n\tif (env->max_states_per_insn < states_cnt)\n\t\tenv->max_states_per_insn = states_cnt;\n\n\tif (!env->bpf_capable && states_cnt > BPF_COMPLEXITY_LIMIT_STATES)\n\t\treturn 0;\n\n\tif (!add_new_state)\n\t\treturn 0;\n\n\t \n\tnew_sl = kzalloc(sizeof(struct bpf_verifier_state_list), GFP_KERNEL);\n\tif (!new_sl)\n\t\treturn -ENOMEM;\n\tenv->total_states++;\n\tenv->peak_states++;\n\tenv->prev_jmps_processed = env->jmps_processed;\n\tenv->prev_insn_processed = env->insn_processed;\n\n\t \n\tif (env->bpf_capable)\n\t\tmark_all_scalars_imprecise(env, cur);\n\n\t \n\tnew = &new_sl->state;\n\terr = copy_verifier_state(new, cur);\n\tif (err) {\n\t\tfree_verifier_state(new, false);\n\t\tkfree(new_sl);\n\t\treturn err;\n\t}\n\tnew->insn_idx = insn_idx;\n\tWARN_ONCE(new->branches != 1,\n\t\t  \"BUG is_state_visited:branches_to_explore=%d insn %d\\n\", new->branches, insn_idx);\n\n\tcur->parent = new;\n\tcur->first_insn_idx = insn_idx;\n\tclear_jmp_history(cur);\n\tnew_sl->next = *explored_state(env, insn_idx);\n\t*explored_state(env, insn_idx) = new_sl;\n\t \n\t \n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tfor (i = j < cur->curframe ? BPF_REG_6 : 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].parent = &new->frame[j]->regs[i];\n\t\tfor (i = 0; i < BPF_REG_FP; i++)\n\t\t\tcur->frame[j]->regs[i].live = REG_LIVE_NONE;\n\t}\n\n\t \n\tfor (j = 0; j <= cur->curframe; j++) {\n\t\tstruct bpf_func_state *frame = cur->frame[j];\n\t\tstruct bpf_func_state *newframe = new->frame[j];\n\n\t\tfor (i = 0; i < frame->allocated_stack / BPF_REG_SIZE; i++) {\n\t\t\tframe->stack[i].spilled_ptr.live = REG_LIVE_NONE;\n\t\t\tframe->stack[i].spilled_ptr.parent =\n\t\t\t\t\t\t&newframe->stack[i].spilled_ptr;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic bool reg_type_mismatch_ok(enum bpf_reg_type type)\n{\n\tswitch (base_type(type)) {\n\tcase PTR_TO_CTX:\n\tcase PTR_TO_SOCKET:\n\tcase PTR_TO_SOCK_COMMON:\n\tcase PTR_TO_TCP_SOCK:\n\tcase PTR_TO_XDP_SOCK:\n\tcase PTR_TO_BTF_ID:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n \nstatic bool reg_type_mismatch(enum bpf_reg_type src, enum bpf_reg_type prev)\n{\n\treturn src != prev && (!reg_type_mismatch_ok(src) ||\n\t\t\t       !reg_type_mismatch_ok(prev));\n}\n\nstatic int save_aux_ptr_type(struct bpf_verifier_env *env, enum bpf_reg_type type,\n\t\t\t     bool allow_trust_missmatch)\n{\n\tenum bpf_reg_type *prev_type = &env->insn_aux_data[env->insn_idx].ptr_type;\n\n\tif (*prev_type == NOT_INIT) {\n\t\t \n\t\t*prev_type = type;\n\t} else if (reg_type_mismatch(type, *prev_type)) {\n\t\t \n\t\tif (allow_trust_missmatch &&\n\t\t    base_type(type) == PTR_TO_BTF_ID &&\n\t\t    base_type(*prev_type) == PTR_TO_BTF_ID) {\n\t\t\t \n\t\t\t*prev_type = PTR_TO_BTF_ID | PTR_UNTRUSTED;\n\t\t} else {\n\t\t\tverbose(env, \"same insn cannot be used with different pointers\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int do_check(struct bpf_verifier_env *env)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state = env->cur_state;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_reg_state *regs;\n\tint insn_cnt = env->prog->len;\n\tbool do_print_state = false;\n\tint prev_insn_idx = -1;\n\n\tfor (;;) {\n\t\tstruct bpf_insn *insn;\n\t\tu8 class;\n\t\tint err;\n\n\t\tenv->prev_insn_idx = prev_insn_idx;\n\t\tif (env->insn_idx >= insn_cnt) {\n\t\t\tverbose(env, \"invalid insn idx %d insn_cnt %d\\n\",\n\t\t\t\tenv->insn_idx, insn_cnt);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn = &insns[env->insn_idx];\n\t\tclass = BPF_CLASS(insn->code);\n\n\t\tif (++env->insn_processed > BPF_COMPLEXITY_LIMIT_INSNS) {\n\t\t\tverbose(env,\n\t\t\t\t\"BPF program is too large. Processed %d insn\\n\",\n\t\t\t\tenv->insn_processed);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\tstate->last_insn_idx = env->prev_insn_idx;\n\n\t\tif (is_prune_point(env, env->insn_idx)) {\n\t\t\terr = is_state_visited(env, env->insn_idx);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\tif (err == 1) {\n\t\t\t\t \n\t\t\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\t\t\tif (do_print_state)\n\t\t\t\t\t\tverbose(env, \"\\nfrom %d to %d%s: safe\\n\",\n\t\t\t\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\t\t\telse\n\t\t\t\t\t\tverbose(env, \"%d: safe\\n\", env->insn_idx);\n\t\t\t\t}\n\t\t\t\tgoto process_bpf_exit;\n\t\t\t}\n\t\t}\n\n\t\tif (is_jmp_point(env, env->insn_idx)) {\n\t\t\terr = push_jmp_history(env, state);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EAGAIN;\n\n\t\tif (need_resched())\n\t\t\tcond_resched();\n\n\t\tif (env->log.level & BPF_LOG_LEVEL2 && do_print_state) {\n\t\t\tverbose(env, \"\\nfrom %d to %d%s:\",\n\t\t\t\tenv->prev_insn_idx, env->insn_idx,\n\t\t\t\tenv->cur_state->speculative ?\n\t\t\t\t\" (speculative execution)\" : \"\");\n\t\t\tprint_verifier_state(env, state->frame[state->curframe], true);\n\t\t\tdo_print_state = false;\n\t\t}\n\n\t\tif (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tconst struct bpf_insn_cbs cbs = {\n\t\t\t\t.cb_call\t= disasm_kfunc_name,\n\t\t\t\t.cb_print\t= verbose,\n\t\t\t\t.private_data\t= env,\n\t\t\t};\n\n\t\t\tif (verifier_state_scratched(env))\n\t\t\t\tprint_insn_state(env, state->frame[state->curframe]);\n\n\t\t\tverbose_linfo(env, env->insn_idx, \"; \");\n\t\t\tenv->prev_log_pos = env->log.end_pos;\n\t\t\tverbose(env, \"%d: \", env->insn_idx);\n\t\t\tprint_bpf_insn(&cbs, insn, env->allow_ptr_leaks);\n\t\t\tenv->prev_insn_print_pos = env->log.end_pos - env->prev_log_pos;\n\t\t\tenv->prev_log_pos = env->log.end_pos;\n\t\t}\n\n\t\tif (bpf_prog_is_offloaded(env->prog->aux)) {\n\t\t\terr = bpf_prog_offload_verify_insn(env, env->insn_idx,\n\t\t\t\t\t\t\t   env->prev_insn_idx);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tregs = cur_regs(env);\n\t\tsanitize_mark_insn_seen(env);\n\t\tprev_insn_idx = env->insn_idx;\n\n\t\tif (class == BPF_ALU || class == BPF_ALU64) {\n\t\t\terr = check_alu_op(env, insn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t} else if (class == BPF_LDX) {\n\t\t\tenum bpf_reg_type src_reg_type;\n\n\t\t\t \n\n\t\t\t \n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = check_reg_arg(env, insn->dst_reg, DST_OP_NO_MARK);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tsrc_reg_type = regs[insn->src_reg].type;\n\n\t\t\t \n\t\t\terr = check_mem_access(env, env->insn_idx, insn->src_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_READ, insn->dst_reg, false,\n\t\t\t\t\t       BPF_MODE(insn->code) == BPF_MEMSX);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = save_aux_ptr_type(env, src_reg_type, true);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else if (class == BPF_STX) {\n\t\t\tenum bpf_reg_type dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_ATOMIC) {\n\t\t\t\terr = check_atomic(env, env->insn_idx, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM || insn->imm != 0) {\n\t\t\t\tverbose(env, \"BPF_STX uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t \n\t\t\terr = check_reg_arg(env, insn->src_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\t \n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t \n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, insn->src_reg, false, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = save_aux_ptr_type(env, dst_reg_type, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else if (class == BPF_ST) {\n\t\t\tenum bpf_reg_type dst_reg_type;\n\n\t\t\tif (BPF_MODE(insn->code) != BPF_MEM ||\n\t\t\t    insn->src_reg != BPF_REG_0) {\n\t\t\t\tverbose(env, \"BPF_ST uses reserved fields\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t \n\t\t\terr = check_reg_arg(env, insn->dst_reg, SRC_OP);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tdst_reg_type = regs[insn->dst_reg].type;\n\n\t\t\t \n\t\t\terr = check_mem_access(env, env->insn_idx, insn->dst_reg,\n\t\t\t\t\t       insn->off, BPF_SIZE(insn->code),\n\t\t\t\t\t       BPF_WRITE, -1, false, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\terr = save_aux_ptr_type(env, dst_reg_type, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t} else if (class == BPF_JMP || class == BPF_JMP32) {\n\t\t\tu8 opcode = BPF_OP(insn->code);\n\n\t\t\tenv->jmps_processed++;\n\t\t\tif (opcode == BPF_CALL) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    (insn->src_reg != BPF_PSEUDO_KFUNC_CALL\n\t\t\t\t     && insn->off != 0) ||\n\t\t\t\t    (insn->src_reg != BPF_REG_0 &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_CALL &&\n\t\t\t\t     insn->src_reg != BPF_PSEUDO_KFUNC_CALL) ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_CALL uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_lock.ptr) {\n\t\t\t\t\tif ((insn->src_reg == BPF_REG_0 && insn->imm != BPF_FUNC_spin_unlock) ||\n\t\t\t\t\t    (insn->src_reg == BPF_PSEUDO_CALL) ||\n\t\t\t\t\t    (insn->src_reg == BPF_PSEUDO_KFUNC_CALL &&\n\t\t\t\t\t     (insn->off != 0 || !is_bpf_graph_api_kfunc(insn->imm)))) {\n\t\t\t\t\t\tverbose(env, \"function calls are not allowed while holding a lock\\n\");\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\t\t\terr = check_func_call(env, insn, &env->insn_idx);\n\t\t\t\telse if (insn->src_reg == BPF_PSEUDO_KFUNC_CALL)\n\t\t\t\t\terr = check_kfunc_call(env, insn, &env->insn_idx);\n\t\t\t\telse\n\t\t\t\t\terr = check_helper_call(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tmark_reg_scratched(env, BPF_REG_0);\n\t\t\t} else if (opcode == BPF_JA) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    (class == BPF_JMP && insn->imm != 0) ||\n\t\t\t\t    (class == BPF_JMP32 && insn->off != 0)) {\n\t\t\t\t\tverbose(env, \"BPF_JA uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (class == BPF_JMP)\n\t\t\t\t\tenv->insn_idx += insn->off + 1;\n\t\t\t\telse\n\t\t\t\t\tenv->insn_idx += insn->imm + 1;\n\t\t\t\tcontinue;\n\n\t\t\t} else if (opcode == BPF_EXIT) {\n\t\t\t\tif (BPF_SRC(insn->code) != BPF_K ||\n\t\t\t\t    insn->imm != 0 ||\n\t\t\t\t    insn->src_reg != BPF_REG_0 ||\n\t\t\t\t    insn->dst_reg != BPF_REG_0 ||\n\t\t\t\t    class == BPF_JMP32) {\n\t\t\t\t\tverbose(env, \"BPF_EXIT uses reserved fields\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_lock.ptr &&\n\t\t\t\t    !in_rbtree_lock_required_cb(env)) {\n\t\t\t\t\tverbose(env, \"bpf_spin_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (env->cur_state->active_rcu_lock &&\n\t\t\t\t    !in_rbtree_lock_required_cb(env)) {\n\t\t\t\t\tverbose(env, \"bpf_rcu_read_unlock is missing\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\terr = check_reference_leak(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tif (state->curframe) {\n\t\t\t\t\t \n\t\t\t\t\terr = prepare_func_exit(env, &env->insn_idx);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\terr = check_return_code(env);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\nprocess_bpf_exit:\n\t\t\t\tmark_verifier_state_scratched(env);\n\t\t\t\tupdate_branch_counts(env, env->cur_state);\n\t\t\t\terr = pop_stack(env, &prev_insn_idx,\n\t\t\t\t\t\t&env->insn_idx, pop_log);\n\t\t\t\tif (err < 0) {\n\t\t\t\t\tif (err != -ENOENT)\n\t\t\t\t\t\treturn err;\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tdo_print_state = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = check_cond_jmp_op(env, insn, &env->insn_idx);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t} else if (class == BPF_LD) {\n\t\t\tu8 mode = BPF_MODE(insn->code);\n\n\t\t\tif (mode == BPF_ABS || mode == BPF_IND) {\n\t\t\t\terr = check_ld_abs(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t} else if (mode == BPF_IMM) {\n\t\t\t\terr = check_ld_imm(env, insn);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\n\t\t\t\tenv->insn_idx++;\n\t\t\t\tsanitize_mark_insn_seen(env);\n\t\t\t} else {\n\t\t\t\tverbose(env, \"invalid BPF_LD mode\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tverbose(env, \"unknown insn class %d\\n\", class);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tenv->insn_idx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int find_btf_percpu_datasec(struct btf *btf)\n{\n\tconst struct btf_type *t;\n\tconst char *tname;\n\tint i, n;\n\n\t \n\tn = btf_nr_types(btf);\n\tif (btf_is_module(btf))\n\t\ti = btf_nr_types(btf_vmlinux);\n\telse\n\t\ti = 1;\n\n\tfor(; i < n; i++) {\n\t\tt = btf_type_by_id(btf, i);\n\t\tif (BTF_INFO_KIND(t->info) != BTF_KIND_DATASEC)\n\t\t\tcontinue;\n\n\t\ttname = btf_name_by_offset(btf, t->name_off);\n\t\tif (!strcmp(tname, \".data..percpu\"))\n\t\t\treturn i;\n\t}\n\n\treturn -ENOENT;\n}\n\n \nstatic int check_pseudo_btf_id(struct bpf_verifier_env *env,\n\t\t\t       struct bpf_insn *insn,\n\t\t\t       struct bpf_insn_aux_data *aux)\n{\n\tconst struct btf_var_secinfo *vsi;\n\tconst struct btf_type *datasec;\n\tstruct btf_mod_pair *btf_mod;\n\tconst struct btf_type *t;\n\tconst char *sym_name;\n\tbool percpu = false;\n\tu32 type, id = insn->imm;\n\tstruct btf *btf;\n\ts32 datasec_id;\n\tu64 addr;\n\tint i, btf_fd, err;\n\n\tbtf_fd = insn[1].imm;\n\tif (btf_fd) {\n\t\tbtf = btf_get_by_fd(btf_fd);\n\t\tif (IS_ERR(btf)) {\n\t\t\tverbose(env, \"invalid module BTF object FD specified.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (!btf_vmlinux) {\n\t\t\tverbose(env, \"kernel is missing BTF, make sure CONFIG_DEBUG_INFO_BTF=y is specified in Kconfig.\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbtf = btf_vmlinux;\n\t\tbtf_get(btf);\n\t}\n\n\tt = btf_type_by_id(btf, id);\n\tif (!t) {\n\t\tverbose(env, \"ldimm64 insn specifies invalid btf_id %d.\\n\", id);\n\t\terr = -ENOENT;\n\t\tgoto err_put;\n\t}\n\n\tif (!btf_type_is_var(t) && !btf_type_is_func(t)) {\n\t\tverbose(env, \"pseudo btf_id %d in ldimm64 isn't KIND_VAR or KIND_FUNC\\n\", id);\n\t\terr = -EINVAL;\n\t\tgoto err_put;\n\t}\n\n\tsym_name = btf_name_by_offset(btf, t->name_off);\n\taddr = kallsyms_lookup_name(sym_name);\n\tif (!addr) {\n\t\tverbose(env, \"ldimm64 failed to find the address for kernel symbol '%s'.\\n\",\n\t\t\tsym_name);\n\t\terr = -ENOENT;\n\t\tgoto err_put;\n\t}\n\tinsn[0].imm = (u32)addr;\n\tinsn[1].imm = addr >> 32;\n\n\tif (btf_type_is_func(t)) {\n\t\taux->btf_var.reg_type = PTR_TO_MEM | MEM_RDONLY;\n\t\taux->btf_var.mem_size = 0;\n\t\tgoto check_btf;\n\t}\n\n\tdatasec_id = find_btf_percpu_datasec(btf);\n\tif (datasec_id > 0) {\n\t\tdatasec = btf_type_by_id(btf, datasec_id);\n\t\tfor_each_vsi(i, datasec, vsi) {\n\t\t\tif (vsi->type == id) {\n\t\t\t\tpercpu = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\ttype = t->type;\n\tt = btf_type_skip_modifiers(btf, type, NULL);\n\tif (percpu) {\n\t\taux->btf_var.reg_type = PTR_TO_BTF_ID | MEM_PERCPU;\n\t\taux->btf_var.btf = btf;\n\t\taux->btf_var.btf_id = type;\n\t} else if (!btf_type_is_struct(t)) {\n\t\tconst struct btf_type *ret;\n\t\tconst char *tname;\n\t\tu32 tsize;\n\n\t\t \n\t\tret = btf_resolve_size(btf, t, &tsize);\n\t\tif (IS_ERR(ret)) {\n\t\t\ttname = btf_name_by_offset(btf, t->name_off);\n\t\t\tverbose(env, \"ldimm64 unable to resolve the size of type '%s': %ld\\n\",\n\t\t\t\ttname, PTR_ERR(ret));\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_put;\n\t\t}\n\t\taux->btf_var.reg_type = PTR_TO_MEM | MEM_RDONLY;\n\t\taux->btf_var.mem_size = tsize;\n\t} else {\n\t\taux->btf_var.reg_type = PTR_TO_BTF_ID;\n\t\taux->btf_var.btf = btf;\n\t\taux->btf_var.btf_id = type;\n\t}\ncheck_btf:\n\t \n\tfor (i = 0; i < env->used_btf_cnt; i++) {\n\t\tif (env->used_btfs[i].btf == btf) {\n\t\t\tbtf_put(btf);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tif (env->used_btf_cnt >= MAX_USED_BTFS) {\n\t\terr = -E2BIG;\n\t\tgoto err_put;\n\t}\n\n\tbtf_mod = &env->used_btfs[env->used_btf_cnt];\n\tbtf_mod->btf = btf;\n\tbtf_mod->module = NULL;\n\n\t \n\tif (btf_is_module(btf)) {\n\t\tbtf_mod->module = btf_try_get_module(btf);\n\t\tif (!btf_mod->module) {\n\t\t\terr = -ENXIO;\n\t\t\tgoto err_put;\n\t\t}\n\t}\n\n\tenv->used_btf_cnt++;\n\n\treturn 0;\nerr_put:\n\tbtf_put(btf);\n\treturn err;\n}\n\nstatic bool is_tracing_prog_type(enum bpf_prog_type type)\n{\n\tswitch (type) {\n\tcase BPF_PROG_TYPE_KPROBE:\n\tcase BPF_PROG_TYPE_TRACEPOINT:\n\tcase BPF_PROG_TYPE_PERF_EVENT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT:\n\tcase BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int check_map_prog_compatibility(struct bpf_verifier_env *env,\n\t\t\t\t\tstruct bpf_map *map,\n\t\t\t\t\tstruct bpf_prog *prog)\n\n{\n\tenum bpf_prog_type prog_type = resolve_prog_type(prog);\n\n\tif (btf_record_has_field(map->record, BPF_LIST_HEAD) ||\n\t    btf_record_has_field(map->record, BPF_RB_ROOT)) {\n\t\tif (is_tracing_prog_type(prog_type)) {\n\t\t\tverbose(env, \"tracing progs cannot use bpf_{list_head,rb_root} yet\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (btf_record_has_field(map->record, BPF_SPIN_LOCK)) {\n\t\tif (prog_type == BPF_PROG_TYPE_SOCKET_FILTER) {\n\t\t\tverbose(env, \"socket filter progs cannot use bpf_spin_lock yet\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_tracing_prog_type(prog_type)) {\n\t\t\tverbose(env, \"tracing progs cannot use bpf_spin_lock yet\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (btf_record_has_field(map->record, BPF_TIMER)) {\n\t\tif (is_tracing_prog_type(prog_type)) {\n\t\t\tverbose(env, \"tracing progs cannot use bpf_timer yet\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif ((bpf_prog_is_offloaded(prog->aux) || bpf_map_is_offloaded(map)) &&\n\t    !bpf_offload_prog_map_match(prog, map)) {\n\t\tverbose(env, \"offload device mismatch between prog and map\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (map->map_type == BPF_MAP_TYPE_STRUCT_OPS) {\n\t\tverbose(env, \"bpf_struct_ops map cannot be used in prog\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (prog->aux->sleepable)\n\t\tswitch (map->map_type) {\n\t\tcase BPF_MAP_TYPE_HASH:\n\t\tcase BPF_MAP_TYPE_LRU_HASH:\n\t\tcase BPF_MAP_TYPE_ARRAY:\n\t\tcase BPF_MAP_TYPE_PERCPU_HASH:\n\t\tcase BPF_MAP_TYPE_PERCPU_ARRAY:\n\t\tcase BPF_MAP_TYPE_LRU_PERCPU_HASH:\n\t\tcase BPF_MAP_TYPE_ARRAY_OF_MAPS:\n\t\tcase BPF_MAP_TYPE_HASH_OF_MAPS:\n\t\tcase BPF_MAP_TYPE_RINGBUF:\n\t\tcase BPF_MAP_TYPE_USER_RINGBUF:\n\t\tcase BPF_MAP_TYPE_INODE_STORAGE:\n\t\tcase BPF_MAP_TYPE_SK_STORAGE:\n\t\tcase BPF_MAP_TYPE_TASK_STORAGE:\n\t\tcase BPF_MAP_TYPE_CGRP_STORAGE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tverbose(env,\n\t\t\t\t\"Sleepable programs can only use array, hash, ringbuf and local storage maps\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\treturn 0;\n}\n\nstatic bool bpf_map_is_cgroup_storage(struct bpf_map *map)\n{\n\treturn (map->map_type == BPF_MAP_TYPE_CGROUP_STORAGE ||\n\t\tmap->map_type == BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE);\n}\n\n \nstatic int resolve_pseudo_ldimm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, j, err;\n\n\terr = bpf_prog_calc_tag(env->prog);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t    ((BPF_MODE(insn->code) != BPF_MEM && BPF_MODE(insn->code) != BPF_MEMSX) ||\n\t\t    insn->imm != 0)) {\n\t\t\tverbose(env, \"BPF_LDX uses reserved fields\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\n\t\t\tstruct bpf_insn_aux_data *aux;\n\t\t\tstruct bpf_map *map;\n\t\t\tstruct fd f;\n\t\t\tu64 addr;\n\t\t\tu32 fd;\n\n\t\t\tif (i == insn_cnt - 1 || insn[1].code != 0 ||\n\t\t\t    insn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\n\t\t\t    insn[1].off != 0) {\n\t\t\t\tverbose(env, \"invalid bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (insn[0].src_reg == 0)\n\t\t\t\t \n\t\t\t\tgoto next_insn;\n\n\t\t\tif (insn[0].src_reg == BPF_PSEUDO_BTF_ID) {\n\t\t\t\taux = &env->insn_aux_data[i];\n\t\t\t\terr = check_pseudo_btf_id(env, insn, aux);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tgoto next_insn;\n\t\t\t}\n\n\t\t\tif (insn[0].src_reg == BPF_PSEUDO_FUNC) {\n\t\t\t\taux = &env->insn_aux_data[i];\n\t\t\t\taux->ptr_type = PTR_TO_FUNC;\n\t\t\t\tgoto next_insn;\n\t\t\t}\n\n\t\t\t \n\t\t\tswitch (insn[0].src_reg) {\n\t\t\tcase BPF_PSEUDO_MAP_VALUE:\n\t\t\tcase BPF_PSEUDO_MAP_IDX_VALUE:\n\t\t\t\tbreak;\n\t\t\tcase BPF_PSEUDO_MAP_FD:\n\t\t\tcase BPF_PSEUDO_MAP_IDX:\n\t\t\t\tif (insn[1].imm == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tfallthrough;\n\t\t\tdefault:\n\t\t\t\tverbose(env, \"unrecognized bpf_ld_imm64 insn\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tswitch (insn[0].src_reg) {\n\t\t\tcase BPF_PSEUDO_MAP_IDX_VALUE:\n\t\t\tcase BPF_PSEUDO_MAP_IDX:\n\t\t\t\tif (bpfptr_is_null(env->fd_array)) {\n\t\t\t\t\tverbose(env, \"fd_idx without fd_array is invalid\\n\");\n\t\t\t\t\treturn -EPROTO;\n\t\t\t\t}\n\t\t\t\tif (copy_from_bpfptr_offset(&fd, env->fd_array,\n\t\t\t\t\t\t\t    insn[0].imm * sizeof(fd),\n\t\t\t\t\t\t\t    sizeof(fd)))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tfd = insn[0].imm;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tf = fdget(fd);\n\t\t\tmap = __bpf_map_get(f);\n\t\t\tif (IS_ERR(map)) {\n\t\t\t\tverbose(env, \"fd %d is not pointing to valid bpf_map\\n\",\n\t\t\t\t\tinsn[0].imm);\n\t\t\t\treturn PTR_ERR(map);\n\t\t\t}\n\n\t\t\terr = check_map_prog_compatibility(env, map, env->prog);\n\t\t\tif (err) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\taux = &env->insn_aux_data[i];\n\t\t\tif (insn[0].src_reg == BPF_PSEUDO_MAP_FD ||\n\t\t\t    insn[0].src_reg == BPF_PSEUDO_MAP_IDX) {\n\t\t\t\taddr = (unsigned long)map;\n\t\t\t} else {\n\t\t\t\tu32 off = insn[1].imm;\n\n\t\t\t\tif (off >= BPF_MAX_VAR_OFF) {\n\t\t\t\t\tverbose(env, \"direct value offset of %u is not allowed\\n\", off);\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tif (!map->ops->map_direct_value_addr) {\n\t\t\t\t\tverbose(env, \"no direct value access support for this map type\\n\");\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\terr = map->ops->map_direct_value_addr(map, &addr, off);\n\t\t\t\tif (err) {\n\t\t\t\t\tverbose(env, \"invalid access to map value pointer, value_size=%u off=%u\\n\",\n\t\t\t\t\t\tmap->value_size, off);\n\t\t\t\t\tfdput(f);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\n\t\t\t\taux->map_off = off;\n\t\t\t\taddr += off;\n\t\t\t}\n\n\t\t\tinsn[0].imm = (u32)addr;\n\t\t\tinsn[1].imm = addr >> 32;\n\n\t\t\t \n\t\t\tfor (j = 0; j < env->used_map_cnt; j++) {\n\t\t\t\tif (env->used_maps[j] == map) {\n\t\t\t\t\taux->map_index = j;\n\t\t\t\t\tfdput(f);\n\t\t\t\t\tgoto next_insn;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (env->used_map_cnt >= MAX_USED_MAPS) {\n\t\t\t\tfdput(f);\n\t\t\t\treturn -E2BIG;\n\t\t\t}\n\n\t\t\t \n\t\t\tbpf_map_inc(map);\n\n\t\t\taux->map_index = env->used_map_cnt;\n\t\t\tenv->used_maps[env->used_map_cnt++] = map;\n\n\t\t\tif (bpf_map_is_cgroup_storage(map) &&\n\t\t\t    bpf_cgroup_storage_assign(env->prog->aux, map)) {\n\t\t\t\tverbose(env, \"only one cgroup storage of each type is allowed\\n\");\n\t\t\t\tfdput(f);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\n\t\t\tfdput(f);\nnext_insn:\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!bpf_opcode_in_insntable(insn->code)) {\n\t\t\tverbose(env, \"unknown opcode %02x\\n\", insn->code);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\treturn 0;\n}\n\n \nstatic void release_maps(struct bpf_verifier_env *env)\n{\n\t__bpf_free_used_maps(env->prog->aux, env->used_maps,\n\t\t\t     env->used_map_cnt);\n}\n\n \nstatic void release_btfs(struct bpf_verifier_env *env)\n{\n\t__bpf_free_used_btfs(env->prog->aux, env->used_btfs,\n\t\t\t     env->used_btf_cnt);\n}\n\n \nstatic void convert_pseudo_ld_imm64(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (insn->code != (BPF_LD | BPF_IMM | BPF_DW))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_FUNC)\n\t\t\tcontinue;\n\t\tinsn->src_reg = 0;\n\t}\n}\n\n \nstatic void adjust_insn_aux_data(struct bpf_verifier_env *env,\n\t\t\t\t struct bpf_insn_aux_data *new_data,\n\t\t\t\t struct bpf_prog *new_prog, u32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *old_data = env->insn_aux_data;\n\tstruct bpf_insn *insn = new_prog->insnsi;\n\tu32 old_seen = old_data[off].seen;\n\tu32 prog_len;\n\tint i;\n\n\t \n\told_data[off].zext_dst = insn_has_def32(env, insn + off + cnt - 1);\n\n\tif (cnt == 1)\n\t\treturn;\n\tprog_len = new_prog->len;\n\n\tmemcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);\n\tmemcpy(new_data + off + cnt - 1, old_data + off,\n\t       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));\n\tfor (i = off; i < off + cnt - 1; i++) {\n\t\t \n\t\tnew_data[i].seen = old_seen;\n\t\tnew_data[i].zext_dst = insn_has_def32(env, insn + i);\n\t}\n\tenv->insn_aux_data = new_data;\n\tvfree(old_data);\n}\n\nstatic void adjust_subprog_starts(struct bpf_verifier_env *env, u32 off, u32 len)\n{\n\tint i;\n\n\tif (len == 1)\n\t\treturn;\n\t \n\tfor (i = 0; i <= env->subprog_cnt; i++) {\n\t\tif (env->subprog_info[i].start <= off)\n\t\t\tcontinue;\n\t\tenv->subprog_info[i].start += len - 1;\n\t}\n}\n\nstatic void adjust_poke_descs(struct bpf_prog *prog, u32 off, u32 len)\n{\n\tstruct bpf_jit_poke_descriptor *tab = prog->aux->poke_tab;\n\tint i, sz = prog->aux->size_poke_tab;\n\tstruct bpf_jit_poke_descriptor *desc;\n\n\tfor (i = 0; i < sz; i++) {\n\t\tdesc = &tab[i];\n\t\tif (desc->insn_idx <= off)\n\t\t\tcontinue;\n\t\tdesc->insn_idx += len - 1;\n\t}\n}\n\nstatic struct bpf_prog *bpf_patch_insn_data(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t\t    const struct bpf_insn *patch, u32 len)\n{\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_insn_aux_data *new_data = NULL;\n\n\tif (len > 1) {\n\t\tnew_data = vzalloc(array_size(env->prog->len + len - 1,\n\t\t\t\t\t      sizeof(struct bpf_insn_aux_data)));\n\t\tif (!new_data)\n\t\t\treturn NULL;\n\t}\n\n\tnew_prog = bpf_patch_insn_single(env->prog, off, patch, len);\n\tif (IS_ERR(new_prog)) {\n\t\tif (PTR_ERR(new_prog) == -ERANGE)\n\t\t\tverbose(env,\n\t\t\t\t\"insn %d cannot be patched due to 16-bit range\\n\",\n\t\t\t\tenv->insn_aux_data[off].orig_idx);\n\t\tvfree(new_data);\n\t\treturn NULL;\n\t}\n\tadjust_insn_aux_data(env, new_data, new_prog, off, len);\n\tadjust_subprog_starts(env, off, len);\n\tadjust_poke_descs(new_prog, off, len);\n\treturn new_prog;\n}\n\nstatic int adjust_subprog_starts_after_remove(struct bpf_verifier_env *env,\n\t\t\t\t\t      u32 off, u32 cnt)\n{\n\tint i, j;\n\n\t \n\tfor (i = 0; i < env->subprog_cnt; i++)\n\t\tif (env->subprog_info[i].start >= off)\n\t\t\tbreak;\n\t \n\tfor (j = i; j < env->subprog_cnt; j++)\n\t\tif (env->subprog_info[j].start >= off + cnt)\n\t\t\tbreak;\n\t \n\tif (env->subprog_info[j].start != off + cnt)\n\t\tj--;\n\n\tif (j > i) {\n\t\tstruct bpf_prog_aux *aux = env->prog->aux;\n\t\tint move;\n\n\t\t \n\t\tmove = env->subprog_cnt + 1 - j;\n\n\t\tmemmove(env->subprog_info + i,\n\t\t\tenv->subprog_info + j,\n\t\t\tsizeof(*env->subprog_info) * move);\n\t\tenv->subprog_cnt -= j - i;\n\n\t\t \n\t\tif (aux->func_info) {\n\t\t\tmove = aux->func_info_cnt - j;\n\n\t\t\tmemmove(aux->func_info + i,\n\t\t\t\taux->func_info + j,\n\t\t\t\tsizeof(*aux->func_info) * move);\n\t\t\taux->func_info_cnt -= j - i;\n\t\t\t \n\t\t}\n\t} else {\n\t\t \n\t\tif (env->subprog_info[i].start == off)\n\t\t\ti++;\n\t}\n\n\t \n\tfor (; i <= env->subprog_cnt; i++)\n\t\tenv->subprog_info[i].start -= cnt;\n\n\treturn 0;\n}\n\nstatic int bpf_adj_linfo_after_remove(struct bpf_verifier_env *env, u32 off,\n\t\t\t\t      u32 cnt)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tu32 i, l_off, l_cnt, nr_linfo;\n\tstruct bpf_line_info *linfo;\n\n\tnr_linfo = prog->aux->nr_linfo;\n\tif (!nr_linfo)\n\t\treturn 0;\n\n\tlinfo = prog->aux->linfo;\n\n\t \n\tfor (i = 0; i < nr_linfo; i++)\n\t\tif (linfo[i].insn_off >= off)\n\t\t\tbreak;\n\n\tl_off = i;\n\tl_cnt = 0;\n\tfor (; i < nr_linfo; i++)\n\t\tif (linfo[i].insn_off < off + cnt)\n\t\t\tl_cnt++;\n\t\telse\n\t\t\tbreak;\n\n\t \n\tif (prog->len != off && l_cnt &&\n\t    (i == nr_linfo || linfo[i].insn_off != off + cnt)) {\n\t\tl_cnt--;\n\t\tlinfo[--i].insn_off = off + cnt;\n\t}\n\n\t \n\tif (l_cnt) {\n\t\tmemmove(linfo + l_off, linfo + i,\n\t\t\tsizeof(*linfo) * (nr_linfo - i));\n\n\t\tprog->aux->nr_linfo -= l_cnt;\n\t\tnr_linfo = prog->aux->nr_linfo;\n\t}\n\n\t \n\tfor (i = l_off; i < nr_linfo; i++)\n\t\tlinfo[i].insn_off -= cnt;\n\n\t \n\tfor (i = 0; i <= env->subprog_cnt; i++)\n\t\tif (env->subprog_info[i].linfo_idx > l_off) {\n\t\t\t \n\t\t\tif (env->subprog_info[i].linfo_idx >= l_off + l_cnt)\n\t\t\t\tenv->subprog_info[i].linfo_idx -= l_cnt;\n\t\t\telse\n\t\t\t\tenv->subprog_info[i].linfo_idx = l_off;\n\t\t}\n\n\treturn 0;\n}\n\nstatic int verifier_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tunsigned int orig_prog_len = env->prog->len;\n\tint err;\n\n\tif (bpf_prog_is_offloaded(env->prog->aux))\n\t\tbpf_prog_offload_remove_insns(env, off, cnt);\n\n\terr = bpf_remove_insns(env->prog, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\terr = adjust_subprog_starts_after_remove(env, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\terr = bpf_adj_linfo_after_remove(env, off, cnt);\n\tif (err)\n\t\treturn err;\n\n\tmemmove(aux_data + off,\taux_data + off + cnt,\n\t\tsizeof(*aux_data) * (orig_prog_len - off - cnt));\n\n\treturn 0;\n}\n\n \nstatic void sanitize_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn trap = BPF_JMP_IMM(BPF_JA, 0, 0, -1);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (aux_data[i].seen)\n\t\t\tcontinue;\n\t\tmemcpy(insn + i, &trap, sizeof(trap));\n\t\taux_data[i].zext_dst = false;\n\t}\n}\n\nstatic bool insn_is_cond_jump(u8 code)\n{\n\tu8 op;\n\n\top = BPF_OP(code);\n\tif (BPF_CLASS(code) == BPF_JMP32)\n\t\treturn op != BPF_JA;\n\n\tif (BPF_CLASS(code) != BPF_JMP)\n\t\treturn false;\n\n\treturn op != BPF_JA && op != BPF_EXIT && op != BPF_CALL;\n}\n\nstatic void opt_hard_wire_dead_code_branches(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct bpf_insn ja = BPF_JMP_IMM(BPF_JA, 0, 0, 0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tconst int insn_cnt = env->prog->len;\n\tint i;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tif (!insn_is_cond_jump(insn->code))\n\t\t\tcontinue;\n\n\t\tif (!aux_data[i + 1].seen)\n\t\t\tja.off = insn->off;\n\t\telse if (!aux_data[i + 1 + insn->off].seen)\n\t\t\tja.off = 0;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (bpf_prog_is_offloaded(env->prog->aux))\n\t\t\tbpf_prog_offload_replace_insn(env, i, &ja);\n\n\t\tmemcpy(insn, &ja, sizeof(ja));\n\t}\n}\n\nstatic int opt_remove_dead_code(struct bpf_verifier_env *env)\n{\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tint insn_cnt = env->prog->len;\n\tint i, err;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tint j;\n\n\t\tj = 0;\n\t\twhile (i + j < insn_cnt && !aux_data[i + j].seen)\n\t\t\tj++;\n\t\tif (!j)\n\t\t\tcontinue;\n\n\t\terr = verifier_remove_insns(env, i, j);\n\t\tif (err)\n\t\t\treturn err;\n\t\tinsn_cnt = env->prog->len;\n\t}\n\n\treturn 0;\n}\n\nstatic int opt_remove_nops(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_insn ja = BPF_JMP_IMM(BPF_JA, 0, 0, 0);\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tint i, err;\n\n\tfor (i = 0; i < insn_cnt; i++) {\n\t\tif (memcmp(&insn[i], &ja, sizeof(ja)))\n\t\t\tcontinue;\n\n\t\terr = verifier_remove_insns(env, i, 1);\n\t\tif (err)\n\t\t\treturn err;\n\t\tinsn_cnt--;\n\t\ti--;\n\t}\n\n\treturn 0;\n}\n\nstatic int opt_subreg_zext_lo32_rnd_hi32(struct bpf_verifier_env *env,\n\t\t\t\t\t const union bpf_attr *attr)\n{\n\tstruct bpf_insn *patch, zext_patch[2], rnd_hi32_patch[4];\n\tstruct bpf_insn_aux_data *aux = env->insn_aux_data;\n\tint i, patch_len, delta = 0, len = env->prog->len;\n\tstruct bpf_insn *insns = env->prog->insnsi;\n\tstruct bpf_prog *new_prog;\n\tbool rnd_hi32;\n\n\trnd_hi32 = attr->prog_flags & BPF_F_TEST_RND_HI32;\n\tzext_patch[1] = BPF_ZEXT_REG(0);\n\trnd_hi32_patch[1] = BPF_ALU64_IMM(BPF_MOV, BPF_REG_AX, 0);\n\trnd_hi32_patch[2] = BPF_ALU64_IMM(BPF_LSH, BPF_REG_AX, 32);\n\trnd_hi32_patch[3] = BPF_ALU64_REG(BPF_OR, 0, BPF_REG_AX);\n\tfor (i = 0; i < len; i++) {\n\t\tint adj_idx = i + delta;\n\t\tstruct bpf_insn insn;\n\t\tint load_reg;\n\n\t\tinsn = insns[adj_idx];\n\t\tload_reg = insn_def_regno(&insn);\n\t\tif (!aux[adj_idx].zext_dst) {\n\t\t\tu8 code, class;\n\t\t\tu32 imm_rnd;\n\n\t\t\tif (!rnd_hi32)\n\t\t\t\tcontinue;\n\n\t\t\tcode = insn.code;\n\t\t\tclass = BPF_CLASS(code);\n\t\t\tif (load_reg == -1)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (is_reg64(env, &insn, load_reg, NULL, DST_OP)) {\n\t\t\t\tif (class == BPF_LD &&\n\t\t\t\t    BPF_MODE(code) == BPF_IMM)\n\t\t\t\t\ti++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (class == BPF_LDX &&\n\t\t\t    aux[adj_idx].ptr_type == PTR_TO_CTX)\n\t\t\t\tcontinue;\n\n\t\t\timm_rnd = get_random_u32();\n\t\t\trnd_hi32_patch[0] = insn;\n\t\t\trnd_hi32_patch[1].imm = imm_rnd;\n\t\t\trnd_hi32_patch[3].dst_reg = load_reg;\n\t\t\tpatch = rnd_hi32_patch;\n\t\t\tpatch_len = 4;\n\t\t\tgoto apply_patch_buffer;\n\t\t}\n\n\t\t \n\t\tif (!bpf_jit_needs_zext() && !is_cmpxchg_insn(&insn))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (bpf_pseudo_kfunc_call(&insn))\n\t\t\tcontinue;\n\n\t\tif (WARN_ON(load_reg == -1)) {\n\t\t\tverbose(env, \"verifier bug. zext_dst is set, but no reg is defined\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tzext_patch[0] = insn;\n\t\tzext_patch[1].dst_reg = load_reg;\n\t\tzext_patch[1].src_reg = load_reg;\n\t\tpatch = zext_patch;\n\t\tpatch_len = 2;\napply_patch_buffer:\n\t\tnew_prog = bpf_patch_insn_data(env, adj_idx, patch, patch_len);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\t\tenv->prog = new_prog;\n\t\tinsns = new_prog->insnsi;\n\t\taux = env->insn_aux_data;\n\t\tdelta += patch_len - 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int convert_ctx_accesses(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_verifier_ops *ops = env->ops;\n\tint i, cnt, size, ctx_field_size, delta = 0;\n\tconst int insn_cnt = env->prog->len;\n\tstruct bpf_insn insn_buf[16], *insn;\n\tu32 target_size, size_default, off;\n\tstruct bpf_prog *new_prog;\n\tenum bpf_access_type type;\n\tbool is_narrower_load;\n\n\tif (ops->gen_prologue || env->seen_direct_write) {\n\t\tif (!ops->gen_prologue) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tcnt = ops->gen_prologue(insn_buf, env->seen_direct_write,\n\t\t\t\t\tenv->prog);\n\t\tif (cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t} else if (cnt) {\n\t\t\tnew_prog = bpf_patch_insn_data(env, 0, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = new_prog;\n\t\t\tdelta += cnt - 1;\n\t\t}\n\t}\n\n\tif (bpf_prog_is_offloaded(env->prog->aux))\n\t\treturn 0;\n\n\tinsn = env->prog->insnsi + delta;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tbpf_convert_ctx_access_t convert_ctx_access;\n\t\tu8 mode;\n\n\t\tif (insn->code == (BPF_LDX | BPF_MEM | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_W) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEM | BPF_DW) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEMSX | BPF_B) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEMSX | BPF_H) ||\n\t\t    insn->code == (BPF_LDX | BPF_MEMSX | BPF_W)) {\n\t\t\ttype = BPF_READ;\n\t\t} else if (insn->code == (BPF_STX | BPF_MEM | BPF_B) ||\n\t\t\t   insn->code == (BPF_STX | BPF_MEM | BPF_H) ||\n\t\t\t   insn->code == (BPF_STX | BPF_MEM | BPF_W) ||\n\t\t\t   insn->code == (BPF_STX | BPF_MEM | BPF_DW) ||\n\t\t\t   insn->code == (BPF_ST | BPF_MEM | BPF_B) ||\n\t\t\t   insn->code == (BPF_ST | BPF_MEM | BPF_H) ||\n\t\t\t   insn->code == (BPF_ST | BPF_MEM | BPF_W) ||\n\t\t\t   insn->code == (BPF_ST | BPF_MEM | BPF_DW)) {\n\t\t\ttype = BPF_WRITE;\n\t\t} else {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (type == BPF_WRITE &&\n\t\t    env->insn_aux_data[i + delta].sanitize_stack_spill) {\n\t\t\tstruct bpf_insn patch[] = {\n\t\t\t\t*insn,\n\t\t\t\tBPF_ST_NOSPEC(),\n\t\t\t};\n\n\t\t\tcnt = ARRAY_SIZE(patch);\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patch, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tswitch ((int)env->insn_aux_data[i + delta].ptr_type) {\n\t\tcase PTR_TO_CTX:\n\t\t\tif (!ops->convert_ctx_access)\n\t\t\t\tcontinue;\n\t\t\tconvert_ctx_access = ops->convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_SOCKET:\n\t\tcase PTR_TO_SOCK_COMMON:\n\t\t\tconvert_ctx_access = bpf_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_TCP_SOCK:\n\t\t\tconvert_ctx_access = bpf_tcp_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_XDP_SOCK:\n\t\t\tconvert_ctx_access = bpf_xdp_sock_convert_ctx_access;\n\t\t\tbreak;\n\t\tcase PTR_TO_BTF_ID:\n\t\tcase PTR_TO_BTF_ID | PTR_UNTRUSTED:\n\t\t \n\t\tcase PTR_TO_BTF_ID | MEM_ALLOC | PTR_UNTRUSTED:\n\t\t\tif (type == BPF_READ) {\n\t\t\t\tif (BPF_MODE(insn->code) == BPF_MEM)\n\t\t\t\t\tinsn->code = BPF_LDX | BPF_PROBE_MEM |\n\t\t\t\t\t\t     BPF_SIZE((insn)->code);\n\t\t\t\telse\n\t\t\t\t\tinsn->code = BPF_LDX | BPF_PROBE_MEMSX |\n\t\t\t\t\t\t     BPF_SIZE((insn)->code);\n\t\t\t\tenv->prog->aux->num_exentries++;\n\t\t\t}\n\t\t\tcontinue;\n\t\tdefault:\n\t\t\tcontinue;\n\t\t}\n\n\t\tctx_field_size = env->insn_aux_data[i + delta].ctx_field_size;\n\t\tsize = BPF_LDST_BYTES(insn);\n\t\tmode = BPF_MODE(insn->code);\n\n\t\t \n\t\tis_narrower_load = size < ctx_field_size;\n\t\tsize_default = bpf_ctx_off_adjust_machine(ctx_field_size);\n\t\toff = insn->off;\n\t\tif (is_narrower_load) {\n\t\t\tu8 size_code;\n\n\t\t\tif (type == BPF_WRITE) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx access misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tsize_code = BPF_H;\n\t\t\tif (ctx_field_size == 4)\n\t\t\t\tsize_code = BPF_W;\n\t\t\telse if (ctx_field_size == 8)\n\t\t\t\tsize_code = BPF_DW;\n\n\t\t\tinsn->off = off & ~(size_default - 1);\n\t\t\tinsn->code = BPF_LDX | BPF_MEM | size_code;\n\t\t}\n\n\t\ttarget_size = 0;\n\t\tcnt = convert_ctx_access(type, insn, insn_buf, env->prog,\n\t\t\t\t\t &target_size);\n\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf) ||\n\t\t    (ctx_field_size && !target_size)) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (is_narrower_load && size < target_size) {\n\t\t\tu8 shift = bpf_ctx_narrow_access_offset(\n\t\t\t\toff, size, size_default) * 8;\n\t\t\tif (shift && cnt + 1 >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier narrow ctx load misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (ctx_field_size <= 4) {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1 << size * 8) - 1);\n\t\t\t} else {\n\t\t\t\tif (shift)\n\t\t\t\t\tinsn_buf[cnt++] = BPF_ALU64_IMM(BPF_RSH,\n\t\t\t\t\t\t\t\t\tinsn->dst_reg,\n\t\t\t\t\t\t\t\t\tshift);\n\t\t\t\tinsn_buf[cnt++] = BPF_ALU32_IMM(BPF_AND, insn->dst_reg,\n\t\t\t\t\t\t\t\t(1ULL << size * 8) - 1);\n\t\t\t}\n\t\t}\n\t\tif (mode == BPF_MEMSX)\n\t\t\tinsn_buf[cnt++] = BPF_RAW_INSN(BPF_ALU64 | BPF_MOV | BPF_X,\n\t\t\t\t\t\t       insn->dst_reg, insn->dst_reg,\n\t\t\t\t\t\t       size * 8, 0);\n\n\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\tif (!new_prog)\n\t\t\treturn -ENOMEM;\n\n\t\tdelta += cnt - 1;\n\n\t\t \n\t\tenv->prog = new_prog;\n\t\tinsn      = new_prog->insnsi + i + delta;\n\t}\n\n\treturn 0;\n}\n\nstatic int jit_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog, **func, *tmp;\n\tint i, j, subprog_start, subprog_end = 0, len, subprog;\n\tstruct bpf_map *map_ptr;\n\tstruct bpf_insn *insn;\n\tvoid *old_bpf_func;\n\tint err, num_exentries;\n\n\tif (env->subprog_cnt <= 1)\n\t\treturn 0;\n\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (!bpf_pseudo_func(insn) && !bpf_pseudo_call(insn))\n\t\t\tcontinue;\n\n\t\t \n\t\tsubprog = find_subprog(env, i + insn->imm + 1);\n\t\tif (subprog < 0) {\n\t\t\tWARN_ONCE(1, \"verifier bug. No program starts at insn %d\\n\",\n\t\t\t\t  i + insn->imm + 1);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\t \n\t\tinsn->off = subprog;\n\t\t \n\t\tenv->insn_aux_data[i].call_imm = insn->imm;\n\t\t \n\t\tinsn->imm = 1;\n\t\tif (bpf_pseudo_func(insn))\n\t\t\t \n\t\t\tinsn[1].imm = 1;\n\t}\n\n\terr = bpf_prog_alloc_jited_linfo(prog);\n\tif (err)\n\t\tgoto out_undo_insn;\n\n\terr = -ENOMEM;\n\tfunc = kcalloc(env->subprog_cnt, sizeof(prog), GFP_KERNEL);\n\tif (!func)\n\t\tgoto out_undo_insn;\n\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tsubprog_start = subprog_end;\n\t\tsubprog_end = env->subprog_info[i + 1].start;\n\n\t\tlen = subprog_end - subprog_start;\n\t\t \n\t\tfunc[i] = bpf_prog_alloc_no_stats(bpf_prog_size(len), GFP_USER);\n\t\tif (!func[i])\n\t\t\tgoto out_free;\n\t\tmemcpy(func[i]->insnsi, &prog->insnsi[subprog_start],\n\t\t       len * sizeof(struct bpf_insn));\n\t\tfunc[i]->type = prog->type;\n\t\tfunc[i]->len = len;\n\t\tif (bpf_prog_calc_tag(func[i]))\n\t\t\tgoto out_free;\n\t\tfunc[i]->is_func = 1;\n\t\tfunc[i]->aux->func_idx = i;\n\t\t \n\t\tfunc[i]->aux->btf = prog->aux->btf;\n\t\tfunc[i]->aux->func_info = prog->aux->func_info;\n\t\tfunc[i]->aux->func_info_cnt = prog->aux->func_info_cnt;\n\t\tfunc[i]->aux->poke_tab = prog->aux->poke_tab;\n\t\tfunc[i]->aux->size_poke_tab = prog->aux->size_poke_tab;\n\n\t\tfor (j = 0; j < prog->aux->size_poke_tab; j++) {\n\t\t\tstruct bpf_jit_poke_descriptor *poke;\n\n\t\t\tpoke = &prog->aux->poke_tab[j];\n\t\t\tif (poke->insn_idx < subprog_end &&\n\t\t\t    poke->insn_idx >= subprog_start)\n\t\t\t\tpoke->aux = func[i]->aux;\n\t\t}\n\n\t\tfunc[i]->aux->name[0] = 'F';\n\t\tfunc[i]->aux->stack_depth = env->subprog_info[i].stack_depth;\n\t\tfunc[i]->jit_requested = 1;\n\t\tfunc[i]->blinding_requested = prog->blinding_requested;\n\t\tfunc[i]->aux->kfunc_tab = prog->aux->kfunc_tab;\n\t\tfunc[i]->aux->kfunc_btf_tab = prog->aux->kfunc_btf_tab;\n\t\tfunc[i]->aux->linfo = prog->aux->linfo;\n\t\tfunc[i]->aux->nr_linfo = prog->aux->nr_linfo;\n\t\tfunc[i]->aux->jited_linfo = prog->aux->jited_linfo;\n\t\tfunc[i]->aux->linfo_idx = env->subprog_info[i].linfo_idx;\n\t\tnum_exentries = 0;\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (BPF_CLASS(insn->code) == BPF_LDX &&\n\t\t\t    (BPF_MODE(insn->code) == BPF_PROBE_MEM ||\n\t\t\t     BPF_MODE(insn->code) == BPF_PROBE_MEMSX))\n\t\t\t\tnum_exentries++;\n\t\t}\n\t\tfunc[i]->aux->num_exentries = num_exentries;\n\t\tfunc[i]->aux->tail_call_reachable = env->subprog_info[i].tail_call_reachable;\n\t\tfunc[i] = bpf_int_jit_compile(func[i]);\n\t\tif (!func[i]->jited) {\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t \n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tinsn = func[i]->insnsi;\n\t\tfor (j = 0; j < func[i]->len; j++, insn++) {\n\t\t\tif (bpf_pseudo_func(insn)) {\n\t\t\t\tsubprog = insn->off;\n\t\t\t\tinsn[0].imm = (u32)(long)func[subprog]->bpf_func;\n\t\t\t\tinsn[1].imm = ((u64)(long)func[subprog]->bpf_func) >> 32;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!bpf_pseudo_call(insn))\n\t\t\t\tcontinue;\n\t\t\tsubprog = insn->off;\n\t\t\tinsn->imm = BPF_CALL_IMM(func[subprog]->bpf_func);\n\t\t}\n\n\t\t \n\t\tfunc[i]->aux->func = func;\n\t\tfunc[i]->aux->func_cnt = env->subprog_cnt;\n\t}\n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\told_bpf_func = func[i]->bpf_func;\n\t\ttmp = bpf_int_jit_compile(func[i]);\n\t\tif (tmp != func[i] || func[i]->bpf_func != old_bpf_func) {\n\t\t\tverbose(env, \"JIT doesn't support bpf-to-bpf calls\\n\");\n\t\t\terr = -ENOTSUPP;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\t \n\tfor (i = 1; i < env->subprog_cnt; i++) {\n\t\tbpf_prog_lock_ro(func[i]);\n\t\tbpf_prog_kallsyms_add(func[i]);\n\t}\n\n\t \n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (bpf_pseudo_func(insn)) {\n\t\t\tinsn[0].imm = env->insn_aux_data[i].call_imm;\n\t\t\tinsn[1].imm = insn->off;\n\t\t\tinsn->off = 0;\n\t\t\tcontinue;\n\t\t}\n\t\tif (!bpf_pseudo_call(insn))\n\t\t\tcontinue;\n\t\tinsn->off = env->insn_aux_data[i].call_imm;\n\t\tsubprog = find_subprog(env, i + insn->off + 1);\n\t\tinsn->imm = subprog;\n\t}\n\n\tprog->jited = 1;\n\tprog->bpf_func = func[0]->bpf_func;\n\tprog->jited_len = func[0]->jited_len;\n\tprog->aux->extable = func[0]->aux->extable;\n\tprog->aux->num_exentries = func[0]->aux->num_exentries;\n\tprog->aux->func = func;\n\tprog->aux->func_cnt = env->subprog_cnt;\n\tbpf_prog_jit_attempt_done(prog);\n\treturn 0;\nout_free:\n\t \n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tmap_ptr->ops->map_poke_untrack(map_ptr, prog->aux);\n\t}\n\t \n\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\tif (!func[i])\n\t\t\tcontinue;\n\t\tfunc[i]->aux->poke_tab = NULL;\n\t\tbpf_jit_free(func[i]);\n\t}\n\tkfree(func);\nout_undo_insn:\n\t \n\tprog->jit_requested = 0;\n\tprog->blinding_requested = 0;\n\tfor (i = 0, insn = prog->insnsi; i < prog->len; i++, insn++) {\n\t\tif (!bpf_pseudo_call(insn))\n\t\t\tcontinue;\n\t\tinsn->off = 0;\n\t\tinsn->imm = env->insn_aux_data[i].call_imm;\n\t}\n\tbpf_prog_jit_attempt_done(prog);\n\treturn err;\n}\n\nstatic int fixup_call_args(struct bpf_verifier_env *env)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_insn *insn = prog->insnsi;\n\tbool has_kfunc_call = bpf_prog_has_kfunc_call(prog);\n\tint i, depth;\n#endif\n\tint err = 0;\n\n\tif (env->prog->jit_requested &&\n\t    !bpf_prog_is_offloaded(env->prog->aux)) {\n\t\terr = jit_subprogs(env);\n\t\tif (err == 0)\n\t\t\treturn 0;\n\t\tif (err == -EFAULT)\n\t\t\treturn err;\n\t}\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tif (has_kfunc_call) {\n\t\tverbose(env, \"calling kernel functions are not allowed in non-JITed programs\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (env->subprog_cnt > 1 && env->prog->aux->tail_call_reachable) {\n\t\t \n\t\tverbose(env, \"tail_calls are not allowed in non-JITed programs with bpf-to-bpf calls\\n\");\n\t\treturn -EINVAL;\n\t}\n\tfor (i = 0; i < prog->len; i++, insn++) {\n\t\tif (bpf_pseudo_func(insn)) {\n\t\t\t \n\t\t\tverbose(env, \"callbacks are not allowed in non-JITed programs\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!bpf_pseudo_call(insn))\n\t\t\tcontinue;\n\t\tdepth = get_callee_stack_depth(env, insn, i);\n\t\tif (depth < 0)\n\t\t\treturn depth;\n\t\tbpf_patch_call_args(insn, depth);\n\t}\n\terr = 0;\n#endif\n\treturn err;\n}\n\n \nstatic void specialize_kfunc(struct bpf_verifier_env *env,\n\t\t\t     u32 func_id, u16 offset, unsigned long *addr)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tbool seen_direct_write;\n\tvoid *xdp_kfunc;\n\tbool is_rdonly;\n\n\tif (bpf_dev_bound_kfunc_id(func_id)) {\n\t\txdp_kfunc = bpf_dev_bound_resolve_kfunc(prog, func_id);\n\t\tif (xdp_kfunc) {\n\t\t\t*addr = (unsigned long)xdp_kfunc;\n\t\t\treturn;\n\t\t}\n\t\t \n\t}\n\n\tif (offset)\n\t\treturn;\n\n\tif (func_id == special_kfunc_list[KF_bpf_dynptr_from_skb]) {\n\t\tseen_direct_write = env->seen_direct_write;\n\t\tis_rdonly = !may_access_direct_pkt_data(env, NULL, BPF_WRITE);\n\n\t\tif (is_rdonly)\n\t\t\t*addr = (unsigned long)bpf_dynptr_from_skb_rdonly;\n\n\t\t \n\t\tenv->seen_direct_write = seen_direct_write;\n\t}\n}\n\nstatic void __fixup_collection_insert_kfunc(struct bpf_insn_aux_data *insn_aux,\n\t\t\t\t\t    u16 struct_meta_reg,\n\t\t\t\t\t    u16 node_offset_reg,\n\t\t\t\t\t    struct bpf_insn *insn,\n\t\t\t\t\t    struct bpf_insn *insn_buf,\n\t\t\t\t\t    int *cnt)\n{\n\tstruct btf_struct_meta *kptr_struct_meta = insn_aux->kptr_struct_meta;\n\tstruct bpf_insn addr[2] = { BPF_LD_IMM64(struct_meta_reg, (long)kptr_struct_meta) };\n\n\tinsn_buf[0] = addr[0];\n\tinsn_buf[1] = addr[1];\n\tinsn_buf[2] = BPF_MOV64_IMM(node_offset_reg, insn_aux->insert_off);\n\tinsn_buf[3] = *insn;\n\t*cnt = 4;\n}\n\nstatic int fixup_kfunc_call(struct bpf_verifier_env *env, struct bpf_insn *insn,\n\t\t\t    struct bpf_insn *insn_buf, int insn_idx, int *cnt)\n{\n\tconst struct bpf_kfunc_desc *desc;\n\n\tif (!insn->imm) {\n\t\tverbose(env, \"invalid kernel function call not eliminated in verifier pass\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t*cnt = 0;\n\n\t \n\tdesc = find_kfunc_desc(env->prog, insn->imm, insn->off);\n\tif (!desc) {\n\t\tverbose(env, \"verifier internal error: kernel function descriptor not found for func_id %u\\n\",\n\t\t\tinsn->imm);\n\t\treturn -EFAULT;\n\t}\n\n\tif (!bpf_jit_supports_far_kfunc_call())\n\t\tinsn->imm = BPF_CALL_IMM(desc->addr);\n\tif (insn->off)\n\t\treturn 0;\n\tif (desc->func_id == special_kfunc_list[KF_bpf_obj_new_impl]) {\n\t\tstruct btf_struct_meta *kptr_struct_meta = env->insn_aux_data[insn_idx].kptr_struct_meta;\n\t\tstruct bpf_insn addr[2] = { BPF_LD_IMM64(BPF_REG_2, (long)kptr_struct_meta) };\n\t\tu64 obj_new_size = env->insn_aux_data[insn_idx].obj_new_size;\n\n\t\tinsn_buf[0] = BPF_MOV64_IMM(BPF_REG_1, obj_new_size);\n\t\tinsn_buf[1] = addr[0];\n\t\tinsn_buf[2] = addr[1];\n\t\tinsn_buf[3] = *insn;\n\t\t*cnt = 4;\n\t} else if (desc->func_id == special_kfunc_list[KF_bpf_obj_drop_impl] ||\n\t\t   desc->func_id == special_kfunc_list[KF_bpf_refcount_acquire_impl]) {\n\t\tstruct btf_struct_meta *kptr_struct_meta = env->insn_aux_data[insn_idx].kptr_struct_meta;\n\t\tstruct bpf_insn addr[2] = { BPF_LD_IMM64(BPF_REG_2, (long)kptr_struct_meta) };\n\n\t\tif (desc->func_id == special_kfunc_list[KF_bpf_refcount_acquire_impl] &&\n\t\t    !kptr_struct_meta) {\n\t\t\tverbose(env, \"verifier internal error: kptr_struct_meta expected at insn_idx %d\\n\",\n\t\t\t\tinsn_idx);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tinsn_buf[0] = addr[0];\n\t\tinsn_buf[1] = addr[1];\n\t\tinsn_buf[2] = *insn;\n\t\t*cnt = 3;\n\t} else if (desc->func_id == special_kfunc_list[KF_bpf_list_push_back_impl] ||\n\t\t   desc->func_id == special_kfunc_list[KF_bpf_list_push_front_impl] ||\n\t\t   desc->func_id == special_kfunc_list[KF_bpf_rbtree_add_impl]) {\n\t\tstruct btf_struct_meta *kptr_struct_meta = env->insn_aux_data[insn_idx].kptr_struct_meta;\n\t\tint struct_meta_reg = BPF_REG_3;\n\t\tint node_offset_reg = BPF_REG_4;\n\n\t\t \n\t\tif (desc->func_id == special_kfunc_list[KF_bpf_rbtree_add_impl]) {\n\t\t\tstruct_meta_reg = BPF_REG_4;\n\t\t\tnode_offset_reg = BPF_REG_5;\n\t\t}\n\n\t\tif (!kptr_struct_meta) {\n\t\t\tverbose(env, \"verifier internal error: kptr_struct_meta expected at insn_idx %d\\n\",\n\t\t\t\tinsn_idx);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\t__fixup_collection_insert_kfunc(&env->insn_aux_data[insn_idx], struct_meta_reg,\n\t\t\t\t\t\tnode_offset_reg, insn, insn_buf, cnt);\n\t} else if (desc->func_id == special_kfunc_list[KF_bpf_cast_to_kern_ctx] ||\n\t\t   desc->func_id == special_kfunc_list[KF_bpf_rdonly_cast]) {\n\t\tinsn_buf[0] = BPF_MOV64_REG(BPF_REG_0, BPF_REG_1);\n\t\t*cnt = 1;\n\t}\n\treturn 0;\n}\n\n \nstatic int do_misc_fixups(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tenum bpf_attach_type eatype = prog->expected_attach_type;\n\tenum bpf_prog_type prog_type = resolve_prog_type(prog);\n\tstruct bpf_insn *insn = prog->insnsi;\n\tconst struct bpf_func_proto *fn;\n\tconst int insn_cnt = prog->len;\n\tconst struct bpf_map_ops *ops;\n\tstruct bpf_insn_aux_data *aux;\n\tstruct bpf_insn insn_buf[16];\n\tstruct bpf_prog *new_prog;\n\tstruct bpf_map *map_ptr;\n\tint i, ret, cnt, delta = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\t \n\t\tif (insn->code == (BPF_ALU64 | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_DIV | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_MOD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU | BPF_DIV | BPF_X)) {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\t\t\tbool isdiv = BPF_OP(insn->code) == BPF_DIV;\n\t\t\tstruct bpf_insn *patchlet;\n\t\t\tstruct bpf_insn chk_and_div[] = {\n\t\t\t\t \n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JNE | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 2, 0),\n\t\t\t\tBPF_ALU32_REG(BPF_XOR, insn->dst_reg, insn->dst_reg),\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\t*insn,\n\t\t\t};\n\t\t\tstruct bpf_insn chk_and_mod[] = {\n\t\t\t\t \n\t\t\t\tBPF_RAW_INSN((is64 ? BPF_JMP : BPF_JMP32) |\n\t\t\t\t\t     BPF_JEQ | BPF_K, insn->src_reg,\n\t\t\t\t\t     0, 1 + (is64 ? 0 : 1), 0),\n\t\t\t\t*insn,\n\t\t\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 1),\n\t\t\t\tBPF_MOV32_REG(insn->dst_reg, insn->dst_reg),\n\t\t\t};\n\n\t\t\tpatchlet = isdiv ? chk_and_div : chk_and_mod;\n\t\t\tcnt = isdiv ? ARRAY_SIZE(chk_and_div) :\n\t\t\t\t      ARRAY_SIZE(chk_and_mod) - (is64 ? 2 : 0);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, patchlet, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (BPF_CLASS(insn->code) == BPF_LD &&\n\t\t    (BPF_MODE(insn->code) == BPF_ABS ||\n\t\t     BPF_MODE(insn->code) == BPF_IND)) {\n\t\t\tcnt = env->ops->gen_ld_abs(insn, insn_buf);\n\t\t\tif (cnt == 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||\n\t\t    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {\n\t\t\tconst u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;\n\t\t\tconst u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;\n\t\t\tstruct bpf_insn *patch = &insn_buf[0];\n\t\t\tbool issrc, isneg, isimm;\n\t\t\tu32 off_reg;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (!aux->alu_state ||\n\t\t\t    aux->alu_state == BPF_ALU_NON_POINTER)\n\t\t\t\tcontinue;\n\n\t\t\tisneg = aux->alu_state & BPF_ALU_NEG_VALUE;\n\t\t\tissrc = (aux->alu_state & BPF_ALU_SANITIZE) ==\n\t\t\t\tBPF_ALU_SANITIZE_SRC;\n\t\t\tisimm = aux->alu_state & BPF_ALU_IMMEDIATE;\n\n\t\t\toff_reg = issrc ? insn->src_reg : insn->dst_reg;\n\t\t\tif (isimm) {\n\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n\t\t\t} else {\n\t\t\t\tif (isneg)\n\t\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\t\t*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);\n\t\t\t\t*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);\n\t\t\t}\n\t\t\tif (!issrc)\n\t\t\t\t*patch++ = BPF_MOV64_REG(insn->dst_reg, insn->src_reg);\n\t\t\tinsn->src_reg = BPF_REG_AX;\n\t\t\tif (isneg)\n\t\t\t\tinsn->code = insn->code == code_add ?\n\t\t\t\t\t     code_sub : code_add;\n\t\t\t*patch++ = *insn;\n\t\t\tif (issrc && isneg && !isimm)\n\t\t\t\t*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);\n\t\t\tcnt = patch - insn_buf;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->code != (BPF_JMP | BPF_CALL))\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_CALL)\n\t\t\tcontinue;\n\t\tif (insn->src_reg == BPF_PSEUDO_KFUNC_CALL) {\n\t\t\tret = fixup_kfunc_call(env, insn, insn_buf, i + delta, &cnt);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tif (cnt == 0)\n\t\t\t\tcontinue;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta\t += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn\t  = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->imm == BPF_FUNC_get_route_realm)\n\t\t\tprog->dst_needed = 1;\n\t\tif (insn->imm == BPF_FUNC_get_prandom_u32)\n\t\t\tbpf_user_rnd_init_once();\n\t\tif (insn->imm == BPF_FUNC_override_return)\n\t\t\tprog->kprobe_override = 1;\n\t\tif (insn->imm == BPF_FUNC_tail_call) {\n\t\t\t \n\t\t\tprog->cb_access = 1;\n\t\t\tif (!allow_tail_call_in_subprogs(env))\n\t\t\t\tprog->aux->stack_depth = MAX_BPF_STACK;\n\t\t\tprog->aux->max_pkt_offset = MAX_PACKET_OFF;\n\n\t\t\t \n\t\t\tinsn->imm = 0;\n\t\t\tinsn->code = BPF_JMP | BPF_TAIL_CALL;\n\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (env->bpf_capable && !prog->blinding_requested &&\n\t\t\t    prog->jit_requested &&\n\t\t\t    !bpf_map_key_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_poisoned(aux) &&\n\t\t\t    !bpf_map_ptr_unpriv(aux)) {\n\t\t\t\tstruct bpf_jit_poke_descriptor desc = {\n\t\t\t\t\t.reason = BPF_POKE_REASON_TAIL_CALL,\n\t\t\t\t\t.tail_call.map = BPF_MAP_PTR(aux->map_ptr_state),\n\t\t\t\t\t.tail_call.key = bpf_map_key_immediate(aux),\n\t\t\t\t\t.insn_idx = i + delta,\n\t\t\t\t};\n\n\t\t\t\tret = bpf_jit_add_poke_descriptor(prog, &desc);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tverbose(env, \"adding tail call poke descriptor failed\\n\");\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\n\t\t\t\tinsn->imm = ret + 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!bpf_map_ptr_unpriv(aux))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (bpf_map_ptr_poisoned(aux)) {\n\t\t\t\tverbose(env, \"tail_call abusing map_ptr\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tinsn_buf[0] = BPF_JMP_IMM(BPF_JGE, BPF_REG_3,\n\t\t\t\t\t\t  map_ptr->max_entries, 2);\n\t\t\tinsn_buf[1] = BPF_ALU32_IMM(BPF_AND, BPF_REG_3,\n\t\t\t\t\t\t    container_of(map_ptr,\n\t\t\t\t\t\t\t\t struct bpf_array,\n\t\t\t\t\t\t\t\t map)->index_mask);\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (insn->imm == BPF_FUNC_timer_set_callback) {\n\t\t\t \n\t\t\tstruct bpf_insn ld_addrs[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_3, (long)prog->aux),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_addrs[0];\n\t\t\tinsn_buf[1] = ld_addrs[1];\n\t\t\tinsn_buf[2] = *insn;\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\tif (is_storage_get_function(insn->imm)) {\n\t\t\tif (!env->prog->aux->sleepable ||\n\t\t\t    env->insn_aux_data[i + delta].storage_get_func_atomic)\n\t\t\t\tinsn_buf[0] = BPF_MOV64_IMM(BPF_REG_5, (__force __s32)GFP_ATOMIC);\n\t\t\telse\n\t\t\t\tinsn_buf[0] = BPF_MOV64_IMM(BPF_REG_5, (__force __s32)GFP_KERNEL);\n\t\t\tinsn_buf[1] = *insn;\n\t\t\tcnt = 2;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn = new_prog->insnsi + i + delta;\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\t \n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    (insn->imm == BPF_FUNC_map_lookup_elem ||\n\t\t     insn->imm == BPF_FUNC_map_update_elem ||\n\t\t     insn->imm == BPF_FUNC_map_delete_elem ||\n\t\t     insn->imm == BPF_FUNC_map_push_elem   ||\n\t\t     insn->imm == BPF_FUNC_map_pop_elem    ||\n\t\t     insn->imm == BPF_FUNC_map_peek_elem   ||\n\t\t     insn->imm == BPF_FUNC_redirect_map    ||\n\t\t     insn->imm == BPF_FUNC_for_each_map_elem ||\n\t\t     insn->imm == BPF_FUNC_map_lookup_percpu_elem)) {\n\t\t\taux = &env->insn_aux_data[i + delta];\n\t\t\tif (bpf_map_ptr_poisoned(aux))\n\t\t\t\tgoto patch_call_imm;\n\n\t\t\tmap_ptr = BPF_MAP_PTR(aux->map_ptr_state);\n\t\t\tops = map_ptr->ops;\n\t\t\tif (insn->imm == BPF_FUNC_map_lookup_elem &&\n\t\t\t    ops->map_gen_lookup) {\n\t\t\t\tcnt = ops->map_gen_lookup(map_ptr, insn_buf);\n\t\t\t\tif (cnt == -EOPNOTSUPP)\n\t\t\t\t\tgoto patch_map_ops_generic;\n\t\t\t\tif (cnt <= 0 || cnt >= ARRAY_SIZE(insn_buf)) {\n\t\t\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta,\n\t\t\t\t\t\t\t       insn_buf, cnt);\n\t\t\t\tif (!new_prog)\n\t\t\t\t\treturn -ENOMEM;\n\n\t\t\t\tdelta    += cnt - 1;\n\t\t\t\tenv->prog = prog = new_prog;\n\t\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_delete_elem,\n\t\t\t\t     (long (*)(struct bpf_map *map, void *key))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_update_elem,\n\t\t\t\t     (long (*)(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_push_elem,\n\t\t\t\t     (long (*)(struct bpf_map *map, void *value,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_pop_elem,\n\t\t\t\t     (long (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_peek_elem,\n\t\t\t\t     (long (*)(struct bpf_map *map, void *value))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_redirect,\n\t\t\t\t     (long (*)(struct bpf_map *map, u64 index, u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_for_each_callback,\n\t\t\t\t     (long (*)(struct bpf_map *map,\n\t\t\t\t\t      bpf_callback_t callback_fn,\n\t\t\t\t\t      void *callback_ctx,\n\t\t\t\t\t      u64 flags))NULL));\n\t\t\tBUILD_BUG_ON(!__same_type(ops->map_lookup_percpu_elem,\n\t\t\t\t     (void *(*)(struct bpf_map *map, void *key, u32 cpu))NULL));\n\npatch_map_ops_generic:\n\t\t\tswitch (insn->imm) {\n\t\t\tcase BPF_FUNC_map_lookup_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_lookup_elem);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_update_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_update_elem);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_delete_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_delete_elem);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_push_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_push_elem);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_pop_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_pop_elem);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_peek_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_peek_elem);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_redirect_map:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_redirect);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_for_each_map_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_for_each_callback);\n\t\t\t\tcontinue;\n\t\t\tcase BPF_FUNC_map_lookup_percpu_elem:\n\t\t\t\tinsn->imm = BPF_CALL_IMM(ops->map_lookup_percpu_elem);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgoto patch_call_imm;\n\t\t}\n\n\t\t \n\t\tif (prog->jit_requested && BITS_PER_LONG == 64 &&\n\t\t    insn->imm == BPF_FUNC_jiffies64) {\n\t\t\tstruct bpf_insn ld_jiffies_addr[2] = {\n\t\t\t\tBPF_LD_IMM64(BPF_REG_0,\n\t\t\t\t\t     (unsigned long)&jiffies),\n\t\t\t};\n\n\t\t\tinsn_buf[0] = ld_jiffies_addr[0];\n\t\t\tinsn_buf[1] = ld_jiffies_addr[1];\n\t\t\tinsn_buf[2] = BPF_LDX_MEM(BPF_DW, BPF_REG_0,\n\t\t\t\t\t\t  BPF_REG_0, 0);\n\t\t\tcnt = 3;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf,\n\t\t\t\t\t\t       cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (prog_type == BPF_PROG_TYPE_TRACING &&\n\t\t    insn->imm == BPF_FUNC_get_func_arg) {\n\t\t\t \n\t\t\tinsn_buf[0] = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1, -8);\n\t\t\tinsn_buf[1] = BPF_JMP32_REG(BPF_JGE, BPF_REG_2, BPF_REG_0, 6);\n\t\t\tinsn_buf[2] = BPF_ALU64_IMM(BPF_LSH, BPF_REG_2, 3);\n\t\t\tinsn_buf[3] = BPF_ALU64_REG(BPF_ADD, BPF_REG_2, BPF_REG_1);\n\t\t\tinsn_buf[4] = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_2, 0);\n\t\t\tinsn_buf[5] = BPF_STX_MEM(BPF_DW, BPF_REG_3, BPF_REG_0, 0);\n\t\t\tinsn_buf[6] = BPF_MOV64_IMM(BPF_REG_0, 0);\n\t\t\tinsn_buf[7] = BPF_JMP_A(1);\n\t\t\tinsn_buf[8] = BPF_MOV64_IMM(BPF_REG_0, -EINVAL);\n\t\t\tcnt = 9;\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (prog_type == BPF_PROG_TYPE_TRACING &&\n\t\t    insn->imm == BPF_FUNC_get_func_ret) {\n\t\t\tif (eatype == BPF_TRACE_FEXIT ||\n\t\t\t    eatype == BPF_MODIFY_RETURN) {\n\t\t\t\t \n\t\t\t\tinsn_buf[0] = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1, -8);\n\t\t\t\tinsn_buf[1] = BPF_ALU64_IMM(BPF_LSH, BPF_REG_0, 3);\n\t\t\t\tinsn_buf[2] = BPF_ALU64_REG(BPF_ADD, BPF_REG_0, BPF_REG_1);\n\t\t\t\tinsn_buf[3] = BPF_LDX_MEM(BPF_DW, BPF_REG_3, BPF_REG_0, 0);\n\t\t\t\tinsn_buf[4] = BPF_STX_MEM(BPF_DW, BPF_REG_2, BPF_REG_3, 0);\n\t\t\t\tinsn_buf[5] = BPF_MOV64_IMM(BPF_REG_0, 0);\n\t\t\t\tcnt = 6;\n\t\t\t} else {\n\t\t\t\tinsn_buf[0] = BPF_MOV64_IMM(BPF_REG_0, -EOPNOTSUPP);\n\t\t\t\tcnt = 1;\n\t\t\t}\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta    += cnt - 1;\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (prog_type == BPF_PROG_TYPE_TRACING &&\n\t\t    insn->imm == BPF_FUNC_get_func_arg_cnt) {\n\t\t\t \n\t\t\tinsn_buf[0] = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1, -8);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, 1);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (prog_type == BPF_PROG_TYPE_TRACING &&\n\t\t    insn->imm == BPF_FUNC_get_func_ip) {\n\t\t\t \n\t\t\tinsn_buf[0] = BPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1, -16);\n\n\t\t\tnew_prog = bpf_patch_insn_data(env, i + delta, insn_buf, 1);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tenv->prog = prog = new_prog;\n\t\t\tinsn      = new_prog->insnsi + i + delta;\n\t\t\tcontinue;\n\t\t}\n\npatch_call_imm:\n\t\tfn = env->ops->get_func_proto(insn->imm, env->prog);\n\t\t \n\t\tif (!fn->func) {\n\t\t\tverbose(env,\n\t\t\t\t\"kernel subsystem misconfigured func %s#%d\\n\",\n\t\t\t\tfunc_id_name(insn->imm), insn->imm);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tinsn->imm = fn->func - __bpf_call_base;\n\t}\n\n\t \n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tmap_ptr = prog->aux->poke_tab[i].tail_call.map;\n\t\tif (!map_ptr->ops->map_poke_track ||\n\t\t    !map_ptr->ops->map_poke_untrack ||\n\t\t    !map_ptr->ops->map_poke_run) {\n\t\t\tverbose(env, \"bpf verifier is misconfigured\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = map_ptr->ops->map_poke_track(map_ptr, prog->aux);\n\t\tif (ret < 0) {\n\t\t\tverbose(env, \"tracking tail call prog failed\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tsort_kfunc_descs_by_imm_off(env->prog);\n\n\treturn 0;\n}\n\nstatic struct bpf_prog *inline_bpf_loop(struct bpf_verifier_env *env,\n\t\t\t\t\tint position,\n\t\t\t\t\ts32 stack_base,\n\t\t\t\t\tu32 callback_subprogno,\n\t\t\t\t\tu32 *cnt)\n{\n\ts32 r6_offset = stack_base + 0 * BPF_REG_SIZE;\n\ts32 r7_offset = stack_base + 1 * BPF_REG_SIZE;\n\ts32 r8_offset = stack_base + 2 * BPF_REG_SIZE;\n\tint reg_loop_max = BPF_REG_6;\n\tint reg_loop_cnt = BPF_REG_7;\n\tint reg_loop_ctx = BPF_REG_8;\n\n\tstruct bpf_prog *new_prog;\n\tu32 callback_start;\n\tu32 call_insn_offset;\n\ts32 callback_offset;\n\n\t \n\tstruct bpf_insn insn_buf[] = {\n\t\t \n\t\tBPF_JMP_IMM(BPF_JLE, BPF_REG_1, BPF_MAX_LOOPS, 2),\n\t\tBPF_MOV32_IMM(BPF_REG_0, -E2BIG),\n\t\tBPF_JMP_IMM(BPF_JA, 0, 0, 16),\n\t\t \n\t\tBPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_6, r6_offset),\n\t\tBPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_7, r7_offset),\n\t\tBPF_STX_MEM(BPF_DW, BPF_REG_10, BPF_REG_8, r8_offset),\n\t\t \n\t\tBPF_MOV64_REG(reg_loop_max, BPF_REG_1),\n\t\tBPF_MOV32_IMM(reg_loop_cnt, 0),\n\t\tBPF_MOV64_REG(reg_loop_ctx, BPF_REG_3),\n\t\t \n\t\tBPF_JMP_REG(BPF_JGE, reg_loop_cnt, reg_loop_max, 5),\n\t\t \n\t\tBPF_MOV64_REG(BPF_REG_1, reg_loop_cnt),\n\t\tBPF_MOV64_REG(BPF_REG_2, reg_loop_ctx),\n\t\tBPF_CALL_REL(0),\n\t\t \n\t\tBPF_ALU64_IMM(BPF_ADD, reg_loop_cnt, 1),\n\t\t \n\t\tBPF_JMP_IMM(BPF_JEQ, BPF_REG_0, 0, -6),\n\t\t \n\t\tBPF_MOV64_REG(BPF_REG_0, reg_loop_cnt),\n\t\t \n\t\tBPF_LDX_MEM(BPF_DW, BPF_REG_6, BPF_REG_10, r6_offset),\n\t\tBPF_LDX_MEM(BPF_DW, BPF_REG_7, BPF_REG_10, r7_offset),\n\t\tBPF_LDX_MEM(BPF_DW, BPF_REG_8, BPF_REG_10, r8_offset),\n\t};\n\n\t*cnt = ARRAY_SIZE(insn_buf);\n\tnew_prog = bpf_patch_insn_data(env, position, insn_buf, *cnt);\n\tif (!new_prog)\n\t\treturn new_prog;\n\n\t \n\tcallback_start = env->subprog_info[callback_subprogno].start;\n\t \n\tcall_insn_offset = position + 12;\n\tcallback_offset = callback_start - call_insn_offset - 1;\n\tnew_prog->insnsi[call_insn_offset].imm = callback_offset;\n\n\treturn new_prog;\n}\n\nstatic bool is_bpf_loop_call(struct bpf_insn *insn)\n{\n\treturn insn->code == (BPF_JMP | BPF_CALL) &&\n\t\tinsn->src_reg == 0 &&\n\t\tinsn->imm == BPF_FUNC_loop;\n}\n\n \nstatic int optimize_bpf_loop(struct bpf_verifier_env *env)\n{\n\tstruct bpf_subprog_info *subprogs = env->subprog_info;\n\tint i, cur_subprog = 0, cnt, delta = 0;\n\tstruct bpf_insn *insn = env->prog->insnsi;\n\tint insn_cnt = env->prog->len;\n\tu16 stack_depth = subprogs[cur_subprog].stack_depth;\n\tu16 stack_depth_roundup = round_up(stack_depth, 8) - stack_depth;\n\tu16 stack_depth_extra = 0;\n\n\tfor (i = 0; i < insn_cnt; i++, insn++) {\n\t\tstruct bpf_loop_inline_state *inline_state =\n\t\t\t&env->insn_aux_data[i + delta].loop_inline_state;\n\n\t\tif (is_bpf_loop_call(insn) && inline_state->fit_for_inline) {\n\t\t\tstruct bpf_prog *new_prog;\n\n\t\t\tstack_depth_extra = BPF_REG_SIZE * 3 + stack_depth_roundup;\n\t\t\tnew_prog = inline_bpf_loop(env,\n\t\t\t\t\t\t   i + delta,\n\t\t\t\t\t\t   -(stack_depth + stack_depth_extra),\n\t\t\t\t\t\t   inline_state->callback_subprogno,\n\t\t\t\t\t\t   &cnt);\n\t\t\tif (!new_prog)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tdelta     += cnt - 1;\n\t\t\tenv->prog  = new_prog;\n\t\t\tinsn       = new_prog->insnsi + i + delta;\n\t\t}\n\n\t\tif (subprogs[cur_subprog + 1].start == i + delta + 1) {\n\t\t\tsubprogs[cur_subprog].stack_depth += stack_depth_extra;\n\t\t\tcur_subprog++;\n\t\t\tstack_depth = subprogs[cur_subprog].stack_depth;\n\t\t\tstack_depth_roundup = round_up(stack_depth, 8) - stack_depth;\n\t\t\tstack_depth_extra = 0;\n\t\t}\n\t}\n\n\tenv->prog->aux->stack_depth = env->subprog_info[0].stack_depth;\n\n\treturn 0;\n}\n\nstatic void free_states(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state_list *sl, *sln;\n\tint i;\n\n\tsl = env->free_list;\n\twhile (sl) {\n\t\tsln = sl->next;\n\t\tfree_verifier_state(&sl->state, false);\n\t\tkfree(sl);\n\t\tsl = sln;\n\t}\n\tenv->free_list = NULL;\n\n\tif (!env->explored_states)\n\t\treturn;\n\n\tfor (i = 0; i < state_htab_size(env); i++) {\n\t\tsl = env->explored_states[i];\n\n\t\twhile (sl) {\n\t\t\tsln = sl->next;\n\t\t\tfree_verifier_state(&sl->state, false);\n\t\t\tkfree(sl);\n\t\t\tsl = sln;\n\t\t}\n\t\tenv->explored_states[i] = NULL;\n\t}\n}\n\nstatic int do_check_common(struct bpf_verifier_env *env, int subprog)\n{\n\tbool pop_log = !(env->log.level & BPF_LOG_LEVEL2);\n\tstruct bpf_verifier_state *state;\n\tstruct bpf_reg_state *regs;\n\tint ret, i;\n\n\tenv->prev_linfo = NULL;\n\tenv->pass_cnt++;\n\n\tstate = kzalloc(sizeof(struct bpf_verifier_state), GFP_KERNEL);\n\tif (!state)\n\t\treturn -ENOMEM;\n\tstate->curframe = 0;\n\tstate->speculative = false;\n\tstate->branches = 1;\n\tstate->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);\n\tif (!state->frame[0]) {\n\t\tkfree(state);\n\t\treturn -ENOMEM;\n\t}\n\tenv->cur_state = state;\n\tinit_func_state(env, state->frame[0],\n\t\t\tBPF_MAIN_FUNC  ,\n\t\t\t0  ,\n\t\t\tsubprog);\n\tstate->first_insn_idx = env->subprog_info[subprog].start;\n\tstate->last_insn_idx = -1;\n\n\tregs = state->frame[state->curframe]->regs;\n\tif (subprog || env->prog->type == BPF_PROG_TYPE_EXT) {\n\t\tret = btf_prepare_func_args(env, subprog, regs);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tfor (i = BPF_REG_1; i <= BPF_REG_5; i++) {\n\t\t\tif (regs[i].type == PTR_TO_CTX)\n\t\t\t\tmark_reg_known_zero(env, regs, i);\n\t\t\telse if (regs[i].type == SCALAR_VALUE)\n\t\t\t\tmark_reg_unknown(env, regs, i);\n\t\t\telse if (base_type(regs[i].type) == PTR_TO_MEM) {\n\t\t\t\tconst u32 mem_size = regs[i].mem_size;\n\n\t\t\t\tmark_reg_known_zero(env, regs, i);\n\t\t\t\tregs[i].mem_size = mem_size;\n\t\t\t\tregs[i].id = ++env->id_gen;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tregs[BPF_REG_1].type = PTR_TO_CTX;\n\t\tmark_reg_known_zero(env, regs, BPF_REG_1);\n\t\tret = btf_check_subprog_arg_match(env, subprog, regs);\n\t\tif (ret == -EFAULT)\n\t\t\t \n\t\t\tgoto out;\n\t}\n\n\tret = do_check(env);\nout:\n\t \n\tif (env->cur_state) {\n\t\tfree_verifier_state(env->cur_state, true);\n\t\tenv->cur_state = NULL;\n\t}\n\twhile (!pop_stack(env, NULL, NULL, false));\n\tif (!ret && pop_log)\n\t\tbpf_vlog_reset(&env->log, 0);\n\tfree_states(env);\n\treturn ret;\n}\n\n \nstatic int do_check_subprogs(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog_aux *aux = env->prog->aux;\n\tint i, ret;\n\n\tif (!aux->func_info)\n\t\treturn 0;\n\n\tfor (i = 1; i < env->subprog_cnt; i++) {\n\t\tif (aux->func_info_aux[i].linkage != BTF_FUNC_GLOBAL)\n\t\t\tcontinue;\n\t\tenv->insn_idx = env->subprog_info[i].start;\n\t\tWARN_ON_ONCE(env->insn_idx == 0);\n\t\tret = do_check_common(env, i);\n\t\tif (ret) {\n\t\t\treturn ret;\n\t\t} else if (env->log.level & BPF_LOG_LEVEL) {\n\t\t\tverbose(env,\n\t\t\t\t\"Func#%d is safe for any args that match its prototype\\n\",\n\t\t\t\ti);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int do_check_main(struct bpf_verifier_env *env)\n{\n\tint ret;\n\n\tenv->insn_idx = 0;\n\tret = do_check_common(env, 0);\n\tif (!ret)\n\t\tenv->prog->aux->stack_depth = env->subprog_info[0].stack_depth;\n\treturn ret;\n}\n\n\nstatic void print_verification_stats(struct bpf_verifier_env *env)\n{\n\tint i;\n\n\tif (env->log.level & BPF_LOG_STATS) {\n\t\tverbose(env, \"verification time %lld usec\\n\",\n\t\t\tdiv_u64(env->verification_time, 1000));\n\t\tverbose(env, \"stack depth \");\n\t\tfor (i = 0; i < env->subprog_cnt; i++) {\n\t\t\tu32 depth = env->subprog_info[i].stack_depth;\n\n\t\t\tverbose(env, \"%d\", depth);\n\t\t\tif (i + 1 < env->subprog_cnt)\n\t\t\t\tverbose(env, \"+\");\n\t\t}\n\t\tverbose(env, \"\\n\");\n\t}\n\tverbose(env, \"processed %d insns (limit %d) max_states_per_insn %d \"\n\t\t\"total_states %d peak_states %d mark_read %d\\n\",\n\t\tenv->insn_processed, BPF_COMPLEXITY_LIMIT_INSNS,\n\t\tenv->max_states_per_insn, env->total_states,\n\t\tenv->peak_states, env->longest_mark_read_walk);\n}\n\nstatic int check_struct_ops_btf_id(struct bpf_verifier_env *env)\n{\n\tconst struct btf_type *t, *func_proto;\n\tconst struct bpf_struct_ops *st_ops;\n\tconst struct btf_member *member;\n\tstruct bpf_prog *prog = env->prog;\n\tu32 btf_id, member_idx;\n\tconst char *mname;\n\n\tif (!prog->gpl_compatible) {\n\t\tverbose(env, \"struct ops programs must have a GPL compatible license\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tbtf_id = prog->aux->attach_btf_id;\n\tst_ops = bpf_struct_ops_find(btf_id);\n\tif (!st_ops) {\n\t\tverbose(env, \"attach_btf_id %u is not a supported struct\\n\",\n\t\t\tbtf_id);\n\t\treturn -ENOTSUPP;\n\t}\n\n\tt = st_ops->type;\n\tmember_idx = prog->expected_attach_type;\n\tif (member_idx >= btf_type_vlen(t)) {\n\t\tverbose(env, \"attach to invalid member idx %u of struct %s\\n\",\n\t\t\tmember_idx, st_ops->name);\n\t\treturn -EINVAL;\n\t}\n\n\tmember = &btf_type_member(t)[member_idx];\n\tmname = btf_name_by_offset(btf_vmlinux, member->name_off);\n\tfunc_proto = btf_type_resolve_func_ptr(btf_vmlinux, member->type,\n\t\t\t\t\t       NULL);\n\tif (!func_proto) {\n\t\tverbose(env, \"attach to invalid member %s(@idx %u) of struct %s\\n\",\n\t\t\tmname, member_idx, st_ops->name);\n\t\treturn -EINVAL;\n\t}\n\n\tif (st_ops->check_member) {\n\t\tint err = st_ops->check_member(t, member, prog);\n\n\t\tif (err) {\n\t\t\tverbose(env, \"attach to unsupported member %s of struct %s\\n\",\n\t\t\t\tmname, st_ops->name);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tprog->aux->attach_func_proto = func_proto;\n\tprog->aux->attach_func_name = mname;\n\tenv->ops = st_ops->verifier_ops;\n\n\treturn 0;\n}\n#define SECURITY_PREFIX \"security_\"\n\nstatic int check_attach_modify_return(unsigned long addr, const char *func_name)\n{\n\tif (within_error_injection_list(addr) ||\n\t    !strncmp(SECURITY_PREFIX, func_name, sizeof(SECURITY_PREFIX) - 1))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\n \nBTF_SET_START(btf_non_sleepable_error_inject)\n \nBTF_ID(func, __filemap_add_folio)\nBTF_ID(func, should_fail_alloc_page)\nBTF_ID(func, should_failslab)\nBTF_SET_END(btf_non_sleepable_error_inject)\n\nstatic int check_non_sleepable_error_inject(u32 btf_id)\n{\n\treturn btf_id_set_contains(&btf_non_sleepable_error_inject, btf_id);\n}\n\nint bpf_check_attach_target(struct bpf_verifier_log *log,\n\t\t\t    const struct bpf_prog *prog,\n\t\t\t    const struct bpf_prog *tgt_prog,\n\t\t\t    u32 btf_id,\n\t\t\t    struct bpf_attach_target_info *tgt_info)\n{\n\tbool prog_extension = prog->type == BPF_PROG_TYPE_EXT;\n\tconst char prefix[] = \"btf_trace_\";\n\tint ret = 0, subprog = -1, i;\n\tconst struct btf_type *t;\n\tbool conservative = true;\n\tconst char *tname;\n\tstruct btf *btf;\n\tlong addr = 0;\n\tstruct module *mod = NULL;\n\n\tif (!btf_id) {\n\t\tbpf_log(log, \"Tracing programs must provide btf_id\\n\");\n\t\treturn -EINVAL;\n\t}\n\tbtf = tgt_prog ? tgt_prog->aux->btf : prog->aux->attach_btf;\n\tif (!btf) {\n\t\tbpf_log(log,\n\t\t\t\"FENTRY/FEXIT program can only be attached to another program annotated with BTF\\n\");\n\t\treturn -EINVAL;\n\t}\n\tt = btf_type_by_id(btf, btf_id);\n\tif (!t) {\n\t\tbpf_log(log, \"attach_btf_id %u is invalid\\n\", btf_id);\n\t\treturn -EINVAL;\n\t}\n\ttname = btf_name_by_offset(btf, t->name_off);\n\tif (!tname) {\n\t\tbpf_log(log, \"attach_btf_id %u doesn't have a name\\n\", btf_id);\n\t\treturn -EINVAL;\n\t}\n\tif (tgt_prog) {\n\t\tstruct bpf_prog_aux *aux = tgt_prog->aux;\n\n\t\tif (bpf_prog_is_dev_bound(prog->aux) &&\n\t\t    !bpf_prog_dev_bound_match(prog, tgt_prog)) {\n\t\t\tbpf_log(log, \"Target program bound device mismatch\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tfor (i = 0; i < aux->func_info_cnt; i++)\n\t\t\tif (aux->func_info[i].type_id == btf_id) {\n\t\t\t\tsubprog = i;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (subprog == -1) {\n\t\t\tbpf_log(log, \"Subprog %s doesn't exist\\n\", tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tconservative = aux->func_info_aux[subprog].unreliable;\n\t\tif (prog_extension) {\n\t\t\tif (conservative) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"Cannot replace static functions\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!prog->jit_requested) {\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"Extension programs should be JITed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\tif (!tgt_prog->jited) {\n\t\t\tbpf_log(log, \"Can attach to only JITed progs\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tgt_prog->type == prog->type) {\n\t\t\t \n\t\t\tbpf_log(log, \"Cannot recursively attach\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tgt_prog->type == BPF_PROG_TYPE_TRACING &&\n\t\t    prog_extension &&\n\t\t    (tgt_prog->expected_attach_type == BPF_TRACE_FENTRY ||\n\t\t     tgt_prog->expected_attach_type == BPF_TRACE_FEXIT)) {\n\t\t\t \n\t\t\tbpf_log(log, \"Cannot extend fentry/fexit\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tif (prog_extension) {\n\t\t\tbpf_log(log, \"Cannot replace kernel functions\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tswitch (prog->expected_attach_type) {\n\tcase BPF_TRACE_RAW_TP:\n\t\tif (tgt_prog) {\n\t\t\tbpf_log(log,\n\t\t\t\t\"Only FENTRY/FEXIT progs are attachable to another BPF prog\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (!btf_type_is_typedef(t)) {\n\t\t\tbpf_log(log, \"attach_btf_id %u is not a typedef\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (strncmp(prefix, tname, sizeof(prefix) - 1)) {\n\t\t\tbpf_log(log, \"attach_btf_id %u points to wrong type name %s\\n\",\n\t\t\t\tbtf_id, tname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttname += sizeof(prefix) - 1;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_ptr(t))\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\t \n\t\t\treturn -EINVAL;\n\n\t\tbreak;\n\tcase BPF_TRACE_ITER:\n\t\tif (!btf_type_is_func(t)) {\n\t\t\tbpf_log(log, \"attach_btf_id %u is not a function\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\treturn -EINVAL;\n\t\tret = btf_distill_func_proto(log, btf, t, tname, &tgt_info->fmodel);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tbreak;\n\tdefault:\n\t\tif (!prog_extension)\n\t\t\treturn -EINVAL;\n\t\tfallthrough;\n\tcase BPF_MODIFY_RETURN:\n\tcase BPF_LSM_MAC:\n\tcase BPF_LSM_CGROUP:\n\tcase BPF_TRACE_FENTRY:\n\tcase BPF_TRACE_FEXIT:\n\t\tif (!btf_type_is_func(t)) {\n\t\t\tbpf_log(log, \"attach_btf_id %u is not a function\\n\",\n\t\t\t\tbtf_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (prog_extension &&\n\t\t    btf_check_type_match(log, prog, btf, t))\n\t\t\treturn -EINVAL;\n\t\tt = btf_type_by_id(btf, t->type);\n\t\tif (!btf_type_is_func_proto(t))\n\t\t\treturn -EINVAL;\n\n\t\tif ((prog->aux->saved_dst_prog_type || prog->aux->saved_dst_attach_type) &&\n\t\t    (!tgt_prog || prog->aux->saved_dst_prog_type != tgt_prog->type ||\n\t\t     prog->aux->saved_dst_attach_type != tgt_prog->expected_attach_type))\n\t\t\treturn -EINVAL;\n\n\t\tif (tgt_prog && conservative)\n\t\t\tt = NULL;\n\n\t\tret = btf_distill_func_proto(log, btf, t, tname, &tgt_info->fmodel);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tif (tgt_prog) {\n\t\t\tif (subprog == 0)\n\t\t\t\taddr = (long) tgt_prog->bpf_func;\n\t\t\telse\n\t\t\t\taddr = (long) tgt_prog->aux->func[subprog]->bpf_func;\n\t\t} else {\n\t\t\tif (btf_is_module(btf)) {\n\t\t\t\tmod = btf_try_get_module(btf);\n\t\t\t\tif (mod)\n\t\t\t\t\taddr = find_kallsyms_symbol_value(mod, tname);\n\t\t\t\telse\n\t\t\t\t\taddr = 0;\n\t\t\t} else {\n\t\t\t\taddr = kallsyms_lookup_name(tname);\n\t\t\t}\n\t\t\tif (!addr) {\n\t\t\t\tmodule_put(mod);\n\t\t\t\tbpf_log(log,\n\t\t\t\t\t\"The address of function %s cannot be found\\n\",\n\t\t\t\t\ttname);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t}\n\n\t\tif (prog->aux->sleepable) {\n\t\t\tret = -EINVAL;\n\t\t\tswitch (prog->type) {\n\t\t\tcase BPF_PROG_TYPE_TRACING:\n\n\t\t\t\t \n\t\t\t\tif (!check_non_sleepable_error_inject(btf_id) &&\n\t\t\t\t    within_error_injection_list(addr))\n\t\t\t\t\tret = 0;\n\t\t\t\t \n\t\t\t\telse {\n\t\t\t\t\tu32 *flags = btf_kfunc_is_modify_return(btf, btf_id,\n\t\t\t\t\t\t\t\t\t\tprog);\n\n\t\t\t\t\tif (flags && (*flags & KF_SLEEPABLE))\n\t\t\t\t\t\tret = 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase BPF_PROG_TYPE_LSM:\n\t\t\t\t \n\t\t\t\tif (bpf_lsm_is_sleepable_hook(btf_id))\n\t\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (ret) {\n\t\t\t\tmodule_put(mod);\n\t\t\t\tbpf_log(log, \"%s is not sleepable\\n\", tname);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t} else if (prog->expected_attach_type == BPF_MODIFY_RETURN) {\n\t\t\tif (tgt_prog) {\n\t\t\t\tmodule_put(mod);\n\t\t\t\tbpf_log(log, \"can't modify return codes of BPF programs\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tret = -EINVAL;\n\t\t\tif (btf_kfunc_is_modify_return(btf, btf_id, prog) ||\n\t\t\t    !check_attach_modify_return(addr, tname))\n\t\t\t\tret = 0;\n\t\t\tif (ret) {\n\t\t\t\tmodule_put(mod);\n\t\t\t\tbpf_log(log, \"%s() is not modifiable\\n\", tname);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\t}\n\ttgt_info->tgt_addr = addr;\n\ttgt_info->tgt_name = tname;\n\ttgt_info->tgt_type = t;\n\ttgt_info->tgt_mod = mod;\n\treturn 0;\n}\n\nBTF_SET_START(btf_id_deny)\nBTF_ID_UNUSED\n#ifdef CONFIG_SMP\nBTF_ID(func, migrate_disable)\nBTF_ID(func, migrate_enable)\n#endif\n#if !defined CONFIG_PREEMPT_RCU && !defined CONFIG_TINY_RCU\nBTF_ID(func, rcu_read_unlock_strict)\n#endif\n#if defined(CONFIG_DEBUG_PREEMPT) || defined(CONFIG_TRACE_PREEMPT_TOGGLE)\nBTF_ID(func, preempt_count_add)\nBTF_ID(func, preempt_count_sub)\n#endif\n#ifdef CONFIG_PREEMPT_RCU\nBTF_ID(func, __rcu_read_lock)\nBTF_ID(func, __rcu_read_unlock)\n#endif\nBTF_SET_END(btf_id_deny)\n\nstatic bool can_be_sleepable(struct bpf_prog *prog)\n{\n\tif (prog->type == BPF_PROG_TYPE_TRACING) {\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_TRACE_FENTRY:\n\t\tcase BPF_TRACE_FEXIT:\n\t\tcase BPF_MODIFY_RETURN:\n\t\tcase BPF_TRACE_ITER:\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn prog->type == BPF_PROG_TYPE_LSM ||\n\t       prog->type == BPF_PROG_TYPE_KPROBE   ||\n\t       prog->type == BPF_PROG_TYPE_STRUCT_OPS;\n}\n\nstatic int check_attach_btf_id(struct bpf_verifier_env *env)\n{\n\tstruct bpf_prog *prog = env->prog;\n\tstruct bpf_prog *tgt_prog = prog->aux->dst_prog;\n\tstruct bpf_attach_target_info tgt_info = {};\n\tu32 btf_id = prog->aux->attach_btf_id;\n\tstruct bpf_trampoline *tr;\n\tint ret;\n\tu64 key;\n\n\tif (prog->type == BPF_PROG_TYPE_SYSCALL) {\n\t\tif (prog->aux->sleepable)\n\t\t\t \n\t\t\treturn 0;\n\t\tverbose(env, \"Syscall programs can only be sleepable\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (prog->aux->sleepable && !can_be_sleepable(prog)) {\n\t\tverbose(env, \"Only fentry/fexit/fmod_ret, lsm, iter, uprobe, and struct_ops programs can be sleepable\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (prog->type == BPF_PROG_TYPE_STRUCT_OPS)\n\t\treturn check_struct_ops_btf_id(env);\n\n\tif (prog->type != BPF_PROG_TYPE_TRACING &&\n\t    prog->type != BPF_PROG_TYPE_LSM &&\n\t    prog->type != BPF_PROG_TYPE_EXT)\n\t\treturn 0;\n\n\tret = bpf_check_attach_target(&env->log, prog, tgt_prog, btf_id, &tgt_info);\n\tif (ret)\n\t\treturn ret;\n\n\tif (tgt_prog && prog->type == BPF_PROG_TYPE_EXT) {\n\t\t \n\t\tenv->ops = bpf_verifier_ops[tgt_prog->type];\n\t\tprog->expected_attach_type = tgt_prog->expected_attach_type;\n\t}\n\n\t \n\tprog->aux->attach_func_proto = tgt_info.tgt_type;\n\tprog->aux->attach_func_name = tgt_info.tgt_name;\n\tprog->aux->mod = tgt_info.tgt_mod;\n\n\tif (tgt_prog) {\n\t\tprog->aux->saved_dst_prog_type = tgt_prog->type;\n\t\tprog->aux->saved_dst_attach_type = tgt_prog->expected_attach_type;\n\t}\n\n\tif (prog->expected_attach_type == BPF_TRACE_RAW_TP) {\n\t\tprog->aux->attach_btf_trace = true;\n\t\treturn 0;\n\t} else if (prog->expected_attach_type == BPF_TRACE_ITER) {\n\t\tif (!bpf_iter_prog_supported(prog))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\tif (prog->type == BPF_PROG_TYPE_LSM) {\n\t\tret = bpf_lsm_verify_prog(&env->log, prog);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t} else if (prog->type == BPF_PROG_TYPE_TRACING &&\n\t\t   btf_id_set_contains(&btf_id_deny, btf_id)) {\n\t\treturn -EINVAL;\n\t}\n\n\tkey = bpf_trampoline_compute_key(tgt_prog, prog->aux->attach_btf, btf_id);\n\ttr = bpf_trampoline_get(key, &tgt_info);\n\tif (!tr)\n\t\treturn -ENOMEM;\n\n\tif (tgt_prog && tgt_prog->aux->tail_call_reachable)\n\t\ttr->flags = BPF_TRAMP_F_TAIL_CALL_CTX;\n\n\tprog->aux->dst_trampoline = tr;\n\treturn 0;\n}\n\nstruct btf *bpf_get_btf_vmlinux(void)\n{\n\tif (!btf_vmlinux && IS_ENABLED(CONFIG_DEBUG_INFO_BTF)) {\n\t\tmutex_lock(&bpf_verifier_lock);\n\t\tif (!btf_vmlinux)\n\t\t\tbtf_vmlinux = btf_parse_vmlinux();\n\t\tmutex_unlock(&bpf_verifier_lock);\n\t}\n\treturn btf_vmlinux;\n}\n\nint bpf_check(struct bpf_prog **prog, union bpf_attr *attr, bpfptr_t uattr, __u32 uattr_size)\n{\n\tu64 start_time = ktime_get_ns();\n\tstruct bpf_verifier_env *env;\n\tint i, len, ret = -EINVAL, err;\n\tu32 log_true_size;\n\tbool is_priv;\n\n\t \n\tif (ARRAY_SIZE(bpf_verifier_ops) == 0)\n\t\treturn -EINVAL;\n\n\t \n\tenv = kzalloc(sizeof(struct bpf_verifier_env), GFP_KERNEL);\n\tif (!env)\n\t\treturn -ENOMEM;\n\n\tenv->bt.env = env;\n\n\tlen = (*prog)->len;\n\tenv->insn_aux_data =\n\t\tvzalloc(array_size(sizeof(struct bpf_insn_aux_data), len));\n\tret = -ENOMEM;\n\tif (!env->insn_aux_data)\n\t\tgoto err_free_env;\n\tfor (i = 0; i < len; i++)\n\t\tenv->insn_aux_data[i].orig_idx = i;\n\tenv->prog = *prog;\n\tenv->ops = bpf_verifier_ops[env->prog->type];\n\tenv->fd_array = make_bpfptr(attr->fd_array, uattr.is_kernel);\n\tis_priv = bpf_capable();\n\n\tbpf_get_btf_vmlinux();\n\n\t \n\tif (!is_priv)\n\t\tmutex_lock(&bpf_verifier_lock);\n\n\t \n\tret = bpf_vlog_init(&env->log, attr->log_level,\n\t\t\t    (char __user *) (unsigned long) attr->log_buf,\n\t\t\t    attr->log_size);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tmark_verifier_state_clean(env);\n\n\tif (IS_ERR(btf_vmlinux)) {\n\t\t \n\t\tverbose(env, \"in-kernel BTF is malformed\\n\");\n\t\tret = PTR_ERR(btf_vmlinux);\n\t\tgoto skip_full_check;\n\t}\n\n\tenv->strict_alignment = !!(attr->prog_flags & BPF_F_STRICT_ALIGNMENT);\n\tif (!IS_ENABLED(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS))\n\t\tenv->strict_alignment = true;\n\tif (attr->prog_flags & BPF_F_ANY_ALIGNMENT)\n\t\tenv->strict_alignment = false;\n\n\tenv->allow_ptr_leaks = bpf_allow_ptr_leaks();\n\tenv->allow_uninit_stack = bpf_allow_uninit_stack();\n\tenv->bypass_spec_v1 = bpf_bypass_spec_v1();\n\tenv->bypass_spec_v4 = bpf_bypass_spec_v4();\n\tenv->bpf_capable = bpf_capable();\n\n\tif (is_priv)\n\t\tenv->test_state_freq = attr->prog_flags & BPF_F_TEST_STATE_FREQ;\n\n\tenv->explored_states = kvcalloc(state_htab_size(env),\n\t\t\t\t       sizeof(struct bpf_verifier_state_list *),\n\t\t\t\t       GFP_USER);\n\tret = -ENOMEM;\n\tif (!env->explored_states)\n\t\tgoto skip_full_check;\n\n\tret = add_subprog_and_kfunc(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_subprogs(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_btf_info(env, attr, uattr);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = check_attach_btf_id(env);\n\tif (ret)\n\t\tgoto skip_full_check;\n\n\tret = resolve_pseudo_ldimm64(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tif (bpf_prog_is_offloaded(env->prog->aux)) {\n\t\tret = bpf_prog_offload_verifier_prep(env->prog);\n\t\tif (ret)\n\t\t\tgoto skip_full_check;\n\t}\n\n\tret = check_cfg(env);\n\tif (ret < 0)\n\t\tgoto skip_full_check;\n\n\tret = do_check_subprogs(env);\n\tret = ret ?: do_check_main(env);\n\n\tif (ret == 0 && bpf_prog_is_offloaded(env->prog->aux))\n\t\tret = bpf_prog_offload_finalize(env);\n\nskip_full_check:\n\tkvfree(env->explored_states);\n\n\tif (ret == 0)\n\t\tret = check_max_stack_depth(env);\n\n\t \n\tif (ret == 0)\n\t\tret = optimize_bpf_loop(env);\n\n\tif (is_priv) {\n\t\tif (ret == 0)\n\t\t\topt_hard_wire_dead_code_branches(env);\n\t\tif (ret == 0)\n\t\t\tret = opt_remove_dead_code(env);\n\t\tif (ret == 0)\n\t\t\tret = opt_remove_nops(env);\n\t} else {\n\t\tif (ret == 0)\n\t\t\tsanitize_dead_code(env);\n\t}\n\n\tif (ret == 0)\n\t\t \n\t\tret = convert_ctx_accesses(env);\n\n\tif (ret == 0)\n\t\tret = do_misc_fixups(env);\n\n\t \n\tif (ret == 0 && !bpf_prog_is_offloaded(env->prog->aux)) {\n\t\tret = opt_subreg_zext_lo32_rnd_hi32(env, attr);\n\t\tenv->prog->aux->verifier_zext = bpf_jit_needs_zext() ? !ret\n\t\t\t\t\t\t\t\t     : false;\n\t}\n\n\tif (ret == 0)\n\t\tret = fixup_call_args(env);\n\n\tenv->verification_time = ktime_get_ns() - start_time;\n\tprint_verification_stats(env);\n\tenv->prog->aux->verified_insns = env->insn_processed;\n\n\t \n\terr = bpf_vlog_finalize(&env->log, &log_true_size);\n\tif (err)\n\t\tret = err;\n\n\tif (uattr_size >= offsetofend(union bpf_attr, log_true_size) &&\n\t    copy_to_bpfptr_offset(uattr, offsetof(union bpf_attr, log_true_size),\n\t\t\t\t  &log_true_size, sizeof(log_true_size))) {\n\t\tret = -EFAULT;\n\t\tgoto err_release_maps;\n\t}\n\n\tif (ret)\n\t\tgoto err_release_maps;\n\n\tif (env->used_map_cnt) {\n\t\t \n\t\tenv->prog->aux->used_maps = kmalloc_array(env->used_map_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_maps[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\n\t\tif (!env->prog->aux->used_maps) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_maps, env->used_maps,\n\t\t       sizeof(env->used_maps[0]) * env->used_map_cnt);\n\t\tenv->prog->aux->used_map_cnt = env->used_map_cnt;\n\t}\n\tif (env->used_btf_cnt) {\n\t\t \n\t\tenv->prog->aux->used_btfs = kmalloc_array(env->used_btf_cnt,\n\t\t\t\t\t\t\t  sizeof(env->used_btfs[0]),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!env->prog->aux->used_btfs) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_release_maps;\n\t\t}\n\n\t\tmemcpy(env->prog->aux->used_btfs, env->used_btfs,\n\t\t       sizeof(env->used_btfs[0]) * env->used_btf_cnt);\n\t\tenv->prog->aux->used_btf_cnt = env->used_btf_cnt;\n\t}\n\tif (env->used_map_cnt || env->used_btf_cnt) {\n\t\t \n\t\tconvert_pseudo_ld_imm64(env);\n\t}\n\n\tadjust_btf_func(env);\n\nerr_release_maps:\n\tif (!env->prog->aux->used_maps)\n\t\t \n\t\trelease_maps(env);\n\tif (!env->prog->aux->used_btfs)\n\t\trelease_btfs(env);\n\n\t \n\tif (env->prog->type == BPF_PROG_TYPE_EXT)\n\t\tenv->prog->expected_attach_type = 0;\n\n\t*prog = env->prog;\nerr_unlock:\n\tif (!is_priv)\n\t\tmutex_unlock(&bpf_verifier_lock);\n\tvfree(env->insn_aux_data);\nerr_free_env:\n\tkfree(env);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}