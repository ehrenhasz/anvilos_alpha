{
  "module_name": "queue_stack_maps.c",
  "hash_id": "7174958325c2b53db0ec2cf0d95a35fc99708895d3566c48066cd46a97b8187b",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/queue_stack_maps.c",
  "human_readable_source": "\n \n#include <linux/bpf.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/btf_ids.h>\n#include \"percpu_freelist.h\"\n\n#define QUEUE_STACK_CREATE_FLAG_MASK \\\n\t(BPF_F_NUMA_NODE | BPF_F_ACCESS_MASK)\n\nstruct bpf_queue_stack {\n\tstruct bpf_map map;\n\traw_spinlock_t lock;\n\tu32 head, tail;\n\tu32 size;  \n\n\tchar elements[] __aligned(8);\n};\n\nstatic struct bpf_queue_stack *bpf_queue_stack(struct bpf_map *map)\n{\n\treturn container_of(map, struct bpf_queue_stack, map);\n}\n\nstatic bool queue_stack_map_is_empty(struct bpf_queue_stack *qs)\n{\n\treturn qs->head == qs->tail;\n}\n\nstatic bool queue_stack_map_is_full(struct bpf_queue_stack *qs)\n{\n\tu32 head = qs->head + 1;\n\n\tif (unlikely(head >= qs->size))\n\t\thead = 0;\n\n\treturn head == qs->tail;\n}\n\n \nstatic int queue_stack_map_alloc_check(union bpf_attr *attr)\n{\n\t \n\tif (attr->max_entries == 0 || attr->key_size != 0 ||\n\t    attr->value_size == 0 ||\n\t    attr->map_flags & ~QUEUE_STACK_CREATE_FLAG_MASK ||\n\t    !bpf_map_flags_access_ok(attr->map_flags))\n\t\treturn -EINVAL;\n\n\tif (attr->value_size > KMALLOC_MAX_SIZE)\n\t\t \n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nstatic struct bpf_map *queue_stack_map_alloc(union bpf_attr *attr)\n{\n\tint numa_node = bpf_map_attr_numa_node(attr);\n\tstruct bpf_queue_stack *qs;\n\tu64 size, queue_size;\n\n\tsize = (u64) attr->max_entries + 1;\n\tqueue_size = sizeof(*qs) + size * attr->value_size;\n\n\tqs = bpf_map_area_alloc(queue_size, numa_node);\n\tif (!qs)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&qs->map, attr);\n\n\tqs->size = size;\n\n\traw_spin_lock_init(&qs->lock);\n\n\treturn &qs->map;\n}\n\n \nstatic void queue_stack_map_free(struct bpf_map *map)\n{\n\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);\n\n\tbpf_map_area_free(qs);\n}\n\nstatic long __queue_map_get(struct bpf_map *map, void *value, bool delete)\n{\n\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);\n\tunsigned long flags;\n\tint err = 0;\n\tvoid *ptr;\n\n\tif (in_nmi()) {\n\t\tif (!raw_spin_trylock_irqsave(&qs->lock, flags))\n\t\t\treturn -EBUSY;\n\t} else {\n\t\traw_spin_lock_irqsave(&qs->lock, flags);\n\t}\n\n\tif (queue_stack_map_is_empty(qs)) {\n\t\tmemset(value, 0, qs->map.value_size);\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tptr = &qs->elements[qs->tail * qs->map.value_size];\n\tmemcpy(value, ptr, qs->map.value_size);\n\n\tif (delete) {\n\t\tif (unlikely(++qs->tail >= qs->size))\n\t\t\tqs->tail = 0;\n\t}\n\nout:\n\traw_spin_unlock_irqrestore(&qs->lock, flags);\n\treturn err;\n}\n\n\nstatic long __stack_map_get(struct bpf_map *map, void *value, bool delete)\n{\n\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);\n\tunsigned long flags;\n\tint err = 0;\n\tvoid *ptr;\n\tu32 index;\n\n\tif (in_nmi()) {\n\t\tif (!raw_spin_trylock_irqsave(&qs->lock, flags))\n\t\t\treturn -EBUSY;\n\t} else {\n\t\traw_spin_lock_irqsave(&qs->lock, flags);\n\t}\n\n\tif (queue_stack_map_is_empty(qs)) {\n\t\tmemset(value, 0, qs->map.value_size);\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tindex = qs->head - 1;\n\tif (unlikely(index >= qs->size))\n\t\tindex = qs->size - 1;\n\n\tptr = &qs->elements[index * qs->map.value_size];\n\tmemcpy(value, ptr, qs->map.value_size);\n\n\tif (delete)\n\t\tqs->head = index;\n\nout:\n\traw_spin_unlock_irqrestore(&qs->lock, flags);\n\treturn err;\n}\n\n \nstatic long queue_map_peek_elem(struct bpf_map *map, void *value)\n{\n\treturn __queue_map_get(map, value, false);\n}\n\n \nstatic long stack_map_peek_elem(struct bpf_map *map, void *value)\n{\n\treturn __stack_map_get(map, value, false);\n}\n\n \nstatic long queue_map_pop_elem(struct bpf_map *map, void *value)\n{\n\treturn __queue_map_get(map, value, true);\n}\n\n \nstatic long stack_map_pop_elem(struct bpf_map *map, void *value)\n{\n\treturn __stack_map_get(map, value, true);\n}\n\n \nstatic long queue_stack_map_push_elem(struct bpf_map *map, void *value,\n\t\t\t\t      u64 flags)\n{\n\tstruct bpf_queue_stack *qs = bpf_queue_stack(map);\n\tunsigned long irq_flags;\n\tint err = 0;\n\tvoid *dst;\n\n\t \n\tbool replace = (flags & BPF_EXIST);\n\n\t \n\tif (flags & BPF_NOEXIST || flags > BPF_EXIST)\n\t\treturn -EINVAL;\n\n\tif (in_nmi()) {\n\t\tif (!raw_spin_trylock_irqsave(&qs->lock, irq_flags))\n\t\t\treturn -EBUSY;\n\t} else {\n\t\traw_spin_lock_irqsave(&qs->lock, irq_flags);\n\t}\n\n\tif (queue_stack_map_is_full(qs)) {\n\t\tif (!replace) {\n\t\t\terr = -E2BIG;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (unlikely(++qs->tail >= qs->size))\n\t\t\tqs->tail = 0;\n\t}\n\n\tdst = &qs->elements[qs->head * qs->map.value_size];\n\tmemcpy(dst, value, qs->map.value_size);\n\n\tif (unlikely(++qs->head >= qs->size))\n\t\tqs->head = 0;\n\nout:\n\traw_spin_unlock_irqrestore(&qs->lock, irq_flags);\n\treturn err;\n}\n\n \nstatic void *queue_stack_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\treturn NULL;\n}\n\n \nstatic long queue_stack_map_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t\tvoid *value, u64 flags)\n{\n\treturn -EINVAL;\n}\n\n \nstatic long queue_stack_map_delete_elem(struct bpf_map *map, void *key)\n{\n\treturn -EINVAL;\n}\n\n \nstatic int queue_stack_map_get_next_key(struct bpf_map *map, void *key,\n\t\t\t\t\tvoid *next_key)\n{\n\treturn -EINVAL;\n}\n\nstatic u64 queue_stack_map_mem_usage(const struct bpf_map *map)\n{\n\tu64 usage = sizeof(struct bpf_queue_stack);\n\n\tusage += ((u64)map->max_entries + 1) * map->value_size;\n\treturn usage;\n}\n\nBTF_ID_LIST_SINGLE(queue_map_btf_ids, struct, bpf_queue_stack)\nconst struct bpf_map_ops queue_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = queue_stack_map_alloc_check,\n\t.map_alloc = queue_stack_map_alloc,\n\t.map_free = queue_stack_map_free,\n\t.map_lookup_elem = queue_stack_map_lookup_elem,\n\t.map_update_elem = queue_stack_map_update_elem,\n\t.map_delete_elem = queue_stack_map_delete_elem,\n\t.map_push_elem = queue_stack_map_push_elem,\n\t.map_pop_elem = queue_map_pop_elem,\n\t.map_peek_elem = queue_map_peek_elem,\n\t.map_get_next_key = queue_stack_map_get_next_key,\n\t.map_mem_usage = queue_stack_map_mem_usage,\n\t.map_btf_id = &queue_map_btf_ids[0],\n};\n\nconst struct bpf_map_ops stack_map_ops = {\n\t.map_meta_equal = bpf_map_meta_equal,\n\t.map_alloc_check = queue_stack_map_alloc_check,\n\t.map_alloc = queue_stack_map_alloc,\n\t.map_free = queue_stack_map_free,\n\t.map_lookup_elem = queue_stack_map_lookup_elem,\n\t.map_update_elem = queue_stack_map_update_elem,\n\t.map_delete_elem = queue_stack_map_delete_elem,\n\t.map_push_elem = queue_stack_map_push_elem,\n\t.map_pop_elem = stack_map_pop_elem,\n\t.map_peek_elem = stack_map_peek_elem,\n\t.map_get_next_key = queue_stack_map_get_next_key,\n\t.map_mem_usage = queue_stack_map_mem_usage,\n\t.map_btf_id = &queue_map_btf_ids[0],\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}