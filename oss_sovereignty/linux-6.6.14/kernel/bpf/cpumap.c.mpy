{
  "module_name": "cpumap.c",
  "hash_id": "53aa4d41cba843023fb381a73b393114a437b0fede615c34e6089835ed7c9857",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/cpumap.c",
  "human_readable_source": "\n \n\n \n \n#include <linux/bitops.h>\n#include <linux/bpf.h>\n#include <linux/filter.h>\n#include <linux/ptr_ring.h>\n#include <net/xdp.h>\n\n#include <linux/sched.h>\n#include <linux/workqueue.h>\n#include <linux/kthread.h>\n#include <linux/completion.h>\n#include <trace/events/xdp.h>\n#include <linux/btf_ids.h>\n\n#include <linux/netdevice.h>    \n#include <linux/etherdevice.h>  \n\n \n\n#define CPU_MAP_BULK_SIZE 8   \nstruct bpf_cpu_map_entry;\nstruct bpf_cpu_map;\n\nstruct xdp_bulk_queue {\n\tvoid *q[CPU_MAP_BULK_SIZE];\n\tstruct list_head flush_node;\n\tstruct bpf_cpu_map_entry *obj;\n\tunsigned int count;\n};\n\n \nstruct bpf_cpu_map_entry {\n\tu32 cpu;     \n\tint map_id;  \n\n\t \n\tstruct xdp_bulk_queue __percpu *bulkq;\n\n\t \n\tstruct ptr_ring *queue;\n\tstruct task_struct *kthread;\n\n\tstruct bpf_cpumap_val value;\n\tstruct bpf_prog *prog;\n\n\tstruct completion kthread_running;\n\tstruct rcu_work free_work;\n};\n\nstruct bpf_cpu_map {\n\tstruct bpf_map map;\n\t \n\tstruct bpf_cpu_map_entry __rcu **cpu_map;\n};\n\nstatic DEFINE_PER_CPU(struct list_head, cpu_map_flush_list);\n\nstatic struct bpf_map *cpu_map_alloc(union bpf_attr *attr)\n{\n\tu32 value_size = attr->value_size;\n\tstruct bpf_cpu_map *cmap;\n\n\t \n\tif (attr->max_entries == 0 || attr->key_size != 4 ||\n\t    (value_size != offsetofend(struct bpf_cpumap_val, qsize) &&\n\t     value_size != offsetofend(struct bpf_cpumap_val, bpf_prog.fd)) ||\n\t    attr->map_flags & ~BPF_F_NUMA_NODE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (attr->max_entries > NR_CPUS)\n\t\treturn ERR_PTR(-E2BIG);\n\n\tcmap = bpf_map_area_alloc(sizeof(*cmap), NUMA_NO_NODE);\n\tif (!cmap)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpf_map_init_from_attr(&cmap->map, attr);\n\n\t \n\tcmap->cpu_map = bpf_map_area_alloc(cmap->map.max_entries *\n\t\t\t\t\t   sizeof(struct bpf_cpu_map_entry *),\n\t\t\t\t\t   cmap->map.numa_node);\n\tif (!cmap->cpu_map) {\n\t\tbpf_map_area_free(cmap);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn &cmap->map;\n}\n\nstatic void __cpu_map_ring_cleanup(struct ptr_ring *ring)\n{\n\t \n\tvoid *ptr;\n\n\twhile ((ptr = ptr_ring_consume(ring))) {\n\t\tWARN_ON_ONCE(1);\n\t\tif (unlikely(__ptr_test_bit(0, &ptr))) {\n\t\t\t__ptr_clear_bit(0, &ptr);\n\t\t\tkfree_skb(ptr);\n\t\t\tcontinue;\n\t\t}\n\t\txdp_return_frame(ptr);\n\t}\n}\n\nstatic void cpu_map_bpf_prog_run_skb(struct bpf_cpu_map_entry *rcpu,\n\t\t\t\t     struct list_head *listp,\n\t\t\t\t     struct xdp_cpumap_stats *stats)\n{\n\tstruct sk_buff *skb, *tmp;\n\tstruct xdp_buff xdp;\n\tu32 act;\n\tint err;\n\n\tlist_for_each_entry_safe(skb, tmp, listp, list) {\n\t\tact = bpf_prog_run_generic_xdp(skb, &xdp, rcpu->prog);\n\t\tswitch (act) {\n\t\tcase XDP_PASS:\n\t\t\tbreak;\n\t\tcase XDP_REDIRECT:\n\t\t\tskb_list_del_init(skb);\n\t\t\terr = xdp_do_generic_redirect(skb->dev, skb, &xdp,\n\t\t\t\t\t\t      rcpu->prog);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tstats->drop++;\n\t\t\t} else {\n\t\t\t\tstats->redirect++;\n\t\t\t}\n\t\t\treturn;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(NULL, rcpu->prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_ABORTED:\n\t\t\ttrace_xdp_exception(skb->dev, rcpu->prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_DROP:\n\t\t\tskb_list_del_init(skb);\n\t\t\tkfree_skb(skb);\n\t\t\tstats->drop++;\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic int cpu_map_bpf_prog_run_xdp(struct bpf_cpu_map_entry *rcpu,\n\t\t\t\t    void **frames, int n,\n\t\t\t\t    struct xdp_cpumap_stats *stats)\n{\n\tstruct xdp_rxq_info rxq;\n\tstruct xdp_buff xdp;\n\tint i, nframes = 0;\n\n\txdp_set_return_frame_no_direct();\n\txdp.rxq = &rxq;\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\t\tu32 act;\n\t\tint err;\n\n\t\trxq.dev = xdpf->dev_rx;\n\t\trxq.mem = xdpf->mem;\n\t\t \n\n\t\txdp_convert_frame_to_buff(xdpf, &xdp);\n\n\t\tact = bpf_prog_run_xdp(rcpu->prog, &xdp);\n\t\tswitch (act) {\n\t\tcase XDP_PASS:\n\t\t\terr = xdp_update_frame_from_buff(&xdp, xdpf);\n\t\t\tif (err < 0) {\n\t\t\t\txdp_return_frame(xdpf);\n\t\t\t\tstats->drop++;\n\t\t\t} else {\n\t\t\t\tframes[nframes++] = xdpf;\n\t\t\t\tstats->pass++;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase XDP_REDIRECT:\n\t\t\terr = xdp_do_redirect(xdpf->dev_rx, &xdp,\n\t\t\t\t\t      rcpu->prog);\n\t\t\tif (unlikely(err)) {\n\t\t\t\txdp_return_frame(xdpf);\n\t\t\t\tstats->drop++;\n\t\t\t} else {\n\t\t\t\tstats->redirect++;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbpf_warn_invalid_xdp_action(NULL, rcpu->prog, act);\n\t\t\tfallthrough;\n\t\tcase XDP_DROP:\n\t\t\txdp_return_frame(xdpf);\n\t\t\tstats->drop++;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\txdp_clear_return_frame_no_direct();\n\n\treturn nframes;\n}\n\n#define CPUMAP_BATCH 8\n\nstatic int cpu_map_bpf_prog_run(struct bpf_cpu_map_entry *rcpu, void **frames,\n\t\t\t\tint xdp_n, struct xdp_cpumap_stats *stats,\n\t\t\t\tstruct list_head *list)\n{\n\tint nframes;\n\n\tif (!rcpu->prog)\n\t\treturn xdp_n;\n\n\trcu_read_lock_bh();\n\n\tnframes = cpu_map_bpf_prog_run_xdp(rcpu, frames, xdp_n, stats);\n\n\tif (stats->redirect)\n\t\txdp_do_flush();\n\n\tif (unlikely(!list_empty(list)))\n\t\tcpu_map_bpf_prog_run_skb(rcpu, list, stats);\n\n\trcu_read_unlock_bh();  \n\n\treturn nframes;\n}\n\nstatic int cpu_map_kthread_run(void *data)\n{\n\tstruct bpf_cpu_map_entry *rcpu = data;\n\n\tcomplete(&rcpu->kthread_running);\n\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t \n\twhile (!kthread_should_stop() || !__ptr_ring_empty(rcpu->queue)) {\n\t\tstruct xdp_cpumap_stats stats = {};  \n\t\tunsigned int kmem_alloc_drops = 0, sched = 0;\n\t\tgfp_t gfp = __GFP_ZERO | GFP_ATOMIC;\n\t\tint i, n, m, nframes, xdp_n;\n\t\tvoid *frames[CPUMAP_BATCH];\n\t\tvoid *skbs[CPUMAP_BATCH];\n\t\tLIST_HEAD(list);\n\n\t\t \n\t\tif (__ptr_ring_empty(rcpu->queue)) {\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\t \n\t\t\tif (__ptr_ring_empty(rcpu->queue)) {\n\t\t\t\tschedule();\n\t\t\t\tsched = 1;\n\t\t\t} else {\n\t\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\t}\n\t\t} else {\n\t\t\tsched = cond_resched();\n\t\t}\n\n\t\t \n\t\tn = __ptr_ring_consume_batched(rcpu->queue, frames,\n\t\t\t\t\t       CPUMAP_BATCH);\n\t\tfor (i = 0, xdp_n = 0; i < n; i++) {\n\t\t\tvoid *f = frames[i];\n\t\t\tstruct page *page;\n\n\t\t\tif (unlikely(__ptr_test_bit(0, &f))) {\n\t\t\t\tstruct sk_buff *skb = f;\n\n\t\t\t\t__ptr_clear_bit(0, &skb);\n\t\t\t\tlist_add_tail(&skb->list, &list);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tframes[xdp_n++] = f;\n\t\t\tpage = virt_to_page(f);\n\n\t\t\t \n\t\t\tprefetchw(page);\n\t\t}\n\n\t\t \n\t\tnframes = cpu_map_bpf_prog_run(rcpu, frames, xdp_n, &stats, &list);\n\t\tif (nframes) {\n\t\t\tm = kmem_cache_alloc_bulk(skbuff_cache, gfp, nframes, skbs);\n\t\t\tif (unlikely(m == 0)) {\n\t\t\t\tfor (i = 0; i < nframes; i++)\n\t\t\t\t\tskbs[i] = NULL;  \n\t\t\t\tkmem_alloc_drops += nframes;\n\t\t\t}\n\t\t}\n\n\t\tlocal_bh_disable();\n\t\tfor (i = 0; i < nframes; i++) {\n\t\t\tstruct xdp_frame *xdpf = frames[i];\n\t\t\tstruct sk_buff *skb = skbs[i];\n\n\t\t\tskb = __xdp_build_skb_from_frame(xdpf, skb,\n\t\t\t\t\t\t\t xdpf->dev_rx);\n\t\t\tif (!skb) {\n\t\t\t\txdp_return_frame(xdpf);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tlist_add_tail(&skb->list, &list);\n\t\t}\n\t\tnetif_receive_skb_list(&list);\n\n\t\t \n\t\ttrace_xdp_cpumap_kthread(rcpu->map_id, n, kmem_alloc_drops,\n\t\t\t\t\t sched, &stats);\n\n\t\tlocal_bh_enable();  \n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn 0;\n}\n\nstatic int __cpu_map_load_bpf_program(struct bpf_cpu_map_entry *rcpu,\n\t\t\t\t      struct bpf_map *map, int fd)\n{\n\tstruct bpf_prog *prog;\n\n\tprog = bpf_prog_get_type(fd, BPF_PROG_TYPE_XDP);\n\tif (IS_ERR(prog))\n\t\treturn PTR_ERR(prog);\n\n\tif (prog->expected_attach_type != BPF_XDP_CPUMAP ||\n\t    !bpf_prog_map_compatible(map, prog)) {\n\t\tbpf_prog_put(prog);\n\t\treturn -EINVAL;\n\t}\n\n\trcpu->value.bpf_prog.id = prog->aux->id;\n\trcpu->prog = prog;\n\n\treturn 0;\n}\n\nstatic struct bpf_cpu_map_entry *\n__cpu_map_entry_alloc(struct bpf_map *map, struct bpf_cpumap_val *value,\n\t\t      u32 cpu)\n{\n\tint numa, err, i, fd = value->bpf_prog.fd;\n\tgfp_t gfp = GFP_KERNEL | __GFP_NOWARN;\n\tstruct bpf_cpu_map_entry *rcpu;\n\tstruct xdp_bulk_queue *bq;\n\n\t \n\tnuma = cpu_to_node(cpu);\n\n\trcpu = bpf_map_kmalloc_node(map, sizeof(*rcpu), gfp | __GFP_ZERO, numa);\n\tif (!rcpu)\n\t\treturn NULL;\n\n\t \n\trcpu->bulkq = bpf_map_alloc_percpu(map, sizeof(*rcpu->bulkq),\n\t\t\t\t\t   sizeof(void *), gfp);\n\tif (!rcpu->bulkq)\n\t\tgoto free_rcu;\n\n\tfor_each_possible_cpu(i) {\n\t\tbq = per_cpu_ptr(rcpu->bulkq, i);\n\t\tbq->obj = rcpu;\n\t}\n\n\t \n\trcpu->queue = bpf_map_kmalloc_node(map, sizeof(*rcpu->queue), gfp,\n\t\t\t\t\t   numa);\n\tif (!rcpu->queue)\n\t\tgoto free_bulkq;\n\n\terr = ptr_ring_init(rcpu->queue, value->qsize, gfp);\n\tif (err)\n\t\tgoto free_queue;\n\n\trcpu->cpu    = cpu;\n\trcpu->map_id = map->id;\n\trcpu->value.qsize  = value->qsize;\n\n\tif (fd > 0 && __cpu_map_load_bpf_program(rcpu, map, fd))\n\t\tgoto free_ptr_ring;\n\n\t \n\tinit_completion(&rcpu->kthread_running);\n\trcpu->kthread = kthread_create_on_node(cpu_map_kthread_run, rcpu, numa,\n\t\t\t\t\t       \"cpumap/%d/map:%d\", cpu,\n\t\t\t\t\t       map->id);\n\tif (IS_ERR(rcpu->kthread))\n\t\tgoto free_prog;\n\n\t \n\tkthread_bind(rcpu->kthread, cpu);\n\twake_up_process(rcpu->kthread);\n\n\t \n\twait_for_completion(&rcpu->kthread_running);\n\n\treturn rcpu;\n\nfree_prog:\n\tif (rcpu->prog)\n\t\tbpf_prog_put(rcpu->prog);\nfree_ptr_ring:\n\tptr_ring_cleanup(rcpu->queue, NULL);\nfree_queue:\n\tkfree(rcpu->queue);\nfree_bulkq:\n\tfree_percpu(rcpu->bulkq);\nfree_rcu:\n\tkfree(rcpu);\n\treturn NULL;\n}\n\nstatic void __cpu_map_entry_free(struct work_struct *work)\n{\n\tstruct bpf_cpu_map_entry *rcpu;\n\n\t \n\trcpu = container_of(to_rcu_work(work), struct bpf_cpu_map_entry, free_work);\n\n\t \n\tkthread_stop(rcpu->kthread);\n\n\tif (rcpu->prog)\n\t\tbpf_prog_put(rcpu->prog);\n\t \n\t__cpu_map_ring_cleanup(rcpu->queue);\n\tptr_ring_cleanup(rcpu->queue, NULL);\n\tkfree(rcpu->queue);\n\tfree_percpu(rcpu->bulkq);\n\tkfree(rcpu);\n}\n\n \nstatic void __cpu_map_entry_replace(struct bpf_cpu_map *cmap,\n\t\t\t\t    u32 key_cpu, struct bpf_cpu_map_entry *rcpu)\n{\n\tstruct bpf_cpu_map_entry *old_rcpu;\n\n\told_rcpu = unrcu_pointer(xchg(&cmap->cpu_map[key_cpu], RCU_INITIALIZER(rcpu)));\n\tif (old_rcpu) {\n\t\tINIT_RCU_WORK(&old_rcpu->free_work, __cpu_map_entry_free);\n\t\tqueue_rcu_work(system_wq, &old_rcpu->free_work);\n\t}\n}\n\nstatic long cpu_map_delete_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);\n\tu32 key_cpu = *(u32 *)key;\n\n\tif (key_cpu >= map->max_entries)\n\t\treturn -EINVAL;\n\n\t \n\t__cpu_map_entry_replace(cmap, key_cpu, NULL);\n\treturn 0;\n}\n\nstatic long cpu_map_update_elem(struct bpf_map *map, void *key, void *value,\n\t\t\t\tu64 map_flags)\n{\n\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);\n\tstruct bpf_cpumap_val cpumap_value = {};\n\tstruct bpf_cpu_map_entry *rcpu;\n\t \n\tu32 key_cpu = *(u32 *)key;\n\n\tmemcpy(&cpumap_value, value, map->value_size);\n\n\tif (unlikely(map_flags > BPF_EXIST))\n\t\treturn -EINVAL;\n\tif (unlikely(key_cpu >= cmap->map.max_entries))\n\t\treturn -E2BIG;\n\tif (unlikely(map_flags == BPF_NOEXIST))\n\t\treturn -EEXIST;\n\tif (unlikely(cpumap_value.qsize > 16384))  \n\t\treturn -EOVERFLOW;\n\n\t \n\tif (key_cpu >= nr_cpumask_bits || !cpu_possible(key_cpu))\n\t\treturn -ENODEV;\n\n\tif (cpumap_value.qsize == 0) {\n\t\trcpu = NULL;  \n\t} else {\n\t\t \n\t\trcpu = __cpu_map_entry_alloc(map, &cpumap_value, key_cpu);\n\t\tif (!rcpu)\n\t\t\treturn -ENOMEM;\n\t}\n\trcu_read_lock();\n\t__cpu_map_entry_replace(cmap, key_cpu, rcpu);\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic void cpu_map_free(struct bpf_map *map)\n{\n\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);\n\tu32 i;\n\n\t \n\tsynchronize_rcu();\n\n\t \n\tfor (i = 0; i < cmap->map.max_entries; i++) {\n\t\tstruct bpf_cpu_map_entry *rcpu;\n\n\t\trcpu = rcu_dereference_raw(cmap->cpu_map[i]);\n\t\tif (!rcpu)\n\t\t\tcontinue;\n\n\t\t \n\t\t__cpu_map_entry_free(&rcpu->free_work.work);\n\t}\n\tbpf_map_area_free(cmap->cpu_map);\n\tbpf_map_area_free(cmap);\n}\n\n \nstatic void *__cpu_map_lookup_elem(struct bpf_map *map, u32 key)\n{\n\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);\n\tstruct bpf_cpu_map_entry *rcpu;\n\n\tif (key >= map->max_entries)\n\t\treturn NULL;\n\n\trcpu = rcu_dereference_check(cmap->cpu_map[key],\n\t\t\t\t     rcu_read_lock_bh_held());\n\treturn rcpu;\n}\n\nstatic void *cpu_map_lookup_elem(struct bpf_map *map, void *key)\n{\n\tstruct bpf_cpu_map_entry *rcpu =\n\t\t__cpu_map_lookup_elem(map, *(u32 *)key);\n\n\treturn rcpu ? &rcpu->value : NULL;\n}\n\nstatic int cpu_map_get_next_key(struct bpf_map *map, void *key, void *next_key)\n{\n\tstruct bpf_cpu_map *cmap = container_of(map, struct bpf_cpu_map, map);\n\tu32 index = key ? *(u32 *)key : U32_MAX;\n\tu32 *next = next_key;\n\n\tif (index >= cmap->map.max_entries) {\n\t\t*next = 0;\n\t\treturn 0;\n\t}\n\n\tif (index == cmap->map.max_entries - 1)\n\t\treturn -ENOENT;\n\t*next = index + 1;\n\treturn 0;\n}\n\nstatic long cpu_map_redirect(struct bpf_map *map, u64 index, u64 flags)\n{\n\treturn __bpf_xdp_redirect_map(map, index, flags, 0,\n\t\t\t\t      __cpu_map_lookup_elem);\n}\n\nstatic u64 cpu_map_mem_usage(const struct bpf_map *map)\n{\n\tu64 usage = sizeof(struct bpf_cpu_map);\n\n\t \n\tusage += (u64)map->max_entries * sizeof(struct bpf_cpu_map_entry *);\n\treturn usage;\n}\n\nBTF_ID_LIST_SINGLE(cpu_map_btf_ids, struct, bpf_cpu_map)\nconst struct bpf_map_ops cpu_map_ops = {\n\t.map_meta_equal\t\t= bpf_map_meta_equal,\n\t.map_alloc\t\t= cpu_map_alloc,\n\t.map_free\t\t= cpu_map_free,\n\t.map_delete_elem\t= cpu_map_delete_elem,\n\t.map_update_elem\t= cpu_map_update_elem,\n\t.map_lookup_elem\t= cpu_map_lookup_elem,\n\t.map_get_next_key\t= cpu_map_get_next_key,\n\t.map_check_btf\t\t= map_check_no_btf,\n\t.map_mem_usage\t\t= cpu_map_mem_usage,\n\t.map_btf_id\t\t= &cpu_map_btf_ids[0],\n\t.map_redirect\t\t= cpu_map_redirect,\n};\n\nstatic void bq_flush_to_queue(struct xdp_bulk_queue *bq)\n{\n\tstruct bpf_cpu_map_entry *rcpu = bq->obj;\n\tunsigned int processed = 0, drops = 0;\n\tconst int to_cpu = rcpu->cpu;\n\tstruct ptr_ring *q;\n\tint i;\n\n\tif (unlikely(!bq->count))\n\t\treturn;\n\n\tq = rcpu->queue;\n\tspin_lock(&q->producer_lock);\n\n\tfor (i = 0; i < bq->count; i++) {\n\t\tstruct xdp_frame *xdpf = bq->q[i];\n\t\tint err;\n\n\t\terr = __ptr_ring_produce(q, xdpf);\n\t\tif (err) {\n\t\t\tdrops++;\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\t}\n\t\tprocessed++;\n\t}\n\tbq->count = 0;\n\tspin_unlock(&q->producer_lock);\n\n\t__list_del_clearprev(&bq->flush_node);\n\n\t \n\ttrace_xdp_cpumap_enqueue(rcpu->map_id, processed, drops, to_cpu);\n}\n\n \nstatic void bq_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_frame *xdpf)\n{\n\tstruct list_head *flush_list = this_cpu_ptr(&cpu_map_flush_list);\n\tstruct xdp_bulk_queue *bq = this_cpu_ptr(rcpu->bulkq);\n\n\tif (unlikely(bq->count == CPU_MAP_BULK_SIZE))\n\t\tbq_flush_to_queue(bq);\n\n\t \n\tbq->q[bq->count++] = xdpf;\n\n\tif (!bq->flush_node.prev)\n\t\tlist_add(&bq->flush_node, flush_list);\n}\n\nint cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx)\n{\n\t \n\txdpf->dev_rx = dev_rx;\n\n\tbq_enqueue(rcpu, xdpf);\n\treturn 0;\n}\n\nint cpu_map_generic_redirect(struct bpf_cpu_map_entry *rcpu,\n\t\t\t     struct sk_buff *skb)\n{\n\tint ret;\n\n\t__skb_pull(skb, skb->mac_len);\n\tskb_set_redirected(skb, false);\n\t__ptr_set_bit(0, &skb);\n\n\tret = ptr_ring_produce(rcpu->queue, skb);\n\tif (ret < 0)\n\t\tgoto trace;\n\n\twake_up_process(rcpu->kthread);\ntrace:\n\ttrace_xdp_cpumap_enqueue(rcpu->map_id, !ret, !!ret, rcpu->cpu);\n\treturn ret;\n}\n\nvoid __cpu_map_flush(void)\n{\n\tstruct list_head *flush_list = this_cpu_ptr(&cpu_map_flush_list);\n\tstruct xdp_bulk_queue *bq, *tmp;\n\n\tlist_for_each_entry_safe(bq, tmp, flush_list, flush_node) {\n\t\tbq_flush_to_queue(bq);\n\n\t\t \n\t\twake_up_process(bq->obj->kthread);\n\t}\n}\n\nstatic int __init cpu_map_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_LIST_HEAD(&per_cpu(cpu_map_flush_list, cpu));\n\treturn 0;\n}\n\nsubsys_initcall(cpu_map_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}