{
  "module_name": "cgroup.c",
  "hash_id": "bd8169564899af8b7e310312fecf225b92b3a707d7afda24e39a7b04ae8a2f4c",
  "original_prompt": "Ingested from linux-6.6.14/kernel/bpf/cgroup.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/atomic.h>\n#include <linux/cgroup.h>\n#include <linux/filter.h>\n#include <linux/slab.h>\n#include <linux/sysctl.h>\n#include <linux/string.h>\n#include <linux/bpf.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/bpf_lsm.h>\n#include <linux/bpf_verifier.h>\n#include <net/sock.h>\n#include <net/bpf_sk_storage.h>\n\n#include \"../cgroup/cgroup-internal.h\"\n\nDEFINE_STATIC_KEY_ARRAY_FALSE(cgroup_bpf_enabled_key, MAX_CGROUP_BPF_ATTACH_TYPE);\nEXPORT_SYMBOL(cgroup_bpf_enabled_key);\n\n \nstatic __always_inline int\nbpf_prog_run_array_cg(const struct cgroup_bpf *cgrp,\n\t\t      enum cgroup_bpf_attach_type atype,\n\t\t      const void *ctx, bpf_prog_run_fn run_prog,\n\t\t      int retval, u32 *ret_flags)\n{\n\tconst struct bpf_prog_array_item *item;\n\tconst struct bpf_prog *prog;\n\tconst struct bpf_prog_array *array;\n\tstruct bpf_run_ctx *old_run_ctx;\n\tstruct bpf_cg_run_ctx run_ctx;\n\tu32 func_ret;\n\n\trun_ctx.retval = retval;\n\tmigrate_disable();\n\trcu_read_lock();\n\tarray = rcu_dereference(cgrp->effective[atype]);\n\titem = &array->items[0];\n\told_run_ctx = bpf_set_run_ctx(&run_ctx.run_ctx);\n\twhile ((prog = READ_ONCE(item->prog))) {\n\t\trun_ctx.prog_item = item;\n\t\tfunc_ret = run_prog(prog, ctx);\n\t\tif (ret_flags) {\n\t\t\t*(ret_flags) |= (func_ret >> 1);\n\t\t\tfunc_ret &= 1;\n\t\t}\n\t\tif (!func_ret && !IS_ERR_VALUE((long)run_ctx.retval))\n\t\t\trun_ctx.retval = -EPERM;\n\t\titem++;\n\t}\n\tbpf_reset_run_ctx(old_run_ctx);\n\trcu_read_unlock();\n\tmigrate_enable();\n\treturn run_ctx.retval;\n}\n\nunsigned int __cgroup_bpf_run_lsm_sock(const void *ctx,\n\t\t\t\t       const struct bpf_insn *insn)\n{\n\tconst struct bpf_prog *shim_prog;\n\tstruct sock *sk;\n\tstruct cgroup *cgrp;\n\tint ret = 0;\n\tu64 *args;\n\n\targs = (u64 *)ctx;\n\tsk = (void *)(unsigned long)args[0];\n\t \n\tshim_prog = (const struct bpf_prog *)((void *)insn - offsetof(struct bpf_prog, insnsi));\n\n\tcgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\tif (likely(cgrp))\n\t\tret = bpf_prog_run_array_cg(&cgrp->bpf,\n\t\t\t\t\t    shim_prog->aux->cgroup_atype,\n\t\t\t\t\t    ctx, bpf_prog_run, 0, NULL);\n\treturn ret;\n}\n\nunsigned int __cgroup_bpf_run_lsm_socket(const void *ctx,\n\t\t\t\t\t const struct bpf_insn *insn)\n{\n\tconst struct bpf_prog *shim_prog;\n\tstruct socket *sock;\n\tstruct cgroup *cgrp;\n\tint ret = 0;\n\tu64 *args;\n\n\targs = (u64 *)ctx;\n\tsock = (void *)(unsigned long)args[0];\n\t \n\tshim_prog = (const struct bpf_prog *)((void *)insn - offsetof(struct bpf_prog, insnsi));\n\n\tcgrp = sock_cgroup_ptr(&sock->sk->sk_cgrp_data);\n\tif (likely(cgrp))\n\t\tret = bpf_prog_run_array_cg(&cgrp->bpf,\n\t\t\t\t\t    shim_prog->aux->cgroup_atype,\n\t\t\t\t\t    ctx, bpf_prog_run, 0, NULL);\n\treturn ret;\n}\n\nunsigned int __cgroup_bpf_run_lsm_current(const void *ctx,\n\t\t\t\t\t  const struct bpf_insn *insn)\n{\n\tconst struct bpf_prog *shim_prog;\n\tstruct cgroup *cgrp;\n\tint ret = 0;\n\n\t \n\tshim_prog = (const struct bpf_prog *)((void *)insn - offsetof(struct bpf_prog, insnsi));\n\n\t \n\tcgrp = task_dfl_cgroup(current);\n\tif (likely(cgrp))\n\t\tret = bpf_prog_run_array_cg(&cgrp->bpf,\n\t\t\t\t\t    shim_prog->aux->cgroup_atype,\n\t\t\t\t\t    ctx, bpf_prog_run, 0, NULL);\n\treturn ret;\n}\n\n#ifdef CONFIG_BPF_LSM\nstruct cgroup_lsm_atype {\n\tu32 attach_btf_id;\n\tint refcnt;\n};\n\nstatic struct cgroup_lsm_atype cgroup_lsm_atype[CGROUP_LSM_NUM];\n\nstatic enum cgroup_bpf_attach_type\nbpf_cgroup_atype_find(enum bpf_attach_type attach_type, u32 attach_btf_id)\n{\n\tint i;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tif (attach_type != BPF_LSM_CGROUP)\n\t\treturn to_cgroup_bpf_attach_type(attach_type);\n\n\tfor (i = 0; i < ARRAY_SIZE(cgroup_lsm_atype); i++)\n\t\tif (cgroup_lsm_atype[i].attach_btf_id == attach_btf_id)\n\t\t\treturn CGROUP_LSM_START + i;\n\n\tfor (i = 0; i < ARRAY_SIZE(cgroup_lsm_atype); i++)\n\t\tif (cgroup_lsm_atype[i].attach_btf_id == 0)\n\t\t\treturn CGROUP_LSM_START + i;\n\n\treturn -E2BIG;\n\n}\n\nvoid bpf_cgroup_atype_get(u32 attach_btf_id, int cgroup_atype)\n{\n\tint i = cgroup_atype - CGROUP_LSM_START;\n\n\tlockdep_assert_held(&cgroup_mutex);\n\n\tWARN_ON_ONCE(cgroup_lsm_atype[i].attach_btf_id &&\n\t\t     cgroup_lsm_atype[i].attach_btf_id != attach_btf_id);\n\n\tcgroup_lsm_atype[i].attach_btf_id = attach_btf_id;\n\tcgroup_lsm_atype[i].refcnt++;\n}\n\nvoid bpf_cgroup_atype_put(int cgroup_atype)\n{\n\tint i = cgroup_atype - CGROUP_LSM_START;\n\n\tcgroup_lock();\n\tif (--cgroup_lsm_atype[i].refcnt <= 0)\n\t\tcgroup_lsm_atype[i].attach_btf_id = 0;\n\tWARN_ON_ONCE(cgroup_lsm_atype[i].refcnt < 0);\n\tcgroup_unlock();\n}\n#else\nstatic enum cgroup_bpf_attach_type\nbpf_cgroup_atype_find(enum bpf_attach_type attach_type, u32 attach_btf_id)\n{\n\tif (attach_type != BPF_LSM_CGROUP)\n\t\treturn to_cgroup_bpf_attach_type(attach_type);\n\treturn -EOPNOTSUPP;\n}\n#endif  \n\nvoid cgroup_bpf_offline(struct cgroup *cgrp)\n{\n\tcgroup_get(cgrp);\n\tpercpu_ref_kill(&cgrp->bpf.refcnt);\n}\n\nstatic void bpf_cgroup_storages_free(struct bpf_cgroup_storage *storages[])\n{\n\tenum bpf_cgroup_storage_type stype;\n\n\tfor_each_cgroup_storage_type(stype)\n\t\tbpf_cgroup_storage_free(storages[stype]);\n}\n\nstatic int bpf_cgroup_storages_alloc(struct bpf_cgroup_storage *storages[],\n\t\t\t\t     struct bpf_cgroup_storage *new_storages[],\n\t\t\t\t     enum bpf_attach_type type,\n\t\t\t\t     struct bpf_prog *prog,\n\t\t\t\t     struct cgroup *cgrp)\n{\n\tenum bpf_cgroup_storage_type stype;\n\tstruct bpf_cgroup_storage_key key;\n\tstruct bpf_map *map;\n\n\tkey.cgroup_inode_id = cgroup_id(cgrp);\n\tkey.attach_type = type;\n\n\tfor_each_cgroup_storage_type(stype) {\n\t\tmap = prog->aux->cgroup_storage[stype];\n\t\tif (!map)\n\t\t\tcontinue;\n\n\t\tstorages[stype] = cgroup_storage_lookup((void *)map, &key, false);\n\t\tif (storages[stype])\n\t\t\tcontinue;\n\n\t\tstorages[stype] = bpf_cgroup_storage_alloc(prog, stype);\n\t\tif (IS_ERR(storages[stype])) {\n\t\t\tbpf_cgroup_storages_free(new_storages);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tnew_storages[stype] = storages[stype];\n\t}\n\n\treturn 0;\n}\n\nstatic void bpf_cgroup_storages_assign(struct bpf_cgroup_storage *dst[],\n\t\t\t\t       struct bpf_cgroup_storage *src[])\n{\n\tenum bpf_cgroup_storage_type stype;\n\n\tfor_each_cgroup_storage_type(stype)\n\t\tdst[stype] = src[stype];\n}\n\nstatic void bpf_cgroup_storages_link(struct bpf_cgroup_storage *storages[],\n\t\t\t\t     struct cgroup *cgrp,\n\t\t\t\t     enum bpf_attach_type attach_type)\n{\n\tenum bpf_cgroup_storage_type stype;\n\n\tfor_each_cgroup_storage_type(stype)\n\t\tbpf_cgroup_storage_link(storages[stype], cgrp, attach_type);\n}\n\n \nstatic void bpf_cgroup_link_auto_detach(struct bpf_cgroup_link *link)\n{\n\tcgroup_put(link->cgroup);\n\tlink->cgroup = NULL;\n}\n\n \nstatic void cgroup_bpf_release(struct work_struct *work)\n{\n\tstruct cgroup *p, *cgrp = container_of(work, struct cgroup,\n\t\t\t\t\t       bpf.release_work);\n\tstruct bpf_prog_array *old_array;\n\tstruct list_head *storages = &cgrp->bpf.storages;\n\tstruct bpf_cgroup_storage *storage, *stmp;\n\n\tunsigned int atype;\n\n\tcgroup_lock();\n\n\tfor (atype = 0; atype < ARRAY_SIZE(cgrp->bpf.progs); atype++) {\n\t\tstruct hlist_head *progs = &cgrp->bpf.progs[atype];\n\t\tstruct bpf_prog_list *pl;\n\t\tstruct hlist_node *pltmp;\n\n\t\thlist_for_each_entry_safe(pl, pltmp, progs, node) {\n\t\t\thlist_del(&pl->node);\n\t\t\tif (pl->prog) {\n\t\t\t\tif (pl->prog->expected_attach_type == BPF_LSM_CGROUP)\n\t\t\t\t\tbpf_trampoline_unlink_cgroup_shim(pl->prog);\n\t\t\t\tbpf_prog_put(pl->prog);\n\t\t\t}\n\t\t\tif (pl->link) {\n\t\t\t\tif (pl->link->link.prog->expected_attach_type == BPF_LSM_CGROUP)\n\t\t\t\t\tbpf_trampoline_unlink_cgroup_shim(pl->link->link.prog);\n\t\t\t\tbpf_cgroup_link_auto_detach(pl->link);\n\t\t\t}\n\t\t\tkfree(pl);\n\t\t\tstatic_branch_dec(&cgroup_bpf_enabled_key[atype]);\n\t\t}\n\t\told_array = rcu_dereference_protected(\n\t\t\t\tcgrp->bpf.effective[atype],\n\t\t\t\tlockdep_is_held(&cgroup_mutex));\n\t\tbpf_prog_array_free(old_array);\n\t}\n\n\tlist_for_each_entry_safe(storage, stmp, storages, list_cg) {\n\t\tbpf_cgroup_storage_unlink(storage);\n\t\tbpf_cgroup_storage_free(storage);\n\t}\n\n\tcgroup_unlock();\n\n\tfor (p = cgroup_parent(cgrp); p; p = cgroup_parent(p))\n\t\tcgroup_bpf_put(p);\n\n\tpercpu_ref_exit(&cgrp->bpf.refcnt);\n\tcgroup_put(cgrp);\n}\n\n \nstatic void cgroup_bpf_release_fn(struct percpu_ref *ref)\n{\n\tstruct cgroup *cgrp = container_of(ref, struct cgroup, bpf.refcnt);\n\n\tINIT_WORK(&cgrp->bpf.release_work, cgroup_bpf_release);\n\tqueue_work(system_wq, &cgrp->bpf.release_work);\n}\n\n \nstatic struct bpf_prog *prog_list_prog(struct bpf_prog_list *pl)\n{\n\tif (pl->prog)\n\t\treturn pl->prog;\n\tif (pl->link)\n\t\treturn pl->link->link.prog;\n\treturn NULL;\n}\n\n \nstatic u32 prog_list_length(struct hlist_head *head)\n{\n\tstruct bpf_prog_list *pl;\n\tu32 cnt = 0;\n\n\thlist_for_each_entry(pl, head, node) {\n\t\tif (!prog_list_prog(pl))\n\t\t\tcontinue;\n\t\tcnt++;\n\t}\n\treturn cnt;\n}\n\n \nstatic bool hierarchy_allows_attach(struct cgroup *cgrp,\n\t\t\t\t    enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup *p;\n\n\tp = cgroup_parent(cgrp);\n\tif (!p)\n\t\treturn true;\n\tdo {\n\t\tu32 flags = p->bpf.flags[atype];\n\t\tu32 cnt;\n\n\t\tif (flags & BPF_F_ALLOW_MULTI)\n\t\t\treturn true;\n\t\tcnt = prog_list_length(&p->bpf.progs[atype]);\n\t\tWARN_ON_ONCE(cnt > 1);\n\t\tif (cnt == 1)\n\t\t\treturn !!(flags & BPF_F_ALLOW_OVERRIDE);\n\t\tp = cgroup_parent(p);\n\t} while (p);\n\treturn true;\n}\n\n \nstatic int compute_effective_progs(struct cgroup *cgrp,\n\t\t\t\t   enum cgroup_bpf_attach_type atype,\n\t\t\t\t   struct bpf_prog_array **array)\n{\n\tstruct bpf_prog_array_item *item;\n\tstruct bpf_prog_array *progs;\n\tstruct bpf_prog_list *pl;\n\tstruct cgroup *p = cgrp;\n\tint cnt = 0;\n\n\t \n\tdo {\n\t\tif (cnt == 0 || (p->bpf.flags[atype] & BPF_F_ALLOW_MULTI))\n\t\t\tcnt += prog_list_length(&p->bpf.progs[atype]);\n\t\tp = cgroup_parent(p);\n\t} while (p);\n\n\tprogs = bpf_prog_array_alloc(cnt, GFP_KERNEL);\n\tif (!progs)\n\t\treturn -ENOMEM;\n\n\t \n\tcnt = 0;\n\tp = cgrp;\n\tdo {\n\t\tif (cnt > 0 && !(p->bpf.flags[atype] & BPF_F_ALLOW_MULTI))\n\t\t\tcontinue;\n\n\t\thlist_for_each_entry(pl, &p->bpf.progs[atype], node) {\n\t\t\tif (!prog_list_prog(pl))\n\t\t\t\tcontinue;\n\n\t\t\titem = &progs->items[cnt];\n\t\t\titem->prog = prog_list_prog(pl);\n\t\t\tbpf_cgroup_storages_assign(item->cgroup_storage,\n\t\t\t\t\t\t   pl->storage);\n\t\t\tcnt++;\n\t\t}\n\t} while ((p = cgroup_parent(p)));\n\n\t*array = progs;\n\treturn 0;\n}\n\nstatic void activate_effective_progs(struct cgroup *cgrp,\n\t\t\t\t     enum cgroup_bpf_attach_type atype,\n\t\t\t\t     struct bpf_prog_array *old_array)\n{\n\told_array = rcu_replace_pointer(cgrp->bpf.effective[atype], old_array,\n\t\t\t\t\tlockdep_is_held(&cgroup_mutex));\n\t \n\tbpf_prog_array_free(old_array);\n}\n\n \nint cgroup_bpf_inherit(struct cgroup *cgrp)\n{\n \n#define\tNR ARRAY_SIZE(cgrp->bpf.effective)\n\tstruct bpf_prog_array *arrays[NR] = {};\n\tstruct cgroup *p;\n\tint ret, i;\n\n\tret = percpu_ref_init(&cgrp->bpf.refcnt, cgroup_bpf_release_fn, 0,\n\t\t\t      GFP_KERNEL);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (p = cgroup_parent(cgrp); p; p = cgroup_parent(p))\n\t\tcgroup_bpf_get(p);\n\n\tfor (i = 0; i < NR; i++)\n\t\tINIT_HLIST_HEAD(&cgrp->bpf.progs[i]);\n\n\tINIT_LIST_HEAD(&cgrp->bpf.storages);\n\n\tfor (i = 0; i < NR; i++)\n\t\tif (compute_effective_progs(cgrp, i, &arrays[i]))\n\t\t\tgoto cleanup;\n\n\tfor (i = 0; i < NR; i++)\n\t\tactivate_effective_progs(cgrp, i, arrays[i]);\n\n\treturn 0;\ncleanup:\n\tfor (i = 0; i < NR; i++)\n\t\tbpf_prog_array_free(arrays[i]);\n\n\tfor (p = cgroup_parent(cgrp); p; p = cgroup_parent(p))\n\t\tcgroup_bpf_put(p);\n\n\tpercpu_ref_exit(&cgrp->bpf.refcnt);\n\n\treturn -ENOMEM;\n}\n\nstatic int update_effective_progs(struct cgroup *cgrp,\n\t\t\t\t  enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup_subsys_state *css;\n\tint err;\n\n\t \n\tcss_for_each_descendant_pre(css, &cgrp->self) {\n\t\tstruct cgroup *desc = container_of(css, struct cgroup, self);\n\n\t\tif (percpu_ref_is_zero(&desc->bpf.refcnt))\n\t\t\tcontinue;\n\n\t\terr = compute_effective_progs(desc, atype, &desc->bpf.inactive);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t}\n\n\t \n\tcss_for_each_descendant_pre(css, &cgrp->self) {\n\t\tstruct cgroup *desc = container_of(css, struct cgroup, self);\n\n\t\tif (percpu_ref_is_zero(&desc->bpf.refcnt)) {\n\t\t\tif (unlikely(desc->bpf.inactive)) {\n\t\t\t\tbpf_prog_array_free(desc->bpf.inactive);\n\t\t\t\tdesc->bpf.inactive = NULL;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tactivate_effective_progs(desc, atype, desc->bpf.inactive);\n\t\tdesc->bpf.inactive = NULL;\n\t}\n\n\treturn 0;\n\ncleanup:\n\t \n\tcss_for_each_descendant_pre(css, &cgrp->self) {\n\t\tstruct cgroup *desc = container_of(css, struct cgroup, self);\n\n\t\tbpf_prog_array_free(desc->bpf.inactive);\n\t\tdesc->bpf.inactive = NULL;\n\t}\n\n\treturn err;\n}\n\n#define BPF_CGROUP_MAX_PROGS 64\n\nstatic struct bpf_prog_list *find_attach_entry(struct hlist_head *progs,\n\t\t\t\t\t       struct bpf_prog *prog,\n\t\t\t\t\t       struct bpf_cgroup_link *link,\n\t\t\t\t\t       struct bpf_prog *replace_prog,\n\t\t\t\t\t       bool allow_multi)\n{\n\tstruct bpf_prog_list *pl;\n\n\t \n\tif (!allow_multi) {\n\t\tif (hlist_empty(progs))\n\t\t\treturn NULL;\n\t\treturn hlist_entry(progs->first, typeof(*pl), node);\n\t}\n\n\thlist_for_each_entry(pl, progs, node) {\n\t\tif (prog && pl->prog == prog && prog != replace_prog)\n\t\t\t \n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tif (link && pl->link == link)\n\t\t\t \n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\tif (replace_prog) {\n\t\thlist_for_each_entry(pl, progs, node) {\n\t\t\tif (pl->prog == replace_prog)\n\t\t\t\t \n\t\t\t\treturn pl;\n\t\t}\n\t\t \n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\treturn NULL;\n}\n\n \nstatic int __cgroup_bpf_attach(struct cgroup *cgrp,\n\t\t\t       struct bpf_prog *prog, struct bpf_prog *replace_prog,\n\t\t\t       struct bpf_cgroup_link *link,\n\t\t\t       enum bpf_attach_type type, u32 flags)\n{\n\tu32 saved_flags = (flags & (BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI));\n\tstruct bpf_prog *old_prog = NULL;\n\tstruct bpf_cgroup_storage *storage[MAX_BPF_CGROUP_STORAGE_TYPE] = {};\n\tstruct bpf_cgroup_storage *new_storage[MAX_BPF_CGROUP_STORAGE_TYPE] = {};\n\tstruct bpf_prog *new_prog = prog ? : link->link.prog;\n\tenum cgroup_bpf_attach_type atype;\n\tstruct bpf_prog_list *pl;\n\tstruct hlist_head *progs;\n\tint err;\n\n\tif (((flags & BPF_F_ALLOW_OVERRIDE) && (flags & BPF_F_ALLOW_MULTI)) ||\n\t    ((flags & BPF_F_REPLACE) && !(flags & BPF_F_ALLOW_MULTI)))\n\t\t \n\t\treturn -EINVAL;\n\tif (link && (prog || replace_prog))\n\t\t \n\t\treturn -EINVAL;\n\tif (!!replace_prog != !!(flags & BPF_F_REPLACE))\n\t\t \n\t\treturn -EINVAL;\n\n\tatype = bpf_cgroup_atype_find(type, new_prog->aux->attach_btf_id);\n\tif (atype < 0)\n\t\treturn -EINVAL;\n\n\tprogs = &cgrp->bpf.progs[atype];\n\n\tif (!hierarchy_allows_attach(cgrp, atype))\n\t\treturn -EPERM;\n\n\tif (!hlist_empty(progs) && cgrp->bpf.flags[atype] != saved_flags)\n\t\t \n\t\treturn -EPERM;\n\n\tif (prog_list_length(progs) >= BPF_CGROUP_MAX_PROGS)\n\t\treturn -E2BIG;\n\n\tpl = find_attach_entry(progs, prog, link, replace_prog,\n\t\t\t       flags & BPF_F_ALLOW_MULTI);\n\tif (IS_ERR(pl))\n\t\treturn PTR_ERR(pl);\n\n\tif (bpf_cgroup_storages_alloc(storage, new_storage, type,\n\t\t\t\t      prog ? : link->link.prog, cgrp))\n\t\treturn -ENOMEM;\n\n\tif (pl) {\n\t\told_prog = pl->prog;\n\t} else {\n\t\tstruct hlist_node *last = NULL;\n\n\t\tpl = kmalloc(sizeof(*pl), GFP_KERNEL);\n\t\tif (!pl) {\n\t\t\tbpf_cgroup_storages_free(new_storage);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (hlist_empty(progs))\n\t\t\thlist_add_head(&pl->node, progs);\n\t\telse\n\t\t\thlist_for_each(last, progs) {\n\t\t\t\tif (last->next)\n\t\t\t\t\tcontinue;\n\t\t\t\thlist_add_behind(&pl->node, last);\n\t\t\t\tbreak;\n\t\t\t}\n\t}\n\n\tpl->prog = prog;\n\tpl->link = link;\n\tbpf_cgroup_storages_assign(pl->storage, storage);\n\tcgrp->bpf.flags[atype] = saved_flags;\n\n\tif (type == BPF_LSM_CGROUP) {\n\t\terr = bpf_trampoline_link_cgroup_shim(new_prog, atype);\n\t\tif (err)\n\t\t\tgoto cleanup;\n\t}\n\n\terr = update_effective_progs(cgrp, atype);\n\tif (err)\n\t\tgoto cleanup_trampoline;\n\n\tif (old_prog) {\n\t\tif (type == BPF_LSM_CGROUP)\n\t\t\tbpf_trampoline_unlink_cgroup_shim(old_prog);\n\t\tbpf_prog_put(old_prog);\n\t} else {\n\t\tstatic_branch_inc(&cgroup_bpf_enabled_key[atype]);\n\t}\n\tbpf_cgroup_storages_link(new_storage, cgrp, type);\n\treturn 0;\n\ncleanup_trampoline:\n\tif (type == BPF_LSM_CGROUP)\n\t\tbpf_trampoline_unlink_cgroup_shim(new_prog);\n\ncleanup:\n\tif (old_prog) {\n\t\tpl->prog = old_prog;\n\t\tpl->link = NULL;\n\t}\n\tbpf_cgroup_storages_free(new_storage);\n\tif (!old_prog) {\n\t\thlist_del(&pl->node);\n\t\tkfree(pl);\n\t}\n\treturn err;\n}\n\nstatic int cgroup_bpf_attach(struct cgroup *cgrp,\n\t\t\t     struct bpf_prog *prog, struct bpf_prog *replace_prog,\n\t\t\t     struct bpf_cgroup_link *link,\n\t\t\t     enum bpf_attach_type type,\n\t\t\t     u32 flags)\n{\n\tint ret;\n\n\tcgroup_lock();\n\tret = __cgroup_bpf_attach(cgrp, prog, replace_prog, link, type, flags);\n\tcgroup_unlock();\n\treturn ret;\n}\n\n \nstatic void replace_effective_prog(struct cgroup *cgrp,\n\t\t\t\t   enum cgroup_bpf_attach_type atype,\n\t\t\t\t   struct bpf_cgroup_link *link)\n{\n\tstruct bpf_prog_array_item *item;\n\tstruct cgroup_subsys_state *css;\n\tstruct bpf_prog_array *progs;\n\tstruct bpf_prog_list *pl;\n\tstruct hlist_head *head;\n\tstruct cgroup *cg;\n\tint pos;\n\n\tcss_for_each_descendant_pre(css, &cgrp->self) {\n\t\tstruct cgroup *desc = container_of(css, struct cgroup, self);\n\n\t\tif (percpu_ref_is_zero(&desc->bpf.refcnt))\n\t\t\tcontinue;\n\n\t\t \n\t\tfor (pos = 0, cg = desc; cg; cg = cgroup_parent(cg)) {\n\t\t\tif (pos && !(cg->bpf.flags[atype] & BPF_F_ALLOW_MULTI))\n\t\t\t\tcontinue;\n\n\t\t\thead = &cg->bpf.progs[atype];\n\t\t\thlist_for_each_entry(pl, head, node) {\n\t\t\t\tif (!prog_list_prog(pl))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (pl->link == link)\n\t\t\t\t\tgoto found;\n\t\t\t\tpos++;\n\t\t\t}\n\t\t}\nfound:\n\t\tBUG_ON(!cg);\n\t\tprogs = rcu_dereference_protected(\n\t\t\t\tdesc->bpf.effective[atype],\n\t\t\t\tlockdep_is_held(&cgroup_mutex));\n\t\titem = &progs->items[pos];\n\t\tWRITE_ONCE(item->prog, link->link.prog);\n\t}\n}\n\n \nstatic int __cgroup_bpf_replace(struct cgroup *cgrp,\n\t\t\t\tstruct bpf_cgroup_link *link,\n\t\t\t\tstruct bpf_prog *new_prog)\n{\n\tenum cgroup_bpf_attach_type atype;\n\tstruct bpf_prog *old_prog;\n\tstruct bpf_prog_list *pl;\n\tstruct hlist_head *progs;\n\tbool found = false;\n\n\tatype = bpf_cgroup_atype_find(link->type, new_prog->aux->attach_btf_id);\n\tif (atype < 0)\n\t\treturn -EINVAL;\n\n\tprogs = &cgrp->bpf.progs[atype];\n\n\tif (link->link.prog->type != new_prog->type)\n\t\treturn -EINVAL;\n\n\thlist_for_each_entry(pl, progs, node) {\n\t\tif (pl->link == link) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\treturn -ENOENT;\n\n\told_prog = xchg(&link->link.prog, new_prog);\n\treplace_effective_prog(cgrp, atype, link);\n\tbpf_prog_put(old_prog);\n\treturn 0;\n}\n\nstatic int cgroup_bpf_replace(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t      struct bpf_prog *old_prog)\n{\n\tstruct bpf_cgroup_link *cg_link;\n\tint ret;\n\n\tcg_link = container_of(link, struct bpf_cgroup_link, link);\n\n\tcgroup_lock();\n\t \n\tif (!cg_link->cgroup) {\n\t\tret = -ENOLINK;\n\t\tgoto out_unlock;\n\t}\n\tif (old_prog && link->prog != old_prog) {\n\t\tret = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\tret = __cgroup_bpf_replace(cg_link->cgroup, cg_link, new_prog);\nout_unlock:\n\tcgroup_unlock();\n\treturn ret;\n}\n\nstatic struct bpf_prog_list *find_detach_entry(struct hlist_head *progs,\n\t\t\t\t\t       struct bpf_prog *prog,\n\t\t\t\t\t       struct bpf_cgroup_link *link,\n\t\t\t\t\t       bool allow_multi)\n{\n\tstruct bpf_prog_list *pl;\n\n\tif (!allow_multi) {\n\t\tif (hlist_empty(progs))\n\t\t\t \n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\t \n\t\treturn hlist_entry(progs->first, typeof(*pl), node);\n\t}\n\n\tif (!prog && !link)\n\t\t \n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\thlist_for_each_entry(pl, progs, node) {\n\t\tif (pl->prog == prog && pl->link == link)\n\t\t\treturn pl;\n\t}\n\treturn ERR_PTR(-ENOENT);\n}\n\n \nstatic void purge_effective_progs(struct cgroup *cgrp, struct bpf_prog *prog,\n\t\t\t\t  struct bpf_cgroup_link *link,\n\t\t\t\t  enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup_subsys_state *css;\n\tstruct bpf_prog_array *progs;\n\tstruct bpf_prog_list *pl;\n\tstruct hlist_head *head;\n\tstruct cgroup *cg;\n\tint pos;\n\n\t \n\tcss_for_each_descendant_pre(css, &cgrp->self) {\n\t\tstruct cgroup *desc = container_of(css, struct cgroup, self);\n\n\t\tif (percpu_ref_is_zero(&desc->bpf.refcnt))\n\t\t\tcontinue;\n\n\t\t \n\t\tfor (pos = 0, cg = desc; cg; cg = cgroup_parent(cg)) {\n\t\t\tif (pos && !(cg->bpf.flags[atype] & BPF_F_ALLOW_MULTI))\n\t\t\t\tcontinue;\n\n\t\t\thead = &cg->bpf.progs[atype];\n\t\t\thlist_for_each_entry(pl, head, node) {\n\t\t\t\tif (!prog_list_prog(pl))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (pl->prog == prog && pl->link == link)\n\t\t\t\t\tgoto found;\n\t\t\t\tpos++;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tcontinue;\nfound:\n\t\tprogs = rcu_dereference_protected(\n\t\t\t\tdesc->bpf.effective[atype],\n\t\t\t\tlockdep_is_held(&cgroup_mutex));\n\n\t\t \n\t\tWARN_ONCE(bpf_prog_array_delete_safe_at(progs, pos),\n\t\t\t  \"Failed to purge a prog from array at index %d\", pos);\n\t}\n}\n\n \nstatic int __cgroup_bpf_detach(struct cgroup *cgrp, struct bpf_prog *prog,\n\t\t\t       struct bpf_cgroup_link *link, enum bpf_attach_type type)\n{\n\tenum cgroup_bpf_attach_type atype;\n\tstruct bpf_prog *old_prog;\n\tstruct bpf_prog_list *pl;\n\tstruct hlist_head *progs;\n\tu32 attach_btf_id = 0;\n\tu32 flags;\n\n\tif (prog)\n\t\tattach_btf_id = prog->aux->attach_btf_id;\n\tif (link)\n\t\tattach_btf_id = link->link.prog->aux->attach_btf_id;\n\n\tatype = bpf_cgroup_atype_find(type, attach_btf_id);\n\tif (atype < 0)\n\t\treturn -EINVAL;\n\n\tprogs = &cgrp->bpf.progs[atype];\n\tflags = cgrp->bpf.flags[atype];\n\n\tif (prog && link)\n\t\t \n\t\treturn -EINVAL;\n\n\tpl = find_detach_entry(progs, prog, link, flags & BPF_F_ALLOW_MULTI);\n\tif (IS_ERR(pl))\n\t\treturn PTR_ERR(pl);\n\n\t \n\told_prog = pl->prog;\n\tpl->prog = NULL;\n\tpl->link = NULL;\n\n\tif (update_effective_progs(cgrp, atype)) {\n\t\t \n\t\tpl->prog = old_prog;\n\t\tpl->link = link;\n\t\tpurge_effective_progs(cgrp, old_prog, link, atype);\n\t}\n\n\t \n\thlist_del(&pl->node);\n\n\tkfree(pl);\n\tif (hlist_empty(progs))\n\t\t \n\t\tcgrp->bpf.flags[atype] = 0;\n\tif (old_prog) {\n\t\tif (type == BPF_LSM_CGROUP)\n\t\t\tbpf_trampoline_unlink_cgroup_shim(old_prog);\n\t\tbpf_prog_put(old_prog);\n\t}\n\tstatic_branch_dec(&cgroup_bpf_enabled_key[atype]);\n\treturn 0;\n}\n\nstatic int cgroup_bpf_detach(struct cgroup *cgrp, struct bpf_prog *prog,\n\t\t\t     enum bpf_attach_type type)\n{\n\tint ret;\n\n\tcgroup_lock();\n\tret = __cgroup_bpf_detach(cgrp, prog, NULL, type);\n\tcgroup_unlock();\n\treturn ret;\n}\n\n \nstatic int __cgroup_bpf_query(struct cgroup *cgrp, const union bpf_attr *attr,\n\t\t\t      union bpf_attr __user *uattr)\n{\n\t__u32 __user *prog_attach_flags = u64_to_user_ptr(attr->query.prog_attach_flags);\n\tbool effective_query = attr->query.query_flags & BPF_F_QUERY_EFFECTIVE;\n\t__u32 __user *prog_ids = u64_to_user_ptr(attr->query.prog_ids);\n\tenum bpf_attach_type type = attr->query.attach_type;\n\tenum cgroup_bpf_attach_type from_atype, to_atype;\n\tenum cgroup_bpf_attach_type atype;\n\tstruct bpf_prog_array *effective;\n\tint cnt, ret = 0, i;\n\tint total_cnt = 0;\n\tu32 flags;\n\n\tif (effective_query && prog_attach_flags)\n\t\treturn -EINVAL;\n\n\tif (type == BPF_LSM_CGROUP) {\n\t\tif (!effective_query && attr->query.prog_cnt &&\n\t\t    prog_ids && !prog_attach_flags)\n\t\t\treturn -EINVAL;\n\n\t\tfrom_atype = CGROUP_LSM_START;\n\t\tto_atype = CGROUP_LSM_END;\n\t\tflags = 0;\n\t} else {\n\t\tfrom_atype = to_cgroup_bpf_attach_type(type);\n\t\tif (from_atype < 0)\n\t\t\treturn -EINVAL;\n\t\tto_atype = from_atype;\n\t\tflags = cgrp->bpf.flags[from_atype];\n\t}\n\n\tfor (atype = from_atype; atype <= to_atype; atype++) {\n\t\tif (effective_query) {\n\t\t\teffective = rcu_dereference_protected(cgrp->bpf.effective[atype],\n\t\t\t\t\t\t\t      lockdep_is_held(&cgroup_mutex));\n\t\t\ttotal_cnt += bpf_prog_array_length(effective);\n\t\t} else {\n\t\t\ttotal_cnt += prog_list_length(&cgrp->bpf.progs[atype]);\n\t\t}\n\t}\n\n\t \n\tflags = effective_query ? 0 : flags;\n\tif (copy_to_user(&uattr->query.attach_flags, &flags, sizeof(flags)))\n\t\treturn -EFAULT;\n\tif (copy_to_user(&uattr->query.prog_cnt, &total_cnt, sizeof(total_cnt)))\n\t\treturn -EFAULT;\n\tif (attr->query.prog_cnt == 0 || !prog_ids || !total_cnt)\n\t\t \n\t\treturn 0;\n\n\tif (attr->query.prog_cnt < total_cnt) {\n\t\ttotal_cnt = attr->query.prog_cnt;\n\t\tret = -ENOSPC;\n\t}\n\n\tfor (atype = from_atype; atype <= to_atype && total_cnt; atype++) {\n\t\tif (effective_query) {\n\t\t\teffective = rcu_dereference_protected(cgrp->bpf.effective[atype],\n\t\t\t\t\t\t\t      lockdep_is_held(&cgroup_mutex));\n\t\t\tcnt = min_t(int, bpf_prog_array_length(effective), total_cnt);\n\t\t\tret = bpf_prog_array_copy_to_user(effective, prog_ids, cnt);\n\t\t} else {\n\t\t\tstruct hlist_head *progs;\n\t\t\tstruct bpf_prog_list *pl;\n\t\t\tstruct bpf_prog *prog;\n\t\t\tu32 id;\n\n\t\t\tprogs = &cgrp->bpf.progs[atype];\n\t\t\tcnt = min_t(int, prog_list_length(progs), total_cnt);\n\t\t\ti = 0;\n\t\t\thlist_for_each_entry(pl, progs, node) {\n\t\t\t\tprog = prog_list_prog(pl);\n\t\t\t\tid = prog->aux->id;\n\t\t\t\tif (copy_to_user(prog_ids + i, &id, sizeof(id)))\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\tif (++i == cnt)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (prog_attach_flags) {\n\t\t\t\tflags = cgrp->bpf.flags[atype];\n\n\t\t\t\tfor (i = 0; i < cnt; i++)\n\t\t\t\t\tif (copy_to_user(prog_attach_flags + i,\n\t\t\t\t\t\t\t &flags, sizeof(flags)))\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\tprog_attach_flags += cnt;\n\t\t\t}\n\t\t}\n\n\t\tprog_ids += cnt;\n\t\ttotal_cnt -= cnt;\n\t}\n\treturn ret;\n}\n\nstatic int cgroup_bpf_query(struct cgroup *cgrp, const union bpf_attr *attr,\n\t\t\t    union bpf_attr __user *uattr)\n{\n\tint ret;\n\n\tcgroup_lock();\n\tret = __cgroup_bpf_query(cgrp, attr, uattr);\n\tcgroup_unlock();\n\treturn ret;\n}\n\nint cgroup_bpf_prog_attach(const union bpf_attr *attr,\n\t\t\t   enum bpf_prog_type ptype, struct bpf_prog *prog)\n{\n\tstruct bpf_prog *replace_prog = NULL;\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\tcgrp = cgroup_get_from_fd(attr->target_fd);\n\tif (IS_ERR(cgrp))\n\t\treturn PTR_ERR(cgrp);\n\n\tif ((attr->attach_flags & BPF_F_ALLOW_MULTI) &&\n\t    (attr->attach_flags & BPF_F_REPLACE)) {\n\t\treplace_prog = bpf_prog_get_type(attr->replace_bpf_fd, ptype);\n\t\tif (IS_ERR(replace_prog)) {\n\t\t\tcgroup_put(cgrp);\n\t\t\treturn PTR_ERR(replace_prog);\n\t\t}\n\t}\n\n\tret = cgroup_bpf_attach(cgrp, prog, replace_prog, NULL,\n\t\t\t\tattr->attach_type, attr->attach_flags);\n\n\tif (replace_prog)\n\t\tbpf_prog_put(replace_prog);\n\tcgroup_put(cgrp);\n\treturn ret;\n}\n\nint cgroup_bpf_prog_detach(const union bpf_attr *attr, enum bpf_prog_type ptype)\n{\n\tstruct bpf_prog *prog;\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\tcgrp = cgroup_get_from_fd(attr->target_fd);\n\tif (IS_ERR(cgrp))\n\t\treturn PTR_ERR(cgrp);\n\n\tprog = bpf_prog_get_type(attr->attach_bpf_fd, ptype);\n\tif (IS_ERR(prog))\n\t\tprog = NULL;\n\n\tret = cgroup_bpf_detach(cgrp, prog, attr->attach_type);\n\tif (prog)\n\t\tbpf_prog_put(prog);\n\n\tcgroup_put(cgrp);\n\treturn ret;\n}\n\nstatic void bpf_cgroup_link_release(struct bpf_link *link)\n{\n\tstruct bpf_cgroup_link *cg_link =\n\t\tcontainer_of(link, struct bpf_cgroup_link, link);\n\tstruct cgroup *cg;\n\n\t \n\tif (!cg_link->cgroup)\n\t\treturn;\n\n\tcgroup_lock();\n\n\t \n\tif (!cg_link->cgroup) {\n\t\tcgroup_unlock();\n\t\treturn;\n\t}\n\n\tWARN_ON(__cgroup_bpf_detach(cg_link->cgroup, NULL, cg_link,\n\t\t\t\t    cg_link->type));\n\tif (cg_link->type == BPF_LSM_CGROUP)\n\t\tbpf_trampoline_unlink_cgroup_shim(cg_link->link.prog);\n\n\tcg = cg_link->cgroup;\n\tcg_link->cgroup = NULL;\n\n\tcgroup_unlock();\n\n\tcgroup_put(cg);\n}\n\nstatic void bpf_cgroup_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_cgroup_link *cg_link =\n\t\tcontainer_of(link, struct bpf_cgroup_link, link);\n\n\tkfree(cg_link);\n}\n\nstatic int bpf_cgroup_link_detach(struct bpf_link *link)\n{\n\tbpf_cgroup_link_release(link);\n\n\treturn 0;\n}\n\nstatic void bpf_cgroup_link_show_fdinfo(const struct bpf_link *link,\n\t\t\t\t\tstruct seq_file *seq)\n{\n\tstruct bpf_cgroup_link *cg_link =\n\t\tcontainer_of(link, struct bpf_cgroup_link, link);\n\tu64 cg_id = 0;\n\n\tcgroup_lock();\n\tif (cg_link->cgroup)\n\t\tcg_id = cgroup_id(cg_link->cgroup);\n\tcgroup_unlock();\n\n\tseq_printf(seq,\n\t\t   \"cgroup_id:\\t%llu\\n\"\n\t\t   \"attach_type:\\t%d\\n\",\n\t\t   cg_id,\n\t\t   cg_link->type);\n}\n\nstatic int bpf_cgroup_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t\t  struct bpf_link_info *info)\n{\n\tstruct bpf_cgroup_link *cg_link =\n\t\tcontainer_of(link, struct bpf_cgroup_link, link);\n\tu64 cg_id = 0;\n\n\tcgroup_lock();\n\tif (cg_link->cgroup)\n\t\tcg_id = cgroup_id(cg_link->cgroup);\n\tcgroup_unlock();\n\n\tinfo->cgroup.cgroup_id = cg_id;\n\tinfo->cgroup.attach_type = cg_link->type;\n\treturn 0;\n}\n\nstatic const struct bpf_link_ops bpf_cgroup_link_lops = {\n\t.release = bpf_cgroup_link_release,\n\t.dealloc = bpf_cgroup_link_dealloc,\n\t.detach = bpf_cgroup_link_detach,\n\t.update_prog = cgroup_bpf_replace,\n\t.show_fdinfo = bpf_cgroup_link_show_fdinfo,\n\t.fill_link_info = bpf_cgroup_link_fill_link_info,\n};\n\nint cgroup_bpf_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_cgroup_link *link;\n\tstruct cgroup *cgrp;\n\tint err;\n\n\tif (attr->link_create.flags)\n\t\treturn -EINVAL;\n\n\tcgrp = cgroup_get_from_fd(attr->link_create.target_fd);\n\tif (IS_ERR(cgrp))\n\t\treturn PTR_ERR(cgrp);\n\n\tlink = kzalloc(sizeof(*link), GFP_USER);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put_cgroup;\n\t}\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_CGROUP, &bpf_cgroup_link_lops,\n\t\t      prog);\n\tlink->cgroup = cgrp;\n\tlink->type = attr->link_create.attach_type;\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err) {\n\t\tkfree(link);\n\t\tgoto out_put_cgroup;\n\t}\n\n\terr = cgroup_bpf_attach(cgrp, NULL, NULL, link,\n\t\t\t\tlink->type, BPF_F_ALLOW_MULTI);\n\tif (err) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\tgoto out_put_cgroup;\n\t}\n\n\treturn bpf_link_settle(&link_primer);\n\nout_put_cgroup:\n\tcgroup_put(cgrp);\n\treturn err;\n}\n\nint cgroup_bpf_prog_query(const union bpf_attr *attr,\n\t\t\t  union bpf_attr __user *uattr)\n{\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\tcgrp = cgroup_get_from_fd(attr->query.target_fd);\n\tif (IS_ERR(cgrp))\n\t\treturn PTR_ERR(cgrp);\n\n\tret = cgroup_bpf_query(cgrp, attr, uattr);\n\n\tcgroup_put(cgrp);\n\treturn ret;\n}\n\n \nint __cgroup_bpf_run_filter_skb(struct sock *sk,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tenum cgroup_bpf_attach_type atype)\n{\n\tunsigned int offset = skb->data - skb_network_header(skb);\n\tstruct sock *save_sk;\n\tvoid *saved_data_end;\n\tstruct cgroup *cgrp;\n\tint ret;\n\n\tif (!sk || !sk_fullsock(sk))\n\t\treturn 0;\n\n\tif (sk->sk_family != AF_INET && sk->sk_family != AF_INET6)\n\t\treturn 0;\n\n\tcgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\tsave_sk = skb->sk;\n\tskb->sk = sk;\n\t__skb_push(skb, offset);\n\n\t \n\tbpf_compute_and_save_data_end(skb, &saved_data_end);\n\n\tif (atype == CGROUP_INET_EGRESS) {\n\t\tu32 flags = 0;\n\t\tbool cn;\n\n\t\tret = bpf_prog_run_array_cg(&cgrp->bpf, atype, skb,\n\t\t\t\t\t    __bpf_prog_run_save_cb, 0, &flags);\n\n\t\t \n\n\t\tcn = flags & BPF_RET_SET_CN;\n\t\tif (ret && !IS_ERR_VALUE((long)ret))\n\t\t\tret = -EFAULT;\n\t\tif (!ret)\n\t\t\tret = (cn ? NET_XMIT_CN : NET_XMIT_SUCCESS);\n\t\telse\n\t\t\tret = (cn ? NET_XMIT_DROP : ret);\n\t} else {\n\t\tret = bpf_prog_run_array_cg(&cgrp->bpf, atype,\n\t\t\t\t\t    skb, __bpf_prog_run_save_cb, 0,\n\t\t\t\t\t    NULL);\n\t\tif (ret && !IS_ERR_VALUE((long)ret))\n\t\t\tret = -EFAULT;\n\t}\n\tbpf_restore_data_end(skb, saved_data_end);\n\t__skb_pull(skb, offset);\n\tskb->sk = save_sk;\n\n\treturn ret;\n}\nEXPORT_SYMBOL(__cgroup_bpf_run_filter_skb);\n\n \nint __cgroup_bpf_run_filter_sk(struct sock *sk,\n\t\t\t       enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\n\treturn bpf_prog_run_array_cg(&cgrp->bpf, atype, sk, bpf_prog_run, 0,\n\t\t\t\t     NULL);\n}\nEXPORT_SYMBOL(__cgroup_bpf_run_filter_sk);\n\n \nint __cgroup_bpf_run_filter_sock_addr(struct sock *sk,\n\t\t\t\t      struct sockaddr *uaddr,\n\t\t\t\t      enum cgroup_bpf_attach_type atype,\n\t\t\t\t      void *t_ctx,\n\t\t\t\t      u32 *flags)\n{\n\tstruct bpf_sock_addr_kern ctx = {\n\t\t.sk = sk,\n\t\t.uaddr = uaddr,\n\t\t.t_ctx = t_ctx,\n\t};\n\tstruct sockaddr_storage unspec;\n\tstruct cgroup *cgrp;\n\n\t \n\tif (sk->sk_family != AF_INET && sk->sk_family != AF_INET6)\n\t\treturn 0;\n\n\tif (!ctx.uaddr) {\n\t\tmemset(&unspec, 0, sizeof(unspec));\n\t\tctx.uaddr = (struct sockaddr *)&unspec;\n\t}\n\n\tcgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\treturn bpf_prog_run_array_cg(&cgrp->bpf, atype, &ctx, bpf_prog_run,\n\t\t\t\t     0, flags);\n}\nEXPORT_SYMBOL(__cgroup_bpf_run_filter_sock_addr);\n\n \nint __cgroup_bpf_run_filter_sock_ops(struct sock *sk,\n\t\t\t\t     struct bpf_sock_ops_kern *sock_ops,\n\t\t\t\t     enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\n\treturn bpf_prog_run_array_cg(&cgrp->bpf, atype, sock_ops, bpf_prog_run,\n\t\t\t\t     0, NULL);\n}\nEXPORT_SYMBOL(__cgroup_bpf_run_filter_sock_ops);\n\nint __cgroup_bpf_check_dev_permission(short dev_type, u32 major, u32 minor,\n\t\t\t\t      short access, enum cgroup_bpf_attach_type atype)\n{\n\tstruct cgroup *cgrp;\n\tstruct bpf_cgroup_dev_ctx ctx = {\n\t\t.access_type = (access << 16) | dev_type,\n\t\t.major = major,\n\t\t.minor = minor,\n\t};\n\tint ret;\n\n\trcu_read_lock();\n\tcgrp = task_dfl_cgroup(current);\n\tret = bpf_prog_run_array_cg(&cgrp->bpf, atype, &ctx, bpf_prog_run, 0,\n\t\t\t\t    NULL);\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nBPF_CALL_2(bpf_get_local_storage, struct bpf_map *, map, u64, flags)\n{\n\t \n\tenum bpf_cgroup_storage_type stype = cgroup_storage_type(map);\n\tstruct bpf_cgroup_storage *storage;\n\tstruct bpf_cg_run_ctx *ctx;\n\tvoid *ptr;\n\n\t \n\tctx = container_of(current->bpf_ctx, struct bpf_cg_run_ctx, run_ctx);\n\tstorage = ctx->prog_item->cgroup_storage[stype];\n\n\tif (stype == BPF_CGROUP_STORAGE_SHARED)\n\t\tptr = &READ_ONCE(storage->buf)->data[0];\n\telse\n\t\tptr = this_cpu_ptr(storage->percpu_buf);\n\n\treturn (unsigned long)ptr;\n}\n\nconst struct bpf_func_proto bpf_get_local_storage_proto = {\n\t.func\t\t= bpf_get_local_storage,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_PTR_TO_MAP_VALUE,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_0(bpf_get_retval)\n{\n\tstruct bpf_cg_run_ctx *ctx =\n\t\tcontainer_of(current->bpf_ctx, struct bpf_cg_run_ctx, run_ctx);\n\n\treturn ctx->retval;\n}\n\nconst struct bpf_func_proto bpf_get_retval_proto = {\n\t.func\t\t= bpf_get_retval,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_1(bpf_set_retval, int, retval)\n{\n\tstruct bpf_cg_run_ctx *ctx =\n\t\tcontainer_of(current->bpf_ctx, struct bpf_cg_run_ctx, run_ctx);\n\n\tctx->retval = retval;\n\treturn 0;\n}\n\nconst struct bpf_func_proto bpf_set_retval_proto = {\n\t.func\t\t= bpf_set_retval,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto *\ncgroup_dev_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tfunc_proto = cgroup_current_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic bool cgroup_dev_is_valid_access(int off, int size,\n\t\t\t\t       enum bpf_access_type type,\n\t\t\t\t       const struct bpf_prog *prog,\n\t\t\t\t       struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (type == BPF_WRITE)\n\t\treturn false;\n\n\tif (off < 0 || off + size > sizeof(struct bpf_cgroup_dev_ctx))\n\t\treturn false;\n\t \n\tif (off % size != 0)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_cgroup_dev_ctx, access_type):\n\t\tbpf_ctx_record_field_size(info, size_default);\n\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_default))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nconst struct bpf_prog_ops cg_dev_prog_ops = {\n};\n\nconst struct bpf_verifier_ops cg_dev_verifier_ops = {\n\t.get_func_proto\t\t= cgroup_dev_func_proto,\n\t.is_valid_access\t= cgroup_dev_is_valid_access,\n};\n\n \nint __cgroup_bpf_run_filter_sysctl(struct ctl_table_header *head,\n\t\t\t\t   struct ctl_table *table, int write,\n\t\t\t\t   char **buf, size_t *pcount, loff_t *ppos,\n\t\t\t\t   enum cgroup_bpf_attach_type atype)\n{\n\tstruct bpf_sysctl_kern ctx = {\n\t\t.head = head,\n\t\t.table = table,\n\t\t.write = write,\n\t\t.ppos = ppos,\n\t\t.cur_val = NULL,\n\t\t.cur_len = PAGE_SIZE,\n\t\t.new_val = NULL,\n\t\t.new_len = 0,\n\t\t.new_updated = 0,\n\t};\n\tstruct cgroup *cgrp;\n\tloff_t pos = 0;\n\tint ret;\n\n\tctx.cur_val = kmalloc_track_caller(ctx.cur_len, GFP_KERNEL);\n\tif (!ctx.cur_val ||\n\t    table->proc_handler(table, 0, ctx.cur_val, &ctx.cur_len, &pos)) {\n\t\t \n\t\tctx.cur_len = 0;\n\t}\n\n\tif (write && *buf && *pcount) {\n\t\t \n\t\tctx.new_val = kmalloc_track_caller(PAGE_SIZE, GFP_KERNEL);\n\t\tctx.new_len = min_t(size_t, PAGE_SIZE, *pcount);\n\t\tif (ctx.new_val) {\n\t\t\tmemcpy(ctx.new_val, *buf, ctx.new_len);\n\t\t} else {\n\t\t\t \n\t\t\tctx.new_len = 0;\n\t\t}\n\t}\n\n\trcu_read_lock();\n\tcgrp = task_dfl_cgroup(current);\n\tret = bpf_prog_run_array_cg(&cgrp->bpf, atype, &ctx, bpf_prog_run, 0,\n\t\t\t\t    NULL);\n\trcu_read_unlock();\n\n\tkfree(ctx.cur_val);\n\n\tif (ret == 1 && ctx.new_updated) {\n\t\tkfree(*buf);\n\t\t*buf = ctx.new_val;\n\t\t*pcount = ctx.new_len;\n\t} else {\n\t\tkfree(ctx.new_val);\n\t}\n\n\treturn ret;\n}\n\n#ifdef CONFIG_NET\nstatic int sockopt_alloc_buf(struct bpf_sockopt_kern *ctx, int max_optlen,\n\t\t\t     struct bpf_sockopt_buf *buf)\n{\n\tif (unlikely(max_optlen < 0))\n\t\treturn -EINVAL;\n\n\tif (unlikely(max_optlen > PAGE_SIZE)) {\n\t\t \n\t\tmax_optlen = PAGE_SIZE;\n\t}\n\n\tif (max_optlen <= sizeof(buf->data)) {\n\t\t \n\t\tctx->optval = buf->data;\n\t\tctx->optval_end = ctx->optval + max_optlen;\n\t\treturn max_optlen;\n\t}\n\n\tctx->optval = kzalloc(max_optlen, GFP_USER);\n\tif (!ctx->optval)\n\t\treturn -ENOMEM;\n\n\tctx->optval_end = ctx->optval + max_optlen;\n\n\treturn max_optlen;\n}\n\nstatic void sockopt_free_buf(struct bpf_sockopt_kern *ctx,\n\t\t\t     struct bpf_sockopt_buf *buf)\n{\n\tif (ctx->optval == buf->data)\n\t\treturn;\n\tkfree(ctx->optval);\n}\n\nstatic bool sockopt_buf_allocated(struct bpf_sockopt_kern *ctx,\n\t\t\t\t  struct bpf_sockopt_buf *buf)\n{\n\treturn ctx->optval != buf->data;\n}\n\nint __cgroup_bpf_run_filter_setsockopt(struct sock *sk, int *level,\n\t\t\t\t       int *optname, char __user *optval,\n\t\t\t\t       int *optlen, char **kernel_optval)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\tstruct bpf_sockopt_buf buf = {};\n\tstruct bpf_sockopt_kern ctx = {\n\t\t.sk = sk,\n\t\t.level = *level,\n\t\t.optname = *optname,\n\t};\n\tint ret, max_optlen;\n\n\t \n\tmax_optlen = max_t(int, 16, *optlen);\n\tmax_optlen = sockopt_alloc_buf(&ctx, max_optlen, &buf);\n\tif (max_optlen < 0)\n\t\treturn max_optlen;\n\n\tctx.optlen = *optlen;\n\n\tif (copy_from_user(ctx.optval, optval, min(*optlen, max_optlen)) != 0) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\tret = bpf_prog_run_array_cg(&cgrp->bpf, CGROUP_SETSOCKOPT,\n\t\t\t\t    &ctx, bpf_prog_run, 0, NULL);\n\trelease_sock(sk);\n\n\tif (ret)\n\t\tgoto out;\n\n\tif (ctx.optlen == -1) {\n\t\t \n\t\tret = 1;\n\t} else if (ctx.optlen > max_optlen || ctx.optlen < -1) {\n\t\t \n\t\tif (*optlen > PAGE_SIZE && ctx.optlen >= 0) {\n\t\t\tpr_info_once(\"bpf setsockopt: ignoring program buffer with optlen=%d (max_optlen=%d)\\n\",\n\t\t\t\t     ctx.optlen, max_optlen);\n\t\t\tret = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tret = -EFAULT;\n\t} else {\n\t\t \n\t\tret = 0;\n\n\t\t \n\t\t*level = ctx.level;\n\t\t*optname = ctx.optname;\n\n\t\t \n\t\tif (ctx.optlen != 0) {\n\t\t\t*optlen = ctx.optlen;\n\t\t\t \n\t\t\tif (!sockopt_buf_allocated(&ctx, &buf)) {\n\t\t\t\tvoid *p = kmalloc(ctx.optlen, GFP_USER);\n\n\t\t\t\tif (!p) {\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tmemcpy(p, ctx.optval, ctx.optlen);\n\t\t\t\t*kernel_optval = p;\n\t\t\t} else {\n\t\t\t\t*kernel_optval = ctx.optval;\n\t\t\t}\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t}\n\nout:\n\tsockopt_free_buf(&ctx, &buf);\n\treturn ret;\n}\n\nint __cgroup_bpf_run_filter_getsockopt(struct sock *sk, int level,\n\t\t\t\t       int optname, char __user *optval,\n\t\t\t\t       int __user *optlen, int max_optlen,\n\t\t\t\t       int retval)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\tstruct bpf_sockopt_buf buf = {};\n\tstruct bpf_sockopt_kern ctx = {\n\t\t.sk = sk,\n\t\t.level = level,\n\t\t.optname = optname,\n\t\t.current_task = current,\n\t};\n\tint orig_optlen;\n\tint ret;\n\n\torig_optlen = max_optlen;\n\tctx.optlen = max_optlen;\n\tmax_optlen = sockopt_alloc_buf(&ctx, max_optlen, &buf);\n\tif (max_optlen < 0)\n\t\treturn max_optlen;\n\n\tif (!retval) {\n\t\t \n\n\t\tif (get_user(ctx.optlen, optlen)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (ctx.optlen < 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\torig_optlen = ctx.optlen;\n\n\t\tif (copy_from_user(ctx.optval, optval,\n\t\t\t\t   min(ctx.optlen, max_optlen)) != 0) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tlock_sock(sk);\n\tret = bpf_prog_run_array_cg(&cgrp->bpf, CGROUP_GETSOCKOPT,\n\t\t\t\t    &ctx, bpf_prog_run, retval, NULL);\n\trelease_sock(sk);\n\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (optval && (ctx.optlen > max_optlen || ctx.optlen < 0)) {\n\t\tif (orig_optlen > PAGE_SIZE && ctx.optlen >= 0) {\n\t\t\tpr_info_once(\"bpf getsockopt: ignoring program buffer with optlen=%d (max_optlen=%d)\\n\",\n\t\t\t\t     ctx.optlen, max_optlen);\n\t\t\tret = retval;\n\t\t\tgoto out;\n\t\t}\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (ctx.optlen != 0) {\n\t\tif (optval && copy_to_user(optval, ctx.optval, ctx.optlen)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tif (put_user(ctx.optlen, optlen)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tsockopt_free_buf(&ctx, &buf);\n\treturn ret;\n}\n\nint __cgroup_bpf_run_filter_getsockopt_kern(struct sock *sk, int level,\n\t\t\t\t\t    int optname, void *optval,\n\t\t\t\t\t    int *optlen, int retval)\n{\n\tstruct cgroup *cgrp = sock_cgroup_ptr(&sk->sk_cgrp_data);\n\tstruct bpf_sockopt_kern ctx = {\n\t\t.sk = sk,\n\t\t.level = level,\n\t\t.optname = optname,\n\t\t.optlen = *optlen,\n\t\t.optval = optval,\n\t\t.optval_end = optval + *optlen,\n\t\t.current_task = current,\n\t};\n\tint ret;\n\n\t \n\n\tret = bpf_prog_run_array_cg(&cgrp->bpf, CGROUP_GETSOCKOPT,\n\t\t\t\t    &ctx, bpf_prog_run, retval, NULL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ctx.optlen > *optlen)\n\t\treturn -EFAULT;\n\n\t \n\tif (ctx.optlen != 0)\n\t\t*optlen = ctx.optlen;\n\n\treturn ret;\n}\n#endif\n\nstatic ssize_t sysctl_cpy_dir(const struct ctl_dir *dir, char **bufp,\n\t\t\t      size_t *lenp)\n{\n\tssize_t tmp_ret = 0, ret;\n\n\tif (dir->header.parent) {\n\t\ttmp_ret = sysctl_cpy_dir(dir->header.parent, bufp, lenp);\n\t\tif (tmp_ret < 0)\n\t\t\treturn tmp_ret;\n\t}\n\n\tret = strscpy(*bufp, dir->header.ctl_table[0].procname, *lenp);\n\tif (ret < 0)\n\t\treturn ret;\n\t*bufp += ret;\n\t*lenp -= ret;\n\tret += tmp_ret;\n\n\t \n\tif (!ret)\n\t\treturn ret;\n\n\ttmp_ret = strscpy(*bufp, \"/\", *lenp);\n\tif (tmp_ret < 0)\n\t\treturn tmp_ret;\n\t*bufp += tmp_ret;\n\t*lenp -= tmp_ret;\n\n\treturn ret + tmp_ret;\n}\n\nBPF_CALL_4(bpf_sysctl_get_name, struct bpf_sysctl_kern *, ctx, char *, buf,\n\t   size_t, buf_len, u64, flags)\n{\n\tssize_t tmp_ret = 0, ret;\n\n\tif (!buf)\n\t\treturn -EINVAL;\n\n\tif (!(flags & BPF_F_SYSCTL_BASE_NAME)) {\n\t\tif (!ctx->head)\n\t\t\treturn -EINVAL;\n\t\ttmp_ret = sysctl_cpy_dir(ctx->head->parent, &buf, &buf_len);\n\t\tif (tmp_ret < 0)\n\t\t\treturn tmp_ret;\n\t}\n\n\tret = strscpy(buf, ctx->table->procname, buf_len);\n\n\treturn ret < 0 ? ret : tmp_ret + ret;\n}\n\nstatic const struct bpf_func_proto bpf_sysctl_get_name_proto = {\n\t.func\t\t= bpf_sysctl_get_name,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic int copy_sysctl_value(char *dst, size_t dst_len, char *src,\n\t\t\t     size_t src_len)\n{\n\tif (!dst)\n\t\treturn -EINVAL;\n\n\tif (!dst_len)\n\t\treturn -E2BIG;\n\n\tif (!src || !src_len) {\n\t\tmemset(dst, 0, dst_len);\n\t\treturn -EINVAL;\n\t}\n\n\tmemcpy(dst, src, min(dst_len, src_len));\n\n\tif (dst_len > src_len) {\n\t\tmemset(dst + src_len, '\\0', dst_len - src_len);\n\t\treturn src_len;\n\t}\n\n\tdst[dst_len - 1] = '\\0';\n\n\treturn -E2BIG;\n}\n\nBPF_CALL_3(bpf_sysctl_get_current_value, struct bpf_sysctl_kern *, ctx,\n\t   char *, buf, size_t, buf_len)\n{\n\treturn copy_sysctl_value(buf, buf_len, ctx->cur_val, ctx->cur_len);\n}\n\nstatic const struct bpf_func_proto bpf_sysctl_get_current_value_proto = {\n\t.func\t\t= bpf_sysctl_get_current_value,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_3(bpf_sysctl_get_new_value, struct bpf_sysctl_kern *, ctx, char *, buf,\n\t   size_t, buf_len)\n{\n\tif (!ctx->write) {\n\t\tif (buf && buf_len)\n\t\t\tmemset(buf, '\\0', buf_len);\n\t\treturn -EINVAL;\n\t}\n\treturn copy_sysctl_value(buf, buf_len, ctx->new_val, ctx->new_len);\n}\n\nstatic const struct bpf_func_proto bpf_sysctl_get_new_value_proto = {\n\t.func\t\t= bpf_sysctl_get_new_value,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nBPF_CALL_3(bpf_sysctl_set_new_value, struct bpf_sysctl_kern *, ctx,\n\t   const char *, buf, size_t, buf_len)\n{\n\tif (!ctx->write || !ctx->new_val || !ctx->new_len || !buf || !buf_len)\n\t\treturn -EINVAL;\n\n\tif (buf_len > PAGE_SIZE - 1)\n\t\treturn -E2BIG;\n\n\tmemcpy(ctx->new_val, buf, buf_len);\n\tctx->new_len = buf_len;\n\tctx->new_updated = 1;\n\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_sysctl_set_new_value_proto = {\n\t.func\t\t= bpf_sysctl_set_new_value,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *\nsysctl_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tfunc_proto = cgroup_current_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_sysctl_get_name:\n\t\treturn &bpf_sysctl_get_name_proto;\n\tcase BPF_FUNC_sysctl_get_current_value:\n\t\treturn &bpf_sysctl_get_current_value_proto;\n\tcase BPF_FUNC_sysctl_get_new_value:\n\t\treturn &bpf_sysctl_get_new_value_proto;\n\tcase BPF_FUNC_sysctl_set_new_value:\n\t\treturn &bpf_sysctl_set_new_value_proto;\n\tcase BPF_FUNC_ktime_get_coarse_ns:\n\t\treturn &bpf_ktime_get_coarse_ns_proto;\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic bool sysctl_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t   const struct bpf_prog *prog,\n\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off + size > sizeof(struct bpf_sysctl) || off % size)\n\t\treturn false;\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_sysctl, write):\n\t\tif (type != BPF_READ)\n\t\t\treturn false;\n\t\tbpf_ctx_record_field_size(info, size_default);\n\t\treturn bpf_ctx_narrow_access_ok(off, size, size_default);\n\tcase bpf_ctx_range(struct bpf_sysctl, file_pos):\n\t\tif (type == BPF_READ) {\n\t\t\tbpf_ctx_record_field_size(info, size_default);\n\t\t\treturn bpf_ctx_narrow_access_ok(off, size, size_default);\n\t\t} else {\n\t\t\treturn size == size_default;\n\t\t}\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic u32 sysctl_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t     const struct bpf_insn *si,\n\t\t\t\t     struct bpf_insn *insn_buf,\n\t\t\t\t     struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\tu32 read_size;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sysctl, write):\n\t\t*insn++ = BPF_LDX_MEM(\n\t\t\tBPF_SIZE(si->code), si->dst_reg, si->src_reg,\n\t\t\tbpf_target_off(struct bpf_sysctl_kern, write,\n\t\t\t\t       sizeof_field(struct bpf_sysctl_kern,\n\t\t\t\t\t\t    write),\n\t\t\t\t       target_size));\n\t\tbreak;\n\tcase offsetof(struct bpf_sysctl, file_pos):\n\t\t \n\t\tif (type == BPF_WRITE) {\n\t\t\tint treg = BPF_REG_9;\n\n\t\t\tif (si->src_reg == treg || si->dst_reg == treg)\n\t\t\t\t--treg;\n\t\t\tif (si->src_reg == treg || si->dst_reg == treg)\n\t\t\t\t--treg;\n\t\t\t*insn++ = BPF_STX_MEM(\n\t\t\t\tBPF_DW, si->dst_reg, treg,\n\t\t\t\toffsetof(struct bpf_sysctl_kern, tmp_reg));\n\t\t\t*insn++ = BPF_LDX_MEM(\n\t\t\t\tBPF_FIELD_SIZEOF(struct bpf_sysctl_kern, ppos),\n\t\t\t\ttreg, si->dst_reg,\n\t\t\t\toffsetof(struct bpf_sysctl_kern, ppos));\n\t\t\t*insn++ = BPF_RAW_INSN(\n\t\t\t\tBPF_CLASS(si->code) | BPF_MEM | BPF_SIZEOF(u32),\n\t\t\t\ttreg, si->src_reg,\n\t\t\t\tbpf_ctx_narrow_access_offset(\n\t\t\t\t\t0, sizeof(u32), sizeof(loff_t)),\n\t\t\t\tsi->imm);\n\t\t\t*insn++ = BPF_LDX_MEM(\n\t\t\t\tBPF_DW, treg, si->dst_reg,\n\t\t\t\toffsetof(struct bpf_sysctl_kern, tmp_reg));\n\t\t} else {\n\t\t\t*insn++ = BPF_LDX_MEM(\n\t\t\t\tBPF_FIELD_SIZEOF(struct bpf_sysctl_kern, ppos),\n\t\t\t\tsi->dst_reg, si->src_reg,\n\t\t\t\toffsetof(struct bpf_sysctl_kern, ppos));\n\t\t\tread_size = bpf_size_to_bytes(BPF_SIZE(si->code));\n\t\t\t*insn++ = BPF_LDX_MEM(\n\t\t\t\tBPF_SIZE(si->code), si->dst_reg, si->dst_reg,\n\t\t\t\tbpf_ctx_narrow_access_offset(\n\t\t\t\t\t0, read_size, sizeof(loff_t)));\n\t\t}\n\t\t*target_size = sizeof(u32);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_verifier_ops cg_sysctl_verifier_ops = {\n\t.get_func_proto\t\t= sysctl_func_proto,\n\t.is_valid_access\t= sysctl_is_valid_access,\n\t.convert_ctx_access\t= sysctl_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops cg_sysctl_prog_ops = {\n};\n\n#ifdef CONFIG_NET\nBPF_CALL_1(bpf_get_netns_cookie_sockopt, struct bpf_sockopt_kern *, ctx)\n{\n\tconst struct net *net = ctx ? sock_net(ctx->sk) : &init_net;\n\n\treturn net->net_cookie;\n}\n\nstatic const struct bpf_func_proto bpf_get_netns_cookie_sockopt_proto = {\n\t.func\t\t= bpf_get_netns_cookie_sockopt,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX_OR_NULL,\n};\n#endif\n\nstatic const struct bpf_func_proto *\ncg_sockopt_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *func_proto;\n\n\tfunc_proto = cgroup_common_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tfunc_proto = cgroup_current_func_proto(func_id, prog);\n\tif (func_proto)\n\t\treturn func_proto;\n\n\tswitch (func_id) {\n#ifdef CONFIG_NET\n\tcase BPF_FUNC_get_netns_cookie:\n\t\treturn &bpf_get_netns_cookie_sockopt_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_proto;\n\tcase BPF_FUNC_setsockopt:\n\t\tif (prog->expected_attach_type == BPF_CGROUP_SETSOCKOPT)\n\t\t\treturn &bpf_sk_setsockopt_proto;\n\t\treturn NULL;\n\tcase BPF_FUNC_getsockopt:\n\t\tif (prog->expected_attach_type == BPF_CGROUP_SETSOCKOPT)\n\t\t\treturn &bpf_sk_getsockopt_proto;\n\t\treturn NULL;\n#endif\n#ifdef CONFIG_INET\n\tcase BPF_FUNC_tcp_sock:\n\t\treturn &bpf_tcp_sock_proto;\n#endif\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_event_output_data_proto;\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic bool cg_sockopt_is_valid_access(int off, int size,\n\t\t\t\t       enum bpf_access_type type,\n\t\t\t\t       const struct bpf_prog *prog,\n\t\t\t\t       struct bpf_insn_access_aux *info)\n{\n\tconst int size_default = sizeof(__u32);\n\n\tif (off < 0 || off >= sizeof(struct bpf_sockopt))\n\t\treturn false;\n\n\tif (off % size != 0)\n\t\treturn false;\n\n\tif (type == BPF_WRITE) {\n\t\tswitch (off) {\n\t\tcase offsetof(struct bpf_sockopt, retval):\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\treturn prog->expected_attach_type ==\n\t\t\t\tBPF_CGROUP_GETSOCKOPT;\n\t\tcase offsetof(struct bpf_sockopt, optname):\n\t\t\tfallthrough;\n\t\tcase offsetof(struct bpf_sockopt, level):\n\t\t\tif (size != size_default)\n\t\t\t\treturn false;\n\t\t\treturn prog->expected_attach_type ==\n\t\t\t\tBPF_CGROUP_SETSOCKOPT;\n\t\tcase offsetof(struct bpf_sockopt, optlen):\n\t\t\treturn size == size_default;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tswitch (off) {\n\tcase offsetof(struct bpf_sockopt, sk):\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_SOCKET;\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, optval):\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_PACKET;\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, optval_end):\n\t\tif (size != sizeof(__u64))\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_PACKET_END;\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, retval):\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\treturn prog->expected_attach_type == BPF_CGROUP_GETSOCKOPT;\n\tdefault:\n\t\tif (size != size_default)\n\t\t\treturn false;\n\t\tbreak;\n\t}\n\treturn true;\n}\n\n#define CG_SOCKOPT_READ_FIELD(F)\t\t\t\t\t\\\n\tBPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sockopt_kern, F),\t\\\n\t\t    si->dst_reg, si->src_reg,\t\t\t\t\\\n\t\t    offsetof(struct bpf_sockopt_kern, F))\n\n#define CG_SOCKOPT_WRITE_FIELD(F)\t\t\t\t\t\\\n\tBPF_RAW_INSN((BPF_FIELD_SIZEOF(struct bpf_sockopt_kern, F) |\t\\\n\t\t      BPF_MEM | BPF_CLASS(si->code)),\t\t\t\\\n\t\t     si->dst_reg, si->src_reg,\t\t\t\t\\\n\t\t     offsetof(struct bpf_sockopt_kern, F),\t\t\\\n\t\t     si->imm)\n\nstatic u32 cg_sockopt_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t const struct bpf_insn *si,\n\t\t\t\t\t struct bpf_insn *insn_buf,\n\t\t\t\t\t struct bpf_prog *prog,\n\t\t\t\t\t u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_sockopt, sk):\n\t\t*insn++ = CG_SOCKOPT_READ_FIELD(sk);\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, level):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = CG_SOCKOPT_WRITE_FIELD(level);\n\t\telse\n\t\t\t*insn++ = CG_SOCKOPT_READ_FIELD(level);\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, optname):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = CG_SOCKOPT_WRITE_FIELD(optname);\n\t\telse\n\t\t\t*insn++ = CG_SOCKOPT_READ_FIELD(optname);\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, optlen):\n\t\tif (type == BPF_WRITE)\n\t\t\t*insn++ = CG_SOCKOPT_WRITE_FIELD(optlen);\n\t\telse\n\t\t\t*insn++ = CG_SOCKOPT_READ_FIELD(optlen);\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, retval):\n\t\tBUILD_BUG_ON(offsetof(struct bpf_cg_run_ctx, run_ctx) != 0);\n\n\t\tif (type == BPF_WRITE) {\n\t\t\tint treg = BPF_REG_9;\n\n\t\t\tif (si->src_reg == treg || si->dst_reg == treg)\n\t\t\t\t--treg;\n\t\t\tif (si->src_reg == treg || si->dst_reg == treg)\n\t\t\t\t--treg;\n\t\t\t*insn++ = BPF_STX_MEM(BPF_DW, si->dst_reg, treg,\n\t\t\t\t\t      offsetof(struct bpf_sockopt_kern, tmp_reg));\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sockopt_kern, current_task),\n\t\t\t\t\t      treg, si->dst_reg,\n\t\t\t\t\t      offsetof(struct bpf_sockopt_kern, current_task));\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct task_struct, bpf_ctx),\n\t\t\t\t\t      treg, treg,\n\t\t\t\t\t      offsetof(struct task_struct, bpf_ctx));\n\t\t\t*insn++ = BPF_RAW_INSN(BPF_CLASS(si->code) | BPF_MEM |\n\t\t\t\t\t       BPF_FIELD_SIZEOF(struct bpf_cg_run_ctx, retval),\n\t\t\t\t\t       treg, si->src_reg,\n\t\t\t\t\t       offsetof(struct bpf_cg_run_ctx, retval),\n\t\t\t\t\t       si->imm);\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_DW, treg, si->dst_reg,\n\t\t\t\t\t      offsetof(struct bpf_sockopt_kern, tmp_reg));\n\t\t} else {\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_sockopt_kern, current_task),\n\t\t\t\t\t      si->dst_reg, si->src_reg,\n\t\t\t\t\t      offsetof(struct bpf_sockopt_kern, current_task));\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct task_struct, bpf_ctx),\n\t\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t\t      offsetof(struct task_struct, bpf_ctx));\n\t\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_cg_run_ctx, retval),\n\t\t\t\t\t      si->dst_reg, si->dst_reg,\n\t\t\t\t\t      offsetof(struct bpf_cg_run_ctx, retval));\n\t\t}\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, optval):\n\t\t*insn++ = CG_SOCKOPT_READ_FIELD(optval);\n\t\tbreak;\n\tcase offsetof(struct bpf_sockopt, optval_end):\n\t\t*insn++ = CG_SOCKOPT_READ_FIELD(optval_end);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nstatic int cg_sockopt_get_prologue(struct bpf_insn *insn_buf,\n\t\t\t\t   bool direct_write,\n\t\t\t\t   const struct bpf_prog *prog)\n{\n\t \n\treturn 0;\n}\n\nconst struct bpf_verifier_ops cg_sockopt_verifier_ops = {\n\t.get_func_proto\t\t= cg_sockopt_func_proto,\n\t.is_valid_access\t= cg_sockopt_is_valid_access,\n\t.convert_ctx_access\t= cg_sockopt_convert_ctx_access,\n\t.gen_prologue\t\t= cg_sockopt_get_prologue,\n};\n\nconst struct bpf_prog_ops cg_sockopt_prog_ops = {\n};\n\n \nconst struct bpf_func_proto *\ncgroup_common_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_get_local_storage:\n\t\treturn &bpf_get_local_storage_proto;\n\tcase BPF_FUNC_get_retval:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET_INGRESS:\n\t\tcase BPF_CGROUP_INET_EGRESS:\n\t\tcase BPF_CGROUP_SOCK_OPS:\n\t\tcase BPF_CGROUP_UDP4_RECVMSG:\n\t\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\t\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\t\t\treturn NULL;\n\t\tdefault:\n\t\t\treturn &bpf_get_retval_proto;\n\t\t}\n\tcase BPF_FUNC_set_retval:\n\t\tswitch (prog->expected_attach_type) {\n\t\tcase BPF_CGROUP_INET_INGRESS:\n\t\tcase BPF_CGROUP_INET_EGRESS:\n\t\tcase BPF_CGROUP_SOCK_OPS:\n\t\tcase BPF_CGROUP_UDP4_RECVMSG:\n\t\tcase BPF_CGROUP_UDP6_RECVMSG:\n\t\tcase BPF_CGROUP_INET4_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET6_GETPEERNAME:\n\t\tcase BPF_CGROUP_INET4_GETSOCKNAME:\n\t\tcase BPF_CGROUP_INET6_GETSOCKNAME:\n\t\t\treturn NULL;\n\t\tdefault:\n\t\t\treturn &bpf_set_retval_proto;\n\t\t}\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\n \nconst struct bpf_func_proto *\ncgroup_current_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tcase BPF_FUNC_get_current_pid_tgid:\n\t\treturn &bpf_get_current_pid_tgid_proto;\n\tcase BPF_FUNC_get_current_comm:\n\t\treturn &bpf_get_current_comm_proto;\n#ifdef CONFIG_CGROUP_NET_CLASSID\n\tcase BPF_FUNC_get_cgroup_classid:\n\t\treturn &bpf_get_cgroup_classid_curr_proto;\n#endif\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}