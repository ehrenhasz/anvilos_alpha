{
  "module_name": "trace_irqsoff.c",
  "hash_id": "3f53e83724416950e754330e3185fb3821a072cc68758a9c6f99e7c5bf272ad9",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace_irqsoff.c",
  "human_readable_source": "\n \n#include <linux/kallsyms.h>\n#include <linux/uaccess.h>\n#include <linux/module.h>\n#include <linux/ftrace.h>\n#include <linux/kprobes.h>\n\n#include \"trace.h\"\n\n#include <trace/events/preemptirq.h>\n\n#if defined(CONFIG_IRQSOFF_TRACER) || defined(CONFIG_PREEMPT_TRACER)\nstatic struct trace_array\t\t*irqsoff_trace __read_mostly;\nstatic int\t\t\t\ttracer_enabled __read_mostly;\n\nstatic DEFINE_PER_CPU(int, tracing_cpu);\n\nstatic DEFINE_RAW_SPINLOCK(max_trace_lock);\n\nenum {\n\tTRACER_IRQS_OFF\t\t= (1 << 1),\n\tTRACER_PREEMPT_OFF\t= (1 << 2),\n};\n\nstatic int trace_type __read_mostly;\n\nstatic int save_flags;\n\nstatic void stop_irqsoff_tracer(struct trace_array *tr, int graph);\nstatic int start_irqsoff_tracer(struct trace_array *tr, int graph);\n\n#ifdef CONFIG_PREEMPT_TRACER\nstatic inline int\npreempt_trace(int pc)\n{\n\treturn ((trace_type & TRACER_PREEMPT_OFF) && pc);\n}\n#else\n# define preempt_trace(pc) (0)\n#endif\n\n#ifdef CONFIG_IRQSOFF_TRACER\nstatic inline int\nirq_trace(void)\n{\n\treturn ((trace_type & TRACER_IRQS_OFF) &&\n\t\tirqs_disabled());\n}\n#else\n# define irq_trace() (0)\n#endif\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\nstatic int irqsoff_display_graph(struct trace_array *tr, int set);\n# define is_graph(tr) ((tr)->trace_flags & TRACE_ITER_DISPLAY_GRAPH)\n#else\nstatic inline int irqsoff_display_graph(struct trace_array *tr, int set)\n{\n\treturn -EINVAL;\n}\n# define is_graph(tr) false\n#endif\n\n \nstatic __cacheline_aligned_in_smp\tunsigned long max_sequence;\n\n#ifdef CONFIG_FUNCTION_TRACER\n \nstatic int func_prolog_dec(struct trace_array *tr,\n\t\t\t   struct trace_array_cpu **data,\n\t\t\t   unsigned long *flags)\n{\n\tlong disabled;\n\tint cpu;\n\n\t \n\tcpu = raw_smp_processor_id();\n\tif (likely(!per_cpu(tracing_cpu, cpu)))\n\t\treturn 0;\n\n\tlocal_save_flags(*flags);\n\t \n\tif (!irqs_disabled_flags(*flags) && !preempt_count())\n\t\treturn 0;\n\n\t*data = per_cpu_ptr(tr->array_buffer.data, cpu);\n\tdisabled = atomic_inc_return(&(*data)->disabled);\n\n\tif (likely(disabled == 1))\n\t\treturn 1;\n\n\tatomic_dec(&(*data)->disabled);\n\n\treturn 0;\n}\n\n \nstatic void\nirqsoff_tracer_call(unsigned long ip, unsigned long parent_ip,\n\t\t    struct ftrace_ops *op, struct ftrace_regs *fregs)\n{\n\tstruct trace_array *tr = irqsoff_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\n\tif (!func_prolog_dec(tr, &data, &flags))\n\t\treturn;\n\n\ttrace_ctx = tracing_gen_ctx_flags(flags);\n\n\ttrace_function(tr, ip, parent_ip, trace_ctx);\n\n\tatomic_dec(&data->disabled);\n}\n#endif  \n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\nstatic int irqsoff_display_graph(struct trace_array *tr, int set)\n{\n\tint cpu;\n\n\tif (!(is_graph(tr) ^ set))\n\t\treturn 0;\n\n\tstop_irqsoff_tracer(irqsoff_trace, !set);\n\n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(tracing_cpu, cpu) = 0;\n\n\ttr->max_latency = 0;\n\ttracing_reset_online_cpus(&irqsoff_trace->array_buffer);\n\n\treturn start_irqsoff_tracer(irqsoff_trace, set);\n}\n\nstatic int irqsoff_graph_entry(struct ftrace_graph_ent *trace)\n{\n\tstruct trace_array *tr = irqsoff_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\tint ret;\n\n\tif (ftrace_graph_ignore_func(trace))\n\t\treturn 0;\n\t \n\tif (ftrace_graph_notrace_addr(trace->func))\n\t\treturn 1;\n\n\tif (!func_prolog_dec(tr, &data, &flags))\n\t\treturn 0;\n\n\ttrace_ctx = tracing_gen_ctx_flags(flags);\n\tret = __trace_graph_entry(tr, trace, trace_ctx);\n\tatomic_dec(&data->disabled);\n\n\treturn ret;\n}\n\nstatic void irqsoff_graph_return(struct ftrace_graph_ret *trace)\n{\n\tstruct trace_array *tr = irqsoff_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\n\tftrace_graph_addr_finish(trace);\n\n\tif (!func_prolog_dec(tr, &data, &flags))\n\t\treturn;\n\n\ttrace_ctx = tracing_gen_ctx_flags(flags);\n\t__trace_graph_return(tr, trace, trace_ctx);\n\tatomic_dec(&data->disabled);\n}\n\nstatic struct fgraph_ops fgraph_ops = {\n\t.entryfunc\t\t= &irqsoff_graph_entry,\n\t.retfunc\t\t= &irqsoff_graph_return,\n};\n\nstatic void irqsoff_trace_open(struct trace_iterator *iter)\n{\n\tif (is_graph(iter->tr))\n\t\tgraph_trace_open(iter);\n\telse\n\t\titer->private = NULL;\n}\n\nstatic void irqsoff_trace_close(struct trace_iterator *iter)\n{\n\tif (iter->private)\n\t\tgraph_trace_close(iter);\n}\n\n#define GRAPH_TRACER_FLAGS (TRACE_GRAPH_PRINT_CPU | \\\n\t\t\t    TRACE_GRAPH_PRINT_PROC | \\\n\t\t\t    TRACE_GRAPH_PRINT_REL_TIME | \\\n\t\t\t    TRACE_GRAPH_PRINT_DURATION)\n\nstatic enum print_line_t irqsoff_print_line(struct trace_iterator *iter)\n{\n\t \n\tif (is_graph(iter->tr))\n\t\treturn print_graph_function_flags(iter, GRAPH_TRACER_FLAGS);\n\n\treturn TRACE_TYPE_UNHANDLED;\n}\n\nstatic void irqsoff_print_header(struct seq_file *s)\n{\n\tstruct trace_array *tr = irqsoff_trace;\n\n\tif (is_graph(tr))\n\t\tprint_graph_headers_flags(s, GRAPH_TRACER_FLAGS);\n\telse\n\t\ttrace_default_header(s);\n}\n\nstatic void\n__trace_function(struct trace_array *tr,\n\t\t unsigned long ip, unsigned long parent_ip,\n\t\t unsigned int trace_ctx)\n{\n\tif (is_graph(tr))\n\t\ttrace_graph_function(tr, ip, parent_ip, trace_ctx);\n\telse\n\t\ttrace_function(tr, ip, parent_ip, trace_ctx);\n}\n\n#else\n#define __trace_function trace_function\n\nstatic enum print_line_t irqsoff_print_line(struct trace_iterator *iter)\n{\n\treturn TRACE_TYPE_UNHANDLED;\n}\n\nstatic void irqsoff_trace_open(struct trace_iterator *iter) { }\nstatic void irqsoff_trace_close(struct trace_iterator *iter) { }\n\n#ifdef CONFIG_FUNCTION_TRACER\nstatic void irqsoff_print_header(struct seq_file *s)\n{\n\ttrace_default_header(s);\n}\n#else\nstatic void irqsoff_print_header(struct seq_file *s)\n{\n\ttrace_latency_header(s);\n}\n#endif  \n#endif  \n\n \nstatic bool report_latency(struct trace_array *tr, u64 delta)\n{\n\tif (tracing_thresh) {\n\t\tif (delta < tracing_thresh)\n\t\t\treturn false;\n\t} else {\n\t\tif (delta <= tr->max_latency)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic void\ncheck_critical_timing(struct trace_array *tr,\n\t\t      struct trace_array_cpu *data,\n\t\t      unsigned long parent_ip,\n\t\t      int cpu)\n{\n\tu64 T0, T1, delta;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\n\tT0 = data->preempt_timestamp;\n\tT1 = ftrace_now(cpu);\n\tdelta = T1-T0;\n\n\ttrace_ctx = tracing_gen_ctx();\n\n\tif (!report_latency(tr, delta))\n\t\tgoto out;\n\n\traw_spin_lock_irqsave(&max_trace_lock, flags);\n\n\t \n\tif (!report_latency(tr, delta))\n\t\tgoto out_unlock;\n\n\t__trace_function(tr, CALLER_ADDR0, parent_ip, trace_ctx);\n\t \n\t__trace_stack(tr, trace_ctx, 5);\n\n\tif (data->critical_sequence != max_sequence)\n\t\tgoto out_unlock;\n\n\tdata->critical_end = parent_ip;\n\n\tif (likely(!is_tracing_stopped())) {\n\t\ttr->max_latency = delta;\n\t\tupdate_max_tr_single(tr, current, cpu);\n\t}\n\n\tmax_sequence++;\n\nout_unlock:\n\traw_spin_unlock_irqrestore(&max_trace_lock, flags);\n\nout:\n\tdata->critical_sequence = max_sequence;\n\tdata->preempt_timestamp = ftrace_now(cpu);\n\t__trace_function(tr, CALLER_ADDR0, parent_ip, trace_ctx);\n}\n\nstatic nokprobe_inline void\nstart_critical_timing(unsigned long ip, unsigned long parent_ip)\n{\n\tint cpu;\n\tstruct trace_array *tr = irqsoff_trace;\n\tstruct trace_array_cpu *data;\n\n\tif (!tracer_enabled || !tracing_is_enabled())\n\t\treturn;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (per_cpu(tracing_cpu, cpu))\n\t\treturn;\n\n\tdata = per_cpu_ptr(tr->array_buffer.data, cpu);\n\n\tif (unlikely(!data) || atomic_read(&data->disabled))\n\t\treturn;\n\n\tatomic_inc(&data->disabled);\n\n\tdata->critical_sequence = max_sequence;\n\tdata->preempt_timestamp = ftrace_now(cpu);\n\tdata->critical_start = parent_ip ? : ip;\n\n\t__trace_function(tr, ip, parent_ip, tracing_gen_ctx());\n\n\tper_cpu(tracing_cpu, cpu) = 1;\n\n\tatomic_dec(&data->disabled);\n}\n\nstatic nokprobe_inline void\nstop_critical_timing(unsigned long ip, unsigned long parent_ip)\n{\n\tint cpu;\n\tstruct trace_array *tr = irqsoff_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned int trace_ctx;\n\n\tcpu = raw_smp_processor_id();\n\t \n\tif (unlikely(per_cpu(tracing_cpu, cpu)))\n\t\tper_cpu(tracing_cpu, cpu) = 0;\n\telse\n\t\treturn;\n\n\tif (!tracer_enabled || !tracing_is_enabled())\n\t\treturn;\n\n\tdata = per_cpu_ptr(tr->array_buffer.data, cpu);\n\n\tif (unlikely(!data) ||\n\t    !data->critical_start || atomic_read(&data->disabled))\n\t\treturn;\n\n\tatomic_inc(&data->disabled);\n\n\ttrace_ctx = tracing_gen_ctx();\n\t__trace_function(tr, ip, parent_ip, trace_ctx);\n\tcheck_critical_timing(tr, data, parent_ip ? : ip, cpu);\n\tdata->critical_start = 0;\n\tatomic_dec(&data->disabled);\n}\n\n \nvoid start_critical_timings(void)\n{\n\tif (preempt_trace(preempt_count()) || irq_trace())\n\t\tstart_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\n}\nEXPORT_SYMBOL_GPL(start_critical_timings);\nNOKPROBE_SYMBOL(start_critical_timings);\n\nvoid stop_critical_timings(void)\n{\n\tif (preempt_trace(preempt_count()) || irq_trace())\n\t\tstop_critical_timing(CALLER_ADDR0, CALLER_ADDR1);\n}\nEXPORT_SYMBOL_GPL(stop_critical_timings);\nNOKPROBE_SYMBOL(stop_critical_timings);\n\n#ifdef CONFIG_FUNCTION_TRACER\nstatic bool function_enabled;\n\nstatic int register_irqsoff_function(struct trace_array *tr, int graph, int set)\n{\n\tint ret;\n\n\t \n\tif (function_enabled || (!set && !(tr->trace_flags & TRACE_ITER_FUNCTION)))\n\t\treturn 0;\n\n\tif (graph)\n\t\tret = register_ftrace_graph(&fgraph_ops);\n\telse\n\t\tret = register_ftrace_function(tr->ops);\n\n\tif (!ret)\n\t\tfunction_enabled = true;\n\n\treturn ret;\n}\n\nstatic void unregister_irqsoff_function(struct trace_array *tr, int graph)\n{\n\tif (!function_enabled)\n\t\treturn;\n\n\tif (graph)\n\t\tunregister_ftrace_graph(&fgraph_ops);\n\telse\n\t\tunregister_ftrace_function(tr->ops);\n\n\tfunction_enabled = false;\n}\n\nstatic int irqsoff_function_set(struct trace_array *tr, u32 mask, int set)\n{\n\tif (!(mask & TRACE_ITER_FUNCTION))\n\t\treturn 0;\n\n\tif (set)\n\t\tregister_irqsoff_function(tr, is_graph(tr), 1);\n\telse\n\t\tunregister_irqsoff_function(tr, is_graph(tr));\n\treturn 1;\n}\n#else\nstatic int register_irqsoff_function(struct trace_array *tr, int graph, int set)\n{\n\treturn 0;\n}\nstatic void unregister_irqsoff_function(struct trace_array *tr, int graph) { }\nstatic inline int irqsoff_function_set(struct trace_array *tr, u32 mask, int set)\n{\n\treturn 0;\n}\n#endif  \n\nstatic int irqsoff_flag_changed(struct trace_array *tr, u32 mask, int set)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\n\tif (irqsoff_function_set(tr, mask, set))\n\t\treturn 0;\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tif (mask & TRACE_ITER_DISPLAY_GRAPH)\n\t\treturn irqsoff_display_graph(tr, set);\n#endif\n\n\treturn trace_keep_overwrite(tracer, mask, set);\n}\n\nstatic int start_irqsoff_tracer(struct trace_array *tr, int graph)\n{\n\tint ret;\n\n\tret = register_irqsoff_function(tr, graph, 0);\n\n\tif (!ret && tracing_is_enabled())\n\t\ttracer_enabled = 1;\n\telse\n\t\ttracer_enabled = 0;\n\n\treturn ret;\n}\n\nstatic void stop_irqsoff_tracer(struct trace_array *tr, int graph)\n{\n\ttracer_enabled = 0;\n\n\tunregister_irqsoff_function(tr, graph);\n}\n\nstatic bool irqsoff_busy;\n\nstatic int __irqsoff_tracer_init(struct trace_array *tr)\n{\n\tif (irqsoff_busy)\n\t\treturn -EBUSY;\n\n\tsave_flags = tr->trace_flags;\n\n\t \n\tset_tracer_flag(tr, TRACE_ITER_OVERWRITE, 1);\n\tset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, 1);\n\t \n\tset_tracer_flag(tr, TRACE_ITER_PAUSE_ON_TRACE, 1);\n\n\ttr->max_latency = 0;\n\tirqsoff_trace = tr;\n\t \n\tsmp_wmb();\n\n\tftrace_init_array_ops(tr, irqsoff_tracer_call);\n\n\t \n\tif (start_irqsoff_tracer(tr, (tr->flags & TRACE_ARRAY_FL_GLOBAL &&\n\t\t\t\t      is_graph(tr))))\n\t\tprintk(KERN_ERR \"failed to start irqsoff tracer\\n\");\n\n\tirqsoff_busy = true;\n\treturn 0;\n}\n\nstatic void __irqsoff_tracer_reset(struct trace_array *tr)\n{\n\tint lat_flag = save_flags & TRACE_ITER_LATENCY_FMT;\n\tint overwrite_flag = save_flags & TRACE_ITER_OVERWRITE;\n\tint pause_flag = save_flags & TRACE_ITER_PAUSE_ON_TRACE;\n\n\tstop_irqsoff_tracer(tr, is_graph(tr));\n\n\tset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, lat_flag);\n\tset_tracer_flag(tr, TRACE_ITER_OVERWRITE, overwrite_flag);\n\tset_tracer_flag(tr, TRACE_ITER_PAUSE_ON_TRACE, pause_flag);\n\tftrace_reset_array_ops(tr);\n\n\tirqsoff_busy = false;\n}\n\nstatic void irqsoff_tracer_start(struct trace_array *tr)\n{\n\ttracer_enabled = 1;\n}\n\nstatic void irqsoff_tracer_stop(struct trace_array *tr)\n{\n\ttracer_enabled = 0;\n}\n\n#ifdef CONFIG_IRQSOFF_TRACER\n \nvoid tracer_hardirqs_on(unsigned long a0, unsigned long a1)\n{\n\tif (!preempt_trace(preempt_count()) && irq_trace())\n\t\tstop_critical_timing(a0, a1);\n}\nNOKPROBE_SYMBOL(tracer_hardirqs_on);\n\nvoid tracer_hardirqs_off(unsigned long a0, unsigned long a1)\n{\n\tif (!preempt_trace(preempt_count()) && irq_trace())\n\t\tstart_critical_timing(a0, a1);\n}\nNOKPROBE_SYMBOL(tracer_hardirqs_off);\n\nstatic int irqsoff_tracer_init(struct trace_array *tr)\n{\n\ttrace_type = TRACER_IRQS_OFF;\n\n\treturn __irqsoff_tracer_init(tr);\n}\n\nstatic void irqsoff_tracer_reset(struct trace_array *tr)\n{\n\t__irqsoff_tracer_reset(tr);\n}\n\nstatic struct tracer irqsoff_tracer __read_mostly =\n{\n\t.name\t\t= \"irqsoff\",\n\t.init\t\t= irqsoff_tracer_init,\n\t.reset\t\t= irqsoff_tracer_reset,\n\t.start\t\t= irqsoff_tracer_start,\n\t.stop\t\t= irqsoff_tracer_stop,\n\t.print_max\t= true,\n\t.print_header   = irqsoff_print_header,\n\t.print_line     = irqsoff_print_line,\n\t.flag_changed\t= irqsoff_flag_changed,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest    = trace_selftest_startup_irqsoff,\n#endif\n\t.open           = irqsoff_trace_open,\n\t.close          = irqsoff_trace_close,\n\t.allow_instances = true,\n\t.use_max_tr\t= true,\n};\n#endif  \n\n#ifdef CONFIG_PREEMPT_TRACER\nvoid tracer_preempt_on(unsigned long a0, unsigned long a1)\n{\n\tif (preempt_trace(preempt_count()) && !irq_trace())\n\t\tstop_critical_timing(a0, a1);\n}\n\nvoid tracer_preempt_off(unsigned long a0, unsigned long a1)\n{\n\tif (preempt_trace(preempt_count()) && !irq_trace())\n\t\tstart_critical_timing(a0, a1);\n}\n\nstatic int preemptoff_tracer_init(struct trace_array *tr)\n{\n\ttrace_type = TRACER_PREEMPT_OFF;\n\n\treturn __irqsoff_tracer_init(tr);\n}\n\nstatic void preemptoff_tracer_reset(struct trace_array *tr)\n{\n\t__irqsoff_tracer_reset(tr);\n}\n\nstatic struct tracer preemptoff_tracer __read_mostly =\n{\n\t.name\t\t= \"preemptoff\",\n\t.init\t\t= preemptoff_tracer_init,\n\t.reset\t\t= preemptoff_tracer_reset,\n\t.start\t\t= irqsoff_tracer_start,\n\t.stop\t\t= irqsoff_tracer_stop,\n\t.print_max\t= true,\n\t.print_header   = irqsoff_print_header,\n\t.print_line     = irqsoff_print_line,\n\t.flag_changed\t= irqsoff_flag_changed,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest    = trace_selftest_startup_preemptoff,\n#endif\n\t.open\t\t= irqsoff_trace_open,\n\t.close\t\t= irqsoff_trace_close,\n\t.allow_instances = true,\n\t.use_max_tr\t= true,\n};\n#endif  \n\n#if defined(CONFIG_IRQSOFF_TRACER) && defined(CONFIG_PREEMPT_TRACER)\n\nstatic int preemptirqsoff_tracer_init(struct trace_array *tr)\n{\n\ttrace_type = TRACER_IRQS_OFF | TRACER_PREEMPT_OFF;\n\n\treturn __irqsoff_tracer_init(tr);\n}\n\nstatic void preemptirqsoff_tracer_reset(struct trace_array *tr)\n{\n\t__irqsoff_tracer_reset(tr);\n}\n\nstatic struct tracer preemptirqsoff_tracer __read_mostly =\n{\n\t.name\t\t= \"preemptirqsoff\",\n\t.init\t\t= preemptirqsoff_tracer_init,\n\t.reset\t\t= preemptirqsoff_tracer_reset,\n\t.start\t\t= irqsoff_tracer_start,\n\t.stop\t\t= irqsoff_tracer_stop,\n\t.print_max\t= true,\n\t.print_header   = irqsoff_print_header,\n\t.print_line     = irqsoff_print_line,\n\t.flag_changed\t= irqsoff_flag_changed,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest    = trace_selftest_startup_preemptirqsoff,\n#endif\n\t.open\t\t= irqsoff_trace_open,\n\t.close\t\t= irqsoff_trace_close,\n\t.allow_instances = true,\n\t.use_max_tr\t= true,\n};\n#endif\n\n__init static int init_irqsoff_tracer(void)\n{\n#ifdef CONFIG_IRQSOFF_TRACER\n\tregister_tracer(&irqsoff_tracer);\n#endif\n#ifdef CONFIG_PREEMPT_TRACER\n\tregister_tracer(&preemptoff_tracer);\n#endif\n#if defined(CONFIG_IRQSOFF_TRACER) && defined(CONFIG_PREEMPT_TRACER)\n\tregister_tracer(&preemptirqsoff_tracer);\n#endif\n\n\treturn 0;\n}\ncore_initcall(init_irqsoff_tracer);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}