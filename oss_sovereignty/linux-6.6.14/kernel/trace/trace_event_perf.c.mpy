{
  "module_name": "trace_event_perf.c",
  "hash_id": "e951c7fed051c0fc1e3b864f0b30f081b84a216b198978b1a88331a78171b96a",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace_event_perf.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/kprobes.h>\n#include <linux/security.h>\n#include \"trace.h\"\n#include \"trace_probe.h\"\n\nstatic char __percpu *perf_trace_buf[PERF_NR_CONTEXTS];\n\n \ntypedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])\n\tperf_trace_t;\n\n \nstatic int\ttotal_ref_count;\n\nstatic int perf_trace_event_perm(struct trace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\tint ret;\n\n\tif (tp_event->perf_perm) {\n\t\tret = tp_event->perf_perm(tp_event, p_event);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tif (p_event->parent)\n\t\treturn 0;\n\n\t \n\n\t \n\tif (ftrace_event_is_function(tp_event)) {\n\t\tret = perf_allow_tracepoint(&p_event->attr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (!is_sampling_event(p_event))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!p_event->attr.exclude_callchain_user)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (p_event->attr.sample_type & PERF_SAMPLE_STACK_USER)\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))\n\t\treturn 0;\n\n\t \n\tif (p_event->attach_state == PERF_ATTACH_TASK) {\n\t\tif (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)\n\t\t\treturn 0;\n\t}\n\n\t \n\tret = perf_allow_tracepoint(&p_event->attr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic int perf_trace_event_reg(struct trace_event_call *tp_event,\n\t\t\t\tstruct perf_event *p_event)\n{\n\tstruct hlist_head __percpu *list;\n\tint ret = -ENOMEM;\n\tint cpu;\n\n\tp_event->tp_event = tp_event;\n\tif (tp_event->perf_refcount++ > 0)\n\t\treturn 0;\n\n\tlist = alloc_percpu(struct hlist_head);\n\tif (!list)\n\t\tgoto fail;\n\n\tfor_each_possible_cpu(cpu)\n\t\tINIT_HLIST_HEAD(per_cpu_ptr(list, cpu));\n\n\ttp_event->perf_events = list;\n\n\tif (!total_ref_count) {\n\t\tchar __percpu *buf;\n\t\tint i;\n\n\t\tfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\n\t\t\tbuf = (char __percpu *)alloc_percpu(perf_trace_t);\n\t\t\tif (!buf)\n\t\t\t\tgoto fail;\n\n\t\t\tperf_trace_buf[i] = buf;\n\t\t}\n\t}\n\n\tret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER, NULL);\n\tif (ret)\n\t\tgoto fail;\n\n\ttotal_ref_count++;\n\treturn 0;\n\nfail:\n\tif (!total_ref_count) {\n\t\tint i;\n\n\t\tfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\n\t\t\tfree_percpu(perf_trace_buf[i]);\n\t\t\tperf_trace_buf[i] = NULL;\n\t\t}\n\t}\n\n\tif (!--tp_event->perf_refcount) {\n\t\tfree_percpu(tp_event->perf_events);\n\t\ttp_event->perf_events = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic void perf_trace_event_unreg(struct perf_event *p_event)\n{\n\tstruct trace_event_call *tp_event = p_event->tp_event;\n\tint i;\n\n\tif (--tp_event->perf_refcount > 0)\n\t\treturn;\n\n\ttp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER, NULL);\n\n\t \n\ttracepoint_synchronize_unregister();\n\n\tfree_percpu(tp_event->perf_events);\n\ttp_event->perf_events = NULL;\n\n\tif (!--total_ref_count) {\n\t\tfor (i = 0; i < PERF_NR_CONTEXTS; i++) {\n\t\t\tfree_percpu(perf_trace_buf[i]);\n\t\t\tperf_trace_buf[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic int perf_trace_event_open(struct perf_event *p_event)\n{\n\tstruct trace_event_call *tp_event = p_event->tp_event;\n\treturn tp_event->class->reg(tp_event, TRACE_REG_PERF_OPEN, p_event);\n}\n\nstatic void perf_trace_event_close(struct perf_event *p_event)\n{\n\tstruct trace_event_call *tp_event = p_event->tp_event;\n\ttp_event->class->reg(tp_event, TRACE_REG_PERF_CLOSE, p_event);\n}\n\nstatic int perf_trace_event_init(struct trace_event_call *tp_event,\n\t\t\t\t struct perf_event *p_event)\n{\n\tint ret;\n\n\tret = perf_trace_event_perm(tp_event, p_event);\n\tif (ret)\n\t\treturn ret;\n\n\tret = perf_trace_event_reg(tp_event, p_event);\n\tif (ret)\n\t\treturn ret;\n\n\tret = perf_trace_event_open(p_event);\n\tif (ret) {\n\t\tperf_trace_event_unreg(p_event);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nint perf_trace_init(struct perf_event *p_event)\n{\n\tstruct trace_event_call *tp_event;\n\tu64 event_id = p_event->attr.config;\n\tint ret = -EINVAL;\n\n\tmutex_lock(&event_mutex);\n\tlist_for_each_entry(tp_event, &ftrace_events, list) {\n\t\tif (tp_event->event.type == event_id &&\n\t\t    tp_event->class && tp_event->class->reg &&\n\t\t    trace_event_try_get_ref(tp_event)) {\n\t\t\tret = perf_trace_event_init(tp_event, p_event);\n\t\t\tif (ret)\n\t\t\t\ttrace_event_put_ref(tp_event);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nvoid perf_trace_destroy(struct perf_event *p_event)\n{\n\tmutex_lock(&event_mutex);\n\tperf_trace_event_close(p_event);\n\tperf_trace_event_unreg(p_event);\n\ttrace_event_put_ref(p_event->tp_event);\n\tmutex_unlock(&event_mutex);\n}\n\n#ifdef CONFIG_KPROBE_EVENTS\nint perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)\n{\n\tint ret;\n\tchar *func = NULL;\n\tstruct trace_event_call *tp_event;\n\n\tif (p_event->attr.kprobe_func) {\n\t\tfunc = strndup_user(u64_to_user_ptr(p_event->attr.kprobe_func),\n\t\t\t\t    KSYM_NAME_LEN);\n\t\tif (IS_ERR(func)) {\n\t\t\tret = PTR_ERR(func);\n\t\t\treturn (ret == -EINVAL) ? -E2BIG : ret;\n\t\t}\n\n\t\tif (func[0] == '\\0') {\n\t\t\tkfree(func);\n\t\t\tfunc = NULL;\n\t\t}\n\t}\n\n\ttp_event = create_local_trace_kprobe(\n\t\tfunc, (void *)(unsigned long)(p_event->attr.kprobe_addr),\n\t\tp_event->attr.probe_offset, is_retprobe);\n\tif (IS_ERR(tp_event)) {\n\t\tret = PTR_ERR(tp_event);\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&event_mutex);\n\tret = perf_trace_event_init(tp_event, p_event);\n\tif (ret)\n\t\tdestroy_local_trace_kprobe(tp_event);\n\tmutex_unlock(&event_mutex);\nout:\n\tkfree(func);\n\treturn ret;\n}\n\nvoid perf_kprobe_destroy(struct perf_event *p_event)\n{\n\tmutex_lock(&event_mutex);\n\tperf_trace_event_close(p_event);\n\tperf_trace_event_unreg(p_event);\n\ttrace_event_put_ref(p_event->tp_event);\n\tmutex_unlock(&event_mutex);\n\n\tdestroy_local_trace_kprobe(p_event->tp_event);\n}\n#endif  \n\n#ifdef CONFIG_UPROBE_EVENTS\nint perf_uprobe_init(struct perf_event *p_event,\n\t\t     unsigned long ref_ctr_offset, bool is_retprobe)\n{\n\tint ret;\n\tchar *path = NULL;\n\tstruct trace_event_call *tp_event;\n\n\tif (!p_event->attr.uprobe_path)\n\t\treturn -EINVAL;\n\n\tpath = strndup_user(u64_to_user_ptr(p_event->attr.uprobe_path),\n\t\t\t    PATH_MAX);\n\tif (IS_ERR(path)) {\n\t\tret = PTR_ERR(path);\n\t\treturn (ret == -EINVAL) ? -E2BIG : ret;\n\t}\n\tif (path[0] == '\\0') {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttp_event = create_local_trace_uprobe(path, p_event->attr.probe_offset,\n\t\t\t\t\t     ref_ctr_offset, is_retprobe);\n\tif (IS_ERR(tp_event)) {\n\t\tret = PTR_ERR(tp_event);\n\t\tgoto out;\n\t}\n\n\t \n\tmutex_lock(&event_mutex);\n\tret = perf_trace_event_init(tp_event, p_event);\n\tif (ret)\n\t\tdestroy_local_trace_uprobe(tp_event);\n\tmutex_unlock(&event_mutex);\nout:\n\tkfree(path);\n\treturn ret;\n}\n\nvoid perf_uprobe_destroy(struct perf_event *p_event)\n{\n\tmutex_lock(&event_mutex);\n\tperf_trace_event_close(p_event);\n\tperf_trace_event_unreg(p_event);\n\ttrace_event_put_ref(p_event->tp_event);\n\tmutex_unlock(&event_mutex);\n\tdestroy_local_trace_uprobe(p_event->tp_event);\n}\n#endif  \n\nint perf_trace_add(struct perf_event *p_event, int flags)\n{\n\tstruct trace_event_call *tp_event = p_event->tp_event;\n\n\tif (!(flags & PERF_EF_START))\n\t\tp_event->hw.state = PERF_HES_STOPPED;\n\n\t \n\tif (!tp_event->class->reg(tp_event, TRACE_REG_PERF_ADD, p_event)) {\n\t\tstruct hlist_head __percpu *pcpu_list;\n\t\tstruct hlist_head *list;\n\n\t\tpcpu_list = tp_event->perf_events;\n\t\tif (WARN_ON_ONCE(!pcpu_list))\n\t\t\treturn -EINVAL;\n\n\t\tlist = this_cpu_ptr(pcpu_list);\n\t\thlist_add_head_rcu(&p_event->hlist_entry, list);\n\t}\n\n\treturn 0;\n}\n\nvoid perf_trace_del(struct perf_event *p_event, int flags)\n{\n\tstruct trace_event_call *tp_event = p_event->tp_event;\n\n\t \n\tif (!tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event))\n\t\thlist_del_rcu(&p_event->hlist_entry);\n}\n\nvoid *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp)\n{\n\tchar *raw_data;\n\tint rctx;\n\n\tBUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));\n\n\tif (WARN_ONCE(size > PERF_MAX_TRACE_SIZE,\n\t\t      \"perf buffer not large enough, wanted %d, have %d\",\n\t\t      size, PERF_MAX_TRACE_SIZE))\n\t\treturn NULL;\n\n\t*rctxp = rctx = perf_swevent_get_recursion_context();\n\tif (rctx < 0)\n\t\treturn NULL;\n\n\tif (regs)\n\t\t*regs = this_cpu_ptr(&__perf_regs[rctx]);\n\traw_data = this_cpu_ptr(perf_trace_buf[rctx]);\n\n\t \n\tmemset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));\n\treturn raw_data;\n}\nEXPORT_SYMBOL_GPL(perf_trace_buf_alloc);\nNOKPROBE_SYMBOL(perf_trace_buf_alloc);\n\nvoid perf_trace_buf_update(void *record, u16 type)\n{\n\tstruct trace_entry *entry = record;\n\n\ttracing_generic_entry_update(entry, type, tracing_gen_ctx());\n}\nNOKPROBE_SYMBOL(perf_trace_buf_update);\n\n#ifdef CONFIG_FUNCTION_TRACER\nstatic void\nperf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,\n\t\t\t  struct ftrace_ops *ops,  struct ftrace_regs *fregs)\n{\n\tstruct ftrace_entry *entry;\n\tstruct perf_event *event;\n\tstruct hlist_head head;\n\tstruct pt_regs regs;\n\tint rctx;\n\tint bit;\n\n\tif (!rcu_is_watching())\n\t\treturn;\n\n\tbit = ftrace_test_recursion_trylock(ip, parent_ip);\n\tif (bit < 0)\n\t\treturn;\n\n\tif ((unsigned long)ops->private != smp_processor_id())\n\t\tgoto out;\n\n\tevent = container_of(ops, struct perf_event, ftrace_ops);\n\n\t \n\thead.first = &event->hlist_entry;\n\n#define ENTRY_SIZE (ALIGN(sizeof(struct ftrace_entry) + sizeof(u32), \\\n\t\t    sizeof(u64)) - sizeof(u32))\n\n\tBUILD_BUG_ON(ENTRY_SIZE > PERF_MAX_TRACE_SIZE);\n\n\tmemset(&regs, 0, sizeof(regs));\n\tperf_fetch_caller_regs(&regs);\n\n\tentry = perf_trace_buf_alloc(ENTRY_SIZE, NULL, &rctx);\n\tif (!entry)\n\t\tgoto out;\n\n\tentry->ip = ip;\n\tentry->parent_ip = parent_ip;\n\tperf_trace_buf_submit(entry, ENTRY_SIZE, rctx, TRACE_FN,\n\t\t\t      1, &regs, &head, NULL);\n\nout:\n\tftrace_test_recursion_unlock(bit);\n#undef ENTRY_SIZE\n}\n\nstatic int perf_ftrace_function_register(struct perf_event *event)\n{\n\tstruct ftrace_ops *ops = &event->ftrace_ops;\n\n\tops->func    = perf_ftrace_function_call;\n\tops->private = (void *)(unsigned long)nr_cpu_ids;\n\n\treturn register_ftrace_function(ops);\n}\n\nstatic int perf_ftrace_function_unregister(struct perf_event *event)\n{\n\tstruct ftrace_ops *ops = &event->ftrace_ops;\n\tint ret = unregister_ftrace_function(ops);\n\tftrace_free_filter(ops);\n\treturn ret;\n}\n\nint perf_ftrace_event_register(struct trace_event_call *call,\n\t\t\t       enum trace_reg type, void *data)\n{\n\tstruct perf_event *event = data;\n\n\tswitch (type) {\n\tcase TRACE_REG_REGISTER:\n\tcase TRACE_REG_UNREGISTER:\n\t\tbreak;\n\tcase TRACE_REG_PERF_REGISTER:\n\tcase TRACE_REG_PERF_UNREGISTER:\n\t\treturn 0;\n\tcase TRACE_REG_PERF_OPEN:\n\t\treturn perf_ftrace_function_register(data);\n\tcase TRACE_REG_PERF_CLOSE:\n\t\treturn perf_ftrace_function_unregister(data);\n\tcase TRACE_REG_PERF_ADD:\n\t\tevent->ftrace_ops.private = (void *)(unsigned long)smp_processor_id();\n\t\treturn 1;\n\tcase TRACE_REG_PERF_DEL:\n\t\tevent->ftrace_ops.private = (void *)(unsigned long)nr_cpu_ids;\n\t\treturn 1;\n\t}\n\n\treturn -EINVAL;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}