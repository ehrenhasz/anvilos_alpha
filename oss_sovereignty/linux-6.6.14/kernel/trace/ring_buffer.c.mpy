{
  "module_name": "ring_buffer.c",
  "hash_id": "985694303fe69922dd2df13d5a7889850165d604387bf86260ff7b861fefcb72",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/ring_buffer.c",
  "human_readable_source": "\n \n#include <linux/trace_recursion.h>\n#include <linux/trace_events.h>\n#include <linux/ring_buffer.h>\n#include <linux/trace_clock.h>\n#include <linux/sched/clock.h>\n#include <linux/trace_seq.h>\n#include <linux/spinlock.h>\n#include <linux/irq_work.h>\n#include <linux/security.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\t \n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/mutex.h>\n#include <linux/delay.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/hash.h>\n#include <linux/list.h>\n#include <linux/cpu.h>\n#include <linux/oom.h>\n\n#include <asm/local.h>\n\n \n#define TS_MSB\t\t(0xf8ULL << 56)\n#define ABS_TS_MASK\t(~TS_MSB)\n\nstatic void update_pages_handler(struct work_struct *work);\n\n \nint ring_buffer_print_entry_header(struct trace_seq *s)\n{\n\ttrace_seq_puts(s, \"# compressed entry header\\n\");\n\ttrace_seq_puts(s, \"\\ttype_len    :    5 bits\\n\");\n\ttrace_seq_puts(s, \"\\ttime_delta  :   27 bits\\n\");\n\ttrace_seq_puts(s, \"\\tarray       :   32 bits\\n\");\n\ttrace_seq_putc(s, '\\n');\n\ttrace_seq_printf(s, \"\\tpadding     : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_PADDING);\n\ttrace_seq_printf(s, \"\\ttime_extend : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_TIME_EXTEND);\n\ttrace_seq_printf(s, \"\\ttime_stamp : type == %d\\n\",\n\t\t\t RINGBUF_TYPE_TIME_STAMP);\n\ttrace_seq_printf(s, \"\\tdata max type_len  == %d\\n\",\n\t\t\t RINGBUF_TYPE_DATA_TYPE_LEN_MAX);\n\n\treturn !trace_seq_has_overflowed(s);\n}\n\n \n\n \n#define RB_BUFFER_OFF\t\t(1 << 20)\n\n#define BUF_PAGE_HDR_SIZE offsetof(struct buffer_data_page, data)\n\n#define RB_EVNT_HDR_SIZE (offsetof(struct ring_buffer_event, array))\n#define RB_ALIGNMENT\t\t4U\n#define RB_MAX_SMALL_DATA\t(RB_ALIGNMENT * RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n#define RB_EVNT_MIN_SIZE\t8U\t \n\n#ifndef CONFIG_HAVE_64BIT_ALIGNED_ACCESS\n# define RB_FORCE_8BYTE_ALIGNMENT\t0\n# define RB_ARCH_ALIGNMENT\t\tRB_ALIGNMENT\n#else\n# define RB_FORCE_8BYTE_ALIGNMENT\t1\n# define RB_ARCH_ALIGNMENT\t\t8U\n#endif\n\n#define RB_ALIGN_DATA\t\t__aligned(RB_ARCH_ALIGNMENT)\n\n \n#define RINGBUF_TYPE_DATA 0 ... RINGBUF_TYPE_DATA_TYPE_LEN_MAX\n\nenum {\n\tRB_LEN_TIME_EXTEND = 8,\n\tRB_LEN_TIME_STAMP =  8,\n};\n\n#define skip_time_extend(event) \\\n\t((struct ring_buffer_event *)((char *)event + RB_LEN_TIME_EXTEND))\n\n#define extended_time(event) \\\n\t(event->type_len >= RINGBUF_TYPE_TIME_EXTEND)\n\nstatic inline bool rb_null_event(struct ring_buffer_event *event)\n{\n\treturn event->type_len == RINGBUF_TYPE_PADDING && !event->time_delta;\n}\n\nstatic void rb_event_set_padding(struct ring_buffer_event *event)\n{\n\t \n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\tevent->time_delta = 0;\n}\n\nstatic unsigned\nrb_event_data_length(struct ring_buffer_event *event)\n{\n\tunsigned length;\n\n\tif (event->type_len)\n\t\tlength = event->type_len * RB_ALIGNMENT;\n\telse\n\t\tlength = event->array[0];\n\treturn length + RB_EVNT_HDR_SIZE;\n}\n\n \nstatic inline unsigned\nrb_event_length(struct ring_buffer_event *event)\n{\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event))\n\t\t\t \n\t\t\treturn -1;\n\t\treturn  event->array[0] + RB_EVNT_HDR_SIZE;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\treturn RB_LEN_TIME_EXTEND;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\treturn RB_LEN_TIME_STAMP;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\treturn rb_event_data_length(event);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\t \n\treturn 0;\n}\n\n \nstatic inline unsigned\nrb_event_ts_length(struct ring_buffer_event *event)\n{\n\tunsigned len = 0;\n\n\tif (extended_time(event)) {\n\t\t \n\t\tlen = RB_LEN_TIME_EXTEND;\n\t\tevent = skip_time_extend(event);\n\t}\n\treturn len + rb_event_length(event);\n}\n\n \nunsigned ring_buffer_event_length(struct ring_buffer_event *event)\n{\n\tunsigned length;\n\n\tif (extended_time(event))\n\t\tevent = skip_time_extend(event);\n\n\tlength = rb_event_length(event);\n\tif (event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n\t\treturn length;\n\tlength -= RB_EVNT_HDR_SIZE;\n\tif (length > RB_MAX_SMALL_DATA + sizeof(event->array[0]))\n                length -= sizeof(event->array[0]);\n\treturn length;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_event_length);\n\n \nstatic __always_inline void *\nrb_event_data(struct ring_buffer_event *event)\n{\n\tif (extended_time(event))\n\t\tevent = skip_time_extend(event);\n\tWARN_ON_ONCE(event->type_len > RINGBUF_TYPE_DATA_TYPE_LEN_MAX);\n\t \n\tif (event->type_len)\n\t\treturn (void *)&event->array[0];\n\t \n\treturn (void *)&event->array[1];\n}\n\n \nvoid *ring_buffer_event_data(struct ring_buffer_event *event)\n{\n\treturn rb_event_data(event);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_event_data);\n\n#define for_each_buffer_cpu(buffer, cpu)\t\t\\\n\tfor_each_cpu(cpu, buffer->cpumask)\n\n#define for_each_online_buffer_cpu(buffer, cpu)\t\t\\\n\tfor_each_cpu_and(cpu, buffer->cpumask, cpu_online_mask)\n\n#define TS_SHIFT\t27\n#define TS_MASK\t\t((1ULL << TS_SHIFT) - 1)\n#define TS_DELTA_TEST\t(~TS_MASK)\n\nstatic u64 rb_event_time_stamp(struct ring_buffer_event *event)\n{\n\tu64 ts;\n\n\tts = event->array[0];\n\tts <<= TS_SHIFT;\n\tts += event->time_delta;\n\n\treturn ts;\n}\n\n \n#define RB_MISSED_EVENTS\t(1 << 31)\n \n#define RB_MISSED_STORED\t(1 << 30)\n\nstruct buffer_data_page {\n\tu64\t\t time_stamp;\t \n\tlocal_t\t\t commit;\t \n\tunsigned char\t data[] RB_ALIGN_DATA;\t \n};\n\n \nstruct buffer_page {\n\tstruct list_head list;\t\t \n\tlocal_t\t\t write;\t\t \n\tunsigned\t read;\t\t \n\tlocal_t\t\t entries;\t \n\tunsigned long\t real_end;\t \n\tstruct buffer_data_page *page;\t \n};\n\n \n#define RB_WRITE_MASK\t\t0xfffff\n#define RB_WRITE_INTCNT\t\t(1 << 20)\n\nstatic void rb_init_page(struct buffer_data_page *bpage)\n{\n\tlocal_set(&bpage->commit, 0);\n}\n\nstatic __always_inline unsigned int rb_page_commit(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->page->commit);\n}\n\nstatic void free_buffer_page(struct buffer_page *bpage)\n{\n\tfree_page((unsigned long)bpage->page);\n\tkfree(bpage);\n}\n\n \nstatic inline bool test_time_stamp(u64 delta)\n{\n\treturn !!(delta & TS_DELTA_TEST);\n}\n\n#define BUF_PAGE_SIZE (PAGE_SIZE - BUF_PAGE_HDR_SIZE)\n\n \n#define BUF_MAX_DATA_SIZE (BUF_PAGE_SIZE - (sizeof(u32) * 2))\n\nint ring_buffer_print_page_header(struct trace_seq *s)\n{\n\tstruct buffer_data_page field;\n\n\ttrace_seq_printf(s, \"\\tfield: u64 timestamp;\\t\"\n\t\t\t \"offset:0;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)sizeof(field.time_stamp),\n\t\t\t (unsigned int)is_signed_type(u64));\n\n\ttrace_seq_printf(s, \"\\tfield: local_t commit;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), commit),\n\t\t\t (unsigned int)sizeof(field.commit),\n\t\t\t (unsigned int)is_signed_type(long));\n\n\ttrace_seq_printf(s, \"\\tfield: int overwrite;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), commit),\n\t\t\t 1,\n\t\t\t (unsigned int)is_signed_type(long));\n\n\ttrace_seq_printf(s, \"\\tfield: char data;\\t\"\n\t\t\t \"offset:%u;\\tsize:%u;\\tsigned:%u;\\n\",\n\t\t\t (unsigned int)offsetof(typeof(field), data),\n\t\t\t (unsigned int)BUF_PAGE_SIZE,\n\t\t\t (unsigned int)is_signed_type(char));\n\n\treturn !trace_seq_has_overflowed(s);\n}\n\nstruct rb_irq_work {\n\tstruct irq_work\t\t\twork;\n\twait_queue_head_t\t\twaiters;\n\twait_queue_head_t\t\tfull_waiters;\n\tlong\t\t\t\twait_index;\n\tbool\t\t\t\twaiters_pending;\n\tbool\t\t\t\tfull_waiters_pending;\n\tbool\t\t\t\twakeup_full;\n};\n\n \nstruct rb_event_info {\n\tu64\t\t\tts;\n\tu64\t\t\tdelta;\n\tu64\t\t\tbefore;\n\tu64\t\t\tafter;\n\tunsigned long\t\tlength;\n\tstruct buffer_page\t*tail_page;\n\tint\t\t\tadd_timestamp;\n};\n\n \nenum {\n\tRB_ADD_STAMP_NONE\t\t= 0,\n\tRB_ADD_STAMP_EXTEND\t\t= BIT(1),\n\tRB_ADD_STAMP_ABSOLUTE\t\t= BIT(2),\n\tRB_ADD_STAMP_FORCE\t\t= BIT(3)\n};\n \nenum {\n\tRB_CTX_TRANSITION,\n\tRB_CTX_NMI,\n\tRB_CTX_IRQ,\n\tRB_CTX_SOFTIRQ,\n\tRB_CTX_NORMAL,\n\tRB_CTX_MAX\n};\n\n#if BITS_PER_LONG == 32\n#define RB_TIME_32\n#endif\n\n \n\n\n#ifdef RB_TIME_32\n\nstruct rb_time_struct {\n\tlocal_t\t\tcnt;\n\tlocal_t\t\ttop;\n\tlocal_t\t\tbottom;\n\tlocal_t\t\tmsb;\n};\n#else\n#include <asm/local64.h>\nstruct rb_time_struct {\n\tlocal64_t\ttime;\n};\n#endif\ntypedef struct rb_time_struct rb_time_t;\n\n#define MAX_NEST\t5\n\n \nstruct ring_buffer_per_cpu {\n\tint\t\t\t\tcpu;\n\tatomic_t\t\t\trecord_disabled;\n\tatomic_t\t\t\tresize_disabled;\n\tstruct trace_buffer\t*buffer;\n\traw_spinlock_t\t\t\treader_lock;\t \n\tarch_spinlock_t\t\t\tlock;\n\tstruct lock_class_key\t\tlock_key;\n\tstruct buffer_data_page\t\t*free_page;\n\tunsigned long\t\t\tnr_pages;\n\tunsigned int\t\t\tcurrent_context;\n\tstruct list_head\t\t*pages;\n\tstruct buffer_page\t\t*head_page;\t \n\tstruct buffer_page\t\t*tail_page;\t \n\tstruct buffer_page\t\t*commit_page;\t \n\tstruct buffer_page\t\t*reader_page;\n\tunsigned long\t\t\tlost_events;\n\tunsigned long\t\t\tlast_overrun;\n\tunsigned long\t\t\tnest;\n\tlocal_t\t\t\t\tentries_bytes;\n\tlocal_t\t\t\t\tentries;\n\tlocal_t\t\t\t\toverrun;\n\tlocal_t\t\t\t\tcommit_overrun;\n\tlocal_t\t\t\t\tdropped_events;\n\tlocal_t\t\t\t\tcommitting;\n\tlocal_t\t\t\t\tcommits;\n\tlocal_t\t\t\t\tpages_touched;\n\tlocal_t\t\t\t\tpages_lost;\n\tlocal_t\t\t\t\tpages_read;\n\tlong\t\t\t\tlast_pages_touch;\n\tsize_t\t\t\t\tshortest_full;\n\tunsigned long\t\t\tread;\n\tunsigned long\t\t\tread_bytes;\n\trb_time_t\t\t\twrite_stamp;\n\trb_time_t\t\t\tbefore_stamp;\n\tu64\t\t\t\tevent_stamp[MAX_NEST];\n\tu64\t\t\t\tread_stamp;\n\t \n\tunsigned long\t\t\tpages_removed;\n\t \n\tlong\t\t\t\tnr_pages_to_update;\n\tstruct list_head\t\tnew_pages;  \n\tstruct work_struct\t\tupdate_pages_work;\n\tstruct completion\t\tupdate_done;\n\n\tstruct rb_irq_work\t\tirq_work;\n};\n\nstruct trace_buffer {\n\tunsigned\t\t\tflags;\n\tint\t\t\t\tcpus;\n\tatomic_t\t\t\trecord_disabled;\n\tatomic_t\t\t\tresizing;\n\tcpumask_var_t\t\t\tcpumask;\n\n\tstruct lock_class_key\t\t*reader_lock_key;\n\n\tstruct mutex\t\t\tmutex;\n\n\tstruct ring_buffer_per_cpu\t**buffers;\n\n\tstruct hlist_node\t\tnode;\n\tu64\t\t\t\t(*clock)(void);\n\n\tstruct rb_irq_work\t\tirq_work;\n\tbool\t\t\t\ttime_stamp_abs;\n};\n\nstruct ring_buffer_iter {\n\tstruct ring_buffer_per_cpu\t*cpu_buffer;\n\tunsigned long\t\t\thead;\n\tunsigned long\t\t\tnext_event;\n\tstruct buffer_page\t\t*head_page;\n\tstruct buffer_page\t\t*cache_reader_page;\n\tunsigned long\t\t\tcache_read;\n\tunsigned long\t\t\tcache_pages_removed;\n\tu64\t\t\t\tread_stamp;\n\tu64\t\t\t\tpage_stamp;\n\tstruct ring_buffer_event\t*event;\n\tint\t\t\t\tmissed_events;\n};\n\n#ifdef RB_TIME_32\n\n \n#define RB_TIME_SHIFT\t30\n#define RB_TIME_VAL_MASK ((1 << RB_TIME_SHIFT) - 1)\n#define RB_TIME_MSB_SHIFT\t 60\n\nstatic inline int rb_time_cnt(unsigned long val)\n{\n\treturn (val >> RB_TIME_SHIFT) & 3;\n}\n\nstatic inline u64 rb_time_val(unsigned long top, unsigned long bottom)\n{\n\tu64 val;\n\n\tval = top & RB_TIME_VAL_MASK;\n\tval <<= RB_TIME_SHIFT;\n\tval |= bottom & RB_TIME_VAL_MASK;\n\n\treturn val;\n}\n\nstatic inline bool __rb_time_read(rb_time_t *t, u64 *ret, unsigned long *cnt)\n{\n\tunsigned long top, bottom, msb;\n\tunsigned long c;\n\n\t \n\tdo {\n\t\tc = local_read(&t->cnt);\n\t\ttop = local_read(&t->top);\n\t\tbottom = local_read(&t->bottom);\n\t\tmsb = local_read(&t->msb);\n\t} while (c != local_read(&t->cnt));\n\n\t*cnt = rb_time_cnt(top);\n\n\t \n\tif (*cnt != rb_time_cnt(msb) || *cnt != rb_time_cnt(bottom))\n\t\treturn false;\n\n\t \n\t*ret = rb_time_val(top, bottom) | ((u64)msb << RB_TIME_MSB_SHIFT);\n\treturn true;\n}\n\nstatic bool rb_time_read(rb_time_t *t, u64 *ret)\n{\n\tunsigned long cnt;\n\n\treturn __rb_time_read(t, ret, &cnt);\n}\n\nstatic inline unsigned long rb_time_val_cnt(unsigned long val, unsigned long cnt)\n{\n\treturn (val & RB_TIME_VAL_MASK) | ((cnt & 3) << RB_TIME_SHIFT);\n}\n\nstatic inline void rb_time_split(u64 val, unsigned long *top, unsigned long *bottom,\n\t\t\t\t unsigned long *msb)\n{\n\t*top = (unsigned long)((val >> RB_TIME_SHIFT) & RB_TIME_VAL_MASK);\n\t*bottom = (unsigned long)(val & RB_TIME_VAL_MASK);\n\t*msb = (unsigned long)(val >> RB_TIME_MSB_SHIFT);\n}\n\nstatic inline void rb_time_val_set(local_t *t, unsigned long val, unsigned long cnt)\n{\n\tval = rb_time_val_cnt(val, cnt);\n\tlocal_set(t, val);\n}\n\nstatic void rb_time_set(rb_time_t *t, u64 val)\n{\n\tunsigned long cnt, top, bottom, msb;\n\n\trb_time_split(val, &top, &bottom, &msb);\n\n\t \n\tdo {\n\t\tcnt = local_inc_return(&t->cnt);\n\t\trb_time_val_set(&t->top, top, cnt);\n\t\trb_time_val_set(&t->bottom, bottom, cnt);\n\t\trb_time_val_set(&t->msb, val >> RB_TIME_MSB_SHIFT, cnt);\n\t} while (cnt != local_read(&t->cnt));\n}\n\nstatic inline bool\nrb_time_read_cmpxchg(local_t *l, unsigned long expect, unsigned long set)\n{\n\treturn local_try_cmpxchg(l, &expect, set);\n}\n\n#else  \n\n \n\nstatic inline bool rb_time_read(rb_time_t *t, u64 *ret)\n{\n\t*ret = local64_read(&t->time);\n\treturn true;\n}\nstatic void rb_time_set(rb_time_t *t, u64 val)\n{\n\tlocal64_set(&t->time, val);\n}\n#endif\n\n \n\n#ifdef RB_VERIFY_EVENT\nstatic struct list_head *rb_list_head(struct list_head *list);\nstatic void verify_event(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t void *event)\n{\n\tstruct buffer_page *page = cpu_buffer->commit_page;\n\tstruct buffer_page *tail_page = READ_ONCE(cpu_buffer->tail_page);\n\tstruct list_head *next;\n\tlong commit, write;\n\tunsigned long addr = (unsigned long)event;\n\tbool done = false;\n\tint stop = 0;\n\n\t \n\tdo {\n\t\tif (page == tail_page || WARN_ON_ONCE(stop++ > 100))\n\t\t\tdone = true;\n\t\tcommit = local_read(&page->page->commit);\n\t\twrite = local_read(&page->write);\n\t\tif (addr >= (unsigned long)&page->page->data[commit] &&\n\t\t    addr < (unsigned long)&page->page->data[write])\n\t\t\treturn;\n\n\t\tnext = rb_list_head(page->list.next);\n\t\tpage = list_entry(next, struct buffer_page, list);\n\t} while (!done);\n\tWARN_ON_ONCE(1);\n}\n#else\nstatic inline void verify_event(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t void *event)\n{\n}\n#endif\n\n \nstatic inline u64 rb_fix_abs_ts(u64 abs, u64 save_ts)\n{\n\tif (save_ts & TS_MSB) {\n\t\tabs |= save_ts & TS_MSB;\n\t\t \n\t\tif (unlikely(abs < save_ts))\n\t\t\tabs += 1ULL << 59;\n\t}\n\treturn abs;\n}\n\nstatic inline u64 rb_time_stamp(struct trace_buffer *buffer);\n\n \nu64 ring_buffer_event_time_stamp(struct trace_buffer *buffer,\n\t\t\t\t struct ring_buffer_event *event)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[smp_processor_id()];\n\tunsigned int nest;\n\tu64 ts;\n\n\t \n\tif (event->type_len == RINGBUF_TYPE_TIME_STAMP) {\n\t\tts = rb_event_time_stamp(event);\n\t\treturn rb_fix_abs_ts(ts, cpu_buffer->tail_page->page->time_stamp);\n\t}\n\n\tnest = local_read(&cpu_buffer->committing);\n\tverify_event(cpu_buffer, event);\n\tif (WARN_ON_ONCE(!nest))\n\t\tgoto fail;\n\n\t \n\tif (likely(--nest < MAX_NEST))\n\t\treturn cpu_buffer->event_stamp[nest];\n\n\t \n\tWARN_ONCE(1, \"nest (%d) greater than max\", nest);\n\n fail:\n\t \n\tif (!rb_time_read(&cpu_buffer->write_stamp, &ts))\n\t\t \n\t\tts = rb_time_stamp(cpu_buffer->buffer);\n\n\treturn ts;\n}\n\n \nsize_t ring_buffer_nr_pages(struct trace_buffer *buffer, int cpu)\n{\n\treturn buffer->buffers[cpu]->nr_pages;\n}\n\n \nsize_t ring_buffer_nr_dirty_pages(struct trace_buffer *buffer, int cpu)\n{\n\tsize_t read;\n\tsize_t lost;\n\tsize_t cnt;\n\n\tread = local_read(&buffer->buffers[cpu]->pages_read);\n\tlost = local_read(&buffer->buffers[cpu]->pages_lost);\n\tcnt = local_read(&buffer->buffers[cpu]->pages_touched);\n\n\tif (WARN_ON_ONCE(cnt < lost))\n\t\treturn 0;\n\n\tcnt -= lost;\n\n\t \n\tif (cnt < read) {\n\t\tWARN_ON_ONCE(read > cnt + 1);\n\t\treturn 0;\n\t}\n\n\treturn cnt - read;\n}\n\nstatic __always_inline bool full_hit(struct trace_buffer *buffer, int cpu, int full)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tsize_t nr_pages;\n\tsize_t dirty;\n\n\tnr_pages = cpu_buffer->nr_pages;\n\tif (!nr_pages || !full)\n\t\treturn true;\n\n\t \n\tdirty = ring_buffer_nr_dirty_pages(buffer, cpu) + 1;\n\n\treturn (dirty * 100) >= (full * nr_pages);\n}\n\n \nstatic void rb_wake_up_waiters(struct irq_work *work)\n{\n\tstruct rb_irq_work *rbwork = container_of(work, struct rb_irq_work, work);\n\n\twake_up_all(&rbwork->waiters);\n\tif (rbwork->full_waiters_pending || rbwork->wakeup_full) {\n\t\trbwork->wakeup_full = false;\n\t\trbwork->full_waiters_pending = false;\n\t\twake_up_all(&rbwork->full_waiters);\n\t}\n}\n\n \nvoid ring_buffer_wake_waiters(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct rb_irq_work *rbwork;\n\n\tif (!buffer)\n\t\treturn;\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\n\t\t \n\t\tfor_each_buffer_cpu(buffer, cpu)\n\t\t\tring_buffer_wake_waiters(buffer, cpu);\n\n\t\trbwork = &buffer->irq_work;\n\t} else {\n\t\tif (WARN_ON_ONCE(!buffer->buffers))\n\t\t\treturn;\n\t\tif (WARN_ON_ONCE(cpu >= nr_cpu_ids))\n\t\t\treturn;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t \n\t\tif (!cpu_buffer)\n\t\t\treturn;\n\t\trbwork = &cpu_buffer->irq_work;\n\t}\n\n\trbwork->wait_index++;\n\t \n\tsmp_wmb();\n\n\t \n\tirq_work_queue(&rbwork->work);\n}\n\n \nint ring_buffer_wait(struct trace_buffer *buffer, int cpu, int full)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tDEFINE_WAIT(wait);\n\tstruct rb_irq_work *work;\n\tlong wait_index;\n\tint ret = 0;\n\n\t \n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\twork = &buffer->irq_work;\n\t\t \n\t\tfull = 0;\n\t} else {\n\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn -ENODEV;\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\twork = &cpu_buffer->irq_work;\n\t}\n\n\twait_index = READ_ONCE(work->wait_index);\n\n\twhile (true) {\n\t\tif (full)\n\t\t\tprepare_to_wait(&work->full_waiters, &wait, TASK_INTERRUPTIBLE);\n\t\telse\n\t\t\tprepare_to_wait(&work->waiters, &wait, TASK_INTERRUPTIBLE);\n\n\t\t \n\t\tif (full)\n\t\t\twork->full_waiters_pending = true;\n\t\telse\n\t\t\twork->waiters_pending = true;\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer))\n\t\t\tbreak;\n\n\t\tif (cpu != RING_BUFFER_ALL_CPUS &&\n\t\t    !ring_buffer_empty_cpu(buffer, cpu)) {\n\t\t\tunsigned long flags;\n\t\t\tbool pagebusy;\n\t\t\tbool done;\n\n\t\t\tif (!full)\n\t\t\t\tbreak;\n\n\t\t\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t\t\tpagebusy = cpu_buffer->reader_page == cpu_buffer->commit_page;\n\t\t\tdone = !pagebusy && full_hit(buffer, cpu, full);\n\n\t\t\tif (!cpu_buffer->shortest_full ||\n\t\t\t    cpu_buffer->shortest_full > full)\n\t\t\t\tcpu_buffer->shortest_full = full;\n\t\t\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\t\t\tif (done)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\n\t\t \n\t\tsmp_rmb();\n\t\tif (wait_index != work->wait_index)\n\t\t\tbreak;\n\t}\n\n\tif (full)\n\t\tfinish_wait(&work->full_waiters, &wait);\n\telse\n\t\tfinish_wait(&work->waiters, &wait);\n\n\treturn ret;\n}\n\n \n__poll_t ring_buffer_poll_wait(struct trace_buffer *buffer, int cpu,\n\t\t\t  struct file *filp, poll_table *poll_table, int full)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct rb_irq_work *work;\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\twork = &buffer->irq_work;\n\t\tfull = 0;\n\t} else {\n\t\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\t\treturn -EINVAL;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\twork = &cpu_buffer->irq_work;\n\t}\n\n\tif (full) {\n\t\tpoll_wait(filp, &work->full_waiters, poll_table);\n\t\twork->full_waiters_pending = true;\n\t\tif (!cpu_buffer->shortest_full ||\n\t\t    cpu_buffer->shortest_full > full)\n\t\t\tcpu_buffer->shortest_full = full;\n\t} else {\n\t\tpoll_wait(filp, &work->waiters, poll_table);\n\t\twork->waiters_pending = true;\n\t}\n\n\t \n\tsmp_mb();\n\n\tif (full)\n\t\treturn full_hit(buffer, cpu, full) ? EPOLLIN | EPOLLRDNORM : 0;\n\n\tif ((cpu == RING_BUFFER_ALL_CPUS && !ring_buffer_empty(buffer)) ||\n\t    (cpu != RING_BUFFER_ALL_CPUS && !ring_buffer_empty_cpu(buffer, cpu)))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\treturn 0;\n}\n\n \n#define RB_WARN_ON(b, cond)\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tint _____ret = unlikely(cond);\t\t\t\t\\\n\t\tif (_____ret) {\t\t\t\t\t\t\\\n\t\t\tif (__same_type(*(b), struct ring_buffer_per_cpu)) { \\\n\t\t\t\tstruct ring_buffer_per_cpu *__b =\t\\\n\t\t\t\t\t(void *)b;\t\t\t\\\n\t\t\t\tatomic_inc(&__b->buffer->record_disabled); \\\n\t\t\t} else\t\t\t\t\t\t\\\n\t\t\t\tatomic_inc(&b->record_disabled);\t\\\n\t\t\tWARN_ON(1);\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t_____ret;\t\t\t\t\t\t\\\n\t})\n\n \n#define DEBUG_SHIFT 0\n\nstatic inline u64 rb_time_stamp(struct trace_buffer *buffer)\n{\n\tu64 ts;\n\n\t \n\tif (IS_ENABLED(CONFIG_RETPOLINE) && likely(buffer->clock == trace_clock_local))\n\t\tts = trace_clock_local();\n\telse\n\t\tts = buffer->clock();\n\n\t \n\treturn ts << DEBUG_SHIFT;\n}\n\nu64 ring_buffer_time_stamp(struct trace_buffer *buffer)\n{\n\tu64 time;\n\n\tpreempt_disable_notrace();\n\ttime = rb_time_stamp(buffer);\n\tpreempt_enable_notrace();\n\n\treturn time;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_time_stamp);\n\nvoid ring_buffer_normalize_time_stamp(struct trace_buffer *buffer,\n\t\t\t\t      int cpu, u64 *ts)\n{\n\t \n\t*ts >>= DEBUG_SHIFT;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_normalize_time_stamp);\n\n \n\n#define RB_PAGE_NORMAL\t\t0UL\n#define RB_PAGE_HEAD\t\t1UL\n#define RB_PAGE_UPDATE\t\t2UL\n\n\n#define RB_FLAG_MASK\t\t3UL\n\n \n#define RB_PAGE_MOVED\t\t4UL\n\n \nstatic struct list_head *rb_list_head(struct list_head *list)\n{\n\tunsigned long val = (unsigned long)list;\n\n\treturn (struct list_head *)(val & ~RB_FLAG_MASK);\n}\n\n \nstatic inline int\nrb_is_head_page(struct buffer_page *page, struct list_head *list)\n{\n\tunsigned long val;\n\n\tval = (unsigned long)list->next;\n\n\tif ((val & ~RB_FLAG_MASK) != (unsigned long)&page->list)\n\t\treturn RB_PAGE_MOVED;\n\n\treturn val & RB_FLAG_MASK;\n}\n\n \nstatic bool rb_is_reader_page(struct buffer_page *page)\n{\n\tstruct list_head *list = page->list.prev;\n\n\treturn rb_list_head(list->next) != &page->list;\n}\n\n \nstatic void rb_set_list_to_head(struct list_head *list)\n{\n\tunsigned long *ptr;\n\n\tptr = (unsigned long *)&list->next;\n\t*ptr |= RB_PAGE_HEAD;\n\t*ptr &= ~RB_PAGE_UPDATE;\n}\n\n \nstatic void rb_head_page_activate(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *head;\n\n\thead = cpu_buffer->head_page;\n\tif (!head)\n\t\treturn;\n\n\t \n\trb_set_list_to_head(head->list.prev);\n}\n\nstatic void rb_list_head_clear(struct list_head *list)\n{\n\tunsigned long *ptr = (unsigned long *)&list->next;\n\n\t*ptr &= ~RB_FLAG_MASK;\n}\n\n \nstatic void\nrb_head_page_deactivate(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *hd;\n\n\t \n\trb_list_head_clear(cpu_buffer->pages);\n\n\tlist_for_each(hd, cpu_buffer->pages)\n\t\trb_list_head_clear(hd);\n}\n\nstatic int rb_head_page_set(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t    struct buffer_page *head,\n\t\t\t    struct buffer_page *prev,\n\t\t\t    int old_flag, int new_flag)\n{\n\tstruct list_head *list;\n\tunsigned long val = (unsigned long)&head->list;\n\tunsigned long ret;\n\n\tlist = &prev->list;\n\n\tval &= ~RB_FLAG_MASK;\n\n\tret = cmpxchg((unsigned long *)&list->next,\n\t\t      val | old_flag, val | new_flag);\n\n\t \n\tif ((ret & ~RB_FLAG_MASK) != val)\n\t\treturn RB_PAGE_MOVED;\n\n\treturn ret & RB_FLAG_MASK;\n}\n\nstatic int rb_head_page_set_update(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t   struct buffer_page *head,\n\t\t\t\t   struct buffer_page *prev,\n\t\t\t\t   int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_UPDATE);\n}\n\nstatic int rb_head_page_set_head(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t struct buffer_page *head,\n\t\t\t\t struct buffer_page *prev,\n\t\t\t\t int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_HEAD);\n}\n\nstatic int rb_head_page_set_normal(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t   struct buffer_page *head,\n\t\t\t\t   struct buffer_page *prev,\n\t\t\t\t   int old_flag)\n{\n\treturn rb_head_page_set(cpu_buffer, head, prev,\n\t\t\t\told_flag, RB_PAGE_NORMAL);\n}\n\nstatic inline void rb_inc_page(struct buffer_page **bpage)\n{\n\tstruct list_head *p = rb_list_head((*bpage)->list.next);\n\n\t*bpage = list_entry(p, struct buffer_page, list);\n}\n\nstatic struct buffer_page *\nrb_set_head_page(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *head;\n\tstruct buffer_page *page;\n\tstruct list_head *list;\n\tint i;\n\n\tif (RB_WARN_ON(cpu_buffer, !cpu_buffer->head_page))\n\t\treturn NULL;\n\n\t \n\tlist = cpu_buffer->pages;\n\tif (RB_WARN_ON(cpu_buffer, rb_list_head(list->prev->next) != list))\n\t\treturn NULL;\n\n\tpage = head = cpu_buffer->head_page;\n\t \n\tfor (i = 0; i < 3; i++) {\n\t\tdo {\n\t\t\tif (rb_is_head_page(page, page->list.prev)) {\n\t\t\t\tcpu_buffer->head_page = page;\n\t\t\t\treturn page;\n\t\t\t}\n\t\t\trb_inc_page(&page);\n\t\t} while (page != head);\n\t}\n\n\tRB_WARN_ON(cpu_buffer, 1);\n\n\treturn NULL;\n}\n\nstatic bool rb_head_page_replace(struct buffer_page *old,\n\t\t\t\tstruct buffer_page *new)\n{\n\tunsigned long *ptr = (unsigned long *)&old->list.prev->next;\n\tunsigned long val;\n\n\tval = *ptr & ~RB_FLAG_MASK;\n\tval |= RB_PAGE_HEAD;\n\n\treturn try_cmpxchg(ptr, &val, (unsigned long)&new->list);\n}\n\n \nstatic void rb_tail_page_update(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t       struct buffer_page *tail_page,\n\t\t\t       struct buffer_page *next_page)\n{\n\tunsigned long old_entries;\n\tunsigned long old_write;\n\n\t \n\told_write = local_add_return(RB_WRITE_INTCNT, &next_page->write);\n\told_entries = local_add_return(RB_WRITE_INTCNT, &next_page->entries);\n\n\tlocal_inc(&cpu_buffer->pages_touched);\n\t \n\tbarrier();\n\n\t \n\tif (tail_page == READ_ONCE(cpu_buffer->tail_page)) {\n\t\t \n\t\tunsigned long val = old_write & ~RB_WRITE_MASK;\n\t\tunsigned long eval = old_entries & ~RB_WRITE_MASK;\n\n\t\t \n\t\t(void)local_cmpxchg(&next_page->write, old_write, val);\n\t\t(void)local_cmpxchg(&next_page->entries, old_entries, eval);\n\n\t\t \n\t\tlocal_set(&next_page->page->commit, 0);\n\n\t\t \n\t\t(void)cmpxchg(&cpu_buffer->tail_page, tail_page, next_page);\n\t}\n}\n\nstatic void rb_check_bpage(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t  struct buffer_page *bpage)\n{\n\tunsigned long val = (unsigned long)bpage;\n\n\tRB_WARN_ON(cpu_buffer, val & RB_FLAG_MASK);\n}\n\n \nstatic void rb_check_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *head = rb_list_head(cpu_buffer->pages);\n\tstruct list_head *tmp;\n\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t\trb_list_head(rb_list_head(head->next)->prev) != head))\n\t\treturn;\n\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t\trb_list_head(rb_list_head(head->prev)->next) != head))\n\t\treturn;\n\n\tfor (tmp = rb_list_head(head->next); tmp != head; tmp = rb_list_head(tmp->next)) {\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t\trb_list_head(rb_list_head(tmp->next)->prev) != tmp))\n\t\t\treturn;\n\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t\trb_list_head(rb_list_head(tmp->prev)->next) != tmp))\n\t\t\treturn;\n\t}\n}\n\nstatic int __rb_allocate_pages(struct ring_buffer_per_cpu *cpu_buffer,\n\t\tlong nr_pages, struct list_head *pages)\n{\n\tstruct buffer_page *bpage, *tmp;\n\tbool user_thread = current->mm != NULL;\n\tgfp_t mflags;\n\tlong i;\n\n\t \n\ti = si_mem_available();\n\tif (i < nr_pages)\n\t\treturn -ENOMEM;\n\n\t \n\tmflags = GFP_KERNEL | __GFP_RETRY_MAYFAIL;\n\n\t \n\tif (user_thread)\n\t\tset_current_oom_origin();\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct page *page;\n\n\t\tbpage = kzalloc_node(ALIGN(sizeof(*bpage), cache_line_size()),\n\t\t\t\t    mflags, cpu_to_node(cpu_buffer->cpu));\n\t\tif (!bpage)\n\t\t\tgoto free_pages;\n\n\t\trb_check_bpage(cpu_buffer, bpage);\n\n\t\tlist_add(&bpage->list, pages);\n\n\t\tpage = alloc_pages_node(cpu_to_node(cpu_buffer->cpu), mflags, 0);\n\t\tif (!page)\n\t\t\tgoto free_pages;\n\t\tbpage->page = page_address(page);\n\t\trb_init_page(bpage->page);\n\n\t\tif (user_thread && fatal_signal_pending(current))\n\t\t\tgoto free_pages;\n\t}\n\tif (user_thread)\n\t\tclear_current_oom_origin();\n\n\treturn 0;\n\nfree_pages:\n\tlist_for_each_entry_safe(bpage, tmp, pages, list) {\n\t\tlist_del_init(&bpage->list);\n\t\tfree_buffer_page(bpage);\n\t}\n\tif (user_thread)\n\t\tclear_current_oom_origin();\n\n\treturn -ENOMEM;\n}\n\nstatic int rb_allocate_pages(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t     unsigned long nr_pages)\n{\n\tLIST_HEAD(pages);\n\n\tWARN_ON(!nr_pages);\n\n\tif (__rb_allocate_pages(cpu_buffer, nr_pages, &pages))\n\t\treturn -ENOMEM;\n\n\t \n\tcpu_buffer->pages = pages.next;\n\tlist_del(&pages);\n\n\tcpu_buffer->nr_pages = nr_pages;\n\n\trb_check_pages(cpu_buffer);\n\n\treturn 0;\n}\n\nstatic struct ring_buffer_per_cpu *\nrb_allocate_cpu_buffer(struct trace_buffer *buffer, long nr_pages, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *bpage;\n\tstruct page *page;\n\tint ret;\n\n\tcpu_buffer = kzalloc_node(ALIGN(sizeof(*cpu_buffer), cache_line_size()),\n\t\t\t\t  GFP_KERNEL, cpu_to_node(cpu));\n\tif (!cpu_buffer)\n\t\treturn NULL;\n\n\tcpu_buffer->cpu = cpu;\n\tcpu_buffer->buffer = buffer;\n\traw_spin_lock_init(&cpu_buffer->reader_lock);\n\tlockdep_set_class(&cpu_buffer->reader_lock, buffer->reader_lock_key);\n\tcpu_buffer->lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\tINIT_WORK(&cpu_buffer->update_pages_work, update_pages_handler);\n\tinit_completion(&cpu_buffer->update_done);\n\tinit_irq_work(&cpu_buffer->irq_work.work, rb_wake_up_waiters);\n\tinit_waitqueue_head(&cpu_buffer->irq_work.waiters);\n\tinit_waitqueue_head(&cpu_buffer->irq_work.full_waiters);\n\n\tbpage = kzalloc_node(ALIGN(sizeof(*bpage), cache_line_size()),\n\t\t\t    GFP_KERNEL, cpu_to_node(cpu));\n\tif (!bpage)\n\t\tgoto fail_free_buffer;\n\n\trb_check_bpage(cpu_buffer, bpage);\n\n\tcpu_buffer->reader_page = bpage;\n\tpage = alloc_pages_node(cpu_to_node(cpu), GFP_KERNEL, 0);\n\tif (!page)\n\t\tgoto fail_free_reader;\n\tbpage->page = page_address(page);\n\trb_init_page(bpage->page);\n\n\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);\n\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\n\tret = rb_allocate_pages(cpu_buffer, nr_pages);\n\tif (ret < 0)\n\t\tgoto fail_free_reader;\n\n\tcpu_buffer->head_page\n\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);\n\tcpu_buffer->tail_page = cpu_buffer->commit_page = cpu_buffer->head_page;\n\n\trb_head_page_activate(cpu_buffer);\n\n\treturn cpu_buffer;\n\n fail_free_reader:\n\tfree_buffer_page(cpu_buffer->reader_page);\n\n fail_free_buffer:\n\tkfree(cpu_buffer);\n\treturn NULL;\n}\n\nstatic void rb_free_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *head = cpu_buffer->pages;\n\tstruct buffer_page *bpage, *tmp;\n\n\tirq_work_sync(&cpu_buffer->irq_work.work);\n\n\tfree_buffer_page(cpu_buffer->reader_page);\n\n\tif (head) {\n\t\trb_head_page_deactivate(cpu_buffer);\n\n\t\tlist_for_each_entry_safe(bpage, tmp, head, list) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t\tbpage = list_entry(head, struct buffer_page, list);\n\t\tfree_buffer_page(bpage);\n\t}\n\n\tfree_page((unsigned long)cpu_buffer->free_page);\n\n\tkfree(cpu_buffer);\n}\n\n \nstruct trace_buffer *__ring_buffer_alloc(unsigned long size, unsigned flags,\n\t\t\t\t\tstruct lock_class_key *key)\n{\n\tstruct trace_buffer *buffer;\n\tlong nr_pages;\n\tint bsize;\n\tint cpu;\n\tint ret;\n\n\t \n\tbuffer = kzalloc(ALIGN(sizeof(*buffer), cache_line_size()),\n\t\t\t GFP_KERNEL);\n\tif (!buffer)\n\t\treturn NULL;\n\n\tif (!zalloc_cpumask_var(&buffer->cpumask, GFP_KERNEL))\n\t\tgoto fail_free_buffer;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\tbuffer->flags = flags;\n\tbuffer->clock = trace_clock_local;\n\tbuffer->reader_lock_key = key;\n\n\tinit_irq_work(&buffer->irq_work.work, rb_wake_up_waiters);\n\tinit_waitqueue_head(&buffer->irq_work.waiters);\n\n\t \n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\tbuffer->cpus = nr_cpu_ids;\n\n\tbsize = sizeof(void *) * nr_cpu_ids;\n\tbuffer->buffers = kzalloc(ALIGN(bsize, cache_line_size()),\n\t\t\t\t  GFP_KERNEL);\n\tif (!buffer->buffers)\n\t\tgoto fail_free_cpumask;\n\n\tcpu = raw_smp_processor_id();\n\tcpumask_set_cpu(cpu, buffer->cpumask);\n\tbuffer->buffers[cpu] = rb_allocate_cpu_buffer(buffer, nr_pages, cpu);\n\tif (!buffer->buffers[cpu])\n\t\tgoto fail_free_buffers;\n\n\tret = cpuhp_state_add_instance(CPUHP_TRACE_RB_PREPARE, &buffer->node);\n\tif (ret < 0)\n\t\tgoto fail_free_buffers;\n\n\tmutex_init(&buffer->mutex);\n\n\treturn buffer;\n\n fail_free_buffers:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tif (buffer->buffers[cpu])\n\t\t\trb_free_cpu_buffer(buffer->buffers[cpu]);\n\t}\n\tkfree(buffer->buffers);\n\n fail_free_cpumask:\n\tfree_cpumask_var(buffer->cpumask);\n\n fail_free_buffer:\n\tkfree(buffer);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(__ring_buffer_alloc);\n\n \nvoid\nring_buffer_free(struct trace_buffer *buffer)\n{\n\tint cpu;\n\n\tcpuhp_state_remove_instance(CPUHP_TRACE_RB_PREPARE, &buffer->node);\n\n\tirq_work_sync(&buffer->irq_work.work);\n\n\tfor_each_buffer_cpu(buffer, cpu)\n\t\trb_free_cpu_buffer(buffer->buffers[cpu]);\n\n\tkfree(buffer->buffers);\n\tfree_cpumask_var(buffer->cpumask);\n\n\tkfree(buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_free);\n\nvoid ring_buffer_set_clock(struct trace_buffer *buffer,\n\t\t\t   u64 (*clock)(void))\n{\n\tbuffer->clock = clock;\n}\n\nvoid ring_buffer_set_time_stamp_abs(struct trace_buffer *buffer, bool abs)\n{\n\tbuffer->time_stamp_abs = abs;\n}\n\nbool ring_buffer_time_stamp_abs(struct trace_buffer *buffer)\n{\n\treturn buffer->time_stamp_abs;\n}\n\nstatic void rb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer);\n\nstatic inline unsigned long rb_page_entries(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->entries) & RB_WRITE_MASK;\n}\n\nstatic inline unsigned long rb_page_write(struct buffer_page *bpage)\n{\n\treturn local_read(&bpage->write) & RB_WRITE_MASK;\n}\n\nstatic bool\nrb_remove_pages(struct ring_buffer_per_cpu *cpu_buffer, unsigned long nr_pages)\n{\n\tstruct list_head *tail_page, *to_remove, *next_page;\n\tstruct buffer_page *to_remove_page, *tmp_iter_page;\n\tstruct buffer_page *last_page, *first_page;\n\tunsigned long nr_removed;\n\tunsigned long head_bit;\n\tint page_entries;\n\n\thead_bit = 0;\n\n\traw_spin_lock_irq(&cpu_buffer->reader_lock);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\t \n\ttail_page = &cpu_buffer->tail_page->list;\n\n\t \n\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)\n\t\ttail_page = rb_list_head(tail_page->next);\n\tto_remove = tail_page;\n\n\t \n\tfirst_page = list_entry(rb_list_head(to_remove->next),\n\t\t\t\tstruct buffer_page, list);\n\n\tfor (nr_removed = 0; nr_removed < nr_pages; nr_removed++) {\n\t\tto_remove = rb_list_head(to_remove)->next;\n\t\thead_bit |= (unsigned long)to_remove & RB_PAGE_HEAD;\n\t}\n\t \n\tcpu_buffer->pages_removed += nr_removed;\n\n\tnext_page = rb_list_head(to_remove)->next;\n\n\t \n\ttail_page->next = (struct list_head *)((unsigned long)next_page |\n\t\t\t\t\t\thead_bit);\n\tnext_page = rb_list_head(next_page);\n\tnext_page->prev = tail_page;\n\n\t \n\tcpu_buffer->pages = next_page;\n\n\t \n\tif (head_bit)\n\t\tcpu_buffer->head_page = list_entry(next_page,\n\t\t\t\t\t\tstruct buffer_page, list);\n\n\t \n\tatomic_dec(&cpu_buffer->record_disabled);\n\traw_spin_unlock_irq(&cpu_buffer->reader_lock);\n\n\tRB_WARN_ON(cpu_buffer, list_empty(cpu_buffer->pages));\n\n\t \n\tlast_page = list_entry(rb_list_head(to_remove), struct buffer_page,\n\t\t\t\tlist);\n\ttmp_iter_page = first_page;\n\n\tdo {\n\t\tcond_resched();\n\n\t\tto_remove_page = tmp_iter_page;\n\t\trb_inc_page(&tmp_iter_page);\n\n\t\t \n\t\tpage_entries = rb_page_entries(to_remove_page);\n\t\tif (page_entries) {\n\t\t\t \n\t\t\tlocal_add(page_entries, &cpu_buffer->overrun);\n\t\t\tlocal_sub(rb_page_commit(to_remove_page), &cpu_buffer->entries_bytes);\n\t\t\tlocal_inc(&cpu_buffer->pages_lost);\n\t\t}\n\n\t\t \n\t\tfree_buffer_page(to_remove_page);\n\t\tnr_removed--;\n\n\t} while (to_remove_page != last_page);\n\n\tRB_WARN_ON(cpu_buffer, nr_removed);\n\n\treturn nr_removed == 0;\n}\n\nstatic bool\nrb_insert_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct list_head *pages = &cpu_buffer->new_pages;\n\tunsigned long flags;\n\tbool success;\n\tint retries;\n\n\t \n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t \n\tretries = 10;\n\tsuccess = false;\n\twhile (retries--) {\n\t\tstruct list_head *head_page, *prev_page, *r;\n\t\tstruct list_head *last_page, *first_page;\n\t\tstruct list_head *head_page_with_bit;\n\t\tstruct buffer_page *hpage = rb_set_head_page(cpu_buffer);\n\n\t\tif (!hpage)\n\t\t\tbreak;\n\t\thead_page = &hpage->list;\n\t\tprev_page = head_page->prev;\n\n\t\tfirst_page = pages->next;\n\t\tlast_page  = pages->prev;\n\n\t\thead_page_with_bit = (struct list_head *)\n\t\t\t\t     ((unsigned long)head_page | RB_PAGE_HEAD);\n\n\t\tlast_page->next = head_page_with_bit;\n\t\tfirst_page->prev = prev_page;\n\n\t\tr = cmpxchg(&prev_page->next, head_page_with_bit, first_page);\n\n\t\tif (r == head_page_with_bit) {\n\t\t\t \n\t\t\thead_page->prev = last_page;\n\t\t\tsuccess = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (success)\n\t\tINIT_LIST_HEAD(pages);\n\t \n\tRB_WARN_ON(cpu_buffer, !success);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\t \n\tif (!success) {\n\t\tstruct buffer_page *bpage, *tmp;\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\t list) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n\treturn success;\n}\n\nstatic void rb_update_pages(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tbool success;\n\n\tif (cpu_buffer->nr_pages_to_update > 0)\n\t\tsuccess = rb_insert_pages(cpu_buffer);\n\telse\n\t\tsuccess = rb_remove_pages(cpu_buffer,\n\t\t\t\t\t-cpu_buffer->nr_pages_to_update);\n\n\tif (success)\n\t\tcpu_buffer->nr_pages += cpu_buffer->nr_pages_to_update;\n}\n\nstatic void update_pages_handler(struct work_struct *work)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = container_of(work,\n\t\t\tstruct ring_buffer_per_cpu, update_pages_work);\n\trb_update_pages(cpu_buffer);\n\tcomplete(&cpu_buffer->update_done);\n}\n\n \nint ring_buffer_resize(struct trace_buffer *buffer, unsigned long size,\n\t\t\tint cpu_id)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long nr_pages;\n\tint cpu, err;\n\n\t \n\tif (!buffer)\n\t\treturn 0;\n\n\t \n\tif (cpu_id != RING_BUFFER_ALL_CPUS &&\n\t    !cpumask_test_cpu(cpu_id, buffer->cpumask))\n\t\treturn 0;\n\n\tnr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);\n\n\t \n\tif (nr_pages < 2)\n\t\tnr_pages = 2;\n\n\t \n\tmutex_lock(&buffer->mutex);\n\tatomic_inc(&buffer->resizing);\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\t \n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {\n\t\t\t\terr = -EBUSY;\n\t\t\t\tgoto out_err_unlock;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\t\tcpu_buffer->nr_pages;\n\t\t\t \n\t\t\tif (cpu_buffer->nr_pages_to_update <= 0)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\t\tif (__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t\t&cpu_buffer->new_pages)) {\n\t\t\t\t \n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\n\t\t\tcond_resched();\n\t\t}\n\n\t\tcpus_read_lock();\n\t\t \n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (!cpu_online(cpu)) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tmigrate_disable();\n\t\t\t\tif (cpu != smp_processor_id()) {\n\t\t\t\t\tmigrate_enable();\n\t\t\t\t\tschedule_work_on(cpu,\n\t\t\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\t\t} else {\n\t\t\t\t\tupdate_pages_handler(&cpu_buffer->update_pages_work);\n\t\t\t\t\tmigrate_enable();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\tif (!cpu_buffer->nr_pages_to_update)\n\t\t\t\tcontinue;\n\n\t\t\tif (cpu_online(cpu))\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\t}\n\n\t\tcpus_read_unlock();\n\t} else {\n\t\tcpu_buffer = buffer->buffers[cpu_id];\n\n\t\tif (nr_pages == cpu_buffer->nr_pages)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (atomic_read(&cpu_buffer->resize_disabled)) {\n\t\t\terr = -EBUSY;\n\t\t\tgoto out_err_unlock;\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = nr_pages -\n\t\t\t\t\t\tcpu_buffer->nr_pages;\n\n\t\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\t\tif (cpu_buffer->nr_pages_to_update > 0 &&\n\t\t\t__rb_allocate_pages(cpu_buffer, cpu_buffer->nr_pages_to_update,\n\t\t\t\t\t    &cpu_buffer->new_pages)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\tcpus_read_lock();\n\n\t\t \n\t\tif (!cpu_online(cpu_id))\n\t\t\trb_update_pages(cpu_buffer);\n\t\telse {\n\t\t\t \n\t\t\tmigrate_disable();\n\t\t\tif (cpu_id == smp_processor_id()) {\n\t\t\t\trb_update_pages(cpu_buffer);\n\t\t\t\tmigrate_enable();\n\t\t\t} else {\n\t\t\t\tmigrate_enable();\n\t\t\t\tschedule_work_on(cpu_id,\n\t\t\t\t\t\t &cpu_buffer->update_pages_work);\n\t\t\t\twait_for_completion(&cpu_buffer->update_done);\n\t\t\t}\n\t\t}\n\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\t\tcpus_read_unlock();\n\t}\n\n out:\n\t \n\tif (atomic_read(&buffer->record_disabled)) {\n\t\tatomic_inc(&buffer->record_disabled);\n\t\t \n\t\tsynchronize_rcu();\n\t\tfor_each_buffer_cpu(buffer, cpu) {\n\t\t\tcpu_buffer = buffer->buffers[cpu];\n\t\t\trb_check_pages(cpu_buffer);\n\t\t}\n\t\tatomic_dec(&buffer->record_disabled);\n\t}\n\n\tatomic_dec(&buffer->resizing);\n\tmutex_unlock(&buffer->mutex);\n\treturn 0;\n\n out_err:\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tstruct buffer_page *bpage, *tmp;\n\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tcpu_buffer->nr_pages_to_update = 0;\n\n\t\tif (list_empty(&cpu_buffer->new_pages))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(bpage, tmp, &cpu_buffer->new_pages,\n\t\t\t\t\tlist) {\n\t\t\tlist_del_init(&bpage->list);\n\t\t\tfree_buffer_page(bpage);\n\t\t}\n\t}\n out_err_unlock:\n\tatomic_dec(&buffer->resizing);\n\tmutex_unlock(&buffer->mutex);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_resize);\n\nvoid ring_buffer_change_overwrite(struct trace_buffer *buffer, int val)\n{\n\tmutex_lock(&buffer->mutex);\n\tif (val)\n\t\tbuffer->flags |= RB_FL_OVERWRITE;\n\telse\n\t\tbuffer->flags &= ~RB_FL_OVERWRITE;\n\tmutex_unlock(&buffer->mutex);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_change_overwrite);\n\nstatic __always_inline void *__rb_page_index(struct buffer_page *bpage, unsigned index)\n{\n\treturn bpage->page->data + index;\n}\n\nstatic __always_inline struct ring_buffer_event *\nrb_reader_event(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn __rb_page_index(cpu_buffer->reader_page,\n\t\t\t       cpu_buffer->reader_page->read);\n}\n\nstatic struct ring_buffer_event *\nrb_iter_head_event(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *iter_head_page = iter->head_page;\n\tunsigned long commit;\n\tunsigned length;\n\n\tif (iter->head != iter->next_event)\n\t\treturn iter->event;\n\n\t \n\tcommit = rb_page_commit(iter_head_page);\n\tsmp_rmb();\n\n\t \n\tif (iter->head > commit - 8)\n\t\tgoto reset;\n\n\tevent = __rb_page_index(iter_head_page, iter->head);\n\tlength = rb_event_length(event);\n\n\t \n\tbarrier();\n\n\tif ((iter->head + length) > commit || length > BUF_PAGE_SIZE)\n\t\t \n\t\tgoto reset;\n\n\tmemcpy(iter->event, event, length);\n\t \n\tsmp_rmb();\n\n\t \n\tif (iter->page_stamp != iter_head_page->page->time_stamp ||\n\t    commit > rb_page_commit(iter_head_page))\n\t\tgoto reset;\n\n\titer->next_event = iter->head + length;\n\treturn iter->event;\n reset:\n\t \n\titer->page_stamp = iter->read_stamp = iter->head_page->page->time_stamp;\n\titer->head = 0;\n\titer->next_event = 0;\n\titer->missed_events = 1;\n\treturn NULL;\n}\n\n \nstatic __always_inline unsigned rb_page_size(struct buffer_page *bpage)\n{\n\treturn rb_page_commit(bpage);\n}\n\nstatic __always_inline unsigned\nrb_commit_index(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn rb_page_commit(cpu_buffer->commit_page);\n}\n\nstatic __always_inline unsigned\nrb_event_index(struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\n\treturn (addr & ~PAGE_MASK) - BUF_PAGE_HDR_SIZE;\n}\n\nstatic void rb_inc_iter(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\n\t \n\tif (iter->head_page == cpu_buffer->reader_page)\n\t\titer->head_page = rb_set_head_page(cpu_buffer);\n\telse\n\t\trb_inc_page(&iter->head_page);\n\n\titer->page_stamp = iter->read_stamp = iter->head_page->page->time_stamp;\n\titer->head = 0;\n\titer->next_event = 0;\n}\n\n \nstatic int\nrb_handle_head_page(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t    struct buffer_page *tail_page,\n\t\t    struct buffer_page *next_page)\n{\n\tstruct buffer_page *new_head;\n\tint entries;\n\tint type;\n\tint ret;\n\n\tentries = rb_page_entries(next_page);\n\n\t \n\ttype = rb_head_page_set_update(cpu_buffer, next_page, tail_page,\n\t\t\t\t       RB_PAGE_HEAD);\n\n\t \n\n\tswitch (type) {\n\tcase RB_PAGE_HEAD:\n\t\t \n\t\tlocal_add(entries, &cpu_buffer->overrun);\n\t\tlocal_sub(rb_page_commit(next_page), &cpu_buffer->entries_bytes);\n\t\tlocal_inc(&cpu_buffer->pages_lost);\n\n\t\t \n\n\t\t \n\t\tbreak;\n\n\tcase RB_PAGE_UPDATE:\n\t\t \n\t\tbreak;\n\tcase RB_PAGE_NORMAL:\n\t\t \n\t\treturn 1;\n\tcase RB_PAGE_MOVED:\n\t\t \n\t\treturn 1;\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);  \n\t\treturn -1;\n\t}\n\n\t \n\tnew_head = next_page;\n\trb_inc_page(&new_head);\n\n\tret = rb_head_page_set_head(cpu_buffer, new_head, next_page,\n\t\t\t\t    RB_PAGE_NORMAL);\n\n\t \n\tswitch (ret) {\n\tcase RB_PAGE_HEAD:\n\tcase RB_PAGE_NORMAL:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\treturn -1;\n\t}\n\n\t \n\tif (ret == RB_PAGE_NORMAL) {\n\t\tstruct buffer_page *buffer_tail_page;\n\n\t\tbuffer_tail_page = READ_ONCE(cpu_buffer->tail_page);\n\t\t \n\t\tif (buffer_tail_page != tail_page &&\n\t\t    buffer_tail_page != next_page)\n\t\t\trb_head_page_set_normal(cpu_buffer, new_head,\n\t\t\t\t\t\tnext_page,\n\t\t\t\t\t\tRB_PAGE_HEAD);\n\t}\n\n\t \n\tif (type == RB_PAGE_HEAD) {\n\t\tret = rb_head_page_set_normal(cpu_buffer, next_page,\n\t\t\t\t\t      tail_page,\n\t\t\t\t\t      RB_PAGE_UPDATE);\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       ret != RB_PAGE_UPDATE))\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline void\nrb_reset_tail(struct ring_buffer_per_cpu *cpu_buffer,\n\t      unsigned long tail, struct rb_event_info *info)\n{\n\tstruct buffer_page *tail_page = info->tail_page;\n\tstruct ring_buffer_event *event;\n\tunsigned long length = info->length;\n\n\t \n\tif (tail >= BUF_PAGE_SIZE) {\n\t\t \n\t\tif (tail == BUF_PAGE_SIZE)\n\t\t\ttail_page->real_end = 0;\n\n\t\tlocal_sub(length, &tail_page->write);\n\t\treturn;\n\t}\n\n\tevent = __rb_page_index(tail_page, tail);\n\n\t \n\ttail_page->real_end = tail;\n\n\t \n\tif (tail > (BUF_PAGE_SIZE - RB_EVNT_MIN_SIZE)) {\n\t\t \n\n\t\t \n\t\trb_event_set_padding(event);\n\n\t\t \n\t\tsmp_wmb();\n\n\t\t \n\t\tlocal_sub(length, &tail_page->write);\n\t\treturn;\n\t}\n\n\t \n\tevent->array[0] = (BUF_PAGE_SIZE - tail) - RB_EVNT_HDR_SIZE;\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\t \n\tevent->time_delta = 1;\n\n\t \n\tlocal_add(BUF_PAGE_SIZE - tail, &cpu_buffer->entries_bytes);\n\n\t \n\tsmp_wmb();\n\n\t \n\tlength = (tail + length) - BUF_PAGE_SIZE;\n\tlocal_sub(length, &tail_page->write);\n}\n\nstatic inline void rb_end_commit(struct ring_buffer_per_cpu *cpu_buffer);\n\n \nstatic noinline struct ring_buffer_event *\nrb_move_tail(struct ring_buffer_per_cpu *cpu_buffer,\n\t     unsigned long tail, struct rb_event_info *info)\n{\n\tstruct buffer_page *tail_page = info->tail_page;\n\tstruct buffer_page *commit_page = cpu_buffer->commit_page;\n\tstruct trace_buffer *buffer = cpu_buffer->buffer;\n\tstruct buffer_page *next_page;\n\tint ret;\n\n\tnext_page = tail_page;\n\n\trb_inc_page(&next_page);\n\n\t \n\tif (unlikely(next_page == commit_page)) {\n\t\tlocal_inc(&cpu_buffer->commit_overrun);\n\t\tgoto out_reset;\n\t}\n\n\t \n\tif (rb_is_head_page(next_page, &tail_page->list)) {\n\n\t\t \n\t\tif (!rb_is_reader_page(cpu_buffer->commit_page)) {\n\t\t\t \n\t\t\tif (!(buffer->flags & RB_FL_OVERWRITE)) {\n\t\t\t\tlocal_inc(&cpu_buffer->dropped_events);\n\t\t\t\tgoto out_reset;\n\t\t\t}\n\n\t\t\tret = rb_handle_head_page(cpu_buffer,\n\t\t\t\t\t\t  tail_page,\n\t\t\t\t\t\t  next_page);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_reset;\n\t\t\tif (ret)\n\t\t\t\tgoto out_again;\n\t\t} else {\n\t\t\t \n\t\t\tif (unlikely((cpu_buffer->commit_page !=\n\t\t\t\t      cpu_buffer->tail_page) &&\n\t\t\t\t     (cpu_buffer->commit_page ==\n\t\t\t\t      cpu_buffer->reader_page))) {\n\t\t\t\tlocal_inc(&cpu_buffer->commit_overrun);\n\t\t\t\tgoto out_reset;\n\t\t\t}\n\t\t}\n\t}\n\n\trb_tail_page_update(cpu_buffer, tail_page, next_page);\n\n out_again:\n\n\trb_reset_tail(cpu_buffer, tail, info);\n\n\t \n\trb_end_commit(cpu_buffer);\n\t \n\tlocal_inc(&cpu_buffer->committing);\n\n\t \n\treturn ERR_PTR(-EAGAIN);\n\n out_reset:\n\t \n\trb_reset_tail(cpu_buffer, tail, info);\n\n\treturn NULL;\n}\n\n \nstatic struct ring_buffer_event *\nrb_add_time_stamp(struct ring_buffer_event *event, u64 delta, bool abs)\n{\n\tif (abs)\n\t\tevent->type_len = RINGBUF_TYPE_TIME_STAMP;\n\telse\n\t\tevent->type_len = RINGBUF_TYPE_TIME_EXTEND;\n\n\t \n\tif (abs || rb_event_index(event)) {\n\t\tevent->time_delta = delta & TS_MASK;\n\t\tevent->array[0] = delta >> TS_SHIFT;\n\t} else {\n\t\t \n\t\tevent->time_delta = 0;\n\t\tevent->array[0] = 0;\n\t}\n\n\treturn skip_time_extend(event);\n}\n\n#ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\nstatic inline bool sched_clock_stable(void)\n{\n\treturn true;\n}\n#endif\n\nstatic void\nrb_check_timestamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t   struct rb_event_info *info)\n{\n\tu64 write_stamp;\n\n\tWARN_ONCE(1, \"Delta way too big! %llu ts=%llu before=%llu after=%llu write stamp=%llu\\n%s\",\n\t\t  (unsigned long long)info->delta,\n\t\t  (unsigned long long)info->ts,\n\t\t  (unsigned long long)info->before,\n\t\t  (unsigned long long)info->after,\n\t\t  (unsigned long long)(rb_time_read(&cpu_buffer->write_stamp, &write_stamp) ? write_stamp : 0),\n\t\t  sched_clock_stable() ? \"\" :\n\t\t  \"If you just came from a suspend/resume,\\n\"\n\t\t  \"please switch to the trace global clock:\\n\"\n\t\t  \"  echo global > /sys/kernel/tracing/trace_clock\\n\"\n\t\t  \"or add trace_clock=global to the kernel command line\\n\");\n}\n\nstatic void rb_add_timestamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t\t      struct ring_buffer_event **event,\n\t\t\t\t      struct rb_event_info *info,\n\t\t\t\t      u64 *delta,\n\t\t\t\t      unsigned int *length)\n{\n\tbool abs = info->add_timestamp &\n\t\t(RB_ADD_STAMP_FORCE | RB_ADD_STAMP_ABSOLUTE);\n\n\tif (unlikely(info->delta > (1ULL << 59))) {\n\t\t \n\t\tif (abs && (info->ts & TS_MSB)) {\n\t\t\tinfo->delta &= ABS_TS_MASK;\n\n\t\t \n\t\t} else if (info->before == info->after && info->before > info->ts) {\n\t\t\t \n\t\t\tstatic int once;\n\n\t\t\t \n\t\t\tif (!once) {\n\t\t\t\tonce++;\n\t\t\t\tpr_warn(\"Ring buffer clock went backwards: %llu -> %llu\\n\",\n\t\t\t\t\tinfo->before, info->ts);\n\t\t\t}\n\t\t} else\n\t\t\trb_check_timestamp(cpu_buffer, info);\n\t\tif (!abs)\n\t\t\tinfo->delta = 0;\n\t}\n\t*event = rb_add_time_stamp(*event, info->delta, abs);\n\t*length -= RB_LEN_TIME_EXTEND;\n\t*delta = 0;\n}\n\n \nstatic void\nrb_update_event(struct ring_buffer_per_cpu *cpu_buffer,\n\t\tstruct ring_buffer_event *event,\n\t\tstruct rb_event_info *info)\n{\n\tunsigned length = info->length;\n\tu64 delta = info->delta;\n\tunsigned int nest = local_read(&cpu_buffer->committing) - 1;\n\n\tif (!WARN_ON_ONCE(nest >= MAX_NEST))\n\t\tcpu_buffer->event_stamp[nest] = info->ts;\n\n\t \n\tif (unlikely(info->add_timestamp))\n\t\trb_add_timestamp(cpu_buffer, &event, info, &delta, &length);\n\n\tevent->time_delta = delta;\n\tlength -= RB_EVNT_HDR_SIZE;\n\tif (length > RB_MAX_SMALL_DATA || RB_FORCE_8BYTE_ALIGNMENT) {\n\t\tevent->type_len = 0;\n\t\tevent->array[0] = length;\n\t} else\n\t\tevent->type_len = DIV_ROUND_UP(length, RB_ALIGNMENT);\n}\n\nstatic unsigned rb_calculate_event_length(unsigned length)\n{\n\tstruct ring_buffer_event event;  \n\n\t \n\tif (!length)\n\t\tlength++;\n\n\tif (length > RB_MAX_SMALL_DATA || RB_FORCE_8BYTE_ALIGNMENT)\n\t\tlength += sizeof(event.array[0]);\n\n\tlength += RB_EVNT_HDR_SIZE;\n\tlength = ALIGN(length, RB_ARCH_ALIGNMENT);\n\n\t \n\tif (length == RB_LEN_TIME_EXTEND + RB_ALIGNMENT)\n\t\tlength += RB_ALIGNMENT;\n\n\treturn length;\n}\n\nstatic inline bool\nrb_try_to_discard(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t  struct ring_buffer_event *event)\n{\n\tunsigned long new_index, old_index;\n\tstruct buffer_page *bpage;\n\tunsigned long addr;\n\n\tnew_index = rb_event_index(event);\n\told_index = new_index + rb_event_ts_length(event);\n\taddr = (unsigned long)event;\n\taddr &= PAGE_MASK;\n\n\tbpage = READ_ONCE(cpu_buffer->tail_page);\n\n\t \n\tif (bpage->page == (void *)addr && rb_page_write(bpage) == old_index) {\n\t\tunsigned long write_mask =\n\t\t\tlocal_read(&bpage->write) & ~RB_WRITE_MASK;\n\t\tunsigned long event_length = rb_event_length(event);\n\n\t\t \n\t\trb_time_set(&cpu_buffer->before_stamp, 0);\n\n\t\t \n\n\t\t \n\t\told_index += write_mask;\n\t\tnew_index += write_mask;\n\n\t\t \n\t\tif (local_try_cmpxchg(&bpage->write, &old_index, new_index)) {\n\t\t\t \n\t\t\tlocal_sub(event_length, &cpu_buffer->entries_bytes);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\treturn false;\n}\n\nstatic void rb_start_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tlocal_inc(&cpu_buffer->committing);\n\tlocal_inc(&cpu_buffer->commits);\n}\n\nstatic __always_inline void\nrb_set_commit_to_write(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long max_count;\n\n\t \n again:\n\tmax_count = cpu_buffer->nr_pages * 100;\n\n\twhile (cpu_buffer->commit_page != READ_ONCE(cpu_buffer->tail_page)) {\n\t\tif (RB_WARN_ON(cpu_buffer, !(--max_count)))\n\t\t\treturn;\n\t\tif (RB_WARN_ON(cpu_buffer,\n\t\t\t       rb_is_reader_page(cpu_buffer->tail_page)))\n\t\t\treturn;\n\t\t \n\t\tlocal_set(&cpu_buffer->commit_page->page->commit,\n\t\t\t  rb_page_write(cpu_buffer->commit_page));\n\t\trb_inc_page(&cpu_buffer->commit_page);\n\t\t \n\t\tbarrier();\n\t}\n\twhile (rb_commit_index(cpu_buffer) !=\n\t       rb_page_write(cpu_buffer->commit_page)) {\n\n\t\t \n\t\tsmp_wmb();\n\t\tlocal_set(&cpu_buffer->commit_page->page->commit,\n\t\t\t  rb_page_write(cpu_buffer->commit_page));\n\t\tRB_WARN_ON(cpu_buffer,\n\t\t\t   local_read(&cpu_buffer->commit_page->page->commit) &\n\t\t\t   ~RB_WRITE_MASK);\n\t\tbarrier();\n\t}\n\n\t \n\tbarrier();\n\n\t \n\tif (unlikely(cpu_buffer->commit_page != READ_ONCE(cpu_buffer->tail_page)))\n\t\tgoto again;\n}\n\nstatic __always_inline void rb_end_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long commits;\n\n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       !local_read(&cpu_buffer->committing)))\n\t\treturn;\n\n again:\n\tcommits = local_read(&cpu_buffer->commits);\n\t \n\tbarrier();\n\tif (local_read(&cpu_buffer->committing) == 1)\n\t\trb_set_commit_to_write(cpu_buffer);\n\n\tlocal_dec(&cpu_buffer->committing);\n\n\t \n\tbarrier();\n\n\t \n\tif (unlikely(local_read(&cpu_buffer->commits) != commits) &&\n\t    !local_read(&cpu_buffer->committing)) {\n\t\tlocal_inc(&cpu_buffer->committing);\n\t\tgoto again;\n\t}\n}\n\nstatic inline void rb_event_discard(struct ring_buffer_event *event)\n{\n\tif (extended_time(event))\n\t\tevent = skip_time_extend(event);\n\n\t \n\tevent->array[0] = rb_event_data_length(event) - RB_EVNT_HDR_SIZE;\n\tevent->type_len = RINGBUF_TYPE_PADDING;\n\t \n\tif (!event->time_delta)\n\t\tevent->time_delta = 1;\n}\n\nstatic void rb_commit(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tlocal_inc(&cpu_buffer->entries);\n\trb_end_commit(cpu_buffer);\n}\n\nstatic __always_inline void\nrb_wakeups(struct trace_buffer *buffer, struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tif (buffer->irq_work.waiters_pending) {\n\t\tbuffer->irq_work.waiters_pending = false;\n\t\t \n\t\tirq_work_queue(&buffer->irq_work.work);\n\t}\n\n\tif (cpu_buffer->irq_work.waiters_pending) {\n\t\tcpu_buffer->irq_work.waiters_pending = false;\n\t\t \n\t\tirq_work_queue(&cpu_buffer->irq_work.work);\n\t}\n\n\tif (cpu_buffer->last_pages_touch == local_read(&cpu_buffer->pages_touched))\n\t\treturn;\n\n\tif (cpu_buffer->reader_page == cpu_buffer->commit_page)\n\t\treturn;\n\n\tif (!cpu_buffer->irq_work.full_waiters_pending)\n\t\treturn;\n\n\tcpu_buffer->last_pages_touch = local_read(&cpu_buffer->pages_touched);\n\n\tif (!full_hit(buffer, cpu_buffer->cpu, cpu_buffer->shortest_full))\n\t\treturn;\n\n\tcpu_buffer->irq_work.wakeup_full = true;\n\tcpu_buffer->irq_work.full_waiters_pending = false;\n\t \n\tirq_work_queue(&cpu_buffer->irq_work.work);\n}\n\n#ifdef CONFIG_RING_BUFFER_RECORD_RECURSION\n# define do_ring_buffer_record_recursion()\t\\\n\tdo_ftrace_record_recursion(_THIS_IP_, _RET_IP_)\n#else\n# define do_ring_buffer_record_recursion() do { } while (0)\n#endif\n\n \n\nstatic __always_inline bool\ntrace_recursive_lock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned int val = cpu_buffer->current_context;\n\tint bit = interrupt_context_level();\n\n\tbit = RB_CTX_NORMAL - bit;\n\n\tif (unlikely(val & (1 << (bit + cpu_buffer->nest)))) {\n\t\t \n\t\tbit = RB_CTX_TRANSITION;\n\t\tif (val & (1 << (bit + cpu_buffer->nest))) {\n\t\t\tdo_ring_buffer_record_recursion();\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tval |= (1 << (bit + cpu_buffer->nest));\n\tcpu_buffer->current_context = val;\n\n\treturn false;\n}\n\nstatic __always_inline void\ntrace_recursive_unlock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tcpu_buffer->current_context &=\n\t\tcpu_buffer->current_context - (1 << cpu_buffer->nest);\n}\n\n \n#define NESTED_BITS 5\n\n \nvoid ring_buffer_nest_start(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t \n\tpreempt_disable_notrace();\n\tcpu = raw_smp_processor_id();\n\tcpu_buffer = buffer->buffers[cpu];\n\t \n\tcpu_buffer->nest += NESTED_BITS;\n}\n\n \nvoid ring_buffer_nest_end(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t \n\tcpu = raw_smp_processor_id();\n\tcpu_buffer = buffer->buffers[cpu];\n\t \n\tcpu_buffer->nest -= NESTED_BITS;\n\tpreempt_enable_notrace();\n}\n\n \nint ring_buffer_unlock_commit(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu = raw_smp_processor_id();\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\trb_commit(cpu_buffer);\n\n\trb_wakeups(buffer, cpu_buffer);\n\n\ttrace_recursive_unlock(cpu_buffer);\n\n\tpreempt_enable_notrace();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_unlock_commit);\n\n \n#define CHECK_FULL_PAGE\t\t1L\n\n#ifdef CONFIG_RING_BUFFER_VALIDATE_TIME_DELTAS\nstatic void dump_buffer_page(struct buffer_data_page *bpage,\n\t\t\t     struct rb_event_info *info,\n\t\t\t     unsigned long tail)\n{\n\tstruct ring_buffer_event *event;\n\tu64 ts, delta;\n\tint e;\n\n\tts = bpage->time_stamp;\n\tpr_warn(\"  [%lld] PAGE TIME STAMP\\n\", ts);\n\n\tfor (e = 0; e < tail; e += rb_event_length(event)) {\n\n\t\tevent = (struct ring_buffer_event *)(bpage->data + e);\n\n\t\tswitch (event->type_len) {\n\n\t\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t\tdelta = rb_event_time_stamp(event);\n\t\t\tts += delta;\n\t\t\tpr_warn(\"  [%lld] delta:%lld TIME EXTEND\\n\", ts, delta);\n\t\t\tbreak;\n\n\t\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t\tdelta = rb_event_time_stamp(event);\n\t\t\tts = rb_fix_abs_ts(delta, ts);\n\t\t\tpr_warn(\"  [%lld] absolute:%lld TIME STAMP\\n\", ts, delta);\n\t\t\tbreak;\n\n\t\tcase RINGBUF_TYPE_PADDING:\n\t\t\tts += event->time_delta;\n\t\t\tpr_warn(\"  [%lld] delta:%d PADDING\\n\", ts, event->time_delta);\n\t\t\tbreak;\n\n\t\tcase RINGBUF_TYPE_DATA:\n\t\t\tts += event->time_delta;\n\t\t\tpr_warn(\"  [%lld] delta:%d\\n\", ts, event->time_delta);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic DEFINE_PER_CPU(atomic_t, checking);\nstatic atomic_t ts_dump;\n\n \nstatic void check_buffer(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t struct rb_event_info *info,\n\t\t\t unsigned long tail)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_data_page *bpage;\n\tu64 ts, delta;\n\tbool full = false;\n\tint e;\n\n\tbpage = info->tail_page->page;\n\n\tif (tail == CHECK_FULL_PAGE) {\n\t\tfull = true;\n\t\ttail = local_read(&bpage->commit);\n\t} else if (info->add_timestamp &\n\t\t   (RB_ADD_STAMP_FORCE | RB_ADD_STAMP_ABSOLUTE)) {\n\t\t \n\t\treturn;\n\t}\n\n\t \n\tif (tail <= 8 || tail > local_read(&bpage->commit))\n\t\treturn;\n\n\t \n\tif (atomic_inc_return(this_cpu_ptr(&checking)) != 1)\n\t\tgoto out;\n\n\tts = bpage->time_stamp;\n\n\tfor (e = 0; e < tail; e += rb_event_length(event)) {\n\n\t\tevent = (struct ring_buffer_event *)(bpage->data + e);\n\n\t\tswitch (event->type_len) {\n\n\t\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t\tdelta = rb_event_time_stamp(event);\n\t\t\tts += delta;\n\t\t\tbreak;\n\n\t\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\t\tdelta = rb_event_time_stamp(event);\n\t\t\tts = rb_fix_abs_ts(delta, ts);\n\t\t\tbreak;\n\n\t\tcase RINGBUF_TYPE_PADDING:\n\t\t\tif (event->time_delta == 1)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase RINGBUF_TYPE_DATA:\n\t\t\tts += event->time_delta;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\t}\n\t}\n\tif ((full && ts > info->ts) ||\n\t    (!full && ts + info->delta != info->ts)) {\n\t\t \n\t\tif (atomic_inc_return(&ts_dump) != 1) {\n\t\t\tatomic_dec(&ts_dump);\n\t\t\tgoto out;\n\t\t}\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t\t \n\t\tWARN_ON_ONCE(system_state != SYSTEM_BOOTING);\n\t\tpr_warn(\"[CPU: %d]TIME DOES NOT MATCH expected:%lld actual:%lld delta:%lld before:%lld after:%lld%s\\n\",\n\t\t\tcpu_buffer->cpu,\n\t\t\tts + info->delta, info->ts, info->delta,\n\t\t\tinfo->before, info->after,\n\t\t\tfull ? \" (full)\" : \"\");\n\t\tdump_buffer_page(bpage, info, tail);\n\t\tatomic_dec(&ts_dump);\n\t\t \n\t\treturn;\n\t}\nout:\n\tatomic_dec(this_cpu_ptr(&checking));\n}\n#else\nstatic inline void check_buffer(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t\t struct rb_event_info *info,\n\t\t\t unsigned long tail)\n{\n}\n#endif  \n\nstatic struct ring_buffer_event *\n__rb_reserve_next(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t  struct rb_event_info *info)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *tail_page;\n\tunsigned long tail, write, w;\n\tbool a_ok;\n\tbool b_ok;\n\n\t \n\ttail_page = info->tail_page = READ_ONCE(cpu_buffer->tail_page);\n\n  \tw = local_read(&tail_page->write) & RB_WRITE_MASK;\n\tbarrier();\n\tb_ok = rb_time_read(&cpu_buffer->before_stamp, &info->before);\n\ta_ok = rb_time_read(&cpu_buffer->write_stamp, &info->after);\n\tbarrier();\n\tinfo->ts = rb_time_stamp(cpu_buffer->buffer);\n\n\tif ((info->add_timestamp & RB_ADD_STAMP_ABSOLUTE)) {\n\t\tinfo->delta = info->ts;\n\t} else {\n\t\t \n\t\tif (!w) {\n\t\t\t \n\t\t\tinfo->delta = 0;\n\t\t} else if (unlikely(!a_ok || !b_ok || info->before != info->after)) {\n\t\t\tinfo->add_timestamp |= RB_ADD_STAMP_FORCE | RB_ADD_STAMP_EXTEND;\n\t\t\tinfo->length += RB_LEN_TIME_EXTEND;\n\t\t} else {\n\t\t\tinfo->delta = info->ts - info->after;\n\t\t\tif (unlikely(test_time_stamp(info->delta))) {\n\t\t\t\tinfo->add_timestamp |= RB_ADD_STAMP_EXTEND;\n\t\t\t\tinfo->length += RB_LEN_TIME_EXTEND;\n\t\t\t}\n\t\t}\n\t}\n\n  \trb_time_set(&cpu_buffer->before_stamp, info->ts);\n\n  \twrite = local_add_return(info->length, &tail_page->write);\n\n\t \n\twrite &= RB_WRITE_MASK;\n\n\ttail = write - info->length;\n\n\t \n\tif (unlikely(write > BUF_PAGE_SIZE)) {\n\t\tcheck_buffer(cpu_buffer, info, CHECK_FULL_PAGE);\n\t\treturn rb_move_tail(cpu_buffer, tail, info);\n\t}\n\n\tif (likely(tail == w)) {\n\t\t \n  \t\trb_time_set(&cpu_buffer->write_stamp, info->ts);\n\t\t \n\t\tif (likely(!(info->add_timestamp &\n\t\t\t     (RB_ADD_STAMP_FORCE | RB_ADD_STAMP_ABSOLUTE))))\n\t\t\t \n\t\t\tinfo->delta = info->ts - info->after;\n\t\telse\n\t\t\t \n\t\t\tinfo->delta = info->ts;\n\t\tcheck_buffer(cpu_buffer, info, tail);\n\t} else {\n\t\tu64 ts;\n\t\t \n\n\t\t \n\t\ta_ok = rb_time_read(&cpu_buffer->before_stamp, &info->before);\n\t\tRB_WARN_ON(cpu_buffer, !a_ok);\n\n\t\t \n\t\tts = rb_time_stamp(cpu_buffer->buffer);\n\t\trb_time_set(&cpu_buffer->before_stamp, ts);\n\n\t\tbarrier();\n  \t\ta_ok = rb_time_read(&cpu_buffer->write_stamp, &info->after);\n\t\t \n\t\tRB_WARN_ON(cpu_buffer, !a_ok);\n\t\tbarrier();\n  \t\tif (write == (local_read(&tail_page->write) & RB_WRITE_MASK) &&\n\t\t    info->after == info->before && info->after < ts) {\n\t\t\t \n\t\t\tinfo->delta = ts - info->after;\n\t\t} else {\n\t\t\t \n\t\t\tinfo->delta = 0;\n\t\t}\n\t\tinfo->ts = ts;\n\t\tinfo->add_timestamp &= ~RB_ADD_STAMP_FORCE;\n\t}\n\n\t \n\tif (unlikely(!tail && !(info->add_timestamp &\n\t\t\t\t(RB_ADD_STAMP_FORCE | RB_ADD_STAMP_ABSOLUTE))))\n\t\tinfo->delta = 0;\n\n\t \n\n\tevent = __rb_page_index(tail_page, tail);\n\trb_update_event(cpu_buffer, event, info);\n\n\tlocal_inc(&tail_page->entries);\n\n\t \n\tif (unlikely(!tail))\n\t\ttail_page->page->time_stamp = info->ts;\n\n\t \n\tlocal_add(info->length, &cpu_buffer->entries_bytes);\n\n\treturn event;\n}\n\nstatic __always_inline struct ring_buffer_event *\nrb_reserve_next_event(struct trace_buffer *buffer,\n\t\t      struct ring_buffer_per_cpu *cpu_buffer,\n\t\t      unsigned long length)\n{\n\tstruct ring_buffer_event *event;\n\tstruct rb_event_info info;\n\tint nr_loops = 0;\n\tint add_ts_default;\n\n\t \n\tif (!IS_ENABLED(CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG) &&\n\t    (unlikely(in_nmi()))) {\n\t\treturn NULL;\n\t}\n\n\trb_start_commit(cpu_buffer);\n\t \n\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t \n\tbarrier();\n\tif (unlikely(READ_ONCE(cpu_buffer->buffer) != buffer)) {\n\t\tlocal_dec(&cpu_buffer->committing);\n\t\tlocal_dec(&cpu_buffer->commits);\n\t\treturn NULL;\n\t}\n#endif\n\n\tinfo.length = rb_calculate_event_length(length);\n\n\tif (ring_buffer_time_stamp_abs(cpu_buffer->buffer)) {\n\t\tadd_ts_default = RB_ADD_STAMP_ABSOLUTE;\n\t\tinfo.length += RB_LEN_TIME_EXTEND;\n\t\tif (info.length > BUF_MAX_DATA_SIZE)\n\t\t\tgoto out_fail;\n\t} else {\n\t\tadd_ts_default = RB_ADD_STAMP_NONE;\n\t}\n\n again:\n\tinfo.add_timestamp = add_ts_default;\n\tinfo.delta = 0;\n\n\t \n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 1000))\n\t\tgoto out_fail;\n\n\tevent = __rb_reserve_next(cpu_buffer, &info);\n\n\tif (unlikely(PTR_ERR(event) == -EAGAIN)) {\n\t\tif (info.add_timestamp & (RB_ADD_STAMP_FORCE | RB_ADD_STAMP_EXTEND))\n\t\t\tinfo.length -= RB_LEN_TIME_EXTEND;\n\t\tgoto again;\n\t}\n\n\tif (likely(event))\n\t\treturn event;\n out_fail:\n\trb_end_commit(cpu_buffer);\n\treturn NULL;\n}\n\n \nstruct ring_buffer_event *\nring_buffer_lock_reserve(struct trace_buffer *buffer, unsigned long length)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tint cpu;\n\n\t \n\tpreempt_disable_notrace();\n\n\tif (unlikely(atomic_read(&buffer->record_disabled)))\n\t\tgoto out;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (unlikely(!cpumask_test_cpu(cpu, buffer->cpumask)))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\tif (unlikely(atomic_read(&cpu_buffer->record_disabled)))\n\t\tgoto out;\n\n\tif (unlikely(length > BUF_MAX_DATA_SIZE))\n\t\tgoto out;\n\n\tif (unlikely(trace_recursive_lock(cpu_buffer)))\n\t\tgoto out;\n\n\tevent = rb_reserve_next_event(buffer, cpu_buffer, length);\n\tif (!event)\n\t\tgoto out_unlock;\n\n\treturn event;\n\n out_unlock:\n\ttrace_recursive_unlock(cpu_buffer);\n out:\n\tpreempt_enable_notrace();\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_lock_reserve);\n\n \nstatic inline void\nrb_decrement_entry(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t   struct ring_buffer_event *event)\n{\n\tunsigned long addr = (unsigned long)event;\n\tstruct buffer_page *bpage = cpu_buffer->commit_page;\n\tstruct buffer_page *start;\n\n\taddr &= PAGE_MASK;\n\n\t \n\tif (likely(bpage->page == (void *)addr)) {\n\t\tlocal_dec(&bpage->entries);\n\t\treturn;\n\t}\n\n\t \n\trb_inc_page(&bpage);\n\tstart = bpage;\n\tdo {\n\t\tif (bpage->page == (void *)addr) {\n\t\t\tlocal_dec(&bpage->entries);\n\t\t\treturn;\n\t\t}\n\t\trb_inc_page(&bpage);\n\t} while (bpage != start);\n\n\t \n\tRB_WARN_ON(cpu_buffer, 1);\n}\n\n \nvoid ring_buffer_discard_commit(struct trace_buffer *buffer,\n\t\t\t\tstruct ring_buffer_event *event)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t \n\trb_event_discard(event);\n\n\tcpu = smp_processor_id();\n\tcpu_buffer = buffer->buffers[cpu];\n\n\t \n\tRB_WARN_ON(buffer, !local_read(&cpu_buffer->committing));\n\n\trb_decrement_entry(cpu_buffer, event);\n\tif (rb_try_to_discard(cpu_buffer, event))\n\t\tgoto out;\n\n out:\n\trb_end_commit(cpu_buffer);\n\n\ttrace_recursive_unlock(cpu_buffer);\n\n\tpreempt_enable_notrace();\n\n}\nEXPORT_SYMBOL_GPL(ring_buffer_discard_commit);\n\n \nint ring_buffer_write(struct trace_buffer *buffer,\n\t\t      unsigned long length,\n\t\t      void *data)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tvoid *body;\n\tint ret = -EBUSY;\n\tint cpu;\n\n\tpreempt_disable_notrace();\n\n\tif (atomic_read(&buffer->record_disabled))\n\t\tgoto out;\n\n\tcpu = raw_smp_processor_id();\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\tif (atomic_read(&cpu_buffer->record_disabled))\n\t\tgoto out;\n\n\tif (length > BUF_MAX_DATA_SIZE)\n\t\tgoto out;\n\n\tif (unlikely(trace_recursive_lock(cpu_buffer)))\n\t\tgoto out;\n\n\tevent = rb_reserve_next_event(buffer, cpu_buffer, length);\n\tif (!event)\n\t\tgoto out_unlock;\n\n\tbody = rb_event_data(event);\n\n\tmemcpy(body, data, length);\n\n\trb_commit(cpu_buffer);\n\n\trb_wakeups(buffer, cpu_buffer);\n\n\tret = 0;\n\n out_unlock:\n\ttrace_recursive_unlock(cpu_buffer);\n\n out:\n\tpreempt_enable_notrace();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_write);\n\nstatic bool rb_per_cpu_empty(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *reader = cpu_buffer->reader_page;\n\tstruct buffer_page *head = rb_set_head_page(cpu_buffer);\n\tstruct buffer_page *commit = cpu_buffer->commit_page;\n\n\t \n\tif (unlikely(!head))\n\t\treturn true;\n\n\t \n\tif (reader->read != rb_page_commit(reader))\n\t\treturn false;\n\n\t \n\tif (commit == reader)\n\t\treturn true;\n\n\t \n\tif (commit != head)\n\t\treturn false;\n\n\t \n\treturn rb_page_commit(commit) == 0;\n}\n\n \nvoid ring_buffer_record_disable(struct trace_buffer *buffer)\n{\n\tatomic_inc(&buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_disable);\n\n \nvoid ring_buffer_record_enable(struct trace_buffer *buffer)\n{\n\tatomic_dec(&buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_enable);\n\n \nvoid ring_buffer_record_off(struct trace_buffer *buffer)\n{\n\tunsigned int rd;\n\tunsigned int new_rd;\n\n\trd = atomic_read(&buffer->record_disabled);\n\tdo {\n\t\tnew_rd = rd | RB_BUFFER_OFF;\n\t} while (!atomic_try_cmpxchg(&buffer->record_disabled, &rd, new_rd));\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_off);\n\n \nvoid ring_buffer_record_on(struct trace_buffer *buffer)\n{\n\tunsigned int rd;\n\tunsigned int new_rd;\n\n\trd = atomic_read(&buffer->record_disabled);\n\tdo {\n\t\tnew_rd = rd & ~RB_BUFFER_OFF;\n\t} while (!atomic_try_cmpxchg(&buffer->record_disabled, &rd, new_rd));\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_on);\n\n \nbool ring_buffer_record_is_on(struct trace_buffer *buffer)\n{\n\treturn !atomic_read(&buffer->record_disabled);\n}\n\n \nbool ring_buffer_record_is_set_on(struct trace_buffer *buffer)\n{\n\treturn !(atomic_read(&buffer->record_disabled) & RB_BUFFER_OFF);\n}\n\n \nvoid ring_buffer_record_disable_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tatomic_inc(&cpu_buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_disable_cpu);\n\n \nvoid ring_buffer_record_enable_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tatomic_dec(&cpu_buffer->record_disabled);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_record_enable_cpu);\n\n \nstatic inline unsigned long\nrb_num_of_entries(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn local_read(&cpu_buffer->entries) -\n\t\t(local_read(&cpu_buffer->overrun) + cpu_buffer->read);\n}\n\n \nu64 ring_buffer_oldest_event_ts(struct trace_buffer *buffer, int cpu)\n{\n\tunsigned long flags;\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *bpage;\n\tu64 ret = 0;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\t \n\tif (cpu_buffer->tail_page == cpu_buffer->reader_page)\n\t\tbpage = cpu_buffer->reader_page;\n\telse\n\t\tbpage = rb_set_head_page(cpu_buffer);\n\tif (bpage)\n\t\tret = bpage->page->time_stamp;\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_oldest_event_ts);\n\n \nunsigned long ring_buffer_bytes_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->entries_bytes) - cpu_buffer->read_bytes;\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_bytes_cpu);\n\n \nunsigned long ring_buffer_entries_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\treturn rb_num_of_entries(cpu_buffer);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_entries_cpu);\n\n \nunsigned long ring_buffer_overrun_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->overrun);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_overrun_cpu);\n\n \nunsigned long\nring_buffer_commit_overrun_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->commit_overrun);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_commit_overrun_cpu);\n\n \nunsigned long\nring_buffer_dropped_events_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tret = local_read(&cpu_buffer->dropped_events);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_dropped_events_cpu);\n\n \nunsigned long\nring_buffer_read_events_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\treturn cpu_buffer->read;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_events_cpu);\n\n \nunsigned long ring_buffer_entries(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long entries = 0;\n\tint cpu;\n\n\t \n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tentries += rb_num_of_entries(cpu_buffer);\n\t}\n\n\treturn entries;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_entries);\n\n \nunsigned long ring_buffer_overruns(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long overruns = 0;\n\tint cpu;\n\n\t \n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\toverruns += local_read(&cpu_buffer->overrun);\n\t}\n\n\treturn overruns;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_overruns);\n\nstatic void rb_iter_reset(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\n\t \n\titer->head_page = cpu_buffer->reader_page;\n\titer->head = cpu_buffer->reader_page->read;\n\titer->next_event = iter->head;\n\n\titer->cache_reader_page = iter->head_page;\n\titer->cache_read = cpu_buffer->read;\n\titer->cache_pages_removed = cpu_buffer->pages_removed;\n\n\tif (iter->head) {\n\t\titer->read_stamp = cpu_buffer->read_stamp;\n\t\titer->page_stamp = cpu_buffer->reader_page->page->time_stamp;\n\t} else {\n\t\titer->read_stamp = iter->head_page->page->time_stamp;\n\t\titer->page_stamp = iter->read_stamp;\n\t}\n}\n\n \nvoid ring_buffer_iter_reset(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\n\tif (!iter)\n\t\treturn;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\trb_iter_reset(iter);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_reset);\n\n \nint ring_buffer_iter_empty(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_page *reader;\n\tstruct buffer_page *head_page;\n\tstruct buffer_page *commit_page;\n\tstruct buffer_page *curr_commit_page;\n\tunsigned commit;\n\tu64 curr_commit_ts;\n\tu64 commit_ts;\n\n\tcpu_buffer = iter->cpu_buffer;\n\treader = cpu_buffer->reader_page;\n\thead_page = cpu_buffer->head_page;\n\tcommit_page = cpu_buffer->commit_page;\n\tcommit_ts = commit_page->page->time_stamp;\n\n\t \n\tsmp_rmb();\n\tcommit = rb_page_commit(commit_page);\n\t \n\tsmp_rmb();\n\n\t \n\tcurr_commit_page = READ_ONCE(cpu_buffer->commit_page);\n\tcurr_commit_ts = READ_ONCE(curr_commit_page->page->time_stamp);\n\n\t \n\tif (curr_commit_page != commit_page ||\n\t    curr_commit_ts != commit_ts)\n\t\treturn 0;\n\n\t \n\treturn ((iter->head_page == commit_page && iter->head >= commit) ||\n\t\t(iter->head_page == reader && commit_page == head_page &&\n\t\t head_page->read == commit &&\n\t\t iter->head == rb_page_commit(cpu_buffer->reader_page)));\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_empty);\n\nstatic void\nrb_update_read_stamp(struct ring_buffer_per_cpu *cpu_buffer,\n\t\t     struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\tdelta = rb_event_time_stamp(event);\n\t\tcpu_buffer->read_stamp += delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\tdelta = rb_event_time_stamp(event);\n\t\tdelta = rb_fix_abs_ts(delta, cpu_buffer->read_stamp);\n\t\tcpu_buffer->read_stamp = delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tcpu_buffer->read_stamp += event->time_delta;\n\t\treturn;\n\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);\n\t}\n}\n\nstatic void\nrb_update_iter_read_stamp(struct ring_buffer_iter *iter,\n\t\t\t  struct ring_buffer_event *event)\n{\n\tu64 delta;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\tdelta = rb_event_time_stamp(event);\n\t\titer->read_stamp += delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\tdelta = rb_event_time_stamp(event);\n\t\tdelta = rb_fix_abs_ts(delta, iter->read_stamp);\n\t\titer->read_stamp = delta;\n\t\treturn;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\titer->read_stamp += event->time_delta;\n\t\treturn;\n\n\tdefault:\n\t\tRB_WARN_ON(iter->cpu_buffer, 1);\n\t}\n}\n\nstatic struct buffer_page *\nrb_get_reader_page(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *reader = NULL;\n\tunsigned long overwrite;\n\tunsigned long flags;\n\tint nr_loops = 0;\n\tbool ret;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\n again:\n\t \n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 3)) {\n\t\treader = NULL;\n\t\tgoto out;\n\t}\n\n\treader = cpu_buffer->reader_page;\n\n\t \n\tif (cpu_buffer->reader_page->read < rb_page_size(reader))\n\t\tgoto out;\n\n\t \n\tif (RB_WARN_ON(cpu_buffer,\n\t\t       cpu_buffer->reader_page->read > rb_page_size(reader)))\n\t\tgoto out;\n\n\t \n\treader = NULL;\n\tif (cpu_buffer->commit_page == cpu_buffer->reader_page)\n\t\tgoto out;\n\n\t \n\tif (rb_num_of_entries(cpu_buffer) == 0)\n\t\tgoto out;\n\n\t \n\tlocal_set(&cpu_buffer->reader_page->write, 0);\n\tlocal_set(&cpu_buffer->reader_page->entries, 0);\n\tlocal_set(&cpu_buffer->reader_page->page->commit, 0);\n\tcpu_buffer->reader_page->real_end = 0;\n\n spin:\n\t \n\treader = rb_set_head_page(cpu_buffer);\n\tif (!reader)\n\t\tgoto out;\n\tcpu_buffer->reader_page->list.next = rb_list_head(reader->list.next);\n\tcpu_buffer->reader_page->list.prev = reader->list.prev;\n\n\t \n\tcpu_buffer->pages = reader->list.prev;\n\n\t \n\trb_set_list_to_head(&cpu_buffer->reader_page->list);\n\n\t \n\tsmp_mb();\n\toverwrite = local_read(&(cpu_buffer->overrun));\n\n\t \n\n\tret = rb_head_page_replace(reader, cpu_buffer->reader_page);\n\n\t \n\tif (!ret)\n\t\tgoto spin;\n\n\t \n\trb_list_head(reader->list.next)->prev = &cpu_buffer->reader_page->list;\n\trb_inc_page(&cpu_buffer->head_page);\n\n\tlocal_inc(&cpu_buffer->pages_read);\n\n\t \n\tcpu_buffer->reader_page = reader;\n\tcpu_buffer->reader_page->read = 0;\n\n\tif (overwrite != cpu_buffer->last_overrun) {\n\t\tcpu_buffer->lost_events = overwrite - cpu_buffer->last_overrun;\n\t\tcpu_buffer->last_overrun = overwrite;\n\t}\n\n\tgoto again;\n\n out:\n\t \n\tif (reader && reader->read == 0)\n\t\tcpu_buffer->read_stamp = reader->page->time_stamp;\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\tlocal_irq_restore(flags);\n\n\t \n#define USECS_WAIT\t1000000\n        for (nr_loops = 0; nr_loops < USECS_WAIT; nr_loops++) {\n\t\t \n\t\tif (likely(!reader || rb_page_write(reader) <= BUF_PAGE_SIZE))\n\t\t\tbreak;\n\n\t\tudelay(1);\n\n\t\t \n\t\tsmp_rmb();\n\t}\n\n\t \n\tif (RB_WARN_ON(cpu_buffer, nr_loops == USECS_WAIT))\n\t\treader = NULL;\n\n\t \n\tsmp_rmb();\n\n\n\treturn reader;\n}\n\nstatic void rb_advance_reader(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *reader;\n\tunsigned length;\n\n\treader = rb_get_reader_page(cpu_buffer);\n\n\t \n\tif (RB_WARN_ON(cpu_buffer, !reader))\n\t\treturn;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tif (event->type_len <= RINGBUF_TYPE_DATA_TYPE_LEN_MAX)\n\t\tcpu_buffer->read++;\n\n\trb_update_read_stamp(cpu_buffer, event);\n\n\tlength = rb_event_length(event);\n\tcpu_buffer->reader_page->read += length;\n\tcpu_buffer->read_bytes += length;\n}\n\nstatic void rb_advance_iter(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\t \n\tif (iter->head == iter->next_event) {\n\t\t \n\t\tif (rb_iter_head_event(iter) == NULL)\n\t\t\treturn;\n\t}\n\n\titer->head = iter->next_event;\n\n\t \n\tif (iter->next_event >= rb_page_size(iter->head_page)) {\n\t\t \n\t\tif (iter->head_page == cpu_buffer->commit_page)\n\t\t\treturn;\n\t\trb_inc_iter(iter);\n\t\treturn;\n\t}\n\n\trb_update_iter_read_stamp(iter, iter->event);\n}\n\nstatic int rb_lost_events(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\treturn cpu_buffer->lost_events;\n}\n\nstatic struct ring_buffer_event *\nrb_buffer_peek(struct ring_buffer_per_cpu *cpu_buffer, u64 *ts,\n\t       unsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct buffer_page *reader;\n\tint nr_loops = 0;\n\n\tif (ts)\n\t\t*ts = 0;\n again:\n\t \n\tif (RB_WARN_ON(cpu_buffer, ++nr_loops > 2))\n\t\treturn NULL;\n\n\treader = rb_get_reader_page(cpu_buffer);\n\tif (!reader)\n\t\treturn NULL;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event))\n\t\t\tRB_WARN_ON(cpu_buffer, 1);\n\t\t \n\t\treturn event;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t \n\t\trb_advance_reader(cpu_buffer);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\tif (ts) {\n\t\t\t*ts = rb_event_time_stamp(event);\n\t\t\t*ts = rb_fix_abs_ts(*ts, reader->page->time_stamp);\n\t\t\tring_buffer_normalize_time_stamp(cpu_buffer->buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\t \n\t\trb_advance_reader(cpu_buffer);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tif (ts && !(*ts)) {\n\t\t\t*ts = cpu_buffer->read_stamp + event->time_delta;\n\t\t\tring_buffer_normalize_time_stamp(cpu_buffer->buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\tif (lost_events)\n\t\t\t*lost_events = rb_lost_events(cpu_buffer);\n\t\treturn event;\n\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_peek);\n\nstatic struct ring_buffer_event *\nrb_iter_peek(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct trace_buffer *buffer;\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tint nr_loops = 0;\n\n\tif (ts)\n\t\t*ts = 0;\n\n\tcpu_buffer = iter->cpu_buffer;\n\tbuffer = cpu_buffer->buffer;\n\n\t \n\tif (unlikely(iter->cache_read != cpu_buffer->read ||\n\t\t     iter->cache_reader_page != cpu_buffer->reader_page ||\n\t\t     iter->cache_pages_removed != cpu_buffer->pages_removed))\n\t\trb_iter_reset(iter);\n\n again:\n\tif (ring_buffer_iter_empty(iter))\n\t\treturn NULL;\n\n\t \n\tif (++nr_loops > 3)\n\t\treturn NULL;\n\n\tif (rb_per_cpu_empty(cpu_buffer))\n\t\treturn NULL;\n\n\tif (iter->head >= rb_page_size(iter->head_page)) {\n\t\trb_inc_iter(iter);\n\t\tgoto again;\n\t}\n\n\tevent = rb_iter_head_event(iter);\n\tif (!event)\n\t\tgoto again;\n\n\tswitch (event->type_len) {\n\tcase RINGBUF_TYPE_PADDING:\n\t\tif (rb_null_event(event)) {\n\t\t\trb_inc_iter(iter);\n\t\t\tgoto again;\n\t\t}\n\t\trb_advance_iter(iter);\n\t\treturn event;\n\n\tcase RINGBUF_TYPE_TIME_EXTEND:\n\t\t \n\t\trb_advance_iter(iter);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_TIME_STAMP:\n\t\tif (ts) {\n\t\t\t*ts = rb_event_time_stamp(event);\n\t\t\t*ts = rb_fix_abs_ts(*ts, iter->head_page->page->time_stamp);\n\t\t\tring_buffer_normalize_time_stamp(cpu_buffer->buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\t \n\t\trb_advance_iter(iter);\n\t\tgoto again;\n\n\tcase RINGBUF_TYPE_DATA:\n\t\tif (ts && !(*ts)) {\n\t\t\t*ts = iter->read_stamp + event->time_delta;\n\t\t\tring_buffer_normalize_time_stamp(buffer,\n\t\t\t\t\t\t\t cpu_buffer->cpu, ts);\n\t\t}\n\t\treturn event;\n\n\tdefault:\n\t\tRB_WARN_ON(cpu_buffer, 1);\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_peek);\n\nstatic inline bool rb_reader_lock(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tif (likely(!in_nmi())) {\n\t\traw_spin_lock(&cpu_buffer->reader_lock);\n\t\treturn true;\n\t}\n\n\t \n\tif (raw_spin_trylock(&cpu_buffer->reader_lock))\n\t\treturn true;\n\n\t \n\tatomic_inc(&cpu_buffer->record_disabled);\n\treturn false;\n}\n\nstatic inline void\nrb_reader_unlock(struct ring_buffer_per_cpu *cpu_buffer, bool locked)\n{\n\tif (likely(locked))\n\t\traw_spin_unlock(&cpu_buffer->reader_lock);\n}\n\n \nstruct ring_buffer_event *\nring_buffer_peek(struct trace_buffer *buffer, int cpu, u64 *ts,\n\t\t unsigned long *lost_events)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tstruct ring_buffer_event *event;\n\tunsigned long flags;\n\tbool dolock;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn NULL;\n\n again:\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\tevent = rb_buffer_peek(cpu_buffer, ts, lost_events);\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\trb_advance_reader(cpu_buffer);\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\n\n \nbool ring_buffer_iter_dropped(struct ring_buffer_iter *iter)\n{\n\tbool ret = iter->missed_events != 0;\n\n\titer->missed_events = 0;\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_dropped);\n\n \nstruct ring_buffer_event *\nring_buffer_iter_peek(struct ring_buffer_iter *iter, u64 *ts)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tstruct ring_buffer_event *event;\n\tunsigned long flags;\n\n again:\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\tevent = rb_iter_peek(iter, ts);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\n\n \nstruct ring_buffer_event *\nring_buffer_consume(struct trace_buffer *buffer, int cpu, u64 *ts,\n\t\t    unsigned long *lost_events)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_event *event = NULL;\n\tunsigned long flags;\n\tbool dolock;\n\n again:\n\t \n\tpreempt_disable();\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\n\tevent = rb_buffer_peek(cpu_buffer, ts, lost_events);\n\tif (event) {\n\t\tcpu_buffer->lost_events = 0;\n\t\trb_advance_reader(cpu_buffer);\n\t}\n\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n out:\n\tpreempt_enable();\n\n\tif (event && event->type_len == RINGBUF_TYPE_PADDING)\n\t\tgoto again;\n\n\treturn event;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_consume);\n\n \nstruct ring_buffer_iter *\nring_buffer_read_prepare(struct trace_buffer *buffer, int cpu, gfp_t flags)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct ring_buffer_iter *iter;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn NULL;\n\n\titer = kzalloc(sizeof(*iter), flags);\n\tif (!iter)\n\t\treturn NULL;\n\n\t \n\titer->event = kmalloc(BUF_PAGE_SIZE, flags);\n\tif (!iter->event) {\n\t\tkfree(iter);\n\t\treturn NULL;\n\t}\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\titer->cpu_buffer = cpu_buffer;\n\n\tatomic_inc(&cpu_buffer->resize_disabled);\n\n\treturn iter;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_prepare);\n\n \nvoid\nring_buffer_read_prepare_sync(void)\n{\n\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_prepare_sync);\n\n \nvoid\nring_buffer_read_start(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\n\tif (!iter)\n\t\treturn;\n\n\tcpu_buffer = iter->cpu_buffer;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\trb_iter_reset(iter);\n\tarch_spin_unlock(&cpu_buffer->lock);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_start);\n\n \nvoid\nring_buffer_read_finish(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tunsigned long flags;\n\n\t \n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\trb_check_pages(cpu_buffer);\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n\tatomic_dec(&cpu_buffer->resize_disabled);\n\tkfree(iter->event);\n\tkfree(iter);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_finish);\n\n \nvoid ring_buffer_iter_advance(struct ring_buffer_iter *iter)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = iter->cpu_buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\trb_advance_iter(iter);\n\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_iter_advance);\n\n \nunsigned long ring_buffer_size(struct trace_buffer *buffer, int cpu)\n{\n\t \n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\treturn BUF_PAGE_SIZE * buffer->buffers[cpu]->nr_pages;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_size);\n\nstatic void rb_clear_buffer_page(struct buffer_page *page)\n{\n\tlocal_set(&page->write, 0);\n\tlocal_set(&page->entries, 0);\n\trb_init_page(page->page);\n\tpage->read = 0;\n}\n\nstatic void\nrb_reset_cpu(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tstruct buffer_page *page;\n\n\trb_head_page_deactivate(cpu_buffer);\n\n\tcpu_buffer->head_page\n\t\t= list_entry(cpu_buffer->pages, struct buffer_page, list);\n\trb_clear_buffer_page(cpu_buffer->head_page);\n\tlist_for_each_entry(page, cpu_buffer->pages, list) {\n\t\trb_clear_buffer_page(page);\n\t}\n\n\tcpu_buffer->tail_page = cpu_buffer->head_page;\n\tcpu_buffer->commit_page = cpu_buffer->head_page;\n\n\tINIT_LIST_HEAD(&cpu_buffer->reader_page->list);\n\tINIT_LIST_HEAD(&cpu_buffer->new_pages);\n\trb_clear_buffer_page(cpu_buffer->reader_page);\n\n\tlocal_set(&cpu_buffer->entries_bytes, 0);\n\tlocal_set(&cpu_buffer->overrun, 0);\n\tlocal_set(&cpu_buffer->commit_overrun, 0);\n\tlocal_set(&cpu_buffer->dropped_events, 0);\n\tlocal_set(&cpu_buffer->entries, 0);\n\tlocal_set(&cpu_buffer->committing, 0);\n\tlocal_set(&cpu_buffer->commits, 0);\n\tlocal_set(&cpu_buffer->pages_touched, 0);\n\tlocal_set(&cpu_buffer->pages_lost, 0);\n\tlocal_set(&cpu_buffer->pages_read, 0);\n\tcpu_buffer->last_pages_touch = 0;\n\tcpu_buffer->shortest_full = 0;\n\tcpu_buffer->read = 0;\n\tcpu_buffer->read_bytes = 0;\n\n\trb_time_set(&cpu_buffer->write_stamp, 0);\n\trb_time_set(&cpu_buffer->before_stamp, 0);\n\n\tmemset(cpu_buffer->event_stamp, 0, sizeof(cpu_buffer->event_stamp));\n\n\tcpu_buffer->lost_events = 0;\n\tcpu_buffer->last_overrun = 0;\n\n\trb_head_page_activate(cpu_buffer);\n\tcpu_buffer->pages_removed = 0;\n}\n\n \nstatic void reset_disabled_cpu_buffer(struct ring_buffer_per_cpu *cpu_buffer)\n{\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\tif (RB_WARN_ON(cpu_buffer, local_read(&cpu_buffer->committing)))\n\t\tgoto out;\n\n\tarch_spin_lock(&cpu_buffer->lock);\n\n\trb_reset_cpu(cpu_buffer);\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\n out:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n}\n\n \nvoid ring_buffer_reset_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn;\n\n\t \n\tmutex_lock(&buffer->mutex);\n\n\tatomic_inc(&cpu_buffer->resize_disabled);\n\tatomic_inc(&cpu_buffer->record_disabled);\n\n\t \n\tsynchronize_rcu();\n\n\treset_disabled_cpu_buffer(cpu_buffer);\n\n\tatomic_dec(&cpu_buffer->record_disabled);\n\tatomic_dec(&cpu_buffer->resize_disabled);\n\n\tmutex_unlock(&buffer->mutex);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_reset_cpu);\n\n \n#define RESET_BIT\t(1 << 30)\n\n \nvoid ring_buffer_reset_online_cpus(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t \n\tmutex_lock(&buffer->mutex);\n\n\tfor_each_online_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_add(RESET_BIT, &cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\t \n\t\tif (!(atomic_read(&cpu_buffer->resize_disabled) & RESET_BIT))\n\t\t\tcontinue;\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_sub(RESET_BIT, &cpu_buffer->resize_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n}\n\n \nvoid ring_buffer_reset(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tint cpu;\n\n\t \n\tmutex_lock(&buffer->mutex);\n\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\tatomic_inc(&cpu_buffer->resize_disabled);\n\t\tatomic_inc(&cpu_buffer->record_disabled);\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\n\t\treset_disabled_cpu_buffer(cpu_buffer);\n\n\t\tatomic_dec(&cpu_buffer->record_disabled);\n\t\tatomic_dec(&cpu_buffer->resize_disabled);\n\t}\n\n\tmutex_unlock(&buffer->mutex);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_reset);\n\n \nbool ring_buffer_empty(struct trace_buffer *buffer)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\tbool dolock;\n\tbool ret;\n\tint cpu;\n\n\t \n\tfor_each_buffer_cpu(buffer, cpu) {\n\t\tcpu_buffer = buffer->buffers[cpu];\n\t\tlocal_irq_save(flags);\n\t\tdolock = rb_reader_lock(cpu_buffer);\n\t\tret = rb_per_cpu_empty(cpu_buffer);\n\t\trb_reader_unlock(cpu_buffer, dolock);\n\t\tlocal_irq_restore(flags);\n\n\t\tif (!ret)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_empty);\n\n \nbool ring_buffer_empty_cpu(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tunsigned long flags;\n\tbool dolock;\n\tbool ret;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn true;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tdolock = rb_reader_lock(cpu_buffer);\n\tret = rb_per_cpu_empty(cpu_buffer);\n\trb_reader_unlock(cpu_buffer, dolock);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_empty_cpu);\n\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n \nint ring_buffer_swap_cpu(struct trace_buffer *buffer_a,\n\t\t\t struct trace_buffer *buffer_b, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer_a;\n\tstruct ring_buffer_per_cpu *cpu_buffer_b;\n\tint ret = -EINVAL;\n\n\tif (!cpumask_test_cpu(cpu, buffer_a->cpumask) ||\n\t    !cpumask_test_cpu(cpu, buffer_b->cpumask))\n\t\tgoto out;\n\n\tcpu_buffer_a = buffer_a->buffers[cpu];\n\tcpu_buffer_b = buffer_b->buffers[cpu];\n\n\t \n\tif (cpu_buffer_a->nr_pages != cpu_buffer_b->nr_pages)\n\t\tgoto out;\n\n\tret = -EAGAIN;\n\n\tif (atomic_read(&buffer_a->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&buffer_b->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&cpu_buffer_a->record_disabled))\n\t\tgoto out;\n\n\tif (atomic_read(&cpu_buffer_b->record_disabled))\n\t\tgoto out;\n\n\t \n\tatomic_inc(&cpu_buffer_a->record_disabled);\n\tatomic_inc(&cpu_buffer_b->record_disabled);\n\n\tret = -EBUSY;\n\tif (local_read(&cpu_buffer_a->committing))\n\t\tgoto out_dec;\n\tif (local_read(&cpu_buffer_b->committing))\n\t\tgoto out_dec;\n\n\t \n\tif (atomic_read(&buffer_a->resizing))\n\t\tgoto out_dec;\n\tif (atomic_read(&buffer_b->resizing))\n\t\tgoto out_dec;\n\n\tbuffer_a->buffers[cpu] = cpu_buffer_b;\n\tbuffer_b->buffers[cpu] = cpu_buffer_a;\n\n\tcpu_buffer_b->buffer = buffer_a;\n\tcpu_buffer_a->buffer = buffer_b;\n\n\tret = 0;\n\nout_dec:\n\tatomic_dec(&cpu_buffer_a->record_disabled);\n\tatomic_dec(&cpu_buffer_b->record_disabled);\nout:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_swap_cpu);\n#endif  \n\n \nvoid *ring_buffer_alloc_read_page(struct trace_buffer *buffer, int cpu)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_data_page *bpage = NULL;\n\tunsigned long flags;\n\tstruct page *page;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tcpu_buffer = buffer->buffers[cpu];\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\n\tif (cpu_buffer->free_page) {\n\t\tbpage = cpu_buffer->free_page;\n\t\tcpu_buffer->free_page = NULL;\n\t}\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\tlocal_irq_restore(flags);\n\n\tif (bpage)\n\t\tgoto out;\n\n\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbpage = page_address(page);\n\n out:\n\trb_init_page(bpage);\n\n\treturn bpage;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_alloc_read_page);\n\n \nvoid ring_buffer_free_read_page(struct trace_buffer *buffer, int cpu, void *data)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer;\n\tstruct buffer_data_page *bpage = data;\n\tstruct page *page = virt_to_page(bpage);\n\tunsigned long flags;\n\n\tif (!buffer || !buffer->buffers || !buffer->buffers[cpu])\n\t\treturn;\n\n\tcpu_buffer = buffer->buffers[cpu];\n\n\t \n\tif (page_ref_count(page) > 1)\n\t\tgoto out;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&cpu_buffer->lock);\n\n\tif (!cpu_buffer->free_page) {\n\t\tcpu_buffer->free_page = bpage;\n\t\tbpage = NULL;\n\t}\n\n\tarch_spin_unlock(&cpu_buffer->lock);\n\tlocal_irq_restore(flags);\n\n out:\n\tfree_page((unsigned long)bpage);\n}\nEXPORT_SYMBOL_GPL(ring_buffer_free_read_page);\n\n \nint ring_buffer_read_page(struct trace_buffer *buffer,\n\t\t\t  void **data_page, size_t len, int cpu, int full)\n{\n\tstruct ring_buffer_per_cpu *cpu_buffer = buffer->buffers[cpu];\n\tstruct ring_buffer_event *event;\n\tstruct buffer_data_page *bpage;\n\tstruct buffer_page *reader;\n\tunsigned long missed_events;\n\tunsigned long flags;\n\tunsigned int commit;\n\tunsigned int read;\n\tu64 save_timestamp;\n\tint ret = -1;\n\n\tif (!cpumask_test_cpu(cpu, buffer->cpumask))\n\t\tgoto out;\n\n\t \n\tif (len <= BUF_PAGE_HDR_SIZE)\n\t\tgoto out;\n\n\tlen -= BUF_PAGE_HDR_SIZE;\n\n\tif (!data_page)\n\t\tgoto out;\n\n\tbpage = *data_page;\n\tif (!bpage)\n\t\tgoto out;\n\n\traw_spin_lock_irqsave(&cpu_buffer->reader_lock, flags);\n\n\treader = rb_get_reader_page(cpu_buffer);\n\tif (!reader)\n\t\tgoto out_unlock;\n\n\tevent = rb_reader_event(cpu_buffer);\n\n\tread = reader->read;\n\tcommit = rb_page_commit(reader);\n\n\t \n\tmissed_events = cpu_buffer->lost_events;\n\n\t \n\tif (read || (len < (commit - read)) ||\n\t    cpu_buffer->reader_page == cpu_buffer->commit_page) {\n\t\tstruct buffer_data_page *rpage = cpu_buffer->reader_page->page;\n\t\tunsigned int rpos = read;\n\t\tunsigned int pos = 0;\n\t\tunsigned int size;\n\n\t\t \n\t\tif (full &&\n\t\t    (!read || (len < (commit - read)) ||\n\t\t     cpu_buffer->reader_page == cpu_buffer->commit_page))\n\t\t\tgoto out_unlock;\n\n\t\tif (len > (commit - read))\n\t\t\tlen = (commit - read);\n\n\t\t \n\t\tsize = rb_event_ts_length(event);\n\n\t\tif (len < size)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tsave_timestamp = cpu_buffer->read_stamp;\n\n\t\t \n\t\tdo {\n\t\t\t \n\t\t\tsize = rb_event_length(event);\n\t\t\tmemcpy(bpage->data + pos, rpage->data + rpos, size);\n\n\t\t\tlen -= size;\n\n\t\t\trb_advance_reader(cpu_buffer);\n\t\t\trpos = reader->read;\n\t\t\tpos += size;\n\n\t\t\tif (rpos >= commit)\n\t\t\t\tbreak;\n\n\t\t\tevent = rb_reader_event(cpu_buffer);\n\t\t\t \n\t\t\tsize = rb_event_ts_length(event);\n\t\t} while (len >= size);\n\n\t\t \n\t\tlocal_set(&bpage->commit, pos);\n\t\tbpage->time_stamp = save_timestamp;\n\n\t\t \n\t\tread = 0;\n\t} else {\n\t\t \n\t\tcpu_buffer->read += rb_page_entries(reader);\n\t\tcpu_buffer->read_bytes += rb_page_commit(reader);\n\n\t\t \n\t\trb_init_page(bpage);\n\t\tbpage = reader->page;\n\t\treader->page = *data_page;\n\t\tlocal_set(&reader->write, 0);\n\t\tlocal_set(&reader->entries, 0);\n\t\treader->read = 0;\n\t\t*data_page = bpage;\n\n\t\t \n\t\tif (reader->real_end)\n\t\t\tlocal_set(&bpage->commit, reader->real_end);\n\t}\n\tret = read;\n\n\tcpu_buffer->lost_events = 0;\n\n\tcommit = local_read(&bpage->commit);\n\t \n\tif (missed_events) {\n\t\t \n\t\tif (BUF_PAGE_SIZE - commit >= sizeof(missed_events)) {\n\t\t\tmemcpy(&bpage->data[commit], &missed_events,\n\t\t\t       sizeof(missed_events));\n\t\t\tlocal_add(RB_MISSED_STORED, &bpage->commit);\n\t\t\tcommit += sizeof(missed_events);\n\t\t}\n\t\tlocal_add(RB_MISSED_EVENTS, &bpage->commit);\n\t}\n\n\t \n\tif (commit < BUF_PAGE_SIZE)\n\t\tmemset(&bpage->data[commit], 0, BUF_PAGE_SIZE - commit);\n\n out_unlock:\n\traw_spin_unlock_irqrestore(&cpu_buffer->reader_lock, flags);\n\n out:\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(ring_buffer_read_page);\n\n \nint trace_rb_cpu_prepare(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct trace_buffer *buffer;\n\tlong nr_pages_same;\n\tint cpu_i;\n\tunsigned long nr_pages;\n\n\tbuffer = container_of(node, struct trace_buffer, node);\n\tif (cpumask_test_cpu(cpu, buffer->cpumask))\n\t\treturn 0;\n\n\tnr_pages = 0;\n\tnr_pages_same = 1;\n\t \n\tfor_each_buffer_cpu(buffer, cpu_i) {\n\t\t \n\t\tif (nr_pages == 0)\n\t\t\tnr_pages = buffer->buffers[cpu_i]->nr_pages;\n\t\tif (nr_pages != buffer->buffers[cpu_i]->nr_pages) {\n\t\t\tnr_pages_same = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\t \n\tif (!nr_pages_same)\n\t\tnr_pages = 2;\n\tbuffer->buffers[cpu] =\n\t\trb_allocate_cpu_buffer(buffer, nr_pages, cpu);\n\tif (!buffer->buffers[cpu]) {\n\t\tWARN(1, \"failed to allocate ring buffer on CPU %u\\n\",\n\t\t     cpu);\n\t\treturn -ENOMEM;\n\t}\n\tsmp_wmb();\n\tcpumask_set_cpu(cpu, buffer->cpumask);\n\treturn 0;\n}\n\n#ifdef CONFIG_RING_BUFFER_STARTUP_TEST\n \nstatic struct task_struct *rb_threads[NR_CPUS] __initdata;\n\nstruct rb_test_data {\n\tstruct trace_buffer *buffer;\n\tunsigned long\t\tevents;\n\tunsigned long\t\tbytes_written;\n\tunsigned long\t\tbytes_alloc;\n\tunsigned long\t\tbytes_dropped;\n\tunsigned long\t\tevents_nested;\n\tunsigned long\t\tbytes_written_nested;\n\tunsigned long\t\tbytes_alloc_nested;\n\tunsigned long\t\tbytes_dropped_nested;\n\tint\t\t\tmin_size_nested;\n\tint\t\t\tmax_size_nested;\n\tint\t\t\tmax_size;\n\tint\t\t\tmin_size;\n\tint\t\t\tcpu;\n\tint\t\t\tcnt;\n};\n\nstatic struct rb_test_data rb_data[NR_CPUS] __initdata;\n\n \n#define RB_TEST_BUFFER_SIZE\t1048576\n\nstatic char rb_string[] __initdata =\n\t\"abcdefghijklmnopqrstuvwxyz1234567890!@#$%^&*()?+\\\\\"\n\t\"?+|:';\\\",.<>/?abcdefghijklmnopqrstuvwxyz1234567890\"\n\t\"!@#$%^&*()?+\\\\?+|:';\\\",.<>/?abcdefghijklmnopqrstuv\";\n\nstatic bool rb_test_started __initdata;\n\nstruct rb_item {\n\tint size;\n\tchar str[];\n};\n\nstatic __init int rb_write_something(struct rb_test_data *data, bool nested)\n{\n\tstruct ring_buffer_event *event;\n\tstruct rb_item *item;\n\tbool started;\n\tint event_len;\n\tint size;\n\tint len;\n\tint cnt;\n\n\t \n\tcnt = data->cnt + (nested ? 27 : 0);\n\n\t \n\tsize = (cnt * 68 / 25) % (sizeof(rb_string) - 1);\n\n\tlen = size + sizeof(struct rb_item);\n\n\tstarted = rb_test_started;\n\t \n\tsmp_rmb();\n\n\tevent = ring_buffer_lock_reserve(data->buffer, len);\n\tif (!event) {\n\t\t \n\t\tif (started) {\n\t\t\tif (nested)\n\t\t\t\tdata->bytes_dropped += len;\n\t\t\telse\n\t\t\t\tdata->bytes_dropped_nested += len;\n\t\t}\n\t\treturn len;\n\t}\n\n\tevent_len = ring_buffer_event_length(event);\n\n\tif (RB_WARN_ON(data->buffer, event_len < len))\n\t\tgoto out;\n\n\titem = ring_buffer_event_data(event);\n\titem->size = size;\n\tmemcpy(item->str, rb_string, size);\n\n\tif (nested) {\n\t\tdata->bytes_alloc_nested += event_len;\n\t\tdata->bytes_written_nested += len;\n\t\tdata->events_nested++;\n\t\tif (!data->min_size_nested || len < data->min_size_nested)\n\t\t\tdata->min_size_nested = len;\n\t\tif (len > data->max_size_nested)\n\t\t\tdata->max_size_nested = len;\n\t} else {\n\t\tdata->bytes_alloc += event_len;\n\t\tdata->bytes_written += len;\n\t\tdata->events++;\n\t\tif (!data->min_size || len < data->min_size)\n\t\t\tdata->max_size = len;\n\t\tif (len > data->max_size)\n\t\t\tdata->max_size = len;\n\t}\n\n out:\n\tring_buffer_unlock_commit(data->buffer);\n\n\treturn 0;\n}\n\nstatic __init int rb_test(void *arg)\n{\n\tstruct rb_test_data *data = arg;\n\n\twhile (!kthread_should_stop()) {\n\t\trb_write_something(data, false);\n\t\tdata->cnt++;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t \n\t\tusleep_range(((data->cnt % 3) + 1) * 100, 1000);\n\t}\n\n\treturn 0;\n}\n\nstatic __init void rb_ipi(void *ignore)\n{\n\tstruct rb_test_data *data;\n\tint cpu = smp_processor_id();\n\n\tdata = &rb_data[cpu];\n\trb_write_something(data, true);\n}\n\nstatic __init int rb_hammer_test(void *arg)\n{\n\twhile (!kthread_should_stop()) {\n\n\t\t \n\t\tsmp_call_function(rb_ipi, NULL, 1);\n\t\t \n\t\tschedule();\n\t}\n\n\treturn 0;\n}\n\nstatic __init int test_ringbuffer(void)\n{\n\tstruct task_struct *rb_hammer;\n\tstruct trace_buffer *buffer;\n\tint cpu;\n\tint ret = 0;\n\n\tif (security_locked_down(LOCKDOWN_TRACEFS)) {\n\t\tpr_warn(\"Lockdown is enabled, skipping ring buffer tests\\n\");\n\t\treturn 0;\n\t}\n\n\tpr_info(\"Running ring buffer tests...\\n\");\n\n\tbuffer = ring_buffer_alloc(RB_TEST_BUFFER_SIZE, RB_FL_OVERWRITE);\n\tif (WARN_ON(!buffer))\n\t\treturn 0;\n\n\t \n\tring_buffer_record_off(buffer);\n\n\tfor_each_online_cpu(cpu) {\n\t\trb_data[cpu].buffer = buffer;\n\t\trb_data[cpu].cpu = cpu;\n\t\trb_data[cpu].cnt = cpu;\n\t\trb_threads[cpu] = kthread_run_on_cpu(rb_test, &rb_data[cpu],\n\t\t\t\t\t\t     cpu, \"rbtester/%u\");\n\t\tif (WARN_ON(IS_ERR(rb_threads[cpu]))) {\n\t\t\tpr_cont(\"FAILED\\n\");\n\t\t\tret = PTR_ERR(rb_threads[cpu]);\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\t \n\trb_hammer = kthread_run(rb_hammer_test, NULL, \"rbhammer\");\n\tif (WARN_ON(IS_ERR(rb_hammer))) {\n\t\tpr_cont(\"FAILED\\n\");\n\t\tret = PTR_ERR(rb_hammer);\n\t\tgoto out_free;\n\t}\n\n\tring_buffer_record_on(buffer);\n\t \n\tsmp_wmb();\n\trb_test_started = true;\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\t ;\n\tschedule_timeout(10 * HZ);\n\n\tkthread_stop(rb_hammer);\n\n out_free:\n\tfor_each_online_cpu(cpu) {\n\t\tif (!rb_threads[cpu])\n\t\t\tbreak;\n\t\tkthread_stop(rb_threads[cpu]);\n\t}\n\tif (ret) {\n\t\tring_buffer_free(buffer);\n\t\treturn ret;\n\t}\n\n\t \n\tpr_info(\"finished\\n\");\n\tfor_each_online_cpu(cpu) {\n\t\tstruct ring_buffer_event *event;\n\t\tstruct rb_test_data *data = &rb_data[cpu];\n\t\tstruct rb_item *item;\n\t\tunsigned long total_events;\n\t\tunsigned long total_dropped;\n\t\tunsigned long total_written;\n\t\tunsigned long total_alloc;\n\t\tunsigned long total_read = 0;\n\t\tunsigned long total_size = 0;\n\t\tunsigned long total_len = 0;\n\t\tunsigned long total_lost = 0;\n\t\tunsigned long lost;\n\t\tint big_event_size;\n\t\tint small_event_size;\n\n\t\tret = -1;\n\n\t\ttotal_events = data->events + data->events_nested;\n\t\ttotal_written = data->bytes_written + data->bytes_written_nested;\n\t\ttotal_alloc = data->bytes_alloc + data->bytes_alloc_nested;\n\t\ttotal_dropped = data->bytes_dropped + data->bytes_dropped_nested;\n\n\t\tbig_event_size = data->max_size + data->max_size_nested;\n\t\tsmall_event_size = data->min_size + data->min_size_nested;\n\n\t\tpr_info(\"CPU %d:\\n\", cpu);\n\t\tpr_info(\"              events:    %ld\\n\", total_events);\n\t\tpr_info(\"       dropped bytes:    %ld\\n\", total_dropped);\n\t\tpr_info(\"       alloced bytes:    %ld\\n\", total_alloc);\n\t\tpr_info(\"       written bytes:    %ld\\n\", total_written);\n\t\tpr_info(\"       biggest event:    %d\\n\", big_event_size);\n\t\tpr_info(\"      smallest event:    %d\\n\", small_event_size);\n\n\t\tif (RB_WARN_ON(buffer, total_dropped))\n\t\t\tbreak;\n\n\t\tret = 0;\n\n\t\twhile ((event = ring_buffer_consume(buffer, cpu, NULL, &lost))) {\n\t\t\ttotal_lost += lost;\n\t\t\titem = ring_buffer_event_data(event);\n\t\t\ttotal_len += ring_buffer_event_length(event);\n\t\t\ttotal_size += item->size + sizeof(struct rb_item);\n\t\t\tif (memcmp(&item->str[0], rb_string, item->size) != 0) {\n\t\t\t\tpr_info(\"FAILED!\\n\");\n\t\t\t\tpr_info(\"buffer had: %.*s\\n\", item->size, item->str);\n\t\t\t\tpr_info(\"expected:   %.*s\\n\", item->size, rb_string);\n\t\t\t\tRB_WARN_ON(buffer, 1);\n\t\t\t\tret = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\ttotal_read++;\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = -1;\n\n\t\tpr_info(\"         read events:   %ld\\n\", total_read);\n\t\tpr_info(\"         lost events:   %ld\\n\", total_lost);\n\t\tpr_info(\"        total events:   %ld\\n\", total_lost + total_read);\n\t\tpr_info(\"  recorded len bytes:   %ld\\n\", total_len);\n\t\tpr_info(\" recorded size bytes:   %ld\\n\", total_size);\n\t\tif (total_lost) {\n\t\t\tpr_info(\" With dropped events, record len and size may not match\\n\"\n\t\t\t\t\" alloced and written from above\\n\");\n\t\t} else {\n\t\t\tif (RB_WARN_ON(buffer, total_len != total_alloc ||\n\t\t\t\t       total_size != total_written))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (RB_WARN_ON(buffer, total_lost + total_read != total_events))\n\t\t\tbreak;\n\n\t\tret = 0;\n\t}\n\tif (!ret)\n\t\tpr_info(\"Ring buffer PASSED!\\n\");\n\n\tring_buffer_free(buffer);\n\treturn 0;\n}\n\nlate_initcall(test_ringbuffer);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}