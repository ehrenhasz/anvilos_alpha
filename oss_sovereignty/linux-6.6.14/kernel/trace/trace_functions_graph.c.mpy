{
  "module_name": "trace_functions_graph.c",
  "hash_id": "099dc038481414ce5dfa985a8766e8ada8829a21a01fc76ca2b8ba8bca6a39db",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace_functions_graph.c",
  "human_readable_source": "\n \n#include <linux/uaccess.h>\n#include <linux/ftrace.h>\n#include <linux/interrupt.h>\n#include <linux/slab.h>\n#include <linux/fs.h>\n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n \nstatic int ftrace_graph_skip_irqs;\n\nstruct fgraph_cpu_data {\n\tpid_t\t\tlast_pid;\n\tint\t\tdepth;\n\tint\t\tdepth_irq;\n\tint\t\tignore;\n\tunsigned long\tenter_funcs[FTRACE_RETFUNC_DEPTH];\n};\n\nstruct fgraph_data {\n\tstruct fgraph_cpu_data __percpu *cpu_data;\n\n\t \n\tstruct ftrace_graph_ent_entry\tent;\n\tstruct ftrace_graph_ret_entry\tret;\n\tint\t\t\t\tfailed;\n\tint\t\t\t\tcpu;\n};\n\n#define TRACE_GRAPH_INDENT\t2\n\nunsigned int fgraph_max_depth;\n\nstatic struct tracer_opt trace_opts[] = {\n\t \n\t{ TRACER_OPT(funcgraph-overrun, TRACE_GRAPH_PRINT_OVERRUN) },\n\t \n\t{ TRACER_OPT(funcgraph-cpu, TRACE_GRAPH_PRINT_CPU) },\n\t \n\t{ TRACER_OPT(funcgraph-overhead, TRACE_GRAPH_PRINT_OVERHEAD) },\n\t \n\t{ TRACER_OPT(funcgraph-proc, TRACE_GRAPH_PRINT_PROC) },\n\t \n\t{ TRACER_OPT(funcgraph-duration, TRACE_GRAPH_PRINT_DURATION) },\n\t \n\t{ TRACER_OPT(funcgraph-abstime, TRACE_GRAPH_PRINT_ABS_TIME) },\n\t \n\t{ TRACER_OPT(funcgraph-irqs, TRACE_GRAPH_PRINT_IRQS) },\n\t \n\t{ TRACER_OPT(funcgraph-tail, TRACE_GRAPH_PRINT_TAIL) },\n#ifdef CONFIG_FUNCTION_GRAPH_RETVAL\n\t \n\t{ TRACER_OPT(funcgraph-retval, TRACE_GRAPH_PRINT_RETVAL) },\n\t \n\t{ TRACER_OPT(funcgraph-retval-hex, TRACE_GRAPH_PRINT_RETVAL_HEX) },\n#endif\n\t \n\t{ TRACER_OPT(sleep-time, TRACE_GRAPH_SLEEP_TIME) },\n\n#ifdef CONFIG_FUNCTION_PROFILER\n\t \n\t{ TRACER_OPT(graph-time, TRACE_GRAPH_GRAPH_TIME) },\n#endif\n\n\t{ }  \n};\n\nstatic struct tracer_flags tracer_flags = {\n\t \n\t.val = TRACE_GRAPH_PRINT_CPU | TRACE_GRAPH_PRINT_OVERHEAD |\n\t       TRACE_GRAPH_PRINT_DURATION | TRACE_GRAPH_PRINT_IRQS |\n\t       TRACE_GRAPH_SLEEP_TIME | TRACE_GRAPH_GRAPH_TIME,\n\t.opts = trace_opts\n};\n\nstatic struct trace_array *graph_array;\n\n \nenum {\n\tFLAGS_FILL_FULL  = 1 << TRACE_GRAPH_PRINT_FILL_SHIFT,\n\tFLAGS_FILL_START = 2 << TRACE_GRAPH_PRINT_FILL_SHIFT,\n\tFLAGS_FILL_END   = 3 << TRACE_GRAPH_PRINT_FILL_SHIFT,\n};\n\nstatic void\nprint_graph_duration(struct trace_array *tr, unsigned long long duration,\n\t\t     struct trace_seq *s, u32 flags);\n\nint __trace_graph_entry(struct trace_array *tr,\n\t\t\t\tstruct ftrace_graph_ent *trace,\n\t\t\t\tunsigned int trace_ctx)\n{\n\tstruct trace_event_call *call = &event_funcgraph_entry;\n\tstruct ring_buffer_event *event;\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\tstruct ftrace_graph_ent_entry *entry;\n\n\tevent = trace_buffer_lock_reserve(buffer, TRACE_GRAPH_ENT,\n\t\t\t\t\t  sizeof(*entry), trace_ctx);\n\tif (!event)\n\t\treturn 0;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->graph_ent\t\t\t= *trace;\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\ttrace_buffer_unlock_commit_nostack(buffer, event);\n\n\treturn 1;\n}\n\nstatic inline int ftrace_graph_ignore_irqs(void)\n{\n\tif (!ftrace_graph_skip_irqs || trace_recursion_test(TRACE_IRQ_BIT))\n\t\treturn 0;\n\n\treturn in_hardirq();\n}\n\nint trace_graph_entry(struct ftrace_graph_ent *trace)\n{\n\tstruct trace_array *tr = graph_array;\n\tstruct trace_array_cpu *data;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\tlong disabled;\n\tint ret;\n\tint cpu;\n\n\tif (trace_recursion_test(TRACE_GRAPH_NOTRACE_BIT))\n\t\treturn 0;\n\n\t \n\tif (ftrace_graph_notrace_addr(trace->func)) {\n\t\ttrace_recursion_set(TRACE_GRAPH_NOTRACE_BIT);\n\t\t \n\t\treturn 1;\n\t}\n\n\tif (!ftrace_trace_task(tr))\n\t\treturn 0;\n\n\tif (ftrace_graph_ignore_func(trace))\n\t\treturn 0;\n\n\tif (ftrace_graph_ignore_irqs())\n\t\treturn 0;\n\n\t \n\tif (tracing_thresh)\n\t\treturn 1;\n\n\tlocal_irq_save(flags);\n\tcpu = raw_smp_processor_id();\n\tdata = per_cpu_ptr(tr->array_buffer.data, cpu);\n\tdisabled = atomic_inc_return(&data->disabled);\n\tif (likely(disabled == 1)) {\n\t\ttrace_ctx = tracing_gen_ctx_flags(flags);\n\t\tret = __trace_graph_entry(tr, trace, trace_ctx);\n\t} else {\n\t\tret = 0;\n\t}\n\n\tatomic_dec(&data->disabled);\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic void\n__trace_graph_function(struct trace_array *tr,\n\t\tunsigned long ip, unsigned int trace_ctx)\n{\n\tu64 time = trace_clock_local();\n\tstruct ftrace_graph_ent ent = {\n\t\t.func  = ip,\n\t\t.depth = 0,\n\t};\n\tstruct ftrace_graph_ret ret = {\n\t\t.func     = ip,\n\t\t.depth    = 0,\n\t\t.calltime = time,\n\t\t.rettime  = time,\n\t};\n\n\t__trace_graph_entry(tr, &ent, trace_ctx);\n\t__trace_graph_return(tr, &ret, trace_ctx);\n}\n\nvoid\ntrace_graph_function(struct trace_array *tr,\n\t\tunsigned long ip, unsigned long parent_ip,\n\t\tunsigned int trace_ctx)\n{\n\t__trace_graph_function(tr, ip, trace_ctx);\n}\n\nvoid __trace_graph_return(struct trace_array *tr,\n\t\t\t\tstruct ftrace_graph_ret *trace,\n\t\t\t\tunsigned int trace_ctx)\n{\n\tstruct trace_event_call *call = &event_funcgraph_exit;\n\tstruct ring_buffer_event *event;\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\tstruct ftrace_graph_ret_entry *entry;\n\n\tevent = trace_buffer_lock_reserve(buffer, TRACE_GRAPH_RET,\n\t\t\t\t\t  sizeof(*entry), trace_ctx);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ret\t\t\t\t= *trace;\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\ttrace_buffer_unlock_commit_nostack(buffer, event);\n}\n\nvoid trace_graph_return(struct ftrace_graph_ret *trace)\n{\n\tstruct trace_array *tr = graph_array;\n\tstruct trace_array_cpu *data;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\tlong disabled;\n\tint cpu;\n\n\tftrace_graph_addr_finish(trace);\n\n\tif (trace_recursion_test(TRACE_GRAPH_NOTRACE_BIT)) {\n\t\ttrace_recursion_clear(TRACE_GRAPH_NOTRACE_BIT);\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\tcpu = raw_smp_processor_id();\n\tdata = per_cpu_ptr(tr->array_buffer.data, cpu);\n\tdisabled = atomic_inc_return(&data->disabled);\n\tif (likely(disabled == 1)) {\n\t\ttrace_ctx = tracing_gen_ctx_flags(flags);\n\t\t__trace_graph_return(tr, trace, trace_ctx);\n\t}\n\tatomic_dec(&data->disabled);\n\tlocal_irq_restore(flags);\n}\n\nvoid set_graph_array(struct trace_array *tr)\n{\n\tgraph_array = tr;\n\n\t \n\n\tsmp_mb();\n}\n\nstatic void trace_graph_thresh_return(struct ftrace_graph_ret *trace)\n{\n\tftrace_graph_addr_finish(trace);\n\n\tif (trace_recursion_test(TRACE_GRAPH_NOTRACE_BIT)) {\n\t\ttrace_recursion_clear(TRACE_GRAPH_NOTRACE_BIT);\n\t\treturn;\n\t}\n\n\tif (tracing_thresh &&\n\t    (trace->rettime - trace->calltime < tracing_thresh))\n\t\treturn;\n\telse\n\t\ttrace_graph_return(trace);\n}\n\nstatic struct fgraph_ops funcgraph_thresh_ops = {\n\t.entryfunc = &trace_graph_entry,\n\t.retfunc = &trace_graph_thresh_return,\n};\n\nstatic struct fgraph_ops funcgraph_ops = {\n\t.entryfunc = &trace_graph_entry,\n\t.retfunc = &trace_graph_return,\n};\n\nstatic int graph_trace_init(struct trace_array *tr)\n{\n\tint ret;\n\n\tset_graph_array(tr);\n\tif (tracing_thresh)\n\t\tret = register_ftrace_graph(&funcgraph_thresh_ops);\n\telse\n\t\tret = register_ftrace_graph(&funcgraph_ops);\n\tif (ret)\n\t\treturn ret;\n\ttracing_start_cmdline_record();\n\n\treturn 0;\n}\n\nstatic void graph_trace_reset(struct trace_array *tr)\n{\n\ttracing_stop_cmdline_record();\n\tif (tracing_thresh)\n\t\tunregister_ftrace_graph(&funcgraph_thresh_ops);\n\telse\n\t\tunregister_ftrace_graph(&funcgraph_ops);\n}\n\nstatic int graph_trace_update_thresh(struct trace_array *tr)\n{\n\tgraph_trace_reset(tr);\n\treturn graph_trace_init(tr);\n}\n\nstatic int max_bytes_for_cpu;\n\nstatic void print_graph_cpu(struct trace_seq *s, int cpu)\n{\n\t \n\ttrace_seq_printf(s, \" %*d) \", max_bytes_for_cpu, cpu);\n}\n\n#define TRACE_GRAPH_PROCINFO_LENGTH\t14\n\nstatic void print_graph_proc(struct trace_seq *s, pid_t pid)\n{\n\tchar comm[TASK_COMM_LEN];\n\t \n\tchar pid_str[11];\n\tint spaces = 0;\n\tint len;\n\tint i;\n\n\ttrace_find_cmdline(pid, comm);\n\tcomm[7] = '\\0';\n\tsprintf(pid_str, \"%d\", pid);\n\n\t \n\tlen = strlen(comm) + strlen(pid_str) + 1;\n\n\tif (len < TRACE_GRAPH_PROCINFO_LENGTH)\n\t\tspaces = TRACE_GRAPH_PROCINFO_LENGTH - len;\n\n\t \n\tfor (i = 0; i < spaces / 2; i++)\n\t\ttrace_seq_putc(s, ' ');\n\n\ttrace_seq_printf(s, \"%s-%s\", comm, pid_str);\n\n\t \n\tfor (i = 0; i < spaces - (spaces / 2); i++)\n\t\ttrace_seq_putc(s, ' ');\n}\n\n\nstatic void print_graph_lat_fmt(struct trace_seq *s, struct trace_entry *entry)\n{\n\ttrace_seq_putc(s, ' ');\n\ttrace_print_lat_fmt(s, entry);\n\ttrace_seq_puts(s, \" | \");\n}\n\n \nstatic void\nverif_pid(struct trace_seq *s, pid_t pid, int cpu, struct fgraph_data *data)\n{\n\tpid_t prev_pid;\n\tpid_t *last_pid;\n\n\tif (!data)\n\t\treturn;\n\n\tlast_pid = &(per_cpu_ptr(data->cpu_data, cpu)->last_pid);\n\n\tif (*last_pid == pid)\n\t\treturn;\n\n\tprev_pid = *last_pid;\n\t*last_pid = pid;\n\n\tif (prev_pid == -1)\n\t\treturn;\n \n\ttrace_seq_puts(s, \" ------------------------------------------\\n\");\n\tprint_graph_cpu(s, cpu);\n\tprint_graph_proc(s, prev_pid);\n\ttrace_seq_puts(s, \" => \");\n\tprint_graph_proc(s, pid);\n\ttrace_seq_puts(s, \"\\n ------------------------------------------\\n\\n\");\n}\n\nstatic struct ftrace_graph_ret_entry *\nget_return_for_leaf(struct trace_iterator *iter,\n\t\tstruct ftrace_graph_ent_entry *curr)\n{\n\tstruct fgraph_data *data = iter->private;\n\tstruct ring_buffer_iter *ring_iter = NULL;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_graph_ret_entry *next;\n\n\t \n\tif (data && data->failed) {\n\t\tcurr = &data->ent;\n\t\tnext = &data->ret;\n\t} else {\n\n\t\tring_iter = trace_buffer_iter(iter, iter->cpu);\n\n\t\t \n\t\tif (ring_iter)\n\t\t\tevent = ring_buffer_iter_peek(ring_iter, NULL);\n\t\telse {\n\t\t\t \n\t\t\tring_buffer_consume(iter->array_buffer->buffer, iter->cpu,\n\t\t\t\t\t    NULL, NULL);\n\t\t\tevent = ring_buffer_peek(iter->array_buffer->buffer, iter->cpu,\n\t\t\t\t\t\t NULL, NULL);\n\t\t}\n\n\t\tif (!event)\n\t\t\treturn NULL;\n\n\t\tnext = ring_buffer_event_data(event);\n\n\t\tif (data) {\n\t\t\t \n\t\t\tdata->ent = *curr;\n\t\t\t \n\t\t\tif (next->ent.type == TRACE_GRAPH_RET)\n\t\t\t\tdata->ret = *next;\n\t\t\telse\n\t\t\t\tdata->ret.ent.type = next->ent.type;\n\t\t}\n\t}\n\n\tif (next->ent.type != TRACE_GRAPH_RET)\n\t\treturn NULL;\n\n\tif (curr->ent.pid != next->ent.pid ||\n\t\t\tcurr->graph_ent.func != next->ret.func)\n\t\treturn NULL;\n\n\t \n\tif (ring_iter)\n\t\tring_buffer_iter_advance(ring_iter);\n\n\treturn next;\n}\n\nstatic void print_graph_abs_time(u64 t, struct trace_seq *s)\n{\n\tunsigned long usecs_rem;\n\n\tusecs_rem = do_div(t, NSEC_PER_SEC);\n\tusecs_rem /= 1000;\n\n\ttrace_seq_printf(s, \"%5lu.%06lu |  \",\n\t\t\t (unsigned long)t, usecs_rem);\n}\n\nstatic void\nprint_graph_rel_time(struct trace_iterator *iter, struct trace_seq *s)\n{\n\tunsigned long long usecs;\n\n\tusecs = iter->ts - iter->array_buffer->time_start;\n\tdo_div(usecs, NSEC_PER_USEC);\n\n\ttrace_seq_printf(s, \"%9llu us |  \", usecs);\n}\n\nstatic void\nprint_graph_irq(struct trace_iterator *iter, unsigned long addr,\n\t\tenum trace_type type, int cpu, pid_t pid, u32 flags)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *ent = iter->ent;\n\n\tif (addr < (unsigned long)__irqentry_text_start ||\n\t\taddr >= (unsigned long)__irqentry_text_end)\n\t\treturn;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\t \n\t\tif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\n\t\t\tprint_graph_abs_time(iter->ts, s);\n\n\t\t \n\t\tif (flags & TRACE_GRAPH_PRINT_REL_TIME)\n\t\t\tprint_graph_rel_time(iter, s);\n\n\t\t \n\t\tif (flags & TRACE_GRAPH_PRINT_CPU)\n\t\t\tprint_graph_cpu(s, cpu);\n\n\t\t \n\t\tif (flags & TRACE_GRAPH_PRINT_PROC) {\n\t\t\tprint_graph_proc(s, pid);\n\t\t\ttrace_seq_puts(s, \" | \");\n\t\t}\n\n\t\t \n\t\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\t\tprint_graph_lat_fmt(s, ent);\n\t}\n\n\t \n\tprint_graph_duration(tr, 0, s, flags | FLAGS_FILL_START);\n\n\tif (type == TRACE_GRAPH_ENT)\n\t\ttrace_seq_puts(s, \"==========>\");\n\telse\n\t\ttrace_seq_puts(s, \"<==========\");\n\n\tprint_graph_duration(tr, 0, s, flags | FLAGS_FILL_END);\n\ttrace_seq_putc(s, '\\n');\n}\n\nvoid\ntrace_print_graph_duration(unsigned long long duration, struct trace_seq *s)\n{\n\tunsigned long nsecs_rem = do_div(duration, 1000);\n\t \n\tchar usecs_str[21];\n\tchar nsecs_str[5];\n\tint len;\n\tint i;\n\n\tsprintf(usecs_str, \"%lu\", (unsigned long) duration);\n\n\t \n\ttrace_seq_printf(s, \"%s\", usecs_str);\n\n\tlen = strlen(usecs_str);\n\n\t \n\tif (len < 7) {\n\t\tsize_t slen = min_t(size_t, sizeof(nsecs_str), 8UL - len);\n\n\t\tsnprintf(nsecs_str, slen, \"%03lu\", nsecs_rem);\n\t\ttrace_seq_printf(s, \".%s\", nsecs_str);\n\t\tlen += strlen(nsecs_str) + 1;\n\t}\n\n\ttrace_seq_puts(s, \" us \");\n\n\t \n\tfor (i = len; i < 8; i++)\n\t\ttrace_seq_putc(s, ' ');\n}\n\nstatic void\nprint_graph_duration(struct trace_array *tr, unsigned long long duration,\n\t\t     struct trace_seq *s, u32 flags)\n{\n\tif (!(flags & TRACE_GRAPH_PRINT_DURATION) ||\n\t    !(tr->trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\t \n\tswitch (flags & TRACE_GRAPH_PRINT_FILL_MASK) {\n\tcase FLAGS_FILL_FULL:\n\t\ttrace_seq_puts(s, \"              |  \");\n\t\treturn;\n\tcase FLAGS_FILL_START:\n\t\ttrace_seq_puts(s, \"  \");\n\t\treturn;\n\tcase FLAGS_FILL_END:\n\t\ttrace_seq_puts(s, \" |\");\n\t\treturn;\n\t}\n\n\t \n\tif (flags & TRACE_GRAPH_PRINT_OVERHEAD)\n\t\ttrace_seq_printf(s, \"%c \", trace_find_mark(duration));\n\telse\n\t\ttrace_seq_puts(s, \"  \");\n\n\ttrace_print_graph_duration(duration, s);\n\ttrace_seq_puts(s, \"|  \");\n}\n\n#ifdef CONFIG_FUNCTION_GRAPH_RETVAL\n\n#define __TRACE_GRAPH_PRINT_RETVAL TRACE_GRAPH_PRINT_RETVAL\n\nstatic void print_graph_retval(struct trace_seq *s, unsigned long retval,\n\t\t\t\tbool leaf, void *func, bool hex_format)\n{\n\tunsigned long err_code = 0;\n\n\tif (retval == 0 || hex_format)\n\t\tgoto done;\n\n\t \n\tif (IS_ENABLED(CONFIG_64BIT) && (retval & BIT(31)) &&\n\t\t(((u64)retval) >> 32) == 0) {\n\t\t \n\t\terr_code = (unsigned long)(s32)retval;\n\t} else {\n\t\terr_code = retval;\n\t}\n\n\tif (!IS_ERR_VALUE(err_code))\n\t\terr_code = 0;\n\ndone:\n\tif (leaf) {\n\t\tif (hex_format || (err_code == 0))\n\t\t\ttrace_seq_printf(s, \"%ps(); /* = 0x%lx */\\n\",\n\t\t\t\t\tfunc, retval);\n\t\telse\n\t\t\ttrace_seq_printf(s, \"%ps(); /* = %ld */\\n\",\n\t\t\t\t\tfunc, err_code);\n\t} else {\n\t\tif (hex_format || (err_code == 0))\n\t\t\ttrace_seq_printf(s, \"} /* %ps = 0x%lx */\\n\",\n\t\t\t\t\tfunc, retval);\n\t\telse\n\t\t\ttrace_seq_printf(s, \"} /* %ps = %ld */\\n\",\n\t\t\t\t\tfunc, err_code);\n\t}\n}\n\n#else\n\n#define __TRACE_GRAPH_PRINT_RETVAL 0\n\n#define print_graph_retval(_seq, _retval, _leaf, _func, _format) do {} while (0)\n\n#endif\n\n \nstatic enum print_line_t\nprint_graph_entry_leaf(struct trace_iterator *iter,\n\t\tstruct ftrace_graph_ent_entry *entry,\n\t\tstruct ftrace_graph_ret_entry *ret_entry,\n\t\tstruct trace_seq *s, u32 flags)\n{\n\tstruct fgraph_data *data = iter->private;\n\tstruct trace_array *tr = iter->tr;\n\tstruct ftrace_graph_ret *graph_ret;\n\tstruct ftrace_graph_ent *call;\n\tunsigned long long duration;\n\tint cpu = iter->cpu;\n\tint i;\n\n\tgraph_ret = &ret_entry->ret;\n\tcall = &entry->graph_ent;\n\tduration = graph_ret->rettime - graph_ret->calltime;\n\n\tif (data) {\n\t\tstruct fgraph_cpu_data *cpu_data;\n\n\t\tcpu_data = per_cpu_ptr(data->cpu_data, cpu);\n\n\t\t \n\t\tcpu_data->depth = call->depth - 1;\n\n\t\t \n\t\tif (call->depth < FTRACE_RETFUNC_DEPTH &&\n\t\t    !WARN_ON_ONCE(call->depth < 0))\n\t\t\tcpu_data->enter_funcs[call->depth] = 0;\n\t}\n\n\t \n\tprint_graph_duration(tr, duration, s, flags);\n\n\t \n\tfor (i = 0; i < call->depth * TRACE_GRAPH_INDENT; i++)\n\t\ttrace_seq_putc(s, ' ');\n\n\t \n\tif (flags & __TRACE_GRAPH_PRINT_RETVAL)\n\t\tprint_graph_retval(s, graph_ret->retval, true, (void *)call->func,\n\t\t\t\t!!(flags & TRACE_GRAPH_PRINT_RETVAL_HEX));\n\telse\n\t\ttrace_seq_printf(s, \"%ps();\\n\", (void *)call->func);\n\n\tprint_graph_irq(iter, graph_ret->func, TRACE_GRAPH_RET,\n\t\t\tcpu, iter->ent->pid, flags);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t\nprint_graph_entry_nested(struct trace_iterator *iter,\n\t\t\t struct ftrace_graph_ent_entry *entry,\n\t\t\t struct trace_seq *s, int cpu, u32 flags)\n{\n\tstruct ftrace_graph_ent *call = &entry->graph_ent;\n\tstruct fgraph_data *data = iter->private;\n\tstruct trace_array *tr = iter->tr;\n\tint i;\n\n\tif (data) {\n\t\tstruct fgraph_cpu_data *cpu_data;\n\t\tint cpu = iter->cpu;\n\n\t\tcpu_data = per_cpu_ptr(data->cpu_data, cpu);\n\t\tcpu_data->depth = call->depth;\n\n\t\t \n\t\tif (call->depth < FTRACE_RETFUNC_DEPTH &&\n\t\t    !WARN_ON_ONCE(call->depth < 0))\n\t\t\tcpu_data->enter_funcs[call->depth] = call->func;\n\t}\n\n\t \n\tprint_graph_duration(tr, 0, s, flags | FLAGS_FILL_FULL);\n\n\t \n\tfor (i = 0; i < call->depth * TRACE_GRAPH_INDENT; i++)\n\t\ttrace_seq_putc(s, ' ');\n\n\ttrace_seq_printf(s, \"%ps() {\\n\", (void *)call->func);\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\t \n\treturn TRACE_TYPE_NO_CONSUME;\n}\n\nstatic void\nprint_graph_prologue(struct trace_iterator *iter, struct trace_seq *s,\n\t\t     int type, unsigned long addr, u32 flags)\n{\n\tstruct fgraph_data *data = iter->private;\n\tstruct trace_entry *ent = iter->ent;\n\tstruct trace_array *tr = iter->tr;\n\tint cpu = iter->cpu;\n\n\t \n\tverif_pid(s, ent->pid, cpu, data);\n\n\tif (type)\n\t\t \n\t\tprint_graph_irq(iter, addr, type, cpu, ent->pid, flags);\n\n\tif (!(tr->trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\t \n\tif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\n\t\tprint_graph_abs_time(iter->ts, s);\n\n\t \n\tif (flags & TRACE_GRAPH_PRINT_REL_TIME)\n\t\tprint_graph_rel_time(iter, s);\n\n\t \n\tif (flags & TRACE_GRAPH_PRINT_CPU)\n\t\tprint_graph_cpu(s, cpu);\n\n\t \n\tif (flags & TRACE_GRAPH_PRINT_PROC) {\n\t\tprint_graph_proc(s, ent->pid);\n\t\ttrace_seq_puts(s, \" | \");\n\t}\n\n\t \n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\tprint_graph_lat_fmt(s, ent);\n\n\treturn;\n}\n\n \nstatic int\ncheck_irq_entry(struct trace_iterator *iter, u32 flags,\n\t\tunsigned long addr, int depth)\n{\n\tint cpu = iter->cpu;\n\tint *depth_irq;\n\tstruct fgraph_data *data = iter->private;\n\n\t \n\tif ((flags & TRACE_GRAPH_PRINT_IRQS) ||\n\t    (!data))\n\t\treturn 0;\n\n\tdepth_irq = &(per_cpu_ptr(data->cpu_data, cpu)->depth_irq);\n\n\t \n\tif (*depth_irq >= 0)\n\t\treturn 1;\n\n\tif ((addr < (unsigned long)__irqentry_text_start) ||\n\t    (addr >= (unsigned long)__irqentry_text_end))\n\t\treturn 0;\n\n\t \n\t*depth_irq = depth;\n\treturn 1;\n}\n\n \nstatic int\ncheck_irq_return(struct trace_iterator *iter, u32 flags, int depth)\n{\n\tint cpu = iter->cpu;\n\tint *depth_irq;\n\tstruct fgraph_data *data = iter->private;\n\n\t \n\tif ((flags & TRACE_GRAPH_PRINT_IRQS) ||\n\t    (!data))\n\t\treturn 0;\n\n\tdepth_irq = &(per_cpu_ptr(data->cpu_data, cpu)->depth_irq);\n\n\t \n\tif (*depth_irq == -1)\n\t\treturn 0;\n\n\t \n\tif (*depth_irq >= depth) {\n\t\t*depth_irq = -1;\n\t\treturn 1;\n\t}\n\n\t \n\treturn 1;\n}\n\nstatic enum print_line_t\nprint_graph_entry(struct ftrace_graph_ent_entry *field, struct trace_seq *s,\n\t\t\tstruct trace_iterator *iter, u32 flags)\n{\n\tstruct fgraph_data *data = iter->private;\n\tstruct ftrace_graph_ent *call = &field->graph_ent;\n\tstruct ftrace_graph_ret_entry *leaf_ret;\n\tstatic enum print_line_t ret;\n\tint cpu = iter->cpu;\n\n\tif (check_irq_entry(iter, flags, call->func, call->depth))\n\t\treturn TRACE_TYPE_HANDLED;\n\n\tprint_graph_prologue(iter, s, TRACE_GRAPH_ENT, call->func, flags);\n\n\tleaf_ret = get_return_for_leaf(iter, field);\n\tif (leaf_ret)\n\t\tret = print_graph_entry_leaf(iter, field, leaf_ret, s, flags);\n\telse\n\t\tret = print_graph_entry_nested(iter, field, s, cpu, flags);\n\n\tif (data) {\n\t\t \n\t\tif (s->full) {\n\t\t\tdata->failed = 1;\n\t\t\tdata->cpu = cpu;\n\t\t} else\n\t\t\tdata->failed = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic enum print_line_t\nprint_graph_return(struct ftrace_graph_ret *trace, struct trace_seq *s,\n\t\t   struct trace_entry *ent, struct trace_iterator *iter,\n\t\t   u32 flags)\n{\n\tunsigned long long duration = trace->rettime - trace->calltime;\n\tstruct fgraph_data *data = iter->private;\n\tstruct trace_array *tr = iter->tr;\n\tpid_t pid = ent->pid;\n\tint cpu = iter->cpu;\n\tint func_match = 1;\n\tint i;\n\n\tif (check_irq_return(iter, flags, trace->depth))\n\t\treturn TRACE_TYPE_HANDLED;\n\n\tif (data) {\n\t\tstruct fgraph_cpu_data *cpu_data;\n\t\tint cpu = iter->cpu;\n\n\t\tcpu_data = per_cpu_ptr(data->cpu_data, cpu);\n\n\t\t \n\t\tcpu_data->depth = trace->depth - 1;\n\n\t\tif (trace->depth < FTRACE_RETFUNC_DEPTH &&\n\t\t    !WARN_ON_ONCE(trace->depth < 0)) {\n\t\t\tif (cpu_data->enter_funcs[trace->depth] != trace->func)\n\t\t\t\tfunc_match = 0;\n\t\t\tcpu_data->enter_funcs[trace->depth] = 0;\n\t\t}\n\t}\n\n\tprint_graph_prologue(iter, s, 0, 0, flags);\n\n\t \n\tprint_graph_duration(tr, duration, s, flags);\n\n\t \n\tfor (i = 0; i < trace->depth * TRACE_GRAPH_INDENT; i++)\n\t\ttrace_seq_putc(s, ' ');\n\n\t \n\tif (flags & __TRACE_GRAPH_PRINT_RETVAL) {\n\t\tprint_graph_retval(s, trace->retval, false, (void *)trace->func,\n\t\t\t!!(flags & TRACE_GRAPH_PRINT_RETVAL_HEX));\n\t} else {\n\t\t \n\t\tif (func_match && !(flags & TRACE_GRAPH_PRINT_TAIL))\n\t\t\ttrace_seq_puts(s, \"}\\n\");\n\t\telse\n\t\t\ttrace_seq_printf(s, \"} /* %ps */\\n\", (void *)trace->func);\n\t}\n\n\t \n\tif (flags & TRACE_GRAPH_PRINT_OVERRUN)\n\t\ttrace_seq_printf(s, \" (Overruns: %u)\\n\",\n\t\t\t\t trace->overrun);\n\n\tprint_graph_irq(iter, trace->func, TRACE_GRAPH_RET,\n\t\t\tcpu, pid, flags);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t\nprint_graph_comment(struct trace_seq *s, struct trace_entry *ent,\n\t\t    struct trace_iterator *iter, u32 flags)\n{\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long sym_flags = (tr->trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct fgraph_data *data = iter->private;\n\tstruct trace_event *event;\n\tint depth = 0;\n\tint ret;\n\tint i;\n\n\tif (data)\n\t\tdepth = per_cpu_ptr(data->cpu_data, iter->cpu)->depth;\n\n\tprint_graph_prologue(iter, s, 0, 0, flags);\n\n\t \n\tprint_graph_duration(tr, 0, s, flags | FLAGS_FILL_FULL);\n\n\t \n\tif (depth > 0)\n\t\tfor (i = 0; i < (depth + 1) * TRACE_GRAPH_INDENT; i++)\n\t\t\ttrace_seq_putc(s, ' ');\n\n\t \n\ttrace_seq_puts(s, \"/* \");\n\n\tswitch (iter->ent->type) {\n\tcase TRACE_BPUTS:\n\t\tret = trace_print_bputs_msg_only(iter);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase TRACE_BPRINT:\n\t\tret = trace_print_bprintk_msg_only(iter);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t\tbreak;\n\tcase TRACE_PRINT:\n\t\tret = trace_print_printk_msg_only(iter);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t\tbreak;\n\tdefault:\n\t\tevent = ftrace_find_event(ent->type);\n\t\tif (!event)\n\t\t\treturn TRACE_TYPE_UNHANDLED;\n\n\t\tret = event->funcs->trace(iter, sym_flags, event);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t}\n\n\tif (trace_seq_has_overflowed(s))\n\t\tgoto out;\n\n\t \n\tif (s->buffer[s->seq.len - 1] == '\\n') {\n\t\ts->buffer[s->seq.len - 1] = '\\0';\n\t\ts->seq.len--;\n\t}\n\n\ttrace_seq_puts(s, \" */\\n\");\n out:\n\treturn trace_handle_return(s);\n}\n\n\nenum print_line_t\nprint_graph_function_flags(struct trace_iterator *iter, u32 flags)\n{\n\tstruct ftrace_graph_ent_entry *field;\n\tstruct fgraph_data *data = iter->private;\n\tstruct trace_entry *entry = iter->ent;\n\tstruct trace_seq *s = &iter->seq;\n\tint cpu = iter->cpu;\n\tint ret;\n\n\tif (data && per_cpu_ptr(data->cpu_data, cpu)->ignore) {\n\t\tper_cpu_ptr(data->cpu_data, cpu)->ignore = 0;\n\t\treturn TRACE_TYPE_HANDLED;\n\t}\n\n\t \n\tif (data && data->failed) {\n\t\tfield = &data->ent;\n\t\titer->cpu = data->cpu;\n\t\tret = print_graph_entry(field, s, iter, flags);\n\t\tif (ret == TRACE_TYPE_HANDLED && iter->cpu != cpu) {\n\t\t\tper_cpu_ptr(data->cpu_data, iter->cpu)->ignore = 1;\n\t\t\tret = TRACE_TYPE_NO_CONSUME;\n\t\t}\n\t\titer->cpu = cpu;\n\t\treturn ret;\n\t}\n\n\tswitch (entry->type) {\n\tcase TRACE_GRAPH_ENT: {\n\t\t \n\t\tstruct ftrace_graph_ent_entry saved;\n\t\ttrace_assign_type(field, entry);\n\t\tsaved = *field;\n\t\treturn print_graph_entry(&saved, s, iter, flags);\n\t}\n\tcase TRACE_GRAPH_RET: {\n\t\tstruct ftrace_graph_ret_entry *field;\n\t\ttrace_assign_type(field, entry);\n\t\treturn print_graph_return(&field->ret, s, entry, iter, flags);\n\t}\n\tcase TRACE_STACK:\n\tcase TRACE_FN:\n\t\t \n\t\treturn TRACE_TYPE_UNHANDLED;\n\n\tdefault:\n\t\treturn print_graph_comment(s, entry, iter, flags);\n\t}\n\n\treturn TRACE_TYPE_HANDLED;\n}\n\nstatic enum print_line_t\nprint_graph_function(struct trace_iterator *iter)\n{\n\treturn print_graph_function_flags(iter, tracer_flags.val);\n}\n\nstatic enum print_line_t\nprint_graph_function_event(struct trace_iterator *iter, int flags,\n\t\t\t   struct trace_event *event)\n{\n\treturn print_graph_function(iter);\n}\n\nstatic void print_lat_header(struct seq_file *s, u32 flags)\n{\n\tstatic const char spaces[] = \"                \"\t \n\t\t\"    \"\t\t\t\t\t \n\t\t\"                 \";\t\t\t \n\tint size = 0;\n\n\tif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\n\t\tsize += 16;\n\tif (flags & TRACE_GRAPH_PRINT_REL_TIME)\n\t\tsize += 16;\n\tif (flags & TRACE_GRAPH_PRINT_CPU)\n\t\tsize += 4;\n\tif (flags & TRACE_GRAPH_PRINT_PROC)\n\t\tsize += 17;\n\n\tseq_printf(s, \"#%.*s  _-----=> irqs-off        \\n\", size, spaces);\n\tseq_printf(s, \"#%.*s / _----=> need-resched    \\n\", size, spaces);\n\tseq_printf(s, \"#%.*s| / _---=> hardirq/softirq \\n\", size, spaces);\n\tseq_printf(s, \"#%.*s|| / _--=> preempt-depth   \\n\", size, spaces);\n\tseq_printf(s, \"#%.*s||| /                      \\n\", size, spaces);\n}\n\nstatic void __print_graph_headers_flags(struct trace_array *tr,\n\t\t\t\t\tstruct seq_file *s, u32 flags)\n{\n\tint lat = tr->trace_flags & TRACE_ITER_LATENCY_FMT;\n\n\tif (lat)\n\t\tprint_lat_header(s, flags);\n\n\t \n\tseq_putc(s, '#');\n\tif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\n\t\tseq_puts(s, \"     TIME       \");\n\tif (flags & TRACE_GRAPH_PRINT_REL_TIME)\n\t\tseq_puts(s, \"   REL TIME     \");\n\tif (flags & TRACE_GRAPH_PRINT_CPU)\n\t\tseq_puts(s, \" CPU\");\n\tif (flags & TRACE_GRAPH_PRINT_PROC)\n\t\tseq_puts(s, \"  TASK/PID       \");\n\tif (lat)\n\t\tseq_puts(s, \"||||   \");\n\tif (flags & TRACE_GRAPH_PRINT_DURATION)\n\t\tseq_puts(s, \"  DURATION   \");\n\tseq_puts(s, \"               FUNCTION CALLS\\n\");\n\n\t \n\tseq_putc(s, '#');\n\tif (flags & TRACE_GRAPH_PRINT_ABS_TIME)\n\t\tseq_puts(s, \"      |         \");\n\tif (flags & TRACE_GRAPH_PRINT_REL_TIME)\n\t\tseq_puts(s, \"      |         \");\n\tif (flags & TRACE_GRAPH_PRINT_CPU)\n\t\tseq_puts(s, \" |  \");\n\tif (flags & TRACE_GRAPH_PRINT_PROC)\n\t\tseq_puts(s, \"   |    |        \");\n\tif (lat)\n\t\tseq_puts(s, \"||||   \");\n\tif (flags & TRACE_GRAPH_PRINT_DURATION)\n\t\tseq_puts(s, \"   |   |      \");\n\tseq_puts(s, \"               |   |   |   |\\n\");\n}\n\nstatic void print_graph_headers(struct seq_file *s)\n{\n\tprint_graph_headers_flags(s, tracer_flags.val);\n}\n\nvoid print_graph_headers_flags(struct seq_file *s, u32 flags)\n{\n\tstruct trace_iterator *iter = s->private;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT) {\n\t\t \n\t\tif (trace_empty(iter))\n\t\t\treturn;\n\n\t\tprint_trace_header(s, iter);\n\t}\n\n\t__print_graph_headers_flags(tr, s, flags);\n}\n\nvoid graph_trace_open(struct trace_iterator *iter)\n{\n\t \n\tstruct fgraph_data *data;\n\tgfp_t gfpflags;\n\tint cpu;\n\n\titer->private = NULL;\n\n\t \n\tgfpflags = (in_atomic() || irqs_disabled()) ? GFP_ATOMIC : GFP_KERNEL;\n\n\tdata = kzalloc(sizeof(*data), gfpflags);\n\tif (!data)\n\t\tgoto out_err;\n\n\tdata->cpu_data = alloc_percpu_gfp(struct fgraph_cpu_data, gfpflags);\n\tif (!data->cpu_data)\n\t\tgoto out_err_free;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tpid_t *pid = &(per_cpu_ptr(data->cpu_data, cpu)->last_pid);\n\t\tint *depth = &(per_cpu_ptr(data->cpu_data, cpu)->depth);\n\t\tint *ignore = &(per_cpu_ptr(data->cpu_data, cpu)->ignore);\n\t\tint *depth_irq = &(per_cpu_ptr(data->cpu_data, cpu)->depth_irq);\n\n\t\t*pid = -1;\n\t\t*depth = 0;\n\t\t*ignore = 0;\n\t\t*depth_irq = -1;\n\t}\n\n\titer->private = data;\n\n\treturn;\n\n out_err_free:\n\tkfree(data);\n out_err:\n\tpr_warn(\"function graph tracer: not enough memory\\n\");\n}\n\nvoid graph_trace_close(struct trace_iterator *iter)\n{\n\tstruct fgraph_data *data = iter->private;\n\n\tif (data) {\n\t\tfree_percpu(data->cpu_data);\n\t\tkfree(data);\n\t}\n}\n\nstatic int\nfunc_graph_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\tif (bit == TRACE_GRAPH_PRINT_IRQS)\n\t\tftrace_graph_skip_irqs = !set;\n\n\tif (bit == TRACE_GRAPH_SLEEP_TIME)\n\t\tftrace_graph_sleep_time_control(set);\n\n\tif (bit == TRACE_GRAPH_GRAPH_TIME)\n\t\tftrace_graph_graph_time_control(set);\n\n\treturn 0;\n}\n\nstatic struct trace_event_functions graph_functions = {\n\t.trace\t\t= print_graph_function_event,\n};\n\nstatic struct trace_event graph_trace_entry_event = {\n\t.type\t\t= TRACE_GRAPH_ENT,\n\t.funcs\t\t= &graph_functions,\n};\n\nstatic struct trace_event graph_trace_ret_event = {\n\t.type\t\t= TRACE_GRAPH_RET,\n\t.funcs\t\t= &graph_functions\n};\n\nstatic struct tracer graph_trace __tracer_data = {\n\t.name\t\t= \"function_graph\",\n\t.update_thresh\t= graph_trace_update_thresh,\n\t.open\t\t= graph_trace_open,\n\t.pipe_open\t= graph_trace_open,\n\t.close\t\t= graph_trace_close,\n\t.pipe_close\t= graph_trace_close,\n\t.init\t\t= graph_trace_init,\n\t.reset\t\t= graph_trace_reset,\n\t.print_line\t= print_graph_function,\n\t.print_header\t= print_graph_headers,\n\t.flags\t\t= &tracer_flags,\n\t.set_flag\t= func_graph_set_flag,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest\t= trace_selftest_startup_function_graph,\n#endif\n};\n\n\nstatic ssize_t\ngraph_depth_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t  loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tfgraph_max_depth = val;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ngraph_depth_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t loff_t *ppos)\n{\n\tchar buf[15];  \n\tint n;\n\n\tn = sprintf(buf, \"%d\\n\", fgraph_max_depth);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, n);\n}\n\nstatic const struct file_operations graph_depth_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.write\t\t= graph_depth_write,\n\t.read\t\t= graph_depth_read,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic __init int init_graph_tracefs(void)\n{\n\tint ret;\n\n\tret = tracing_init_dentry();\n\tif (ret)\n\t\treturn 0;\n\n\ttrace_create_file(\"max_graph_depth\", TRACE_MODE_WRITE, NULL,\n\t\t\t  NULL, &graph_depth_fops);\n\n\treturn 0;\n}\nfs_initcall(init_graph_tracefs);\n\nstatic __init int init_graph_trace(void)\n{\n\tmax_bytes_for_cpu = snprintf(NULL, 0, \"%u\", nr_cpu_ids - 1);\n\n\tif (!register_trace_event(&graph_trace_entry_event)) {\n\t\tpr_warn(\"Warning: could not register graph trace events\\n\");\n\t\treturn 1;\n\t}\n\n\tif (!register_trace_event(&graph_trace_ret_event)) {\n\t\tpr_warn(\"Warning: could not register graph trace events\\n\");\n\t\treturn 1;\n\t}\n\n\treturn register_tracer(&graph_trace);\n}\n\ncore_initcall(init_graph_trace);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}