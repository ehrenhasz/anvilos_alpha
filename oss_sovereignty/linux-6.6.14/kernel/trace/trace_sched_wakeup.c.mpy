{
  "module_name": "trace_sched_wakeup.c",
  "hash_id": "7c2fe57be2addfd2e9fb5318df62dfd2785c238740e72bb81d8c577997639285",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace_sched_wakeup.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/kallsyms.h>\n#include <linux/uaccess.h>\n#include <linux/ftrace.h>\n#include <linux/sched/rt.h>\n#include <linux/sched/deadline.h>\n#include <trace/events/sched.h>\n#include \"trace.h\"\n\nstatic struct trace_array\t*wakeup_trace;\nstatic int __read_mostly\ttracer_enabled;\n\nstatic struct task_struct\t*wakeup_task;\nstatic int\t\t\twakeup_cpu;\nstatic int\t\t\twakeup_current_cpu;\nstatic unsigned\t\t\twakeup_prio = -1;\nstatic bool\t\t\twakeup_rt;\nstatic bool\t\t\twakeup_dl;\nstatic bool\t\t\ttracing_dl;\n\nstatic arch_spinlock_t wakeup_lock =\n\t(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\nstatic void wakeup_reset(struct trace_array *tr);\nstatic void __wakeup_reset(struct trace_array *tr);\nstatic int start_func_tracer(struct trace_array *tr, int graph);\nstatic void stop_func_tracer(struct trace_array *tr, int graph);\n\nstatic int save_flags;\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n# define is_graph(tr) ((tr)->trace_flags & TRACE_ITER_DISPLAY_GRAPH)\n#else\n# define is_graph(tr) false\n#endif\n\n#ifdef CONFIG_FUNCTION_TRACER\n\nstatic bool function_enabled;\n\n \nstatic int\nfunc_prolog_preempt_disable(struct trace_array *tr,\n\t\t\t    struct trace_array_cpu **data,\n\t\t\t    unsigned int *trace_ctx)\n{\n\tlong disabled;\n\tint cpu;\n\n\tif (likely(!wakeup_task))\n\t\treturn 0;\n\n\t*trace_ctx = tracing_gen_ctx();\n\tpreempt_disable_notrace();\n\n\tcpu = raw_smp_processor_id();\n\tif (cpu != wakeup_current_cpu)\n\t\tgoto out_enable;\n\n\t*data = per_cpu_ptr(tr->array_buffer.data, cpu);\n\tdisabled = atomic_inc_return(&(*data)->disabled);\n\tif (unlikely(disabled != 1))\n\t\tgoto out;\n\n\treturn 1;\n\nout:\n\tatomic_dec(&(*data)->disabled);\n\nout_enable:\n\tpreempt_enable_notrace();\n\treturn 0;\n}\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\nstatic int wakeup_display_graph(struct trace_array *tr, int set)\n{\n\tif (!(is_graph(tr) ^ set))\n\t\treturn 0;\n\n\tstop_func_tracer(tr, !set);\n\n\twakeup_reset(wakeup_trace);\n\ttr->max_latency = 0;\n\n\treturn start_func_tracer(tr, set);\n}\n\nstatic int wakeup_graph_entry(struct ftrace_graph_ent *trace)\n{\n\tstruct trace_array *tr = wakeup_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned int trace_ctx;\n\tint ret = 0;\n\n\tif (ftrace_graph_ignore_func(trace))\n\t\treturn 0;\n\t \n\tif (ftrace_graph_notrace_addr(trace->func))\n\t\treturn 1;\n\n\tif (!func_prolog_preempt_disable(tr, &data, &trace_ctx))\n\t\treturn 0;\n\n\tret = __trace_graph_entry(tr, trace, trace_ctx);\n\tatomic_dec(&data->disabled);\n\tpreempt_enable_notrace();\n\n\treturn ret;\n}\n\nstatic void wakeup_graph_return(struct ftrace_graph_ret *trace)\n{\n\tstruct trace_array *tr = wakeup_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned int trace_ctx;\n\n\tftrace_graph_addr_finish(trace);\n\n\tif (!func_prolog_preempt_disable(tr, &data, &trace_ctx))\n\t\treturn;\n\n\t__trace_graph_return(tr, trace, trace_ctx);\n\tatomic_dec(&data->disabled);\n\n\tpreempt_enable_notrace();\n\treturn;\n}\n\nstatic struct fgraph_ops fgraph_wakeup_ops = {\n\t.entryfunc = &wakeup_graph_entry,\n\t.retfunc = &wakeup_graph_return,\n};\n\nstatic void wakeup_trace_open(struct trace_iterator *iter)\n{\n\tif (is_graph(iter->tr))\n\t\tgraph_trace_open(iter);\n\telse\n\t\titer->private = NULL;\n}\n\nstatic void wakeup_trace_close(struct trace_iterator *iter)\n{\n\tif (iter->private)\n\t\tgraph_trace_close(iter);\n}\n\n#define GRAPH_TRACER_FLAGS (TRACE_GRAPH_PRINT_PROC | \\\n\t\t\t    TRACE_GRAPH_PRINT_CPU |  \\\n\t\t\t    TRACE_GRAPH_PRINT_REL_TIME | \\\n\t\t\t    TRACE_GRAPH_PRINT_DURATION | \\\n\t\t\t    TRACE_GRAPH_PRINT_OVERHEAD | \\\n\t\t\t    TRACE_GRAPH_PRINT_IRQS)\n\nstatic enum print_line_t wakeup_print_line(struct trace_iterator *iter)\n{\n\t \n\tif (is_graph(iter->tr))\n\t\treturn print_graph_function_flags(iter, GRAPH_TRACER_FLAGS);\n\n\treturn TRACE_TYPE_UNHANDLED;\n}\n\nstatic void wakeup_print_header(struct seq_file *s)\n{\n\tif (is_graph(wakeup_trace))\n\t\tprint_graph_headers_flags(s, GRAPH_TRACER_FLAGS);\n\telse\n\t\ttrace_default_header(s);\n}\n#endif  \n\n \nstatic void\nwakeup_tracer_call(unsigned long ip, unsigned long parent_ip,\n\t\t   struct ftrace_ops *op, struct ftrace_regs *fregs)\n{\n\tstruct trace_array *tr = wakeup_trace;\n\tstruct trace_array_cpu *data;\n\tunsigned long flags;\n\tunsigned int trace_ctx;\n\n\tif (!func_prolog_preempt_disable(tr, &data, &trace_ctx))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\ttrace_function(tr, ip, parent_ip, trace_ctx);\n\tlocal_irq_restore(flags);\n\n\tatomic_dec(&data->disabled);\n\tpreempt_enable_notrace();\n}\n\nstatic int register_wakeup_function(struct trace_array *tr, int graph, int set)\n{\n\tint ret;\n\n\t \n\tif (function_enabled || (!set && !(tr->trace_flags & TRACE_ITER_FUNCTION)))\n\t\treturn 0;\n\n\tif (graph)\n\t\tret = register_ftrace_graph(&fgraph_wakeup_ops);\n\telse\n\t\tret = register_ftrace_function(tr->ops);\n\n\tif (!ret)\n\t\tfunction_enabled = true;\n\n\treturn ret;\n}\n\nstatic void unregister_wakeup_function(struct trace_array *tr, int graph)\n{\n\tif (!function_enabled)\n\t\treturn;\n\n\tif (graph)\n\t\tunregister_ftrace_graph(&fgraph_wakeup_ops);\n\telse\n\t\tunregister_ftrace_function(tr->ops);\n\n\tfunction_enabled = false;\n}\n\nstatic int wakeup_function_set(struct trace_array *tr, u32 mask, int set)\n{\n\tif (!(mask & TRACE_ITER_FUNCTION))\n\t\treturn 0;\n\n\tif (set)\n\t\tregister_wakeup_function(tr, is_graph(tr), 1);\n\telse\n\t\tunregister_wakeup_function(tr, is_graph(tr));\n\treturn 1;\n}\n#else  \nstatic int register_wakeup_function(struct trace_array *tr, int graph, int set)\n{\n\treturn 0;\n}\nstatic void unregister_wakeup_function(struct trace_array *tr, int graph) { }\nstatic int wakeup_function_set(struct trace_array *tr, u32 mask, int set)\n{\n\treturn 0;\n}\n#endif  \n\n#ifndef CONFIG_FUNCTION_GRAPH_TRACER\nstatic enum print_line_t wakeup_print_line(struct trace_iterator *iter)\n{\n\treturn TRACE_TYPE_UNHANDLED;\n}\n\nstatic void wakeup_trace_open(struct trace_iterator *iter) { }\nstatic void wakeup_trace_close(struct trace_iterator *iter) { }\n\nstatic void wakeup_print_header(struct seq_file *s)\n{\n\ttrace_default_header(s);\n}\n#endif  \n\nstatic void\n__trace_function(struct trace_array *tr,\n\t\t unsigned long ip, unsigned long parent_ip,\n\t\t unsigned int trace_ctx)\n{\n\tif (is_graph(tr))\n\t\ttrace_graph_function(tr, ip, parent_ip, trace_ctx);\n\telse\n\t\ttrace_function(tr, ip, parent_ip, trace_ctx);\n}\n\nstatic int wakeup_flag_changed(struct trace_array *tr, u32 mask, int set)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\n\tif (wakeup_function_set(tr, mask, set))\n\t\treturn 0;\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tif (mask & TRACE_ITER_DISPLAY_GRAPH)\n\t\treturn wakeup_display_graph(tr, set);\n#endif\n\n\treturn trace_keep_overwrite(tracer, mask, set);\n}\n\nstatic int start_func_tracer(struct trace_array *tr, int graph)\n{\n\tint ret;\n\n\tret = register_wakeup_function(tr, graph, 0);\n\n\tif (!ret && tracing_is_enabled())\n\t\ttracer_enabled = 1;\n\telse\n\t\ttracer_enabled = 0;\n\n\treturn ret;\n}\n\nstatic void stop_func_tracer(struct trace_array *tr, int graph)\n{\n\ttracer_enabled = 0;\n\n\tunregister_wakeup_function(tr, graph);\n}\n\n \nstatic bool report_latency(struct trace_array *tr, u64 delta)\n{\n\tif (tracing_thresh) {\n\t\tif (delta < tracing_thresh)\n\t\t\treturn false;\n\t} else {\n\t\tif (delta <= tr->max_latency)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic void\nprobe_wakeup_migrate_task(void *ignore, struct task_struct *task, int cpu)\n{\n\tif (task != wakeup_task)\n\t\treturn;\n\n\twakeup_current_cpu = cpu;\n}\n\nstatic void\ntracing_sched_switch_trace(struct trace_array *tr,\n\t\t\t   struct task_struct *prev,\n\t\t\t   struct task_struct *next,\n\t\t\t   unsigned int trace_ctx)\n{\n\tstruct trace_event_call *call = &event_context_switch;\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ctx_switch_entry *entry;\n\n\tevent = trace_buffer_lock_reserve(buffer, TRACE_CTX,\n\t\t\t\t\t  sizeof(*entry), trace_ctx);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->prev_pid\t\t\t= prev->pid;\n\tentry->prev_prio\t\t= prev->prio;\n\tentry->prev_state\t\t= task_state_index(prev);\n\tentry->next_pid\t\t\t= next->pid;\n\tentry->next_prio\t\t= next->prio;\n\tentry->next_state\t\t= task_state_index(next);\n\tentry->next_cpu\t= task_cpu(next);\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\ttrace_buffer_unlock_commit(tr, buffer, event, trace_ctx);\n}\n\nstatic void\ntracing_sched_wakeup_trace(struct trace_array *tr,\n\t\t\t   struct task_struct *wakee,\n\t\t\t   struct task_struct *curr,\n\t\t\t   unsigned int trace_ctx)\n{\n\tstruct trace_event_call *call = &event_wakeup;\n\tstruct ring_buffer_event *event;\n\tstruct ctx_switch_entry *entry;\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\n\tevent = trace_buffer_lock_reserve(buffer, TRACE_WAKE,\n\t\t\t\t\t  sizeof(*entry), trace_ctx);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->prev_pid\t\t\t= curr->pid;\n\tentry->prev_prio\t\t= curr->prio;\n\tentry->prev_state\t\t= task_state_index(curr);\n\tentry->next_pid\t\t\t= wakee->pid;\n\tentry->next_prio\t\t= wakee->prio;\n\tentry->next_state\t\t= task_state_index(wakee);\n\tentry->next_cpu\t\t\t= task_cpu(wakee);\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\ttrace_buffer_unlock_commit(tr, buffer, event, trace_ctx);\n}\n\nstatic void notrace\nprobe_wakeup_sched_switch(void *ignore, bool preempt,\n\t\t\t  struct task_struct *prev, struct task_struct *next,\n\t\t\t  unsigned int prev_state)\n{\n\tstruct trace_array_cpu *data;\n\tu64 T0, T1, delta;\n\tunsigned long flags;\n\tlong disabled;\n\tint cpu;\n\tunsigned int trace_ctx;\n\n\ttracing_record_cmdline(prev);\n\n\tif (unlikely(!tracer_enabled))\n\t\treturn;\n\n\t \n\tsmp_rmb();\n\n\tif (next != wakeup_task)\n\t\treturn;\n\n\t \n\tcpu = raw_smp_processor_id();\n\tdisabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);\n\tif (likely(disabled != 1))\n\t\tgoto out;\n\n\tlocal_irq_save(flags);\n\ttrace_ctx = tracing_gen_ctx_flags(flags);\n\n\tarch_spin_lock(&wakeup_lock);\n\n\t \n\tif (unlikely(!tracer_enabled || next != wakeup_task))\n\t\tgoto out_unlock;\n\n\t \n\tdata = per_cpu_ptr(wakeup_trace->array_buffer.data, wakeup_cpu);\n\n\t__trace_function(wakeup_trace, CALLER_ADDR0, CALLER_ADDR1, trace_ctx);\n\ttracing_sched_switch_trace(wakeup_trace, prev, next, trace_ctx);\n\t__trace_stack(wakeup_trace, trace_ctx, 0);\n\n\tT0 = data->preempt_timestamp;\n\tT1 = ftrace_now(cpu);\n\tdelta = T1-T0;\n\n\tif (!report_latency(wakeup_trace, delta))\n\t\tgoto out_unlock;\n\n\tif (likely(!is_tracing_stopped())) {\n\t\twakeup_trace->max_latency = delta;\n\t\tupdate_max_tr(wakeup_trace, wakeup_task, wakeup_cpu, NULL);\n\t}\n\nout_unlock:\n\t__wakeup_reset(wakeup_trace);\n\tarch_spin_unlock(&wakeup_lock);\n\tlocal_irq_restore(flags);\nout:\n\tatomic_dec(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);\n}\n\nstatic void __wakeup_reset(struct trace_array *tr)\n{\n\twakeup_cpu = -1;\n\twakeup_prio = -1;\n\ttracing_dl = false;\n\n\tif (wakeup_task)\n\t\tput_task_struct(wakeup_task);\n\n\twakeup_task = NULL;\n}\n\nstatic void wakeup_reset(struct trace_array *tr)\n{\n\tunsigned long flags;\n\n\ttracing_reset_online_cpus(&tr->array_buffer);\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&wakeup_lock);\n\t__wakeup_reset(tr);\n\tarch_spin_unlock(&wakeup_lock);\n\tlocal_irq_restore(flags);\n}\n\nstatic void\nprobe_wakeup(void *ignore, struct task_struct *p)\n{\n\tstruct trace_array_cpu *data;\n\tint cpu = smp_processor_id();\n\tlong disabled;\n\tunsigned int trace_ctx;\n\n\tif (likely(!tracer_enabled))\n\t\treturn;\n\n\ttracing_record_cmdline(p);\n\ttracing_record_cmdline(current);\n\n\t \n\tif (tracing_dl || (wakeup_dl && !dl_task(p)) ||\n\t    (wakeup_rt && !dl_task(p) && !rt_task(p)) ||\n\t    (!dl_task(p) && (p->prio >= wakeup_prio || p->prio >= current->prio)))\n\t\treturn;\n\n\tdisabled = atomic_inc_return(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);\n\tif (unlikely(disabled != 1))\n\t\tgoto out;\n\n\ttrace_ctx = tracing_gen_ctx();\n\n\t \n\tarch_spin_lock(&wakeup_lock);\n\n\t \n\tif (!tracer_enabled || tracing_dl ||\n\t    (!dl_task(p) && p->prio >= wakeup_prio))\n\t\tgoto out_locked;\n\n\t \n\t__wakeup_reset(wakeup_trace);\n\n\twakeup_cpu = task_cpu(p);\n\twakeup_current_cpu = wakeup_cpu;\n\twakeup_prio = p->prio;\n\n\t \n\tif (dl_task(p))\n\t\ttracing_dl = true;\n\telse\n\t\ttracing_dl = false;\n\n\twakeup_task = get_task_struct(p);\n\n\tdata = per_cpu_ptr(wakeup_trace->array_buffer.data, wakeup_cpu);\n\tdata->preempt_timestamp = ftrace_now(cpu);\n\ttracing_sched_wakeup_trace(wakeup_trace, p, current, trace_ctx);\n\t__trace_stack(wakeup_trace, trace_ctx, 0);\n\n\t \n\t__trace_function(wakeup_trace, CALLER_ADDR1, CALLER_ADDR2, trace_ctx);\n\nout_locked:\n\tarch_spin_unlock(&wakeup_lock);\nout:\n\tatomic_dec(&per_cpu_ptr(wakeup_trace->array_buffer.data, cpu)->disabled);\n}\n\nstatic void start_wakeup_tracer(struct trace_array *tr)\n{\n\tint ret;\n\n\tret = register_trace_sched_wakeup(probe_wakeup, NULL);\n\tif (ret) {\n\t\tpr_info(\"wakeup trace: Couldn't activate tracepoint\"\n\t\t\t\" probe to kernel_sched_wakeup\\n\");\n\t\treturn;\n\t}\n\n\tret = register_trace_sched_wakeup_new(probe_wakeup, NULL);\n\tif (ret) {\n\t\tpr_info(\"wakeup trace: Couldn't activate tracepoint\"\n\t\t\t\" probe to kernel_sched_wakeup_new\\n\");\n\t\tgoto fail_deprobe;\n\t}\n\n\tret = register_trace_sched_switch(probe_wakeup_sched_switch, NULL);\n\tif (ret) {\n\t\tpr_info(\"sched trace: Couldn't activate tracepoint\"\n\t\t\t\" probe to kernel_sched_switch\\n\");\n\t\tgoto fail_deprobe_wake_new;\n\t}\n\n\tret = register_trace_sched_migrate_task(probe_wakeup_migrate_task, NULL);\n\tif (ret) {\n\t\tpr_info(\"wakeup trace: Couldn't activate tracepoint\"\n\t\t\t\" probe to kernel_sched_migrate_task\\n\");\n\t\tgoto fail_deprobe_sched_switch;\n\t}\n\n\twakeup_reset(tr);\n\n\t \n\tsmp_wmb();\n\n\tif (start_func_tracer(tr, is_graph(tr)))\n\t\tprintk(KERN_ERR \"failed to start wakeup tracer\\n\");\n\n\treturn;\nfail_deprobe_sched_switch:\n\tunregister_trace_sched_switch(probe_wakeup_sched_switch, NULL);\nfail_deprobe_wake_new:\n\tunregister_trace_sched_wakeup_new(probe_wakeup, NULL);\nfail_deprobe:\n\tunregister_trace_sched_wakeup(probe_wakeup, NULL);\n}\n\nstatic void stop_wakeup_tracer(struct trace_array *tr)\n{\n\ttracer_enabled = 0;\n\tstop_func_tracer(tr, is_graph(tr));\n\tunregister_trace_sched_switch(probe_wakeup_sched_switch, NULL);\n\tunregister_trace_sched_wakeup_new(probe_wakeup, NULL);\n\tunregister_trace_sched_wakeup(probe_wakeup, NULL);\n\tunregister_trace_sched_migrate_task(probe_wakeup_migrate_task, NULL);\n}\n\nstatic bool wakeup_busy;\n\nstatic int __wakeup_tracer_init(struct trace_array *tr)\n{\n\tsave_flags = tr->trace_flags;\n\n\t \n\tset_tracer_flag(tr, TRACE_ITER_OVERWRITE, 1);\n\tset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, 1);\n\n\ttr->max_latency = 0;\n\twakeup_trace = tr;\n\tftrace_init_array_ops(tr, wakeup_tracer_call);\n\tstart_wakeup_tracer(tr);\n\n\twakeup_busy = true;\n\treturn 0;\n}\n\nstatic int wakeup_tracer_init(struct trace_array *tr)\n{\n\tif (wakeup_busy)\n\t\treturn -EBUSY;\n\n\twakeup_dl = false;\n\twakeup_rt = false;\n\treturn __wakeup_tracer_init(tr);\n}\n\nstatic int wakeup_rt_tracer_init(struct trace_array *tr)\n{\n\tif (wakeup_busy)\n\t\treturn -EBUSY;\n\n\twakeup_dl = false;\n\twakeup_rt = true;\n\treturn __wakeup_tracer_init(tr);\n}\n\nstatic int wakeup_dl_tracer_init(struct trace_array *tr)\n{\n\tif (wakeup_busy)\n\t\treturn -EBUSY;\n\n\twakeup_dl = true;\n\twakeup_rt = false;\n\treturn __wakeup_tracer_init(tr);\n}\n\nstatic void wakeup_tracer_reset(struct trace_array *tr)\n{\n\tint lat_flag = save_flags & TRACE_ITER_LATENCY_FMT;\n\tint overwrite_flag = save_flags & TRACE_ITER_OVERWRITE;\n\n\tstop_wakeup_tracer(tr);\n\t \n\twakeup_reset(tr);\n\n\tset_tracer_flag(tr, TRACE_ITER_LATENCY_FMT, lat_flag);\n\tset_tracer_flag(tr, TRACE_ITER_OVERWRITE, overwrite_flag);\n\tftrace_reset_array_ops(tr);\n\twakeup_busy = false;\n}\n\nstatic void wakeup_tracer_start(struct trace_array *tr)\n{\n\twakeup_reset(tr);\n\ttracer_enabled = 1;\n}\n\nstatic void wakeup_tracer_stop(struct trace_array *tr)\n{\n\ttracer_enabled = 0;\n}\n\nstatic struct tracer wakeup_tracer __read_mostly =\n{\n\t.name\t\t= \"wakeup\",\n\t.init\t\t= wakeup_tracer_init,\n\t.reset\t\t= wakeup_tracer_reset,\n\t.start\t\t= wakeup_tracer_start,\n\t.stop\t\t= wakeup_tracer_stop,\n\t.print_max\t= true,\n\t.print_header\t= wakeup_print_header,\n\t.print_line\t= wakeup_print_line,\n\t.flag_changed\t= wakeup_flag_changed,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest    = trace_selftest_startup_wakeup,\n#endif\n\t.open\t\t= wakeup_trace_open,\n\t.close\t\t= wakeup_trace_close,\n\t.allow_instances = true,\n\t.use_max_tr\t= true,\n};\n\nstatic struct tracer wakeup_rt_tracer __read_mostly =\n{\n\t.name\t\t= \"wakeup_rt\",\n\t.init\t\t= wakeup_rt_tracer_init,\n\t.reset\t\t= wakeup_tracer_reset,\n\t.start\t\t= wakeup_tracer_start,\n\t.stop\t\t= wakeup_tracer_stop,\n\t.print_max\t= true,\n\t.print_header\t= wakeup_print_header,\n\t.print_line\t= wakeup_print_line,\n\t.flag_changed\t= wakeup_flag_changed,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest    = trace_selftest_startup_wakeup,\n#endif\n\t.open\t\t= wakeup_trace_open,\n\t.close\t\t= wakeup_trace_close,\n\t.allow_instances = true,\n\t.use_max_tr\t= true,\n};\n\nstatic struct tracer wakeup_dl_tracer __read_mostly =\n{\n\t.name\t\t= \"wakeup_dl\",\n\t.init\t\t= wakeup_dl_tracer_init,\n\t.reset\t\t= wakeup_tracer_reset,\n\t.start\t\t= wakeup_tracer_start,\n\t.stop\t\t= wakeup_tracer_stop,\n\t.print_max\t= true,\n\t.print_header\t= wakeup_print_header,\n\t.print_line\t= wakeup_print_line,\n\t.flag_changed\t= wakeup_flag_changed,\n#ifdef CONFIG_FTRACE_SELFTEST\n\t.selftest    = trace_selftest_startup_wakeup,\n#endif\n\t.open\t\t= wakeup_trace_open,\n\t.close\t\t= wakeup_trace_close,\n\t.allow_instances = true,\n\t.use_max_tr\t= true,\n};\n\n__init static int init_wakeup_tracer(void)\n{\n\tint ret;\n\n\tret = register_tracer(&wakeup_tracer);\n\tif (ret)\n\t\treturn ret;\n\n\tret = register_tracer(&wakeup_rt_tracer);\n\tif (ret)\n\t\treturn ret;\n\n\tret = register_tracer(&wakeup_dl_tracer);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\ncore_initcall(init_wakeup_tracer);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}