{
  "module_name": "trace_events_user.c",
  "hash_id": "9912bce02290c36f492969801871f71f57702072762d4a977d2f1e940d4f97dc",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace_events_user.c",
  "human_readable_source": "\n \n\n#include <linux/bitmap.h>\n#include <linux/cdev.h>\n#include <linux/hashtable.h>\n#include <linux/list.h>\n#include <linux/io.h>\n#include <linux/uio.h>\n#include <linux/ioctl.h>\n#include <linux/jhash.h>\n#include <linux/refcount.h>\n#include <linux/trace_events.h>\n#include <linux/tracefs.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/highmem.h>\n#include <linux/init.h>\n#include <linux/user_events.h>\n#include \"trace_dynevent.h\"\n#include \"trace_output.h\"\n#include \"trace.h\"\n\n#define USER_EVENTS_PREFIX_LEN (sizeof(USER_EVENTS_PREFIX)-1)\n\n#define FIELD_DEPTH_TYPE 0\n#define FIELD_DEPTH_NAME 1\n#define FIELD_DEPTH_SIZE 2\n\n \n#define MAX_EVENT_DESC 512\n#define EVENT_NAME(user_event) ((user_event)->tracepoint.name)\n#define MAX_FIELD_ARRAY_SIZE 1024\n\n \n#define EVENT_STATUS_FTRACE BIT(0)\n#define EVENT_STATUS_PERF BIT(1)\n#define EVENT_STATUS_OTHER BIT(7)\n\n \nenum user_reg_flag {\n\t \n\tUSER_EVENT_REG_PERSIST\t\t= 1U << 0,\n\n\t \n\tUSER_EVENT_REG_MAX\t\t= 1U << 1,\n};\n\n \nstruct user_event_group {\n\tchar\t\t*system_name;\n\tstruct\t\thlist_node node;\n\tstruct\t\tmutex reg_mutex;\n\tDECLARE_HASHTABLE(register_table, 8);\n};\n\n \nstatic struct user_event_group *init_group;\n\n \nstatic unsigned int max_user_events = 32768;\n\n \nstatic unsigned int current_user_events;\n\n \nstruct user_event {\n\tstruct user_event_group\t\t*group;\n\tstruct tracepoint\t\ttracepoint;\n\tstruct trace_event_call\t\tcall;\n\tstruct trace_event_class\tclass;\n\tstruct dyn_event\t\tdevent;\n\tstruct hlist_node\t\tnode;\n\tstruct list_head\t\tfields;\n\tstruct list_head\t\tvalidators;\n\tstruct work_struct\t\tput_work;\n\trefcount_t\t\t\trefcnt;\n\tint\t\t\t\tmin_size;\n\tint\t\t\t\treg_flags;\n\tchar\t\t\t\tstatus;\n};\n\n \nstruct user_event_enabler {\n\tstruct list_head\tmm_enablers_link;\n\tstruct user_event\t*event;\n\tunsigned long\t\taddr;\n\n\t \n\tunsigned long\t\tvalues;\n};\n\n \n#define ENABLE_VAL_BIT_MASK 0x3F\n\n \n#define ENABLE_VAL_FAULTING_BIT 6\n\n \n#define ENABLE_VAL_FREEING_BIT 7\n\n \n#define ENABLE_VAL_32_ON_64_BIT 8\n\n#define ENABLE_VAL_COMPAT_MASK (1 << ENABLE_VAL_32_ON_64_BIT)\n\n \n#define ENABLE_VAL_DUP_MASK (ENABLE_VAL_BIT_MASK | ENABLE_VAL_COMPAT_MASK)\n\n#define ENABLE_BITOPS(e) (&(e)->values)\n\n#define ENABLE_BIT(e) ((int)((e)->values & ENABLE_VAL_BIT_MASK))\n\n \nstruct user_event_enabler_fault {\n\tstruct work_struct\t\twork;\n\tstruct user_event_mm\t\t*mm;\n\tstruct user_event_enabler\t*enabler;\n\tint\t\t\t\tattempt;\n};\n\nstatic struct kmem_cache *fault_cache;\n\n \nstatic LIST_HEAD(user_event_mms);\nstatic DEFINE_SPINLOCK(user_event_mms_lock);\n\n \nstruct user_event_refs {\n\tstruct rcu_head\t\trcu;\n\tint\t\t\tcount;\n\tstruct user_event\t*events[];\n};\n\nstruct user_event_file_info {\n\tstruct user_event_group\t*group;\n\tstruct user_event_refs\t*refs;\n};\n\n#define VALIDATOR_ENSURE_NULL (1 << 0)\n#define VALIDATOR_REL (1 << 1)\n\nstruct user_event_validator {\n\tstruct list_head\tuser_event_link;\n\tint\t\t\toffset;\n\tint\t\t\tflags;\n};\n\nstatic inline void align_addr_bit(unsigned long *addr, int *bit,\n\t\t\t\t  unsigned long *flags)\n{\n\tif (IS_ALIGNED(*addr, sizeof(long))) {\n#ifdef __BIG_ENDIAN\n\t\t \n\t\tif (test_bit(ENABLE_VAL_32_ON_64_BIT, flags))\n\t\t\t*bit += 32;\n#endif\n\t\treturn;\n\t}\n\n\t*addr = ALIGN_DOWN(*addr, sizeof(long));\n\n\t \n#ifdef __LITTLE_ENDIAN\n\t*bit += 32;\n#endif\n}\n\ntypedef void (*user_event_func_t) (struct user_event *user, struct iov_iter *i,\n\t\t\t\t   void *tpdata, bool *faulted);\n\nstatic int user_event_parse(struct user_event_group *group, char *name,\n\t\t\t    char *args, char *flags,\n\t\t\t    struct user_event **newuser, int reg_flags);\n\nstatic struct user_event_mm *user_event_mm_get(struct user_event_mm *mm);\nstatic struct user_event_mm *user_event_mm_get_all(struct user_event *user);\nstatic void user_event_mm_put(struct user_event_mm *mm);\nstatic int destroy_user_event(struct user_event *user);\n\nstatic u32 user_event_key(char *name)\n{\n\treturn jhash(name, strlen(name), 0);\n}\n\nstatic struct user_event *user_event_get(struct user_event *user)\n{\n\trefcount_inc(&user->refcnt);\n\n\treturn user;\n}\n\nstatic void delayed_destroy_user_event(struct work_struct *work)\n{\n\tstruct user_event *user = container_of(\n\t\twork, struct user_event, put_work);\n\n\tmutex_lock(&event_mutex);\n\n\tif (!refcount_dec_and_test(&user->refcnt))\n\t\tgoto out;\n\n\tif (destroy_user_event(user)) {\n\t\t \n\t\tpr_warn(\"user_events: Unable to delete event\\n\");\n\t\trefcount_set(&user->refcnt, 1);\n\t}\nout:\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void user_event_put(struct user_event *user, bool locked)\n{\n\tbool delete;\n\n\tif (unlikely(!user))\n\t\treturn;\n\n\t \n\tif (!locked) {\n\t\tlockdep_assert_not_held(&event_mutex);\n\t\tdelete = refcount_dec_and_mutex_lock(&user->refcnt, &event_mutex);\n\t} else {\n\t\tlockdep_assert_held(&event_mutex);\n\t\tdelete = refcount_dec_and_test(&user->refcnt);\n\t}\n\n\tif (!delete)\n\t\treturn;\n\n\t \n\n\tif (user->reg_flags & USER_EVENT_REG_PERSIST) {\n\t\t \n\t\tpr_alert(\"BUG: Auto-delete engaged on persistent event\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tINIT_WORK(&user->put_work, delayed_destroy_user_event);\n\n\t \n\trefcount_set(&user->refcnt, 1);\n\n\tif (WARN_ON_ONCE(!schedule_work(&user->put_work))) {\n\t\t \n\t\tpr_warn(\"user_events: Unable to queue delayed destroy\\n\");\n\t}\nout:\n\t \n\tif (!locked)\n\t\tmutex_unlock(&event_mutex);\n}\n\nstatic void user_event_group_destroy(struct user_event_group *group)\n{\n\tkfree(group->system_name);\n\tkfree(group);\n}\n\nstatic char *user_event_group_system_name(void)\n{\n\tchar *system_name;\n\tint len = sizeof(USER_EVENTS_SYSTEM) + 1;\n\n\tsystem_name = kmalloc(len, GFP_KERNEL);\n\n\tif (!system_name)\n\t\treturn NULL;\n\n\tsnprintf(system_name, len, \"%s\", USER_EVENTS_SYSTEM);\n\n\treturn system_name;\n}\n\nstatic struct user_event_group *current_user_event_group(void)\n{\n\treturn init_group;\n}\n\nstatic struct user_event_group *user_event_group_create(void)\n{\n\tstruct user_event_group *group;\n\n\tgroup = kzalloc(sizeof(*group), GFP_KERNEL);\n\n\tif (!group)\n\t\treturn NULL;\n\n\tgroup->system_name = user_event_group_system_name();\n\n\tif (!group->system_name)\n\t\tgoto error;\n\n\tmutex_init(&group->reg_mutex);\n\thash_init(group->register_table);\n\n\treturn group;\nerror:\n\tif (group)\n\t\tuser_event_group_destroy(group);\n\n\treturn NULL;\n};\n\nstatic void user_event_enabler_destroy(struct user_event_enabler *enabler,\n\t\t\t\t       bool locked)\n{\n\tlist_del_rcu(&enabler->mm_enablers_link);\n\n\t \n\tuser_event_put(enabler->event, locked);\n\n\tkfree(enabler);\n}\n\nstatic int user_event_mm_fault_in(struct user_event_mm *mm, unsigned long uaddr,\n\t\t\t\t  int attempt)\n{\n\tbool unlocked;\n\tint ret;\n\n\t \n\tif (attempt > 10)\n\t\treturn -EFAULT;\n\n\tmmap_read_lock(mm->mm);\n\n\t \n\tif (refcount_read(&mm->tasks) == 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tret = fixup_user_fault(mm->mm, uaddr, FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE,\n\t\t\t       &unlocked);\nout:\n\tmmap_read_unlock(mm->mm);\n\n\treturn ret;\n}\n\nstatic int user_event_enabler_write(struct user_event_mm *mm,\n\t\t\t\t    struct user_event_enabler *enabler,\n\t\t\t\t    bool fixup_fault, int *attempt);\n\nstatic void user_event_enabler_fault_fixup(struct work_struct *work)\n{\n\tstruct user_event_enabler_fault *fault = container_of(\n\t\twork, struct user_event_enabler_fault, work);\n\tstruct user_event_enabler *enabler = fault->enabler;\n\tstruct user_event_mm *mm = fault->mm;\n\tunsigned long uaddr = enabler->addr;\n\tint attempt = fault->attempt;\n\tint ret;\n\n\tret = user_event_mm_fault_in(mm, uaddr, attempt);\n\n\tif (ret && ret != -ENOENT) {\n\t\tstruct user_event *user = enabler->event;\n\n\t\tpr_warn(\"user_events: Fault for mm: 0x%pK @ 0x%llx event: %s\\n\",\n\t\t\tmm->mm, (unsigned long long)uaddr, EVENT_NAME(user));\n\t}\n\n\t \n\tmutex_lock(&event_mutex);\n\n\t \n\tif (test_bit(ENABLE_VAL_FREEING_BIT, ENABLE_BITOPS(enabler))) {\n\t\tuser_event_enabler_destroy(enabler, true);\n\t\tgoto out;\n\t}\n\n\t \n\tclear_bit(ENABLE_VAL_FAULTING_BIT, ENABLE_BITOPS(enabler));\n\n\tif (!ret) {\n\t\tmmap_read_lock(mm->mm);\n\t\tuser_event_enabler_write(mm, enabler, true, &attempt);\n\t\tmmap_read_unlock(mm->mm);\n\t}\nout:\n\tmutex_unlock(&event_mutex);\n\n\t \n\tuser_event_mm_put(mm);\n\tkmem_cache_free(fault_cache, fault);\n}\n\nstatic bool user_event_enabler_queue_fault(struct user_event_mm *mm,\n\t\t\t\t\t   struct user_event_enabler *enabler,\n\t\t\t\t\t   int attempt)\n{\n\tstruct user_event_enabler_fault *fault;\n\n\tfault = kmem_cache_zalloc(fault_cache, GFP_NOWAIT | __GFP_NOWARN);\n\n\tif (!fault)\n\t\treturn false;\n\n\tINIT_WORK(&fault->work, user_event_enabler_fault_fixup);\n\tfault->mm = user_event_mm_get(mm);\n\tfault->enabler = enabler;\n\tfault->attempt = attempt;\n\n\t \n\tset_bit(ENABLE_VAL_FAULTING_BIT, ENABLE_BITOPS(enabler));\n\n\tif (!schedule_work(&fault->work)) {\n\t\t \n\t\tclear_bit(ENABLE_VAL_FAULTING_BIT, ENABLE_BITOPS(enabler));\n\n\t\tuser_event_mm_put(mm);\n\t\tkmem_cache_free(fault_cache, fault);\n\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int user_event_enabler_write(struct user_event_mm *mm,\n\t\t\t\t    struct user_event_enabler *enabler,\n\t\t\t\t    bool fixup_fault, int *attempt)\n{\n\tunsigned long uaddr = enabler->addr;\n\tunsigned long *ptr;\n\tstruct page *page;\n\tvoid *kaddr;\n\tint bit = ENABLE_BIT(enabler);\n\tint ret;\n\n\tlockdep_assert_held(&event_mutex);\n\tmmap_assert_locked(mm->mm);\n\n\t*attempt += 1;\n\n\t \n\tif (refcount_read(&mm->tasks) == 0)\n\t\treturn -ENOENT;\n\n\tif (unlikely(test_bit(ENABLE_VAL_FAULTING_BIT, ENABLE_BITOPS(enabler)) ||\n\t\t     test_bit(ENABLE_VAL_FREEING_BIT, ENABLE_BITOPS(enabler))))\n\t\treturn -EBUSY;\n\n\talign_addr_bit(&uaddr, &bit, ENABLE_BITOPS(enabler));\n\n\tret = pin_user_pages_remote(mm->mm, uaddr, 1, FOLL_WRITE | FOLL_NOFAULT,\n\t\t\t\t    &page, NULL);\n\n\tif (unlikely(ret <= 0)) {\n\t\tif (!fixup_fault)\n\t\t\treturn -EFAULT;\n\n\t\tif (!user_event_enabler_queue_fault(mm, enabler, *attempt))\n\t\t\tpr_warn(\"user_events: Unable to queue fault handler\\n\");\n\n\t\treturn -EFAULT;\n\t}\n\n\tkaddr = kmap_local_page(page);\n\tptr = kaddr + (uaddr & ~PAGE_MASK);\n\n\t \n\tif (enabler->event && enabler->event->status)\n\t\tset_bit(bit, ptr);\n\telse\n\t\tclear_bit(bit, ptr);\n\n\tkunmap_local(kaddr);\n\tunpin_user_pages_dirty_lock(&page, 1, true);\n\n\treturn 0;\n}\n\nstatic bool user_event_enabler_exists(struct user_event_mm *mm,\n\t\t\t\t      unsigned long uaddr, unsigned char bit)\n{\n\tstruct user_event_enabler *enabler;\n\n\tlist_for_each_entry(enabler, &mm->enablers, mm_enablers_link) {\n\t\tif (enabler->addr == uaddr && ENABLE_BIT(enabler) == bit)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void user_event_enabler_update(struct user_event *user)\n{\n\tstruct user_event_enabler *enabler;\n\tstruct user_event_mm *next;\n\tstruct user_event_mm *mm;\n\tint attempt;\n\n\tlockdep_assert_held(&event_mutex);\n\n\t \n\tmm = user_event_mm_get_all(user);\n\n\twhile (mm) {\n\t\tnext = mm->next;\n\t\tmmap_read_lock(mm->mm);\n\n\t\tlist_for_each_entry(enabler, &mm->enablers, mm_enablers_link) {\n\t\t\tif (enabler->event == user) {\n\t\t\t\tattempt = 0;\n\t\t\t\tuser_event_enabler_write(mm, enabler, true, &attempt);\n\t\t\t}\n\t\t}\n\n\t\tmmap_read_unlock(mm->mm);\n\t\tuser_event_mm_put(mm);\n\t\tmm = next;\n\t}\n}\n\nstatic bool user_event_enabler_dup(struct user_event_enabler *orig,\n\t\t\t\t   struct user_event_mm *mm)\n{\n\tstruct user_event_enabler *enabler;\n\n\t \n\tif (unlikely(test_bit(ENABLE_VAL_FREEING_BIT, ENABLE_BITOPS(orig))))\n\t\treturn true;\n\n\tenabler = kzalloc(sizeof(*enabler), GFP_NOWAIT | __GFP_ACCOUNT);\n\n\tif (!enabler)\n\t\treturn false;\n\n\tenabler->event = user_event_get(orig->event);\n\tenabler->addr = orig->addr;\n\n\t \n\tenabler->values = orig->values & ENABLE_VAL_DUP_MASK;\n\n\t \n\tlist_add(&enabler->mm_enablers_link, &mm->enablers);\n\n\treturn true;\n}\n\nstatic struct user_event_mm *user_event_mm_get(struct user_event_mm *mm)\n{\n\trefcount_inc(&mm->refcnt);\n\n\treturn mm;\n}\n\nstatic struct user_event_mm *user_event_mm_get_all(struct user_event *user)\n{\n\tstruct user_event_mm *found = NULL;\n\tstruct user_event_enabler *enabler;\n\tstruct user_event_mm *mm;\n\n\t \n\tlockdep_assert_held(&event_mutex);\n\n\t \n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(mm, &user_event_mms, mms_link) {\n\t\tlist_for_each_entry_rcu(enabler, &mm->enablers, mm_enablers_link) {\n\t\t\tif (enabler->event == user) {\n\t\t\t\tmm->next = found;\n\t\t\t\tfound = user_event_mm_get(mm);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\n\treturn found;\n}\n\nstatic struct user_event_mm *user_event_mm_alloc(struct task_struct *t)\n{\n\tstruct user_event_mm *user_mm;\n\n\tuser_mm = kzalloc(sizeof(*user_mm), GFP_KERNEL_ACCOUNT);\n\n\tif (!user_mm)\n\t\treturn NULL;\n\n\tuser_mm->mm = t->mm;\n\tINIT_LIST_HEAD(&user_mm->enablers);\n\trefcount_set(&user_mm->refcnt, 1);\n\trefcount_set(&user_mm->tasks, 1);\n\n\t \n\tmmgrab(user_mm->mm);\n\n\treturn user_mm;\n}\n\nstatic void user_event_mm_attach(struct user_event_mm *user_mm, struct task_struct *t)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&user_event_mms_lock, flags);\n\tlist_add_rcu(&user_mm->mms_link, &user_event_mms);\n\tspin_unlock_irqrestore(&user_event_mms_lock, flags);\n\n\tt->user_event_mm = user_mm;\n}\n\nstatic struct user_event_mm *current_user_event_mm(void)\n{\n\tstruct user_event_mm *user_mm = current->user_event_mm;\n\n\tif (user_mm)\n\t\tgoto inc;\n\n\tuser_mm = user_event_mm_alloc(current);\n\n\tif (!user_mm)\n\t\tgoto error;\n\n\tuser_event_mm_attach(user_mm, current);\ninc:\n\trefcount_inc(&user_mm->refcnt);\nerror:\n\treturn user_mm;\n}\n\nstatic void user_event_mm_destroy(struct user_event_mm *mm)\n{\n\tstruct user_event_enabler *enabler, *next;\n\n\tlist_for_each_entry_safe(enabler, next, &mm->enablers, mm_enablers_link)\n\t\tuser_event_enabler_destroy(enabler, false);\n\n\tmmdrop(mm->mm);\n\tkfree(mm);\n}\n\nstatic void user_event_mm_put(struct user_event_mm *mm)\n{\n\tif (mm && refcount_dec_and_test(&mm->refcnt))\n\t\tuser_event_mm_destroy(mm);\n}\n\nstatic void delayed_user_event_mm_put(struct work_struct *work)\n{\n\tstruct user_event_mm *mm;\n\n\tmm = container_of(to_rcu_work(work), struct user_event_mm, put_rwork);\n\tuser_event_mm_put(mm);\n}\n\nvoid user_event_mm_remove(struct task_struct *t)\n{\n\tstruct user_event_mm *mm;\n\tunsigned long flags;\n\n\tmight_sleep();\n\n\tmm = t->user_event_mm;\n\tt->user_event_mm = NULL;\n\n\t \n\tif (!refcount_dec_and_test(&mm->tasks))\n\t\treturn;\n\n\t \n\tspin_lock_irqsave(&user_event_mms_lock, flags);\n\tlist_del_rcu(&mm->mms_link);\n\tspin_unlock_irqrestore(&user_event_mms_lock, flags);\n\n\t \n\tmmap_write_lock(mm->mm);\n\tmmap_write_unlock(mm->mm);\n\n\t \n\tINIT_RCU_WORK(&mm->put_rwork, delayed_user_event_mm_put);\n\tqueue_rcu_work(system_wq, &mm->put_rwork);\n}\n\nvoid user_event_mm_dup(struct task_struct *t, struct user_event_mm *old_mm)\n{\n\tstruct user_event_mm *mm = user_event_mm_alloc(t);\n\tstruct user_event_enabler *enabler;\n\n\tif (!mm)\n\t\treturn;\n\n\trcu_read_lock();\n\n\tlist_for_each_entry_rcu(enabler, &old_mm->enablers, mm_enablers_link) {\n\t\tif (!user_event_enabler_dup(enabler, mm))\n\t\t\tgoto error;\n\t}\n\n\trcu_read_unlock();\n\n\tuser_event_mm_attach(mm, t);\n\treturn;\nerror:\n\trcu_read_unlock();\n\tuser_event_mm_destroy(mm);\n}\n\nstatic bool current_user_event_enabler_exists(unsigned long uaddr,\n\t\t\t\t\t      unsigned char bit)\n{\n\tstruct user_event_mm *user_mm = current_user_event_mm();\n\tbool exists;\n\n\tif (!user_mm)\n\t\treturn false;\n\n\texists = user_event_enabler_exists(user_mm, uaddr, bit);\n\n\tuser_event_mm_put(user_mm);\n\n\treturn exists;\n}\n\nstatic struct user_event_enabler\n*user_event_enabler_create(struct user_reg *reg, struct user_event *user,\n\t\t\t   int *write_result)\n{\n\tstruct user_event_enabler *enabler;\n\tstruct user_event_mm *user_mm;\n\tunsigned long uaddr = (unsigned long)reg->enable_addr;\n\tint attempt = 0;\n\n\tuser_mm = current_user_event_mm();\n\n\tif (!user_mm)\n\t\treturn NULL;\n\n\tenabler = kzalloc(sizeof(*enabler), GFP_KERNEL_ACCOUNT);\n\n\tif (!enabler)\n\t\tgoto out;\n\n\tenabler->event = user;\n\tenabler->addr = uaddr;\n\tenabler->values = reg->enable_bit;\n\n#if BITS_PER_LONG >= 64\n\tif (reg->enable_size == 4)\n\t\tset_bit(ENABLE_VAL_32_ON_64_BIT, ENABLE_BITOPS(enabler));\n#endif\n\nretry:\n\t \n\tmutex_lock(&event_mutex);\n\n\t \n\tmmap_read_lock(user_mm->mm);\n\t*write_result = user_event_enabler_write(user_mm, enabler, false,\n\t\t\t\t\t\t &attempt);\n\tmmap_read_unlock(user_mm->mm);\n\n\t \n\tif (!*write_result) {\n\t\tuser_event_get(user);\n\t\tlist_add_rcu(&enabler->mm_enablers_link, &user_mm->enablers);\n\t}\n\n\tmutex_unlock(&event_mutex);\n\n\tif (*write_result) {\n\t\t \n\t\tif (!user_event_mm_fault_in(user_mm, uaddr, attempt))\n\t\t\tgoto retry;\n\n\t\tkfree(enabler);\n\t\tenabler = NULL;\n\t}\nout:\n\tuser_event_mm_put(user_mm);\n\n\treturn enabler;\n}\n\nstatic __always_inline __must_check\nbool user_event_last_ref(struct user_event *user)\n{\n\tint last = 0;\n\n\tif (user->reg_flags & USER_EVENT_REG_PERSIST)\n\t\tlast = 1;\n\n\treturn refcount_read(&user->refcnt) == last;\n}\n\nstatic __always_inline __must_check\nsize_t copy_nofault(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tsize_t ret;\n\n\tpagefault_disable();\n\n\tret = copy_from_iter_nocache(addr, bytes, i);\n\n\tpagefault_enable();\n\n\treturn ret;\n}\n\nstatic struct list_head *user_event_get_fields(struct trace_event_call *call)\n{\n\tstruct user_event *user = (struct user_event *)call->data;\n\n\treturn &user->fields;\n}\n\n \nstatic int user_event_parse_cmd(struct user_event_group *group,\n\t\t\t\tchar *raw_command, struct user_event **newuser,\n\t\t\t\tint reg_flags)\n{\n\tchar *name = raw_command;\n\tchar *args = strpbrk(name, \" \");\n\tchar *flags;\n\n\tif (args)\n\t\t*args++ = '\\0';\n\n\tflags = strpbrk(name, \":\");\n\n\tif (flags)\n\t\t*flags++ = '\\0';\n\n\treturn user_event_parse(group, name, args, flags, newuser, reg_flags);\n}\n\nstatic int user_field_array_size(const char *type)\n{\n\tconst char *start = strchr(type, '[');\n\tchar val[8];\n\tchar *bracket;\n\tint size = 0;\n\n\tif (start == NULL)\n\t\treturn -EINVAL;\n\n\tif (strscpy(val, start + 1, sizeof(val)) <= 0)\n\t\treturn -EINVAL;\n\n\tbracket = strchr(val, ']');\n\n\tif (!bracket)\n\t\treturn -EINVAL;\n\n\t*bracket = '\\0';\n\n\tif (kstrtouint(val, 0, &size))\n\t\treturn -EINVAL;\n\n\tif (size > MAX_FIELD_ARRAY_SIZE)\n\t\treturn -EINVAL;\n\n\treturn size;\n}\n\nstatic int user_field_size(const char *type)\n{\n\t \n\tif (strcmp(type, \"s64\") == 0)\n\t\treturn sizeof(s64);\n\tif (strcmp(type, \"u64\") == 0)\n\t\treturn sizeof(u64);\n\tif (strcmp(type, \"s32\") == 0)\n\t\treturn sizeof(s32);\n\tif (strcmp(type, \"u32\") == 0)\n\t\treturn sizeof(u32);\n\tif (strcmp(type, \"int\") == 0)\n\t\treturn sizeof(int);\n\tif (strcmp(type, \"unsigned int\") == 0)\n\t\treturn sizeof(unsigned int);\n\tif (strcmp(type, \"s16\") == 0)\n\t\treturn sizeof(s16);\n\tif (strcmp(type, \"u16\") == 0)\n\t\treturn sizeof(u16);\n\tif (strcmp(type, \"short\") == 0)\n\t\treturn sizeof(short);\n\tif (strcmp(type, \"unsigned short\") == 0)\n\t\treturn sizeof(unsigned short);\n\tif (strcmp(type, \"s8\") == 0)\n\t\treturn sizeof(s8);\n\tif (strcmp(type, \"u8\") == 0)\n\t\treturn sizeof(u8);\n\tif (strcmp(type, \"char\") == 0)\n\t\treturn sizeof(char);\n\tif (strcmp(type, \"unsigned char\") == 0)\n\t\treturn sizeof(unsigned char);\n\tif (str_has_prefix(type, \"char[\"))\n\t\treturn user_field_array_size(type);\n\tif (str_has_prefix(type, \"unsigned char[\"))\n\t\treturn user_field_array_size(type);\n\tif (str_has_prefix(type, \"__data_loc \"))\n\t\treturn sizeof(u32);\n\tif (str_has_prefix(type, \"__rel_loc \"))\n\t\treturn sizeof(u32);\n\n\t \n\treturn -EINVAL;\n}\n\nstatic void user_event_destroy_validators(struct user_event *user)\n{\n\tstruct user_event_validator *validator, *next;\n\tstruct list_head *head = &user->validators;\n\n\tlist_for_each_entry_safe(validator, next, head, user_event_link) {\n\t\tlist_del(&validator->user_event_link);\n\t\tkfree(validator);\n\t}\n}\n\nstatic void user_event_destroy_fields(struct user_event *user)\n{\n\tstruct ftrace_event_field *field, *next;\n\tstruct list_head *head = &user->fields;\n\n\tlist_for_each_entry_safe(field, next, head, link) {\n\t\tlist_del(&field->link);\n\t\tkfree(field);\n\t}\n}\n\nstatic int user_event_add_field(struct user_event *user, const char *type,\n\t\t\t\tconst char *name, int offset, int size,\n\t\t\t\tint is_signed, int filter_type)\n{\n\tstruct user_event_validator *validator;\n\tstruct ftrace_event_field *field;\n\tint validator_flags = 0;\n\n\tfield = kmalloc(sizeof(*field), GFP_KERNEL_ACCOUNT);\n\n\tif (!field)\n\t\treturn -ENOMEM;\n\n\tif (str_has_prefix(type, \"__data_loc \"))\n\t\tgoto add_validator;\n\n\tif (str_has_prefix(type, \"__rel_loc \")) {\n\t\tvalidator_flags |= VALIDATOR_REL;\n\t\tgoto add_validator;\n\t}\n\n\tgoto add_field;\n\nadd_validator:\n\tif (strstr(type, \"char\") != NULL)\n\t\tvalidator_flags |= VALIDATOR_ENSURE_NULL;\n\n\tvalidator = kmalloc(sizeof(*validator), GFP_KERNEL_ACCOUNT);\n\n\tif (!validator) {\n\t\tkfree(field);\n\t\treturn -ENOMEM;\n\t}\n\n\tvalidator->flags = validator_flags;\n\tvalidator->offset = offset;\n\n\t \n\tlist_add_tail(&validator->user_event_link, &user->validators);\n\nadd_field:\n\tfield->type = type;\n\tfield->name = name;\n\tfield->offset = offset;\n\tfield->size = size;\n\tfield->is_signed = is_signed;\n\tfield->filter_type = filter_type;\n\n\tif (filter_type == FILTER_OTHER)\n\t\tfield->filter_type = filter_assign_type(type);\n\n\tlist_add(&field->link, &user->fields);\n\n\t \n\tuser->min_size = (offset + size) - sizeof(struct trace_entry);\n\n\treturn 0;\n}\n\n \nstatic int user_event_parse_field(char *field, struct user_event *user,\n\t\t\t\t  u32 *offset)\n{\n\tchar *part, *type, *name;\n\tu32 depth = 0, saved_offset = *offset;\n\tint len, size = -EINVAL;\n\tbool is_struct = false;\n\n\tfield = skip_spaces(field);\n\n\tif (*field == '\\0')\n\t\treturn 0;\n\n\t \n\tlen = str_has_prefix(field, \"unsigned \");\n\tif (len)\n\t\tgoto skip_next;\n\n\tlen = str_has_prefix(field, \"struct \");\n\tif (len) {\n\t\tis_struct = true;\n\t\tgoto skip_next;\n\t}\n\n\tlen = str_has_prefix(field, \"__data_loc unsigned \");\n\tif (len)\n\t\tgoto skip_next;\n\n\tlen = str_has_prefix(field, \"__data_loc \");\n\tif (len)\n\t\tgoto skip_next;\n\n\tlen = str_has_prefix(field, \"__rel_loc unsigned \");\n\tif (len)\n\t\tgoto skip_next;\n\n\tlen = str_has_prefix(field, \"__rel_loc \");\n\tif (len)\n\t\tgoto skip_next;\n\n\tgoto parse;\nskip_next:\n\ttype = field;\n\tfield = strpbrk(field + len, \" \");\n\n\tif (field == NULL)\n\t\treturn -EINVAL;\n\n\t*field++ = '\\0';\n\tdepth++;\nparse:\n\tname = NULL;\n\n\twhile ((part = strsep(&field, \" \")) != NULL) {\n\t\tswitch (depth++) {\n\t\tcase FIELD_DEPTH_TYPE:\n\t\t\ttype = part;\n\t\t\tbreak;\n\t\tcase FIELD_DEPTH_NAME:\n\t\t\tname = part;\n\t\t\tbreak;\n\t\tcase FIELD_DEPTH_SIZE:\n\t\t\tif (!is_struct)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tif (kstrtou32(part, 10, &size))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (depth < FIELD_DEPTH_SIZE || !name)\n\t\treturn -EINVAL;\n\n\tif (depth == FIELD_DEPTH_SIZE)\n\t\tsize = user_field_size(type);\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tif (size < 0)\n\t\treturn size;\n\n\t*offset = saved_offset + size;\n\n\treturn user_event_add_field(user, type, name, saved_offset, size,\n\t\t\t\t    type[0] != 'u', FILTER_OTHER);\n}\n\nstatic int user_event_parse_fields(struct user_event *user, char *args)\n{\n\tchar *field;\n\tu32 offset = sizeof(struct trace_entry);\n\tint ret = -EINVAL;\n\n\tif (args == NULL)\n\t\treturn 0;\n\n\twhile ((field = strsep(&args, \";\")) != NULL) {\n\t\tret = user_event_parse_field(field, user, &offset);\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic struct trace_event_fields user_event_fields_array[1];\n\nstatic const char *user_field_format(const char *type)\n{\n\tif (strcmp(type, \"s64\") == 0)\n\t\treturn \"%lld\";\n\tif (strcmp(type, \"u64\") == 0)\n\t\treturn \"%llu\";\n\tif (strcmp(type, \"s32\") == 0)\n\t\treturn \"%d\";\n\tif (strcmp(type, \"u32\") == 0)\n\t\treturn \"%u\";\n\tif (strcmp(type, \"int\") == 0)\n\t\treturn \"%d\";\n\tif (strcmp(type, \"unsigned int\") == 0)\n\t\treturn \"%u\";\n\tif (strcmp(type, \"s16\") == 0)\n\t\treturn \"%d\";\n\tif (strcmp(type, \"u16\") == 0)\n\t\treturn \"%u\";\n\tif (strcmp(type, \"short\") == 0)\n\t\treturn \"%d\";\n\tif (strcmp(type, \"unsigned short\") == 0)\n\t\treturn \"%u\";\n\tif (strcmp(type, \"s8\") == 0)\n\t\treturn \"%d\";\n\tif (strcmp(type, \"u8\") == 0)\n\t\treturn \"%u\";\n\tif (strcmp(type, \"char\") == 0)\n\t\treturn \"%d\";\n\tif (strcmp(type, \"unsigned char\") == 0)\n\t\treturn \"%u\";\n\tif (strstr(type, \"char[\") != NULL)\n\t\treturn \"%s\";\n\n\t \n\treturn \"%llu\";\n}\n\nstatic bool user_field_is_dyn_string(const char *type, const char **str_func)\n{\n\tif (str_has_prefix(type, \"__data_loc \")) {\n\t\t*str_func = \"__get_str\";\n\t\tgoto check;\n\t}\n\n\tif (str_has_prefix(type, \"__rel_loc \")) {\n\t\t*str_func = \"__get_rel_str\";\n\t\tgoto check;\n\t}\n\n\treturn false;\ncheck:\n\treturn strstr(type, \"char\") != NULL;\n}\n\n#define LEN_OR_ZERO (len ? len - pos : 0)\nstatic int user_dyn_field_set_string(int argc, const char **argv, int *iout,\n\t\t\t\t     char *buf, int len, bool *colon)\n{\n\tint pos = 0, i = *iout;\n\n\t*colon = false;\n\n\tfor (; i < argc; ++i) {\n\t\tif (i != *iout)\n\t\t\tpos += snprintf(buf + pos, LEN_OR_ZERO, \" \");\n\n\t\tpos += snprintf(buf + pos, LEN_OR_ZERO, \"%s\", argv[i]);\n\n\t\tif (strchr(argv[i], ';')) {\n\t\t\t++i;\n\t\t\t*colon = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (len != 0)\n\t\t*iout = i;\n\n\treturn pos + 1;\n}\n\nstatic int user_field_set_string(struct ftrace_event_field *field,\n\t\t\t\t char *buf, int len, bool colon)\n{\n\tint pos = 0;\n\n\tpos += snprintf(buf + pos, LEN_OR_ZERO, \"%s\", field->type);\n\tpos += snprintf(buf + pos, LEN_OR_ZERO, \" \");\n\tpos += snprintf(buf + pos, LEN_OR_ZERO, \"%s\", field->name);\n\n\tif (str_has_prefix(field->type, \"struct \"))\n\t\tpos += snprintf(buf + pos, LEN_OR_ZERO, \" %d\", field->size);\n\n\tif (colon)\n\t\tpos += snprintf(buf + pos, LEN_OR_ZERO, \";\");\n\n\treturn pos + 1;\n}\n\nstatic int user_event_set_print_fmt(struct user_event *user, char *buf, int len)\n{\n\tstruct ftrace_event_field *field;\n\tstruct list_head *head = &user->fields;\n\tint pos = 0, depth = 0;\n\tconst char *str_func;\n\n\tpos += snprintf(buf + pos, LEN_OR_ZERO, \"\\\"\");\n\n\tlist_for_each_entry_reverse(field, head, link) {\n\t\tif (depth != 0)\n\t\t\tpos += snprintf(buf + pos, LEN_OR_ZERO, \" \");\n\n\t\tpos += snprintf(buf + pos, LEN_OR_ZERO, \"%s=%s\",\n\t\t\t\tfield->name, user_field_format(field->type));\n\n\t\tdepth++;\n\t}\n\n\tpos += snprintf(buf + pos, LEN_OR_ZERO, \"\\\"\");\n\n\tlist_for_each_entry_reverse(field, head, link) {\n\t\tif (user_field_is_dyn_string(field->type, &str_func))\n\t\t\tpos += snprintf(buf + pos, LEN_OR_ZERO,\n\t\t\t\t\t\", %s(%s)\", str_func, field->name);\n\t\telse\n\t\t\tpos += snprintf(buf + pos, LEN_OR_ZERO,\n\t\t\t\t\t\", REC->%s\", field->name);\n\t}\n\n\treturn pos + 1;\n}\n#undef LEN_OR_ZERO\n\nstatic int user_event_create_print_fmt(struct user_event *user)\n{\n\tchar *print_fmt;\n\tint len;\n\n\tlen = user_event_set_print_fmt(user, NULL, 0);\n\n\tprint_fmt = kmalloc(len, GFP_KERNEL_ACCOUNT);\n\n\tif (!print_fmt)\n\t\treturn -ENOMEM;\n\n\tuser_event_set_print_fmt(user, print_fmt, len);\n\n\tuser->call.print_fmt = print_fmt;\n\n\treturn 0;\n}\n\nstatic enum print_line_t user_event_print_trace(struct trace_iterator *iter,\n\t\t\t\t\t\tint flags,\n\t\t\t\t\t\tstruct trace_event *event)\n{\n\treturn print_event_fields(iter, event);\n}\n\nstatic struct trace_event_functions user_event_funcs = {\n\t.trace = user_event_print_trace,\n};\n\nstatic int user_event_set_call_visible(struct user_event *user, bool visible)\n{\n\tint ret;\n\tconst struct cred *old_cred;\n\tstruct cred *cred;\n\n\tcred = prepare_creds();\n\n\tif (!cred)\n\t\treturn -ENOMEM;\n\n\t \n\tcred->fsuid = GLOBAL_ROOT_UID;\n\n\told_cred = override_creds(cred);\n\n\tif (visible)\n\t\tret = trace_add_event_call(&user->call);\n\telse\n\t\tret = trace_remove_event_call(&user->call);\n\n\trevert_creds(old_cred);\n\tput_cred(cred);\n\n\treturn ret;\n}\n\nstatic int destroy_user_event(struct user_event *user)\n{\n\tint ret = 0;\n\n\tlockdep_assert_held(&event_mutex);\n\n\t \n\tuser_event_destroy_fields(user);\n\n\tret = user_event_set_call_visible(user, false);\n\n\tif (ret)\n\t\treturn ret;\n\n\tdyn_event_remove(&user->devent);\n\thash_del(&user->node);\n\n\tuser_event_destroy_validators(user);\n\tkfree(user->call.print_fmt);\n\tkfree(EVENT_NAME(user));\n\tkfree(user);\n\n\tif (current_user_events > 0)\n\t\tcurrent_user_events--;\n\telse\n\t\tpr_alert(\"BUG: Bad current_user_events\\n\");\n\n\treturn ret;\n}\n\nstatic struct user_event *find_user_event(struct user_event_group *group,\n\t\t\t\t\t  char *name, u32 *outkey)\n{\n\tstruct user_event *user;\n\tu32 key = user_event_key(name);\n\n\t*outkey = key;\n\n\thash_for_each_possible(group->register_table, user, node, key)\n\t\tif (!strcmp(EVENT_NAME(user), name))\n\t\t\treturn user_event_get(user);\n\n\treturn NULL;\n}\n\nstatic int user_event_validate(struct user_event *user, void *data, int len)\n{\n\tstruct list_head *head = &user->validators;\n\tstruct user_event_validator *validator;\n\tvoid *pos, *end = data + len;\n\tu32 loc, offset, size;\n\n\tlist_for_each_entry(validator, head, user_event_link) {\n\t\tpos = data + validator->offset;\n\n\t\t \n\t\tloc = *(u32 *)pos;\n\t\toffset = loc & 0xffff;\n\t\tsize = loc >> 16;\n\n\t\tif (likely(validator->flags & VALIDATOR_REL))\n\t\t\tpos += offset + sizeof(loc);\n\t\telse\n\t\t\tpos = data + offset;\n\n\t\tpos += size;\n\n\t\tif (unlikely(pos > end))\n\t\t\treturn -EFAULT;\n\n\t\tif (likely(validator->flags & VALIDATOR_ENSURE_NULL))\n\t\t\tif (unlikely(*(char *)(pos - 1) != '\\0'))\n\t\t\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void user_event_ftrace(struct user_event *user, struct iov_iter *i,\n\t\t\t      void *tpdata, bool *faulted)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_entry *entry;\n\tstruct trace_event_buffer event_buffer;\n\tsize_t size = sizeof(*entry) + i->count;\n\n\tfile = (struct trace_event_file *)tpdata;\n\n\tif (!file ||\n\t    !(file->flags & EVENT_FILE_FL_ENABLED) ||\n\t    trace_trigger_soft_disabled(file))\n\t\treturn;\n\n\t \n\tentry = trace_event_buffer_reserve(&event_buffer, file, size);\n\n\tif (unlikely(!entry))\n\t\treturn;\n\n\tif (unlikely(i->count != 0 && !copy_nofault(entry + 1, i->count, i)))\n\t\tgoto discard;\n\n\tif (!list_empty(&user->validators) &&\n\t    unlikely(user_event_validate(user, entry, size)))\n\t\tgoto discard;\n\n\ttrace_event_buffer_commit(&event_buffer);\n\n\treturn;\ndiscard:\n\t*faulted = true;\n\t__trace_event_discard_commit(event_buffer.buffer,\n\t\t\t\t     event_buffer.event);\n}\n\n#ifdef CONFIG_PERF_EVENTS\n \nstatic void user_event_perf(struct user_event *user, struct iov_iter *i,\n\t\t\t    void *tpdata, bool *faulted)\n{\n\tstruct hlist_head *perf_head;\n\n\tperf_head = this_cpu_ptr(user->call.perf_events);\n\n\tif (perf_head && !hlist_empty(perf_head)) {\n\t\tstruct trace_entry *perf_entry;\n\t\tstruct pt_regs *regs;\n\t\tsize_t size = sizeof(*perf_entry) + i->count;\n\t\tint context;\n\n\t\tperf_entry = perf_trace_buf_alloc(ALIGN(size, 8),\n\t\t\t\t\t\t  &regs, &context);\n\n\t\tif (unlikely(!perf_entry))\n\t\t\treturn;\n\n\t\tperf_fetch_caller_regs(regs);\n\n\t\tif (unlikely(i->count != 0 && !copy_nofault(perf_entry + 1, i->count, i)))\n\t\t\tgoto discard;\n\n\t\tif (!list_empty(&user->validators) &&\n\t\t    unlikely(user_event_validate(user, perf_entry, size)))\n\t\t\tgoto discard;\n\n\t\tperf_trace_buf_submit(perf_entry, size, context,\n\t\t\t\t      user->call.event.type, 1, regs,\n\t\t\t\t      perf_head, NULL);\n\n\t\treturn;\ndiscard:\n\t\t*faulted = true;\n\t\tperf_swevent_put_recursion_context(context);\n\t}\n}\n#endif\n\n \nstatic void update_enable_bit_for(struct user_event *user)\n{\n\tstruct tracepoint *tp = &user->tracepoint;\n\tchar status = 0;\n\n\tif (atomic_read(&tp->key.enabled) > 0) {\n\t\tstruct tracepoint_func *probe_func_ptr;\n\t\tuser_event_func_t probe_func;\n\n\t\trcu_read_lock_sched();\n\n\t\tprobe_func_ptr = rcu_dereference_sched(tp->funcs);\n\n\t\tif (probe_func_ptr) {\n\t\t\tdo {\n\t\t\t\tprobe_func = probe_func_ptr->func;\n\n\t\t\t\tif (probe_func == user_event_ftrace)\n\t\t\t\t\tstatus |= EVENT_STATUS_FTRACE;\n#ifdef CONFIG_PERF_EVENTS\n\t\t\t\telse if (probe_func == user_event_perf)\n\t\t\t\t\tstatus |= EVENT_STATUS_PERF;\n#endif\n\t\t\t\telse\n\t\t\t\t\tstatus |= EVENT_STATUS_OTHER;\n\t\t\t} while ((++probe_func_ptr)->func);\n\t\t}\n\n\t\trcu_read_unlock_sched();\n\t}\n\n\tuser->status = status;\n\n\tuser_event_enabler_update(user);\n}\n\n \nstatic int user_event_reg(struct trace_event_call *call,\n\t\t\t  enum trace_reg type,\n\t\t\t  void *data)\n{\n\tstruct user_event *user = (struct user_event *)call->data;\n\tint ret = 0;\n\n\tif (!user)\n\t\treturn -ENOENT;\n\n\tswitch (type) {\n\tcase TRACE_REG_REGISTER:\n\t\tret = tracepoint_probe_register(call->tp,\n\t\t\t\t\t\tcall->class->probe,\n\t\t\t\t\t\tdata);\n\t\tif (!ret)\n\t\t\tgoto inc;\n\t\tbreak;\n\n\tcase TRACE_REG_UNREGISTER:\n\t\ttracepoint_probe_unregister(call->tp,\n\t\t\t\t\t    call->class->probe,\n\t\t\t\t\t    data);\n\t\tgoto dec;\n\n#ifdef CONFIG_PERF_EVENTS\n\tcase TRACE_REG_PERF_REGISTER:\n\t\tret = tracepoint_probe_register(call->tp,\n\t\t\t\t\t\tcall->class->perf_probe,\n\t\t\t\t\t\tdata);\n\t\tif (!ret)\n\t\t\tgoto inc;\n\t\tbreak;\n\n\tcase TRACE_REG_PERF_UNREGISTER:\n\t\ttracepoint_probe_unregister(call->tp,\n\t\t\t\t\t    call->class->perf_probe,\n\t\t\t\t\t    data);\n\t\tgoto dec;\n\n\tcase TRACE_REG_PERF_OPEN:\n\tcase TRACE_REG_PERF_CLOSE:\n\tcase TRACE_REG_PERF_ADD:\n\tcase TRACE_REG_PERF_DEL:\n\t\tbreak;\n#endif\n\t}\n\n\treturn ret;\ninc:\n\tuser_event_get(user);\n\tupdate_enable_bit_for(user);\n\treturn 0;\ndec:\n\tupdate_enable_bit_for(user);\n\tuser_event_put(user, true);\n\treturn 0;\n}\n\nstatic int user_event_create(const char *raw_command)\n{\n\tstruct user_event_group *group;\n\tstruct user_event *user;\n\tchar *name;\n\tint ret;\n\n\tif (!str_has_prefix(raw_command, USER_EVENTS_PREFIX))\n\t\treturn -ECANCELED;\n\n\traw_command += USER_EVENTS_PREFIX_LEN;\n\traw_command = skip_spaces(raw_command);\n\n\tname = kstrdup(raw_command, GFP_KERNEL_ACCOUNT);\n\n\tif (!name)\n\t\treturn -ENOMEM;\n\n\tgroup = current_user_event_group();\n\n\tif (!group) {\n\t\tkfree(name);\n\t\treturn -ENOENT;\n\t}\n\n\tmutex_lock(&group->reg_mutex);\n\n\t \n\tret = user_event_parse_cmd(group, name, &user, USER_EVENT_REG_PERSIST);\n\n\tif (!ret)\n\t\tuser_event_put(user, false);\n\n\tmutex_unlock(&group->reg_mutex);\n\n\tif (ret)\n\t\tkfree(name);\n\n\treturn ret;\n}\n\nstatic int user_event_show(struct seq_file *m, struct dyn_event *ev)\n{\n\tstruct user_event *user = container_of(ev, struct user_event, devent);\n\tstruct ftrace_event_field *field;\n\tstruct list_head *head;\n\tint depth = 0;\n\n\tseq_printf(m, \"%s%s\", USER_EVENTS_PREFIX, EVENT_NAME(user));\n\n\thead = trace_get_fields(&user->call);\n\n\tlist_for_each_entry_reverse(field, head, link) {\n\t\tif (depth == 0)\n\t\t\tseq_puts(m, \" \");\n\t\telse\n\t\t\tseq_puts(m, \"; \");\n\n\t\tseq_printf(m, \"%s %s\", field->type, field->name);\n\n\t\tif (str_has_prefix(field->type, \"struct \"))\n\t\t\tseq_printf(m, \" %d\", field->size);\n\n\t\tdepth++;\n\t}\n\n\tseq_puts(m, \"\\n\");\n\n\treturn 0;\n}\n\nstatic bool user_event_is_busy(struct dyn_event *ev)\n{\n\tstruct user_event *user = container_of(ev, struct user_event, devent);\n\n\treturn !user_event_last_ref(user);\n}\n\nstatic int user_event_free(struct dyn_event *ev)\n{\n\tstruct user_event *user = container_of(ev, struct user_event, devent);\n\n\tif (!user_event_last_ref(user))\n\t\treturn -EBUSY;\n\n\treturn destroy_user_event(user);\n}\n\nstatic bool user_field_match(struct ftrace_event_field *field, int argc,\n\t\t\t     const char **argv, int *iout)\n{\n\tchar *field_name = NULL, *dyn_field_name = NULL;\n\tbool colon = false, match = false;\n\tint dyn_len, len;\n\n\tif (*iout >= argc)\n\t\treturn false;\n\n\tdyn_len = user_dyn_field_set_string(argc, argv, iout, dyn_field_name,\n\t\t\t\t\t    0, &colon);\n\n\tlen = user_field_set_string(field, field_name, 0, colon);\n\n\tif (dyn_len != len)\n\t\treturn false;\n\n\tdyn_field_name = kmalloc(dyn_len, GFP_KERNEL);\n\tfield_name = kmalloc(len, GFP_KERNEL);\n\n\tif (!dyn_field_name || !field_name)\n\t\tgoto out;\n\n\tuser_dyn_field_set_string(argc, argv, iout, dyn_field_name,\n\t\t\t\t  dyn_len, &colon);\n\n\tuser_field_set_string(field, field_name, len, colon);\n\n\tmatch = strcmp(dyn_field_name, field_name) == 0;\nout:\n\tkfree(dyn_field_name);\n\tkfree(field_name);\n\n\treturn match;\n}\n\nstatic bool user_fields_match(struct user_event *user, int argc,\n\t\t\t      const char **argv)\n{\n\tstruct ftrace_event_field *field;\n\tstruct list_head *head = &user->fields;\n\tint i = 0;\n\n\tlist_for_each_entry_reverse(field, head, link) {\n\t\tif (!user_field_match(field, argc, argv, &i))\n\t\t\treturn false;\n\t}\n\n\tif (i != argc)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool user_event_match(const char *system, const char *event,\n\t\t\t     int argc, const char **argv, struct dyn_event *ev)\n{\n\tstruct user_event *user = container_of(ev, struct user_event, devent);\n\tbool match;\n\n\tmatch = strcmp(EVENT_NAME(user), event) == 0 &&\n\t\t(!system || strcmp(system, USER_EVENTS_SYSTEM) == 0);\n\n\tif (match && argc > 0)\n\t\tmatch = user_fields_match(user, argc, argv);\n\telse if (match && argc == 0)\n\t\tmatch = list_empty(&user->fields);\n\n\treturn match;\n}\n\nstatic struct dyn_event_operations user_event_dops = {\n\t.create = user_event_create,\n\t.show = user_event_show,\n\t.is_busy = user_event_is_busy,\n\t.free = user_event_free,\n\t.match = user_event_match,\n};\n\nstatic int user_event_trace_register(struct user_event *user)\n{\n\tint ret;\n\n\tret = register_trace_event(&user->call.event);\n\n\tif (!ret)\n\t\treturn -ENODEV;\n\n\tret = user_event_set_call_visible(user, true);\n\n\tif (ret)\n\t\tunregister_trace_event(&user->call.event);\n\n\treturn ret;\n}\n\n \nstatic int user_event_parse(struct user_event_group *group, char *name,\n\t\t\t    char *args, char *flags,\n\t\t\t    struct user_event **newuser, int reg_flags)\n{\n\tint ret;\n\tu32 key;\n\tstruct user_event *user;\n\tint argc = 0;\n\tchar **argv;\n\n\t \n\tif (reg_flags != 0 || flags != NULL)\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&event_mutex);\n\tuser = find_user_event(group, name, &key);\n\tmutex_unlock(&event_mutex);\n\n\tif (user) {\n\t\tif (args) {\n\t\t\targv = argv_split(GFP_KERNEL, args, &argc);\n\t\t\tif (!argv) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto error;\n\t\t\t}\n\n\t\t\tret = user_fields_match(user, argc, (const char **)argv);\n\t\t\targv_free(argv);\n\n\t\t} else\n\t\t\tret = list_empty(&user->fields);\n\n\t\tif (ret) {\n\t\t\t*newuser = user;\n\t\t\t \n\t\t\tkfree(name);\n\t\t} else {\n\t\t\tret = -EADDRINUSE;\n\t\t\tgoto error;\n\t\t}\n\n\t\treturn 0;\nerror:\n\t\tuser_event_put(user, false);\n\t\treturn ret;\n\t}\n\n\tuser = kzalloc(sizeof(*user), GFP_KERNEL_ACCOUNT);\n\n\tif (!user)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&user->class.fields);\n\tINIT_LIST_HEAD(&user->fields);\n\tINIT_LIST_HEAD(&user->validators);\n\n\tuser->group = group;\n\tuser->tracepoint.name = name;\n\n\tret = user_event_parse_fields(user, args);\n\n\tif (ret)\n\t\tgoto put_user;\n\n\tret = user_event_create_print_fmt(user);\n\n\tif (ret)\n\t\tgoto put_user;\n\n\tuser->call.data = user;\n\tuser->call.class = &user->class;\n\tuser->call.name = name;\n\tuser->call.flags = TRACE_EVENT_FL_TRACEPOINT;\n\tuser->call.tp = &user->tracepoint;\n\tuser->call.event.funcs = &user_event_funcs;\n\tuser->class.system = group->system_name;\n\n\tuser->class.fields_array = user_event_fields_array;\n\tuser->class.get_fields = user_event_get_fields;\n\tuser->class.reg = user_event_reg;\n\tuser->class.probe = user_event_ftrace;\n#ifdef CONFIG_PERF_EVENTS\n\tuser->class.perf_probe = user_event_perf;\n#endif\n\n\tmutex_lock(&event_mutex);\n\n\tif (current_user_events >= max_user_events) {\n\t\tret = -EMFILE;\n\t\tgoto put_user_lock;\n\t}\n\n\tret = user_event_trace_register(user);\n\n\tif (ret)\n\t\tgoto put_user_lock;\n\n\tuser->reg_flags = reg_flags;\n\n\tif (user->reg_flags & USER_EVENT_REG_PERSIST) {\n\t\t \n\t\trefcount_set(&user->refcnt, 2);\n\t} else {\n\t\t \n\t\trefcount_set(&user->refcnt, 1);\n\t}\n\n\tdyn_event_init(&user->devent, &user_event_dops);\n\tdyn_event_add(&user->devent, &user->call);\n\thash_add(group->register_table, &user->node, key);\n\tcurrent_user_events++;\n\n\tmutex_unlock(&event_mutex);\n\n\t*newuser = user;\n\treturn 0;\nput_user_lock:\n\tmutex_unlock(&event_mutex);\nput_user:\n\tuser_event_destroy_fields(user);\n\tuser_event_destroy_validators(user);\n\tkfree(user->call.print_fmt);\n\tkfree(user);\n\treturn ret;\n}\n\n \nstatic int delete_user_event(struct user_event_group *group, char *name)\n{\n\tu32 key;\n\tstruct user_event *user = find_user_event(group, name, &key);\n\n\tif (!user)\n\t\treturn -ENOENT;\n\n\tuser_event_put(user, true);\n\n\tif (!user_event_last_ref(user))\n\t\treturn -EBUSY;\n\n\treturn destroy_user_event(user);\n}\n\n \nstatic ssize_t user_events_write_core(struct file *file, struct iov_iter *i)\n{\n\tstruct user_event_file_info *info = file->private_data;\n\tstruct user_event_refs *refs;\n\tstruct user_event *user = NULL;\n\tstruct tracepoint *tp;\n\tssize_t ret = i->count;\n\tint idx;\n\n\tif (unlikely(copy_from_iter(&idx, sizeof(idx), i) != sizeof(idx)))\n\t\treturn -EFAULT;\n\n\tif (idx < 0)\n\t\treturn -EINVAL;\n\n\trcu_read_lock_sched();\n\n\trefs = rcu_dereference_sched(info->refs);\n\n\t \n\tif (likely(refs && idx < refs->count))\n\t\tuser = refs->events[idx];\n\n\trcu_read_unlock_sched();\n\n\tif (unlikely(user == NULL))\n\t\treturn -ENOENT;\n\n\tif (unlikely(i->count < user->min_size))\n\t\treturn -EINVAL;\n\n\ttp = &user->tracepoint;\n\n\t \n\tif (likely(atomic_read(&tp->key.enabled) > 0)) {\n\t\tstruct tracepoint_func *probe_func_ptr;\n\t\tuser_event_func_t probe_func;\n\t\tstruct iov_iter copy;\n\t\tvoid *tpdata;\n\t\tbool faulted;\n\n\t\tif (unlikely(fault_in_iov_iter_readable(i, i->count)))\n\t\t\treturn -EFAULT;\n\n\t\tfaulted = false;\n\n\t\trcu_read_lock_sched();\n\n\t\tprobe_func_ptr = rcu_dereference_sched(tp->funcs);\n\n\t\tif (probe_func_ptr) {\n\t\t\tdo {\n\t\t\t\tcopy = *i;\n\t\t\t\tprobe_func = probe_func_ptr->func;\n\t\t\t\ttpdata = probe_func_ptr->data;\n\t\t\t\tprobe_func(user, &copy, tpdata, &faulted);\n\t\t\t} while ((++probe_func_ptr)->func);\n\t\t}\n\n\t\trcu_read_unlock_sched();\n\n\t\tif (unlikely(faulted))\n\t\t\treturn -EFAULT;\n\t} else\n\t\treturn -EBADF;\n\n\treturn ret;\n}\n\nstatic int user_events_open(struct inode *node, struct file *file)\n{\n\tstruct user_event_group *group;\n\tstruct user_event_file_info *info;\n\n\tgroup = current_user_event_group();\n\n\tif (!group)\n\t\treturn -ENOENT;\n\n\tinfo = kzalloc(sizeof(*info), GFP_KERNEL_ACCOUNT);\n\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tinfo->group = group;\n\n\tfile->private_data = info;\n\n\treturn 0;\n}\n\nstatic ssize_t user_events_write(struct file *file, const char __user *ubuf,\n\t\t\t\t size_t count, loff_t *ppos)\n{\n\tstruct iovec iov;\n\tstruct iov_iter i;\n\n\tif (unlikely(*ppos != 0))\n\t\treturn -EFAULT;\n\n\tif (unlikely(import_single_range(ITER_SOURCE, (char __user *)ubuf,\n\t\t\t\t\t count, &iov, &i)))\n\t\treturn -EFAULT;\n\n\treturn user_events_write_core(file, &i);\n}\n\nstatic ssize_t user_events_write_iter(struct kiocb *kp, struct iov_iter *i)\n{\n\treturn user_events_write_core(kp->ki_filp, i);\n}\n\nstatic int user_events_ref_add(struct user_event_file_info *info,\n\t\t\t       struct user_event *user)\n{\n\tstruct user_event_group *group = info->group;\n\tstruct user_event_refs *refs, *new_refs;\n\tint i, size, count = 0;\n\n\trefs = rcu_dereference_protected(info->refs,\n\t\t\t\t\t lockdep_is_held(&group->reg_mutex));\n\n\tif (refs) {\n\t\tcount = refs->count;\n\n\t\tfor (i = 0; i < count; ++i)\n\t\t\tif (refs->events[i] == user)\n\t\t\t\treturn i;\n\t}\n\n\tsize = struct_size(refs, events, count + 1);\n\n\tnew_refs = kzalloc(size, GFP_KERNEL_ACCOUNT);\n\n\tif (!new_refs)\n\t\treturn -ENOMEM;\n\n\tnew_refs->count = count + 1;\n\n\tfor (i = 0; i < count; ++i)\n\t\tnew_refs->events[i] = refs->events[i];\n\n\tnew_refs->events[i] = user_event_get(user);\n\n\trcu_assign_pointer(info->refs, new_refs);\n\n\tif (refs)\n\t\tkfree_rcu(refs, rcu);\n\n\treturn i;\n}\n\nstatic long user_reg_get(struct user_reg __user *ureg, struct user_reg *kreg)\n{\n\tu32 size;\n\tlong ret;\n\n\tret = get_user(size, &ureg->size);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (size > PAGE_SIZE)\n\t\treturn -E2BIG;\n\n\tif (size < offsetofend(struct user_reg, write_index))\n\t\treturn -EINVAL;\n\n\tret = copy_struct_from_user(kreg, sizeof(*kreg), ureg, size);\n\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (kreg->flags & ~(USER_EVENT_REG_MAX-1))\n\t\treturn -EINVAL;\n\n\t \n\tswitch (kreg->enable_size) {\n\tcase 4:\n\t\t \n\t\tbreak;\n#if BITS_PER_LONG >= 64\n\tcase 8:\n\t\t \n\t\tbreak;\n#endif\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (kreg->enable_addr % kreg->enable_size)\n\t\treturn -EINVAL;\n\n\t \n\tif (kreg->enable_bit > (kreg->enable_size * BITS_PER_BYTE) - 1)\n\t\treturn -EINVAL;\n\n\t \n\tif (!access_ok((const void __user *)(uintptr_t)kreg->enable_addr,\n\t\t       kreg->enable_size))\n\t\treturn -EFAULT;\n\n\tkreg->size = size;\n\n\treturn 0;\n}\n\n \nstatic long user_events_ioctl_reg(struct user_event_file_info *info,\n\t\t\t\t  unsigned long uarg)\n{\n\tstruct user_reg __user *ureg = (struct user_reg __user *)uarg;\n\tstruct user_reg reg;\n\tstruct user_event *user;\n\tstruct user_event_enabler *enabler;\n\tchar *name;\n\tlong ret;\n\tint write_result;\n\n\tret = user_reg_get(ureg, &reg);\n\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (current_user_event_enabler_exists((unsigned long)reg.enable_addr,\n\t\t\t\t\t      reg.enable_bit))\n\t\treturn -EADDRINUSE;\n\n\tname = strndup_user((const char __user *)(uintptr_t)reg.name_args,\n\t\t\t    MAX_EVENT_DESC);\n\n\tif (IS_ERR(name)) {\n\t\tret = PTR_ERR(name);\n\t\treturn ret;\n\t}\n\n\tret = user_event_parse_cmd(info->group, name, &user, reg.flags);\n\n\tif (ret) {\n\t\tkfree(name);\n\t\treturn ret;\n\t}\n\n\tret = user_events_ref_add(info, user);\n\n\t \n\tuser_event_put(user, false);\n\n\t \n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tenabler = user_event_enabler_create(&reg, user, &write_result);\n\n\tif (!enabler)\n\t\treturn -ENOMEM;\n\n\t \n\tif (write_result)\n\t\treturn write_result;\n\n\tput_user((u32)ret, &ureg->write_index);\n\n\treturn 0;\n}\n\n \nstatic long user_events_ioctl_del(struct user_event_file_info *info,\n\t\t\t\t  unsigned long uarg)\n{\n\tvoid __user *ubuf = (void __user *)uarg;\n\tchar *name;\n\tlong ret;\n\n\tname = strndup_user(ubuf, MAX_EVENT_DESC);\n\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\n\t \n\tmutex_lock(&event_mutex);\n\tret = delete_user_event(info->group, name);\n\tmutex_unlock(&event_mutex);\n\n\tkfree(name);\n\n\treturn ret;\n}\n\nstatic long user_unreg_get(struct user_unreg __user *ureg,\n\t\t\t   struct user_unreg *kreg)\n{\n\tu32 size;\n\tlong ret;\n\n\tret = get_user(size, &ureg->size);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (size > PAGE_SIZE)\n\t\treturn -E2BIG;\n\n\tif (size < offsetofend(struct user_unreg, disable_addr))\n\t\treturn -EINVAL;\n\n\tret = copy_struct_from_user(kreg, sizeof(*kreg), ureg, size);\n\n\t \n\tif (kreg->__reserved || kreg->__reserved2)\n\t\treturn -EINVAL;\n\n\treturn ret;\n}\n\nstatic int user_event_mm_clear_bit(struct user_event_mm *user_mm,\n\t\t\t\t   unsigned long uaddr, unsigned char bit,\n\t\t\t\t   unsigned long flags)\n{\n\tstruct user_event_enabler enabler;\n\tint result;\n\tint attempt = 0;\n\n\tmemset(&enabler, 0, sizeof(enabler));\n\tenabler.addr = uaddr;\n\tenabler.values = bit | flags;\nretry:\n\t \n\tmutex_lock(&event_mutex);\n\n\t \n\tmmap_read_lock(user_mm->mm);\n\tresult = user_event_enabler_write(user_mm, &enabler, false, &attempt);\n\tmmap_read_unlock(user_mm->mm);\n\n\tmutex_unlock(&event_mutex);\n\n\tif (result) {\n\t\t \n\t\tif (!user_event_mm_fault_in(user_mm, uaddr, attempt))\n\t\t\tgoto retry;\n\t}\n\n\treturn result;\n}\n\n \nstatic long user_events_ioctl_unreg(unsigned long uarg)\n{\n\tstruct user_unreg __user *ureg = (struct user_unreg __user *)uarg;\n\tstruct user_event_mm *mm = current->user_event_mm;\n\tstruct user_event_enabler *enabler, *next;\n\tstruct user_unreg reg;\n\tunsigned long flags;\n\tlong ret;\n\n\tret = user_unreg_get(ureg, &reg);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (!mm)\n\t\treturn -ENOENT;\n\n\tflags = 0;\n\tret = -ENOENT;\n\n\t \n\tmutex_lock(&event_mutex);\n\n\tlist_for_each_entry_safe(enabler, next, &mm->enablers, mm_enablers_link) {\n\t\tif (enabler->addr == reg.disable_addr &&\n\t\t    ENABLE_BIT(enabler) == reg.disable_bit) {\n\t\t\tset_bit(ENABLE_VAL_FREEING_BIT, ENABLE_BITOPS(enabler));\n\n\t\t\t \n\t\t\tflags |= enabler->values & ENABLE_VAL_COMPAT_MASK;\n\n\t\t\tif (!test_bit(ENABLE_VAL_FAULTING_BIT, ENABLE_BITOPS(enabler)))\n\t\t\t\tuser_event_enabler_destroy(enabler, true);\n\n\t\t\t \n\t\t\tret = 0;\n\t\t}\n\t}\n\n\tmutex_unlock(&event_mutex);\n\n\t \n\tif (!ret)\n\t\tret = user_event_mm_clear_bit(mm, reg.disable_addr,\n\t\t\t\t\t      reg.disable_bit, flags);\n\n\treturn ret;\n}\n\n \nstatic long user_events_ioctl(struct file *file, unsigned int cmd,\n\t\t\t      unsigned long uarg)\n{\n\tstruct user_event_file_info *info = file->private_data;\n\tstruct user_event_group *group = info->group;\n\tlong ret = -ENOTTY;\n\n\tswitch (cmd) {\n\tcase DIAG_IOCSREG:\n\t\tmutex_lock(&group->reg_mutex);\n\t\tret = user_events_ioctl_reg(info, uarg);\n\t\tmutex_unlock(&group->reg_mutex);\n\t\tbreak;\n\n\tcase DIAG_IOCSDEL:\n\t\tmutex_lock(&group->reg_mutex);\n\t\tret = user_events_ioctl_del(info, uarg);\n\t\tmutex_unlock(&group->reg_mutex);\n\t\tbreak;\n\n\tcase DIAG_IOCSUNREG:\n\t\tmutex_lock(&group->reg_mutex);\n\t\tret = user_events_ioctl_unreg(uarg);\n\t\tmutex_unlock(&group->reg_mutex);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int user_events_release(struct inode *node, struct file *file)\n{\n\tstruct user_event_file_info *info = file->private_data;\n\tstruct user_event_group *group;\n\tstruct user_event_refs *refs;\n\tint i;\n\n\tif (!info)\n\t\treturn -EINVAL;\n\n\tgroup = info->group;\n\n\t \n\tmutex_lock(&group->reg_mutex);\n\n\trefs = info->refs;\n\n\tif (!refs)\n\t\tgoto out;\n\n\t \n\tfor (i = 0; i < refs->count; ++i)\n\t\tuser_event_put(refs->events[i], false);\n\nout:\n\tfile->private_data = NULL;\n\n\tmutex_unlock(&group->reg_mutex);\n\n\tkfree(refs);\n\tkfree(info);\n\n\treturn 0;\n}\n\nstatic const struct file_operations user_data_fops = {\n\t.open\t\t= user_events_open,\n\t.write\t\t= user_events_write,\n\t.write_iter\t= user_events_write_iter,\n\t.unlocked_ioctl\t= user_events_ioctl,\n\t.release\t= user_events_release,\n};\n\nstatic void *user_seq_start(struct seq_file *m, loff_t *pos)\n{\n\tif (*pos)\n\t\treturn NULL;\n\n\treturn (void *)1;\n}\n\nstatic void *user_seq_next(struct seq_file *m, void *p, loff_t *pos)\n{\n\t++*pos;\n\treturn NULL;\n}\n\nstatic void user_seq_stop(struct seq_file *m, void *p)\n{\n}\n\nstatic int user_seq_show(struct seq_file *m, void *p)\n{\n\tstruct user_event_group *group = m->private;\n\tstruct user_event *user;\n\tchar status;\n\tint i, active = 0, busy = 0;\n\n\tif (!group)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&group->reg_mutex);\n\n\thash_for_each(group->register_table, i, user, node) {\n\t\tstatus = user->status;\n\n\t\tseq_printf(m, \"%s\", EVENT_NAME(user));\n\n\t\tif (status != 0)\n\t\t\tseq_puts(m, \" #\");\n\n\t\tif (status != 0) {\n\t\t\tseq_puts(m, \" Used by\");\n\t\t\tif (status & EVENT_STATUS_FTRACE)\n\t\t\t\tseq_puts(m, \" ftrace\");\n\t\t\tif (status & EVENT_STATUS_PERF)\n\t\t\t\tseq_puts(m, \" perf\");\n\t\t\tif (status & EVENT_STATUS_OTHER)\n\t\t\t\tseq_puts(m, \" other\");\n\t\t\tbusy++;\n\t\t}\n\n\t\tseq_puts(m, \"\\n\");\n\t\tactive++;\n\t}\n\n\tmutex_unlock(&group->reg_mutex);\n\n\tseq_puts(m, \"\\n\");\n\tseq_printf(m, \"Active: %d\\n\", active);\n\tseq_printf(m, \"Busy: %d\\n\", busy);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations user_seq_ops = {\n\t.start\t= user_seq_start,\n\t.next\t= user_seq_next,\n\t.stop\t= user_seq_stop,\n\t.show\t= user_seq_show,\n};\n\nstatic int user_status_open(struct inode *node, struct file *file)\n{\n\tstruct user_event_group *group;\n\tint ret;\n\n\tgroup = current_user_event_group();\n\n\tif (!group)\n\t\treturn -ENOENT;\n\n\tret = seq_open(file, &user_seq_ops);\n\n\tif (!ret) {\n\t\t \n\t\tstruct seq_file *m = file->private_data;\n\n\t\tm->private = group;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct file_operations user_status_fops = {\n\t.open\t\t= user_status_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\n \nstatic int create_user_tracefs(void)\n{\n\tstruct dentry *edata, *emmap;\n\n\tedata = tracefs_create_file(\"user_events_data\", TRACE_MODE_WRITE,\n\t\t\t\t    NULL, NULL, &user_data_fops);\n\n\tif (!edata) {\n\t\tpr_warn(\"Could not create tracefs 'user_events_data' entry\\n\");\n\t\tgoto err;\n\t}\n\n\temmap = tracefs_create_file(\"user_events_status\", TRACE_MODE_READ,\n\t\t\t\t    NULL, NULL, &user_status_fops);\n\n\tif (!emmap) {\n\t\ttracefs_remove(edata);\n\t\tpr_warn(\"Could not create tracefs 'user_events_mmap' entry\\n\");\n\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\treturn -ENODEV;\n}\n\nstatic int set_max_user_events_sysctl(struct ctl_table *table, int write,\n\t\t\t\t      void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\n\tret = proc_douintvec(table, write, buffer, lenp, ppos);\n\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nstatic struct ctl_table user_event_sysctls[] = {\n\t{\n\t\t.procname\t= \"user_events_max\",\n\t\t.data\t\t= &max_user_events,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= set_max_user_events_sysctl,\n\t},\n\t{}\n};\n\nstatic int __init trace_events_user_init(void)\n{\n\tint ret;\n\n\tfault_cache = KMEM_CACHE(user_event_enabler_fault, 0);\n\n\tif (!fault_cache)\n\t\treturn -ENOMEM;\n\n\tinit_group = user_event_group_create();\n\n\tif (!init_group) {\n\t\tkmem_cache_destroy(fault_cache);\n\t\treturn -ENOMEM;\n\t}\n\n\tret = create_user_tracefs();\n\n\tif (ret) {\n\t\tpr_warn(\"user_events could not register with tracefs\\n\");\n\t\tuser_event_group_destroy(init_group);\n\t\tkmem_cache_destroy(fault_cache);\n\t\tinit_group = NULL;\n\t\treturn ret;\n\t}\n\n\tif (dyn_event_register(&user_event_dops))\n\t\tpr_warn(\"user_events could not register with dyn_events\\n\");\n\n\tregister_sysctl_init(\"kernel\", user_event_sysctls);\n\n\treturn 0;\n}\n\nfs_initcall(trace_events_user_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}