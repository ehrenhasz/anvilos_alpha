{
  "module_name": "fgraph.c",
  "hash_id": "21099565f2cbbf97e78fc2a9c666248f7a7603fac99f74646cb9c4b1acced36c",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/fgraph.c",
  "human_readable_source": "\n \n#include <linux/jump_label.h>\n#include <linux/suspend.h>\n#include <linux/ftrace.h>\n#include <linux/slab.h>\n\n#include <trace/events/sched.h>\n\n#include \"ftrace_internal.h\"\n#include \"trace.h\"\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n#define ASSIGN_OPS_HASH(opsname, val) \\\n\t.func_hash\t\t= val, \\\n\t.local_hash.regex_lock\t= __MUTEX_INITIALIZER(opsname.local_hash.regex_lock),\n#else\n#define ASSIGN_OPS_HASH(opsname, val)\n#endif\n\nDEFINE_STATIC_KEY_FALSE(kill_ftrace_graph);\nint ftrace_graph_active;\n\n \nstatic bool fgraph_sleep_time = true;\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n \nint __weak ftrace_enable_ftrace_graph_caller(void)\n{\n\treturn 0;\n}\n\n \nint __weak ftrace_disable_ftrace_graph_caller(void)\n{\n\treturn 0;\n}\n#endif\n\n \nvoid ftrace_graph_stop(void)\n{\n\tstatic_branch_enable(&kill_ftrace_graph);\n}\n\n \nstatic int\nftrace_push_return_trace(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tunsigned long long calltime;\n\tint index;\n\n\tif (unlikely(ftrace_graph_is_dead()))\n\t\treturn -EBUSY;\n\n\tif (!current->ret_stack)\n\t\treturn -EBUSY;\n\n\t \n\tsmp_rmb();\n\n\t \n\tif (current->curr_ret_stack == FTRACE_RETFUNC_DEPTH - 1) {\n\t\tatomic_inc(&current->trace_overrun);\n\t\treturn -EBUSY;\n\t}\n\n\tcalltime = trace_clock_local();\n\n\tindex = ++current->curr_ret_stack;\n\tbarrier();\n\tcurrent->ret_stack[index].ret = ret;\n\tcurrent->ret_stack[index].func = func;\n\tcurrent->ret_stack[index].calltime = calltime;\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\tcurrent->ret_stack[index].fp = frame_pointer;\n#endif\n#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR\n\tcurrent->ret_stack[index].retp = retp;\n#endif\n\treturn 0;\n}\n\n \n#ifndef MCOUNT_INSN_SIZE\n \n# ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\n#  error MCOUNT_INSN_SIZE not defined with direct calls enabled\n# endif\n# define MCOUNT_INSN_SIZE 0\n#endif\n\nint function_graph_enter(unsigned long ret, unsigned long func,\n\t\t\t unsigned long frame_pointer, unsigned long *retp)\n{\n\tstruct ftrace_graph_ent trace;\n\n#ifndef CONFIG_HAVE_DYNAMIC_FTRACE_WITH_ARGS\n\t \n\tif (ftrace_direct_func_count &&\n\t    ftrace_find_rec_direct(ret - MCOUNT_INSN_SIZE))\n\t\treturn -EBUSY;\n#endif\n\ttrace.func = func;\n\ttrace.depth = ++current->curr_ret_depth;\n\n\tif (ftrace_push_return_trace(ret, func, frame_pointer, retp))\n\t\tgoto out;\n\n\t \n\tif (!ftrace_graph_entry(&trace))\n\t\tgoto out_ret;\n\n\treturn 0;\n out_ret:\n\tcurrent->curr_ret_stack--;\n out:\n\tcurrent->curr_ret_depth--;\n\treturn -EBUSY;\n}\n\n \nstatic void\nftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,\n\t\t\tunsigned long frame_pointer)\n{\n\tint index;\n\n\tindex = current->curr_ret_stack;\n\n\tif (unlikely(index < 0 || index >= FTRACE_RETFUNC_DEPTH)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t \n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n\n#ifdef HAVE_FUNCTION_GRAPH_FP_TEST\n\t \n\tif (unlikely(current->ret_stack[index].fp != frame_pointer)) {\n\t\tftrace_graph_stop();\n\t\tWARN(1, \"Bad frame pointer: expected %lx, received %lx\\n\"\n\t\t     \"  from func %ps return to %lx\\n\",\n\t\t     current->ret_stack[index].fp,\n\t\t     frame_pointer,\n\t\t     (void *)current->ret_stack[index].func,\n\t\t     current->ret_stack[index].ret);\n\t\t*ret = (unsigned long)panic;\n\t\treturn;\n\t}\n#endif\n\n\t*ret = current->ret_stack[index].ret;\n\ttrace->func = current->ret_stack[index].func;\n\ttrace->calltime = current->ret_stack[index].calltime;\n\ttrace->overrun = atomic_read(&current->trace_overrun);\n\ttrace->depth = current->curr_ret_depth--;\n\t \n\tbarrier();\n}\n\n \nstatic int\nftrace_suspend_notifier_call(struct notifier_block *bl, unsigned long state,\n\t\t\t\t\t\t\tvoid *unused)\n{\n\tswitch (state) {\n\tcase PM_HIBERNATION_PREPARE:\n\t\tpause_graph_tracing();\n\t\tbreak;\n\n\tcase PM_POST_HIBERNATION:\n\t\tunpause_graph_tracing();\n\t\tbreak;\n\t}\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block ftrace_suspend_notifier = {\n\t.notifier_call = ftrace_suspend_notifier_call,\n};\n\n \nstruct fgraph_ret_regs;\n\n \nstatic unsigned long __ftrace_return_to_handler(struct fgraph_ret_regs *ret_regs,\n\t\t\t\t\t\tunsigned long frame_pointer)\n{\n\tstruct ftrace_graph_ret trace;\n\tunsigned long ret;\n\n\tftrace_pop_return_trace(&trace, &ret, frame_pointer);\n#ifdef CONFIG_FUNCTION_GRAPH_RETVAL\n\ttrace.retval = fgraph_ret_regs_return_value(ret_regs);\n#endif\n\ttrace.rettime = trace_clock_local();\n\tftrace_graph_return(&trace);\n\t \n\tbarrier();\n\tcurrent->curr_ret_stack--;\n\n\tif (unlikely(!ret)) {\n\t\tftrace_graph_stop();\n\t\tWARN_ON(1);\n\t\t \n\t\tret = (unsigned long)panic;\n\t}\n\n\treturn ret;\n}\n\n \n#ifdef CONFIG_HAVE_FUNCTION_GRAPH_RETVAL\nunsigned long ftrace_return_to_handler(struct fgraph_ret_regs *ret_regs)\n{\n\treturn __ftrace_return_to_handler(ret_regs,\n\t\t\t\tfgraph_ret_regs_frame_pointer(ret_regs));\n}\n#else\nunsigned long ftrace_return_to_handler(unsigned long frame_pointer)\n{\n\treturn __ftrace_return_to_handler(NULL, frame_pointer);\n}\n#endif\n\n \nstruct ftrace_ret_stack *\nftrace_graph_get_ret_stack(struct task_struct *task, int idx)\n{\n\tidx = task->curr_ret_stack - idx;\n\n\tif (idx >= 0 && idx <= task->curr_ret_stack)\n\t\treturn &task->ret_stack[idx];\n\n\treturn NULL;\n}\n\n \n#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR\nunsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,\n\t\t\t\t    unsigned long ret, unsigned long *retp)\n{\n\tint index = task->curr_ret_stack;\n\tint i;\n\n\tif (ret != (unsigned long)dereference_kernel_function_descriptor(return_to_handler))\n\t\treturn ret;\n\n\tif (index < 0)\n\t\treturn ret;\n\n\tfor (i = 0; i <= index; i++)\n\t\tif (task->ret_stack[i].retp == retp)\n\t\t\treturn task->ret_stack[i].ret;\n\n\treturn ret;\n}\n#else  \nunsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,\n\t\t\t\t    unsigned long ret, unsigned long *retp)\n{\n\tint task_idx;\n\n\tif (ret != (unsigned long)dereference_kernel_function_descriptor(return_to_handler))\n\t\treturn ret;\n\n\ttask_idx = task->curr_ret_stack;\n\n\tif (!task->ret_stack || task_idx < *idx)\n\t\treturn ret;\n\n\ttask_idx -= *idx;\n\t(*idx)++;\n\n\treturn task->ret_stack[task_idx].ret;\n}\n#endif  \n\nstatic struct ftrace_ops graph_ops = {\n\t.func\t\t\t= ftrace_graph_func,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t   FTRACE_OPS_FL_PID |\n\t\t\t\t   FTRACE_OPS_GRAPH_STUB,\n#ifdef FTRACE_GRAPH_TRAMP_ADDR\n\t.trampoline\t\t= FTRACE_GRAPH_TRAMP_ADDR,\n\t \n#endif\n\tASSIGN_OPS_HASH(graph_ops, &global_ops.local_hash)\n};\n\nvoid ftrace_graph_sleep_time_control(bool enable)\n{\n\tfgraph_sleep_time = enable;\n}\n\nint ftrace_graph_entry_stub(struct ftrace_graph_ent *trace)\n{\n\treturn 0;\n}\n\n \nextern void ftrace_stub_graph(struct ftrace_graph_ret *);\n\n \ntrace_func_graph_ret_t ftrace_graph_return = ftrace_stub_graph;\ntrace_func_graph_ent_t ftrace_graph_entry = ftrace_graph_entry_stub;\nstatic trace_func_graph_ent_t __ftrace_graph_entry = ftrace_graph_entry_stub;\n\n \nstatic int alloc_retstack_tasklist(struct ftrace_ret_stack **ret_stack_list)\n{\n\tint i;\n\tint ret = 0;\n\tint start = 0, end = FTRACE_RETSTACK_ALLOC_SIZE;\n\tstruct task_struct *g, *t;\n\n\tfor (i = 0; i < FTRACE_RETSTACK_ALLOC_SIZE; i++) {\n\t\tret_stack_list[i] =\n\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!ret_stack_list[i]) {\n\t\t\tstart = 0;\n\t\t\tend = i;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t}\n\n\trcu_read_lock();\n\tfor_each_process_thread(g, t) {\n\t\tif (start == end) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (t->ret_stack == NULL) {\n\t\t\tatomic_set(&t->trace_overrun, 0);\n\t\t\tt->curr_ret_stack = -1;\n\t\t\tt->curr_ret_depth = -1;\n\t\t\t \n\t\t\tsmp_wmb();\n\t\t\tt->ret_stack = ret_stack_list[start++];\n\t\t}\n\t}\n\nunlock:\n\trcu_read_unlock();\nfree:\n\tfor (i = start; i < end; i++)\n\t\tkfree(ret_stack_list[i]);\n\treturn ret;\n}\n\nstatic void\nftrace_graph_probe_sched_switch(void *ignore, bool preempt,\n\t\t\t\tstruct task_struct *prev,\n\t\t\t\tstruct task_struct *next,\n\t\t\t\tunsigned int prev_state)\n{\n\tunsigned long long timestamp;\n\tint index;\n\n\t \n\tif (fgraph_sleep_time)\n\t\treturn;\n\n\ttimestamp = trace_clock_local();\n\n\tprev->ftrace_timestamp = timestamp;\n\n\t \n\tif (!next->ftrace_timestamp)\n\t\treturn;\n\n\t \n\ttimestamp -= next->ftrace_timestamp;\n\n\tfor (index = next->curr_ret_stack; index >= 0; index--)\n\t\tnext->ret_stack[index].calltime += timestamp;\n}\n\nstatic int ftrace_graph_entry_test(struct ftrace_graph_ent *trace)\n{\n\tif (!ftrace_ops_test(&global_ops, trace->func, NULL))\n\t\treturn 0;\n\treturn __ftrace_graph_entry(trace);\n}\n\n \nvoid update_function_graph_func(void)\n{\n\tstruct ftrace_ops *op;\n\tbool do_test = false;\n\n\t \n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op != &global_ops && op != &graph_ops &&\n\t\t    op != &ftrace_list_end) {\n\t\t\tdo_test = true;\n\t\t\t \n\t\t\tgoto out;\n\t\t}\n\t} while_for_each_ftrace_op(op);\n out:\n\tif (do_test)\n\t\tftrace_graph_entry = ftrace_graph_entry_test;\n\telse\n\t\tftrace_graph_entry = __ftrace_graph_entry;\n}\n\nstatic DEFINE_PER_CPU(struct ftrace_ret_stack *, idle_ret_stack);\n\nstatic void\ngraph_init_task(struct task_struct *t, struct ftrace_ret_stack *ret_stack)\n{\n\tatomic_set(&t->trace_overrun, 0);\n\tt->ftrace_timestamp = 0;\n\t \n\tsmp_wmb();\n\tt->ret_stack = ret_stack;\n}\n\n \nvoid ftrace_graph_init_idle_task(struct task_struct *t, int cpu)\n{\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\t \n\tif (t->ret_stack)\n\t\tWARN_ON(t->ret_stack != per_cpu(idle_ret_stack, cpu));\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = per_cpu(idle_ret_stack, cpu);\n\t\tif (!ret_stack) {\n\t\t\tret_stack =\n\t\t\t\tkmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t      sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t      GFP_KERNEL);\n\t\t\tif (!ret_stack)\n\t\t\t\treturn;\n\t\t\tper_cpu(idle_ret_stack, cpu) = ret_stack;\n\t\t}\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}\n\n \nvoid ftrace_graph_init_task(struct task_struct *t)\n{\n\t \n\tt->ret_stack = NULL;\n\tt->curr_ret_stack = -1;\n\tt->curr_ret_depth = -1;\n\n\tif (ftrace_graph_active) {\n\t\tstruct ftrace_ret_stack *ret_stack;\n\n\t\tret_stack = kmalloc_array(FTRACE_RETFUNC_DEPTH,\n\t\t\t\t\t  sizeof(struct ftrace_ret_stack),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!ret_stack)\n\t\t\treturn;\n\t\tgraph_init_task(t, ret_stack);\n\t}\n}\n\nvoid ftrace_graph_exit_task(struct task_struct *t)\n{\n\tstruct ftrace_ret_stack\t*ret_stack = t->ret_stack;\n\n\tt->ret_stack = NULL;\n\t \n\tbarrier();\n\n\tkfree(ret_stack);\n}\n\n \nstatic int start_graph_tracing(void)\n{\n\tstruct ftrace_ret_stack **ret_stack_list;\n\tint ret, cpu;\n\n\tret_stack_list = kmalloc_array(FTRACE_RETSTACK_ALLOC_SIZE,\n\t\t\t\t       sizeof(struct ftrace_ret_stack *),\n\t\t\t\t       GFP_KERNEL);\n\n\tif (!ret_stack_list)\n\t\treturn -ENOMEM;\n\n\t \n\tfor_each_online_cpu(cpu) {\n\t\tif (!idle_task(cpu)->ret_stack)\n\t\t\tftrace_graph_init_idle_task(idle_task(cpu), cpu);\n\t}\n\n\tdo {\n\t\tret = alloc_retstack_tasklist(ret_stack_list);\n\t} while (ret == -EAGAIN);\n\n\tif (!ret) {\n\t\tret = register_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\t\tif (ret)\n\t\t\tpr_info(\"ftrace_graph: Couldn't activate tracepoint\"\n\t\t\t\t\" probe to kernel_sched_switch\\n\");\n\t}\n\n\tkfree(ret_stack_list);\n\treturn ret;\n}\n\nint register_ftrace_graph(struct fgraph_ops *gops)\n{\n\tint ret = 0;\n\n\tmutex_lock(&ftrace_lock);\n\n\t \n\tif (ftrace_graph_active) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tregister_pm_notifier(&ftrace_suspend_notifier);\n\n\tftrace_graph_active++;\n\tret = start_graph_tracing();\n\tif (ret) {\n\t\tftrace_graph_active--;\n\t\tgoto out;\n\t}\n\n\tftrace_graph_return = gops->retfunc;\n\n\t \n\t__ftrace_graph_entry = gops->entryfunc;\n\tftrace_graph_entry = ftrace_graph_entry_test;\n\tupdate_function_graph_func();\n\n\tret = ftrace_startup(&graph_ops, FTRACE_START_FUNC_RET);\nout:\n\tmutex_unlock(&ftrace_lock);\n\treturn ret;\n}\n\nvoid unregister_ftrace_graph(struct fgraph_ops *gops)\n{\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(!ftrace_graph_active))\n\t\tgoto out;\n\n\tftrace_graph_active--;\n\tftrace_graph_return = ftrace_stub_graph;\n\tftrace_graph_entry = ftrace_graph_entry_stub;\n\t__ftrace_graph_entry = ftrace_graph_entry_stub;\n\tftrace_shutdown(&graph_ops, FTRACE_STOP_FUNC_RET);\n\tunregister_pm_notifier(&ftrace_suspend_notifier);\n\tunregister_trace_sched_switch(ftrace_graph_probe_sched_switch, NULL);\n\n out:\n\tmutex_unlock(&ftrace_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}