{
  "module_name": "trace.c",
  "hash_id": "841825f333d430135db5d55d698a614aa76ab0159294619911a87ee5bd560059",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace.c",
  "human_readable_source": "\n \n#include <linux/ring_buffer.h>\n#include <generated/utsrelease.h>\n#include <linux/stacktrace.h>\n#include <linux/writeback.h>\n#include <linux/kallsyms.h>\n#include <linux/security.h>\n#include <linux/seq_file.h>\n#include <linux/irqflags.h>\n#include <linux/debugfs.h>\n#include <linux/tracefs.h>\n#include <linux/pagemap.h>\n#include <linux/hardirq.h>\n#include <linux/linkage.h>\n#include <linux/uaccess.h>\n#include <linux/vmalloc.h>\n#include <linux/ftrace.h>\n#include <linux/module.h>\n#include <linux/percpu.h>\n#include <linux/splice.h>\n#include <linux/kdebug.h>\n#include <linux/string.h>\n#include <linux/mount.h>\n#include <linux/rwsem.h>\n#include <linux/slab.h>\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/panic_notifier.h>\n#include <linux/poll.h>\n#include <linux/nmi.h>\n#include <linux/fs.h>\n#include <linux/trace.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/rt.h>\n#include <linux/fsnotify.h>\n#include <linux/irq_work.h>\n#include <linux/workqueue.h>\n\n#include <asm/setup.h>  \n\n#include \"trace.h\"\n#include \"trace_output.h\"\n\n \nbool ring_buffer_expanded;\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n \nstatic bool __read_mostly tracing_selftest_running;\n\n \nbool __read_mostly tracing_selftest_disabled;\n\nvoid __init disable_tracing_selftest(const char *reason)\n{\n\tif (!tracing_selftest_disabled) {\n\t\ttracing_selftest_disabled = true;\n\t\tpr_info(\"Ftrace startup test is disabled due to %s\\n\", reason);\n\t}\n}\n#else\n#define tracing_selftest_running\t0\n#define tracing_selftest_disabled\t0\n#endif\n\n \nstatic struct trace_iterator *tracepoint_print_iter;\nint tracepoint_printk;\nstatic bool tracepoint_printk_stop_on_boot __initdata;\nstatic DEFINE_STATIC_KEY_FALSE(tracepoint_printk_key);\n\n \nstatic struct tracer_opt dummy_tracer_opt[] = {\n\t{ }\n};\n\nstatic int\ndummy_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\treturn 0;\n}\n\n \nstatic DEFINE_PER_CPU(bool, trace_taskinfo_save);\n\n \nstatic int tracing_disabled = 1;\n\ncpumask_var_t __read_mostly\ttracing_buffer_mask;\n\n \n\nenum ftrace_dump_mode ftrace_dump_on_oops;\n\n \nint __disable_trace_on_warning;\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\n \nstruct trace_eval_map_head {\n\tstruct module\t\t\t*mod;\n\tunsigned long\t\t\tlength;\n};\n\nunion trace_eval_map_item;\n\nstruct trace_eval_map_tail {\n\t \n\tunion trace_eval_map_item\t*next;\n\tconst char\t\t\t*end;\t \n};\n\nstatic DEFINE_MUTEX(trace_eval_mutex);\n\n \nunion trace_eval_map_item {\n\tstruct trace_eval_map\t\tmap;\n\tstruct trace_eval_map_head\thead;\n\tstruct trace_eval_map_tail\ttail;\n};\n\nstatic union trace_eval_map_item *trace_eval_maps;\n#endif  \n\nint tracing_set_tracer(struct trace_array *tr, const char *buf);\nstatic void ftrace_trace_userstack(struct trace_array *tr,\n\t\t\t\t   struct trace_buffer *buffer,\n\t\t\t\t   unsigned int trace_ctx);\n\n#define MAX_TRACER_SIZE\t\t100\nstatic char bootup_tracer_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *default_bootup_tracer;\n\nstatic bool allocate_snapshot;\nstatic bool snapshot_at_boot;\n\nstatic char boot_instance_info[COMMAND_LINE_SIZE] __initdata;\nstatic int boot_instance_index;\n\nstatic char boot_snapshot_info[COMMAND_LINE_SIZE] __initdata;\nstatic int boot_snapshot_index;\n\nstatic int __init set_cmdline_ftrace(char *str)\n{\n\tstrscpy(bootup_tracer_buf, str, MAX_TRACER_SIZE);\n\tdefault_bootup_tracer = bootup_tracer_buf;\n\t \n\tring_buffer_expanded = true;\n\treturn 1;\n}\n__setup(\"ftrace=\", set_cmdline_ftrace);\n\nstatic int __init set_ftrace_dump_on_oops(char *str)\n{\n\tif (*str++ != '=' || !*str || !strcmp(\"1\", str)) {\n\t\tftrace_dump_on_oops = DUMP_ALL;\n\t\treturn 1;\n\t}\n\n\tif (!strcmp(\"orig_cpu\", str) || !strcmp(\"2\", str)) {\n\t\tftrace_dump_on_oops = DUMP_ORIG;\n                return 1;\n        }\n\n        return 0;\n}\n__setup(\"ftrace_dump_on_oops\", set_ftrace_dump_on_oops);\n\nstatic int __init stop_trace_on_warning(char *str)\n{\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\t__disable_trace_on_warning = 1;\n\treturn 1;\n}\n__setup(\"traceoff_on_warning\", stop_trace_on_warning);\n\nstatic int __init boot_alloc_snapshot(char *str)\n{\n\tchar *slot = boot_snapshot_info + boot_snapshot_index;\n\tint left = sizeof(boot_snapshot_info) - boot_snapshot_index;\n\tint ret;\n\n\tif (str[0] == '=') {\n\t\tstr++;\n\t\tif (strlen(str) >= left)\n\t\t\treturn -1;\n\n\t\tret = snprintf(slot, left, \"%s\\t\", str);\n\t\tboot_snapshot_index += ret;\n\t} else {\n\t\tallocate_snapshot = true;\n\t\t \n\t\tring_buffer_expanded = true;\n\t}\n\treturn 1;\n}\n__setup(\"alloc_snapshot\", boot_alloc_snapshot);\n\n\nstatic int __init boot_snapshot(char *str)\n{\n\tsnapshot_at_boot = true;\n\tboot_alloc_snapshot(str);\n\treturn 1;\n}\n__setup(\"ftrace_boot_snapshot\", boot_snapshot);\n\n\nstatic int __init boot_instance(char *str)\n{\n\tchar *slot = boot_instance_info + boot_instance_index;\n\tint left = sizeof(boot_instance_info) - boot_instance_index;\n\tint ret;\n\n\tif (strlen(str) >= left)\n\t\treturn -1;\n\n\tret = snprintf(slot, left, \"%s\\t\", str);\n\tboot_instance_index += ret;\n\n\treturn 1;\n}\n__setup(\"trace_instance=\", boot_instance);\n\n\nstatic char trace_boot_options_buf[MAX_TRACER_SIZE] __initdata;\n\nstatic int __init set_trace_boot_options(char *str)\n{\n\tstrscpy(trace_boot_options_buf, str, MAX_TRACER_SIZE);\n\treturn 1;\n}\n__setup(\"trace_options=\", set_trace_boot_options);\n\nstatic char trace_boot_clock_buf[MAX_TRACER_SIZE] __initdata;\nstatic char *trace_boot_clock __initdata;\n\nstatic int __init set_trace_boot_clock(char *str)\n{\n\tstrscpy(trace_boot_clock_buf, str, MAX_TRACER_SIZE);\n\ttrace_boot_clock = trace_boot_clock_buf;\n\treturn 1;\n}\n__setup(\"trace_clock=\", set_trace_boot_clock);\n\nstatic int __init set_tracepoint_printk(char *str)\n{\n\t \n\tif (*str == '_')\n\t\treturn 0;\n\n\tif ((strcmp(str, \"=0\") != 0 && strcmp(str, \"=off\") != 0))\n\t\ttracepoint_printk = 1;\n\treturn 1;\n}\n__setup(\"tp_printk\", set_tracepoint_printk);\n\nstatic int __init set_tracepoint_printk_stop(char *str)\n{\n\ttracepoint_printk_stop_on_boot = true;\n\treturn 1;\n}\n__setup(\"tp_printk_stop_on_boot\", set_tracepoint_printk_stop);\n\nunsigned long long ns2usecs(u64 nsec)\n{\n\tnsec += 500;\n\tdo_div(nsec, 1000);\n\treturn nsec;\n}\n\nstatic void\ntrace_process_export(struct trace_export *export,\n\t       struct ring_buffer_event *event, int flag)\n{\n\tstruct trace_entry *entry;\n\tunsigned int size = 0;\n\n\tif (export->flags & flag) {\n\t\tentry = ring_buffer_event_data(event);\n\t\tsize = ring_buffer_event_length(event);\n\t\texport->write(export, entry, size);\n\t}\n}\n\nstatic DEFINE_MUTEX(ftrace_export_lock);\n\nstatic struct trace_export __rcu *ftrace_exports_list __read_mostly;\n\nstatic DEFINE_STATIC_KEY_FALSE(trace_function_exports_enabled);\nstatic DEFINE_STATIC_KEY_FALSE(trace_event_exports_enabled);\nstatic DEFINE_STATIC_KEY_FALSE(trace_marker_exports_enabled);\n\nstatic inline void ftrace_exports_enable(struct trace_export *export)\n{\n\tif (export->flags & TRACE_EXPORT_FUNCTION)\n\t\tstatic_branch_inc(&trace_function_exports_enabled);\n\n\tif (export->flags & TRACE_EXPORT_EVENT)\n\t\tstatic_branch_inc(&trace_event_exports_enabled);\n\n\tif (export->flags & TRACE_EXPORT_MARKER)\n\t\tstatic_branch_inc(&trace_marker_exports_enabled);\n}\n\nstatic inline void ftrace_exports_disable(struct trace_export *export)\n{\n\tif (export->flags & TRACE_EXPORT_FUNCTION)\n\t\tstatic_branch_dec(&trace_function_exports_enabled);\n\n\tif (export->flags & TRACE_EXPORT_EVENT)\n\t\tstatic_branch_dec(&trace_event_exports_enabled);\n\n\tif (export->flags & TRACE_EXPORT_MARKER)\n\t\tstatic_branch_dec(&trace_marker_exports_enabled);\n}\n\nstatic void ftrace_exports(struct ring_buffer_event *event, int flag)\n{\n\tstruct trace_export *export;\n\n\tpreempt_disable_notrace();\n\n\texport = rcu_dereference_raw_check(ftrace_exports_list);\n\twhile (export) {\n\t\ttrace_process_export(export, event, flag);\n\t\texport = rcu_dereference_raw_check(export->next);\n\t}\n\n\tpreempt_enable_notrace();\n}\n\nstatic inline void\nadd_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\trcu_assign_pointer(export->next, *list);\n\t \n\trcu_assign_pointer(*list, export);\n}\n\nstatic inline int\nrm_trace_export(struct trace_export **list, struct trace_export *export)\n{\n\tstruct trace_export **p;\n\n\tfor (p = list; *p != NULL; p = &(*p)->next)\n\t\tif (*p == export)\n\t\t\tbreak;\n\n\tif (*p != export)\n\t\treturn -1;\n\n\trcu_assign_pointer(*p, (*p)->next);\n\n\treturn 0;\n}\n\nstatic inline void\nadd_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tftrace_exports_enable(export);\n\n\tadd_trace_export(list, export);\n}\n\nstatic inline int\nrm_ftrace_export(struct trace_export **list, struct trace_export *export)\n{\n\tint ret;\n\n\tret = rm_trace_export(list, export);\n\tftrace_exports_disable(export);\n\n\treturn ret;\n}\n\nint register_ftrace_export(struct trace_export *export)\n{\n\tif (WARN_ON_ONCE(!export->write))\n\t\treturn -1;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tadd_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_export);\n\nint unregister_ftrace_export(struct trace_export *export)\n{\n\tint ret;\n\n\tmutex_lock(&ftrace_export_lock);\n\n\tret = rm_ftrace_export(&ftrace_exports_list, export);\n\n\tmutex_unlock(&ftrace_export_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_export);\n\n \n#define TRACE_DEFAULT_FLAGS\t\t\t\t\t\t\\\n\t(FUNCTION_DEFAULT_FLAGS |\t\t\t\t\t\\\n\t TRACE_ITER_PRINT_PARENT | TRACE_ITER_PRINTK |\t\t\t\\\n\t TRACE_ITER_ANNOTATE | TRACE_ITER_CONTEXT_INFO |\t\t\\\n\t TRACE_ITER_RECORD_CMD | TRACE_ITER_OVERWRITE |\t\t\t\\\n\t TRACE_ITER_IRQ_INFO | TRACE_ITER_MARKERS |\t\t\t\\\n\t TRACE_ITER_HASH_PTR)\n\n \n#define TOP_LEVEL_TRACE_FLAGS (TRACE_ITER_PRINTK |\t\t\t\\\n\t       TRACE_ITER_PRINTK_MSGONLY | TRACE_ITER_RECORD_CMD)\n\n \n#define ZEROED_TRACE_FLAGS \\\n\t(TRACE_ITER_EVENT_FORK | TRACE_ITER_FUNC_FORK)\n\n \nstatic struct trace_array global_trace = {\n\t.trace_flags = TRACE_DEFAULT_FLAGS,\n};\n\nLIST_HEAD(ftrace_trace_arrays);\n\nint trace_array_get(struct trace_array *this_tr)\n{\n\tstruct trace_array *tr;\n\tint ret = -ENODEV;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr == this_tr) {\n\t\t\ttr->ref++;\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic void __trace_array_put(struct trace_array *this_tr)\n{\n\tWARN_ON(!this_tr->ref);\n\tthis_tr->ref--;\n}\n\n \nvoid trace_array_put(struct trace_array *this_tr)\n{\n\tif (!this_tr)\n\t\treturn;\n\n\tmutex_lock(&trace_types_lock);\n\t__trace_array_put(this_tr);\n\tmutex_unlock(&trace_types_lock);\n}\nEXPORT_SYMBOL_GPL(trace_array_put);\n\nint tracing_check_open_get_tr(struct trace_array *tr)\n{\n\tint ret;\n\n\tret = security_locked_down(LOCKDOWN_TRACEFS);\n\tif (ret)\n\t\treturn ret;\n\n\tif (tracing_disabled)\n\t\treturn -ENODEV;\n\n\tif (tr && trace_array_get(tr) < 0)\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\nint call_filter_check_discard(struct trace_event_call *call, void *rec,\n\t\t\t      struct trace_buffer *buffer,\n\t\t\t      struct ring_buffer_event *event)\n{\n\tif (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&\n\t    !filter_match_preds(call->filter, rec)) {\n\t\t__trace_event_discard_commit(buffer, event);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nbool\ntrace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)\n{\n\treturn trace_pid_list_is_set(filtered_pids, search_pid);\n}\n\n \nbool\ntrace_ignore_this_task(struct trace_pid_list *filtered_pids,\n\t\t       struct trace_pid_list *filtered_no_pids,\n\t\t       struct task_struct *task)\n{\n\t \n\n\treturn (filtered_pids &&\n\t\t!trace_find_filtered_pid(filtered_pids, task->pid)) ||\n\t\t(filtered_no_pids &&\n\t\t trace_find_filtered_pid(filtered_no_pids, task->pid));\n}\n\n \nvoid trace_filter_add_remove_task(struct trace_pid_list *pid_list,\n\t\t\t\t  struct task_struct *self,\n\t\t\t\t  struct task_struct *task)\n{\n\tif (!pid_list)\n\t\treturn;\n\n\t \n\tif (self) {\n\t\tif (!trace_find_filtered_pid(pid_list, self->pid))\n\t\t\treturn;\n\t}\n\n\t \n\tif (self)\n\t\ttrace_pid_list_set(pid_list, task->pid);\n\telse\n\t\ttrace_pid_list_clear(pid_list, task->pid);\n}\n\n \nvoid *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos)\n{\n\tlong pid = (unsigned long)v;\n\tunsigned int next;\n\n\t(*pos)++;\n\n\t \n\tif (trace_pid_list_next(pid_list, pid, &next) < 0)\n\t\treturn NULL;\n\n\tpid = next;\n\n\t \n\treturn (void *)(pid + 1);\n}\n\n \nvoid *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos)\n{\n\tunsigned long pid;\n\tunsigned int first;\n\tloff_t l = 0;\n\n\tif (trace_pid_list_first(pid_list, &first) < 0)\n\t\treturn NULL;\n\n\tpid = first;\n\n\t \n\tfor (pid++; pid && l < *pos;\n\t     pid = (unsigned long)trace_pid_next(pid_list, (void *)pid, &l))\n\t\t;\n\treturn (void *)pid;\n}\n\n \nint trace_pid_show(struct seq_file *m, void *v)\n{\n\tunsigned long pid = (unsigned long)v - 1;\n\n\tseq_printf(m, \"%lu\\n\", pid);\n\treturn 0;\n}\n\n \n#define PID_BUF_SIZE\t\t127\n\nint trace_pid_write(struct trace_pid_list *filtered_pids,\n\t\t    struct trace_pid_list **new_pid_list,\n\t\t    const char __user *ubuf, size_t cnt)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_parser parser;\n\tunsigned long val;\n\tint nr_pids = 0;\n\tssize_t read = 0;\n\tssize_t ret;\n\tloff_t pos;\n\tpid_t pid;\n\n\tif (trace_parser_get_init(&parser, PID_BUF_SIZE + 1))\n\t\treturn -ENOMEM;\n\n\t \n\tpid_list = trace_pid_list_alloc();\n\tif (!pid_list) {\n\t\ttrace_parser_put(&parser);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (filtered_pids) {\n\t\t \n\t\tret = trace_pid_list_first(filtered_pids, &pid);\n\t\twhile (!ret) {\n\t\t\ttrace_pid_list_set(pid_list, pid);\n\t\t\tret = trace_pid_list_next(filtered_pids, pid + 1, &pid);\n\t\t\tnr_pids++;\n\t\t}\n\t}\n\n\tret = 0;\n\twhile (cnt > 0) {\n\n\t\tpos = 0;\n\n\t\tret = trace_get_user(&parser, ubuf, cnt, &pos);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tread += ret;\n\t\tubuf += ret;\n\t\tcnt -= ret;\n\n\t\tif (!trace_parser_loaded(&parser))\n\t\t\tbreak;\n\n\t\tret = -EINVAL;\n\t\tif (kstrtoul(parser.buffer, 0, &val))\n\t\t\tbreak;\n\n\t\tpid = (pid_t)val;\n\n\t\tif (trace_pid_list_set(pid_list, pid) < 0) {\n\t\t\tret = -1;\n\t\t\tbreak;\n\t\t}\n\t\tnr_pids++;\n\n\t\ttrace_parser_clear(&parser);\n\t\tret = 0;\n\t}\n\ttrace_parser_put(&parser);\n\n\tif (ret < 0) {\n\t\ttrace_pid_list_free(pid_list);\n\t\treturn ret;\n\t}\n\n\tif (!nr_pids) {\n\t\t \n\t\ttrace_pid_list_free(pid_list);\n\t\tpid_list = NULL;\n\t}\n\n\t*new_pid_list = pid_list;\n\n\treturn read;\n}\n\nstatic u64 buffer_ftrace_now(struct array_buffer *buf, int cpu)\n{\n\tu64 ts;\n\n\t \n\tif (!buf->buffer)\n\t\treturn trace_clock_local();\n\n\tts = ring_buffer_time_stamp(buf->buffer);\n\tring_buffer_normalize_time_stamp(buf->buffer, cpu, &ts);\n\n\treturn ts;\n}\n\nu64 ftrace_now(int cpu)\n{\n\treturn buffer_ftrace_now(&global_trace.array_buffer, cpu);\n}\n\n \nint tracing_is_enabled(void)\n{\n\t \n\tsmp_rmb();\n\treturn !global_trace.buffer_disabled;\n}\n\n \n#define TRACE_BUF_SIZE_DEFAULT\t1441792UL  \n\nstatic unsigned long\t\ttrace_buf_size = TRACE_BUF_SIZE_DEFAULT;\n\n \nstatic struct tracer\t\t*trace_types __read_mostly;\n\n \nDEFINE_MUTEX(trace_types_lock);\n\n \n\n#ifdef CONFIG_SMP\nstatic DECLARE_RWSEM(all_cpu_access_lock);\nstatic DEFINE_PER_CPU(struct mutex, cpu_access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\t \n\t\tdown_write(&all_cpu_access_lock);\n\t} else {\n\t\t \n\n\t\t \n\t\tdown_read(&all_cpu_access_lock);\n\n\t\t \n\t\tmutex_lock(&per_cpu(cpu_access_lock, cpu));\n\t}\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tup_write(&all_cpu_access_lock);\n\t} else {\n\t\tmutex_unlock(&per_cpu(cpu_access_lock, cpu));\n\t\tup_read(&all_cpu_access_lock);\n\t}\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tmutex_init(&per_cpu(cpu_access_lock, cpu));\n}\n\n#else\n\nstatic DEFINE_MUTEX(access_lock);\n\nstatic inline void trace_access_lock(int cpu)\n{\n\t(void)cpu;\n\tmutex_lock(&access_lock);\n}\n\nstatic inline void trace_access_unlock(int cpu)\n{\n\t(void)cpu;\n\tmutex_unlock(&access_lock);\n}\n\nstatic inline void trace_access_lock_init(void)\n{\n}\n\n#endif\n\n#ifdef CONFIG_STACKTRACE\nstatic void __ftrace_trace_stack(struct trace_buffer *buffer,\n\t\t\t\t unsigned int trace_ctx,\n\t\t\t\t int skip, struct pt_regs *regs);\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct trace_buffer *buffer,\n\t\t\t\t      unsigned int trace_ctx,\n\t\t\t\t      int skip, struct pt_regs *regs);\n\n#else\nstatic inline void __ftrace_trace_stack(struct trace_buffer *buffer,\n\t\t\t\t\tunsigned int trace_ctx,\n\t\t\t\t\tint skip, struct pt_regs *regs)\n{\n}\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct trace_buffer *buffer,\n\t\t\t\t      unsigned long trace_ctx,\n\t\t\t\t      int skip, struct pt_regs *regs)\n{\n}\n\n#endif\n\nstatic __always_inline void\ntrace_event_setup(struct ring_buffer_event *event,\n\t\t  int type, unsigned int trace_ctx)\n{\n\tstruct trace_entry *ent = ring_buffer_event_data(event);\n\n\ttracing_generic_entry_update(ent, type, trace_ctx);\n}\n\nstatic __always_inline struct ring_buffer_event *\n__trace_buffer_lock_reserve(struct trace_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned int trace_ctx)\n{\n\tstruct ring_buffer_event *event;\n\n\tevent = ring_buffer_lock_reserve(buffer, len);\n\tif (event != NULL)\n\t\ttrace_event_setup(event, type, trace_ctx);\n\n\treturn event;\n}\n\nvoid tracer_tracing_on(struct trace_array *tr)\n{\n\tif (tr->array_buffer.buffer)\n\t\tring_buffer_record_on(tr->array_buffer.buffer);\n\t \n\ttr->buffer_disabled = 0;\n\t \n\tsmp_wmb();\n}\n\n \nvoid tracing_on(void)\n{\n\ttracer_tracing_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_on);\n\n\nstatic __always_inline void\n__buffer_unlock_commit(struct trace_buffer *buffer, struct ring_buffer_event *event)\n{\n\t__this_cpu_write(trace_taskinfo_save, true);\n\n\t \n\tif (this_cpu_read(trace_buffered_event) == event) {\n\t\t \n\t\tring_buffer_write(buffer, event->array[0], &event->array[1]);\n\t\t \n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t\t \n\t\tpreempt_enable_notrace();\n\t} else\n\t\tring_buffer_unlock_commit(buffer);\n}\n\nint __trace_array_puts(struct trace_array *tr, unsigned long ip,\n\t\t       const char *str, int size)\n{\n\tstruct ring_buffer_event *event;\n\tstruct trace_buffer *buffer;\n\tstruct print_entry *entry;\n\tunsigned int trace_ctx;\n\tint alloc;\n\n\tif (!(tr->trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tif (unlikely(tracing_selftest_running && tr == &global_trace))\n\t\treturn 0;\n\n\tif (unlikely(tracing_disabled))\n\t\treturn 0;\n\n\talloc = sizeof(*entry) + size + 2;  \n\n\ttrace_ctx = tracing_gen_ctx();\n\tbuffer = tr->array_buffer.buffer;\n\tring_buffer_nest_start(buffer);\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, alloc,\n\t\t\t\t\t    trace_ctx);\n\tif (!event) {\n\t\tsize = 0;\n\t\tgoto out;\n\t}\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, str, size);\n\n\t \n\tif (entry->buf[size - 1] != '\\n') {\n\t\tentry->buf[size] = '\\n';\n\t\tentry->buf[size + 1] = '\\0';\n\t} else\n\t\tentry->buf[size] = '\\0';\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(tr, buffer, trace_ctx, 4, NULL);\n out:\n\tring_buffer_nest_end(buffer);\n\treturn size;\n}\nEXPORT_SYMBOL_GPL(__trace_array_puts);\n\n \nint __trace_puts(unsigned long ip, const char *str, int size)\n{\n\treturn __trace_array_puts(&global_trace, ip, str, size);\n}\nEXPORT_SYMBOL_GPL(__trace_puts);\n\n \nint __trace_bputs(unsigned long ip, const char *str)\n{\n\tstruct ring_buffer_event *event;\n\tstruct trace_buffer *buffer;\n\tstruct bputs_entry *entry;\n\tunsigned int trace_ctx;\n\tint size = sizeof(struct bputs_entry);\n\tint ret = 0;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\ttrace_ctx = tracing_gen_ctx();\n\tbuffer = global_trace.array_buffer.buffer;\n\n\tring_buffer_nest_start(buffer);\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPUTS, size,\n\t\t\t\t\t    trace_ctx);\n\tif (!event)\n\t\tgoto out;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->str\t\t\t= str;\n\n\t__buffer_unlock_commit(buffer, event);\n\tftrace_trace_stack(&global_trace, buffer, trace_ctx, 4, NULL);\n\n\tret = 1;\n out:\n\tring_buffer_nest_end(buffer);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__trace_bputs);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic void tracing_snapshot_instance_cond(struct trace_array *tr,\n\t\t\t\t\t   void *cond_data)\n{\n\tstruct tracer *tracer = tr->current_trace;\n\tunsigned long flags;\n\n\tif (in_nmi()) {\n\t\ttrace_array_puts(tr, \"*** SNAPSHOT CALLED FROM NMI CONTEXT ***\\n\");\n\t\ttrace_array_puts(tr, \"*** snapshot is being ignored        ***\\n\");\n\t\treturn;\n\t}\n\n\tif (!tr->allocated_snapshot) {\n\t\ttrace_array_puts(tr, \"*** SNAPSHOT NOT ALLOCATED ***\\n\");\n\t\ttrace_array_puts(tr, \"*** stopping trace here!   ***\\n\");\n\t\ttracer_tracing_off(tr);\n\t\treturn;\n\t}\n\n\t \n\tif (tracer->use_max_tr) {\n\t\ttrace_array_puts(tr, \"*** LATENCY TRACER ACTIVE ***\\n\");\n\t\ttrace_array_puts(tr, \"*** Can not use snapshot (sorry) ***\\n\");\n\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\tupdate_max_tr(tr, current, smp_processor_id(), cond_data);\n\tlocal_irq_restore(flags);\n}\n\nvoid tracing_snapshot_instance(struct trace_array *tr)\n{\n\ttracing_snapshot_instance_cond(tr, NULL);\n}\n\n \nvoid tracing_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\ttracing_snapshot_instance(tr);\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\n\n \nvoid tracing_snapshot_cond(struct trace_array *tr, void *cond_data)\n{\n\ttracing_snapshot_instance_cond(tr, cond_data);\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_cond);\n\n \nvoid *tracing_cond_snapshot_data(struct trace_array *tr)\n{\n\tvoid *cond_data = NULL;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\n\tif (tr->cond_snapshot)\n\t\tcond_data = tr->cond_snapshot->cond_data;\n\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\treturn cond_data;\n}\nEXPORT_SYMBOL_GPL(tracing_cond_snapshot_data);\n\nstatic int resize_buffer_duplicate_size(struct array_buffer *trace_buf,\n\t\t\t\t\tstruct array_buffer *size_buf, int cpu_id);\nstatic void set_buffer_entries(struct array_buffer *buf, unsigned long val);\n\nint tracing_alloc_snapshot_instance(struct trace_array *tr)\n{\n\tint ret;\n\n\tif (!tr->allocated_snapshot) {\n\n\t\t \n\t\tret = resize_buffer_duplicate_size(&tr->max_buffer,\n\t\t\t\t   &tr->array_buffer, RING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\ttr->allocated_snapshot = true;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_snapshot(struct trace_array *tr)\n{\n\t \n\tring_buffer_resize(tr->max_buffer.buffer, 1, RING_BUFFER_ALL_CPUS);\n\tset_buffer_entries(&tr->max_buffer, 1);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n\ttr->allocated_snapshot = false;\n}\n\n \nint tracing_alloc_snapshot(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\tint ret;\n\n\tret = tracing_alloc_snapshot_instance(tr);\n\tWARN_ON(ret < 0);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\n\n \nvoid tracing_snapshot_alloc(void)\n{\n\tint ret;\n\n\tret = tracing_alloc_snapshot();\n\tif (ret < 0)\n\t\treturn;\n\n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\n\n \nint tracing_snapshot_cond_enable(struct trace_array *tr, void *cond_data,\n\t\t\t\t cond_update_fn_t update)\n{\n\tstruct cond_snapshot *cond_snapshot;\n\tint ret = 0;\n\n\tcond_snapshot = kzalloc(sizeof(*cond_snapshot), GFP_KERNEL);\n\tif (!cond_snapshot)\n\t\treturn -ENOMEM;\n\n\tcond_snapshot->cond_data = cond_data;\n\tcond_snapshot->update = update;\n\n\tmutex_lock(&trace_types_lock);\n\n\tret = tracing_alloc_snapshot_instance(tr);\n\tif (ret)\n\t\tgoto fail_unlock;\n\n\tif (tr->current_trace->use_max_tr) {\n\t\tret = -EBUSY;\n\t\tgoto fail_unlock;\n\t}\n\n\t \n\tif (tr->cond_snapshot) {\n\t\tret = -EBUSY;\n\t\tgoto fail_unlock;\n\t}\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\ttr->cond_snapshot = cond_snapshot;\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n\n fail_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tkfree(cond_snapshot);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_cond_enable);\n\n \nint tracing_snapshot_cond_disable(struct trace_array *tr)\n{\n\tint ret = 0;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\n\tif (!tr->cond_snapshot)\n\t\tret = -EINVAL;\n\telse {\n\t\tkfree(tr->cond_snapshot);\n\t\ttr->cond_snapshot = NULL;\n\t}\n\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_cond_disable);\n#else\nvoid tracing_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but internal snapshot used\");\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot);\nvoid tracing_snapshot_cond(struct trace_array *tr, void *cond_data)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but internal conditional snapshot used\");\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_cond);\nint tracing_alloc_snapshot(void)\n{\n\tWARN_ONCE(1, \"Snapshot feature not enabled, but snapshot allocation used\");\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(tracing_alloc_snapshot);\nvoid tracing_snapshot_alloc(void)\n{\n\t \n\ttracing_snapshot();\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_alloc);\nvoid *tracing_cond_snapshot_data(struct trace_array *tr)\n{\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(tracing_cond_snapshot_data);\nint tracing_snapshot_cond_enable(struct trace_array *tr, void *cond_data, cond_update_fn_t update)\n{\n\treturn -ENODEV;\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_cond_enable);\nint tracing_snapshot_cond_disable(struct trace_array *tr)\n{\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(tracing_snapshot_cond_disable);\n#define free_snapshot(tr)\tdo { } while (0)\n#endif  \n\nvoid tracer_tracing_off(struct trace_array *tr)\n{\n\tif (tr->array_buffer.buffer)\n\t\tring_buffer_record_off(tr->array_buffer.buffer);\n\t \n\ttr->buffer_disabled = 1;\n\t \n\tsmp_wmb();\n}\n\n \nvoid tracing_off(void)\n{\n\ttracer_tracing_off(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_off);\n\nvoid disable_trace_on_warning(void)\n{\n\tif (__disable_trace_on_warning) {\n\t\ttrace_array_printk_buf(global_trace.array_buffer.buffer, _THIS_IP_,\n\t\t\t\"Disabling tracing due to warning\\n\");\n\t\ttracing_off();\n\t}\n}\n\n \nbool tracer_tracing_is_on(struct trace_array *tr)\n{\n\tif (tr->array_buffer.buffer)\n\t\treturn ring_buffer_record_is_on(tr->array_buffer.buffer);\n\treturn !tr->buffer_disabled;\n}\n\n \nint tracing_is_on(void)\n{\n\treturn tracer_tracing_is_on(&global_trace);\n}\nEXPORT_SYMBOL_GPL(tracing_is_on);\n\nstatic int __init set_buf_size(char *str)\n{\n\tunsigned long buf_size;\n\n\tif (!str)\n\t\treturn 0;\n\tbuf_size = memparse(str, &str);\n\t \n\ttrace_buf_size = max(4096UL, buf_size);\n\treturn 1;\n}\n__setup(\"trace_buf_size=\", set_buf_size);\n\nstatic int __init set_tracing_thresh(char *str)\n{\n\tunsigned long threshold;\n\tint ret;\n\n\tif (!str)\n\t\treturn 0;\n\tret = kstrtoul(str, 0, &threshold);\n\tif (ret < 0)\n\t\treturn 0;\n\ttracing_thresh = threshold * 1000;\n\treturn 1;\n}\n__setup(\"tracing_thresh=\", set_tracing_thresh);\n\nunsigned long nsecs_to_usecs(unsigned long nsecs)\n{\n\treturn nsecs / 1000;\n}\n\n \n#undef C\n#define C(a, b) b\n\n \nstatic const char *trace_options[] = {\n\tTRACE_FLAGS\n\tNULL\n};\n\nstatic struct {\n\tu64 (*func)(void);\n\tconst char *name;\n\tint in_ns;\t\t \n} trace_clocks[] = {\n\t{ trace_clock_local,\t\t\"local\",\t1 },\n\t{ trace_clock_global,\t\t\"global\",\t1 },\n\t{ trace_clock_counter,\t\t\"counter\",\t0 },\n\t{ trace_clock_jiffies,\t\t\"uptime\",\t0 },\n\t{ trace_clock,\t\t\t\"perf\",\t\t1 },\n\t{ ktime_get_mono_fast_ns,\t\"mono\",\t\t1 },\n\t{ ktime_get_raw_fast_ns,\t\"mono_raw\",\t1 },\n\t{ ktime_get_boot_fast_ns,\t\"boot\",\t\t1 },\n\t{ ktime_get_tai_fast_ns,\t\"tai\",\t\t1 },\n\tARCH_TRACE_CLOCKS\n};\n\nbool trace_clock_in_ns(struct trace_array *tr)\n{\n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nint trace_parser_get_init(struct trace_parser *parser, int size)\n{\n\tmemset(parser, 0, sizeof(*parser));\n\n\tparser->buffer = kmalloc(size, GFP_KERNEL);\n\tif (!parser->buffer)\n\t\treturn 1;\n\n\tparser->size = size;\n\treturn 0;\n}\n\n \nvoid trace_parser_put(struct trace_parser *parser)\n{\n\tkfree(parser->buffer);\n\tparser->buffer = NULL;\n}\n\n \nint trace_get_user(struct trace_parser *parser, const char __user *ubuf,\n\tsize_t cnt, loff_t *ppos)\n{\n\tchar ch;\n\tsize_t read = 0;\n\tssize_t ret;\n\n\tif (!*ppos)\n\t\ttrace_parser_clear(parser);\n\n\tret = get_user(ch, ubuf++);\n\tif (ret)\n\t\tgoto out;\n\n\tread++;\n\tcnt--;\n\n\t \n\tif (!parser->cont) {\n\t\t \n\t\twhile (cnt && isspace(ch)) {\n\t\t\tret = get_user(ch, ubuf++);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tread++;\n\t\t\tcnt--;\n\t\t}\n\n\t\tparser->idx = 0;\n\n\t\t \n\t\tif (isspace(ch) || !ch) {\n\t\t\t*ppos += read;\n\t\t\tret = read;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\twhile (cnt && !isspace(ch) && ch) {\n\t\tif (parser->idx < parser->size - 1)\n\t\t\tparser->buffer[parser->idx++] = ch;\n\t\telse {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = get_user(ch, ubuf++);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tread++;\n\t\tcnt--;\n\t}\n\n\t \n\tif (isspace(ch) || !ch) {\n\t\tparser->buffer[parser->idx] = 0;\n\t\tparser->cont = false;\n\t} else if (parser->idx < parser->size - 1) {\n\t\tparser->cont = true;\n\t\tparser->buffer[parser->idx++] = ch;\n\t\t \n\t\tparser->buffer[parser->idx] = 0;\n\t} else {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*ppos += read;\n\tret = read;\n\nout:\n\treturn ret;\n}\n\n \nstatic ssize_t trace_seq_to_buffer(struct trace_seq *s, void *buf, size_t cnt)\n{\n\tint len;\n\n\tif (trace_seq_used(s) <= s->seq.readpos)\n\t\treturn -EBUSY;\n\n\tlen = trace_seq_used(s) - s->seq.readpos;\n\tif (cnt > len)\n\t\tcnt = len;\n\tmemcpy(buf, s->buffer + s->seq.readpos, cnt);\n\n\ts->seq.readpos += cnt;\n\treturn cnt;\n}\n\nunsigned long __read_mostly\ttracing_thresh;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic const struct file_operations tracing_max_lat_fops;\n\n#ifdef LATENCY_FS_NOTIFY\n\nstatic struct workqueue_struct *fsnotify_wq;\n\nstatic void latency_fsnotify_workfn(struct work_struct *work)\n{\n\tstruct trace_array *tr = container_of(work, struct trace_array,\n\t\t\t\t\t      fsnotify_work);\n\tfsnotify_inode(tr->d_max_latency->d_inode, FS_MODIFY);\n}\n\nstatic void latency_fsnotify_workfn_irq(struct irq_work *iwork)\n{\n\tstruct trace_array *tr = container_of(iwork, struct trace_array,\n\t\t\t\t\t      fsnotify_irqwork);\n\tqueue_work(fsnotify_wq, &tr->fsnotify_work);\n}\n\nstatic void trace_create_maxlat_file(struct trace_array *tr,\n\t\t\t\t     struct dentry *d_tracer)\n{\n\tINIT_WORK(&tr->fsnotify_work, latency_fsnotify_workfn);\n\tinit_irq_work(&tr->fsnotify_irqwork, latency_fsnotify_workfn_irq);\n\ttr->d_max_latency = trace_create_file(\"tracing_max_latency\",\n\t\t\t\t\t      TRACE_MODE_WRITE,\n\t\t\t\t\t      d_tracer, tr,\n\t\t\t\t\t      &tracing_max_lat_fops);\n}\n\n__init static int latency_fsnotify_init(void)\n{\n\tfsnotify_wq = alloc_workqueue(\"tr_max_lat_wq\",\n\t\t\t\t      WQ_UNBOUND | WQ_HIGHPRI, 0);\n\tif (!fsnotify_wq) {\n\t\tpr_err(\"Unable to allocate tr_max_lat_wq\\n\");\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nlate_initcall_sync(latency_fsnotify_init);\n\nvoid latency_fsnotify(struct trace_array *tr)\n{\n\tif (!fsnotify_wq)\n\t\treturn;\n\t \n\tirq_work_queue(&tr->fsnotify_irqwork);\n}\n\n#else  \n\n#define trace_create_maxlat_file(tr, d_tracer)\t\t\t\t\\\n\ttrace_create_file(\"tracing_max_latency\", TRACE_MODE_WRITE,\t\\\n\t\t\t  d_tracer, tr, &tracing_max_lat_fops)\n\n#endif\n\n \nstatic void\n__update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tstruct array_buffer *trace_buf = &tr->array_buffer;\n\tstruct array_buffer *max_buf = &tr->max_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(trace_buf->data, cpu);\n\tstruct trace_array_cpu *max_data = per_cpu_ptr(max_buf->data, cpu);\n\n\tmax_buf->cpu = cpu;\n\tmax_buf->time_start = data->preempt_timestamp;\n\n\tmax_data->saved_latency = tr->max_latency;\n\tmax_data->critical_start = data->critical_start;\n\tmax_data->critical_end = data->critical_end;\n\n\tstrncpy(max_data->comm, tsk->comm, TASK_COMM_LEN);\n\tmax_data->pid = tsk->pid;\n\t \n\tif (tsk == current)\n\t\tmax_data->uid = current_uid();\n\telse\n\t\tmax_data->uid = task_uid(tsk);\n\n\tmax_data->nice = tsk->static_prio - 20 - MAX_RT_PRIO;\n\tmax_data->policy = tsk->policy;\n\tmax_data->rt_priority = tsk->rt_priority;\n\n\t \n\ttracing_record_cmdline(tsk);\n\tlatency_fsnotify(tr);\n}\n\n \nvoid\nupdate_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu,\n\t      void *cond_data)\n{\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\n\tif (!tr->allocated_snapshot) {\n\t\t \n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\t \n\tif (ring_buffer_record_is_set_on(tr->array_buffer.buffer))\n\t\tring_buffer_record_on(tr->max_buffer.buffer);\n\telse\n\t\tring_buffer_record_off(tr->max_buffer.buffer);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\tif (tr->cond_snapshot && !tr->cond_snapshot->update(tr, cond_data)) {\n\t\tarch_spin_unlock(&tr->max_lock);\n\t\treturn;\n\t}\n#endif\n\tswap(tr->array_buffer.buffer, tr->max_buffer.buffer);\n\n\t__update_max_tr(tr, tsk, cpu);\n\n\tarch_spin_unlock(&tr->max_lock);\n\n\t \n\tring_buffer_wake_waiters(tr->array_buffer.buffer, RING_BUFFER_ALL_CPUS);\n}\n\n \nvoid\nupdate_max_tr_single(struct trace_array *tr, struct task_struct *tsk, int cpu)\n{\n\tint ret;\n\n\tif (tr->stop_count)\n\t\treturn;\n\n\tWARN_ON_ONCE(!irqs_disabled());\n\tif (!tr->allocated_snapshot) {\n\t\t \n\t\tWARN_ON_ONCE(tr->current_trace != &nop_trace);\n\t\treturn;\n\t}\n\n\tarch_spin_lock(&tr->max_lock);\n\n\tret = ring_buffer_swap_cpu(tr->max_buffer.buffer, tr->array_buffer.buffer, cpu);\n\n\tif (ret == -EBUSY) {\n\t\t \n\t\ttrace_array_printk_buf(tr->max_buffer.buffer, _THIS_IP_,\n\t\t\t\"Failed to swap buffers due to commit or resize in progress\\n\");\n\t}\n\n\tWARN_ON_ONCE(ret && ret != -EAGAIN && ret != -EBUSY);\n\n\t__update_max_tr(tr, tsk, cpu);\n\tarch_spin_unlock(&tr->max_lock);\n}\n\n#endif  \n\nstatic int wait_on_pipe(struct trace_iterator *iter, int full)\n{\n\tint ret;\n\n\t \n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn 0;\n\n\tret = ring_buffer_wait(iter->array_buffer->buffer, iter->cpu_file, full);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t \n\tif (iter->snapshot)\n\t\titer->array_buffer = &iter->tr->max_buffer;\n#endif\n\treturn ret;\n}\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\nstatic bool selftests_can_run;\n\nstruct trace_selftests {\n\tstruct list_head\t\tlist;\n\tstruct tracer\t\t\t*type;\n};\n\nstatic LIST_HEAD(postponed_selftests);\n\nstatic int save_selftest(struct tracer *type)\n{\n\tstruct trace_selftests *selftest;\n\n\tselftest = kmalloc(sizeof(*selftest), GFP_KERNEL);\n\tif (!selftest)\n\t\treturn -ENOMEM;\n\n\tselftest->type = type;\n\tlist_add(&selftest->list, &postponed_selftests);\n\treturn 0;\n}\n\nstatic int run_tracer_selftest(struct tracer *type)\n{\n\tstruct trace_array *tr = &global_trace;\n\tstruct tracer *saved_tracer = tr->current_trace;\n\tint ret;\n\n\tif (!type->selftest || tracing_selftest_disabled)\n\t\treturn 0;\n\n\t \n\tif (!selftests_can_run)\n\t\treturn save_selftest(type);\n\n\tif (!tracing_is_on()) {\n\t\tpr_warn(\"Selftest for tracer %s skipped due to tracing disabled\\n\",\n\t\t\ttype->name);\n\t\treturn 0;\n\t}\n\n\t \n\ttracing_reset_online_cpus(&tr->array_buffer);\n\n\ttr->current_trace = type;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\t \n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, trace_buf_size,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t\ttr->allocated_snapshot = true;\n\t}\n#endif\n\n\t \n\tpr_info(\"Testing tracer %s: \", type->name);\n\tret = type->selftest(type, tr);\n\t \n\ttr->current_trace = saved_tracer;\n\tif (ret) {\n\t\tprintk(KERN_CONT \"FAILED!\\n\");\n\t\t \n\t\tWARN_ON(1);\n\t\treturn -1;\n\t}\n\t \n\ttracing_reset_online_cpus(&tr->array_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (type->use_max_tr) {\n\t\ttr->allocated_snapshot = false;\n\n\t\t \n\t\tif (ring_buffer_expanded)\n\t\t\tring_buffer_resize(tr->max_buffer.buffer, 1,\n\t\t\t\t\t   RING_BUFFER_ALL_CPUS);\n\t}\n#endif\n\n\tprintk(KERN_CONT \"PASSED\\n\");\n\treturn 0;\n}\n\nstatic int do_run_tracer_selftest(struct tracer *type)\n{\n\tint ret;\n\n\t \n\tcond_resched();\n\n\ttracing_selftest_running = true;\n\tret = run_tracer_selftest(type);\n\ttracing_selftest_running = false;\n\n\treturn ret;\n}\n\nstatic __init int init_trace_selftests(void)\n{\n\tstruct trace_selftests *p, *n;\n\tstruct tracer *t, **last;\n\tint ret;\n\n\tselftests_can_run = true;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (list_empty(&postponed_selftests))\n\t\tgoto out;\n\n\tpr_info(\"Running postponed tracer tests:\\n\");\n\n\ttracing_selftest_running = true;\n\tlist_for_each_entry_safe(p, n, &postponed_selftests, list) {\n\t\t \n\t\tcond_resched();\n\t\tret = run_tracer_selftest(p->type);\n\t\t \n\t\tif (ret < 0) {\n\t\t\tWARN(1, \"tracer: %s failed selftest, disabling\\n\",\n\t\t\t     p->type->name);\n\t\t\tlast = &trace_types;\n\t\t\tfor (t = trace_types; t; t = t->next) {\n\t\t\t\tif (t == p->type) {\n\t\t\t\t\t*last = t->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tlast = &t->next;\n\t\t\t}\n\t\t}\n\t\tlist_del(&p->list);\n\t\tkfree(p);\n\t}\n\ttracing_selftest_running = false;\n\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\ncore_initcall(init_trace_selftests);\n#else\nstatic inline int run_tracer_selftest(struct tracer *type)\n{\n\treturn 0;\n}\nstatic inline int do_run_tracer_selftest(struct tracer *type)\n{\n\treturn 0;\n}\n#endif  \n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t);\n\nstatic void __init apply_trace_boot_options(void);\n\n \nint __init register_tracer(struct tracer *type)\n{\n\tstruct tracer *t;\n\tint ret = 0;\n\n\tif (!type->name) {\n\t\tpr_info(\"Tracer must have a name\\n\");\n\t\treturn -1;\n\t}\n\n\tif (strlen(type->name) >= MAX_TRACER_SIZE) {\n\t\tpr_info(\"Tracer has a name longer than %d\\n\", MAX_TRACER_SIZE);\n\t\treturn -1;\n\t}\n\n\tif (security_locked_down(LOCKDOWN_TRACEFS)) {\n\t\tpr_warn(\"Can not register tracer %s due to lockdown\\n\",\n\t\t\t   type->name);\n\t\treturn -EPERM;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(type->name, t->name) == 0) {\n\t\t\t \n\t\t\tpr_info(\"Tracer %s already registered\\n\",\n\t\t\t\ttype->name);\n\t\t\tret = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!type->set_flag)\n\t\ttype->set_flag = &dummy_set_flag;\n\tif (!type->flags) {\n\t\t \n\t\ttype->flags = kmalloc(sizeof(*type->flags), GFP_KERNEL);\n\t\tif (!type->flags) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ttype->flags->val = 0;\n\t\ttype->flags->opts = dummy_tracer_opt;\n\t} else\n\t\tif (!type->flags->opts)\n\t\t\ttype->flags->opts = dummy_tracer_opt;\n\n\t \n\ttype->flags->trace = type;\n\n\tret = do_run_tracer_selftest(type);\n\tif (ret < 0)\n\t\tgoto out;\n\n\ttype->next = trace_types;\n\ttrace_types = type;\n\tadd_tracer_options(&global_trace, type);\n\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\tif (ret || !default_bootup_tracer)\n\t\tgoto out_unlock;\n\n\tif (strncmp(default_bootup_tracer, type->name, MAX_TRACER_SIZE))\n\t\tgoto out_unlock;\n\n\tprintk(KERN_INFO \"Starting tracer '%s'\\n\", type->name);\n\t \n\ttracing_set_tracer(&global_trace, type->name);\n\tdefault_bootup_tracer = NULL;\n\n\tapply_trace_boot_options();\n\n\t \n\tdisable_tracing_selftest(\"running a tracer\");\n\n out_unlock:\n\treturn ret;\n}\n\nstatic void tracing_reset_cpu(struct array_buffer *buf, int cpu)\n{\n\tstruct trace_buffer *buffer = buf->buffer;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t \n\tsynchronize_rcu();\n\tring_buffer_reset_cpu(buffer, cpu);\n\n\tring_buffer_record_enable(buffer);\n}\n\nvoid tracing_reset_online_cpus(struct array_buffer *buf)\n{\n\tstruct trace_buffer *buffer = buf->buffer;\n\n\tif (!buffer)\n\t\treturn;\n\n\tring_buffer_record_disable(buffer);\n\n\t \n\tsynchronize_rcu();\n\n\tbuf->time_start = buffer_ftrace_now(buf, buf->cpu);\n\n\tring_buffer_reset_online_cpus(buffer);\n\n\tring_buffer_record_enable(buffer);\n}\n\n \nvoid tracing_reset_all_online_cpus_unlocked(void)\n{\n\tstruct trace_array *tr;\n\n\tlockdep_assert_held(&trace_types_lock);\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->clear_trace)\n\t\t\tcontinue;\n\t\ttr->clear_trace = false;\n\t\ttracing_reset_online_cpus(&tr->array_buffer);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\t}\n}\n\nvoid tracing_reset_all_online_cpus(void)\n{\n\tmutex_lock(&trace_types_lock);\n\ttracing_reset_all_online_cpus_unlocked();\n\tmutex_unlock(&trace_types_lock);\n}\n\n \nstatic int *tgid_map;\n\n \nstatic size_t tgid_map_max;\n\n#define SAVED_CMDLINES_DEFAULT 128\n#define NO_CMDLINE_MAP UINT_MAX\n \nstatic arch_spinlock_t trace_cmdline_lock = __ARCH_SPIN_LOCK_UNLOCKED;\nstruct saved_cmdlines_buffer {\n\tunsigned map_pid_to_cmdline[PID_MAX_DEFAULT+1];\n\tunsigned *map_cmdline_to_pid;\n\tunsigned cmdline_num;\n\tint cmdline_idx;\n\tchar *saved_cmdlines;\n};\nstatic struct saved_cmdlines_buffer *savedcmd;\n\nstatic inline char *get_saved_cmdlines(int idx)\n{\n\treturn &savedcmd->saved_cmdlines[idx * TASK_COMM_LEN];\n}\n\nstatic inline void set_cmdline(int idx, const char *cmdline)\n{\n\tstrncpy(get_saved_cmdlines(idx), cmdline, TASK_COMM_LEN);\n}\n\nstatic int allocate_cmdlines_buffer(unsigned int val,\n\t\t\t\t    struct saved_cmdlines_buffer *s)\n{\n\ts->map_cmdline_to_pid = kmalloc_array(val,\n\t\t\t\t\t      sizeof(*s->map_cmdline_to_pid),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!s->map_cmdline_to_pid)\n\t\treturn -ENOMEM;\n\n\ts->saved_cmdlines = kmalloc_array(TASK_COMM_LEN, val, GFP_KERNEL);\n\tif (!s->saved_cmdlines) {\n\t\tkfree(s->map_cmdline_to_pid);\n\t\treturn -ENOMEM;\n\t}\n\n\ts->cmdline_idx = 0;\n\ts->cmdline_num = val;\n\tmemset(&s->map_pid_to_cmdline, NO_CMDLINE_MAP,\n\t       sizeof(s->map_pid_to_cmdline));\n\tmemset(s->map_cmdline_to_pid, NO_CMDLINE_MAP,\n\t       val * sizeof(*s->map_cmdline_to_pid));\n\n\treturn 0;\n}\n\nstatic int trace_create_savedcmd(void)\n{\n\tint ret;\n\n\tsavedcmd = kmalloc(sizeof(*savedcmd), GFP_KERNEL);\n\tif (!savedcmd)\n\t\treturn -ENOMEM;\n\n\tret = allocate_cmdlines_buffer(SAVED_CMDLINES_DEFAULT, savedcmd);\n\tif (ret < 0) {\n\t\tkfree(savedcmd);\n\t\tsavedcmd = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nint is_tracing_stopped(void)\n{\n\treturn global_trace.stop_count;\n}\n\nstatic void tracing_start_tr(struct trace_array *tr)\n{\n\tstruct trace_buffer *buffer;\n\tunsigned long flags;\n\n\tif (tracing_disabled)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\tif (--tr->stop_count) {\n\t\tif (WARN_ON_ONCE(tr->stop_count < 0)) {\n\t\t\t \n\t\t\ttr->stop_count = 0;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\tarch_spin_lock(&tr->max_lock);\n\n\tbuffer = tr->array_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = tr->max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_enable(buffer);\n#endif\n\n\tarch_spin_unlock(&tr->max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\n \nvoid tracing_start(void)\n\n{\n\treturn tracing_start_tr(&global_trace);\n}\n\nstatic void tracing_stop_tr(struct trace_array *tr)\n{\n\tstruct trace_buffer *buffer;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&tr->start_lock, flags);\n\tif (tr->stop_count++)\n\t\tgoto out;\n\n\t \n\tarch_spin_lock(&tr->max_lock);\n\n\tbuffer = tr->array_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbuffer = tr->max_buffer.buffer;\n\tif (buffer)\n\t\tring_buffer_record_disable(buffer);\n#endif\n\n\tarch_spin_unlock(&tr->max_lock);\n\n out:\n\traw_spin_unlock_irqrestore(&tr->start_lock, flags);\n}\n\n \nvoid tracing_stop(void)\n{\n\treturn tracing_stop_tr(&global_trace);\n}\n\nstatic int trace_save_cmdline(struct task_struct *tsk)\n{\n\tunsigned tpid, idx;\n\n\t \n\tif (!tsk->pid)\n\t\treturn 1;\n\n\ttpid = tsk->pid & (PID_MAX_DEFAULT - 1);\n\n\t \n\tlockdep_assert_preemption_disabled();\n\tif (!arch_spin_trylock(&trace_cmdline_lock))\n\t\treturn 0;\n\n\tidx = savedcmd->map_pid_to_cmdline[tpid];\n\tif (idx == NO_CMDLINE_MAP) {\n\t\tidx = (savedcmd->cmdline_idx + 1) % savedcmd->cmdline_num;\n\n\t\tsavedcmd->map_pid_to_cmdline[tpid] = idx;\n\t\tsavedcmd->cmdline_idx = idx;\n\t}\n\n\tsavedcmd->map_cmdline_to_pid[idx] = tsk->pid;\n\tset_cmdline(idx, tsk->comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\n\treturn 1;\n}\n\nstatic void __trace_find_cmdline(int pid, char comm[])\n{\n\tunsigned map;\n\tint tpid;\n\n\tif (!pid) {\n\t\tstrcpy(comm, \"<idle>\");\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(pid < 0)) {\n\t\tstrcpy(comm, \"<XXX>\");\n\t\treturn;\n\t}\n\n\ttpid = pid & (PID_MAX_DEFAULT - 1);\n\tmap = savedcmd->map_pid_to_cmdline[tpid];\n\tif (map != NO_CMDLINE_MAP) {\n\t\ttpid = savedcmd->map_cmdline_to_pid[map];\n\t\tif (tpid == pid) {\n\t\t\tstrscpy(comm, get_saved_cmdlines(map), TASK_COMM_LEN);\n\t\t\treturn;\n\t\t}\n\t}\n\tstrcpy(comm, \"<...>\");\n}\n\nvoid trace_find_cmdline(int pid, char comm[])\n{\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\t__trace_find_cmdline(pid, comm);\n\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nstatic int *trace_find_tgid_ptr(int pid)\n{\n\t \n\tint *map = smp_load_acquire(&tgid_map);\n\n\tif (unlikely(!map || pid > tgid_map_max))\n\t\treturn NULL;\n\n\treturn &map[pid];\n}\n\nint trace_find_tgid(int pid)\n{\n\tint *ptr = trace_find_tgid_ptr(pid);\n\n\treturn ptr ? *ptr : 0;\n}\n\nstatic int trace_save_tgid(struct task_struct *tsk)\n{\n\tint *ptr;\n\n\t \n\tif (!tsk->pid)\n\t\treturn 1;\n\n\tptr = trace_find_tgid_ptr(tsk->pid);\n\tif (!ptr)\n\t\treturn 0;\n\n\t*ptr = tsk->tgid;\n\treturn 1;\n}\n\nstatic bool tracing_record_taskinfo_skip(int flags)\n{\n\tif (unlikely(!(flags & (TRACE_RECORD_CMDLINE | TRACE_RECORD_TGID))))\n\t\treturn true;\n\tif (!__this_cpu_read(trace_taskinfo_save))\n\t\treturn true;\n\treturn false;\n}\n\n \nvoid tracing_record_taskinfo(struct task_struct *task, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t \n\tdone = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(task);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(task);\n\n\t \n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n \nvoid tracing_record_taskinfo_sched_switch(struct task_struct *prev,\n\t\t\t\t\t  struct task_struct *next, int flags)\n{\n\tbool done;\n\n\tif (tracing_record_taskinfo_skip(flags))\n\t\treturn;\n\n\t \n\tdone  = !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(prev);\n\tdone &= !(flags & TRACE_RECORD_CMDLINE) || trace_save_cmdline(next);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(prev);\n\tdone &= !(flags & TRACE_RECORD_TGID) || trace_save_tgid(next);\n\n\t \n\tif (!done)\n\t\treturn;\n\n\t__this_cpu_write(trace_taskinfo_save, false);\n}\n\n \nvoid tracing_record_cmdline(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_CMDLINE);\n}\n\nvoid tracing_record_tgid(struct task_struct *task)\n{\n\ttracing_record_taskinfo(task, TRACE_RECORD_TGID);\n}\n\n \nenum print_line_t trace_handle_return(struct trace_seq *s)\n{\n\treturn trace_seq_has_overflowed(s) ?\n\t\tTRACE_TYPE_PARTIAL_LINE : TRACE_TYPE_HANDLED;\n}\nEXPORT_SYMBOL_GPL(trace_handle_return);\n\nstatic unsigned short migration_disable_value(void)\n{\n#if defined(CONFIG_SMP)\n\treturn current->migration_disabled;\n#else\n\treturn 0;\n#endif\n}\n\nunsigned int tracing_gen_ctx_irq_test(unsigned int irqs_status)\n{\n\tunsigned int trace_flags = irqs_status;\n\tunsigned int pc;\n\n\tpc = preempt_count();\n\n\tif (pc & NMI_MASK)\n\t\ttrace_flags |= TRACE_FLAG_NMI;\n\tif (pc & HARDIRQ_MASK)\n\t\ttrace_flags |= TRACE_FLAG_HARDIRQ;\n\tif (in_serving_softirq())\n\t\ttrace_flags |= TRACE_FLAG_SOFTIRQ;\n\tif (softirq_count() >> (SOFTIRQ_SHIFT + 1))\n\t\ttrace_flags |= TRACE_FLAG_BH_OFF;\n\n\tif (tif_need_resched())\n\t\ttrace_flags |= TRACE_FLAG_NEED_RESCHED;\n\tif (test_preempt_need_resched())\n\t\ttrace_flags |= TRACE_FLAG_PREEMPT_RESCHED;\n\treturn (trace_flags << 16) | (min_t(unsigned int, pc & 0xff, 0xf)) |\n\t\t(min_t(unsigned int, migration_disable_value(), 0xf)) << 4;\n}\n\nstruct ring_buffer_event *\ntrace_buffer_lock_reserve(struct trace_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned int trace_ctx)\n{\n\treturn __trace_buffer_lock_reserve(buffer, type, len, trace_ctx);\n}\n\nDEFINE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);\nDEFINE_PER_CPU(int, trace_buffered_event_cnt);\nstatic int trace_buffered_event_ref;\n\n \nvoid trace_buffered_event_enable(void)\n{\n\tstruct ring_buffer_event *event;\n\tstruct page *page;\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (trace_buffered_event_ref++)\n\t\treturn;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tpage = alloc_pages_node(cpu_to_node(cpu),\n\t\t\t\t\tGFP_KERNEL | __GFP_NORETRY, 0);\n\t\t \n\t\tif (!page) {\n\t\t\tpr_err(\"Failed to allocate event buffer\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tevent = page_address(page);\n\t\tmemset(event, 0, sizeof(*event));\n\n\t\tper_cpu(trace_buffered_event, cpu) = event;\n\n\t\tpreempt_disable();\n\t\tif (cpu == smp_processor_id() &&\n\t\t    __this_cpu_read(trace_buffered_event) !=\n\t\t    per_cpu(trace_buffered_event, cpu))\n\t\t\tWARN_ON_ONCE(1);\n\t\tpreempt_enable();\n\t}\n}\n\nstatic void enable_trace_buffered_event(void *data)\n{\n\t \n\tsmp_rmb();\n\tthis_cpu_dec(trace_buffered_event_cnt);\n}\n\nstatic void disable_trace_buffered_event(void *data)\n{\n\tthis_cpu_inc(trace_buffered_event_cnt);\n}\n\n \nvoid trace_buffered_event_disable(void)\n{\n\tint cpu;\n\n\tWARN_ON_ONCE(!mutex_is_locked(&event_mutex));\n\n\tif (WARN_ON_ONCE(!trace_buffered_event_ref))\n\t\treturn;\n\n\tif (--trace_buffered_event_ref)\n\t\treturn;\n\n\t \n\ton_each_cpu_mask(tracing_buffer_mask, disable_trace_buffered_event,\n\t\t\t NULL, true);\n\n\t \n\tsynchronize_rcu();\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tfree_page((unsigned long)per_cpu(trace_buffered_event, cpu));\n\t\tper_cpu(trace_buffered_event, cpu) = NULL;\n\t}\n\n\t \n\tsynchronize_rcu();\n\n\t \n\ton_each_cpu_mask(tracing_buffer_mask, enable_trace_buffered_event, NULL,\n\t\t\t true);\n}\n\nstatic struct trace_buffer *temp_buffer;\n\nstruct ring_buffer_event *\ntrace_event_buffer_lock_reserve(struct trace_buffer **current_rb,\n\t\t\t  struct trace_event_file *trace_file,\n\t\t\t  int type, unsigned long len,\n\t\t\t  unsigned int trace_ctx)\n{\n\tstruct ring_buffer_event *entry;\n\tstruct trace_array *tr = trace_file->tr;\n\tint val;\n\n\t*current_rb = tr->array_buffer.buffer;\n\n\tif (!tr->no_filter_buffering_ref &&\n\t    (trace_file->flags & (EVENT_FILE_FL_SOFT_DISABLED | EVENT_FILE_FL_FILTERED))) {\n\t\tpreempt_disable_notrace();\n\t\t \n\t\tif ((entry = __this_cpu_read(trace_buffered_event))) {\n\t\t\tint max_len = PAGE_SIZE - struct_size(entry, array, 1);\n\n\t\t\tval = this_cpu_inc_return(trace_buffered_event_cnt);\n\n\t\t\t \n\t\t\tif (val == 1 && likely(len <= max_len)) {\n\t\t\t\ttrace_event_setup(entry, type, trace_ctx);\n\t\t\t\tentry->array[0] = len;\n\t\t\t\t \n\t\t\t\treturn entry;\n\t\t\t}\n\t\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t\t}\n\t\t \n\t\tpreempt_enable_notrace();\n\t}\n\n\tentry = __trace_buffer_lock_reserve(*current_rb, type, len,\n\t\t\t\t\t    trace_ctx);\n\t \n\tif (!entry && trace_file->flags & EVENT_FILE_FL_TRIGGER_COND) {\n\t\t*current_rb = temp_buffer;\n\t\tentry = __trace_buffer_lock_reserve(*current_rb, type, len,\n\t\t\t\t\t\t    trace_ctx);\n\t}\n\treturn entry;\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_lock_reserve);\n\nstatic DEFINE_RAW_SPINLOCK(tracepoint_iter_lock);\nstatic DEFINE_MUTEX(tracepoint_printk_mutex);\n\nstatic void output_printk(struct trace_event_buffer *fbuffer)\n{\n\tstruct trace_event_call *event_call;\n\tstruct trace_event_file *file;\n\tstruct trace_event *event;\n\tunsigned long flags;\n\tstruct trace_iterator *iter = tracepoint_print_iter;\n\n\t \n\tif (WARN_ON_ONCE(!iter))\n\t\treturn;\n\n\tevent_call = fbuffer->trace_file->event_call;\n\tif (!event_call || !event_call->event.funcs ||\n\t    !event_call->event.funcs->trace)\n\t\treturn;\n\n\tfile = fbuffer->trace_file;\n\tif (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags) ||\n\t    (unlikely(file->flags & EVENT_FILE_FL_FILTERED) &&\n\t     !filter_match_preds(file->filter, fbuffer->entry)))\n\t\treturn;\n\n\tevent = &fbuffer->trace_file->event_call->event;\n\n\traw_spin_lock_irqsave(&tracepoint_iter_lock, flags);\n\ttrace_seq_init(&iter->seq);\n\titer->ent = fbuffer->entry;\n\tevent_call->event.funcs->trace(iter, 0, event);\n\ttrace_seq_putc(&iter->seq, 0);\n\tprintk(\"%s\", iter->seq.buffer);\n\n\traw_spin_unlock_irqrestore(&tracepoint_iter_lock, flags);\n}\n\nint tracepoint_printk_sysctl(struct ctl_table *table, int write,\n\t\t\t     void *buffer, size_t *lenp,\n\t\t\t     loff_t *ppos)\n{\n\tint save_tracepoint_printk;\n\tint ret;\n\n\tmutex_lock(&tracepoint_printk_mutex);\n\tsave_tracepoint_printk = tracepoint_printk;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\t \n\tif (!tracepoint_print_iter)\n\t\ttracepoint_printk = 0;\n\n\tif (save_tracepoint_printk == tracepoint_printk)\n\t\tgoto out;\n\n\tif (tracepoint_printk)\n\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\telse\n\t\tstatic_key_disable(&tracepoint_printk_key.key);\n\n out:\n\tmutex_unlock(&tracepoint_printk_mutex);\n\n\treturn ret;\n}\n\nvoid trace_event_buffer_commit(struct trace_event_buffer *fbuffer)\n{\n\tenum event_trigger_type tt = ETT_NONE;\n\tstruct trace_event_file *file = fbuffer->trace_file;\n\n\tif (__event_trigger_test_discard(file, fbuffer->buffer, fbuffer->event,\n\t\t\tfbuffer->entry, &tt))\n\t\tgoto discard;\n\n\tif (static_key_false(&tracepoint_printk_key.key))\n\t\toutput_printk(fbuffer);\n\n\tif (static_branch_unlikely(&trace_event_exports_enabled))\n\t\tftrace_exports(fbuffer->event, TRACE_EXPORT_EVENT);\n\n\ttrace_buffer_unlock_commit_regs(file->tr, fbuffer->buffer,\n\t\t\tfbuffer->event, fbuffer->trace_ctx, fbuffer->regs);\n\ndiscard:\n\tif (tt)\n\t\tevent_triggers_post_call(file, tt);\n\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_commit);\n\n \n# define STACK_SKIP 3\n\nvoid trace_buffer_unlock_commit_regs(struct trace_array *tr,\n\t\t\t\t     struct trace_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event,\n\t\t\t\t     unsigned int trace_ctx,\n\t\t\t\t     struct pt_regs *regs)\n{\n\t__buffer_unlock_commit(buffer, event);\n\n\t \n\tftrace_trace_stack(tr, buffer, trace_ctx, regs ? 0 : STACK_SKIP, regs);\n\tftrace_trace_userstack(tr, buffer, trace_ctx);\n}\n\n \nvoid\ntrace_buffer_unlock_commit_nostack(struct trace_buffer *buffer,\n\t\t\t\t   struct ring_buffer_event *event)\n{\n\t__buffer_unlock_commit(buffer, event);\n}\n\nvoid\ntrace_function(struct trace_array *tr, unsigned long ip, unsigned long\n\t       parent_ip, unsigned int trace_ctx)\n{\n\tstruct trace_event_call *call = &event_function;\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FN, sizeof(*entry),\n\t\t\t\t\t    trace_ctx);\n\tif (!event)\n\t\treturn;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\tif (static_branch_unlikely(&trace_function_exports_enabled))\n\t\t\tftrace_exports(event, TRACE_EXPORT_FUNCTION);\n\t\t__buffer_unlock_commit(buffer, event);\n\t}\n}\n\n#ifdef CONFIG_STACKTRACE\n\n \n#define FTRACE_KSTACK_NESTING\t4\n\n#define FTRACE_KSTACK_ENTRIES\t(PAGE_SIZE / FTRACE_KSTACK_NESTING)\n\nstruct ftrace_stack {\n\tunsigned long\t\tcalls[FTRACE_KSTACK_ENTRIES];\n};\n\n\nstruct ftrace_stacks {\n\tstruct ftrace_stack\tstacks[FTRACE_KSTACK_NESTING];\n};\n\nstatic DEFINE_PER_CPU(struct ftrace_stacks, ftrace_stacks);\nstatic DEFINE_PER_CPU(int, ftrace_stack_reserve);\n\nstatic void __ftrace_trace_stack(struct trace_buffer *buffer,\n\t\t\t\t unsigned int trace_ctx,\n\t\t\t\t int skip, struct pt_regs *regs)\n{\n\tstruct trace_event_call *call = &event_kernel_stack;\n\tstruct ring_buffer_event *event;\n\tunsigned int size, nr_entries;\n\tstruct ftrace_stack *fstack;\n\tstruct stack_entry *entry;\n\tint stackidx;\n\n\t \n#ifndef CONFIG_UNWINDER_ORC\n\tif (!regs)\n\t\tskip++;\n#endif\n\n\tpreempt_disable_notrace();\n\n\tstackidx = __this_cpu_inc_return(ftrace_stack_reserve) - 1;\n\n\t \n\tif (WARN_ON_ONCE(stackidx >= FTRACE_KSTACK_NESTING))\n\t\tgoto out;\n\n\t \n\tbarrier();\n\n\tfstack = this_cpu_ptr(ftrace_stacks.stacks) + stackidx;\n\tsize = ARRAY_SIZE(fstack->calls);\n\n\tif (regs) {\n\t\tnr_entries = stack_trace_save_regs(regs, fstack->calls,\n\t\t\t\t\t\t   size, skip);\n\t} else {\n\t\tnr_entries = stack_trace_save(fstack->calls, size, skip);\n\t}\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_STACK,\n\t\t\t\t    struct_size(entry, caller, nr_entries),\n\t\t\t\t    trace_ctx);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\n\tentry->size = nr_entries;\n\tmemcpy(&entry->caller, fstack->calls,\n\t       flex_array_size(entry, caller, nr_entries));\n\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out:\n\t \n\tbarrier();\n\t__this_cpu_dec(ftrace_stack_reserve);\n\tpreempt_enable_notrace();\n\n}\n\nstatic inline void ftrace_trace_stack(struct trace_array *tr,\n\t\t\t\t      struct trace_buffer *buffer,\n\t\t\t\t      unsigned int trace_ctx,\n\t\t\t\t      int skip, struct pt_regs *regs)\n{\n\tif (!(tr->trace_flags & TRACE_ITER_STACKTRACE))\n\t\treturn;\n\n\t__ftrace_trace_stack(buffer, trace_ctx, skip, regs);\n}\n\nvoid __trace_stack(struct trace_array *tr, unsigned int trace_ctx,\n\t\t   int skip)\n{\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\n\tif (rcu_is_watching()) {\n\t\t__ftrace_trace_stack(buffer, trace_ctx, skip, NULL);\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(IS_ENABLED(CONFIG_GENERIC_ENTRY)))\n\t\treturn;\n\n\t \n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\tct_irq_enter_irqson();\n\t__ftrace_trace_stack(buffer, trace_ctx, skip, NULL);\n\tct_irq_exit_irqson();\n}\n\n \nvoid trace_dump_stack(int skip)\n{\n\tif (tracing_disabled || tracing_selftest_running)\n\t\treturn;\n\n#ifndef CONFIG_UNWINDER_ORC\n\t \n\tskip++;\n#endif\n\t__ftrace_trace_stack(global_trace.array_buffer.buffer,\n\t\t\t     tracing_gen_ctx(), skip, NULL);\n}\nEXPORT_SYMBOL_GPL(trace_dump_stack);\n\n#ifdef CONFIG_USER_STACKTRACE_SUPPORT\nstatic DEFINE_PER_CPU(int, user_stack_count);\n\nstatic void\nftrace_trace_userstack(struct trace_array *tr,\n\t\t       struct trace_buffer *buffer, unsigned int trace_ctx)\n{\n\tstruct trace_event_call *call = &event_user_stack;\n\tstruct ring_buffer_event *event;\n\tstruct userstack_entry *entry;\n\n\tif (!(tr->trace_flags & TRACE_ITER_USERSTACKTRACE))\n\t\treturn;\n\n\t \n\tif (unlikely(in_nmi()))\n\t\treturn;\n\n\t \n\tpreempt_disable();\n\tif (__this_cpu_read(user_stack_count))\n\t\tgoto out;\n\n\t__this_cpu_inc(user_stack_count);\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_USER_STACK,\n\t\t\t\t\t    sizeof(*entry), trace_ctx);\n\tif (!event)\n\t\tgoto out_drop_count;\n\tentry\t= ring_buffer_event_data(event);\n\n\tentry->tgid\t\t= current->tgid;\n\tmemset(&entry->caller, 0, sizeof(entry->caller));\n\n\tstack_trace_save_user(entry->caller, FTRACE_STACK_ENTRIES);\n\tif (!call_filter_check_discard(call, entry, buffer, event))\n\t\t__buffer_unlock_commit(buffer, event);\n\n out_drop_count:\n\t__this_cpu_dec(user_stack_count);\n out:\n\tpreempt_enable();\n}\n#else  \nstatic void ftrace_trace_userstack(struct trace_array *tr,\n\t\t\t\t   struct trace_buffer *buffer,\n\t\t\t\t   unsigned int trace_ctx)\n{\n}\n#endif  \n\n#endif  \n\nstatic inline void\nfunc_repeats_set_delta_ts(struct func_repeats_entry *entry,\n\t\t\t  unsigned long long delta)\n{\n\tentry->bottom_delta_ts = delta & U32_MAX;\n\tentry->top_delta_ts = (delta >> 32);\n}\n\nvoid trace_last_func_repeats(struct trace_array *tr,\n\t\t\t     struct trace_func_repeats *last_info,\n\t\t\t     unsigned int trace_ctx)\n{\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\tstruct func_repeats_entry *entry;\n\tstruct ring_buffer_event *event;\n\tu64 delta;\n\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_FUNC_REPEATS,\n\t\t\t\t\t    sizeof(*entry), trace_ctx);\n\tif (!event)\n\t\treturn;\n\n\tdelta = ring_buffer_event_time_stamp(buffer, event) -\n\t\tlast_info->ts_last_call;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = last_info->ip;\n\tentry->parent_ip = last_info->parent_ip;\n\tentry->count = last_info->count;\n\tfunc_repeats_set_delta_ts(entry, delta);\n\n\t__buffer_unlock_commit(buffer, event);\n}\n\n \nstruct trace_buffer_struct {\n\tint nesting;\n\tchar buffer[4][TRACE_BUF_SIZE];\n};\n\nstatic struct trace_buffer_struct __percpu *trace_percpu_buffer;\n\n \nstatic char *get_trace_buf(void)\n{\n\tstruct trace_buffer_struct *buffer = this_cpu_ptr(trace_percpu_buffer);\n\n\tif (!trace_percpu_buffer || buffer->nesting >= 4)\n\t\treturn NULL;\n\n\tbuffer->nesting++;\n\n\t \n\tbarrier();\n\treturn &buffer->buffer[buffer->nesting - 1][0];\n}\n\nstatic void put_trace_buf(void)\n{\n\t \n\tbarrier();\n\tthis_cpu_dec(trace_percpu_buffer->nesting);\n}\n\nstatic int alloc_percpu_trace_buffer(void)\n{\n\tstruct trace_buffer_struct __percpu *buffers;\n\n\tif (trace_percpu_buffer)\n\t\treturn 0;\n\n\tbuffers = alloc_percpu(struct trace_buffer_struct);\n\tif (MEM_FAIL(!buffers, \"Could not allocate percpu trace_printk buffer\"))\n\t\treturn -ENOMEM;\n\n\ttrace_percpu_buffer = buffers;\n\treturn 0;\n}\n\nstatic int buffers_allocated;\n\nvoid trace_printk_init_buffers(void)\n{\n\tif (buffers_allocated)\n\t\treturn;\n\n\tif (alloc_percpu_trace_buffer())\n\t\treturn;\n\n\t \n\n\tpr_warn(\"\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** trace_printk() being used. Allocating extra memory.  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** This means that this is a DEBUG kernel and it is     **\\n\");\n\tpr_warn(\"** unsafe for production use.                           **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"** If you see this message and you are not debugging    **\\n\");\n\tpr_warn(\"** the kernel, report this immediately to your vendor!  **\\n\");\n\tpr_warn(\"**                                                      **\\n\");\n\tpr_warn(\"**   NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE   **\\n\");\n\tpr_warn(\"**********************************************************\\n\");\n\n\t \n\ttracing_update_buffers();\n\n\tbuffers_allocated = 1;\n\n\t \n\tif (global_trace.array_buffer.buffer)\n\t\ttracing_start_cmdline_record();\n}\nEXPORT_SYMBOL_GPL(trace_printk_init_buffers);\n\nvoid trace_printk_start_comm(void)\n{\n\t \n\tif (!buffers_allocated)\n\t\treturn;\n\ttracing_start_cmdline_record();\n}\n\nstatic void trace_printk_start_stop_comm(int enabled)\n{\n\tif (!buffers_allocated)\n\t\treturn;\n\n\tif (enabled)\n\t\ttracing_start_cmdline_record();\n\telse\n\t\ttracing_stop_cmdline_record();\n}\n\n \nint trace_vbprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_bprint;\n\tstruct ring_buffer_event *event;\n\tstruct trace_buffer *buffer;\n\tstruct trace_array *tr = &global_trace;\n\tstruct bprint_entry *entry;\n\tunsigned int trace_ctx;\n\tchar *tbuffer;\n\tint len = 0, size;\n\n\tif (unlikely(tracing_selftest_running || tracing_disabled))\n\t\treturn 0;\n\n\t \n\tpause_graph_tracing();\n\n\ttrace_ctx = tracing_gen_ctx();\n\tpreempt_disable_notrace();\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vbin_printf((u32 *)tbuffer, TRACE_BUF_SIZE/sizeof(int), fmt, args);\n\n\tif (len > TRACE_BUF_SIZE/sizeof(int) || len < 0)\n\t\tgoto out_put;\n\n\tsize = sizeof(*entry) + sizeof(u32) * len;\n\tbuffer = tr->array_buffer.buffer;\n\tring_buffer_nest_start(buffer);\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_BPRINT, size,\n\t\t\t\t\t    trace_ctx);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->fmt\t\t\t= fmt;\n\n\tmemcpy(entry->buf, tbuffer, sizeof(u32) * len);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(tr, buffer, trace_ctx, 6, NULL);\n\t}\n\nout:\n\tring_buffer_nest_end(buffer);\nout_put:\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\nEXPORT_SYMBOL_GPL(trace_vbprintk);\n\n__printf(3, 0)\nstatic int\n__trace_array_vprintk(struct trace_buffer *buffer,\n\t\t      unsigned long ip, const char *fmt, va_list args)\n{\n\tstruct trace_event_call *call = &event_print;\n\tstruct ring_buffer_event *event;\n\tint len = 0, size;\n\tstruct print_entry *entry;\n\tunsigned int trace_ctx;\n\tchar *tbuffer;\n\n\tif (tracing_disabled)\n\t\treturn 0;\n\n\t \n\tpause_graph_tracing();\n\n\ttrace_ctx = tracing_gen_ctx();\n\tpreempt_disable_notrace();\n\n\n\ttbuffer = get_trace_buf();\n\tif (!tbuffer) {\n\t\tlen = 0;\n\t\tgoto out_nobuffer;\n\t}\n\n\tlen = vscnprintf(tbuffer, TRACE_BUF_SIZE, fmt, args);\n\n\tsize = sizeof(*entry) + len + 1;\n\tring_buffer_nest_start(buffer);\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    trace_ctx);\n\tif (!event)\n\t\tgoto out;\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = ip;\n\n\tmemcpy(&entry->buf, tbuffer, len + 1);\n\tif (!call_filter_check_discard(call, entry, buffer, event)) {\n\t\t__buffer_unlock_commit(buffer, event);\n\t\tftrace_trace_stack(&global_trace, buffer, trace_ctx, 6, NULL);\n\t}\n\nout:\n\tring_buffer_nest_end(buffer);\n\tput_trace_buf();\n\nout_nobuffer:\n\tpreempt_enable_notrace();\n\tunpause_graph_tracing();\n\n\treturn len;\n}\n\n__printf(3, 0)\nint trace_array_vprintk(struct trace_array *tr,\n\t\t\tunsigned long ip, const char *fmt, va_list args)\n{\n\tif (tracing_selftest_running && tr == &global_trace)\n\t\treturn 0;\n\n\treturn __trace_array_vprintk(tr->array_buffer.buffer, ip, fmt, args);\n}\n\n \n__printf(3, 0)\nint trace_array_printk(struct trace_array *tr,\n\t\t       unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!tr)\n\t\treturn -ENOENT;\n\n\t \n\tif (tr == &global_trace)\n\t\treturn 0;\n\n\tif (!(tr->trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = trace_array_vprintk(tr, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(trace_array_printk);\n\n \nint trace_array_init_printk(struct trace_array *tr)\n{\n\tif (!tr)\n\t\treturn -ENOENT;\n\n\t \n\tif (tr == &global_trace)\n\t\treturn -EINVAL;\n\n\treturn alloc_percpu_trace_buffer();\n}\nEXPORT_SYMBOL_GPL(trace_array_init_printk);\n\n__printf(3, 4)\nint trace_array_printk_buf(struct trace_buffer *buffer,\n\t\t\t   unsigned long ip, const char *fmt, ...)\n{\n\tint ret;\n\tva_list ap;\n\n\tif (!(global_trace.trace_flags & TRACE_ITER_PRINTK))\n\t\treturn 0;\n\n\tva_start(ap, fmt);\n\tret = __trace_array_vprintk(buffer, ip, fmt, ap);\n\tva_end(ap);\n\treturn ret;\n}\n\n__printf(2, 0)\nint trace_vprintk(unsigned long ip, const char *fmt, va_list args)\n{\n\treturn trace_array_vprintk(&global_trace, ip, fmt, args);\n}\nEXPORT_SYMBOL_GPL(trace_vprintk);\n\nstatic void trace_iterator_increment(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, iter->cpu);\n\n\titer->idx++;\n\tif (buf_iter)\n\t\tring_buffer_iter_advance(buf_iter);\n}\n\nstatic struct trace_entry *\npeek_next_entry(struct trace_iterator *iter, int cpu, u64 *ts,\n\t\tunsigned long *lost_events)\n{\n\tstruct ring_buffer_event *event;\n\tstruct ring_buffer_iter *buf_iter = trace_buffer_iter(iter, cpu);\n\n\tif (buf_iter) {\n\t\tevent = ring_buffer_iter_peek(buf_iter, ts);\n\t\tif (lost_events)\n\t\t\t*lost_events = ring_buffer_iter_dropped(buf_iter) ?\n\t\t\t\t(unsigned long)-1 : 0;\n\t} else {\n\t\tevent = ring_buffer_peek(iter->array_buffer->buffer, cpu, ts,\n\t\t\t\t\t lost_events);\n\t}\n\n\tif (event) {\n\t\titer->ent_size = ring_buffer_event_length(event);\n\t\treturn ring_buffer_event_data(event);\n\t}\n\titer->ent_size = 0;\n\treturn NULL;\n}\n\nstatic struct trace_entry *\n__find_next_entry(struct trace_iterator *iter, int *ent_cpu,\n\t\t  unsigned long *missing_events, u64 *ent_ts)\n{\n\tstruct trace_buffer *buffer = iter->array_buffer->buffer;\n\tstruct trace_entry *ent, *next = NULL;\n\tunsigned long lost_events = 0, next_lost = 0;\n\tint cpu_file = iter->cpu_file;\n\tu64 next_ts = 0, ts;\n\tint next_cpu = -1;\n\tint next_size = 0;\n\tint cpu;\n\n\t \n\tif (cpu_file > RING_BUFFER_ALL_CPUS) {\n\t\tif (ring_buffer_empty_cpu(buffer, cpu_file))\n\t\t\treturn NULL;\n\t\tent = peek_next_entry(iter, cpu_file, ent_ts, missing_events);\n\t\tif (ent_cpu)\n\t\t\t*ent_cpu = cpu_file;\n\n\t\treturn ent;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\n\t\tif (ring_buffer_empty_cpu(buffer, cpu))\n\t\t\tcontinue;\n\n\t\tent = peek_next_entry(iter, cpu, &ts, &lost_events);\n\n\t\t \n\t\tif (ent && (!next || ts < next_ts)) {\n\t\t\tnext = ent;\n\t\t\tnext_cpu = cpu;\n\t\t\tnext_ts = ts;\n\t\t\tnext_lost = lost_events;\n\t\t\tnext_size = iter->ent_size;\n\t\t}\n\t}\n\n\titer->ent_size = next_size;\n\n\tif (ent_cpu)\n\t\t*ent_cpu = next_cpu;\n\n\tif (ent_ts)\n\t\t*ent_ts = next_ts;\n\n\tif (missing_events)\n\t\t*missing_events = next_lost;\n\n\treturn next;\n}\n\n#define STATIC_FMT_BUF_SIZE\t128\nstatic char static_fmt_buf[STATIC_FMT_BUF_SIZE];\n\nchar *trace_iter_expand_format(struct trace_iterator *iter)\n{\n\tchar *tmp;\n\n\t \n\tif (!iter->tr || iter->fmt == static_fmt_buf)\n\t\treturn NULL;\n\n\ttmp = krealloc(iter->fmt, iter->fmt_size + STATIC_FMT_BUF_SIZE,\n\t\t       GFP_KERNEL);\n\tif (tmp) {\n\t\titer->fmt_size += STATIC_FMT_BUF_SIZE;\n\t\titer->fmt = tmp;\n\t}\n\n\treturn tmp;\n}\n\n \nstatic bool trace_safe_str(struct trace_iterator *iter, const char *str,\n\t\t\t   bool star, int len)\n{\n\tunsigned long addr = (unsigned long)str;\n\tstruct trace_event *trace_event;\n\tstruct trace_event_call *event;\n\n\t \n\tif (star && !len)\n\t\treturn true;\n\n\t \n\tif ((addr >= (unsigned long)iter->ent) &&\n\t    (addr < (unsigned long)iter->ent + iter->ent_size))\n\t\treturn true;\n\n\t \n\tif ((addr >= (unsigned long)iter->tmp_seq.buffer) &&\n\t    (addr < (unsigned long)iter->tmp_seq.buffer + PAGE_SIZE))\n\t\treturn true;\n\n\t \n\tif (is_kernel_rodata(addr))\n\t\treturn true;\n\n\tif (trace_is_tracepoint_string(str))\n\t\treturn true;\n\n\t \n\tif (!iter->ent)\n\t\treturn false;\n\n\ttrace_event = ftrace_find_event(iter->ent->type);\n\tif (!trace_event)\n\t\treturn false;\n\n\tevent = container_of(trace_event, struct trace_event_call, event);\n\tif ((event->flags & TRACE_EVENT_FL_DYNAMIC) || !event->module)\n\t\treturn false;\n\n\t \n\tif (within_module_core(addr, event->module))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic const char *show_buffer(struct trace_seq *s)\n{\n\tstruct seq_buf *seq = &s->seq;\n\n\tseq_buf_terminate(seq);\n\n\treturn seq->buffer;\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(trace_no_verify);\n\nstatic int test_can_verify_check(const char *fmt, ...)\n{\n\tchar buf[16];\n\tva_list ap;\n\tint ret;\n\n\t \n\tva_start(ap, fmt);\n\tvsnprintf(buf, 16, \"%d\", ap);\n\tret = va_arg(ap, int);\n\tva_end(ap);\n\n\treturn ret;\n}\n\nstatic void test_can_verify(void)\n{\n\tif (!test_can_verify_check(\"%d %d\", 0, 1)) {\n\t\tpr_info(\"trace event string verifier disabled\\n\");\n\t\tstatic_branch_inc(&trace_no_verify);\n\t}\n}\n\n \nvoid trace_check_vprintf(struct trace_iterator *iter, const char *fmt,\n\t\t\t va_list ap)\n{\n\tconst char *p = fmt;\n\tconst char *str;\n\tint i, j;\n\n\tif (WARN_ON_ONCE(!fmt))\n\t\treturn;\n\n\tif (static_branch_unlikely(&trace_no_verify))\n\t\tgoto print;\n\n\t \n\tif (iter->fmt == static_fmt_buf)\n\t\tgoto print;\n\n\twhile (*p) {\n\t\tbool star = false;\n\t\tint len = 0;\n\n\t\tj = 0;\n\n\t\t \n\t\tfor (i = 0; p[i]; i++) {\n\t\t\tif (i + 1 >= iter->fmt_size) {\n\t\t\t\t \n\t\t\t\tif (!trace_iter_expand_format(iter))\n\t\t\t\t\tgoto print;\n\t\t\t}\n\n\t\t\tif (p[i] == '\\\\' && p[i+1]) {\n\t\t\t\ti++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (p[i] == '%') {\n\t\t\t\t \n\t\t\t\tfor (j = 1; p[i+j]; j++) {\n\t\t\t\t\tif (isdigit(p[i+j]) ||\n\t\t\t\t\t    p[i+j] == '.')\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tif (p[i+j] == '*') {\n\t\t\t\t\t\tstar = true;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (p[i+j] == 's')\n\t\t\t\t\tbreak;\n\t\t\t\tstar = false;\n\t\t\t}\n\t\t\tj = 0;\n\t\t}\n\t\t \n\t\tif (!p[i])\n\t\t\tbreak;\n\n\t\t \n\t\tstrncpy(iter->fmt, p, i);\n\t\titer->fmt[i] = '\\0';\n\t\ttrace_seq_vprintf(&iter->seq, iter->fmt, ap);\n\n\t\t \n\t\tif (iter->seq.full) {\n\t\t\tp = \"\";\n\t\t\tbreak;\n\t\t}\n\n\t\tif (star)\n\t\t\tlen = va_arg(ap, int);\n\n\t\t \n\t\tstr = va_arg(ap, const char *);\n\n\t\t \n\t\tif (WARN_ONCE(!trace_safe_str(iter, str, star, len),\n\t\t\t      \"fmt: '%s' current_buffer: '%s'\",\n\t\t\t      fmt, show_buffer(&iter->seq))) {\n\t\t\tint ret;\n\n\t\t\t \n\t\t\tif (star) {\n\t\t\t\tif (len + 1 > iter->fmt_size)\n\t\t\t\t\tlen = iter->fmt_size - 1;\n\t\t\t\tif (len < 0)\n\t\t\t\t\tlen = 0;\n\t\t\t\tret = copy_from_kernel_nofault(iter->fmt, str, len);\n\t\t\t\titer->fmt[len] = 0;\n\t\t\t\tstar = false;\n\t\t\t} else {\n\t\t\t\tret = strncpy_from_kernel_nofault(iter->fmt, str,\n\t\t\t\t\t\t\t\t  iter->fmt_size);\n\t\t\t}\n\t\t\tif (ret < 0)\n\t\t\t\ttrace_seq_printf(&iter->seq, \"(0x%px)\", str);\n\t\t\telse\n\t\t\t\ttrace_seq_printf(&iter->seq, \"(0x%px:%s)\",\n\t\t\t\t\t\t str, iter->fmt);\n\t\t\tstr = \"[UNSAFE-MEMORY]\";\n\t\t\tstrcpy(iter->fmt, \"%s\");\n\t\t} else {\n\t\t\tstrncpy(iter->fmt, p + i, j + 1);\n\t\t\titer->fmt[j+1] = '\\0';\n\t\t}\n\t\tif (star)\n\t\t\ttrace_seq_printf(&iter->seq, iter->fmt, len, str);\n\t\telse\n\t\t\ttrace_seq_printf(&iter->seq, iter->fmt, str);\n\n\t\tp += i + j + 1;\n\t}\n print:\n\tif (*p)\n\t\ttrace_seq_vprintf(&iter->seq, p, ap);\n}\n\nconst char *trace_event_format(struct trace_iterator *iter, const char *fmt)\n{\n\tconst char *p, *new_fmt;\n\tchar *q;\n\n\tif (WARN_ON_ONCE(!fmt))\n\t\treturn fmt;\n\n\tif (!iter->tr || iter->tr->trace_flags & TRACE_ITER_HASH_PTR)\n\t\treturn fmt;\n\n\tp = fmt;\n\tnew_fmt = q = iter->fmt;\n\twhile (*p) {\n\t\tif (unlikely(q - new_fmt + 3 > iter->fmt_size)) {\n\t\t\tif (!trace_iter_expand_format(iter))\n\t\t\t\treturn fmt;\n\n\t\t\tq += iter->fmt - new_fmt;\n\t\t\tnew_fmt = iter->fmt;\n\t\t}\n\n\t\t*q++ = *p++;\n\n\t\t \n\t\tif (p[-1] == '%') {\n\t\t\tif (p[0] == '%') {\n\t\t\t\t*q++ = *p++;\n\t\t\t} else if (p[0] == 'p' && !isalnum(p[1])) {\n\t\t\t\t*q++ = *p++;\n\t\t\t\t*q++ = 'x';\n\t\t\t}\n\t\t}\n\t}\n\t*q = '\\0';\n\n\treturn new_fmt;\n}\n\n#define STATIC_TEMP_BUF_SIZE\t128\nstatic char static_temp_buf[STATIC_TEMP_BUF_SIZE] __aligned(4);\n\n \nstruct trace_entry *trace_find_next_entry(struct trace_iterator *iter,\n\t\t\t\t\t  int *ent_cpu, u64 *ent_ts)\n{\n\t \n\tint ent_size = iter->ent_size;\n\tstruct trace_entry *entry;\n\n\t \n\tif (iter->temp == static_temp_buf &&\n\t    STATIC_TEMP_BUF_SIZE < ent_size)\n\t\treturn NULL;\n\n\t \n\tif (iter->ent && iter->ent != iter->temp) {\n\t\tif ((!iter->temp || iter->temp_size < iter->ent_size) &&\n\t\t    !WARN_ON_ONCE(iter->temp == static_temp_buf)) {\n\t\t\tvoid *temp;\n\t\t\ttemp = kmalloc(iter->ent_size, GFP_KERNEL);\n\t\t\tif (!temp)\n\t\t\t\treturn NULL;\n\t\t\tkfree(iter->temp);\n\t\t\titer->temp = temp;\n\t\t\titer->temp_size = iter->ent_size;\n\t\t}\n\t\tmemcpy(iter->temp, iter->ent, iter->ent_size);\n\t\titer->ent = iter->temp;\n\t}\n\tentry = __find_next_entry(iter, ent_cpu, NULL, ent_ts);\n\t \n\titer->ent_size = ent_size;\n\n\treturn entry;\n}\n\n \nvoid *trace_find_next_entry_inc(struct trace_iterator *iter)\n{\n\titer->ent = __find_next_entry(iter, &iter->cpu,\n\t\t\t\t      &iter->lost_events, &iter->ts);\n\n\tif (iter->ent)\n\t\ttrace_iterator_increment(iter);\n\n\treturn iter->ent ? iter : NULL;\n}\n\nstatic void trace_consume(struct trace_iterator *iter)\n{\n\tring_buffer_consume(iter->array_buffer->buffer, iter->cpu, &iter->ts,\n\t\t\t    &iter->lost_events);\n}\n\nstatic void *s_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tint i = (int)*pos;\n\tvoid *ent;\n\n\tWARN_ON_ONCE(iter->leftover);\n\n\t(*pos)++;\n\n\t \n\tif (iter->idx > i)\n\t\treturn NULL;\n\n\tif (iter->idx < 0)\n\t\tent = trace_find_next_entry_inc(iter);\n\telse\n\t\tent = iter;\n\n\twhile (ent && iter->idx < i)\n\t\tent = trace_find_next_entry_inc(iter);\n\n\titer->pos = *pos;\n\n\treturn ent;\n}\n\nvoid tracing_iter_reset(struct trace_iterator *iter, int cpu)\n{\n\tstruct ring_buffer_iter *buf_iter;\n\tunsigned long entries = 0;\n\tu64 ts;\n\n\tper_cpu_ptr(iter->array_buffer->data, cpu)->skipped_entries = 0;\n\n\tbuf_iter = trace_buffer_iter(iter, cpu);\n\tif (!buf_iter)\n\t\treturn;\n\n\tring_buffer_iter_reset(buf_iter);\n\n\t \n\twhile (ring_buffer_iter_peek(buf_iter, &ts)) {\n\t\tif (ts >= iter->array_buffer->time_start)\n\t\t\tbreak;\n\t\tentries++;\n\t\tring_buffer_iter_advance(buf_iter);\n\t}\n\n\tper_cpu_ptr(iter->array_buffer->data, cpu)->skipped_entries = entries;\n}\n\n \nstatic void *s_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tint cpu_file = iter->cpu_file;\n\tvoid *p = NULL;\n\tloff_t l = 0;\n\tint cpu;\n\n\tmutex_lock(&trace_types_lock);\n\tif (unlikely(tr->current_trace != iter->trace)) {\n\t\t \n\t\tif (iter->trace->close)\n\t\t\titer->trace->close(iter);\n\t\titer->trace = tr->current_trace;\n\t\t \n\t\tif (iter->trace->open)\n\t\t\titer->trace->open(iter);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn ERR_PTR(-EBUSY);\n#endif\n\n\tif (*pos != iter->pos) {\n\t\titer->ent = NULL;\n\t\titer->cpu = 0;\n\t\titer->idx = -1;\n\n\t\tif (cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\t\tfor_each_tracing_cpu(cpu)\n\t\t\t\ttracing_iter_reset(iter, cpu);\n\t\t} else\n\t\t\ttracing_iter_reset(iter, cpu_file);\n\n\t\titer->leftover = 0;\n\t\tfor (p = iter; p && l < *pos; p = s_next(m, p, &l))\n\t\t\t;\n\n\t} else {\n\t\t \n\t\tif (iter->leftover)\n\t\t\tp = iter;\n\t\telse {\n\t\t\tl = *pos - 1;\n\t\t\tp = s_next(m, p, &l);\n\t\t}\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(cpu_file);\n\treturn p;\n}\n\nstatic void s_stop(struct seq_file *m, void *p)\n{\n\tstruct trace_iterator *iter = m->private;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->trace->use_max_tr)\n\t\treturn;\n#endif\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n}\n\nstatic void\nget_total_entries_cpu(struct array_buffer *buf, unsigned long *total,\n\t\t      unsigned long *entries, int cpu)\n{\n\tunsigned long count;\n\n\tcount = ring_buffer_entries_cpu(buf->buffer, cpu);\n\t \n\tif (per_cpu_ptr(buf->data, cpu)->skipped_entries) {\n\t\tcount -= per_cpu_ptr(buf->data, cpu)->skipped_entries;\n\t\t \n\t\t*total = count;\n\t} else\n\t\t*total = count +\n\t\t\tring_buffer_overrun_cpu(buf->buffer, cpu);\n\t*entries = count;\n}\n\nstatic void\nget_total_entries(struct array_buffer *buf,\n\t\t  unsigned long *total, unsigned long *entries)\n{\n\tunsigned long t, e;\n\tint cpu;\n\n\t*total = 0;\n\t*entries = 0;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tget_total_entries_cpu(buf, &t, &e, cpu);\n\t\t*total += t;\n\t\t*entries += e;\n\t}\n}\n\nunsigned long trace_total_entries_cpu(struct trace_array *tr, int cpu)\n{\n\tunsigned long total, entries;\n\n\tif (!tr)\n\t\ttr = &global_trace;\n\n\tget_total_entries_cpu(&tr->array_buffer, &total, &entries, cpu);\n\n\treturn entries;\n}\n\nunsigned long trace_total_entries(struct trace_array *tr)\n{\n\tunsigned long total, entries;\n\n\tif (!tr)\n\t\ttr = &global_trace;\n\n\tget_total_entries(&tr->array_buffer, &total, &entries);\n\n\treturn entries;\n}\n\nstatic void print_lat_help_header(struct seq_file *m)\n{\n\tseq_puts(m, \"#                    _------=> CPU#            \\n\"\n\t\t    \"#                   / _-----=> irqs-off/BH-disabled\\n\"\n\t\t    \"#                  | / _----=> need-resched    \\n\"\n\t\t    \"#                  || / _---=> hardirq/softirq \\n\"\n\t\t    \"#                  ||| / _--=> preempt-depth   \\n\"\n\t\t    \"#                  |||| / _-=> migrate-disable \\n\"\n\t\t    \"#                  ||||| /     delay           \\n\"\n\t\t    \"#  cmd     pid     |||||| time  |   caller     \\n\"\n\t\t    \"#     \\\\   /        ||||||  \\\\    |    /       \\n\");\n}\n\nstatic void print_event_info(struct array_buffer *buf, struct seq_file *m)\n{\n\tunsigned long total;\n\tunsigned long entries;\n\n\tget_total_entries(buf, &total, &entries);\n\tseq_printf(m, \"# entries-in-buffer/entries-written: %lu/%lu   #P:%d\\n\",\n\t\t   entries, total, num_online_cpus());\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void print_func_help_header(struct array_buffer *buf, struct seq_file *m,\n\t\t\t\t   unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#           TASK-PID    %s CPU#     TIMESTAMP  FUNCTION\\n\", tgid ? \"   TGID   \" : \"\");\n\tseq_printf(m, \"#              | |      %s   |         |         |\\n\",      tgid ? \"     |    \" : \"\");\n}\n\nstatic void print_func_help_header_irq(struct array_buffer *buf, struct seq_file *m,\n\t\t\t\t       unsigned int flags)\n{\n\tbool tgid = flags & TRACE_ITER_RECORD_TGID;\n\tstatic const char space[] = \"            \";\n\tint prec = tgid ? 12 : 2;\n\n\tprint_event_info(buf, m);\n\n\tseq_printf(m, \"#                            %.*s  _-----=> irqs-off/BH-disabled\\n\", prec, space);\n\tseq_printf(m, \"#                            %.*s / _----=> need-resched\\n\", prec, space);\n\tseq_printf(m, \"#                            %.*s| / _---=> hardirq/softirq\\n\", prec, space);\n\tseq_printf(m, \"#                            %.*s|| / _--=> preempt-depth\\n\", prec, space);\n\tseq_printf(m, \"#                            %.*s||| / _-=> migrate-disable\\n\", prec, space);\n\tseq_printf(m, \"#                            %.*s|||| /     delay\\n\", prec, space);\n\tseq_printf(m, \"#           TASK-PID  %.*s CPU#  |||||  TIMESTAMP  FUNCTION\\n\", prec, \"     TGID   \");\n\tseq_printf(m, \"#              | |    %.*s   |   |||||     |         |\\n\", prec, \"       |    \");\n}\n\nvoid\nprint_trace_header(struct seq_file *m, struct trace_iterator *iter)\n{\n\tunsigned long sym_flags = (global_trace.trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct array_buffer *buf = iter->array_buffer;\n\tstruct trace_array_cpu *data = per_cpu_ptr(buf->data, buf->cpu);\n\tstruct tracer *type = iter->trace;\n\tunsigned long entries;\n\tunsigned long total;\n\tconst char *name = type->name;\n\n\tget_total_entries(buf, &total, &entries);\n\n\tseq_printf(m, \"# %s latency trace v1.1.5 on %s\\n\",\n\t\t   name, UTS_RELEASE);\n\tseq_puts(m, \"# -----------------------------------\"\n\t\t \"---------------------------------\\n\");\n\tseq_printf(m, \"# latency: %lu us, #%lu/%lu, CPU#%d |\"\n\t\t   \" (M:%s VP:%d, KP:%d, SP:%d HP:%d\",\n\t\t   nsecs_to_usecs(data->saved_latency),\n\t\t   entries,\n\t\t   total,\n\t\t   buf->cpu,\n\t\t   preempt_model_none()      ? \"server\" :\n\t\t   preempt_model_voluntary() ? \"desktop\" :\n\t\t   preempt_model_full()      ? \"preempt\" :\n\t\t   preempt_model_rt()        ? \"preempt_rt\" :\n\t\t   \"unknown\",\n\t\t    \n\t\t   0, 0, 0, 0);\n#ifdef CONFIG_SMP\n\tseq_printf(m, \" #P:%d)\\n\", num_online_cpus());\n#else\n\tseq_puts(m, \")\\n\");\n#endif\n\tseq_puts(m, \"#    -----------------\\n\");\n\tseq_printf(m, \"#    | task: %.16s-%d \"\n\t\t   \"(uid:%d nice:%ld policy:%ld rt_prio:%ld)\\n\",\n\t\t   data->comm, data->pid,\n\t\t   from_kuid_munged(seq_user_ns(m), data->uid), data->nice,\n\t\t   data->policy, data->rt_priority);\n\tseq_puts(m, \"#    -----------------\\n\");\n\n\tif (data->critical_start) {\n\t\tseq_puts(m, \"#  => started at: \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_start, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#  => ended at:   \");\n\t\tseq_print_ip_sym(&iter->seq, data->critical_end, sym_flags);\n\t\ttrace_print_seq(m, &iter->seq);\n\t\tseq_puts(m, \"\\n#\\n\");\n\t}\n\n\tseq_puts(m, \"#\\n\");\n}\n\nstatic void test_cpu_buff_start(struct trace_iterator *iter)\n{\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (!(tr->trace_flags & TRACE_ITER_ANNOTATE))\n\t\treturn;\n\n\tif (!(iter->iter_flags & TRACE_FILE_ANNOTATE))\n\t\treturn;\n\n\tif (cpumask_available(iter->started) &&\n\t    cpumask_test_cpu(iter->cpu, iter->started))\n\t\treturn;\n\n\tif (per_cpu_ptr(iter->array_buffer->data, iter->cpu)->skipped_entries)\n\t\treturn;\n\n\tif (cpumask_available(iter->started))\n\t\tcpumask_set_cpu(iter->cpu, iter->started);\n\n\t \n\tif (iter->idx > 1)\n\t\ttrace_seq_printf(s, \"##### CPU %u buffer started ####\\n\",\n\t\t\t\titer->cpu);\n}\n\nstatic enum print_line_t print_trace_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned long sym_flags = (tr->trace_flags & TRACE_ITER_SYM_MASK);\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\ttest_cpu_buff_start(iter);\n\n\tevent = ftrace_find_event(entry->type);\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\t\ttrace_print_lat_context(iter);\n\t\telse\n\t\t\ttrace_print_context(iter);\n\t}\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tif (event) {\n\t\tif (tr->trace_flags & TRACE_ITER_FIELDS)\n\t\t\treturn print_event_fields(iter, event);\n\t\treturn event->funcs->trace(iter, sym_flags, event);\n\t}\n\n\ttrace_seq_printf(s, \"Unknown type %d\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_raw_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO)\n\t\ttrace_seq_printf(s, \"%d %d %llu \",\n\t\t\t\t entry->pid, iter->cpu, iter->ts);\n\n\tif (trace_seq_has_overflowed(s))\n\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event)\n\t\treturn event->funcs->raw(iter, 0, event);\n\n\ttrace_seq_printf(s, \"%d ?\\n\", entry->type);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_hex_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tunsigned char newline = '\\n';\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_HEX_FIELD(s, entry->pid);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_HEX_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\tif (event) {\n\t\tenum print_line_t ret = event->funcs->hex(iter, 0, event);\n\t\tif (ret != TRACE_TYPE_HANDLED)\n\t\t\treturn ret;\n\t}\n\n\tSEQ_PUT_FIELD(s, newline);\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t print_bin_fmt(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tstruct trace_entry *entry;\n\tstruct trace_event *event;\n\n\tentry = iter->ent;\n\n\tif (tr->trace_flags & TRACE_ITER_CONTEXT_INFO) {\n\t\tSEQ_PUT_FIELD(s, entry->pid);\n\t\tSEQ_PUT_FIELD(s, iter->cpu);\n\t\tSEQ_PUT_FIELD(s, iter->ts);\n\t\tif (trace_seq_has_overflowed(s))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tevent = ftrace_find_event(entry->type);\n\treturn event ? event->funcs->binary(iter, 0, event) :\n\t\tTRACE_TYPE_HANDLED;\n}\n\nint trace_empty(struct trace_iterator *iter)\n{\n\tstruct ring_buffer_iter *buf_iter;\n\tint cpu;\n\n\t \n\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\tcpu = iter->cpu_file;\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->array_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t\treturn 1;\n\t}\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tbuf_iter = trace_buffer_iter(iter, cpu);\n\t\tif (buf_iter) {\n\t\t\tif (!ring_buffer_iter_empty(buf_iter))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tif (!ring_buffer_empty_cpu(iter->array_buffer->buffer, cpu))\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\n \nenum print_line_t print_trace_line(struct trace_iterator *iter)\n{\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\tenum print_line_t ret;\n\n\tif (iter->lost_events) {\n\t\tif (iter->lost_events == (unsigned long)-1)\n\t\t\ttrace_seq_printf(&iter->seq, \"CPU:%d [LOST EVENTS]\\n\",\n\t\t\t\t\t iter->cpu);\n\t\telse\n\t\t\ttrace_seq_printf(&iter->seq, \"CPU:%d [LOST %lu EVENTS]\\n\",\n\t\t\t\t\t iter->cpu, iter->lost_events);\n\t\tif (trace_seq_has_overflowed(&iter->seq))\n\t\t\treturn TRACE_TYPE_PARTIAL_LINE;\n\t}\n\n\tif (iter->trace && iter->trace->print_line) {\n\t\tret = iter->trace->print_line(iter);\n\t\tif (ret != TRACE_TYPE_UNHANDLED)\n\t\t\treturn ret;\n\t}\n\n\tif (iter->ent->type == TRACE_BPUTS &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bputs_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_BPRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_bprintk_msg_only(iter);\n\n\tif (iter->ent->type == TRACE_PRINT &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK &&\n\t\t\ttrace_flags & TRACE_ITER_PRINTK_MSGONLY)\n\t\treturn trace_print_printk_msg_only(iter);\n\n\tif (trace_flags & TRACE_ITER_BIN)\n\t\treturn print_bin_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_HEX)\n\t\treturn print_hex_fmt(iter);\n\n\tif (trace_flags & TRACE_ITER_RAW)\n\t\treturn print_raw_fmt(iter);\n\n\treturn print_trace_fmt(iter);\n}\n\nvoid trace_latency_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\n\t \n\tif (trace_empty(iter))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT)\n\t\tprint_trace_header(m, iter);\n\n\tif (!(tr->trace_flags & TRACE_ITER_VERBOSE))\n\t\tprint_lat_help_header(m);\n}\n\nvoid trace_default_header(struct seq_file *m)\n{\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long trace_flags = tr->trace_flags;\n\n\tif (!(trace_flags & TRACE_ITER_CONTEXT_INFO))\n\t\treturn;\n\n\tif (iter->iter_flags & TRACE_FILE_LAT_FMT) {\n\t\t \n\t\tif (trace_empty(iter))\n\t\t\treturn;\n\t\tprint_trace_header(m, iter);\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE))\n\t\t\tprint_lat_help_header(m);\n\t} else {\n\t\tif (!(trace_flags & TRACE_ITER_VERBOSE)) {\n\t\t\tif (trace_flags & TRACE_ITER_IRQ_INFO)\n\t\t\t\tprint_func_help_header_irq(iter->array_buffer,\n\t\t\t\t\t\t\t   m, trace_flags);\n\t\t\telse\n\t\t\t\tprint_func_help_header(iter->array_buffer, m,\n\t\t\t\t\t\t       trace_flags);\n\t\t}\n\t}\n}\n\nstatic void test_ftrace_alive(struct seq_file *m)\n{\n\tif (!ftrace_is_dead())\n\t\treturn;\n\tseq_puts(m, \"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\"\n\t\t    \"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic void show_snapshot_main_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Clears and frees snapshot buffer\\n\"\n\t\t    \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer.\\n\"\n\t\t    \"# echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void show_snapshot_percpu_help(struct seq_file *m)\n{\n\tseq_puts(m, \"# echo 0 > snapshot : Invalid for per_cpu snapshot file.\\n\");\n#ifdef CONFIG_RING_BUFFER_ALLOW_SWAP\n\tseq_puts(m, \"# echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.\\n\"\n\t\t    \"#                      Takes a snapshot of the main buffer for this cpu.\\n\");\n#else\n\tseq_puts(m, \"# echo 1 > snapshot : Not supported with this kernel.\\n\"\n\t\t    \"#                     Must use main snapshot file to allocate.\\n\");\n#endif\n\tseq_puts(m, \"# echo 2 > snapshot : Clears this cpu's snapshot buffer (but does not allocate)\\n\"\n\t\t    \"#                      (Doesn't have to be '2' works with any number that\\n\"\n\t\t    \"#                       is not a '0' or '1')\\n\");\n}\n\nstatic void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter)\n{\n\tif (iter->tr->allocated_snapshot)\n\t\tseq_puts(m, \"#\\n# * Snapshot is allocated *\\n#\\n\");\n\telse\n\t\tseq_puts(m, \"#\\n# * Snapshot is freed *\\n#\\n\");\n\n\tseq_puts(m, \"# Snapshot commands:\\n\");\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\tshow_snapshot_main_help(m);\n\telse\n\t\tshow_snapshot_percpu_help(m);\n}\n#else\n \nstatic inline void print_snapshot_help(struct seq_file *m, struct trace_iterator *iter) { }\n#endif\n\nstatic int s_show(struct seq_file *m, void *v)\n{\n\tstruct trace_iterator *iter = v;\n\tint ret;\n\n\tif (iter->ent == NULL) {\n\t\tif (iter->tr) {\n\t\t\tseq_printf(m, \"# tracer: %s\\n\", iter->trace->name);\n\t\t\tseq_puts(m, \"#\\n\");\n\t\t\ttest_ftrace_alive(m);\n\t\t}\n\t\tif (iter->snapshot && trace_empty(iter))\n\t\t\tprint_snapshot_help(m, iter);\n\t\telse if (iter->trace && iter->trace->print_header)\n\t\t\titer->trace->print_header(m);\n\t\telse\n\t\t\ttrace_default_header(m);\n\n\t} else if (iter->leftover) {\n\t\t \n\t\tret = trace_print_seq(m, &iter->seq);\n\n\t\t \n\t\titer->leftover = ret;\n\n\t} else {\n\t\tret = print_trace_line(iter);\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\titer->seq.full = 0;\n\t\t\ttrace_seq_puts(&iter->seq, \"[LINE TOO BIG]\\n\");\n\t\t}\n\t\tret = trace_print_seq(m, &iter->seq);\n\t\t \n\t\titer->leftover = ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline int tracing_get_cpu(struct inode *inode)\n{\n\tif (inode->i_cdev)  \n\t\treturn (long)inode->i_cdev - 1;\n\treturn RING_BUFFER_ALL_CPUS;\n}\n\nstatic const struct seq_operations tracer_seq_ops = {\n\t.start\t\t= s_start,\n\t.next\t\t= s_next,\n\t.stop\t\t= s_stop,\n\t.show\t\t= s_show,\n};\n\n \nstatic void free_trace_iter_content(struct trace_iterator *iter)\n{\n\t \n\tif (iter->fmt != static_fmt_buf)\n\t\tkfree(iter->fmt);\n\n\tkfree(iter->temp);\n\tkfree(iter->buffer_iter);\n\tmutex_destroy(&iter->mutex);\n\tfree_cpumask_var(iter->started);\n}\n\nstatic struct trace_iterator *\n__tracing_open(struct inode *inode, struct file *file, bool snapshot)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (tracing_disabled)\n\t\treturn ERR_PTR(-ENODEV);\n\n\titer = __seq_open_private(file, &tracer_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\titer->buffer_iter = kcalloc(nr_cpu_ids, sizeof(*iter->buffer_iter),\n\t\t\t\t    GFP_KERNEL);\n\tif (!iter->buffer_iter)\n\t\tgoto release;\n\n\t \n\titer->temp = kmalloc(128, GFP_KERNEL);\n\tif (iter->temp)\n\t\titer->temp_size = 128;\n\n\t \n\titer->fmt = NULL;\n\titer->fmt_size = 0;\n\n\tmutex_lock(&trace_types_lock);\n\titer->trace = tr->current_trace;\n\n\tif (!zalloc_cpumask_var(&iter->started, GFP_KERNEL))\n\t\tgoto fail;\n\n\titer->tr = tr;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t \n\tif (tr->current_trace->print_max || snapshot)\n\t\titer->array_buffer = &tr->max_buffer;\n\telse\n#endif\n\t\titer->array_buffer = &tr->array_buffer;\n\titer->snapshot = snapshot;\n\titer->pos = -1;\n\titer->cpu_file = tracing_get_cpu(inode);\n\tmutex_init(&iter->mutex);\n\n\t \n\tif (iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t \n\tif (ring_buffer_overruns(iter->array_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t \n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\t \n\tif (!iter->snapshot && (tr->trace_flags & TRACE_ITER_PAUSE_ON_TRACE))\n\t\ttracing_stop_tr(tr);\n\n\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\titer->buffer_iter[cpu] =\n\t\t\t\tring_buffer_read_prepare(iter->array_buffer->buffer,\n\t\t\t\t\t\t\t cpu, GFP_KERNEL);\n\t\t}\n\t\tring_buffer_read_prepare_sync();\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\t\ttracing_iter_reset(iter, cpu);\n\t\t}\n\t} else {\n\t\tcpu = iter->cpu_file;\n\t\titer->buffer_iter[cpu] =\n\t\t\tring_buffer_read_prepare(iter->array_buffer->buffer,\n\t\t\t\t\t\t cpu, GFP_KERNEL);\n\t\tring_buffer_read_prepare_sync();\n\t\tring_buffer_read_start(iter->buffer_iter[cpu]);\n\t\ttracing_iter_reset(iter, cpu);\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn iter;\n\n fail:\n\tmutex_unlock(&trace_types_lock);\n\tfree_trace_iter_content(iter);\nrelease:\n\tseq_release_private(inode, file);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nint tracing_open_generic(struct inode *inode, struct file *filp)\n{\n\tint ret;\n\n\tret = tracing_check_open_get_tr(NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tfilp->private_data = inode->i_private;\n\treturn 0;\n}\n\nbool tracing_is_disabled(void)\n{\n\treturn (tracing_disabled) ? true: false;\n}\n\n \nint tracing_open_generic_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tfilp->private_data = inode->i_private;\n\n\treturn 0;\n}\n\n \nint tracing_open_file_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_event_file *file = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(file->tr);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&event_mutex);\n\n\t \n\tif (file->flags & EVENT_FILE_FL_FREED) {\n\t\ttrace_array_put(file->tr);\n\t\tret = -ENODEV;\n\t} else {\n\t\tevent_file_get(file);\n\t}\n\n\tmutex_unlock(&event_mutex);\n\tif (ret)\n\t\treturn ret;\n\n\tfilp->private_data = inode->i_private;\n\n\treturn 0;\n}\n\nint tracing_release_file_tr(struct inode *inode, struct file *filp)\n{\n\tstruct trace_event_file *file = inode->i_private;\n\n\ttrace_array_put(file->tr);\n\tevent_file_put(file);\n\n\treturn 0;\n}\n\nint tracing_single_release_file_tr(struct inode *inode, struct file *filp)\n{\n\ttracing_release_file_tr(inode, filp);\n\treturn single_release(inode, filp);\n}\n\nstatic int tracing_mark_open(struct inode *inode, struct file *filp)\n{\n\tstream_open(inode, filp);\n\treturn tracing_open_generic_tr(inode, filp);\n}\n\nstatic int tracing_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\n\tif (!(file->f_mode & FMODE_READ)) {\n\t\ttrace_array_put(tr);\n\t\treturn 0;\n\t}\n\n\t \n\titer = m->private;\n\tmutex_lock(&trace_types_lock);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tif (iter->buffer_iter[cpu])\n\t\t\tring_buffer_read_finish(iter->buffer_iter[cpu]);\n\t}\n\n\tif (iter->trace && iter->trace->close)\n\t\titer->trace->close(iter);\n\n\tif (!iter->snapshot && tr->stop_count)\n\t\t \n\t\ttracing_start_tr(tr);\n\n\t__trace_array_put(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_trace_iter_content(iter);\n\tseq_release_private(inode, file);\n\n\treturn 0;\n}\n\nstatic int tracing_release_generic_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\treturn 0;\n}\n\nstatic int tracing_single_release_tr(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn single_release(inode, file);\n}\n\nstatic int tracing_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC)) {\n\t\tint cpu = tracing_get_cpu(inode);\n\t\tstruct array_buffer *trace_buf = &tr->array_buffer;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tif (tr->current_trace->print_max)\n\t\t\ttrace_buf = &tr->max_buffer;\n#endif\n\n\t\tif (cpu == RING_BUFFER_ALL_CPUS)\n\t\t\ttracing_reset_online_cpus(trace_buf);\n\t\telse\n\t\t\ttracing_reset_cpu(trace_buf, cpu);\n\t}\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, false);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t\telse if (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\t}\n\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\n \nstatic bool\ntrace_ok_for_array(struct tracer *t, struct trace_array *tr)\n{\n\treturn (tr->flags & TRACE_ARRAY_FL_GLOBAL) || t->allow_instances;\n}\n\n \nstatic struct tracer *\nget_tracer_for_array(struct trace_array *tr, struct tracer *t)\n{\n\twhile (t && !trace_ok_for_array(t, tr))\n\t\tt = t->next;\n\n\treturn t;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t = v;\n\n\t(*pos)++;\n\n\tif (t)\n\t\tt = get_tracer_for_array(tr, t->next);\n\n\treturn t;\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct tracer *t;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tt = get_tracer_for_array(tr, trace_types);\n\tfor (; t && l < *pos; t = t_next(m, t, &l))\n\t\t\t;\n\n\treturn t;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct tracer *t = v;\n\n\tif (!t)\n\t\treturn 0;\n\n\tseq_puts(m, t->name);\n\tif (t->next)\n\t\tseq_putc(m, ' ');\n\telse\n\t\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nstatic const struct seq_operations show_traces_seq_ops = {\n\t.start\t\t= t_start,\n\t.next\t\t= t_next,\n\t.stop\t\t= t_stop,\n\t.show\t\t= t_show,\n};\n\nstatic int show_traces_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tret = seq_open(file, &show_traces_seq_ops);\n\tif (ret) {\n\t\ttrace_array_put(tr);\n\t\treturn ret;\n\t}\n\n\tm = file->private_data;\n\tm->private = tr;\n\n\treturn 0;\n}\n\nstatic int show_traces_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\treturn seq_release(inode, file);\n}\n\nstatic ssize_t\ntracing_write_stub(struct file *filp, const char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nloff_t tracing_lseek(struct file *file, loff_t offset, int whence)\n{\n\tint ret;\n\n\tif (file->f_mode & FMODE_READ)\n\t\tret = seq_lseek(file, offset, whence);\n\telse\n\t\tfile->f_pos = ret = 0;\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_fops = {\n\t.open\t\t= tracing_open,\n\t.read\t\t= seq_read,\n\t.read_iter\t= seq_read_iter,\n\t.splice_read\t= copy_splice_read,\n\t.write\t\t= tracing_write_stub,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_release,\n};\n\nstatic const struct file_operations show_traces_fops = {\n\t.open\t\t= show_traces_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= show_traces_release,\n};\n\nstatic ssize_t\ntracing_cpumask_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tchar *mask_str;\n\tint len;\n\n\tlen = snprintf(NULL, 0, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask)) + 1;\n\tmask_str = kmalloc(len, GFP_KERNEL);\n\tif (!mask_str)\n\t\treturn -ENOMEM;\n\n\tlen = snprintf(mask_str, len, \"%*pb\\n\",\n\t\t       cpumask_pr_args(tr->tracing_cpumask));\n\tif (len >= count) {\n\t\tcount = -EINVAL;\n\t\tgoto out_err;\n\t}\n\tcount = simple_read_from_buffer(ubuf, count, ppos, mask_str, len);\n\nout_err:\n\tkfree(mask_str);\n\n\treturn count;\n}\n\nint tracing_set_cpumask(struct trace_array *tr,\n\t\t\tcpumask_var_t tracing_cpumask_new)\n{\n\tint cpu;\n\n\tif (!tr)\n\t\treturn -EINVAL;\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\t \n\t\tif (cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\t!cpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_inc(&per_cpu_ptr(tr->array_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_disable_cpu(tr->array_buffer.buffer, cpu);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\t\tring_buffer_record_disable_cpu(tr->max_buffer.buffer, cpu);\n#endif\n\t\t}\n\t\tif (!cpumask_test_cpu(cpu, tr->tracing_cpumask) &&\n\t\t\t\tcpumask_test_cpu(cpu, tracing_cpumask_new)) {\n\t\t\tatomic_dec(&per_cpu_ptr(tr->array_buffer.data, cpu)->disabled);\n\t\t\tring_buffer_record_enable_cpu(tr->array_buffer.buffer, cpu);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\t\tring_buffer_record_enable_cpu(tr->max_buffer.buffer, cpu);\n#endif\n\t\t}\n\t}\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\n\tcpumask_copy(tr->tracing_cpumask, tracing_cpumask_new);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_cpumask_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t count, loff_t *ppos)\n{\n\tstruct trace_array *tr = file_inode(filp)->i_private;\n\tcpumask_var_t tracing_cpumask_new;\n\tint err;\n\n\tif (!zalloc_cpumask_var(&tracing_cpumask_new, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\terr = cpumask_parse_user(ubuf, count, tracing_cpumask_new);\n\tif (err)\n\t\tgoto err_free;\n\n\terr = tracing_set_cpumask(tr, tracing_cpumask_new);\n\tif (err)\n\t\tgoto err_free;\n\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn count;\n\nerr_free:\n\tfree_cpumask_var(tracing_cpumask_new);\n\n\treturn err;\n}\n\nstatic const struct file_operations tracing_cpumask_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_cpumask_read,\n\t.write\t\t= tracing_cpumask_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic int tracing_trace_options_show(struct seq_file *m, void *v)\n{\n\tstruct tracer_opt *trace_opts;\n\tstruct trace_array *tr = m->private;\n\tu32 tracer_flags;\n\tint i;\n\n\tmutex_lock(&trace_types_lock);\n\ttracer_flags = tr->current_trace->flags->val;\n\ttrace_opts = tr->current_trace->flags->opts;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (tr->trace_flags & (1 << i))\n\t\t\tseq_printf(m, \"%s\\n\", trace_options[i]);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_options[i]);\n\t}\n\n\tfor (i = 0; trace_opts[i].name; i++) {\n\t\tif (tracer_flags & trace_opts[i].bit)\n\t\t\tseq_printf(m, \"%s\\n\", trace_opts[i].name);\n\t\telse\n\t\t\tseq_printf(m, \"no%s\\n\", trace_opts[i].name);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int __set_tracer_option(struct trace_array *tr,\n\t\t\t       struct tracer_flags *tracer_flags,\n\t\t\t       struct tracer_opt *opts, int neg)\n{\n\tstruct tracer *trace = tracer_flags->trace;\n\tint ret;\n\n\tret = trace->set_flag(tr, tracer_flags->val, opts->bit, !neg);\n\tif (ret)\n\t\treturn ret;\n\n\tif (neg)\n\t\ttracer_flags->val &= ~opts->bit;\n\telse\n\t\ttracer_flags->val |= opts->bit;\n\treturn 0;\n}\n\n \nstatic int set_tracer_option(struct trace_array *tr, char *cmp, int neg)\n{\n\tstruct tracer *trace = tr->current_trace;\n\tstruct tracer_flags *tracer_flags = trace->flags;\n\tstruct tracer_opt *opts = NULL;\n\tint i;\n\n\tfor (i = 0; tracer_flags->opts[i].name; i++) {\n\t\topts = &tracer_flags->opts[i];\n\n\t\tif (strcmp(cmp, opts->name) == 0)\n\t\t\treturn __set_tracer_option(tr, trace->flags, opts, neg);\n\t}\n\n\treturn -EINVAL;\n}\n\n \nint trace_keep_overwrite(struct tracer *tracer, u32 mask, int set)\n{\n\tif (tracer->enabled && (mask & TRACE_ITER_OVERWRITE) && !set)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nint set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled)\n{\n\tint *map;\n\n\tif ((mask == TRACE_ITER_RECORD_TGID) ||\n\t    (mask == TRACE_ITER_RECORD_CMD))\n\t\tlockdep_assert_held(&event_mutex);\n\n\t \n\tif (!!(tr->trace_flags & mask) == !!enabled)\n\t\treturn 0;\n\n\t \n\tif (tr->current_trace->flag_changed)\n\t\tif (tr->current_trace->flag_changed(tr, mask, !!enabled))\n\t\t\treturn -EINVAL;\n\n\tif (enabled)\n\t\ttr->trace_flags |= mask;\n\telse\n\t\ttr->trace_flags &= ~mask;\n\n\tif (mask == TRACE_ITER_RECORD_CMD)\n\t\ttrace_event_enable_cmd_record(enabled);\n\n\tif (mask == TRACE_ITER_RECORD_TGID) {\n\t\tif (!tgid_map) {\n\t\t\ttgid_map_max = pid_max;\n\t\t\tmap = kvcalloc(tgid_map_max + 1, sizeof(*tgid_map),\n\t\t\t\t       GFP_KERNEL);\n\n\t\t\t \n\t\t\tsmp_store_release(&tgid_map, map);\n\t\t}\n\t\tif (!tgid_map) {\n\t\t\ttr->trace_flags &= ~TRACE_ITER_RECORD_TGID;\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttrace_event_enable_tgid_record(enabled);\n\t}\n\n\tif (mask == TRACE_ITER_EVENT_FORK)\n\t\ttrace_event_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_FUNC_FORK)\n\t\tftrace_pid_follow_fork(tr, enabled);\n\n\tif (mask == TRACE_ITER_OVERWRITE) {\n\t\tring_buffer_change_overwrite(tr->array_buffer.buffer, enabled);\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t\tring_buffer_change_overwrite(tr->max_buffer.buffer, enabled);\n#endif\n\t}\n\n\tif (mask == TRACE_ITER_PRINTK) {\n\t\ttrace_printk_start_stop_comm(enabled);\n\t\ttrace_printk_control(enabled);\n\t}\n\n\treturn 0;\n}\n\nint trace_set_options(struct trace_array *tr, char *option)\n{\n\tchar *cmp;\n\tint neg = 0;\n\tint ret;\n\tsize_t orig_len = strlen(option);\n\tint len;\n\n\tcmp = strstrip(option);\n\n\tlen = str_has_prefix(cmp, \"no\");\n\tif (len)\n\t\tneg = 1;\n\n\tcmp += len;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = match_string(trace_options, -1, cmp);\n\t \n\tif (ret < 0)\n\t\tret = set_tracer_option(tr, cmp, neg);\n\telse\n\t\tret = set_tracer_flag(tr, 1 << ret, !neg);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\t \n\tif (orig_len > strlen(option))\n\t\toption[strlen(option)] = ' ';\n\n\treturn ret;\n}\n\nstatic void __init apply_trace_boot_options(void)\n{\n\tchar *buf = trace_boot_options_buf;\n\tchar *option;\n\n\twhile (true) {\n\t\toption = strsep(&buf, \",\");\n\n\t\tif (!option)\n\t\t\tbreak;\n\n\t\tif (*option)\n\t\t\ttrace_set_options(&global_trace, option);\n\n\t\t \n\t\tif (buf)\n\t\t\t*(buf - 1) = ',';\n\t}\n}\n\nstatic ssize_t\ntracing_trace_options_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tret = trace_set_options(tr, buf);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_trace_options_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tret = single_open(file, tracing_trace_options_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_iter_fops = {\n\t.open\t\t= tracing_trace_options_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_trace_options_write,\n};\n\nstatic const char readme_msg[] =\n\t\"tracing mini-HOWTO:\\n\\n\"\n\t\"# echo 0 > tracing_on : quick way to disable tracing\\n\"\n\t\"# echo 1 > tracing_on : quick way to re-enable tracing\\n\\n\"\n\t\" Important files:\\n\"\n\t\"  trace\\t\\t\\t- The static contents of the buffer\\n\"\n\t\"\\t\\t\\t  To clear the buffer write into this file: echo > trace\\n\"\n\t\"  trace_pipe\\t\\t- A consuming read to see the contents of the buffer\\n\"\n\t\"  current_tracer\\t- function and latency tracers\\n\"\n\t\"  available_tracers\\t- list of configured tracers for current_tracer\\n\"\n\t\"  error_log\\t- error log for failed commands (that support it)\\n\"\n\t\"  buffer_size_kb\\t- view and modify size of per cpu buffer\\n\"\n\t\"  buffer_total_size_kb  - view total size of all cpu buffers\\n\\n\"\n\t\"  trace_clock\\t\\t- change the clock used to order events\\n\"\n\t\"       local:   Per cpu clock but may not be synced across CPUs\\n\"\n\t\"      global:   Synced across CPUs but slows tracing down.\\n\"\n\t\"     counter:   Not a clock, but just an increment\\n\"\n\t\"      uptime:   Jiffy counter from time of boot\\n\"\n\t\"        perf:   Same clock that perf events use\\n\"\n#ifdef CONFIG_X86_64\n\t\"     x86-tsc:   TSC cycle counter\\n\"\n#endif\n\t\"\\n  timestamp_mode\\t- view the mode used to timestamp events\\n\"\n\t\"       delta:   Delta difference against a buffer-wide timestamp\\n\"\n\t\"    absolute:   Absolute (standalone) timestamp\\n\"\n\t\"\\n  trace_marker\\t\\t- Writes into this file writes into the kernel buffer\\n\"\n\t\"\\n  trace_marker_raw\\t\\t- Writes into this file writes binary data into the kernel buffer\\n\"\n\t\"  tracing_cpumask\\t- Limit which CPUs to trace\\n\"\n\t\"  instances\\t\\t- Make sub-buffers with: mkdir instances/foo\\n\"\n\t\"\\t\\t\\t  Remove sub-buffer with rmdir\\n\"\n\t\"  trace_options\\t\\t- Set format or modify how tracing happens\\n\"\n\t\"\\t\\t\\t  Disable an option by prefixing 'no' to the\\n\"\n\t\"\\t\\t\\t  option name\\n\"\n\t\"  saved_cmdlines_size\\t- echo command number in here to store comm-pid list\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"\\n  available_filter_functions - list of functions that can be filtered on\\n\"\n\t\"  set_ftrace_filter\\t- echo function name in here to only trace these\\n\"\n\t\"\\t\\t\\t  functions\\n\"\n\t\"\\t     accepts: func_full_name or glob-matching-pattern\\n\"\n\t\"\\t     modules: Can select a group via module\\n\"\n\t\"\\t      Format: :mod:<module-name>\\n\"\n\t\"\\t     example: echo :mod:ext3 > set_ftrace_filter\\n\"\n\t\"\\t    triggers: a command to perform when function is hit\\n\"\n\t\"\\t      Format: <function>:<trigger>[:count]\\n\"\n\t\"\\t     trigger: traceon, traceoff\\n\"\n\t\"\\t\\t      enable_event:<system>:<event>\\n\"\n\t\"\\t\\t      disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t      stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t      snapshot\\n\"\n#endif\n\t\"\\t\\t      dump\\n\"\n\t\"\\t\\t      cpudump\\n\"\n\t\"\\t     example: echo do_fault:traceoff > set_ftrace_filter\\n\"\n\t\"\\t              echo do_trap:traceoff:3 > set_ftrace_filter\\n\"\n\t\"\\t     The first one will disable tracing every time do_fault is hit\\n\"\n\t\"\\t     The second will disable tracing at most 3 times when do_trap is hit\\n\"\n\t\"\\t       The first time do trap is hit and it disables tracing, the\\n\"\n\t\"\\t       counter will decrement to 2. If tracing is already disabled,\\n\"\n\t\"\\t       the counter will not decrement. It only decrements when the\\n\"\n\t\"\\t       trigger did work\\n\"\n\t\"\\t     To remove trigger without count:\\n\"\n\t\"\\t       echo '!<function>:<trigger> > set_ftrace_filter\\n\"\n\t\"\\t     To remove trigger with a count:\\n\"\n\t\"\\t       echo '!<function>:<trigger>:0 > set_ftrace_filter\\n\"\n\t\"  set_ftrace_notrace\\t- echo function name in here to never trace.\\n\"\n\t\"\\t    accepts: func_full_name, *func_end, func_begin*, *func_middle*\\n\"\n\t\"\\t    modules: Can select a group via module command :mod:\\n\"\n\t\"\\t    Does not accept triggers\\n\"\n#endif  \n#ifdef CONFIG_FUNCTION_TRACER\n\t\"  set_ftrace_pid\\t- Write pid(s) to only function trace those pids\\n\"\n\t\"\\t\\t    (function)\\n\"\n\t\"  set_ftrace_notrace_pid\\t- Write pid(s) to not function trace those pids\\n\"\n\t\"\\t\\t    (function)\\n\"\n#endif\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t\"  set_graph_function\\t- Trace the nested calls of a function (function_graph)\\n\"\n\t\"  set_graph_notrace\\t- Do not trace the nested calls of a function (function_graph)\\n\"\n\t\"  max_graph_depth\\t- Trace a limited depth of nested calls (0 is unlimited)\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\n  snapshot\\t\\t- Like 'trace' but shows the content of the static\\n\"\n\t\"\\t\\t\\t  snapshot buffer. Read the contents for more\\n\"\n\t\"\\t\\t\\t  information\\n\"\n#endif\n#ifdef CONFIG_STACK_TRACER\n\t\"  stack_trace\\t\\t- Shows the max stack trace when active\\n\"\n\t\"  stack_max_size\\t- Shows current max stack size that was traced\\n\"\n\t\"\\t\\t\\t  Write into this file to reset the max size (trigger a\\n\"\n\t\"\\t\\t\\t  new trace)\\n\"\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t\"  stack_trace_filter\\t- Like set_ftrace_filter but limits what stack_trace\\n\"\n\t\"\\t\\t\\t  traces\\n\"\n#endif\n#endif  \n#ifdef CONFIG_DYNAMIC_EVENTS\n\t\"  dynamic_events\\t\\t- Create/append/remove/show the generic dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"  kprobe_events\\t\\t- Create/append/remove/show the kernel dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\"  uprobe_events\\t\\t- Create/append/remove/show the userspace dynamic events\\n\"\n\t\"\\t\\t\\t  Write into this file to define/undefine new trace events.\\n\"\n#endif\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS) || \\\n    defined(CONFIG_FPROBE_EVENTS)\n\t\"\\t  accepts: event-definitions (one definition per line)\\n\"\n#if defined(CONFIG_KPROBE_EVENTS) || defined(CONFIG_UPROBE_EVENTS)\n\t\"\\t   Format: p[:[<group>/][<event>]] <place> [<args>]\\n\"\n\t\"\\t           r[maxactive][:[<group>/][<event>]] <place> [<args>]\\n\"\n#endif\n#ifdef CONFIG_FPROBE_EVENTS\n\t\"\\t           f[:[<group>/][<event>]] <func-name>[%return] [<args>]\\n\"\n\t\"\\t           t[:[<group>/][<event>]] <tracepoint> [<args>]\\n\"\n#endif\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t           s:[synthetic/]<event> <field> [<field>]\\n\"\n#endif\n\t\"\\t           e[:[<group>/][<event>]] <attached-group>.<attached-event> [<args>] [if <filter>]\\n\"\n\t\"\\t           -:[<group>/][<event>]\\n\"\n#ifdef CONFIG_KPROBE_EVENTS\n\t\"\\t    place: [<module>:]<symbol>[+<offset>]|<memaddr>\\n\"\n  \"place (kretprobe): [<module>:]<symbol>[+<offset>]%return|<memaddr>\\n\"\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n  \"   place (uprobe): <path>:<offset>[%return][(ref_ctr_offset)]\\n\"\n#endif\n\t\"\\t     args: <name>=fetcharg[:type]\\n\"\n\t\"\\t fetcharg: (%<register>|$<efield>), @<address>, @<symbol>[+|-<offset>],\\n\"\n#ifdef CONFIG_HAVE_FUNCTION_ARG_ACCESS_API\n#ifdef CONFIG_PROBE_EVENTS_BTF_ARGS\n\t\"\\t           $stack<index>, $stack, $retval, $comm, $arg<N>,\\n\"\n\t\"\\t           <argname>[->field[->field|.field...]],\\n\"\n#else\n\t\"\\t           $stack<index>, $stack, $retval, $comm, $arg<N>,\\n\"\n#endif\n#else\n\t\"\\t           $stack<index>, $stack, $retval, $comm,\\n\"\n#endif\n\t\"\\t           +|-[u]<offset>(<fetcharg>), \\\\imm-value, \\\\\\\"imm-string\\\"\\n\"\n\t\"\\t     type: s8/16/32/64, u8/16/32/64, x8/16/32/64, char, string, symbol,\\n\"\n\t\"\\t           b<bit-width>@<bit-offset>/<container-size>, ustring,\\n\"\n\t\"\\t           symstr, <type>\\\\[<array-size>\\\\]\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t    field: <stype> <name>;\\n\"\n\t\"\\t    stype: u8/u16/u32/u64, s8/s16/s32/s64, pid_t,\\n\"\n\t\"\\t           [unsigned] char/int/long\\n\"\n#endif\n\t\"\\t    efield: For event probes ('e' types), the field is on of the fields\\n\"\n\t\"\\t            of the <attached-group>/<attached-event>.\\n\"\n#endif\n\t\"  events/\\t\\t- Directory containing all trace event subsystems:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all events\\n\"\n\t\"  events/<system>/\\t- Directory containing all trace events for <system>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of all <system>\\n\"\n\t\"\\t\\t\\t  events\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"  events/<system>/<event>/\\t- Directory containing control files for\\n\"\n\t\"\\t\\t\\t  <event>:\\n\"\n\t\"      enable\\t\\t- Write 0/1 to enable/disable tracing of <event>\\n\"\n\t\"      filter\\t\\t- If set, only events passing filter are traced\\n\"\n\t\"      trigger\\t\\t- If set, a command to perform when event is hit\\n\"\n\t\"\\t    Format: <trigger>[:count][if <filter>]\\n\"\n\t\"\\t   trigger: traceon, traceoff\\n\"\n\t\"\\t            enable_event:<system>:<event>\\n\"\n\t\"\\t            disable_event:<system>:<event>\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t            enable_hist:<system>:<event>\\n\"\n\t\"\\t            disable_hist:<system>:<event>\\n\"\n#endif\n#ifdef CONFIG_STACKTRACE\n\t\"\\t\\t    stacktrace\\n\"\n#endif\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t\\t    snapshot\\n\"\n#endif\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"\\t\\t    hist (see below)\\n\"\n#endif\n\t\"\\t   example: echo traceoff > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo traceoff:3 > events/block/block_unplug/trigger\\n\"\n\t\"\\t            echo 'enable_event:kmem:kmalloc:3 if nr_rq > 1' > \\\\\\n\"\n\t\"\\t                  events/block/block_unplug/trigger\\n\"\n\t\"\\t   The first disables tracing every time block_unplug is hit.\\n\"\n\t\"\\t   The second disables tracing the first 3 times block_unplug is hit.\\n\"\n\t\"\\t   The third enables the kmalloc event the first 3 times block_unplug\\n\"\n\t\"\\t     is hit and has value of greater than 1 for the 'nr_rq' event field.\\n\"\n\t\"\\t   Like function triggers, the counter is only decremented if it\\n\"\n\t\"\\t    enabled or disabled tracing.\\n\"\n\t\"\\t   To remove a trigger without a count:\\n\"\n\t\"\\t     echo '!<trigger> > <system>/<event>/trigger\\n\"\n\t\"\\t   To remove a trigger with a count:\\n\"\n\t\"\\t     echo '!<trigger>:0 > <system>/<event>/trigger\\n\"\n\t\"\\t   Filters can be ignored when removing a trigger.\\n\"\n#ifdef CONFIG_HIST_TRIGGERS\n\t\"      hist trigger\\t- If set, event hits are aggregated into a hash table\\n\"\n\t\"\\t    Format: hist:keys=<field1[,field2,...]>\\n\"\n\t\"\\t            [:<var1>=<field|var_ref|numeric_literal>[,<var2>=...]]\\n\"\n\t\"\\t            [:values=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:sort=<field1[,field2,...]>]\\n\"\n\t\"\\t            [:size=#entries]\\n\"\n\t\"\\t            [:pause][:continue][:clear]\\n\"\n\t\"\\t            [:name=histname1]\\n\"\n\t\"\\t            [:nohitcount]\\n\"\n\t\"\\t            [:<handler>.<action>]\\n\"\n\t\"\\t            [if <filter>]\\n\\n\"\n\t\"\\t    Note, special fields can be used as well:\\n\"\n\t\"\\t            common_timestamp - to record current timestamp\\n\"\n\t\"\\t            common_cpu - to record the CPU the event happened on\\n\"\n\t\"\\n\"\n\t\"\\t    A hist trigger variable can be:\\n\"\n\t\"\\t        - a reference to a field e.g. x=current_timestamp,\\n\"\n\t\"\\t        - a reference to another variable e.g. y=$x,\\n\"\n\t\"\\t        - a numeric literal: e.g. ms_per_sec=1000,\\n\"\n\t\"\\t        - an arithmetic expression: e.g. time_secs=current_timestamp/1000\\n\"\n\t\"\\n\"\n\t\"\\t    hist trigger arithmetic expressions support addition(+), subtraction(-),\\n\"\n\t\"\\t    multiplication(*) and division(/) operators. An operand can be either a\\n\"\n\t\"\\t    variable reference, field or numeric literal.\\n\"\n\t\"\\n\"\n\t\"\\t    When a matching event is hit, an entry is added to a hash\\n\"\n\t\"\\t    table using the key(s) and value(s) named, and the value of a\\n\"\n\t\"\\t    sum called 'hitcount' is incremented.  Keys and values\\n\"\n\t\"\\t    correspond to fields in the event's format description.  Keys\\n\"\n\t\"\\t    can be any field, or the special string 'common_stacktrace'.\\n\"\n\t\"\\t    Compound keys consisting of up to two fields can be specified\\n\"\n\t\"\\t    by the 'keys' keyword.  Values must correspond to numeric\\n\"\n\t\"\\t    fields.  Sort keys consisting of up to two fields can be\\n\"\n\t\"\\t    specified using the 'sort' keyword.  The sort direction can\\n\"\n\t\"\\t    be modified by appending '.descending' or '.ascending' to a\\n\"\n\t\"\\t    sort field.  The 'size' parameter can be used to specify more\\n\"\n\t\"\\t    or fewer than the default 2048 entries for the hashtable size.\\n\"\n\t\"\\t    If a hist trigger is given a name using the 'name' parameter,\\n\"\n\t\"\\t    its histogram data will be shared with other triggers of the\\n\"\n\t\"\\t    same name, and trigger hits will update this common data.\\n\\n\"\n\t\"\\t    Reading the 'hist' file for the event will dump the hash\\n\"\n\t\"\\t    table in its entirety to stdout.  If there are multiple hist\\n\"\n\t\"\\t    triggers attached to an event, there will be a table for each\\n\"\n\t\"\\t    trigger in the output.  The table displayed for a named\\n\"\n\t\"\\t    trigger will be the same as any other instance having the\\n\"\n\t\"\\t    same name.  The default format used to display a given field\\n\"\n\t\"\\t    can be modified by appending any of the following modifiers\\n\"\n\t\"\\t    to the field name, as applicable:\\n\\n\"\n\t\"\\t            .hex        display a number as a hex value\\n\"\n\t\"\\t            .sym        display an address as a symbol\\n\"\n\t\"\\t            .sym-offset display an address as a symbol and offset\\n\"\n\t\"\\t            .execname   display a common_pid as a program name\\n\"\n\t\"\\t            .syscall    display a syscall id as a syscall name\\n\"\n\t\"\\t            .log2       display log2 value rather than raw number\\n\"\n\t\"\\t            .buckets=size  display values in groups of size rather than raw number\\n\"\n\t\"\\t            .usecs      display a common_timestamp in microseconds\\n\"\n\t\"\\t            .percent    display a number of percentage value\\n\"\n\t\"\\t            .graph      display a bar-graph of a value\\n\\n\"\n\t\"\\t    The 'pause' parameter can be used to pause an existing hist\\n\"\n\t\"\\t    trigger or to start a hist trigger but not log any events\\n\"\n\t\"\\t    until told to do so.  'continue' can be used to start or\\n\"\n\t\"\\t    restart a paused hist trigger.\\n\\n\"\n\t\"\\t    The 'clear' parameter will clear the contents of a running\\n\"\n\t\"\\t    hist trigger and leave its current paused/active state\\n\"\n\t\"\\t    unchanged.\\n\\n\"\n\t\"\\t    The 'nohitcount' (or NOHC) parameter will suppress display of\\n\"\n\t\"\\t    raw hitcount in the histogram.\\n\\n\"\n\t\"\\t    The enable_hist and disable_hist triggers can be used to\\n\"\n\t\"\\t    have one event conditionally start and stop another event's\\n\"\n\t\"\\t    already-attached hist trigger.  The syntax is analogous to\\n\"\n\t\"\\t    the enable_event and disable_event triggers.\\n\\n\"\n\t\"\\t    Hist trigger handlers and actions are executed whenever a\\n\"\n\t\"\\t    a histogram entry is added or updated.  They take the form:\\n\\n\"\n\t\"\\t        <handler>.<action>\\n\\n\"\n\t\"\\t    The available handlers are:\\n\\n\"\n\t\"\\t        onmatch(matching.event)  - invoke on addition or update\\n\"\n\t\"\\t        onmax(var)               - invoke if var exceeds current max\\n\"\n\t\"\\t        onchange(var)            - invoke action if var changes\\n\\n\"\n\t\"\\t    The available actions are:\\n\\n\"\n\t\"\\t        trace(<synthetic_event>,param list)  - generate synthetic event\\n\"\n\t\"\\t        save(field,...)                      - save current event fields\\n\"\n#ifdef CONFIG_TRACER_SNAPSHOT\n\t\"\\t        snapshot()                           - snapshot the trace buffer\\n\\n\"\n#endif\n#ifdef CONFIG_SYNTH_EVENTS\n\t\"  events/synthetic_events\\t- Create/append/remove/show synthetic events\\n\"\n\t\"\\t  Write into this file to define/undefine new synthetic events.\\n\"\n\t\"\\t     example: echo 'myevent u64 lat; char name[]; long[] stack' >> synthetic_events\\n\"\n#endif\n#endif\n;\n\nstatic ssize_t\ntracing_readme_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\treturn simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t\treadme_msg, strlen(readme_msg));\n}\n\nstatic const struct file_operations tracing_readme_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_readme_read,\n\t.llseek\t\t= generic_file_llseek,\n};\n\nstatic void *saved_tgids_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tint pid = ++(*pos);\n\n\treturn trace_find_tgid_ptr(pid);\n}\n\nstatic void *saved_tgids_start(struct seq_file *m, loff_t *pos)\n{\n\tint pid = *pos;\n\n\treturn trace_find_tgid_ptr(pid);\n}\n\nstatic void saved_tgids_stop(struct seq_file *m, void *v)\n{\n}\n\nstatic int saved_tgids_show(struct seq_file *m, void *v)\n{\n\tint *entry = (int *)v;\n\tint pid = entry - tgid_map;\n\tint tgid = *entry;\n\n\tif (tgid == 0)\n\t\treturn SEQ_SKIP;\n\n\tseq_printf(m, \"%d %d\\n\", pid, tgid);\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_tgids_seq_ops = {\n\t.start\t\t= saved_tgids_start,\n\t.stop\t\t= saved_tgids_stop,\n\t.next\t\t= saved_tgids_next,\n\t.show\t\t= saved_tgids_show,\n};\n\nstatic int tracing_saved_tgids_open(struct inode *inode, struct file *filp)\n{\n\tint ret;\n\n\tret = tracing_check_open_get_tr(NULL);\n\tif (ret)\n\t\treturn ret;\n\n\treturn seq_open(filp, &tracing_saved_tgids_seq_ops);\n}\n\n\nstatic const struct file_operations tracing_saved_tgids_fops = {\n\t.open\t\t= tracing_saved_tgids_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic void *saved_cmdlines_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunsigned int *ptr = v;\n\n\tif (*pos || m->count)\n\t\tptr++;\n\n\t(*pos)++;\n\n\tfor (; ptr < &savedcmd->map_cmdline_to_pid[savedcmd->cmdline_num];\n\t     ptr++) {\n\t\tif (*ptr == -1 || *ptr == NO_CMDLINE_MAP)\n\t\t\tcontinue;\n\n\t\treturn ptr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *saved_cmdlines_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *v;\n\tloff_t l = 0;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\n\tv = &savedcmd->map_cmdline_to_pid[0];\n\twhile (l <= *pos) {\n\t\tv = saved_cmdlines_next(m, v, &l);\n\t\tif (!v)\n\t\t\treturn NULL;\n\t}\n\n\treturn v;\n}\n\nstatic void saved_cmdlines_stop(struct seq_file *m, void *v)\n{\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n}\n\nstatic int saved_cmdlines_show(struct seq_file *m, void *v)\n{\n\tchar buf[TASK_COMM_LEN];\n\tunsigned int *pid = v;\n\n\t__trace_find_cmdline(*pid, buf);\n\tseq_printf(m, \"%d %s\\n\", *pid, buf);\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_saved_cmdlines_seq_ops = {\n\t.start\t\t= saved_cmdlines_start,\n\t.next\t\t= saved_cmdlines_next,\n\t.stop\t\t= saved_cmdlines_stop,\n\t.show\t\t= saved_cmdlines_show,\n};\n\nstatic int tracing_saved_cmdlines_open(struct inode *inode, struct file *filp)\n{\n\tint ret;\n\n\tret = tracing_check_open_get_tr(NULL);\n\tif (ret)\n\t\treturn ret;\n\n\treturn seq_open(filp, &tracing_saved_cmdlines_seq_ops);\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_fops = {\n\t.open\t\t= tracing_saved_cmdlines_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic ssize_t\ntracing_saved_cmdlines_size_read(struct file *filp, char __user *ubuf,\n\t\t\t\t size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\tr = scnprintf(buf, sizeof(buf), \"%u\\n\", savedcmd->cmdline_num);\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic void free_saved_cmdlines_buffer(struct saved_cmdlines_buffer *s)\n{\n\tkfree(s->saved_cmdlines);\n\tkfree(s->map_cmdline_to_pid);\n\tkfree(s);\n}\n\nstatic int tracing_resize_saved_cmdlines(unsigned int val)\n{\n\tstruct saved_cmdlines_buffer *s, *savedcmd_temp;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tif (allocate_cmdlines_buffer(val, s) < 0) {\n\t\tkfree(s);\n\t\treturn -ENOMEM;\n\t}\n\n\tpreempt_disable();\n\tarch_spin_lock(&trace_cmdline_lock);\n\tsavedcmd_temp = savedcmd;\n\tsavedcmd = s;\n\tarch_spin_unlock(&trace_cmdline_lock);\n\tpreempt_enable();\n\tfree_saved_cmdlines_buffer(savedcmd_temp);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_saved_cmdlines_size_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t  size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!val || val > PID_MAX_DEFAULT)\n\t\treturn -EINVAL;\n\n\tret = tracing_resize_saved_cmdlines((unsigned int)val);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations tracing_saved_cmdlines_size_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_saved_cmdlines_size_read,\n\t.write\t\t= tracing_saved_cmdlines_size_write,\n};\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic union trace_eval_map_item *\nupdate_eval_map(union trace_eval_map_item *ptr)\n{\n\tif (!ptr->map.eval_string) {\n\t\tif (ptr->tail.next) {\n\t\t\tptr = ptr->tail.next;\n\t\t\t \n\t\t\tptr++;\n\t\t} else\n\t\t\treturn NULL;\n\t}\n\treturn ptr;\n}\n\nstatic void *eval_map_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\t \n\t(*pos)++;\n\tptr = update_eval_map(ptr);\n\tif (WARN_ON_ONCE(!ptr))\n\t\treturn NULL;\n\n\tptr++;\n\tptr = update_eval_map(ptr);\n\n\treturn ptr;\n}\n\nstatic void *eval_map_start(struct seq_file *m, loff_t *pos)\n{\n\tunion trace_eval_map_item *v;\n\tloff_t l = 0;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tv = trace_eval_maps;\n\tif (v)\n\t\tv++;\n\n\twhile (v && l < *pos) {\n\t\tv = eval_map_next(m, v, &l);\n\t}\n\n\treturn v;\n}\n\nstatic void eval_map_stop(struct seq_file *m, void *v)\n{\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic int eval_map_show(struct seq_file *m, void *v)\n{\n\tunion trace_eval_map_item *ptr = v;\n\n\tseq_printf(m, \"%s %ld (%s)\\n\",\n\t\t   ptr->map.eval_string, ptr->map.eval_value,\n\t\t   ptr->map.system);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_eval_map_seq_ops = {\n\t.start\t\t= eval_map_start,\n\t.next\t\t= eval_map_next,\n\t.stop\t\t= eval_map_stop,\n\t.show\t\t= eval_map_show,\n};\n\nstatic int tracing_eval_map_open(struct inode *inode, struct file *filp)\n{\n\tint ret;\n\n\tret = tracing_check_open_get_tr(NULL);\n\tif (ret)\n\t\treturn ret;\n\n\treturn seq_open(filp, &tracing_eval_map_seq_ops);\n}\n\nstatic const struct file_operations tracing_eval_map_fops = {\n\t.open\t\t= tracing_eval_map_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= seq_release,\n};\n\nstatic inline union trace_eval_map_item *\ntrace_eval_jmp_to_tail(union trace_eval_map_item *ptr)\n{\n\t \n\treturn ptr + ptr->head.length + 1;\n}\n\nstatic void\ntrace_insert_eval_map_file(struct module *mod, struct trace_eval_map **start,\n\t\t\t   int len)\n{\n\tstruct trace_eval_map **stop;\n\tstruct trace_eval_map **map;\n\tunion trace_eval_map_item *map_array;\n\tunion trace_eval_map_item *ptr;\n\n\tstop = start + len;\n\n\t \n\tmap_array = kmalloc_array(len + 2, sizeof(*map_array), GFP_KERNEL);\n\tif (!map_array) {\n\t\tpr_warn(\"Unable to allocate trace eval mapping\\n\");\n\t\treturn;\n\t}\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tif (!trace_eval_maps)\n\t\ttrace_eval_maps = map_array;\n\telse {\n\t\tptr = trace_eval_maps;\n\t\tfor (;;) {\n\t\t\tptr = trace_eval_jmp_to_tail(ptr);\n\t\t\tif (!ptr->tail.next)\n\t\t\t\tbreak;\n\t\t\tptr = ptr->tail.next;\n\n\t\t}\n\t\tptr->tail.next = map_array;\n\t}\n\tmap_array->head.mod = mod;\n\tmap_array->head.length = len;\n\tmap_array++;\n\n\tfor (map = start; (unsigned long)map < (unsigned long)stop; map++) {\n\t\tmap_array->map = **map;\n\t\tmap_array++;\n\t}\n\tmemset(map_array, 0, sizeof(*map_array));\n\n\tmutex_unlock(&trace_eval_mutex);\n}\n\nstatic void trace_create_eval_file(struct dentry *d_tracer)\n{\n\ttrace_create_file(\"eval_map\", TRACE_MODE_READ, d_tracer,\n\t\t\t  NULL, &tracing_eval_map_fops);\n}\n\n#else  \nstatic inline void trace_create_eval_file(struct dentry *d_tracer) { }\nstatic inline void trace_insert_eval_map_file(struct module *mod,\n\t\t\t      struct trace_eval_map **start, int len) { }\n#endif  \n\nstatic void trace_insert_eval_map(struct module *mod,\n\t\t\t\t  struct trace_eval_map **start, int len)\n{\n\tstruct trace_eval_map **map;\n\n\tif (len <= 0)\n\t\treturn;\n\n\tmap = start;\n\n\ttrace_event_eval_update(map, len);\n\n\ttrace_insert_eval_map_file(mod, start, len);\n}\n\nstatic ssize_t\ntracing_set_trace_read(struct file *filp, char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+2];\n\tint r;\n\n\tmutex_lock(&trace_types_lock);\n\tr = sprintf(buf, \"%s\\n\", tr->current_trace->name);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nint tracer_init(struct tracer *t, struct trace_array *tr)\n{\n\ttracing_reset_online_cpus(&tr->array_buffer);\n\treturn t->init(tr);\n}\n\nstatic void set_buffer_entries(struct array_buffer *buf, unsigned long val)\n{\n\tint cpu;\n\n\tfor_each_tracing_cpu(cpu)\n\t\tper_cpu_ptr(buf->data, cpu)->entries = val;\n}\n\nstatic void update_buffer_entries(struct array_buffer *buf, int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tset_buffer_entries(buf, ring_buffer_size(buf->buffer, 0));\n\t} else {\n\t\tper_cpu_ptr(buf->data, cpu)->entries = ring_buffer_size(buf->buffer, cpu);\n\t}\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n \nstatic int resize_buffer_duplicate_size(struct array_buffer *trace_buf,\n\t\t\t\t\tstruct array_buffer *size_buf, int cpu_id)\n{\n\tint cpu, ret = 0;\n\n\tif (cpu_id == RING_BUFFER_ALL_CPUS) {\n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu)->entries, cpu);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\t\t\tper_cpu_ptr(trace_buf->data, cpu)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu)->entries;\n\t\t}\n\t} else {\n\t\tret = ring_buffer_resize(trace_buf->buffer,\n\t\t\t\t per_cpu_ptr(size_buf->data, cpu_id)->entries, cpu_id);\n\t\tif (ret == 0)\n\t\t\tper_cpu_ptr(trace_buf->data, cpu_id)->entries =\n\t\t\t\tper_cpu_ptr(size_buf->data, cpu_id)->entries;\n\t}\n\n\treturn ret;\n}\n#endif  \n\nstatic int __tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\tunsigned long size, int cpu)\n{\n\tint ret;\n\n\t \n\tring_buffer_expanded = true;\n\n\t \n\tif (!tr->array_buffer.buffer)\n\t\treturn 0;\n\n\t \n\ttracing_stop_tr(tr);\n\n\tret = ring_buffer_resize(tr->array_buffer.buffer, size, cpu);\n\tif (ret < 0)\n\t\tgoto out_start;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (!tr->allocated_snapshot)\n\t\tgoto out;\n\n\tret = ring_buffer_resize(tr->max_buffer.buffer, size, cpu);\n\tif (ret < 0) {\n\t\tint r = resize_buffer_duplicate_size(&tr->array_buffer,\n\t\t\t\t\t\t     &tr->array_buffer, cpu);\n\t\tif (r < 0) {\n\t\t\t \n\t\t\tWARN_ON(1);\n\t\t\ttracing_disabled = 1;\n\t\t}\n\t\tgoto out_start;\n\t}\n\n\tupdate_buffer_entries(&tr->max_buffer, cpu);\n\n out:\n#endif  \n\n\tupdate_buffer_entries(&tr->array_buffer, cpu);\n out_start:\n\ttracing_start_tr(tr);\n\treturn ret;\n}\n\nssize_t tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t  unsigned long size, int cpu_id)\n{\n\tint ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu_id != RING_BUFFER_ALL_CPUS) {\n\t\t \n\t\tif (!cpumask_test_cpu(cpu_id, tracing_buffer_mask)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = __tracing_resize_ring_buffer(tr, size, cpu_id);\n\tif (ret < 0)\n\t\tret = -ENOMEM;\n\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n\n \nint tracing_update_buffers(void)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tif (!ring_buffer_expanded)\n\t\tret = __tracing_resize_ring_buffer(&global_trace, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct trace_option_dentry;\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer);\n\n \nstatic void tracing_set_nop(struct trace_array *tr)\n{\n\tif (tr->current_trace == &nop_trace)\n\t\treturn;\n\t\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n\ttr->current_trace = &nop_trace;\n}\n\nstatic bool tracer_options_updated;\n\nstatic void add_tracer_options(struct trace_array *tr, struct tracer *t)\n{\n\t \n\tif (!tr->dir)\n\t\treturn;\n\n\t \n\tif (!tracer_options_updated)\n\t\treturn;\n\n\tcreate_trace_option_files(tr, t);\n}\n\nint tracing_set_tracer(struct trace_array *tr, const char *buf)\n{\n\tstruct tracer *t;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool had_max_tr;\n#endif\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (!ring_buffer_expanded) {\n\t\tret = __tracing_resize_ring_buffer(tr, trace_buf_size,\n\t\t\t\t\t\tRING_BUFFER_ALL_CPUS);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tret = 0;\n\t}\n\n\tfor (t = trace_types; t; t = t->next) {\n\t\tif (strcmp(t->name, buf) == 0)\n\t\t\tbreak;\n\t}\n\tif (!t) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (t == tr->current_trace)\n\t\tgoto out;\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\tif (t->use_max_tr) {\n\t\tlocal_irq_disable();\n\t\tarch_spin_lock(&tr->max_lock);\n\t\tif (tr->cond_snapshot)\n\t\t\tret = -EBUSY;\n\t\tarch_spin_unlock(&tr->max_lock);\n\t\tlocal_irq_enable();\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n#endif\n\t \n\tif (system_state < SYSTEM_RUNNING && t->noboot) {\n\t\tpr_warn(\"Tracer '%s' is not allowed on command line, ignored\\n\",\n\t\t\tt->name);\n\t\tgoto out;\n\t}\n\n\t \n\tif (!trace_ok_for_array(t, tr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (tr->trace_ref) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\ttrace_branch_disable();\n\n\ttr->current_trace->enabled--;\n\n\tif (tr->current_trace->reset)\n\t\ttr->current_trace->reset(tr);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\thad_max_tr = tr->current_trace->use_max_tr;\n\n\t \n\ttr->current_trace = &nop_trace;\n\n\tif (had_max_tr && !t->use_max_tr) {\n\t\t \n\t\tsynchronize_rcu();\n\t\tfree_snapshot(tr);\n\t}\n\n\tif (t->use_max_tr && !tr->allocated_snapshot) {\n\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n#else\n\ttr->current_trace = &nop_trace;\n#endif\n\n\tif (t->init) {\n\t\tret = tracer_init(t, tr);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\ttr->current_trace = t;\n\ttr->current_trace->enabled++;\n\ttrace_branch_enable(tr);\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_set_trace_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[MAX_TRACER_SIZE+1];\n\tchar *name;\n\tsize_t ret;\n\tint err;\n\n\tret = cnt;\n\n\tif (cnt > MAX_TRACER_SIZE)\n\t\tcnt = MAX_TRACER_SIZE;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tname = strim(buf);\n\n\terr = tracing_set_tracer(tr, name);\n\tif (err)\n\t\treturn err;\n\n\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_nsecs_read(unsigned long *ptr, char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\n\tint r;\n\n\tr = snprintf(buf, sizeof(buf), \"%ld\\n\",\n\t\t     *ptr == (unsigned long)-1 ? -1 : nsecs_to_usecs(*ptr));\n\tif (r > sizeof(buf))\n\t\tr = sizeof(buf);\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_nsecs_write(unsigned long *ptr, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t*ptr = val * 1000;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_thresh_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn tracing_nsecs_read(&tracing_thresh, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_thresh_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tint ret;\n\n\tmutex_lock(&trace_types_lock);\n\tret = tracing_nsecs_write(&tracing_thresh, ubuf, cnt, ppos);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (tr->current_trace->update_thresh) {\n\t\tret = tr->current_trace->update_thresh(tr);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = cnt;\nout:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\nstatic ssize_t\ntracing_max_lat_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\n\treturn tracing_nsecs_read(&tr->max_latency, ubuf, cnt, ppos);\n}\n\nstatic ssize_t\ntracing_max_lat_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\n\treturn tracing_nsecs_write(&tr->max_latency, ubuf, cnt, ppos);\n}\n\n#endif\n\nstatic int open_pipe_on_cpu(struct trace_array *tr, int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tif (cpumask_empty(tr->pipe_cpumask)) {\n\t\t\tcpumask_setall(tr->pipe_cpumask);\n\t\t\treturn 0;\n\t\t}\n\t} else if (!cpumask_test_cpu(cpu, tr->pipe_cpumask)) {\n\t\tcpumask_set_cpu(cpu, tr->pipe_cpumask);\n\t\treturn 0;\n\t}\n\treturn -EBUSY;\n}\n\nstatic void close_pipe_on_cpu(struct trace_array *tr, int cpu)\n{\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tWARN_ON(!cpumask_full(tr->pipe_cpumask));\n\t\tcpumask_clear(tr->pipe_cpumask);\n\t} else {\n\t\tWARN_ON(!cpumask_test_cpu(cpu, tr->pipe_cpumask));\n\t\tcpumask_clear_cpu(cpu, tr->pipe_cpumask);\n\t}\n}\n\nstatic int tracing_open_pipe(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tint cpu;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&trace_types_lock);\n\tcpu = tracing_get_cpu(inode);\n\tret = open_pipe_on_cpu(tr, cpu);\n\tif (ret)\n\t\tgoto fail_pipe_on_cpu;\n\n\t \n\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_alloc_iter;\n\t}\n\n\ttrace_seq_init(&iter->seq);\n\titer->trace = tr->current_trace;\n\n\tif (!alloc_cpumask_var(&iter->started, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tcpumask_setall(iter->started);\n\n\tif (tr->trace_flags & TRACE_ITER_LATENCY_FMT)\n\t\titer->iter_flags |= TRACE_FILE_LAT_FMT;\n\n\t \n\tif (trace_clocks[tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\titer->tr = tr;\n\titer->array_buffer = &tr->array_buffer;\n\titer->cpu_file = cpu;\n\tmutex_init(&iter->mutex);\n\tfilp->private_data = iter;\n\n\tif (iter->trace->pipe_open)\n\t\titer->trace->pipe_open(iter);\n\n\tnonseekable_open(inode, filp);\n\n\ttr->trace_ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n\nfail:\n\tkfree(iter);\nfail_alloc_iter:\n\tclose_pipe_on_cpu(tr, cpu);\nfail_pipe_on_cpu:\n\t__trace_array_put(tr);\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_release_pipe(struct inode *inode, struct file *file)\n{\n\tstruct trace_iterator *iter = file->private_data;\n\tstruct trace_array *tr = inode->i_private;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->trace_ref--;\n\n\tif (iter->trace->pipe_close)\n\t\titer->trace->pipe_close(iter);\n\tclose_pipe_on_cpu(tr, iter->cpu_file);\n\tmutex_unlock(&trace_types_lock);\n\n\tfree_trace_iter_content(iter);\n\tkfree(iter);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic __poll_t\ntrace_poll(struct trace_iterator *iter, struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_array *tr = iter->tr;\n\n\t \n\tif (trace_buffer_iter(iter, iter->cpu_file))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\n\tif (tr->trace_flags & TRACE_ITER_BLOCK)\n\t\t \n\t\treturn EPOLLIN | EPOLLRDNORM;\n\telse\n\t\treturn ring_buffer_poll_wait(iter->array_buffer->buffer, iter->cpu_file,\n\t\t\t\t\t     filp, poll_table, iter->tr->buffer_percent);\n}\n\nstatic __poll_t\ntracing_poll_pipe(struct file *filp, poll_table *poll_table)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\n \nstatic int tracing_wait_pipe(struct file *filp)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tint ret;\n\n\twhile (trace_empty(iter)) {\n\n\t\tif ((filp->f_flags & O_NONBLOCK)) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t \n\t\tif (!tracer_tracing_is_on(iter->tr) && iter->pos)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&iter->mutex);\n\n\t\tret = wait_on_pipe(iter, 0);\n\n\t\tmutex_lock(&iter->mutex);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 1;\n}\n\n \nstatic ssize_t\ntracing_read_pipe(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tstruct trace_iterator *iter = filp->private_data;\n\tssize_t sret;\n\n\t \n\tmutex_lock(&iter->mutex);\n\n\t \n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (sret != -EBUSY)\n\t\tgoto out;\n\n\ttrace_seq_init(&iter->seq);\n\n\tif (iter->trace->read) {\n\t\tsret = iter->trace->read(iter, filp, ubuf, cnt, ppos);\n\t\tif (sret)\n\t\t\tgoto out;\n\t}\n\nwaitagain:\n\tsret = tracing_wait_pipe(filp);\n\tif (sret <= 0)\n\t\tgoto out;\n\n\t \n\tif (trace_empty(iter)) {\n\t\tsret = 0;\n\t\tgoto out;\n\t}\n\n\tif (cnt >= PAGE_SIZE)\n\t\tcnt = PAGE_SIZE - 1;\n\n\t \n\ttrace_iterator_reset(iter);\n\tcpumask_clear(iter->started);\n\ttrace_seq_init(&iter->seq);\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\twhile (trace_find_next_entry_inc(iter) != NULL) {\n\t\tenum print_line_t ret;\n\t\tint save_len = iter->seq.seq.len;\n\n\t\tret = print_trace_line(iter);\n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\t \n\t\t\tif (save_len == 0) {\n\t\t\t\titer->seq.full = 0;\n\t\t\t\ttrace_seq_puts(&iter->seq, \"[LINE TOO BIG]\\n\");\n\t\t\t\ttrace_consume(iter);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\n\t\tif (trace_seq_used(&iter->seq) >= cnt)\n\t\t\tbreak;\n\n\t\t \n\t\tWARN_ONCE(iter->seq.full, \"full flag set for trace type %d\",\n\t\t\t  iter->ent->type);\n\t}\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\n\t \n\tsret = trace_seq_to_user(&iter->seq, ubuf, cnt);\n\tif (iter->seq.seq.readpos >= trace_seq_used(&iter->seq))\n\t\ttrace_seq_init(&iter->seq);\n\n\t \n\tif (sret == -EBUSY)\n\t\tgoto waitagain;\n\nout:\n\tmutex_unlock(&iter->mutex);\n\n\treturn sret;\n}\n\nstatic void tracing_spd_release_pipe(struct splice_pipe_desc *spd,\n\t\t\t\t     unsigned int idx)\n{\n\t__free_page(spd->pages[idx]);\n}\n\nstatic size_t\ntracing_fill_pipe_page(size_t rem, struct trace_iterator *iter)\n{\n\tsize_t count;\n\tint save_len;\n\tint ret;\n\n\t \n\tfor (;;) {\n\t\tsave_len = iter->seq.seq.len;\n\t\tret = print_trace_line(iter);\n\n\t\tif (trace_seq_has_overflowed(&iter->seq)) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (ret == TRACE_TYPE_PARTIAL_LINE) {\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tcount = trace_seq_used(&iter->seq) - save_len;\n\t\tif (rem < count) {\n\t\t\trem = 0;\n\t\t\titer->seq.seq.len = save_len;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\ttrace_consume(iter);\n\t\trem -= count;\n\t\tif (!trace_find_next_entry_inc(iter))\t{\n\t\t\trem = 0;\n\t\t\titer->ent = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn rem;\n}\n\nstatic ssize_t tracing_splice_read_pipe(struct file *filp,\n\t\t\t\t\tloff_t *ppos,\n\t\t\t\t\tstruct pipe_inode_info *pipe,\n\t\t\t\t\tsize_t len,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct trace_iterator *iter = filp->private_data;\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages\t= 0,  \n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &default_pipe_buf_ops,\n\t\t.spd_release\t= tracing_spd_release_pipe,\n\t};\n\tssize_t ret;\n\tsize_t rem;\n\tunsigned int i;\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&iter->mutex);\n\n\tif (iter->trace->splice_read) {\n\t\tret = iter->trace->splice_read(iter, filp,\n\t\t\t\t\t       ppos, pipe, len, flags);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t}\n\n\tret = tracing_wait_pipe(filp);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\n\tif (!iter->ent && !trace_find_next_entry_inc(iter)) {\n\t\tret = -EFAULT;\n\t\tgoto out_err;\n\t}\n\n\ttrace_event_read_lock();\n\ttrace_access_lock(iter->cpu_file);\n\n\t \n\tfor (i = 0, rem = len; i < spd.nr_pages_max && rem; i++) {\n\t\tspd.pages[i] = alloc_page(GFP_KERNEL);\n\t\tif (!spd.pages[i])\n\t\t\tbreak;\n\n\t\trem = tracing_fill_pipe_page(rem, iter);\n\n\t\t \n\t\tret = trace_seq_to_buffer(&iter->seq,\n\t\t\t\t\t  page_address(spd.pages[i]),\n\t\t\t\t\t  trace_seq_used(&iter->seq));\n\t\tif (ret < 0) {\n\t\t\t__free_page(spd.pages[i]);\n\t\t\tbreak;\n\t\t}\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].len = trace_seq_used(&iter->seq);\n\n\t\ttrace_seq_init(&iter->seq);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\ttrace_event_read_unlock();\n\tmutex_unlock(&iter->mutex);\n\n\tspd.nr_pages = i;\n\n\tif (i)\n\t\tret = splice_to_pipe(pipe, &spd);\n\telse\n\t\tret = 0;\nout:\n\tsplice_shrink_spd(&spd);\n\treturn ret;\n\nout_err:\n\tmutex_unlock(&iter->mutex);\n\tgoto out;\n}\n\nstatic ssize_t\ntracing_entries_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tint cpu = tracing_get_cpu(inode);\n\tchar buf[64];\n\tint r = 0;\n\tssize_t ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (cpu == RING_BUFFER_ALL_CPUS) {\n\t\tint cpu, buf_size_same;\n\t\tunsigned long size;\n\n\t\tsize = 0;\n\t\tbuf_size_same = 1;\n\t\t \n\t\tfor_each_tracing_cpu(cpu) {\n\t\t\t \n\t\t\tif (size == 0)\n\t\t\t\tsize = per_cpu_ptr(tr->array_buffer.data, cpu)->entries;\n\t\t\tif (size != per_cpu_ptr(tr->array_buffer.data, cpu)->entries) {\n\t\t\t\tbuf_size_same = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (buf_size_same) {\n\t\t\tif (!ring_buffer_expanded)\n\t\t\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\",\n\t\t\t\t\t    size >> 10,\n\t\t\t\t\t    trace_buf_size >> 10);\n\t\t\telse\n\t\t\t\tr = sprintf(buf, \"%lu\\n\", size >> 10);\n\t\t} else\n\t\t\tr = sprintf(buf, \"X\\n\");\n\t} else\n\t\tr = sprintf(buf, \"%lu\\n\", per_cpu_ptr(tr->array_buffer.data, cpu)->entries >> 10);\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n\treturn ret;\n}\n\nstatic ssize_t\ntracing_entries_write(struct file *filp, const char __user *ubuf,\n\t\t      size_t cnt, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!val)\n\t\treturn -EINVAL;\n\n\t \n\tval <<= 10;\n\tret = tracing_resize_ring_buffer(tr, val, tracing_get_cpu(inode));\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\ntracing_total_entries_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r, cpu;\n\tunsigned long size = 0, expanded_size = 0;\n\n\tmutex_lock(&trace_types_lock);\n\tfor_each_tracing_cpu(cpu) {\n\t\tsize += per_cpu_ptr(tr->array_buffer.data, cpu)->entries >> 10;\n\t\tif (!ring_buffer_expanded)\n\t\t\texpanded_size += trace_buf_size >> 10;\n\t}\n\tif (ring_buffer_expanded)\n\t\tr = sprintf(buf, \"%lu\\n\", size);\n\telse\n\t\tr = sprintf(buf, \"%lu (expanded: %lu)\\n\", size, expanded_size);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\ntracing_free_buffer_write(struct file *filp, const char __user *ubuf,\n\t\t\t  size_t cnt, loff_t *ppos)\n{\n\t \n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int\ntracing_free_buffer_release(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\t \n\tif (tr->trace_flags & TRACE_ITER_STOP_ON_FREE)\n\t\ttracer_tracing_off(tr);\n\t \n\ttracing_resize_ring_buffer(tr, 0, RING_BUFFER_ALL_CPUS);\n\n\ttrace_array_put(tr);\n\n\treturn 0;\n}\n\nstatic ssize_t\ntracing_mark_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tenum event_trigger_type tt = ETT_NONE;\n\tstruct trace_buffer *buffer;\n\tstruct print_entry *entry;\n\tssize_t written;\n\tint size;\n\tint len;\n\n \n#define FAULTED_STR \"<faulted>\"\n#define FAULTED_SIZE (sizeof(FAULTED_STR) - 1)  \n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tsize = sizeof(*entry) + cnt + 2;  \n\n\t \n\tif (cnt < FAULTED_SIZE)\n\t\tsize += FAULTED_SIZE - cnt;\n\n\tbuffer = tr->array_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_PRINT, size,\n\t\t\t\t\t    tracing_gen_ctx());\n\tif (unlikely(!event))\n\t\t \n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\tentry->ip = _THIS_IP_;\n\n\tlen = __copy_from_user_inatomic(&entry->buf, ubuf, cnt);\n\tif (len) {\n\t\tmemcpy(&entry->buf, FAULTED_STR, FAULTED_SIZE);\n\t\tcnt = FAULTED_SIZE;\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\n\tif (tr->trace_marker_file && !list_empty(&tr->trace_marker_file->triggers)) {\n\t\t \n\t\tentry->buf[cnt] = '\\0';\n\t\ttt = event_triggers_call(tr->trace_marker_file, buffer, entry, event);\n\t}\n\n\tif (entry->buf[cnt - 1] != '\\n') {\n\t\tentry->buf[cnt] = '\\n';\n\t\tentry->buf[cnt + 1] = '\\0';\n\t} else\n\t\tentry->buf[cnt] = '\\0';\n\n\tif (static_branch_unlikely(&trace_marker_exports_enabled))\n\t\tftrace_exports(event, TRACE_EXPORT_MARKER);\n\t__buffer_unlock_commit(buffer, event);\n\n\tif (tt)\n\t\tevent_triggers_post_call(tr->trace_marker_file, tt);\n\n\treturn written;\n}\n\n \n#define RAW_DATA_MAX_SIZE (1024*3)\n\nstatic ssize_t\ntracing_mark_raw_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *fpos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct ring_buffer_event *event;\n\tstruct trace_buffer *buffer;\n\tstruct raw_data_entry *entry;\n\tssize_t written;\n\tint size;\n\tint len;\n\n#define FAULT_SIZE_ID (FAULTED_SIZE + sizeof(int))\n\n\tif (tracing_disabled)\n\t\treturn -EINVAL;\n\n\tif (!(tr->trace_flags & TRACE_ITER_MARKERS))\n\t\treturn -EINVAL;\n\n\t \n\tif (cnt < sizeof(unsigned int) || cnt > RAW_DATA_MAX_SIZE)\n\t\treturn -EINVAL;\n\n\tif (cnt > TRACE_BUF_SIZE)\n\t\tcnt = TRACE_BUF_SIZE;\n\n\tBUILD_BUG_ON(TRACE_BUF_SIZE >= PAGE_SIZE);\n\n\tsize = sizeof(*entry) + cnt;\n\tif (cnt < FAULT_SIZE_ID)\n\t\tsize += FAULT_SIZE_ID - cnt;\n\n\tbuffer = tr->array_buffer.buffer;\n\tevent = __trace_buffer_lock_reserve(buffer, TRACE_RAW_DATA, size,\n\t\t\t\t\t    tracing_gen_ctx());\n\tif (!event)\n\t\t \n\t\treturn -EBADF;\n\n\tentry = ring_buffer_event_data(event);\n\n\tlen = __copy_from_user_inatomic(&entry->id, ubuf, cnt);\n\tif (len) {\n\t\tentry->id = -1;\n\t\tmemcpy(&entry->buf, FAULTED_STR, FAULTED_SIZE);\n\t\twritten = -EFAULT;\n\t} else\n\t\twritten = cnt;\n\n\t__buffer_unlock_commit(buffer, event);\n\n\treturn written;\n}\n\nstatic int tracing_clock_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++)\n\t\tseq_printf(m,\n\t\t\t\"%s%s%s%s\", i ? \" \" : \"\",\n\t\t\ti == tr->clock_id ? \"[\" : \"\", trace_clocks[i].name,\n\t\t\ti == tr->clock_id ? \"]\" : \"\");\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nint tracing_set_clock(struct trace_array *tr, const char *clockstr)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(trace_clocks); i++) {\n\t\tif (strcmp(trace_clocks[i].name, clockstr) == 0)\n\t\t\tbreak;\n\t}\n\tif (i == ARRAY_SIZE(trace_clocks))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&trace_types_lock);\n\n\ttr->clock_id = i;\n\n\tring_buffer_set_clock(tr->array_buffer.buffer, trace_clocks[i].func);\n\n\t \n\ttracing_reset_online_cpus(&tr->array_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (tr->max_buffer.buffer)\n\t\tring_buffer_set_clock(tr->max_buffer.buffer, trace_clocks[i].func);\n\ttracing_reset_online_cpus(&tr->max_buffer);\n#endif\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic ssize_t tracing_clock_write(struct file *filp, const char __user *ubuf,\n\t\t\t\t   size_t cnt, loff_t *fpos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tchar buf[64];\n\tconst char *clockstr;\n\tint ret;\n\n\tif (cnt >= sizeof(buf))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(buf, ubuf, cnt))\n\t\treturn -EFAULT;\n\n\tbuf[cnt] = 0;\n\n\tclockstr = strstrip(buf);\n\n\tret = tracing_set_clock(tr, clockstr);\n\tif (ret)\n\t\treturn ret;\n\n\t*fpos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_clock_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tret = single_open(file, tracing_clock_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic int tracing_time_stamp_mode_show(struct seq_file *m, void *v)\n{\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (ring_buffer_time_stamp_abs(tr->array_buffer.buffer))\n\t\tseq_puts(m, \"delta [absolute]\\n\");\n\telse\n\t\tseq_puts(m, \"[delta] absolute\\n\");\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstatic int tracing_time_stamp_mode_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tret = single_open(file, tracing_time_stamp_mode_show, inode->i_private);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nu64 tracing_event_time_stamp(struct trace_buffer *buffer, struct ring_buffer_event *rbe)\n{\n\tif (rbe == this_cpu_read(trace_buffered_event))\n\t\treturn ring_buffer_time_stamp(buffer);\n\n\treturn ring_buffer_event_time_stamp(buffer, rbe);\n}\n\n \nint tracing_set_filter_buffering(struct trace_array *tr, bool set)\n{\n\tint ret = 0;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (set && tr->no_filter_buffering_ref++)\n\t\tgoto out;\n\n\tif (!set) {\n\t\tif (WARN_ON_ONCE(!tr->no_filter_buffering_ref)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t--tr->no_filter_buffering_ref;\n\t}\n out:\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\n\nstruct ftrace_buffer_info {\n\tstruct trace_iterator\titer;\n\tvoid\t\t\t*spare;\n\tunsigned int\t\tspare_cpu;\n\tunsigned int\t\tread;\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic int tracing_snapshot_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct trace_iterator *iter;\n\tstruct seq_file *m;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = __tracing_open(inode, file, true);\n\t\tif (IS_ERR(iter))\n\t\t\tret = PTR_ERR(iter);\n\t} else {\n\t\t \n\t\tret = -ENOMEM;\n\t\tm = kzalloc(sizeof(*m), GFP_KERNEL);\n\t\tif (!m)\n\t\t\tgoto out;\n\t\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\t\tif (!iter) {\n\t\t\tkfree(m);\n\t\t\tgoto out;\n\t\t}\n\t\tret = 0;\n\n\t\titer->tr = tr;\n\t\titer->array_buffer = &tr->max_buffer;\n\t\titer->cpu_file = tracing_get_cpu(inode);\n\t\tm->private = iter;\n\t\tfile->private_data = m;\n\t}\nout:\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic void tracing_swap_cpu_buffer(void *tr)\n{\n\tupdate_max_tr_single((struct trace_array *)tr, current, smp_processor_id());\n}\n\nstatic ssize_t\ntracing_snapshot_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t       loff_t *ppos)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\tunsigned long val;\n\tint ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_lock(&trace_types_lock);\n\n\tif (tr->current_trace->use_max_tr) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tlocal_irq_disable();\n\tarch_spin_lock(&tr->max_lock);\n\tif (tr->cond_snapshot)\n\t\tret = -EBUSY;\n\tarch_spin_unlock(&tr->max_lock);\n\tlocal_irq_enable();\n\tif (ret)\n\t\tgoto out;\n\n\tswitch (val) {\n\tcase 0:\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (tr->allocated_snapshot)\n\t\t\tfree_snapshot(tr);\n\t\tbreak;\n\tcase 1:\n \n#ifndef CONFIG_RING_BUFFER_ALLOW_SWAP\n\t\tif (iter->cpu_file != RING_BUFFER_ALL_CPUS) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tif (tr->allocated_snapshot)\n\t\t\tret = resize_buffer_duplicate_size(&tr->max_buffer,\n\t\t\t\t\t&tr->array_buffer, iter->cpu_file);\n\t\telse\n\t\t\tret = tracing_alloc_snapshot_instance(tr);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\t \n\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS) {\n\t\t\tlocal_irq_disable();\n\t\t\tupdate_max_tr(tr, current, smp_processor_id(), NULL);\n\t\t\tlocal_irq_enable();\n\t\t} else {\n\t\t\tsmp_call_function_single(iter->cpu_file, tracing_swap_cpu_buffer,\n\t\t\t\t\t\t (void *)tr, 1);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (tr->allocated_snapshot) {\n\t\t\tif (iter->cpu_file == RING_BUFFER_ALL_CPUS)\n\t\t\t\ttracing_reset_online_cpus(&tr->max_buffer);\n\t\t\telse\n\t\t\t\ttracing_reset_cpu(&tr->max_buffer, iter->cpu_file);\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (ret >= 0) {\n\t\t*ppos += cnt;\n\t\tret = cnt;\n\t}\nout:\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\n\nstatic int tracing_snapshot_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m = file->private_data;\n\tint ret;\n\n\tret = tracing_release(inode, file);\n\n\tif (file->f_mode & FMODE_READ)\n\t\treturn ret;\n\n\t \n\tif (m)\n\t\tkfree(m->private);\n\tkfree(m);\n\n\treturn 0;\n}\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp);\nstatic ssize_t tracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t\t\t    size_t count, loff_t *ppos);\nstatic int tracing_buffers_release(struct inode *inode, struct file *file);\nstatic ssize_t tracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t   struct pipe_inode_info *pipe, size_t len, unsigned int flags);\n\nstatic int snapshot_raw_open(struct inode *inode, struct file *filp)\n{\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\t \n\tret = tracing_buffers_open(inode, filp);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tinfo = filp->private_data;\n\n\tif (info->iter.trace->use_max_tr) {\n\t\ttracing_buffers_release(inode, filp);\n\t\treturn -EBUSY;\n\t}\n\n\tinfo->iter.snapshot = true;\n\tinfo->iter.array_buffer = &info->iter.tr->max_buffer;\n\n\treturn ret;\n}\n\n#endif  \n\n\nstatic const struct file_operations tracing_thresh_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_thresh_read,\n\t.write\t\t= tracing_thresh_write,\n\t.llseek\t\t= generic_file_llseek,\n};\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nstatic const struct file_operations tracing_max_lat_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_max_lat_read,\n\t.write\t\t= tracing_max_lat_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n#endif\n\nstatic const struct file_operations set_tracer_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_set_trace_read,\n\t.write\t\t= tracing_set_trace_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_pipe_fops = {\n\t.open\t\t= tracing_open_pipe,\n\t.poll\t\t= tracing_poll_pipe,\n\t.read\t\t= tracing_read_pipe,\n\t.splice_read\t= tracing_splice_read_pipe,\n\t.release\t= tracing_release_pipe,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic const struct file_operations tracing_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_entries_read,\n\t.write\t\t= tracing_entries_write,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_total_entries_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_total_entries_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_free_buffer_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.write\t\t= tracing_free_buffer_write,\n\t.release\t= tracing_free_buffer_release,\n};\n\nstatic const struct file_operations tracing_mark_fops = {\n\t.open\t\t= tracing_mark_open,\n\t.write\t\t= tracing_mark_write,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations tracing_mark_raw_fops = {\n\t.open\t\t= tracing_mark_open,\n\t.write\t\t= tracing_mark_raw_write,\n\t.release\t= tracing_release_generic_tr,\n};\n\nstatic const struct file_operations trace_clock_fops = {\n\t.open\t\t= tracing_clock_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n\t.write\t\t= tracing_clock_write,\n};\n\nstatic const struct file_operations trace_time_stamp_mode_fops = {\n\t.open\t\t= tracing_time_stamp_mode_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= tracing_single_release_tr,\n};\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nstatic const struct file_operations snapshot_fops = {\n\t.open\t\t= tracing_snapshot_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= tracing_snapshot_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= tracing_snapshot_release,\n};\n\nstatic const struct file_operations snapshot_raw_fops = {\n\t.open\t\t= snapshot_raw_open,\n\t.read\t\t= tracing_buffers_read,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.llseek\t\t= no_llseek,\n};\n\n#endif  \n\n \nstatic ssize_t\ntrace_min_max_write(struct file *filp, const char __user *ubuf, size_t cnt, loff_t *ppos)\n{\n\tstruct trace_min_max_param *param = filp->private_data;\n\tu64 val;\n\tint err;\n\n\tif (!param)\n\t\treturn -EFAULT;\n\n\terr = kstrtoull_from_user(ubuf, cnt, 10, &val);\n\tif (err)\n\t\treturn err;\n\n\tif (param->lock)\n\t\tmutex_lock(param->lock);\n\n\tif (param->min && val < *param->min)\n\t\terr = -EINVAL;\n\n\tif (param->max && val > *param->max)\n\t\terr = -EINVAL;\n\n\tif (!err)\n\t\t*param->val = val;\n\n\tif (param->lock)\n\t\tmutex_unlock(param->lock);\n\n\tif (err)\n\t\treturn err;\n\n\treturn cnt;\n}\n\n \nstatic ssize_t\ntrace_min_max_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)\n{\n\tstruct trace_min_max_param *param = filp->private_data;\n\tchar buf[U64_STR_SIZE];\n\tint len;\n\tu64 val;\n\n\tif (!param)\n\t\treturn -EFAULT;\n\n\tval = *param->val;\n\n\tif (cnt > sizeof(buf))\n\t\tcnt = sizeof(buf);\n\n\tlen = snprintf(buf, sizeof(buf), \"%llu\\n\", val);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, len);\n}\n\nconst struct file_operations trace_min_max_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= trace_min_max_read,\n\t.write\t\t= trace_min_max_write,\n};\n\n#define TRACING_LOG_ERRS_MAX\t8\n#define TRACING_LOG_LOC_MAX\t128\n\n#define CMD_PREFIX \"  Command: \"\n\nstruct err_info {\n\tconst char\t**errs;\t \n\tu8\t\ttype;\t \n\tu16\t\tpos;\t \n\tu64\t\tts;\n};\n\nstruct tracing_log_err {\n\tstruct list_head\tlist;\n\tstruct err_info\t\tinfo;\n\tchar\t\t\tloc[TRACING_LOG_LOC_MAX];  \n\tchar\t\t\t*cmd;                      \n};\n\nstatic DEFINE_MUTEX(tracing_err_log_lock);\n\nstatic struct tracing_log_err *alloc_tracing_log_err(int len)\n{\n\tstruct tracing_log_err *err;\n\n\terr = kzalloc(sizeof(*err), GFP_KERNEL);\n\tif (!err)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr->cmd = kzalloc(len, GFP_KERNEL);\n\tif (!err->cmd) {\n\t\tkfree(err);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn err;\n}\n\nstatic void free_tracing_log_err(struct tracing_log_err *err)\n{\n\tkfree(err->cmd);\n\tkfree(err);\n}\n\nstatic struct tracing_log_err *get_tracing_log_err(struct trace_array *tr,\n\t\t\t\t\t\t   int len)\n{\n\tstruct tracing_log_err *err;\n\tchar *cmd;\n\n\tif (tr->n_err_log_entries < TRACING_LOG_ERRS_MAX) {\n\t\terr = alloc_tracing_log_err(len);\n\t\tif (PTR_ERR(err) != -ENOMEM)\n\t\t\ttr->n_err_log_entries++;\n\n\t\treturn err;\n\t}\n\tcmd = kzalloc(len, GFP_KERNEL);\n\tif (!cmd)\n\t\treturn ERR_PTR(-ENOMEM);\n\terr = list_first_entry(&tr->err_log, struct tracing_log_err, list);\n\tkfree(err->cmd);\n\terr->cmd = cmd;\n\tlist_del(&err->list);\n\n\treturn err;\n}\n\n \nunsigned int err_pos(char *cmd, const char *str)\n{\n\tchar *found;\n\n\tif (WARN_ON(!strlen(cmd)))\n\t\treturn 0;\n\n\tfound = strstr(cmd, str);\n\tif (found)\n\t\treturn found - cmd;\n\n\treturn 0;\n}\n\n \nvoid tracing_log_err(struct trace_array *tr,\n\t\t     const char *loc, const char *cmd,\n\t\t     const char **errs, u8 type, u16 pos)\n{\n\tstruct tracing_log_err *err;\n\tint len = 0;\n\n\tif (!tr)\n\t\ttr = &global_trace;\n\n\tlen += sizeof(CMD_PREFIX) + 2 * sizeof(\"\\n\") + strlen(cmd) + 1;\n\n\tmutex_lock(&tracing_err_log_lock);\n\terr = get_tracing_log_err(tr, len);\n\tif (PTR_ERR(err) == -ENOMEM) {\n\t\tmutex_unlock(&tracing_err_log_lock);\n\t\treturn;\n\t}\n\n\tsnprintf(err->loc, TRACING_LOG_LOC_MAX, \"%s: error: \", loc);\n\tsnprintf(err->cmd, len, \"\\n\" CMD_PREFIX \"%s\\n\", cmd);\n\n\terr->info.errs = errs;\n\terr->info.type = type;\n\terr->info.pos = pos;\n\terr->info.ts = local_clock();\n\n\tlist_add_tail(&err->list, &tr->err_log);\n\tmutex_unlock(&tracing_err_log_lock);\n}\n\nstatic void clear_tracing_err_log(struct trace_array *tr)\n{\n\tstruct tracing_log_err *err, *next;\n\n\tmutex_lock(&tracing_err_log_lock);\n\tlist_for_each_entry_safe(err, next, &tr->err_log, list) {\n\t\tlist_del(&err->list);\n\t\tfree_tracing_log_err(err);\n\t}\n\n\ttr->n_err_log_entries = 0;\n\tmutex_unlock(&tracing_err_log_lock);\n}\n\nstatic void *tracing_err_log_seq_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&tracing_err_log_lock);\n\n\treturn seq_list_start(&tr->err_log, *pos);\n}\n\nstatic void *tracing_err_log_seq_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\n\treturn seq_list_next(v, &tr->err_log, pos);\n}\n\nstatic void tracing_err_log_seq_stop(struct seq_file *m, void *v)\n{\n\tmutex_unlock(&tracing_err_log_lock);\n}\n\nstatic void tracing_err_log_show_pos(struct seq_file *m, u16 pos)\n{\n\tu16 i;\n\n\tfor (i = 0; i < sizeof(CMD_PREFIX) - 1; i++)\n\t\tseq_putc(m, ' ');\n\tfor (i = 0; i < pos; i++)\n\t\tseq_putc(m, ' ');\n\tseq_puts(m, \"^\\n\");\n}\n\nstatic int tracing_err_log_seq_show(struct seq_file *m, void *v)\n{\n\tstruct tracing_log_err *err = v;\n\n\tif (err) {\n\t\tconst char *err_text = err->info.errs[err->info.type];\n\t\tu64 sec = err->info.ts;\n\t\tu32 nsec;\n\n\t\tnsec = do_div(sec, NSEC_PER_SEC);\n\t\tseq_printf(m, \"[%5llu.%06u] %s%s\", sec, nsec / 1000,\n\t\t\t   err->loc, err_text);\n\t\tseq_printf(m, \"%s\", err->cmd);\n\t\ttracing_err_log_show_pos(m, err->info.pos);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct seq_operations tracing_err_log_seq_ops = {\n\t.start  = tracing_err_log_seq_start,\n\t.next   = tracing_err_log_seq_next,\n\t.stop   = tracing_err_log_seq_stop,\n\t.show   = tracing_err_log_seq_show\n};\n\nstatic int tracing_err_log_open(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tint ret = 0;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif ((file->f_mode & FMODE_WRITE) && (file->f_flags & O_TRUNC))\n\t\tclear_tracing_err_log(tr);\n\n\tif (file->f_mode & FMODE_READ) {\n\t\tret = seq_open(file, &tracing_err_log_seq_ops);\n\t\tif (!ret) {\n\t\t\tstruct seq_file *m = file->private_data;\n\t\t\tm->private = tr;\n\t\t} else {\n\t\t\ttrace_array_put(tr);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic ssize_t tracing_err_log_write(struct file *file,\n\t\t\t\t     const char __user *buffer,\n\t\t\t\t     size_t count, loff_t *ppos)\n{\n\treturn count;\n}\n\nstatic int tracing_err_log_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\tif (file->f_mode & FMODE_READ)\n\t\tseq_release(inode, file);\n\n\treturn 0;\n}\n\nstatic const struct file_operations tracing_err_log_fops = {\n\t.open           = tracing_err_log_open,\n\t.write\t\t= tracing_err_log_write,\n\t.read           = seq_read,\n\t.llseek         = tracing_lseek,\n\t.release        = tracing_err_log_release,\n};\n\nstatic int tracing_buffers_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_array *tr = inode->i_private;\n\tstruct ftrace_buffer_info *info;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tinfo = kvzalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info) {\n\t\ttrace_array_put(tr);\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&trace_types_lock);\n\n\tinfo->iter.tr\t\t= tr;\n\tinfo->iter.cpu_file\t= tracing_get_cpu(inode);\n\tinfo->iter.trace\t= tr->current_trace;\n\tinfo->iter.array_buffer = &tr->array_buffer;\n\tinfo->spare\t\t= NULL;\n\t \n\tinfo->read\t\t= (unsigned int)-1;\n\n\tfilp->private_data = info;\n\n\ttr->trace_ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\n\tret = nonseekable_open(inode, filp);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\n\treturn ret;\n}\n\nstatic __poll_t\ntracing_buffers_poll(struct file *filp, poll_table *poll_table)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\treturn trace_poll(iter, filp, poll_table);\n}\n\nstatic ssize_t\ntracing_buffers_read(struct file *filp, char __user *ubuf,\n\t\t     size_t count, loff_t *ppos)\n{\n\tstruct ftrace_buffer_info *info = filp->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tssize_t ret = 0;\n\tssize_t size;\n\n\tif (!count)\n\t\treturn 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (!info->spare) {\n\t\tinfo->spare = ring_buffer_alloc_read_page(iter->array_buffer->buffer,\n\t\t\t\t\t\t\t  iter->cpu_file);\n\t\tif (IS_ERR(info->spare)) {\n\t\t\tret = PTR_ERR(info->spare);\n\t\t\tinfo->spare = NULL;\n\t\t} else {\n\t\t\tinfo->spare_cpu = iter->cpu_file;\n\t\t}\n\t}\n\tif (!info->spare)\n\t\treturn ret;\n\n\t \n\tif (info->read < PAGE_SIZE)\n\t\tgoto read;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tret = ring_buffer_read_page(iter->array_buffer->buffer,\n\t\t\t\t    &info->spare,\n\t\t\t\t    count,\n\t\t\t\t    iter->cpu_file, 0);\n\ttrace_access_unlock(iter->cpu_file);\n\n\tif (ret < 0) {\n\t\tif (trace_empty(iter)) {\n\t\t\tif ((filp->f_flags & O_NONBLOCK))\n\t\t\t\treturn -EAGAIN;\n\n\t\t\tret = wait_on_pipe(iter, 0);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tgoto again;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tinfo->read = 0;\n read:\n\tsize = PAGE_SIZE - info->read;\n\tif (size > count)\n\t\tsize = count;\n\n\tret = copy_to_user(ubuf, info->spare + info->read, size);\n\tif (ret == size)\n\t\treturn -EFAULT;\n\n\tsize -= ret;\n\n\t*ppos += size;\n\tinfo->read += size;\n\n\treturn size;\n}\n\nstatic int tracing_buffers_release(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\tmutex_lock(&trace_types_lock);\n\n\titer->tr->trace_ref--;\n\n\t__trace_array_put(iter->tr);\n\n\titer->wait_index++;\n\t \n\tsmp_wmb();\n\n\tring_buffer_wake_waiters(iter->array_buffer->buffer, iter->cpu_file);\n\n\tif (info->spare)\n\t\tring_buffer_free_read_page(iter->array_buffer->buffer,\n\t\t\t\t\t   info->spare_cpu, info->spare);\n\tkvfree(info);\n\n\tmutex_unlock(&trace_types_lock);\n\n\treturn 0;\n}\n\nstruct buffer_ref {\n\tstruct trace_buffer\t*buffer;\n\tvoid\t\t\t*page;\n\tint\t\t\tcpu;\n\trefcount_t\t\trefcount;\n};\n\nstatic void buffer_ref_release(struct buffer_ref *ref)\n{\n\tif (!refcount_dec_and_test(&ref->refcount))\n\t\treturn;\n\tring_buffer_free_read_page(ref->buffer, ref->cpu, ref->page);\n\tkfree(ref);\n}\n\nstatic void buffer_pipe_buf_release(struct pipe_inode_info *pipe,\n\t\t\t\t    struct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tbuffer_ref_release(ref);\n\tbuf->private = 0;\n}\n\nstatic bool buffer_pipe_buf_get(struct pipe_inode_info *pipe,\n\t\t\t\tstruct pipe_buffer *buf)\n{\n\tstruct buffer_ref *ref = (struct buffer_ref *)buf->private;\n\n\tif (refcount_read(&ref->refcount) > INT_MAX/2)\n\t\treturn false;\n\n\trefcount_inc(&ref->refcount);\n\treturn true;\n}\n\n \nstatic const struct pipe_buf_operations buffer_pipe_buf_ops = {\n\t.release\t\t= buffer_pipe_buf_release,\n\t.get\t\t\t= buffer_pipe_buf_get,\n};\n\n \nstatic void buffer_spd_release(struct splice_pipe_desc *spd, unsigned int i)\n{\n\tstruct buffer_ref *ref =\n\t\t(struct buffer_ref *)spd->partial[i].private;\n\n\tbuffer_ref_release(ref);\n\tspd->partial[i].private = 0;\n}\n\nstatic ssize_t\ntracing_buffers_splice_read(struct file *file, loff_t *ppos,\n\t\t\t    struct pipe_inode_info *pipe, size_t len,\n\t\t\t    unsigned int flags)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\tstruct partial_page partial_def[PIPE_DEF_BUFFERS];\n\tstruct page *pages_def[PIPE_DEF_BUFFERS];\n\tstruct splice_pipe_desc spd = {\n\t\t.pages\t\t= pages_def,\n\t\t.partial\t= partial_def,\n\t\t.nr_pages_max\t= PIPE_DEF_BUFFERS,\n\t\t.ops\t\t= &buffer_pipe_buf_ops,\n\t\t.spd_release\t= buffer_spd_release,\n\t};\n\tstruct buffer_ref *ref;\n\tint entries, i;\n\tssize_t ret = 0;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tif (iter->snapshot && iter->tr->current_trace->use_max_tr)\n\t\treturn -EBUSY;\n#endif\n\n\tif (*ppos & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tif (len & (PAGE_SIZE - 1)) {\n\t\tif (len < PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tlen &= PAGE_MASK;\n\t}\n\n\tif (splice_grow_spd(pipe, &spd))\n\t\treturn -ENOMEM;\n\n again:\n\ttrace_access_lock(iter->cpu_file);\n\tentries = ring_buffer_entries_cpu(iter->array_buffer->buffer, iter->cpu_file);\n\n\tfor (i = 0; i < spd.nr_pages_max && len && entries; i++, len -= PAGE_SIZE) {\n\t\tstruct page *page;\n\t\tint r;\n\n\t\tref = kzalloc(sizeof(*ref), GFP_KERNEL);\n\t\tif (!ref) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\trefcount_set(&ref->refcount, 1);\n\t\tref->buffer = iter->array_buffer->buffer;\n\t\tref->page = ring_buffer_alloc_read_page(ref->buffer, iter->cpu_file);\n\t\tif (IS_ERR(ref->page)) {\n\t\t\tret = PTR_ERR(ref->page);\n\t\t\tref->page = NULL;\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\t\tref->cpu = iter->cpu_file;\n\n\t\tr = ring_buffer_read_page(ref->buffer, &ref->page,\n\t\t\t\t\t  len, iter->cpu_file, 1);\n\t\tif (r < 0) {\n\t\t\tring_buffer_free_read_page(ref->buffer, ref->cpu,\n\t\t\t\t\t\t   ref->page);\n\t\t\tkfree(ref);\n\t\t\tbreak;\n\t\t}\n\n\t\tpage = virt_to_page(ref->page);\n\n\t\tspd.pages[i] = page;\n\t\tspd.partial[i].len = PAGE_SIZE;\n\t\tspd.partial[i].offset = 0;\n\t\tspd.partial[i].private = (unsigned long)ref;\n\t\tspd.nr_pages++;\n\t\t*ppos += PAGE_SIZE;\n\n\t\tentries = ring_buffer_entries_cpu(iter->array_buffer->buffer, iter->cpu_file);\n\t}\n\n\ttrace_access_unlock(iter->cpu_file);\n\tspd.nr_pages = i;\n\n\t \n\tif (!spd.nr_pages) {\n\t\tlong wait_index;\n\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = -EAGAIN;\n\t\tif ((file->f_flags & O_NONBLOCK) || (flags & SPLICE_F_NONBLOCK))\n\t\t\tgoto out;\n\n\t\twait_index = READ_ONCE(iter->wait_index);\n\n\t\tret = wait_on_pipe(iter, iter->snapshot ? 0 : iter->tr->buffer_percent);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (!tracer_tracing_is_on(iter->tr))\n\t\t\tgoto out;\n\n\t\t \n\t\tsmp_rmb();\n\t\tif (wait_index != iter->wait_index)\n\t\t\tgoto out;\n\n\t\tgoto again;\n\t}\n\n\tret = splice_to_pipe(pipe, &spd);\nout:\n\tsplice_shrink_spd(&spd);\n\n\treturn ret;\n}\n\n \nstatic long tracing_buffers_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct ftrace_buffer_info *info = file->private_data;\n\tstruct trace_iterator *iter = &info->iter;\n\n\tif (cmd)\n\t\treturn -ENOIOCTLCMD;\n\n\tmutex_lock(&trace_types_lock);\n\n\titer->wait_index++;\n\t \n\tsmp_wmb();\n\n\tring_buffer_wake_waiters(iter->array_buffer->buffer, iter->cpu_file);\n\n\tmutex_unlock(&trace_types_lock);\n\treturn 0;\n}\n\nstatic const struct file_operations tracing_buffers_fops = {\n\t.open\t\t= tracing_buffers_open,\n\t.read\t\t= tracing_buffers_read,\n\t.poll\t\t= tracing_buffers_poll,\n\t.release\t= tracing_buffers_release,\n\t.splice_read\t= tracing_buffers_splice_read,\n\t.unlocked_ioctl = tracing_buffers_ioctl,\n\t.llseek\t\t= no_llseek,\n};\n\nstatic ssize_t\ntracing_stats_read(struct file *filp, char __user *ubuf,\n\t\t   size_t count, loff_t *ppos)\n{\n\tstruct inode *inode = file_inode(filp);\n\tstruct trace_array *tr = inode->i_private;\n\tstruct array_buffer *trace_buf = &tr->array_buffer;\n\tint cpu = tracing_get_cpu(inode);\n\tstruct trace_seq *s;\n\tunsigned long cnt;\n\tunsigned long long t;\n\tunsigned long usec_rem;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tcnt = ring_buffer_entries_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"entries: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_commit_overrun_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"commit overrun: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_bytes_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"bytes: %ld\\n\", cnt);\n\n\tif (trace_clocks[tr->clock_id].in_ns) {\n\t\t \n\t\tt = ns2usecs(ring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"oldest event ts: %5llu.%06lu\\n\",\n\t\t\t\t\t\t\t\tt, usec_rem);\n\n\t\tt = ns2usecs(ring_buffer_time_stamp(trace_buf->buffer));\n\t\tusec_rem = do_div(t, USEC_PER_SEC);\n\t\ttrace_seq_printf(s, \"now ts: %5llu.%06lu\\n\", t, usec_rem);\n\t} else {\n\t\t \n\t\ttrace_seq_printf(s, \"oldest event ts: %llu\\n\",\n\t\t\t\tring_buffer_oldest_event_ts(trace_buf->buffer, cpu));\n\n\t\ttrace_seq_printf(s, \"now ts: %llu\\n\",\n\t\t\t\tring_buffer_time_stamp(trace_buf->buffer));\n\t}\n\n\tcnt = ring_buffer_dropped_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"dropped events: %ld\\n\", cnt);\n\n\tcnt = ring_buffer_read_events_cpu(trace_buf->buffer, cpu);\n\ttrace_seq_printf(s, \"read events: %ld\\n\", cnt);\n\n\tcount = simple_read_from_buffer(ubuf, count, ppos,\n\t\t\t\t\ts->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn count;\n}\n\nstatic const struct file_operations tracing_stats_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= tracing_stats_read,\n\t.llseek\t\t= generic_file_llseek,\n\t.release\t= tracing_release_generic_tr,\n};\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\nstatic ssize_t\ntracing_read_dyn_info(struct file *filp, char __user *ubuf,\n\t\t  size_t cnt, loff_t *ppos)\n{\n\tssize_t ret;\n\tchar *buf;\n\tint r;\n\n\t \n\tbuf = kmalloc(256, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tr = scnprintf(buf, 256, \"%ld pages:%ld groups: %ld\\n\",\n\t\t      ftrace_update_tot_cnt,\n\t\t      ftrace_number_of_pages,\n\t\t      ftrace_number_of_groups);\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n\tkfree(buf);\n\treturn ret;\n}\n\nstatic const struct file_operations tracing_dyn_info_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= tracing_read_dyn_info,\n\t.llseek\t\t= generic_file_llseek,\n};\n#endif  \n\n#if defined(CONFIG_TRACER_SNAPSHOT) && defined(CONFIG_DYNAMIC_FTRACE)\nstatic void\nftrace_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\tstruct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\tvoid *data)\n{\n\ttracing_snapshot_instance(tr);\n}\n\nstatic void\nftrace_count_snapshot(unsigned long ip, unsigned long parent_ip,\n\t\t      struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t      void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count) {\n\n\t\tif (*count <= 0)\n\t\t\treturn;\n\n\t\t(*count)--;\n\t}\n\n\ttracing_snapshot_instance(tr);\n}\n\nstatic int\nftrace_snapshot_print(struct seq_file *m, unsigned long ip,\n\t\t      struct ftrace_probe_ops *ops, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tlong *count = NULL;\n\n\tseq_printf(m, \"%ps:\", (void *)ip);\n\n\tseq_puts(m, \"snapshot\");\n\n\tif (mapper)\n\t\tcount = (long *)ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (count)\n\t\tseq_printf(m, \":count=%ld\\n\", *count);\n\telse\n\t\tseq_puts(m, \":unlimited\\n\");\n\n\treturn 0;\n}\n\nstatic int\nftrace_snapshot_init(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *init_data, void **data)\n{\n\tstruct ftrace_func_mapper *mapper = *data;\n\n\tif (!mapper) {\n\t\tmapper = allocate_ftrace_func_mapper();\n\t\tif (!mapper)\n\t\t\treturn -ENOMEM;\n\t\t*data = mapper;\n\t}\n\n\treturn ftrace_func_mapper_add_ip(mapper, ip, init_data);\n}\n\nstatic void\nftrace_snapshot_free(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t     unsigned long ip, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\n\tif (!ip) {\n\t\tif (!mapper)\n\t\t\treturn;\n\t\tfree_ftrace_func_mapper(mapper, NULL);\n\t\treturn;\n\t}\n\n\tftrace_func_mapper_remove_ip(mapper, ip);\n}\n\nstatic struct ftrace_probe_ops snapshot_probe_ops = {\n\t.func\t\t\t= ftrace_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n};\n\nstatic struct ftrace_probe_ops snapshot_count_probe_ops = {\n\t.func\t\t\t= ftrace_count_snapshot,\n\t.print\t\t\t= ftrace_snapshot_print,\n\t.init\t\t\t= ftrace_snapshot_init,\n\t.free\t\t\t= ftrace_snapshot_free,\n};\n\nstatic int\nftrace_trace_snapshot_callback(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t\t       char *glob, char *cmd, char *param, int enable)\n{\n\tstruct ftrace_probe_ops *ops;\n\tvoid *count = (void *)-1;\n\tchar *number;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\t \n\tif (!enable)\n\t\treturn -EINVAL;\n\n\tops = param ? &snapshot_count_probe_ops :  &snapshot_probe_ops;\n\n\tif (glob[0] == '!')\n\t\treturn unregister_ftrace_function_probe_func(glob+1, tr, ops);\n\n\tif (!param)\n\t\tgoto out_reg;\n\n\tnumber = strsep(&param, \":\");\n\n\tif (!strlen(number))\n\t\tgoto out_reg;\n\n\t \n\tret = kstrtoul(number, 0, (unsigned long *)&count);\n\tif (ret)\n\t\treturn ret;\n\n out_reg:\n\tret = tracing_alloc_snapshot_instance(tr);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = register_ftrace_function_probe(glob, tr, ops, count);\n\n out:\n\treturn ret < 0 ? ret : 0;\n}\n\nstatic struct ftrace_func_command ftrace_snapshot_cmd = {\n\t.name\t\t\t= \"snapshot\",\n\t.func\t\t\t= ftrace_trace_snapshot_callback,\n};\n\nstatic __init int register_snapshot_cmd(void)\n{\n\treturn register_ftrace_command(&ftrace_snapshot_cmd);\n}\n#else\nstatic inline __init int register_snapshot_cmd(void) { return 0; }\n#endif  \n\nstatic struct dentry *tracing_get_dentry(struct trace_array *tr)\n{\n\tif (WARN_ON(!tr->dir))\n\t\treturn ERR_PTR(-ENODEV);\n\n\t \n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL)\n\t\treturn NULL;\n\n\t \n\treturn tr->dir;\n}\n\nstatic struct dentry *tracing_dentry_percpu(struct trace_array *tr, int cpu)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->percpu_dir)\n\t\treturn tr->percpu_dir;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->percpu_dir = tracefs_create_dir(\"per_cpu\", d_tracer);\n\n\tMEM_FAIL(!tr->percpu_dir,\n\t\t  \"Could not create tracefs directory 'per_cpu/%d'\\n\", cpu);\n\n\treturn tr->percpu_dir;\n}\n\nstatic struct dentry *\ntrace_create_cpu_file(const char *name, umode_t mode, struct dentry *parent,\n\t\t      void *data, long cpu, const struct file_operations *fops)\n{\n\tstruct dentry *ret = trace_create_file(name, mode, parent, data, fops);\n\n\tif (ret)  \n\t\td_inode(ret)->i_cdev = (void *)(cpu + 1);\n\treturn ret;\n}\n\nstatic void\ntracing_init_tracefs_percpu(struct trace_array *tr, long cpu)\n{\n\tstruct dentry *d_percpu = tracing_dentry_percpu(tr, cpu);\n\tstruct dentry *d_cpu;\n\tchar cpu_dir[30];  \n\n\tif (!d_percpu)\n\t\treturn;\n\n\tsnprintf(cpu_dir, 30, \"cpu%ld\", cpu);\n\td_cpu = tracefs_create_dir(cpu_dir, d_percpu);\n\tif (!d_cpu) {\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", cpu_dir);\n\t\treturn;\n\t}\n\n\t \n\ttrace_create_cpu_file(\"trace_pipe\", TRACE_MODE_READ, d_cpu,\n\t\t\t\ttr, cpu, &tracing_pipe_fops);\n\n\t \n\ttrace_create_cpu_file(\"trace\", TRACE_MODE_WRITE, d_cpu,\n\t\t\t\ttr, cpu, &tracing_fops);\n\n\ttrace_create_cpu_file(\"trace_pipe_raw\", TRACE_MODE_READ, d_cpu,\n\t\t\t\ttr, cpu, &tracing_buffers_fops);\n\n\ttrace_create_cpu_file(\"stats\", TRACE_MODE_READ, d_cpu,\n\t\t\t\ttr, cpu, &tracing_stats_fops);\n\n\ttrace_create_cpu_file(\"buffer_size_kb\", TRACE_MODE_READ, d_cpu,\n\t\t\t\ttr, cpu, &tracing_entries_fops);\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_cpu_file(\"snapshot\", TRACE_MODE_WRITE, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_fops);\n\n\ttrace_create_cpu_file(\"snapshot_raw\", TRACE_MODE_READ, d_cpu,\n\t\t\t\ttr, cpu, &snapshot_raw_fops);\n#endif\n}\n\n#ifdef CONFIG_FTRACE_SELFTEST\n \n#include \"trace_selftest.c\"\n#endif\n\nstatic ssize_t\ntrace_options_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tchar *buf;\n\n\tif (topt->flags->val & topt->opt->bit)\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tstruct trace_option_dentry *topt = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tif (!!(topt->flags->val & topt->opt->bit) != val) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tret = __set_tracer_option(topt->tr, topt->flags,\n\t\t\t\t\t  topt->opt, !val);\n\t\tmutex_unlock(&trace_types_lock);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic int tracing_open_options(struct inode *inode, struct file *filp)\n{\n\tstruct trace_option_dentry *topt = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(topt->tr);\n\tif (ret)\n\t\treturn ret;\n\n\tfilp->private_data = inode->i_private;\n\treturn 0;\n}\n\nstatic int tracing_release_options(struct inode *inode, struct file *file)\n{\n\tstruct trace_option_dentry *topt = file->private_data;\n\n\ttrace_array_put(topt->tr);\n\treturn 0;\n}\n\nstatic const struct file_operations trace_options_fops = {\n\t.open = tracing_open_options,\n\t.read = trace_options_read,\n\t.write = trace_options_write,\n\t.llseek\t= generic_file_llseek,\n\t.release = tracing_release_options,\n};\n\n \nstatic void get_tr_index(void *data, struct trace_array **ptr,\n\t\t\t unsigned int *pindex)\n{\n\t*pindex = *(unsigned char *)data;\n\n\t*ptr = container_of(data - *pindex, struct trace_array,\n\t\t\t    trace_flags_index);\n}\n\nstatic ssize_t\ntrace_options_core_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t\tloff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tchar *buf;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tif (tr->trace_flags & (1 << index))\n\t\tbuf = \"1\\n\";\n\telse\n\t\tbuf = \"0\\n\";\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n}\n\nstatic ssize_t\ntrace_options_core_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t\t loff_t *ppos)\n{\n\tvoid *tr_index = filp->private_data;\n\tstruct trace_array *tr;\n\tunsigned int index;\n\tunsigned long val;\n\tint ret;\n\n\tget_tr_index(tr_index, &tr, &index);\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\tret = set_tracer_flag(tr, 1 << index, val);\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations trace_options_core_fops = {\n\t.open = tracing_open_generic,\n\t.read = trace_options_core_read,\n\t.write = trace_options_core_write,\n\t.llseek = generic_file_llseek,\n};\n\nstruct dentry *trace_create_file(const char *name,\n\t\t\t\t umode_t mode,\n\t\t\t\t struct dentry *parent,\n\t\t\t\t void *data,\n\t\t\t\t const struct file_operations *fops)\n{\n\tstruct dentry *ret;\n\n\tret = tracefs_create_file(name, mode, parent, data, fops);\n\tif (!ret)\n\t\tpr_warn(\"Could not create tracefs '%s' entry\\n\", name);\n\n\treturn ret;\n}\n\n\nstatic struct dentry *trace_options_init_dentry(struct trace_array *tr)\n{\n\tstruct dentry *d_tracer;\n\n\tif (tr->options)\n\t\treturn tr->options;\n\n\td_tracer = tracing_get_dentry(tr);\n\tif (IS_ERR(d_tracer))\n\t\treturn NULL;\n\n\ttr->options = tracefs_create_dir(\"options\", d_tracer);\n\tif (!tr->options) {\n\t\tpr_warn(\"Could not create tracefs directory 'options'\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn tr->options;\n}\n\nstatic void\ncreate_trace_option_file(struct trace_array *tr,\n\t\t\t struct trace_option_dentry *topt,\n\t\t\t struct tracer_flags *flags,\n\t\t\t struct tracer_opt *opt)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\ttopt->flags = flags;\n\ttopt->opt = opt;\n\ttopt->tr = tr;\n\n\ttopt->entry = trace_create_file(opt->name, TRACE_MODE_WRITE,\n\t\t\t\t\tt_options, topt, &trace_options_fops);\n\n}\n\nstatic void\ncreate_trace_option_files(struct trace_array *tr, struct tracer *tracer)\n{\n\tstruct trace_option_dentry *topts;\n\tstruct trace_options *tr_topts;\n\tstruct tracer_flags *flags;\n\tstruct tracer_opt *opts;\n\tint cnt;\n\tint i;\n\n\tif (!tracer)\n\t\treturn;\n\n\tflags = tracer->flags;\n\n\tif (!flags || !flags->opts)\n\t\treturn;\n\n\t \n\tif (!trace_ok_for_array(tracer, tr))\n\t\treturn;\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\t \n\t\tif (WARN_ON_ONCE(tr->topts[i].tracer->flags == tracer->flags))\n\t\t\treturn;\n\t}\n\n\topts = flags->opts;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++)\n\t\t;\n\n\ttopts = kcalloc(cnt + 1, sizeof(*topts), GFP_KERNEL);\n\tif (!topts)\n\t\treturn;\n\n\ttr_topts = krealloc(tr->topts, sizeof(*tr->topts) * (tr->nr_topts + 1),\n\t\t\t    GFP_KERNEL);\n\tif (!tr_topts) {\n\t\tkfree(topts);\n\t\treturn;\n\t}\n\n\ttr->topts = tr_topts;\n\ttr->topts[tr->nr_topts].tracer = tracer;\n\ttr->topts[tr->nr_topts].topts = topts;\n\ttr->nr_topts++;\n\n\tfor (cnt = 0; opts[cnt].name; cnt++) {\n\t\tcreate_trace_option_file(tr, &topts[cnt], flags,\n\t\t\t\t\t &opts[cnt]);\n\t\tMEM_FAIL(topts[cnt].entry == NULL,\n\t\t\t  \"Failed to create trace option: %s\",\n\t\t\t  opts[cnt].name);\n\t}\n}\n\nstatic struct dentry *\ncreate_trace_option_core_file(struct trace_array *tr,\n\t\t\t      const char *option, long index)\n{\n\tstruct dentry *t_options;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn NULL;\n\n\treturn trace_create_file(option, TRACE_MODE_WRITE, t_options,\n\t\t\t\t (void *)&tr->trace_flags_index[index],\n\t\t\t\t &trace_options_core_fops);\n}\n\nstatic void create_trace_options_dir(struct trace_array *tr)\n{\n\tstruct dentry *t_options;\n\tbool top_level = tr == &global_trace;\n\tint i;\n\n\tt_options = trace_options_init_dentry(tr);\n\tif (!t_options)\n\t\treturn;\n\n\tfor (i = 0; trace_options[i]; i++) {\n\t\tif (top_level ||\n\t\t    !((1 << i) & TOP_LEVEL_TRACE_FLAGS))\n\t\t\tcreate_trace_option_core_file(tr, trace_options[i], i);\n\t}\n}\n\nstatic ssize_t\nrb_simple_read(struct file *filp, char __user *ubuf,\n\t       size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tracer_tracing_is_on(tr);\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nrb_simple_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tstruct trace_buffer *buffer = tr->array_buffer.buffer;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (buffer) {\n\t\tmutex_lock(&trace_types_lock);\n\t\tif (!!val == tracer_tracing_is_on(tr)) {\n\t\t\tval = 0;  \n\t\t} else if (val) {\n\t\t\ttracer_tracing_on(tr);\n\t\t\tif (tr->current_trace->start)\n\t\t\t\ttr->current_trace->start(tr);\n\t\t} else {\n\t\t\ttracer_tracing_off(tr);\n\t\t\tif (tr->current_trace->stop)\n\t\t\t\ttr->current_trace->stop(tr);\n\t\t\t \n\t\t\tring_buffer_wake_waiters(buffer, RING_BUFFER_ALL_CPUS);\n\t\t}\n\t\tmutex_unlock(&trace_types_lock);\n\t}\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations rb_simple_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= rb_simple_read,\n\t.write\t\t= rb_simple_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstatic ssize_t\nbuffer_percent_read(struct file *filp, char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tchar buf[64];\n\tint r;\n\n\tr = tr->buffer_percent;\n\tr = sprintf(buf, \"%d\\n\", r);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic ssize_t\nbuffer_percent_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tstruct trace_array *tr = filp->private_data;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val > 100)\n\t\treturn -EINVAL;\n\n\ttr->buffer_percent = val;\n\n\t(*ppos)++;\n\n\treturn cnt;\n}\n\nstatic const struct file_operations buffer_percent_fops = {\n\t.open\t\t= tracing_open_generic_tr,\n\t.read\t\t= buffer_percent_read,\n\t.write\t\t= buffer_percent_write,\n\t.release\t= tracing_release_generic_tr,\n\t.llseek\t\t= default_llseek,\n};\n\nstatic struct dentry *trace_instance_dir;\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer);\n\nstatic int\nallocate_trace_buffer(struct trace_array *tr, struct array_buffer *buf, int size)\n{\n\tenum ring_buffer_flags rb_flags;\n\n\trb_flags = tr->trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0;\n\n\tbuf->tr = tr;\n\n\tbuf->buffer = ring_buffer_alloc(size, rb_flags);\n\tif (!buf->buffer)\n\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tset_buffer_entries(&tr->array_buffer,\n\t\t\t   ring_buffer_size(tr->array_buffer.buffer, 0));\n\n\treturn 0;\n}\n\nstatic void free_trace_buffer(struct array_buffer *buf)\n{\n\tif (buf->buffer) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\tfree_percpu(buf->data);\n\t\tbuf->data = NULL;\n\t}\n}\n\nstatic int allocate_trace_buffers(struct trace_array *tr, int size)\n{\n\tint ret;\n\n\tret = allocate_trace_buffer(tr, &tr->array_buffer, size);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tret = allocate_trace_buffer(tr, &tr->max_buffer,\n\t\t\t\t    allocate_snapshot ? size : 1);\n\tif (MEM_FAIL(ret, \"Failed to allocate trace buffer\\n\")) {\n\t\tfree_trace_buffer(&tr->array_buffer);\n\t\treturn -ENOMEM;\n\t}\n\ttr->allocated_snapshot = allocate_snapshot;\n\n\tallocate_snapshot = false;\n#endif\n\n\treturn 0;\n}\n\nstatic void free_trace_buffers(struct trace_array *tr)\n{\n\tif (!tr)\n\t\treturn;\n\n\tfree_trace_buffer(&tr->array_buffer);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tfree_trace_buffer(&tr->max_buffer);\n#endif\n}\n\nstatic void init_trace_flags_index(struct trace_array *tr)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++)\n\t\ttr->trace_flags_index[i] = i;\n}\n\nstatic void __update_tracer_options(struct trace_array *tr)\n{\n\tstruct tracer *t;\n\n\tfor (t = trace_types; t; t = t->next)\n\t\tadd_tracer_options(tr, t);\n}\n\nstatic void update_tracer_options(struct trace_array *tr)\n{\n\tmutex_lock(&trace_types_lock);\n\ttracer_options_updated = true;\n\t__update_tracer_options(tr);\n\tmutex_unlock(&trace_types_lock);\n}\n\n \nstruct trace_array *trace_array_find(const char *instance)\n{\n\tstruct trace_array *tr, *found = NULL;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, instance) == 0) {\n\t\t\tfound = tr;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn found;\n}\n\nstruct trace_array *trace_array_find_get(const char *instance)\n{\n\tstruct trace_array *tr;\n\n\tmutex_lock(&trace_types_lock);\n\ttr = trace_array_find(instance);\n\tif (tr)\n\t\ttr->ref++;\n\tmutex_unlock(&trace_types_lock);\n\n\treturn tr;\n}\n\nstatic int trace_array_create_dir(struct trace_array *tr)\n{\n\tint ret;\n\n\ttr->dir = tracefs_create_dir(tr->name, trace_instance_dir);\n\tif (!tr->dir)\n\t\treturn -EINVAL;\n\n\tret = event_trace_add_tracer(tr->dir, tr);\n\tif (ret) {\n\t\ttracefs_remove(tr->dir);\n\t\treturn ret;\n\t}\n\n\tinit_tracer_tracefs(tr, tr->dir);\n\t__update_tracer_options(tr);\n\n\treturn ret;\n}\n\nstatic struct trace_array *trace_array_create(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tret = -ENOMEM;\n\ttr = kzalloc(sizeof(*tr), GFP_KERNEL);\n\tif (!tr)\n\t\treturn ERR_PTR(ret);\n\n\ttr->name = kstrdup(name, GFP_KERNEL);\n\tif (!tr->name)\n\t\tgoto out_free_tr;\n\n\tif (!alloc_cpumask_var(&tr->tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\tif (!zalloc_cpumask_var(&tr->pipe_cpumask, GFP_KERNEL))\n\t\tgoto out_free_tr;\n\n\ttr->trace_flags = global_trace.trace_flags & ~ZEROED_TRACE_FLAGS;\n\n\tcpumask_copy(tr->tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&tr->start_lock);\n\n\ttr->max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\ttr->current_trace = &nop_trace;\n\n\tINIT_LIST_HEAD(&tr->systems);\n\tINIT_LIST_HEAD(&tr->events);\n\tINIT_LIST_HEAD(&tr->hist_vars);\n\tINIT_LIST_HEAD(&tr->err_log);\n\n\tif (allocate_trace_buffers(tr, trace_buf_size) < 0)\n\t\tgoto out_free_tr;\n\n\tif (ftrace_allocate_ftrace_ops(tr) < 0)\n\t\tgoto out_free_tr;\n\n\tftrace_init_trace_array(tr);\n\n\tinit_trace_flags_index(tr);\n\n\tif (trace_instance_dir) {\n\t\tret = trace_array_create_dir(tr);\n\t\tif (ret)\n\t\t\tgoto out_free_tr;\n\t} else\n\t\t__trace_early_add_events(tr);\n\n\tlist_add(&tr->list, &ftrace_trace_arrays);\n\n\ttr->ref++;\n\n\treturn tr;\n\n out_free_tr:\n\tftrace_free_ftrace_ops(tr);\n\tfree_trace_buffers(tr);\n\tfree_cpumask_var(tr->pipe_cpumask);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic int instance_mkdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -EEXIST;\n\tif (trace_array_find(name))\n\t\tgoto out_unlock;\n\n\ttr = trace_array_create(name);\n\n\tret = PTR_ERR_OR_ZERO(tr);\n\nout_unlock:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\treturn ret;\n}\n\n \nstruct trace_array *trace_array_get_by_name(const char *name)\n{\n\tstruct trace_array *tr;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr->name && strcmp(tr->name, name) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\ttr = trace_array_create(name);\n\n\tif (IS_ERR(tr))\n\t\ttr = NULL;\nout_unlock:\n\tif (tr)\n\t\ttr->ref++;\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\treturn tr;\n}\nEXPORT_SYMBOL_GPL(trace_array_get_by_name);\n\nstatic int __remove_instance(struct trace_array *tr)\n{\n\tint i;\n\n\t \n\tif (tr->ref > 1 || (tr->current_trace && tr->trace_ref))\n\t\treturn -EBUSY;\n\n\tlist_del(&tr->list);\n\n\t \n\tfor (i = 0; i < TRACE_FLAGS_MAX_SIZE; i++) {\n\t\tif ((1 << i) & ZEROED_TRACE_FLAGS)\n\t\t\tset_tracer_flag(tr, 1 << i, 0);\n\t}\n\n\ttracing_set_nop(tr);\n\tclear_ftrace_function_probes(tr);\n\tevent_trace_del_tracer(tr);\n\tftrace_clear_pids(tr);\n\tftrace_destroy_function_files(tr);\n\ttracefs_remove(tr->dir);\n\tfree_percpu(tr->last_func_repeats);\n\tfree_trace_buffers(tr);\n\tclear_tracing_err_log(tr);\n\n\tfor (i = 0; i < tr->nr_topts; i++) {\n\t\tkfree(tr->topts[i].topts);\n\t}\n\tkfree(tr->topts);\n\n\tfree_cpumask_var(tr->pipe_cpumask);\n\tfree_cpumask_var(tr->tracing_cpumask);\n\tkfree(tr->name);\n\tkfree(tr);\n\n\treturn 0;\n}\n\nint trace_array_destroy(struct trace_array *this_tr)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tif (!this_tr)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\n\t \n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (tr == this_tr) {\n\t\t\tret = __remove_instance(tr);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(trace_array_destroy);\n\nstatic int instance_rmdir(const char *name)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tret = -ENODEV;\n\ttr = trace_array_find(name);\n\tif (tr)\n\t\tret = __remove_instance(tr);\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nstatic __init void create_trace_instances(struct dentry *d_tracer)\n{\n\tstruct trace_array *tr;\n\n\ttrace_instance_dir = tracefs_create_instance_dir(\"instances\", d_tracer,\n\t\t\t\t\t\t\t instance_mkdir,\n\t\t\t\t\t\t\t instance_rmdir);\n\tif (MEM_FAIL(!trace_instance_dir, \"Failed to create instances directory\\n\"))\n\t\treturn;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->name)\n\t\t\tcontinue;\n\t\tif (MEM_FAIL(trace_array_create_dir(tr) < 0,\n\t\t\t     \"Failed to create instance directory\\n\"))\n\t\t\tbreak;\n\t}\n\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void\ninit_tracer_tracefs(struct trace_array *tr, struct dentry *d_tracer)\n{\n\tstruct trace_event_file *file;\n\tint cpu;\n\n\ttrace_create_file(\"available_tracers\", TRACE_MODE_READ, d_tracer,\n\t\t\ttr, &show_traces_fops);\n\n\ttrace_create_file(\"current_tracer\", TRACE_MODE_WRITE, d_tracer,\n\t\t\ttr, &set_tracer_fops);\n\n\ttrace_create_file(\"tracing_cpumask\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &tracing_cpumask_fops);\n\n\ttrace_create_file(\"trace_options\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &tracing_iter_fops);\n\n\ttrace_create_file(\"trace\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &tracing_fops);\n\n\ttrace_create_file(\"trace_pipe\", TRACE_MODE_READ, d_tracer,\n\t\t\t  tr, &tracing_pipe_fops);\n\n\ttrace_create_file(\"buffer_size_kb\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &tracing_entries_fops);\n\n\ttrace_create_file(\"buffer_total_size_kb\", TRACE_MODE_READ, d_tracer,\n\t\t\t  tr, &tracing_total_entries_fops);\n\n\ttrace_create_file(\"free_buffer\", 0200, d_tracer,\n\t\t\t  tr, &tracing_free_buffer_fops);\n\n\ttrace_create_file(\"trace_marker\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_fops);\n\n\tfile = __find_event_file(tr, \"ftrace\", \"print\");\n\tif (file && file->ef)\n\t\teventfs_add_file(\"trigger\", TRACE_MODE_WRITE, file->ef,\n\t\t\t\t  file, &event_trigger_fops);\n\ttr->trace_marker_file = file;\n\n\ttrace_create_file(\"trace_marker_raw\", 0220, d_tracer,\n\t\t\t  tr, &tracing_mark_raw_fops);\n\n\ttrace_create_file(\"trace_clock\", TRACE_MODE_WRITE, d_tracer, tr,\n\t\t\t  &trace_clock_fops);\n\n\ttrace_create_file(\"tracing_on\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &rb_simple_fops);\n\n\ttrace_create_file(\"timestamp_mode\", TRACE_MODE_READ, d_tracer, tr,\n\t\t\t  &trace_time_stamp_mode_fops);\n\n\ttr->buffer_percent = 50;\n\n\ttrace_create_file(\"buffer_percent\", TRACE_MODE_WRITE, d_tracer,\n\t\t\ttr, &buffer_percent_fops);\n\n\tcreate_trace_options_dir(tr);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n\ttrace_create_maxlat_file(tr, d_tracer);\n#endif\n\n\tif (ftrace_create_function_files(tr, d_tracer))\n\t\tMEM_FAIL(1, \"Could not allocate function filter files\");\n\n#ifdef CONFIG_TRACER_SNAPSHOT\n\ttrace_create_file(\"snapshot\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &snapshot_fops);\n#endif\n\n\ttrace_create_file(\"error_log\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t  tr, &tracing_err_log_fops);\n\n\tfor_each_tracing_cpu(cpu)\n\t\ttracing_init_tracefs_percpu(tr, cpu);\n\n\tftrace_init_tracefs(tr, d_tracer);\n}\n\nstatic struct vfsmount *trace_automount(struct dentry *mntpt, void *ingore)\n{\n\tstruct vfsmount *mnt;\n\tstruct file_system_type *type;\n\n\t \n\ttype = get_fs_type(\"tracefs\");\n\tif (!type)\n\t\treturn NULL;\n\tmnt = vfs_submount(mntpt, type, \"tracefs\", NULL);\n\tput_filesystem(type);\n\tif (IS_ERR(mnt))\n\t\treturn NULL;\n\tmntget(mnt);\n\n\treturn mnt;\n}\n\n \nint tracing_init_dentry(void)\n{\n\tstruct trace_array *tr = &global_trace;\n\n\tif (security_locked_down(LOCKDOWN_TRACEFS)) {\n\t\tpr_warn(\"Tracing disabled due to lockdown\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t \n\tif (tr->dir)\n\t\treturn 0;\n\n\tif (WARN_ON(!tracefs_initialized()))\n\t\treturn -ENODEV;\n\n\t \n\ttr->dir = debugfs_create_automount(\"tracing\", NULL,\n\t\t\t\t\t   trace_automount, NULL);\n\n\treturn 0;\n}\n\nextern struct trace_eval_map *__start_ftrace_eval_maps[];\nextern struct trace_eval_map *__stop_ftrace_eval_maps[];\n\nstatic struct workqueue_struct *eval_map_wq __initdata;\nstatic struct work_struct eval_map_work __initdata;\nstatic struct work_struct tracerfs_init_work __initdata;\n\nstatic void __init eval_map_work_func(struct work_struct *work)\n{\n\tint len;\n\n\tlen = __stop_ftrace_eval_maps - __start_ftrace_eval_maps;\n\ttrace_insert_eval_map(NULL, __start_ftrace_eval_maps, len);\n}\n\nstatic int __init trace_eval_init(void)\n{\n\tINIT_WORK(&eval_map_work, eval_map_work_func);\n\n\teval_map_wq = alloc_workqueue(\"eval_map_wq\", WQ_UNBOUND, 0);\n\tif (!eval_map_wq) {\n\t\tpr_err(\"Unable to allocate eval_map_wq\\n\");\n\t\t \n\t\teval_map_work_func(&eval_map_work);\n\t\treturn -ENOMEM;\n\t}\n\n\tqueue_work(eval_map_wq, &eval_map_work);\n\treturn 0;\n}\n\nsubsys_initcall(trace_eval_init);\n\nstatic int __init trace_eval_sync(void)\n{\n\t \n\tif (eval_map_wq)\n\t\tdestroy_workqueue(eval_map_wq);\n\treturn 0;\n}\n\nlate_initcall_sync(trace_eval_sync);\n\n\n#ifdef CONFIG_MODULES\nstatic void trace_module_add_evals(struct module *mod)\n{\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\t \n\tif (trace_module_has_bad_taint(mod))\n\t\treturn;\n\n\ttrace_insert_eval_map(mod, mod->trace_evals, mod->num_trace_evals);\n}\n\n#ifdef CONFIG_TRACE_EVAL_MAP_FILE\nstatic void trace_module_remove_evals(struct module *mod)\n{\n\tunion trace_eval_map_item *map;\n\tunion trace_eval_map_item **last = &trace_eval_maps;\n\n\tif (!mod->num_trace_evals)\n\t\treturn;\n\n\tmutex_lock(&trace_eval_mutex);\n\n\tmap = trace_eval_maps;\n\n\twhile (map) {\n\t\tif (map->head.mod == mod)\n\t\t\tbreak;\n\t\tmap = trace_eval_jmp_to_tail(map);\n\t\tlast = &map->tail.next;\n\t\tmap = map->tail.next;\n\t}\n\tif (!map)\n\t\tgoto out;\n\n\t*last = trace_eval_jmp_to_tail(map)->tail.next;\n\tkfree(map);\n out:\n\tmutex_unlock(&trace_eval_mutex);\n}\n#else\nstatic inline void trace_module_remove_evals(struct module *mod) { }\n#endif  \n\nstatic int trace_module_notify(struct notifier_block *self,\n\t\t\t       unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\ttrace_module_add_evals(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\ttrace_module_remove_evals(mod);\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_module_nb = {\n\t.notifier_call = trace_module_notify,\n\t.priority = 0,\n};\n#endif  \n\nstatic __init void tracer_init_tracefs_work_func(struct work_struct *work)\n{\n\n\tevent_trace_init();\n\n\tinit_tracer_tracefs(&global_trace, NULL);\n\tftrace_init_tracefs_toplevel(&global_trace, NULL);\n\n\ttrace_create_file(\"tracing_thresh\", TRACE_MODE_WRITE, NULL,\n\t\t\t&global_trace, &tracing_thresh_fops);\n\n\ttrace_create_file(\"README\", TRACE_MODE_READ, NULL,\n\t\t\tNULL, &tracing_readme_fops);\n\n\ttrace_create_file(\"saved_cmdlines\", TRACE_MODE_READ, NULL,\n\t\t\tNULL, &tracing_saved_cmdlines_fops);\n\n\ttrace_create_file(\"saved_cmdlines_size\", TRACE_MODE_WRITE, NULL,\n\t\t\t  NULL, &tracing_saved_cmdlines_size_fops);\n\n\ttrace_create_file(\"saved_tgids\", TRACE_MODE_READ, NULL,\n\t\t\tNULL, &tracing_saved_tgids_fops);\n\n\ttrace_create_eval_file(NULL);\n\n#ifdef CONFIG_MODULES\n\tregister_module_notifier(&trace_module_nb);\n#endif\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\ttrace_create_file(\"dyn_ftrace_total_info\", TRACE_MODE_READ, NULL,\n\t\t\tNULL, &tracing_dyn_info_fops);\n#endif\n\n\tcreate_trace_instances(NULL);\n\n\tupdate_tracer_options(&global_trace);\n}\n\nstatic __init int tracer_init_tracefs(void)\n{\n\tint ret;\n\n\ttrace_access_lock_init();\n\n\tret = tracing_init_dentry();\n\tif (ret)\n\t\treturn 0;\n\n\tif (eval_map_wq) {\n\t\tINIT_WORK(&tracerfs_init_work, tracer_init_tracefs_work_func);\n\t\tqueue_work(eval_map_wq, &tracerfs_init_work);\n\t} else {\n\t\ttracer_init_tracefs_work_func(NULL);\n\t}\n\n\trv_init_interface();\n\n\treturn 0;\n}\n\nfs_initcall(tracer_init_tracefs);\n\nstatic int trace_die_panic_handler(struct notifier_block *self,\n\t\t\t\tunsigned long ev, void *unused);\n\nstatic struct notifier_block trace_panic_notifier = {\n\t.notifier_call = trace_die_panic_handler,\n\t.priority = INT_MAX - 1,\n};\n\nstatic struct notifier_block trace_die_notifier = {\n\t.notifier_call = trace_die_panic_handler,\n\t.priority = INT_MAX - 1,\n};\n\n \nstatic int trace_die_panic_handler(struct notifier_block *self,\n\t\t\t\tunsigned long ev, void *unused)\n{\n\tif (!ftrace_dump_on_oops)\n\t\treturn NOTIFY_DONE;\n\n\t \n\tif (self == &trace_die_notifier && ev != DIE_OOPS)\n\t\treturn NOTIFY_DONE;\n\n\tftrace_dump(ftrace_dump_on_oops);\n\n\treturn NOTIFY_DONE;\n}\n\n \n#define TRACE_MAX_PRINT\t\t1000\n\n \n#define KERN_TRACE\t\tKERN_EMERG\n\nvoid\ntrace_printk_seq(struct trace_seq *s)\n{\n\t \n\tif (s->seq.len >= TRACE_MAX_PRINT)\n\t\ts->seq.len = TRACE_MAX_PRINT;\n\n\t \n\tif (WARN_ON_ONCE(s->seq.len >= s->seq.size))\n\t\ts->seq.len = s->seq.size - 1;\n\n\t \n\ts->buffer[s->seq.len] = 0;\n\n\tprintk(KERN_TRACE \"%s\", s->buffer);\n\n\ttrace_seq_init(s);\n}\n\nvoid trace_init_global_iter(struct trace_iterator *iter)\n{\n\titer->tr = &global_trace;\n\titer->trace = iter->tr->current_trace;\n\titer->cpu_file = RING_BUFFER_ALL_CPUS;\n\titer->array_buffer = &global_trace.array_buffer;\n\n\tif (iter->trace && iter->trace->open)\n\t\titer->trace->open(iter);\n\n\t \n\tif (ring_buffer_overruns(iter->array_buffer->buffer))\n\t\titer->iter_flags |= TRACE_FILE_ANNOTATE;\n\n\t \n\tif (trace_clocks[iter->tr->clock_id].in_ns)\n\t\titer->iter_flags |= TRACE_FILE_TIME_IN_NS;\n\n\t \n\titer->temp = static_temp_buf;\n\titer->temp_size = STATIC_TEMP_BUF_SIZE;\n\titer->fmt = static_fmt_buf;\n\titer->fmt_size = STATIC_FMT_BUF_SIZE;\n}\n\nvoid ftrace_dump(enum ftrace_dump_mode oops_dump_mode)\n{\n\t \n\tstatic struct trace_iterator iter;\n\tstatic atomic_t dump_running;\n\tstruct trace_array *tr = &global_trace;\n\tunsigned int old_userobj;\n\tunsigned long flags;\n\tint cnt = 0, cpu;\n\n\t \n\tif (atomic_inc_return(&dump_running) != 1) {\n\t\tatomic_dec(&dump_running);\n\t\treturn;\n\t}\n\n\t \n\ttracing_off();\n\n\tlocal_irq_save(flags);\n\n\t \n\ttrace_init_global_iter(&iter);\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_inc(&per_cpu_ptr(iter.array_buffer->data, cpu)->disabled);\n\t}\n\n\told_userobj = tr->trace_flags & TRACE_ITER_SYM_USEROBJ;\n\n\t \n\ttr->trace_flags &= ~TRACE_ITER_SYM_USEROBJ;\n\n\tswitch (oops_dump_mode) {\n\tcase DUMP_ALL:\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t\tbreak;\n\tcase DUMP_ORIG:\n\t\titer.cpu_file = raw_smp_processor_id();\n\t\tbreak;\n\tcase DUMP_NONE:\n\t\tgoto out_enable;\n\tdefault:\n\t\tprintk(KERN_TRACE \"Bad dumping mode, switching to all CPUs dump\\n\");\n\t\titer.cpu_file = RING_BUFFER_ALL_CPUS;\n\t}\n\n\tprintk(KERN_TRACE \"Dumping ftrace buffer:\\n\");\n\n\t \n\tif (ftrace_is_dead()) {\n\t\tprintk(\"# WARNING: FUNCTION TRACING IS CORRUPTED\\n\");\n\t\tprintk(\"#          MAY BE MISSING FUNCTION EVENTS\\n\");\n\t}\n\n\t \n\n\twhile (!trace_empty(&iter)) {\n\n\t\tif (!cnt)\n\t\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n\t\tcnt++;\n\n\t\ttrace_iterator_reset(&iter);\n\t\titer.iter_flags |= TRACE_FILE_LAT_FMT;\n\n\t\tif (trace_find_next_entry_inc(&iter) != NULL) {\n\t\t\tint ret;\n\n\t\t\tret = print_trace_line(&iter);\n\t\t\tif (ret != TRACE_TYPE_NO_CONSUME)\n\t\t\t\ttrace_consume(&iter);\n\t\t}\n\t\ttouch_nmi_watchdog();\n\n\t\ttrace_printk_seq(&iter.seq);\n\t}\n\n\tif (!cnt)\n\t\tprintk(KERN_TRACE \"   (ftrace buffer empty)\\n\");\n\telse\n\t\tprintk(KERN_TRACE \"---------------------------------\\n\");\n\n out_enable:\n\ttr->trace_flags |= old_userobj;\n\n\tfor_each_tracing_cpu(cpu) {\n\t\tatomic_dec(&per_cpu_ptr(iter.array_buffer->data, cpu)->disabled);\n\t}\n\tatomic_dec(&dump_running);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(ftrace_dump);\n\n#define WRITE_BUFSIZE  4096\n\nssize_t trace_parse_run_command(struct file *file, const char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos,\n\t\t\t\tint (*createfn)(const char *))\n{\n\tchar *kbuf, *buf, *tmp;\n\tint ret = 0;\n\tsize_t done = 0;\n\tsize_t size;\n\n\tkbuf = kmalloc(WRITE_BUFSIZE, GFP_KERNEL);\n\tif (!kbuf)\n\t\treturn -ENOMEM;\n\n\twhile (done < count) {\n\t\tsize = count - done;\n\n\t\tif (size >= WRITE_BUFSIZE)\n\t\t\tsize = WRITE_BUFSIZE - 1;\n\n\t\tif (copy_from_user(kbuf, buffer + done, size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t\tkbuf[size] = '\\0';\n\t\tbuf = kbuf;\n\t\tdo {\n\t\t\ttmp = strchr(buf, '\\n');\n\t\t\tif (tmp) {\n\t\t\t\t*tmp = '\\0';\n\t\t\t\tsize = tmp - buf + 1;\n\t\t\t} else {\n\t\t\t\tsize = strlen(buf);\n\t\t\t\tif (done + size < count) {\n\t\t\t\t\tif (buf != kbuf)\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t \n\t\t\t\t\tpr_warn(\"Line length is too long: Should be less than %d\\n\",\n\t\t\t\t\t\tWRITE_BUFSIZE - 2);\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tdone += size;\n\n\t\t\t \n\t\t\ttmp = strchr(buf, '#');\n\n\t\t\tif (tmp)\n\t\t\t\t*tmp = '\\0';\n\n\t\t\tret = createfn(buf);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbuf += size;\n\n\t\t} while (done < count);\n\t}\n\tret = done;\n\nout:\n\tkfree(kbuf);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_TRACER_MAX_TRACE\n__init static bool tr_needs_alloc_snapshot(const char *name)\n{\n\tchar *test;\n\tint len = strlen(name);\n\tbool ret;\n\n\tif (!boot_snapshot_index)\n\t\treturn false;\n\n\tif (strncmp(name, boot_snapshot_info, len) == 0 &&\n\t    boot_snapshot_info[len] == '\\t')\n\t\treturn true;\n\n\ttest = kmalloc(strlen(name) + 3, GFP_KERNEL);\n\tif (!test)\n\t\treturn false;\n\n\tsprintf(test, \"\\t%s\\t\", name);\n\tret = strstr(boot_snapshot_info, test) == NULL;\n\tkfree(test);\n\treturn ret;\n}\n\n__init static void do_allocate_snapshot(const char *name)\n{\n\tif (!tr_needs_alloc_snapshot(name))\n\t\treturn;\n\n\t \n\tallocate_snapshot = true;\n}\n#else\nstatic inline void do_allocate_snapshot(const char *name) { }\n#endif\n\n__init static void enable_instances(void)\n{\n\tstruct trace_array *tr;\n\tchar *curr_str;\n\tchar *str;\n\tchar *tok;\n\n\t \n\tboot_instance_info[boot_instance_index - 1] = '\\0';\n\tstr = boot_instance_info;\n\n\twhile ((curr_str = strsep(&str, \"\\t\"))) {\n\n\t\ttok = strsep(&curr_str, \",\");\n\n\t\tif (IS_ENABLED(CONFIG_TRACER_MAX_TRACE))\n\t\t\tdo_allocate_snapshot(tok);\n\n\t\ttr = trace_array_get_by_name(tok);\n\t\tif (!tr) {\n\t\t\tpr_warn(\"Failed to create instance buffer %s\\n\", curr_str);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\ttrace_array_put(tr);\n\n\t\twhile ((tok = strsep(&curr_str, \",\"))) {\n\t\t\tearly_enable_events(tr, tok, true);\n\t\t}\n\t}\n}\n\n__init static int tracer_alloc_buffers(void)\n{\n\tint ring_buf_size;\n\tint ret = -ENOMEM;\n\n\n\tif (security_locked_down(LOCKDOWN_TRACEFS)) {\n\t\tpr_warn(\"Tracing disabled due to lockdown\\n\");\n\t\treturn -EPERM;\n\t}\n\n\t \n\tBUILD_BUG_ON(TRACE_ITER_LAST_BIT > TRACE_FLAGS_MAX_SIZE);\n\n\tif (!alloc_cpumask_var(&tracing_buffer_mask, GFP_KERNEL))\n\t\tgoto out;\n\n\tif (!alloc_cpumask_var(&global_trace.tracing_cpumask, GFP_KERNEL))\n\t\tgoto out_free_buffer_mask;\n\n\t \n\tif (&__stop___trace_bprintk_fmt != &__start___trace_bprintk_fmt)\n\t\t \n\t\ttrace_printk_init_buffers();\n\n\t \n\tif (ring_buffer_expanded)\n\t\tring_buf_size = trace_buf_size;\n\telse\n\t\tring_buf_size = 1;\n\n\tcpumask_copy(tracing_buffer_mask, cpu_possible_mask);\n\tcpumask_copy(global_trace.tracing_cpumask, cpu_all_mask);\n\n\traw_spin_lock_init(&global_trace.start_lock);\n\n\t \n\tret = cpuhp_setup_state_multi(CPUHP_TRACE_RB_PREPARE,\n\t\t\t\t      \"trace/RB:prepare\", trace_rb_cpu_prepare,\n\t\t\t\t      NULL);\n\tif (ret < 0)\n\t\tgoto out_free_cpumask;\n\t \n\tret = -ENOMEM;\n\ttemp_buffer = ring_buffer_alloc(PAGE_SIZE, RB_FL_OVERWRITE);\n\tif (!temp_buffer)\n\t\tgoto out_rm_hp_state;\n\n\tif (trace_create_savedcmd() < 0)\n\t\tgoto out_free_temp_buffer;\n\n\tif (!zalloc_cpumask_var(&global_trace.pipe_cpumask, GFP_KERNEL))\n\t\tgoto out_free_savedcmd;\n\n\t \n\tif (allocate_trace_buffers(&global_trace, ring_buf_size) < 0) {\n\t\tMEM_FAIL(1, \"tracer: failed to allocate ring buffer!\\n\");\n\t\tgoto out_free_pipe_cpumask;\n\t}\n\tif (global_trace.buffer_disabled)\n\t\ttracing_off();\n\n\tif (trace_boot_clock) {\n\t\tret = tracing_set_clock(&global_trace, trace_boot_clock);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Trace clock %s not defined, going back to default\\n\",\n\t\t\t\ttrace_boot_clock);\n\t}\n\n\t \n\tglobal_trace.current_trace = &nop_trace;\n\n\tglobal_trace.max_lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;\n\n\tftrace_init_global_array_ops(&global_trace);\n\n\tinit_trace_flags_index(&global_trace);\n\n\tregister_tracer(&nop_trace);\n\n\t \n\tinit_function_trace();\n\n\t \n\ttracing_disabled = 0;\n\n\tatomic_notifier_chain_register(&panic_notifier_list,\n\t\t\t\t       &trace_panic_notifier);\n\n\tregister_die_notifier(&trace_die_notifier);\n\n\tglobal_trace.flags = TRACE_ARRAY_FL_GLOBAL;\n\n\tINIT_LIST_HEAD(&global_trace.systems);\n\tINIT_LIST_HEAD(&global_trace.events);\n\tINIT_LIST_HEAD(&global_trace.hist_vars);\n\tINIT_LIST_HEAD(&global_trace.err_log);\n\tlist_add(&global_trace.list, &ftrace_trace_arrays);\n\n\tapply_trace_boot_options();\n\n\tregister_snapshot_cmd();\n\n\ttest_can_verify();\n\n\treturn 0;\n\nout_free_pipe_cpumask:\n\tfree_cpumask_var(global_trace.pipe_cpumask);\nout_free_savedcmd:\n\tfree_saved_cmdlines_buffer(savedcmd);\nout_free_temp_buffer:\n\tring_buffer_free(temp_buffer);\nout_rm_hp_state:\n\tcpuhp_remove_multi_state(CPUHP_TRACE_RB_PREPARE);\nout_free_cpumask:\n\tfree_cpumask_var(global_trace.tracing_cpumask);\nout_free_buffer_mask:\n\tfree_cpumask_var(tracing_buffer_mask);\nout:\n\treturn ret;\n}\n\nvoid __init ftrace_boot_snapshot(void)\n{\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tstruct trace_array *tr;\n\n\tif (!snapshot_at_boot)\n\t\treturn;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->allocated_snapshot)\n\t\t\tcontinue;\n\n\t\ttracing_snapshot_instance(tr);\n\t\ttrace_array_puts(tr, \"** Boot snapshot taken **\\n\");\n\t}\n#endif\n}\n\nvoid __init early_trace_init(void)\n{\n\tif (tracepoint_printk) {\n\t\ttracepoint_print_iter =\n\t\t\tkzalloc(sizeof(*tracepoint_print_iter), GFP_KERNEL);\n\t\tif (MEM_FAIL(!tracepoint_print_iter,\n\t\t\t     \"Failed to allocate trace iterator\\n\"))\n\t\t\ttracepoint_printk = 0;\n\t\telse\n\t\t\tstatic_key_enable(&tracepoint_printk_key.key);\n\t}\n\ttracer_alloc_buffers();\n\n\tinit_events();\n}\n\nvoid __init trace_init(void)\n{\n\ttrace_event_init();\n\n\tif (boot_instance_index)\n\t\tenable_instances();\n}\n\n__init static void clear_boot_tracer(void)\n{\n\t \n\tif (!default_bootup_tracer)\n\t\treturn;\n\n\tprintk(KERN_INFO \"ftrace bootup tracer '%s' not registered.\\n\",\n\t       default_bootup_tracer);\n\tdefault_bootup_tracer = NULL;\n}\n\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\n__init static void tracing_set_default_clock(void)\n{\n\t \n\tif (!trace_boot_clock && !sched_clock_stable()) {\n\t\tif (security_locked_down(LOCKDOWN_TRACEFS)) {\n\t\t\tpr_warn(\"Can not set tracing clock due to lockdown\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\tprintk(KERN_WARNING\n\t\t       \"Unstable clock detected, switching default tracing clock to \\\"global\\\"\\n\"\n\t\t       \"If you want to keep using the local clock, then add:\\n\"\n\t\t       \"  \\\"trace_clock=local\\\"\\n\"\n\t\t       \"on the kernel command line\\n\");\n\t\ttracing_set_clock(&global_trace, \"global\");\n\t}\n}\n#else\nstatic inline void tracing_set_default_clock(void) { }\n#endif\n\n__init static int late_trace_init(void)\n{\n\tif (tracepoint_printk && tracepoint_printk_stop_on_boot) {\n\t\tstatic_key_disable(&tracepoint_printk_key.key);\n\t\ttracepoint_printk = 0;\n\t}\n\n\ttracing_set_default_clock();\n\tclear_boot_tracer();\n\treturn 0;\n}\n\nlate_initcall_sync(late_trace_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}