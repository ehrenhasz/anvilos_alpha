{
  "module_name": "trace_events.c",
  "hash_id": "6fada9a343e3e6f23f53955ad3a6e1adadca855e474a037545f1cad6442f7936",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace_events.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) fmt\n\n#include <linux/workqueue.h>\n#include <linux/security.h>\n#include <linux/spinlock.h>\n#include <linux/kthread.h>\n#include <linux/tracefs.h>\n#include <linux/uaccess.h>\n#include <linux/module.h>\n#include <linux/ctype.h>\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n\n#include <trace/events/sched.h>\n#include <trace/syscall.h>\n\n#include <asm/setup.h>\n\n#include \"trace_output.h\"\n\n#undef TRACE_SYSTEM\n#define TRACE_SYSTEM \"TRACE_SYSTEM\"\n\nDEFINE_MUTEX(event_mutex);\n\nLIST_HEAD(ftrace_events);\nstatic LIST_HEAD(ftrace_generic_fields);\nstatic LIST_HEAD(ftrace_common_fields);\nstatic bool eventdir_initialized;\n\nstatic LIST_HEAD(module_strings);\n\nstruct module_string {\n\tstruct list_head\tnext;\n\tstruct module\t\t*module;\n\tchar\t\t\t*str;\n};\n\n#define GFP_TRACE (GFP_KERNEL | __GFP_ZERO)\n\nstatic struct kmem_cache *field_cachep;\nstatic struct kmem_cache *file_cachep;\n\nstatic inline int system_refcount(struct event_subsystem *system)\n{\n\treturn system->ref_count;\n}\n\nstatic int system_refcount_inc(struct event_subsystem *system)\n{\n\treturn system->ref_count++;\n}\n\nstatic int system_refcount_dec(struct event_subsystem *system)\n{\n\treturn --system->ref_count;\n}\n\n \n#define do_for_each_event_file(tr, file)\t\t\t\\\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\t\\\n\t\tlist_for_each_entry(file, &tr->events, list)\n\n#define do_for_each_event_file_safe(tr, file)\t\t\t\\\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\t\\\n\t\tstruct trace_event_file *___n;\t\t\t\t\\\n\t\tlist_for_each_entry_safe(file, ___n, &tr->events, list)\n\n#define while_for_each_event_file()\t\t\\\n\t}\n\nstatic struct ftrace_event_field *\n__find_event_field(struct list_head *head, char *name)\n{\n\tstruct ftrace_event_field *field;\n\n\tlist_for_each_entry(field, head, link) {\n\t\tif (!strcmp(field->name, name))\n\t\t\treturn field;\n\t}\n\n\treturn NULL;\n}\n\nstruct ftrace_event_field *\ntrace_find_event_field(struct trace_event_call *call, char *name)\n{\n\tstruct ftrace_event_field *field;\n\tstruct list_head *head;\n\n\thead = trace_get_fields(call);\n\tfield = __find_event_field(head, name);\n\tif (field)\n\t\treturn field;\n\n\tfield = __find_event_field(&ftrace_generic_fields, name);\n\tif (field)\n\t\treturn field;\n\n\treturn __find_event_field(&ftrace_common_fields, name);\n}\n\nstatic int __trace_define_field(struct list_head *head, const char *type,\n\t\t\t\tconst char *name, int offset, int size,\n\t\t\t\tint is_signed, int filter_type, int len)\n{\n\tstruct ftrace_event_field *field;\n\n\tfield = kmem_cache_alloc(field_cachep, GFP_TRACE);\n\tif (!field)\n\t\treturn -ENOMEM;\n\n\tfield->name = name;\n\tfield->type = type;\n\n\tif (filter_type == FILTER_OTHER)\n\t\tfield->filter_type = filter_assign_type(type);\n\telse\n\t\tfield->filter_type = filter_type;\n\n\tfield->offset = offset;\n\tfield->size = size;\n\tfield->is_signed = is_signed;\n\tfield->len = len;\n\n\tlist_add(&field->link, head);\n\n\treturn 0;\n}\n\nint trace_define_field(struct trace_event_call *call, const char *type,\n\t\t       const char *name, int offset, int size, int is_signed,\n\t\t       int filter_type)\n{\n\tstruct list_head *head;\n\n\tif (WARN_ON(!call->class))\n\t\treturn 0;\n\n\thead = trace_get_fields(call);\n\treturn __trace_define_field(head, type, name, offset, size,\n\t\t\t\t    is_signed, filter_type, 0);\n}\nEXPORT_SYMBOL_GPL(trace_define_field);\n\nstatic int trace_define_field_ext(struct trace_event_call *call, const char *type,\n\t\t       const char *name, int offset, int size, int is_signed,\n\t\t       int filter_type, int len)\n{\n\tstruct list_head *head;\n\n\tif (WARN_ON(!call->class))\n\t\treturn 0;\n\n\thead = trace_get_fields(call);\n\treturn __trace_define_field(head, type, name, offset, size,\n\t\t\t\t    is_signed, filter_type, len);\n}\n\n#define __generic_field(type, item, filter_type)\t\t\t\\\n\tret = __trace_define_field(&ftrace_generic_fields, #type,\t\\\n\t\t\t\t   #item, 0, 0, is_signed_type(type),\t\\\n\t\t\t\t   filter_type, 0);\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\n\n#define __common_field(type, item)\t\t\t\t\t\\\n\tret = __trace_define_field(&ftrace_common_fields, #type,\t\\\n\t\t\t\t   \"common_\" #item,\t\t\t\\\n\t\t\t\t   offsetof(typeof(ent), item),\t\t\\\n\t\t\t\t   sizeof(ent.item),\t\t\t\\\n\t\t\t\t   is_signed_type(type), FILTER_OTHER, 0);\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\n\nstatic int trace_define_generic_fields(void)\n{\n\tint ret;\n\n\t__generic_field(int, CPU, FILTER_CPU);\n\t__generic_field(int, cpu, FILTER_CPU);\n\t__generic_field(int, common_cpu, FILTER_CPU);\n\t__generic_field(char *, COMM, FILTER_COMM);\n\t__generic_field(char *, comm, FILTER_COMM);\n\t__generic_field(char *, stacktrace, FILTER_STACKTRACE);\n\t__generic_field(char *, STACKTRACE, FILTER_STACKTRACE);\n\n\treturn ret;\n}\n\nstatic int trace_define_common_fields(void)\n{\n\tint ret;\n\tstruct trace_entry ent;\n\n\t__common_field(unsigned short, type);\n\t__common_field(unsigned char, flags);\n\t \n\t__common_field(unsigned char, preempt_count);\n\t__common_field(int, pid);\n\n\treturn ret;\n}\n\nstatic void trace_destroy_fields(struct trace_event_call *call)\n{\n\tstruct ftrace_event_field *field, *next;\n\tstruct list_head *head;\n\n\thead = trace_get_fields(call);\n\tlist_for_each_entry_safe(field, next, head, link) {\n\t\tlist_del(&field->link);\n\t\tkmem_cache_free(field_cachep, field);\n\t}\n}\n\n \nint trace_event_get_offsets(struct trace_event_call *call)\n{\n\tstruct ftrace_event_field *tail;\n\tstruct list_head *head;\n\n\thead = trace_get_fields(call);\n\t \n\ttail = list_first_entry(head, struct ftrace_event_field, link);\n\treturn tail->offset + tail->size;\n}\n\n \nstatic bool test_field(const char *fmt, struct trace_event_call *call)\n{\n\tstruct trace_event_fields *field = call->class->fields_array;\n\tconst char *array_descriptor;\n\tconst char *p = fmt;\n\tint len;\n\n\tif (!(len = str_has_prefix(fmt, \"REC->\")))\n\t\treturn false;\n\tfmt += len;\n\tfor (p = fmt; *p; p++) {\n\t\tif (!isalnum(*p) && *p != '_')\n\t\t\tbreak;\n\t}\n\tlen = p - fmt;\n\n\tfor (; field->type; field++) {\n\t\tif (strncmp(field->name, fmt, len) ||\n\t\t    field->name[len])\n\t\t\tcontinue;\n\t\tarray_descriptor = strchr(field->type, '[');\n\t\t \n\t\treturn array_descriptor != NULL;\n\t}\n\treturn false;\n}\n\n \nstatic void test_event_printk(struct trace_event_call *call)\n{\n\tu64 dereference_flags = 0;\n\tbool first = true;\n\tconst char *fmt, *c, *r, *a;\n\tint parens = 0;\n\tchar in_quote = 0;\n\tint start_arg = 0;\n\tint arg = 0;\n\tint i;\n\n\tfmt = call->print_fmt;\n\n\tif (!fmt)\n\t\treturn;\n\n\tfor (i = 0; fmt[i]; i++) {\n\t\tswitch (fmt[i]) {\n\t\tcase '\\\\':\n\t\t\ti++;\n\t\t\tif (!fmt[i])\n\t\t\t\treturn;\n\t\t\tcontinue;\n\t\tcase '\"':\n\t\tcase '\\'':\n\t\t\t \n\t\t\tif (first) {\n\t\t\t\tif (fmt[i] == '\\'')\n\t\t\t\t\tcontinue;\n\t\t\t\tif (in_quote) {\n\t\t\t\t\targ = 0;\n\t\t\t\t\tfirst = false;\n\t\t\t\t\t \n\t\t\t\t\tif (!dereference_flags)\n\t\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (in_quote) {\n\t\t\t\tif (in_quote == fmt[i])\n\t\t\t\t\tin_quote = 0;\n\t\t\t} else {\n\t\t\t\tin_quote = fmt[i];\n\t\t\t}\n\t\t\tcontinue;\n\t\tcase '%':\n\t\t\tif (!first || !in_quote)\n\t\t\t\tcontinue;\n\t\t\ti++;\n\t\t\tif (!fmt[i])\n\t\t\t\treturn;\n\t\t\tswitch (fmt[i]) {\n\t\t\tcase '%':\n\t\t\t\tcontinue;\n\t\t\tcase 'p':\n\t\t\t\t \n\t\t\t\tswitch (fmt[i + 1]) {\n\t\t\t\tcase 'B': case 'R': case 'r':\n\t\t\t\tcase 'b': case 'M': case 'm':\n\t\t\t\tcase 'I': case 'i': case 'E':\n\t\t\t\tcase 'U': case 'V': case 'N':\n\t\t\t\tcase 'a': case 'd': case 'D':\n\t\t\t\tcase 'g': case 't': case 'C':\n\t\t\t\tcase 'O': case 'f':\n\t\t\t\t\tif (WARN_ONCE(arg == 63,\n\t\t\t\t\t\t      \"Too many args for event: %s\",\n\t\t\t\t\t\t      trace_event_name(call)))\n\t\t\t\t\t\treturn;\n\t\t\t\t\tdereference_flags |= 1ULL << arg;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t{\n\t\t\t\tbool star = false;\n\t\t\t\tint j;\n\n\t\t\t\t \n\t\t\t\tfor (j = 0; fmt[i + j]; j++) {\n\t\t\t\t\tif (isdigit(fmt[i + j]) ||\n\t\t\t\t\t    fmt[i + j] == '.')\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tif (fmt[i + j] == '*') {\n\t\t\t\t\t\tstar = true;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t\tif ((fmt[i + j] == 's') && star)\n\t\t\t\t\t\targ++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}  \n\n\t\t\t}  \n\t\t\targ++;\n\t\t\tcontinue;\n\t\tcase '(':\n\t\t\tif (in_quote)\n\t\t\t\tcontinue;\n\t\t\tparens++;\n\t\t\tcontinue;\n\t\tcase ')':\n\t\t\tif (in_quote)\n\t\t\t\tcontinue;\n\t\t\tparens--;\n\t\t\tif (WARN_ONCE(parens < 0,\n\t\t\t\t      \"Paren mismatch for event: %s\\narg='%s'\\n%*s\",\n\t\t\t\t      trace_event_name(call),\n\t\t\t\t      fmt + start_arg,\n\t\t\t\t      (i - start_arg) + 5, \"^\"))\n\t\t\t\treturn;\n\t\t\tcontinue;\n\t\tcase ',':\n\t\t\tif (in_quote || parens)\n\t\t\t\tcontinue;\n\t\t\ti++;\n\t\t\twhile (isspace(fmt[i]))\n\t\t\t\ti++;\n\t\t\tstart_arg = i;\n\t\t\tif (!(dereference_flags & (1ULL << arg)))\n\t\t\t\tgoto next_arg;\n\n\t\t\t \n\t\t\tc = strchr(fmt + i, ',');\n\t\t\tr = strstr(fmt + i, \"REC->\");\n\t\t\tif (r && (!c || r < c)) {\n\t\t\t\t \n\t\t\t\ta = strchr(fmt + i, '&');\n\t\t\t\tif ((a && (a < r)) || test_field(r, call))\n\t\t\t\t\tdereference_flags &= ~(1ULL << arg);\n\t\t\t} else if ((r = strstr(fmt + i, \"__get_dynamic_array(\")) &&\n\t\t\t\t   (!c || r < c)) {\n\t\t\t\tdereference_flags &= ~(1ULL << arg);\n\t\t\t} else if ((r = strstr(fmt + i, \"__get_sockaddr(\")) &&\n\t\t\t\t   (!c || r < c)) {\n\t\t\t\tdereference_flags &= ~(1ULL << arg);\n\t\t\t}\n\n\t\tnext_arg:\n\t\t\ti--;\n\t\t\targ++;\n\t\t}\n\t}\n\n\t \n\tif (WARN_ON_ONCE(dereference_flags)) {\n\t\targ = 1;\n\t\twhile (!(dereference_flags & 1)) {\n\t\t\tdereference_flags >>= 1;\n\t\t\targ++;\n\t\t}\n\t\tpr_warn(\"event %s has unsafe dereference of argument %d\\n\",\n\t\t\ttrace_event_name(call), arg);\n\t\tpr_warn(\"print_fmt: %s\\n\", fmt);\n\t}\n}\n\nint trace_event_raw_init(struct trace_event_call *call)\n{\n\tint id;\n\n\tid = register_trace_event(&call->event);\n\tif (!id)\n\t\treturn -ENODEV;\n\n\ttest_event_printk(call);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(trace_event_raw_init);\n\nbool trace_event_ignore_this_pid(struct trace_event_file *trace_file)\n{\n\tstruct trace_array *tr = trace_file->tr;\n\tstruct trace_array_cpu *data;\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_pid_list *pid_list;\n\n\tpid_list = rcu_dereference_raw(tr->filtered_pids);\n\tno_pid_list = rcu_dereference_raw(tr->filtered_no_pids);\n\n\tif (!pid_list && !no_pid_list)\n\t\treturn false;\n\n\tdata = this_cpu_ptr(tr->array_buffer.data);\n\n\treturn data->ignore_pid;\n}\nEXPORT_SYMBOL_GPL(trace_event_ignore_this_pid);\n\nvoid *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,\n\t\t\t\t struct trace_event_file *trace_file,\n\t\t\t\t unsigned long len)\n{\n\tstruct trace_event_call *event_call = trace_file->event_call;\n\n\tif ((trace_file->flags & EVENT_FILE_FL_PID_FILTER) &&\n\t    trace_event_ignore_this_pid(trace_file))\n\t\treturn NULL;\n\n\t \n\tfbuffer->trace_ctx = tracing_gen_ctx_dec();\n\tfbuffer->trace_file = trace_file;\n\n\tfbuffer->event =\n\t\ttrace_event_buffer_lock_reserve(&fbuffer->buffer, trace_file,\n\t\t\t\t\t\tevent_call->event.type, len,\n\t\t\t\t\t\tfbuffer->trace_ctx);\n\tif (!fbuffer->event)\n\t\treturn NULL;\n\n\tfbuffer->regs = NULL;\n\tfbuffer->entry = ring_buffer_event_data(fbuffer->event);\n\treturn fbuffer->entry;\n}\nEXPORT_SYMBOL_GPL(trace_event_buffer_reserve);\n\nint trace_event_reg(struct trace_event_call *call,\n\t\t    enum trace_reg type, void *data)\n{\n\tstruct trace_event_file *file = data;\n\n\tWARN_ON(!(call->flags & TRACE_EVENT_FL_TRACEPOINT));\n\tswitch (type) {\n\tcase TRACE_REG_REGISTER:\n\t\treturn tracepoint_probe_register(call->tp,\n\t\t\t\t\t\t call->class->probe,\n\t\t\t\t\t\t file);\n\tcase TRACE_REG_UNREGISTER:\n\t\ttracepoint_probe_unregister(call->tp,\n\t\t\t\t\t    call->class->probe,\n\t\t\t\t\t    file);\n\t\treturn 0;\n\n#ifdef CONFIG_PERF_EVENTS\n\tcase TRACE_REG_PERF_REGISTER:\n\t\treturn tracepoint_probe_register(call->tp,\n\t\t\t\t\t\t call->class->perf_probe,\n\t\t\t\t\t\t call);\n\tcase TRACE_REG_PERF_UNREGISTER:\n\t\ttracepoint_probe_unregister(call->tp,\n\t\t\t\t\t    call->class->perf_probe,\n\t\t\t\t\t    call);\n\t\treturn 0;\n\tcase TRACE_REG_PERF_OPEN:\n\tcase TRACE_REG_PERF_CLOSE:\n\tcase TRACE_REG_PERF_ADD:\n\tcase TRACE_REG_PERF_DEL:\n\t\treturn 0;\n#endif\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(trace_event_reg);\n\nvoid trace_event_enable_cmd_record(bool enable)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_array *tr;\n\n\tlockdep_assert_held(&event_mutex);\n\n\tdo_for_each_event_file(tr, file) {\n\n\t\tif (!(file->flags & EVENT_FILE_FL_ENABLED))\n\t\t\tcontinue;\n\n\t\tif (enable) {\n\t\t\ttracing_start_cmdline_record();\n\t\t\tset_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);\n\t\t} else {\n\t\t\ttracing_stop_cmdline_record();\n\t\t\tclear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);\n\t\t}\n\t} while_for_each_event_file();\n}\n\nvoid trace_event_enable_tgid_record(bool enable)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_array *tr;\n\n\tlockdep_assert_held(&event_mutex);\n\n\tdo_for_each_event_file(tr, file) {\n\t\tif (!(file->flags & EVENT_FILE_FL_ENABLED))\n\t\t\tcontinue;\n\n\t\tif (enable) {\n\t\t\ttracing_start_tgid_record();\n\t\t\tset_bit(EVENT_FILE_FL_RECORDED_TGID_BIT, &file->flags);\n\t\t} else {\n\t\t\ttracing_stop_tgid_record();\n\t\t\tclear_bit(EVENT_FILE_FL_RECORDED_TGID_BIT,\n\t\t\t\t  &file->flags);\n\t\t}\n\t} while_for_each_event_file();\n}\n\nstatic int __ftrace_event_enable_disable(struct trace_event_file *file,\n\t\t\t\t\t int enable, int soft_disable)\n{\n\tstruct trace_event_call *call = file->event_call;\n\tstruct trace_array *tr = file->tr;\n\tint ret = 0;\n\tint disable;\n\n\tswitch (enable) {\n\tcase 0:\n\t\t \n\t\tif (soft_disable) {\n\t\t\tif (atomic_dec_return(&file->sm_ref) > 0)\n\t\t\t\tbreak;\n\t\t\tdisable = file->flags & EVENT_FILE_FL_SOFT_DISABLED;\n\t\t\tclear_bit(EVENT_FILE_FL_SOFT_MODE_BIT, &file->flags);\n\t\t\t \n\t\t\ttrace_buffered_event_disable();\n\t\t} else\n\t\t\tdisable = !(file->flags & EVENT_FILE_FL_SOFT_MODE);\n\n\t\tif (disable && (file->flags & EVENT_FILE_FL_ENABLED)) {\n\t\t\tclear_bit(EVENT_FILE_FL_ENABLED_BIT, &file->flags);\n\t\t\tif (file->flags & EVENT_FILE_FL_RECORDED_CMD) {\n\t\t\t\ttracing_stop_cmdline_record();\n\t\t\t\tclear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);\n\t\t\t}\n\n\t\t\tif (file->flags & EVENT_FILE_FL_RECORDED_TGID) {\n\t\t\t\ttracing_stop_tgid_record();\n\t\t\t\tclear_bit(EVENT_FILE_FL_RECORDED_TGID_BIT, &file->flags);\n\t\t\t}\n\n\t\t\tcall->class->reg(call, TRACE_REG_UNREGISTER, file);\n\t\t}\n\t\t \n\t\tif (file->flags & EVENT_FILE_FL_SOFT_MODE)\n\t\t\tset_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);\n\t\telse\n\t\t\tclear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);\n\t\tbreak;\n\tcase 1:\n\t\t \n\t\tif (!soft_disable)\n\t\t\tclear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);\n\t\telse {\n\t\t\tif (atomic_inc_return(&file->sm_ref) > 1)\n\t\t\t\tbreak;\n\t\t\tset_bit(EVENT_FILE_FL_SOFT_MODE_BIT, &file->flags);\n\t\t\t \n\t\t\ttrace_buffered_event_enable();\n\t\t}\n\n\t\tif (!(file->flags & EVENT_FILE_FL_ENABLED)) {\n\t\t\tbool cmd = false, tgid = false;\n\n\t\t\t \n\t\t\tif (soft_disable)\n\t\t\t\tset_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);\n\n\t\t\tif (tr->trace_flags & TRACE_ITER_RECORD_CMD) {\n\t\t\t\tcmd = true;\n\t\t\t\ttracing_start_cmdline_record();\n\t\t\t\tset_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);\n\t\t\t}\n\n\t\t\tif (tr->trace_flags & TRACE_ITER_RECORD_TGID) {\n\t\t\t\ttgid = true;\n\t\t\t\ttracing_start_tgid_record();\n\t\t\t\tset_bit(EVENT_FILE_FL_RECORDED_TGID_BIT, &file->flags);\n\t\t\t}\n\n\t\t\tret = call->class->reg(call, TRACE_REG_REGISTER, file);\n\t\t\tif (ret) {\n\t\t\t\tif (cmd)\n\t\t\t\t\ttracing_stop_cmdline_record();\n\t\t\t\tif (tgid)\n\t\t\t\t\ttracing_stop_tgid_record();\n\t\t\t\tpr_info(\"event trace: Could not enable event \"\n\t\t\t\t\t\"%s\\n\", trace_event_name(call));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tset_bit(EVENT_FILE_FL_ENABLED_BIT, &file->flags);\n\n\t\t\t \n\t\t\tset_bit(EVENT_FILE_FL_WAS_ENABLED_BIT, &file->flags);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint trace_event_enable_disable(struct trace_event_file *file,\n\t\t\t       int enable, int soft_disable)\n{\n\treturn __ftrace_event_enable_disable(file, enable, soft_disable);\n}\n\nstatic int ftrace_event_enable_disable(struct trace_event_file *file,\n\t\t\t\t       int enable)\n{\n\treturn __ftrace_event_enable_disable(file, enable, 0);\n}\n\nstatic void ftrace_clear_events(struct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\n\tmutex_lock(&event_mutex);\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tftrace_event_enable_disable(file, 0);\n\t}\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void\nevent_filter_pid_sched_process_exit(void *data, struct task_struct *task)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = data;\n\n\tpid_list = rcu_dereference_raw(tr->filtered_pids);\n\ttrace_filter_add_remove_task(pid_list, NULL, task);\n\n\tpid_list = rcu_dereference_raw(tr->filtered_no_pids);\n\ttrace_filter_add_remove_task(pid_list, NULL, task);\n}\n\nstatic void\nevent_filter_pid_sched_process_fork(void *data,\n\t\t\t\t    struct task_struct *self,\n\t\t\t\t    struct task_struct *task)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = data;\n\n\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\ttrace_filter_add_remove_task(pid_list, self, task);\n\n\tpid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\ttrace_filter_add_remove_task(pid_list, self, task);\n}\n\nvoid trace_event_follow_fork(struct trace_array *tr, bool enable)\n{\n\tif (enable) {\n\t\tregister_trace_prio_sched_process_fork(event_filter_pid_sched_process_fork,\n\t\t\t\t\t\t       tr, INT_MIN);\n\t\tregister_trace_prio_sched_process_free(event_filter_pid_sched_process_exit,\n\t\t\t\t\t\t       tr, INT_MAX);\n\t} else {\n\t\tunregister_trace_sched_process_fork(event_filter_pid_sched_process_fork,\n\t\t\t\t\t\t    tr);\n\t\tunregister_trace_sched_process_free(event_filter_pid_sched_process_exit,\n\t\t\t\t\t\t    tr);\n\t}\n}\n\nstatic void\nevent_filter_pid_sched_switch_probe_pre(void *data, bool preempt,\n\t\t\t\t\tstruct task_struct *prev,\n\t\t\t\t\tstruct task_struct *next,\n\t\t\t\t\tunsigned int prev_state)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_pid_list *pid_list;\n\tbool ret;\n\n\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\tno_pid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\n\t \n\tret = trace_ignore_this_task(NULL, no_pid_list, prev) &&\n\t\ttrace_ignore_this_task(NULL, no_pid_list, next);\n\n\tthis_cpu_write(tr->array_buffer.data->ignore_pid, ret ||\n\t\t       (trace_ignore_this_task(pid_list, NULL, prev) &&\n\t\t\ttrace_ignore_this_task(pid_list, NULL, next)));\n}\n\nstatic void\nevent_filter_pid_sched_switch_probe_post(void *data, bool preempt,\n\t\t\t\t\t struct task_struct *prev,\n\t\t\t\t\t struct task_struct *next,\n\t\t\t\t\t unsigned int prev_state)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_pid_list *pid_list;\n\n\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\tno_pid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\n\tthis_cpu_write(tr->array_buffer.data->ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, no_pid_list, next));\n}\n\nstatic void\nevent_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_pid_list *pid_list;\n\n\t \n\tif (!this_cpu_read(tr->array_buffer.data->ignore_pid))\n\t\treturn;\n\n\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\tno_pid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\n\tthis_cpu_write(tr->array_buffer.data->ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, no_pid_list, task));\n}\n\nstatic void\nevent_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_pid_list *pid_list;\n\n\t \n\tif (this_cpu_read(tr->array_buffer.data->ignore_pid))\n\t\treturn;\n\n\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\tno_pid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\n\t \n\tthis_cpu_write(tr->array_buffer.data->ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, no_pid_list, current));\n}\n\nstatic void unregister_pid_events(struct trace_array *tr)\n{\n\tunregister_trace_sched_switch(event_filter_pid_sched_switch_probe_pre, tr);\n\tunregister_trace_sched_switch(event_filter_pid_sched_switch_probe_post, tr);\n\n\tunregister_trace_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre, tr);\n\tunregister_trace_sched_wakeup(event_filter_pid_sched_wakeup_probe_post, tr);\n\n\tunregister_trace_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_pre, tr);\n\tunregister_trace_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_post, tr);\n\n\tunregister_trace_sched_waking(event_filter_pid_sched_wakeup_probe_pre, tr);\n\tunregister_trace_sched_waking(event_filter_pid_sched_wakeup_probe_post, tr);\n}\n\nstatic void __ftrace_clear_event_pids(struct trace_array *tr, int type)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_event_file *file;\n\tint cpu;\n\n\tpid_list = rcu_dereference_protected(tr->filtered_pids,\n\t\t\t\t\t     lockdep_is_held(&event_mutex));\n\tno_pid_list = rcu_dereference_protected(tr->filtered_no_pids,\n\t\t\t\t\t     lockdep_is_held(&event_mutex));\n\n\t \n\tif (!pid_type_enabled(type, pid_list, no_pid_list))\n\t\treturn;\n\n\tif (!still_need_pid_events(type, pid_list, no_pid_list)) {\n\t\tunregister_pid_events(tr);\n\n\t\tlist_for_each_entry(file, &tr->events, list) {\n\t\t\tclear_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);\n\t\t}\n\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tper_cpu_ptr(tr->array_buffer.data, cpu)->ignore_pid = false;\n\t}\n\n\tif (type & TRACE_PIDS)\n\t\trcu_assign_pointer(tr->filtered_pids, NULL);\n\n\tif (type & TRACE_NO_PIDS)\n\t\trcu_assign_pointer(tr->filtered_no_pids, NULL);\n\n\t \n\ttracepoint_synchronize_unregister();\n\n\tif ((type & TRACE_PIDS) && pid_list)\n\t\ttrace_pid_list_free(pid_list);\n\n\tif ((type & TRACE_NO_PIDS) && no_pid_list)\n\t\ttrace_pid_list_free(no_pid_list);\n}\n\nstatic void ftrace_clear_event_pids(struct trace_array *tr, int type)\n{\n\tmutex_lock(&event_mutex);\n\t__ftrace_clear_event_pids(tr, type);\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void __put_system(struct event_subsystem *system)\n{\n\tstruct event_filter *filter = system->filter;\n\n\tWARN_ON_ONCE(system_refcount(system) == 0);\n\tif (system_refcount_dec(system))\n\t\treturn;\n\n\tlist_del(&system->list);\n\n\tif (filter) {\n\t\tkfree(filter->filter_string);\n\t\tkfree(filter);\n\t}\n\tkfree_const(system->name);\n\tkfree(system);\n}\n\nstatic void __get_system(struct event_subsystem *system)\n{\n\tWARN_ON_ONCE(system_refcount(system) == 0);\n\tsystem_refcount_inc(system);\n}\n\nstatic void __get_system_dir(struct trace_subsystem_dir *dir)\n{\n\tWARN_ON_ONCE(dir->ref_count == 0);\n\tdir->ref_count++;\n\t__get_system(dir->subsystem);\n}\n\nstatic void __put_system_dir(struct trace_subsystem_dir *dir)\n{\n\tWARN_ON_ONCE(dir->ref_count == 0);\n\t \n\tWARN_ON_ONCE(system_refcount(dir->subsystem) == 1 && dir->ref_count != 1);\n\n\t__put_system(dir->subsystem);\n\tif (!--dir->ref_count)\n\t\tkfree(dir);\n}\n\nstatic void put_system(struct trace_subsystem_dir *dir)\n{\n\tmutex_lock(&event_mutex);\n\t__put_system_dir(dir);\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void remove_subsystem(struct trace_subsystem_dir *dir)\n{\n\tif (!dir)\n\t\treturn;\n\n\tif (!--dir->nr_events) {\n\t\teventfs_remove(dir->ef);\n\t\tlist_del(&dir->list);\n\t\t__put_system_dir(dir);\n\t}\n}\n\nvoid event_file_get(struct trace_event_file *file)\n{\n\tatomic_inc(&file->ref);\n}\n\nvoid event_file_put(struct trace_event_file *file)\n{\n\tif (WARN_ON_ONCE(!atomic_read(&file->ref))) {\n\t\tif (file->flags & EVENT_FILE_FL_FREED)\n\t\t\tkmem_cache_free(file_cachep, file);\n\t\treturn;\n\t}\n\n\tif (atomic_dec_and_test(&file->ref)) {\n\t\t \n\t\tif (WARN_ON_ONCE(!(file->flags & EVENT_FILE_FL_FREED)))\n\t\t\treturn;\n\t\tkmem_cache_free(file_cachep, file);\n\t}\n}\n\nstatic void remove_event_file_dir(struct trace_event_file *file)\n{\n\teventfs_remove(file->ef);\n\tlist_del(&file->list);\n\tremove_subsystem(file->system);\n\tfree_event_filter(file->filter);\n\tfile->flags |= EVENT_FILE_FL_FREED;\n\tevent_file_put(file);\n}\n\n \nstatic int\n__ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,\n\t\t\t      const char *sub, const char *event, int set)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_event_call *call;\n\tconst char *name;\n\tint ret = -EINVAL;\n\tint eret = 0;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\n\t\tcall = file->event_call;\n\t\tname = trace_event_name(call);\n\n\t\tif (!name || !call->class || !call->class->reg)\n\t\t\tcontinue;\n\n\t\tif (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)\n\t\t\tcontinue;\n\n\t\tif (match &&\n\t\t    strcmp(match, name) != 0 &&\n\t\t    strcmp(match, call->class->system) != 0)\n\t\t\tcontinue;\n\n\t\tif (sub && strcmp(sub, call->class->system) != 0)\n\t\t\tcontinue;\n\n\t\tif (event && strcmp(event, name) != 0)\n\t\t\tcontinue;\n\n\t\tret = ftrace_event_enable_disable(file, set);\n\n\t\t \n\t\tif (ret && !eret)\n\t\t\teret = ret;\n\n\t\tret = eret;\n\t}\n\n\treturn ret;\n}\n\nstatic int __ftrace_set_clr_event(struct trace_array *tr, const char *match,\n\t\t\t\t  const char *sub, const char *event, int set)\n{\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tret = __ftrace_set_clr_event_nolock(tr, match, sub, event, set);\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\nint ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)\n{\n\tchar *event = NULL, *sub = NULL, *match;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENOENT;\n\t \n\n\tmatch = strsep(&buf, \":\");\n\tif (buf) {\n\t\tsub = match;\n\t\tevent = buf;\n\t\tmatch = NULL;\n\n\t\tif (!strlen(sub) || strcmp(sub, \"*\") == 0)\n\t\t\tsub = NULL;\n\t\tif (!strlen(event) || strcmp(event, \"*\") == 0)\n\t\t\tevent = NULL;\n\t}\n\n\tret = __ftrace_set_clr_event(tr, match, sub, event, set);\n\n\t \n\tif (buf)\n\t\t*(buf - 1) = ':';\n\n\treturn ret;\n}\n\n \nint trace_set_clr_event(const char *system, const char *event, int set)\n{\n\tstruct trace_array *tr = top_trace_array();\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\treturn __ftrace_set_clr_event(tr, NULL, system, event, set);\n}\nEXPORT_SYMBOL_GPL(trace_set_clr_event);\n\n \nint trace_array_set_clr_event(struct trace_array *tr, const char *system,\n\t\tconst char *event, bool enable)\n{\n\tint set;\n\n\tif (!tr)\n\t\treturn -ENOENT;\n\n\tset = (enable == true) ? 1 : 0;\n\treturn __ftrace_set_clr_event(tr, NULL, system, event, set);\n}\nEXPORT_SYMBOL_GPL(trace_array_set_clr_event);\n\n \n#define EVENT_BUF_SIZE\t\t127\n\nstatic ssize_t\nftrace_event_write(struct file *file, const char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tstruct trace_parser parser;\n\tstruct seq_file *m = file->private_data;\n\tstruct trace_array *tr = m->private;\n\tssize_t read, ret;\n\n\tif (!cnt)\n\t\treturn 0;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (trace_parser_get_init(&parser, EVENT_BUF_SIZE + 1))\n\t\treturn -ENOMEM;\n\n\tread = trace_get_user(&parser, ubuf, cnt, ppos);\n\n\tif (read >= 0 && trace_parser_loaded((&parser))) {\n\t\tint set = 1;\n\n\t\tif (*parser.buffer == '!')\n\t\t\tset = 0;\n\n\t\tret = ftrace_set_clr_event(tr, parser.buffer + !set, set);\n\t\tif (ret)\n\t\t\tgoto out_put;\n\t}\n\n\tret = read;\n\n out_put:\n\ttrace_parser_put(&parser);\n\n\treturn ret;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_event_file *file = v;\n\tstruct trace_event_call *call;\n\tstruct trace_array *tr = m->private;\n\n\t(*pos)++;\n\n\tlist_for_each_entry_continue(file, &tr->events, list) {\n\t\tcall = file->event_call;\n\t\t \n\t\tif (call->class && call->class->reg &&\n\t\t    !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))\n\t\t\treturn file;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_array *tr = m->private;\n\tloff_t l;\n\n\tmutex_lock(&event_mutex);\n\n\tfile = list_entry(&tr->events, struct trace_event_file, list);\n\tfor (l = 0; l <= *pos; ) {\n\t\tfile = t_next(m, file, &l);\n\t\tif (!file)\n\t\t\tbreak;\n\t}\n\treturn file;\n}\n\nstatic void *\ns_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_event_file *file = v;\n\tstruct trace_array *tr = m->private;\n\n\t(*pos)++;\n\n\tlist_for_each_entry_continue(file, &tr->events, list) {\n\t\tif (file->flags & EVENT_FILE_FL_ENABLED)\n\t\t\treturn file;\n\t}\n\n\treturn NULL;\n}\n\nstatic void *s_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_array *tr = m->private;\n\tloff_t l;\n\n\tmutex_lock(&event_mutex);\n\n\tfile = list_entry(&tr->events, struct trace_event_file, list);\n\tfor (l = 0; l <= *pos; ) {\n\t\tfile = s_next(m, file, &l);\n\t\tif (!file)\n\t\t\tbreak;\n\t}\n\treturn file;\n}\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct trace_event_file *file = v;\n\tstruct trace_event_call *call = file->event_call;\n\n\tif (strcmp(call->class->system, TRACE_SYSTEM) != 0)\n\t\tseq_printf(m, \"%s:\", call->class->system);\n\tseq_printf(m, \"%s\\n\", trace_event_name(call));\n\n\treturn 0;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&event_mutex);\n}\n\nstatic void *\n__next(struct seq_file *m, void *v, loff_t *pos, int type)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct trace_pid_list *pid_list;\n\n\tif (type == TRACE_PIDS)\n\t\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\telse\n\t\tpid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\n\treturn trace_pid_next(pid_list, v, pos);\n}\n\nstatic void *\np_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\treturn __next(m, v, pos, TRACE_PIDS);\n}\n\nstatic void *\nnp_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\treturn __next(m, v, pos, TRACE_NO_PIDS);\n}\n\nstatic void *__start(struct seq_file *m, loff_t *pos, int type)\n\t__acquires(RCU)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = m->private;\n\n\t \n\tmutex_lock(&event_mutex);\n\trcu_read_lock_sched();\n\n\tif (type == TRACE_PIDS)\n\t\tpid_list = rcu_dereference_sched(tr->filtered_pids);\n\telse\n\t\tpid_list = rcu_dereference_sched(tr->filtered_no_pids);\n\n\tif (!pid_list)\n\t\treturn NULL;\n\n\treturn trace_pid_start(pid_list, pos);\n}\n\nstatic void *p_start(struct seq_file *m, loff_t *pos)\n\t__acquires(RCU)\n{\n\treturn __start(m, pos, TRACE_PIDS);\n}\n\nstatic void *np_start(struct seq_file *m, loff_t *pos)\n\t__acquires(RCU)\n{\n\treturn __start(m, pos, TRACE_NO_PIDS);\n}\n\nstatic void p_stop(struct seq_file *m, void *p)\n\t__releases(RCU)\n{\n\trcu_read_unlock_sched();\n\tmutex_unlock(&event_mutex);\n}\n\nstatic ssize_t\nevent_enable_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t  loff_t *ppos)\n{\n\tstruct trace_event_file *file;\n\tunsigned long flags;\n\tchar buf[4] = \"0\";\n\n\tmutex_lock(&event_mutex);\n\tfile = event_file_data(filp);\n\tif (likely(file))\n\t\tflags = file->flags;\n\tmutex_unlock(&event_mutex);\n\n\tif (!file || flags & EVENT_FILE_FL_FREED)\n\t\treturn -ENODEV;\n\n\tif (flags & EVENT_FILE_FL_ENABLED &&\n\t    !(flags & EVENT_FILE_FL_SOFT_DISABLED))\n\t\tstrcpy(buf, \"1\");\n\n\tif (flags & EVENT_FILE_FL_SOFT_DISABLED ||\n\t    flags & EVENT_FILE_FL_SOFT_MODE)\n\t\tstrcat(buf, \"*\");\n\n\tstrcat(buf, \"\\n\");\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, strlen(buf));\n}\n\nstatic ssize_t\nevent_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t   loff_t *ppos)\n{\n\tstruct trace_event_file *file;\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tswitch (val) {\n\tcase 0:\n\tcase 1:\n\t\tret = -ENODEV;\n\t\tmutex_lock(&event_mutex);\n\t\tfile = event_file_data(filp);\n\t\tif (likely(file && !(file->flags & EVENT_FILE_FL_FREED)))\n\t\t\tret = ftrace_event_enable_disable(file, val);\n\t\tmutex_unlock(&event_mutex);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t*ppos += cnt;\n\n\treturn ret ? ret : cnt;\n}\n\nstatic ssize_t\nsystem_enable_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t   loff_t *ppos)\n{\n\tconst char set_to_char[4] = { '?', '0', '1', 'X' };\n\tstruct trace_subsystem_dir *dir = filp->private_data;\n\tstruct event_subsystem *system = dir->subsystem;\n\tstruct trace_event_call *call;\n\tstruct trace_event_file *file;\n\tstruct trace_array *tr = dir->tr;\n\tchar buf[2];\n\tint set = 0;\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tcall = file->event_call;\n\t\tif ((call->flags & TRACE_EVENT_FL_IGNORE_ENABLE) ||\n\t\t    !trace_event_name(call) || !call->class || !call->class->reg)\n\t\t\tcontinue;\n\n\t\tif (system && strcmp(call->class->system, system->name) != 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tset |= (1 << !!(file->flags & EVENT_FILE_FL_ENABLED));\n\n\t\t \n\t\tif (set == 3)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&event_mutex);\n\n\tbuf[0] = set_to_char[set];\n\tbuf[1] = '\\n';\n\n\tret = simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);\n\n\treturn ret;\n}\n\nstatic ssize_t\nsystem_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t    loff_t *ppos)\n{\n\tstruct trace_subsystem_dir *dir = filp->private_data;\n\tstruct event_subsystem *system = dir->subsystem;\n\tconst char *name = NULL;\n\tunsigned long val;\n\tssize_t ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val != 0 && val != 1)\n\t\treturn -EINVAL;\n\n\t \n\tif (system)\n\t\tname = system->name;\n\n\tret = __ftrace_set_clr_event(dir->tr, NULL, name, NULL, val);\n\tif (ret)\n\t\tgoto out;\n\n\tret = cnt;\n\nout:\n\t*ppos += cnt;\n\n\treturn ret;\n}\n\nenum {\n\tFORMAT_HEADER\t\t= 1,\n\tFORMAT_FIELD_SEPERATOR\t= 2,\n\tFORMAT_PRINTFMT\t\t= 3,\n};\n\nstatic void *f_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_event_call *call = event_file_data(m->private);\n\tstruct list_head *common_head = &ftrace_common_fields;\n\tstruct list_head *head = trace_get_fields(call);\n\tstruct list_head *node = v;\n\n\t(*pos)++;\n\n\tswitch ((unsigned long)v) {\n\tcase FORMAT_HEADER:\n\t\tnode = common_head;\n\t\tbreak;\n\n\tcase FORMAT_FIELD_SEPERATOR:\n\t\tnode = head;\n\t\tbreak;\n\n\tcase FORMAT_PRINTFMT:\n\t\t \n\t\treturn NULL;\n\t}\n\n\tnode = node->prev;\n\tif (node == common_head)\n\t\treturn (void *)FORMAT_FIELD_SEPERATOR;\n\telse if (node == head)\n\t\treturn (void *)FORMAT_PRINTFMT;\n\telse\n\t\treturn node;\n}\n\nstatic int f_show(struct seq_file *m, void *v)\n{\n\tstruct trace_event_call *call = event_file_data(m->private);\n\tstruct ftrace_event_field *field;\n\tconst char *array_descriptor;\n\n\tswitch ((unsigned long)v) {\n\tcase FORMAT_HEADER:\n\t\tseq_printf(m, \"name: %s\\n\", trace_event_name(call));\n\t\tseq_printf(m, \"ID: %d\\n\", call->event.type);\n\t\tseq_puts(m, \"format:\\n\");\n\t\treturn 0;\n\n\tcase FORMAT_FIELD_SEPERATOR:\n\t\tseq_putc(m, '\\n');\n\t\treturn 0;\n\n\tcase FORMAT_PRINTFMT:\n\t\tseq_printf(m, \"\\nprint fmt: %s\\n\",\n\t\t\t   call->print_fmt);\n\t\treturn 0;\n\t}\n\n\tfield = list_entry(v, struct ftrace_event_field, link);\n\t \n\tarray_descriptor = strchr(field->type, '[');\n\n\tif (str_has_prefix(field->type, \"__data_loc\"))\n\t\tarray_descriptor = NULL;\n\n\tif (!array_descriptor)\n\t\tseq_printf(m, \"\\tfield:%s %s;\\toffset:%u;\\tsize:%u;\\tsigned:%d;\\n\",\n\t\t\t   field->type, field->name, field->offset,\n\t\t\t   field->size, !!field->is_signed);\n\telse if (field->len)\n\t\tseq_printf(m, \"\\tfield:%.*s %s[%d];\\toffset:%u;\\tsize:%u;\\tsigned:%d;\\n\",\n\t\t\t   (int)(array_descriptor - field->type),\n\t\t\t   field->type, field->name,\n\t\t\t   field->len, field->offset,\n\t\t\t   field->size, !!field->is_signed);\n\telse\n\t\tseq_printf(m, \"\\tfield:%.*s %s[];\\toffset:%u;\\tsize:%u;\\tsigned:%d;\\n\",\n\t\t\t\t(int)(array_descriptor - field->type),\n\t\t\t\tfield->type, field->name,\n\t\t\t\tfield->offset, field->size, !!field->is_signed);\n\n\treturn 0;\n}\n\nstatic void *f_start(struct seq_file *m, loff_t *pos)\n{\n\tvoid *p = (void *)FORMAT_HEADER;\n\tloff_t l = 0;\n\n\t \n\tmutex_lock(&event_mutex);\n\tif (!event_file_data(m->private))\n\t\treturn ERR_PTR(-ENODEV);\n\n\twhile (l < *pos && p)\n\t\tp = f_next(m, p, &l);\n\n\treturn p;\n}\n\nstatic void f_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&event_mutex);\n}\n\nstatic const struct seq_operations trace_format_seq_ops = {\n\t.start\t\t= f_start,\n\t.next\t\t= f_next,\n\t.stop\t\t= f_stop,\n\t.show\t\t= f_show,\n};\n\nstatic int trace_format_open(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m;\n\tint ret;\n\n\t \n\n\tret = seq_open(file, &trace_format_seq_ops);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tm = file->private_data;\n\tm->private = file;\n\n\treturn 0;\n}\n\nstatic ssize_t\nevent_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)\n{\n\tint id = (long)event_file_data(filp);\n\tchar buf[32];\n\tint len;\n\n\tif (unlikely(!id))\n\t\treturn -ENODEV;\n\n\tlen = sprintf(buf, \"%d\\n\", id);\n\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, len);\n}\n\nstatic ssize_t\nevent_filter_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t  loff_t *ppos)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_seq *s;\n\tint r = -ENODEV;\n\n\tif (*ppos)\n\t\treturn 0;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tmutex_lock(&event_mutex);\n\tfile = event_file_data(filp);\n\tif (file && !(file->flags & EVENT_FILE_FL_FREED))\n\t\tprint_event_filter(file, s);\n\tmutex_unlock(&event_mutex);\n\n\tif (file)\n\t\tr = simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t\t    s->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn r;\n}\n\nstatic ssize_t\nevent_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t   loff_t *ppos)\n{\n\tstruct trace_event_file *file;\n\tchar *buf;\n\tint err = -ENODEV;\n\n\tif (cnt >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tbuf = memdup_user_nul(ubuf, cnt);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\n\tmutex_lock(&event_mutex);\n\tfile = event_file_data(filp);\n\tif (file)\n\t\terr = apply_event_filter(file, buf);\n\tmutex_unlock(&event_mutex);\n\n\tkfree(buf);\n\tif (err < 0)\n\t\treturn err;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic LIST_HEAD(event_subsystems);\n\nstatic int subsystem_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_subsystem_dir *dir = NULL, *iter_dir;\n\tstruct trace_array *tr = NULL, *iter_tr;\n\tstruct event_subsystem *system = NULL;\n\tint ret;\n\n\tif (tracing_is_disabled())\n\t\treturn -ENODEV;\n\n\t \n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(iter_tr, &ftrace_trace_arrays, list) {\n\t\tlist_for_each_entry(iter_dir, &iter_tr->systems, list) {\n\t\t\tif (iter_dir == inode->i_private) {\n\t\t\t\t \n\t\t\t\ttr = iter_tr;\n\t\t\t\tdir = iter_dir;\n\t\t\t\tif (dir->nr_events) {\n\t\t\t\t\t__get_system_dir(dir);\n\t\t\t\t\tsystem = dir->subsystem;\n\t\t\t\t}\n\t\t\t\tgoto exit_loop;\n\t\t\t}\n\t\t}\n\t}\n exit_loop:\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\tif (!system)\n\t\treturn -ENODEV;\n\n\t \n\tif (trace_array_get(tr) < 0) {\n\t\tput_system(dir);\n\t\treturn -ENODEV;\n\t}\n\n\tret = tracing_open_generic(inode, filp);\n\tif (ret < 0) {\n\t\ttrace_array_put(tr);\n\t\tput_system(dir);\n\t}\n\n\treturn ret;\n}\n\nstatic int system_tr_open(struct inode *inode, struct file *filp)\n{\n\tstruct trace_subsystem_dir *dir;\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\t \n\tdir = kzalloc(sizeof(*dir), GFP_KERNEL);\n\tif (!dir)\n\t\treturn -ENOMEM;\n\n\tret = tracing_open_generic_tr(inode, filp);\n\tif (ret < 0) {\n\t\tkfree(dir);\n\t\treturn ret;\n\t}\n\tdir->tr = tr;\n\tfilp->private_data = dir;\n\n\treturn 0;\n}\n\nstatic int subsystem_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_subsystem_dir *dir = file->private_data;\n\n\ttrace_array_put(dir->tr);\n\n\t \n\tif (dir->subsystem)\n\t\tput_system(dir);\n\telse\n\t\tkfree(dir);\n\n\treturn 0;\n}\n\nstatic ssize_t\nsubsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,\n\t\t      loff_t *ppos)\n{\n\tstruct trace_subsystem_dir *dir = filp->private_data;\n\tstruct event_subsystem *system = dir->subsystem;\n\tstruct trace_seq *s;\n\tint r;\n\n\tif (*ppos)\n\t\treturn 0;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tprint_subsystem_event_filter(system, s);\n\tr = simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t    s->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn r;\n}\n\nstatic ssize_t\nsubsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,\n\t\t       loff_t *ppos)\n{\n\tstruct trace_subsystem_dir *dir = filp->private_data;\n\tchar *buf;\n\tint err;\n\n\tif (cnt >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tbuf = memdup_user_nul(ubuf, cnt);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\n\terr = apply_subsystem_event_filter(dir, buf);\n\tkfree(buf);\n\tif (err < 0)\n\t\treturn err;\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\nshow_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)\n{\n\tint (*func)(struct trace_seq *s) = filp->private_data;\n\tstruct trace_seq *s;\n\tint r;\n\n\tif (*ppos)\n\t\treturn 0;\n\n\ts = kmalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ttrace_seq_init(s);\n\n\tfunc(s);\n\tr = simple_read_from_buffer(ubuf, cnt, ppos,\n\t\t\t\t    s->buffer, trace_seq_used(s));\n\n\tkfree(s);\n\n\treturn r;\n}\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_pid_list *no_pid_list;\n\n\t \n\tpid_list = rcu_dereference_protected(tr->filtered_pids,\n\t\t\t\t\t     mutex_is_locked(&event_mutex));\n\tno_pid_list = rcu_dereference_protected(tr->filtered_no_pids,\n\t\t\t\t\t     mutex_is_locked(&event_mutex));\n\n\tthis_cpu_write(tr->array_buffer.data->ignore_pid,\n\t\t       trace_ignore_this_task(pid_list, no_pid_list, current));\n}\n\nstatic void register_pid_events(struct trace_array *tr)\n{\n\t \n\tregister_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_pre,\n\t\t\t\t\t tr, INT_MAX);\n\tregister_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_post,\n\t\t\t\t\t tr, 0);\n\n\tregister_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre,\n\t\t\t\t\t tr, INT_MAX);\n\tregister_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,\n\t\t\t\t\t tr, 0);\n\n\tregister_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_pre,\n\t\t\t\t\t     tr, INT_MAX);\n\tregister_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_post,\n\t\t\t\t\t     tr, 0);\n\n\tregister_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_pre,\n\t\t\t\t\t tr, INT_MAX);\n\tregister_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_post,\n\t\t\t\t\t tr, 0);\n}\n\nstatic ssize_t\nevent_pid_write(struct file *filp, const char __user *ubuf,\n\t\tsize_t cnt, loff_t *ppos, int type)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tstruct trace_pid_list *filtered_pids = NULL;\n\tstruct trace_pid_list *other_pids = NULL;\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_event_file *file;\n\tssize_t ret;\n\n\tif (!cnt)\n\t\treturn 0;\n\n\tret = tracing_update_buffers();\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmutex_lock(&event_mutex);\n\n\tif (type == TRACE_PIDS) {\n\t\tfiltered_pids = rcu_dereference_protected(tr->filtered_pids,\n\t\t\t\t\t\t\t  lockdep_is_held(&event_mutex));\n\t\tother_pids = rcu_dereference_protected(tr->filtered_no_pids,\n\t\t\t\t\t\t\t  lockdep_is_held(&event_mutex));\n\t} else {\n\t\tfiltered_pids = rcu_dereference_protected(tr->filtered_no_pids,\n\t\t\t\t\t\t\t  lockdep_is_held(&event_mutex));\n\t\tother_pids = rcu_dereference_protected(tr->filtered_pids,\n\t\t\t\t\t\t\t  lockdep_is_held(&event_mutex));\n\t}\n\n\tret = trace_pid_write(filtered_pids, &pid_list, ubuf, cnt);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (type == TRACE_PIDS)\n\t\trcu_assign_pointer(tr->filtered_pids, pid_list);\n\telse\n\t\trcu_assign_pointer(tr->filtered_no_pids, pid_list);\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tset_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);\n\t}\n\n\tif (filtered_pids) {\n\t\ttracepoint_synchronize_unregister();\n\t\ttrace_pid_list_free(filtered_pids);\n\t} else if (pid_list && !other_pids) {\n\t\tregister_pid_events(tr);\n\t}\n\n\t \n\ton_each_cpu(ignore_task_cpu, tr, 1);\n\n out:\n\tmutex_unlock(&event_mutex);\n\n\tif (ret > 0)\n\t\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\nftrace_event_pid_write(struct file *filp, const char __user *ubuf,\n\t\t       size_t cnt, loff_t *ppos)\n{\n\treturn event_pid_write(filp, ubuf, cnt, ppos, TRACE_PIDS);\n}\n\nstatic ssize_t\nftrace_event_npid_write(struct file *filp, const char __user *ubuf,\n\t\t\tsize_t cnt, loff_t *ppos)\n{\n\treturn event_pid_write(filp, ubuf, cnt, ppos, TRACE_NO_PIDS);\n}\n\nstatic int ftrace_event_avail_open(struct inode *inode, struct file *file);\nstatic int ftrace_event_set_open(struct inode *inode, struct file *file);\nstatic int ftrace_event_set_pid_open(struct inode *inode, struct file *file);\nstatic int ftrace_event_set_npid_open(struct inode *inode, struct file *file);\nstatic int ftrace_event_release(struct inode *inode, struct file *file);\n\nstatic const struct seq_operations show_event_seq_ops = {\n\t.start = t_start,\n\t.next = t_next,\n\t.show = t_show,\n\t.stop = t_stop,\n};\n\nstatic const struct seq_operations show_set_event_seq_ops = {\n\t.start = s_start,\n\t.next = s_next,\n\t.show = t_show,\n\t.stop = t_stop,\n};\n\nstatic const struct seq_operations show_set_pid_seq_ops = {\n\t.start = p_start,\n\t.next = p_next,\n\t.show = trace_pid_show,\n\t.stop = p_stop,\n};\n\nstatic const struct seq_operations show_set_no_pid_seq_ops = {\n\t.start = np_start,\n\t.next = np_next,\n\t.show = trace_pid_show,\n\t.stop = p_stop,\n};\n\nstatic const struct file_operations ftrace_avail_fops = {\n\t.open = ftrace_event_avail_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\n\nstatic const struct file_operations ftrace_set_event_fops = {\n\t.open = ftrace_event_set_open,\n\t.read = seq_read,\n\t.write = ftrace_event_write,\n\t.llseek = seq_lseek,\n\t.release = ftrace_event_release,\n};\n\nstatic const struct file_operations ftrace_set_event_pid_fops = {\n\t.open = ftrace_event_set_pid_open,\n\t.read = seq_read,\n\t.write = ftrace_event_pid_write,\n\t.llseek = seq_lseek,\n\t.release = ftrace_event_release,\n};\n\nstatic const struct file_operations ftrace_set_event_notrace_pid_fops = {\n\t.open = ftrace_event_set_npid_open,\n\t.read = seq_read,\n\t.write = ftrace_event_npid_write,\n\t.llseek = seq_lseek,\n\t.release = ftrace_event_release,\n};\n\nstatic const struct file_operations ftrace_enable_fops = {\n\t.open = tracing_open_file_tr,\n\t.read = event_enable_read,\n\t.write = event_enable_write,\n\t.release = tracing_release_file_tr,\n\t.llseek = default_llseek,\n};\n\nstatic const struct file_operations ftrace_event_format_fops = {\n\t.open = trace_format_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release,\n};\n\nstatic const struct file_operations ftrace_event_id_fops = {\n\t.read = event_id_read,\n\t.llseek = default_llseek,\n};\n\nstatic const struct file_operations ftrace_event_filter_fops = {\n\t.open = tracing_open_file_tr,\n\t.read = event_filter_read,\n\t.write = event_filter_write,\n\t.release = tracing_release_file_tr,\n\t.llseek = default_llseek,\n};\n\nstatic const struct file_operations ftrace_subsystem_filter_fops = {\n\t.open = subsystem_open,\n\t.read = subsystem_filter_read,\n\t.write = subsystem_filter_write,\n\t.llseek = default_llseek,\n\t.release = subsystem_release,\n};\n\nstatic const struct file_operations ftrace_system_enable_fops = {\n\t.open = subsystem_open,\n\t.read = system_enable_read,\n\t.write = system_enable_write,\n\t.llseek = default_llseek,\n\t.release = subsystem_release,\n};\n\nstatic const struct file_operations ftrace_tr_enable_fops = {\n\t.open = system_tr_open,\n\t.read = system_enable_read,\n\t.write = system_enable_write,\n\t.llseek = default_llseek,\n\t.release = subsystem_release,\n};\n\nstatic const struct file_operations ftrace_show_header_fops = {\n\t.open = tracing_open_generic,\n\t.read = show_header,\n\t.llseek = default_llseek,\n};\n\nstatic int\nftrace_event_open(struct inode *inode, struct file *file,\n\t\t  const struct seq_operations *seq_ops)\n{\n\tstruct seq_file *m;\n\tint ret;\n\n\tret = security_locked_down(LOCKDOWN_TRACEFS);\n\tif (ret)\n\t\treturn ret;\n\n\tret = seq_open(file, seq_ops);\n\tif (ret < 0)\n\t\treturn ret;\n\tm = file->private_data;\n\t \n\tm->private = inode->i_private;\n\n\treturn ret;\n}\n\nstatic int ftrace_event_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn seq_release(inode, file);\n}\n\nstatic int\nftrace_event_avail_open(struct inode *inode, struct file *file)\n{\n\tconst struct seq_operations *seq_ops = &show_event_seq_ops;\n\n\t \n\treturn ftrace_event_open(inode, file, seq_ops);\n}\n\nstatic int\nftrace_event_set_open(struct inode *inode, struct file *file)\n{\n\tconst struct seq_operations *seq_ops = &show_set_event_seq_ops;\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((file->f_mode & FMODE_WRITE) &&\n\t    (file->f_flags & O_TRUNC))\n\t\tftrace_clear_events(tr);\n\n\tret = ftrace_event_open(inode, file, seq_ops);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\treturn ret;\n}\n\nstatic int\nftrace_event_set_pid_open(struct inode *inode, struct file *file)\n{\n\tconst struct seq_operations *seq_ops = &show_set_pid_seq_ops;\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((file->f_mode & FMODE_WRITE) &&\n\t    (file->f_flags & O_TRUNC))\n\t\tftrace_clear_event_pids(tr, TRACE_PIDS);\n\n\tret = ftrace_event_open(inode, file, seq_ops);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\treturn ret;\n}\n\nstatic int\nftrace_event_set_npid_open(struct inode *inode, struct file *file)\n{\n\tconst struct seq_operations *seq_ops = &show_set_no_pid_seq_ops;\n\tstruct trace_array *tr = inode->i_private;\n\tint ret;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((file->f_mode & FMODE_WRITE) &&\n\t    (file->f_flags & O_TRUNC))\n\t\tftrace_clear_event_pids(tr, TRACE_NO_PIDS);\n\n\tret = ftrace_event_open(inode, file, seq_ops);\n\tif (ret < 0)\n\t\ttrace_array_put(tr);\n\treturn ret;\n}\n\nstatic struct event_subsystem *\ncreate_new_subsystem(const char *name)\n{\n\tstruct event_subsystem *system;\n\n\t \n\tsystem = kmalloc(sizeof(*system), GFP_KERNEL);\n\tif (!system)\n\t\treturn NULL;\n\n\tsystem->ref_count = 1;\n\n\t \n\tsystem->name = kstrdup_const(name, GFP_KERNEL);\n\tif (!system->name)\n\t\tgoto out_free;\n\n\tsystem->filter = kzalloc(sizeof(struct event_filter), GFP_KERNEL);\n\tif (!system->filter)\n\t\tgoto out_free;\n\n\tlist_add(&system->list, &event_subsystems);\n\n\treturn system;\n\n out_free:\n\tkfree_const(system->name);\n\tkfree(system);\n\treturn NULL;\n}\n\nstatic struct eventfs_file *\nevent_subsystem_dir(struct trace_array *tr, const char *name,\n\t\t    struct trace_event_file *file, struct dentry *parent)\n{\n\tstruct event_subsystem *system, *iter;\n\tstruct trace_subsystem_dir *dir;\n\tstruct eventfs_file *ef;\n\tint res;\n\n\t \n\tlist_for_each_entry(dir, &tr->systems, list) {\n\t\tsystem = dir->subsystem;\n\t\tif (strcmp(system->name, name) == 0) {\n\t\t\tdir->nr_events++;\n\t\t\tfile->system = dir;\n\t\t\treturn dir->ef;\n\t\t}\n\t}\n\n\t \n\tsystem = NULL;\n\tlist_for_each_entry(iter, &event_subsystems, list) {\n\t\tif (strcmp(iter->name, name) == 0) {\n\t\t\tsystem = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdir = kmalloc(sizeof(*dir), GFP_KERNEL);\n\tif (!dir)\n\t\tgoto out_fail;\n\n\tif (!system) {\n\t\tsystem = create_new_subsystem(name);\n\t\tif (!system)\n\t\t\tgoto out_free;\n\t} else\n\t\t__get_system(system);\n\n\tef = eventfs_add_subsystem_dir(name, parent);\n\tif (IS_ERR(ef)) {\n\t\tpr_warn(\"Failed to create system directory %s\\n\", name);\n\t\t__put_system(system);\n\t\tgoto out_free;\n\t}\n\n\tdir->ef = ef;\n\tdir->tr = tr;\n\tdir->ref_count = 1;\n\tdir->nr_events = 1;\n\tdir->subsystem = system;\n\tfile->system = dir;\n\n\t \n\tif (strcmp(name, \"ftrace\") != 0) {\n\n\t\tres = eventfs_add_file(\"filter\", TRACE_MODE_WRITE,\n\t\t\t\t\t    dir->ef, dir,\n\t\t\t\t\t    &ftrace_subsystem_filter_fops);\n\t\tif (res) {\n\t\t\tkfree(system->filter);\n\t\t\tsystem->filter = NULL;\n\t\t\tpr_warn(\"Could not create tracefs '%s/filter' entry\\n\", name);\n\t\t}\n\n\t\teventfs_add_file(\"enable\", TRACE_MODE_WRITE, dir->ef, dir,\n\t\t\t\t  &ftrace_system_enable_fops);\n\t}\n\n\tlist_add(&dir->list, &tr->systems);\n\n\treturn dir->ef;\n\n out_free:\n\tkfree(dir);\n out_fail:\n\t \n\tif (!dir || !system)\n\t\tpr_warn(\"No memory to create event subsystem %s\\n\", name);\n\treturn NULL;\n}\n\nstatic int\nevent_define_fields(struct trace_event_call *call)\n{\n\tstruct list_head *head;\n\tint ret = 0;\n\n\t \n\thead = trace_get_fields(call);\n\tif (list_empty(head)) {\n\t\tstruct trace_event_fields *field = call->class->fields_array;\n\t\tunsigned int offset = sizeof(struct trace_entry);\n\n\t\tfor (; field->type; field++) {\n\t\t\tif (field->type == TRACE_FUNCTION_TYPE) {\n\t\t\t\tfield->define_fields(call);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\toffset = ALIGN(offset, field->align);\n\t\t\tret = trace_define_field_ext(call, field->type, field->name,\n\t\t\t\t\t\t offset, field->size,\n\t\t\t\t\t\t field->is_signed, field->filter_type,\n\t\t\t\t\t\t field->len);\n\t\t\tif (WARN_ON_ONCE(ret)) {\n\t\t\t\tpr_err(\"error code is %d\\n\", ret);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\toffset += field->size;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int\nevent_create_dir(struct dentry *parent, struct trace_event_file *file)\n{\n\tstruct trace_event_call *call = file->event_call;\n\tstruct eventfs_file *ef_subsystem = NULL;\n\tstruct trace_array *tr = file->tr;\n\tstruct eventfs_file *ef;\n\tconst char *name;\n\tint ret;\n\n\t \n\tif (WARN_ON_ONCE(strcmp(call->class->system, TRACE_SYSTEM) == 0))\n\t\treturn -ENODEV;\n\n\tef_subsystem = event_subsystem_dir(tr, call->class->system, file, parent);\n\tif (!ef_subsystem)\n\t\treturn -ENOMEM;\n\n\tname = trace_event_name(call);\n\tef = eventfs_add_dir(name, ef_subsystem);\n\tif (IS_ERR(ef)) {\n\t\tpr_warn(\"Could not create tracefs '%s' directory\\n\", name);\n\t\treturn -1;\n\t}\n\n\tfile->ef = ef;\n\n\tif (call->class->reg && !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))\n\t\teventfs_add_file(\"enable\", TRACE_MODE_WRITE, file->ef, file,\n\t\t\t\t  &ftrace_enable_fops);\n\n#ifdef CONFIG_PERF_EVENTS\n\tif (call->event.type && call->class->reg)\n\t\teventfs_add_file(\"id\", TRACE_MODE_READ, file->ef,\n\t\t\t\t  (void *)(long)call->event.type,\n\t\t\t\t  &ftrace_event_id_fops);\n#endif\n\n\tret = event_define_fields(call);\n\tif (ret < 0) {\n\t\tpr_warn(\"Could not initialize trace point events/%s\\n\", name);\n\t\treturn ret;\n\t}\n\n\t \n\tif (!(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)) {\n\t\teventfs_add_file(\"filter\", TRACE_MODE_WRITE, file->ef,\n\t\t\t\t  file, &ftrace_event_filter_fops);\n\n\t\teventfs_add_file(\"trigger\", TRACE_MODE_WRITE, file->ef,\n\t\t\t\t  file, &event_trigger_fops);\n\t}\n\n#ifdef CONFIG_HIST_TRIGGERS\n\teventfs_add_file(\"hist\", TRACE_MODE_READ, file->ef, file,\n\t\t\t  &event_hist_fops);\n#endif\n#ifdef CONFIG_HIST_TRIGGERS_DEBUG\n\teventfs_add_file(\"hist_debug\", TRACE_MODE_READ, file->ef, file,\n\t\t\t  &event_hist_debug_fops);\n#endif\n\teventfs_add_file(\"format\", TRACE_MODE_READ, file->ef, call,\n\t\t\t  &ftrace_event_format_fops);\n\n#ifdef CONFIG_TRACE_EVENT_INJECT\n\tif (call->event.type && call->class->reg)\n\t\teventfs_add_file(\"inject\", 0200, file->ef, file,\n\t\t\t\t  &event_inject_fops);\n#endif\n\n\treturn 0;\n}\n\nstatic void remove_event_from_tracers(struct trace_event_call *call)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_array *tr;\n\n\tdo_for_each_event_file_safe(tr, file) {\n\t\tif (file->event_call != call)\n\t\t\tcontinue;\n\n\t\tremove_event_file_dir(file);\n\t\t \n\t\tbreak;\n\t} while_for_each_event_file();\n}\n\nstatic void event_remove(struct trace_event_call *call)\n{\n\tstruct trace_array *tr;\n\tstruct trace_event_file *file;\n\n\tdo_for_each_event_file(tr, file) {\n\t\tif (file->event_call != call)\n\t\t\tcontinue;\n\n\t\tif (file->flags & EVENT_FILE_FL_WAS_ENABLED)\n\t\t\ttr->clear_trace = true;\n\n\t\tftrace_event_enable_disable(file, 0);\n\t\t \n\t\tbreak;\n\t} while_for_each_event_file();\n\n\tif (call->event.funcs)\n\t\t__unregister_trace_event(&call->event);\n\tremove_event_from_tracers(call);\n\tlist_del(&call->list);\n}\n\nstatic int event_init(struct trace_event_call *call)\n{\n\tint ret = 0;\n\tconst char *name;\n\n\tname = trace_event_name(call);\n\tif (WARN_ON(!name))\n\t\treturn -EINVAL;\n\n\tif (call->class->raw_init) {\n\t\tret = call->class->raw_init(call);\n\t\tif (ret < 0 && ret != -ENOSYS)\n\t\t\tpr_warn(\"Could not initialize trace events/%s\\n\", name);\n\t}\n\n\treturn ret;\n}\n\nstatic int\n__register_event(struct trace_event_call *call, struct module *mod)\n{\n\tint ret;\n\n\tret = event_init(call);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tlist_add(&call->list, &ftrace_events);\n\tif (call->flags & TRACE_EVENT_FL_DYNAMIC)\n\t\tatomic_set(&call->refcnt, 0);\n\telse\n\t\tcall->module = mod;\n\n\treturn 0;\n}\n\nstatic char *eval_replace(char *ptr, struct trace_eval_map *map, int len)\n{\n\tint rlen;\n\tint elen;\n\n\t \n\telen = snprintf(ptr, 0, \"%ld\", map->eval_value);\n\t \n\tif (len < elen)\n\t\treturn NULL;\n\n\tsnprintf(ptr, elen + 1, \"%ld\", map->eval_value);\n\n\t \n\trlen = strlen(ptr + len);\n\tmemmove(ptr + elen, ptr + len, rlen);\n\t \n\tptr[elen + rlen] = 0;\n\n\treturn ptr + elen;\n}\n\nstatic void update_event_printk(struct trace_event_call *call,\n\t\t\t\tstruct trace_eval_map *map)\n{\n\tchar *ptr;\n\tint quote = 0;\n\tint len = strlen(map->eval_string);\n\n\tfor (ptr = call->print_fmt; *ptr; ptr++) {\n\t\tif (*ptr == '\\\\') {\n\t\t\tptr++;\n\t\t\t \n\t\t\tif (!*ptr)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\t\tif (*ptr == '\"') {\n\t\t\tquote ^= 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (quote)\n\t\t\tcontinue;\n\t\tif (isdigit(*ptr)) {\n\t\t\t \n\t\t\tdo {\n\t\t\t\tptr++;\n\t\t\t\t \n\t\t\t} while (isalnum(*ptr));\n\t\t\tif (!*ptr)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\tif (isalpha(*ptr) || *ptr == '_') {\n\t\t\tif (strncmp(map->eval_string, ptr, len) == 0 &&\n\t\t\t    !isalnum(ptr[len]) && ptr[len] != '_') {\n\t\t\t\tptr = eval_replace(ptr, map, len);\n\t\t\t\t \n\t\t\t\tif (WARN_ON_ONCE(!ptr))\n\t\t\t\t\treturn;\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\tskip_more:\n\t\t\tdo {\n\t\t\t\tptr++;\n\t\t\t} while (isalnum(*ptr) || *ptr == '_');\n\t\t\tif (!*ptr)\n\t\t\t\tbreak;\n\t\t\t \n\t\t\tif (*ptr == '.' || (ptr[0] == '-' && ptr[1] == '>')) {\n\t\t\t\tptr += *ptr == '.' ? 1 : 2;\n\t\t\t\tif (!*ptr)\n\t\t\t\t\tbreak;\n\t\t\t\tgoto skip_more;\n\t\t\t}\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t}\n}\n\nstatic void add_str_to_module(struct module *module, char *str)\n{\n\tstruct module_string *modstr;\n\n\tmodstr = kmalloc(sizeof(*modstr), GFP_KERNEL);\n\n\t \n\tif (WARN_ON_ONCE(!modstr))\n\t\treturn;\n\n\tmodstr->module = module;\n\tmodstr->str = str;\n\n\tlist_add(&modstr->next, &module_strings);\n}\n\nstatic void update_event_fields(struct trace_event_call *call,\n\t\t\t\tstruct trace_eval_map *map)\n{\n\tstruct ftrace_event_field *field;\n\tstruct list_head *head;\n\tchar *ptr;\n\tchar *str;\n\tint len = strlen(map->eval_string);\n\n\t \n\tif (WARN_ON_ONCE(call->flags & TRACE_EVENT_FL_DYNAMIC))\n\t\treturn;\n\n\thead = trace_get_fields(call);\n\tlist_for_each_entry(field, head, link) {\n\t\tptr = strchr(field->type, '[');\n\t\tif (!ptr)\n\t\t\tcontinue;\n\t\tptr++;\n\n\t\tif (!isalpha(*ptr) && *ptr != '_')\n\t\t\tcontinue;\n\n\t\tif (strncmp(map->eval_string, ptr, len) != 0)\n\t\t\tcontinue;\n\n\t\tstr = kstrdup(field->type, GFP_KERNEL);\n\t\tif (WARN_ON_ONCE(!str))\n\t\t\treturn;\n\t\tptr = str + (ptr - field->type);\n\t\tptr = eval_replace(ptr, map, len);\n\t\t \n\t\tif (WARN_ON_ONCE(!ptr)) {\n\t\t\tkfree(str);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (call->module)\n\t\t\tadd_str_to_module(call->module, str);\n\n\t\tfield->type = str;\n\t}\n}\n\nvoid trace_event_eval_update(struct trace_eval_map **map, int len)\n{\n\tstruct trace_event_call *call, *p;\n\tconst char *last_system = NULL;\n\tbool first = false;\n\tint last_i;\n\tint i;\n\n\tdown_write(&trace_event_sem);\n\tlist_for_each_entry_safe(call, p, &ftrace_events, list) {\n\t\t \n\t\tif (!last_system || call->class->system != last_system) {\n\t\t\tfirst = true;\n\t\t\tlast_i = 0;\n\t\t\tlast_system = call->class->system;\n\t\t}\n\n\t\t \n\t\tfor (i = last_i; i < len; i++) {\n\t\t\tif (call->class->system == map[i]->system) {\n\t\t\t\t \n\t\t\t\tif (first) {\n\t\t\t\t\tlast_i = i;\n\t\t\t\t\tfirst = false;\n\t\t\t\t}\n\t\t\t\tupdate_event_printk(call, map[i]);\n\t\t\t\tupdate_event_fields(call, map[i]);\n\t\t\t}\n\t\t}\n\t\tcond_resched();\n\t}\n\tup_write(&trace_event_sem);\n}\n\nstatic struct trace_event_file *\ntrace_create_new_event(struct trace_event_call *call,\n\t\t       struct trace_array *tr)\n{\n\tstruct trace_pid_list *no_pid_list;\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_event_file *file;\n\tunsigned int first;\n\n\tfile = kmem_cache_alloc(file_cachep, GFP_TRACE);\n\tif (!file)\n\t\treturn NULL;\n\n\tpid_list = rcu_dereference_protected(tr->filtered_pids,\n\t\t\t\t\t     lockdep_is_held(&event_mutex));\n\tno_pid_list = rcu_dereference_protected(tr->filtered_no_pids,\n\t\t\t\t\t     lockdep_is_held(&event_mutex));\n\n\tif (!trace_pid_list_first(pid_list, &first) ||\n\t    !trace_pid_list_first(no_pid_list, &first))\n\t\tfile->flags |= EVENT_FILE_FL_PID_FILTER;\n\n\tfile->event_call = call;\n\tfile->tr = tr;\n\tatomic_set(&file->sm_ref, 0);\n\tatomic_set(&file->tm_ref, 0);\n\tINIT_LIST_HEAD(&file->triggers);\n\tlist_add(&file->list, &tr->events);\n\tevent_file_get(file);\n\n\treturn file;\n}\n\n#define MAX_BOOT_TRIGGERS 32\n\nstatic struct boot_triggers {\n\tconst char\t\t*event;\n\tchar\t\t\t*trigger;\n} bootup_triggers[MAX_BOOT_TRIGGERS];\n\nstatic char bootup_trigger_buf[COMMAND_LINE_SIZE];\nstatic int nr_boot_triggers;\n\nstatic __init int setup_trace_triggers(char *str)\n{\n\tchar *trigger;\n\tchar *buf;\n\tint i;\n\n\tstrscpy(bootup_trigger_buf, str, COMMAND_LINE_SIZE);\n\tring_buffer_expanded = true;\n\tdisable_tracing_selftest(\"running event triggers\");\n\n\tbuf = bootup_trigger_buf;\n\tfor (i = 0; i < MAX_BOOT_TRIGGERS; i++) {\n\t\ttrigger = strsep(&buf, \",\");\n\t\tif (!trigger)\n\t\t\tbreak;\n\t\tbootup_triggers[i].event = strsep(&trigger, \".\");\n\t\tbootup_triggers[i].trigger = trigger;\n\t\tif (!bootup_triggers[i].trigger)\n\t\t\tbreak;\n\t}\n\n\tnr_boot_triggers = i;\n\treturn 1;\n}\n__setup(\"trace_trigger=\", setup_trace_triggers);\n\n \nstatic int\n__trace_add_new_event(struct trace_event_call *call, struct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\n\tfile = trace_create_new_event(call, tr);\n\tif (!file)\n\t\treturn -ENOMEM;\n\n\tif (eventdir_initialized)\n\t\treturn event_create_dir(tr->event_dir, file);\n\telse\n\t\treturn event_define_fields(call);\n}\n\nstatic void trace_early_triggers(struct trace_event_file *file, const char *name)\n{\n\tint ret;\n\tint i;\n\n\tfor (i = 0; i < nr_boot_triggers; i++) {\n\t\tif (strcmp(name, bootup_triggers[i].event))\n\t\t\tcontinue;\n\t\tmutex_lock(&event_mutex);\n\t\tret = trigger_process_regex(file, bootup_triggers[i].trigger);\n\t\tmutex_unlock(&event_mutex);\n\t\tif (ret)\n\t\t\tpr_err(\"Failed to register trigger '%s' on event %s\\n\",\n\t\t\t       bootup_triggers[i].trigger,\n\t\t\t       bootup_triggers[i].event);\n\t}\n}\n\n \nstatic int\n__trace_early_add_new_event(struct trace_event_call *call,\n\t\t\t    struct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\tint ret;\n\n\tfile = trace_create_new_event(call, tr);\n\tif (!file)\n\t\treturn -ENOMEM;\n\n\tret = event_define_fields(call);\n\tif (ret)\n\t\treturn ret;\n\n\ttrace_early_triggers(file, trace_event_name(call));\n\n\treturn 0;\n}\n\nstruct ftrace_module_file_ops;\nstatic void __add_event_to_tracers(struct trace_event_call *call);\n\n \nint trace_add_event_call(struct trace_event_call *call)\n{\n\tint ret;\n\tlockdep_assert_held(&event_mutex);\n\n\tmutex_lock(&trace_types_lock);\n\n\tret = __register_event(call, NULL);\n\tif (ret >= 0)\n\t\t__add_event_to_tracers(call);\n\n\tmutex_unlock(&trace_types_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(trace_add_event_call);\n\n \nstatic void __trace_remove_event_call(struct trace_event_call *call)\n{\n\tevent_remove(call);\n\ttrace_destroy_fields(call);\n\tfree_event_filter(call->filter);\n\tcall->filter = NULL;\n}\n\nstatic int probe_remove_event_call(struct trace_event_call *call)\n{\n\tstruct trace_array *tr;\n\tstruct trace_event_file *file;\n\n#ifdef CONFIG_PERF_EVENTS\n\tif (call->perf_refcount)\n\t\treturn -EBUSY;\n#endif\n\tdo_for_each_event_file(tr, file) {\n\t\tif (file->event_call != call)\n\t\t\tcontinue;\n\t\t \n\t\tif (file->flags & EVENT_FILE_FL_ENABLED)\n\t\t\tgoto busy;\n\n\t\tif (file->flags & EVENT_FILE_FL_WAS_ENABLED)\n\t\t\ttr->clear_trace = true;\n\t\t \n\t\tbreak;\n\t} while_for_each_event_file();\n\n\t__trace_remove_event_call(call);\n\n\treturn 0;\n busy:\n\t \n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\ttr->clear_trace = false;\n\t}\n\treturn -EBUSY;\n}\n\n \nint trace_remove_event_call(struct trace_event_call *call)\n{\n\tint ret;\n\n\tlockdep_assert_held(&event_mutex);\n\n\tmutex_lock(&trace_types_lock);\n\tdown_write(&trace_event_sem);\n\tret = probe_remove_event_call(call);\n\tup_write(&trace_event_sem);\n\tmutex_unlock(&trace_types_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(trace_remove_event_call);\n\n#define for_each_event(event, start, end)\t\t\t\\\n\tfor (event = start;\t\t\t\t\t\\\n\t     (unsigned long)event < (unsigned long)end;\t\t\\\n\t     event++)\n\n#ifdef CONFIG_MODULES\n\nstatic void trace_module_add_events(struct module *mod)\n{\n\tstruct trace_event_call **call, **start, **end;\n\n\tif (!mod->num_trace_events)\n\t\treturn;\n\n\t \n\tif (trace_module_has_bad_taint(mod)) {\n\t\tpr_err(\"%s: module has bad taint, not creating trace events\\n\",\n\t\t       mod->name);\n\t\treturn;\n\t}\n\n\tstart = mod->trace_events;\n\tend = mod->trace_events + mod->num_trace_events;\n\n\tfor_each_event(call, start, end) {\n\t\t__register_event(*call, mod);\n\t\t__add_event_to_tracers(*call);\n\t}\n}\n\nstatic void trace_module_remove_events(struct module *mod)\n{\n\tstruct trace_event_call *call, *p;\n\tstruct module_string *modstr, *m;\n\n\tdown_write(&trace_event_sem);\n\tlist_for_each_entry_safe(call, p, &ftrace_events, list) {\n\t\tif ((call->flags & TRACE_EVENT_FL_DYNAMIC) || !call->module)\n\t\t\tcontinue;\n\t\tif (call->module == mod)\n\t\t\t__trace_remove_event_call(call);\n\t}\n\t \n\tlist_for_each_entry_safe(modstr, m, &module_strings, next) {\n\t\tif (modstr->module != mod)\n\t\t\tcontinue;\n\t\tlist_del(&modstr->next);\n\t\tkfree(modstr->str);\n\t\tkfree(modstr);\n\t}\n\tup_write(&trace_event_sem);\n\n\t \n\ttracing_reset_all_online_cpus_unlocked();\n}\n\nstatic int trace_module_notify(struct notifier_block *self,\n\t\t\t       unsigned long val, void *data)\n{\n\tstruct module *mod = data;\n\n\tmutex_lock(&event_mutex);\n\tmutex_lock(&trace_types_lock);\n\tswitch (val) {\n\tcase MODULE_STATE_COMING:\n\t\ttrace_module_add_events(mod);\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\ttrace_module_remove_events(mod);\n\t\tbreak;\n\t}\n\tmutex_unlock(&trace_types_lock);\n\tmutex_unlock(&event_mutex);\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block trace_module_nb = {\n\t.notifier_call = trace_module_notify,\n\t.priority = 1,  \n};\n#endif  \n\n \nstatic void\n__trace_add_event_dirs(struct trace_array *tr)\n{\n\tstruct trace_event_call *call;\n\tint ret;\n\n\tlist_for_each_entry(call, &ftrace_events, list) {\n\t\tret = __trace_add_new_event(call, tr);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Could not create directory for event %s\\n\",\n\t\t\t\ttrace_event_name(call));\n\t}\n}\n\n \nstruct trace_event_file *\n__find_event_file(struct trace_array *tr, const char *system, const char *event)\n{\n\tstruct trace_event_file *file;\n\tstruct trace_event_call *call;\n\tconst char *name;\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\n\t\tcall = file->event_call;\n\t\tname = trace_event_name(call);\n\n\t\tif (!name || !call->class)\n\t\t\tcontinue;\n\n\t\tif (strcmp(event, name) == 0 &&\n\t\t    strcmp(system, call->class->system) == 0)\n\t\t\treturn file;\n\t}\n\treturn NULL;\n}\n\n \nstruct trace_event_file *\nfind_event_file(struct trace_array *tr, const char *system, const char *event)\n{\n\tstruct trace_event_file *file;\n\n\tfile = __find_event_file(tr, system, event);\n\tif (!file || !file->event_call->class->reg ||\n\t    file->event_call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)\n\t\treturn NULL;\n\n\treturn file;\n}\n\n \nstruct trace_event_file *trace_get_event_file(const char *instance,\n\t\t\t\t\t      const char *system,\n\t\t\t\t\t      const char *event)\n{\n\tstruct trace_array *tr = top_trace_array();\n\tstruct trace_event_file *file = NULL;\n\tint ret = -EINVAL;\n\n\tif (instance) {\n\t\ttr = trace_array_find_get(instance);\n\t\tif (!tr)\n\t\t\treturn ERR_PTR(-ENOENT);\n\t} else {\n\t\tret = trace_array_get(tr);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\tmutex_lock(&event_mutex);\n\n\tfile = find_event_file(tr, system, event);\n\tif (!file) {\n\t\ttrace_array_put(tr);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tret = trace_event_try_get_ref(file->event_call);\n\tif (!ret) {\n\t\ttrace_array_put(tr);\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tret = 0;\n out:\n\tmutex_unlock(&event_mutex);\n\n\tif (ret)\n\t\tfile = ERR_PTR(ret);\n\n\treturn file;\n}\nEXPORT_SYMBOL_GPL(trace_get_event_file);\n\n \nvoid trace_put_event_file(struct trace_event_file *file)\n{\n\tmutex_lock(&event_mutex);\n\ttrace_event_put_ref(file->event_call);\n\tmutex_unlock(&event_mutex);\n\n\ttrace_array_put(file->tr);\n}\nEXPORT_SYMBOL_GPL(trace_put_event_file);\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\n \n#define ENABLE_EVENT_STR\t\"enable_event\"\n#define DISABLE_EVENT_STR\t\"disable_event\"\n\nstruct event_probe_data {\n\tstruct trace_event_file\t*file;\n\tunsigned long\t\t\tcount;\n\tint\t\t\t\tref;\n\tbool\t\t\t\tenable;\n};\n\nstatic void update_event_probe(struct event_probe_data *data)\n{\n\tif (data->enable)\n\t\tclear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);\n\telse\n\t\tset_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);\n}\n\nstatic void\nevent_enable_probe(unsigned long ip, unsigned long parent_ip,\n\t\t   struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t   void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tstruct event_probe_data *edata;\n\tvoid **pdata;\n\n\tpdata = ftrace_func_mapper_find_ip(mapper, ip);\n\tif (!pdata || !*pdata)\n\t\treturn;\n\n\tedata = *pdata;\n\tupdate_event_probe(edata);\n}\n\nstatic void\nevent_enable_count_probe(unsigned long ip, unsigned long parent_ip,\n\t\t\t struct trace_array *tr, struct ftrace_probe_ops *ops,\n\t\t\t void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tstruct event_probe_data *edata;\n\tvoid **pdata;\n\n\tpdata = ftrace_func_mapper_find_ip(mapper, ip);\n\tif (!pdata || !*pdata)\n\t\treturn;\n\n\tedata = *pdata;\n\n\tif (!edata->count)\n\t\treturn;\n\n\t \n\tif (edata->enable == !(edata->file->flags & EVENT_FILE_FL_SOFT_DISABLED))\n\t\treturn;\n\n\tif (edata->count != -1)\n\t\t(edata->count)--;\n\n\tupdate_event_probe(edata);\n}\n\nstatic int\nevent_enable_print(struct seq_file *m, unsigned long ip,\n\t\t   struct ftrace_probe_ops *ops, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tstruct event_probe_data *edata;\n\tvoid **pdata;\n\n\tpdata = ftrace_func_mapper_find_ip(mapper, ip);\n\n\tif (WARN_ON_ONCE(!pdata || !*pdata))\n\t\treturn 0;\n\n\tedata = *pdata;\n\n\tseq_printf(m, \"%ps:\", (void *)ip);\n\n\tseq_printf(m, \"%s:%s:%s\",\n\t\t   edata->enable ? ENABLE_EVENT_STR : DISABLE_EVENT_STR,\n\t\t   edata->file->event_call->class->system,\n\t\t   trace_event_name(edata->file->event_call));\n\n\tif (edata->count == -1)\n\t\tseq_puts(m, \":unlimited\\n\");\n\telse\n\t\tseq_printf(m, \":count=%ld\\n\", edata->count);\n\n\treturn 0;\n}\n\nstatic int\nevent_enable_init(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t  unsigned long ip, void *init_data, void **data)\n{\n\tstruct ftrace_func_mapper *mapper = *data;\n\tstruct event_probe_data *edata = init_data;\n\tint ret;\n\n\tif (!mapper) {\n\t\tmapper = allocate_ftrace_func_mapper();\n\t\tif (!mapper)\n\t\t\treturn -ENODEV;\n\t\t*data = mapper;\n\t}\n\n\tret = ftrace_func_mapper_add_ip(mapper, ip, edata);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tedata->ref++;\n\n\treturn 0;\n}\n\nstatic int free_probe_data(void *data)\n{\n\tstruct event_probe_data *edata = data;\n\n\tedata->ref--;\n\tif (!edata->ref) {\n\t\t \n\t\t__ftrace_event_enable_disable(edata->file, 0, 1);\n\t\ttrace_event_put_ref(edata->file->event_call);\n\t\tkfree(edata);\n\t}\n\treturn 0;\n}\n\nstatic void\nevent_enable_free(struct ftrace_probe_ops *ops, struct trace_array *tr,\n\t\t  unsigned long ip, void *data)\n{\n\tstruct ftrace_func_mapper *mapper = data;\n\tstruct event_probe_data *edata;\n\n\tif (!ip) {\n\t\tif (!mapper)\n\t\t\treturn;\n\t\tfree_ftrace_func_mapper(mapper, free_probe_data);\n\t\treturn;\n\t}\n\n\tedata = ftrace_func_mapper_remove_ip(mapper, ip);\n\n\tif (WARN_ON_ONCE(!edata))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(edata->ref <= 0))\n\t\treturn;\n\n\tfree_probe_data(edata);\n}\n\nstatic struct ftrace_probe_ops event_enable_probe_ops = {\n\t.func\t\t\t= event_enable_probe,\n\t.print\t\t\t= event_enable_print,\n\t.init\t\t\t= event_enable_init,\n\t.free\t\t\t= event_enable_free,\n};\n\nstatic struct ftrace_probe_ops event_enable_count_probe_ops = {\n\t.func\t\t\t= event_enable_count_probe,\n\t.print\t\t\t= event_enable_print,\n\t.init\t\t\t= event_enable_init,\n\t.free\t\t\t= event_enable_free,\n};\n\nstatic struct ftrace_probe_ops event_disable_probe_ops = {\n\t.func\t\t\t= event_enable_probe,\n\t.print\t\t\t= event_enable_print,\n\t.init\t\t\t= event_enable_init,\n\t.free\t\t\t= event_enable_free,\n};\n\nstatic struct ftrace_probe_ops event_disable_count_probe_ops = {\n\t.func\t\t\t= event_enable_count_probe,\n\t.print\t\t\t= event_enable_print,\n\t.init\t\t\t= event_enable_init,\n\t.free\t\t\t= event_enable_free,\n};\n\nstatic int\nevent_enable_func(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t  char *glob, char *cmd, char *param, int enabled)\n{\n\tstruct trace_event_file *file;\n\tstruct ftrace_probe_ops *ops;\n\tstruct event_probe_data *data;\n\tconst char *system;\n\tconst char *event;\n\tchar *number;\n\tbool enable;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\t \n\tif (!enabled || !param)\n\t\treturn -EINVAL;\n\n\tsystem = strsep(&param, \":\");\n\tif (!param)\n\t\treturn -EINVAL;\n\n\tevent = strsep(&param, \":\");\n\n\tmutex_lock(&event_mutex);\n\n\tret = -EINVAL;\n\tfile = find_event_file(tr, system, event);\n\tif (!file)\n\t\tgoto out;\n\n\tenable = strcmp(cmd, ENABLE_EVENT_STR) == 0;\n\n\tif (enable)\n\t\tops = param ? &event_enable_count_probe_ops : &event_enable_probe_ops;\n\telse\n\t\tops = param ? &event_disable_count_probe_ops : &event_disable_probe_ops;\n\n\tif (glob[0] == '!') {\n\t\tret = unregister_ftrace_function_probe_func(glob+1, tr, ops);\n\t\tgoto out;\n\t}\n\n\tret = -ENOMEM;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\tgoto out;\n\n\tdata->enable = enable;\n\tdata->count = -1;\n\tdata->file = file;\n\n\tif (!param)\n\t\tgoto out_reg;\n\n\tnumber = strsep(&param, \":\");\n\n\tret = -EINVAL;\n\tif (!strlen(number))\n\t\tgoto out_free;\n\n\t \n\tret = kstrtoul(number, 0, &data->count);\n\tif (ret)\n\t\tgoto out_free;\n\n out_reg:\n\t \n\tret = trace_event_try_get_ref(file->event_call);\n\tif (!ret) {\n\t\tret = -EBUSY;\n\t\tgoto out_free;\n\t}\n\n\tret = __ftrace_event_enable_disable(file, 1, 1);\n\tif (ret < 0)\n\t\tgoto out_put;\n\n\tret = register_ftrace_function_probe(glob, tr, ops, data);\n\t \n\tif (!ret) {\n\t\tret = -ENOENT;\n\t\tgoto out_disable;\n\t} else if (ret < 0)\n\t\tgoto out_disable;\n\t \n\tret = 0;\n out:\n\tmutex_unlock(&event_mutex);\n\treturn ret;\n\n out_disable:\n\t__ftrace_event_enable_disable(file, 0, 1);\n out_put:\n\ttrace_event_put_ref(file->event_call);\n out_free:\n\tkfree(data);\n\tgoto out;\n}\n\nstatic struct ftrace_func_command event_enable_cmd = {\n\t.name\t\t\t= ENABLE_EVENT_STR,\n\t.func\t\t\t= event_enable_func,\n};\n\nstatic struct ftrace_func_command event_disable_cmd = {\n\t.name\t\t\t= DISABLE_EVENT_STR,\n\t.func\t\t\t= event_enable_func,\n};\n\nstatic __init int register_event_cmds(void)\n{\n\tint ret;\n\n\tret = register_ftrace_command(&event_enable_cmd);\n\tif (WARN_ON(ret < 0))\n\t\treturn ret;\n\tret = register_ftrace_command(&event_disable_cmd);\n\tif (WARN_ON(ret < 0))\n\t\tunregister_ftrace_command(&event_enable_cmd);\n\treturn ret;\n}\n#else\nstatic inline int register_event_cmds(void) { return 0; }\n#endif  \n\n \nstatic void __trace_early_add_event_dirs(struct trace_array *tr)\n{\n\tstruct trace_event_file *file;\n\tint ret;\n\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\t\tret = event_create_dir(tr->event_dir, file);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Could not create directory for event %s\\n\",\n\t\t\t\ttrace_event_name(file->event_call));\n\t}\n}\n\n \nvoid __trace_early_add_events(struct trace_array *tr)\n{\n\tstruct trace_event_call *call;\n\tint ret;\n\n\tlist_for_each_entry(call, &ftrace_events, list) {\n\t\t \n\t\tif (!(call->flags & TRACE_EVENT_FL_DYNAMIC) &&\n\t\t    WARN_ON_ONCE(call->module))\n\t\t\tcontinue;\n\n\t\tret = __trace_early_add_new_event(call, tr);\n\t\tif (ret < 0)\n\t\t\tpr_warn(\"Could not create early event %s\\n\",\n\t\t\t\ttrace_event_name(call));\n\t}\n}\n\n \nstatic void\n__trace_remove_event_dirs(struct trace_array *tr)\n{\n\tstruct trace_event_file *file, *next;\n\n\tlist_for_each_entry_safe(file, next, &tr->events, list)\n\t\tremove_event_file_dir(file);\n}\n\nstatic void __add_event_to_tracers(struct trace_event_call *call)\n{\n\tstruct trace_array *tr;\n\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list)\n\t\t__trace_add_new_event(call, tr);\n}\n\nextern struct trace_event_call *__start_ftrace_events[];\nextern struct trace_event_call *__stop_ftrace_events[];\n\nstatic char bootup_event_buf[COMMAND_LINE_SIZE] __initdata;\n\nstatic __init int setup_trace_event(char *str)\n{\n\tstrscpy(bootup_event_buf, str, COMMAND_LINE_SIZE);\n\tring_buffer_expanded = true;\n\tdisable_tracing_selftest(\"running event tracing\");\n\n\treturn 1;\n}\n__setup(\"trace_event=\", setup_trace_event);\n\n \nstatic int\ncreate_event_toplevel_files(struct dentry *parent, struct trace_array *tr)\n{\n\tstruct dentry *d_events;\n\tstruct dentry *entry;\n\tint error = 0;\n\n\tentry = trace_create_file(\"set_event\", TRACE_MODE_WRITE, parent,\n\t\t\t\t  tr, &ftrace_set_event_fops);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\td_events = eventfs_create_events_dir(\"events\", parent);\n\tif (IS_ERR(d_events)) {\n\t\tpr_warn(\"Could not create tracefs 'events' directory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\terror = eventfs_add_events_file(\"enable\", TRACE_MODE_WRITE, d_events,\n\t\t\t\t  tr, &ftrace_tr_enable_fops);\n\tif (error)\n\t\treturn -ENOMEM;\n\n\t \n\n\ttrace_create_file(\"set_event_pid\", TRACE_MODE_WRITE, parent,\n\t\t\t  tr, &ftrace_set_event_pid_fops);\n\n\ttrace_create_file(\"set_event_notrace_pid\",\n\t\t\t  TRACE_MODE_WRITE, parent, tr,\n\t\t\t  &ftrace_set_event_notrace_pid_fops);\n\n\t \n\teventfs_add_events_file(\"header_page\", TRACE_MODE_READ, d_events,\n\t\t\t\t  ring_buffer_print_page_header,\n\t\t\t\t  &ftrace_show_header_fops);\n\n\teventfs_add_events_file(\"header_event\", TRACE_MODE_READ, d_events,\n\t\t\t\t  ring_buffer_print_entry_header,\n\t\t\t\t  &ftrace_show_header_fops);\n\n\ttr->event_dir = d_events;\n\n\treturn 0;\n}\n\n \nint event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)\n{\n\tint ret;\n\n\tlockdep_assert_held(&event_mutex);\n\n\tret = create_event_toplevel_files(parent, tr);\n\tif (ret)\n\t\tgoto out;\n\n\tdown_write(&trace_event_sem);\n\t \n\tif (unlikely(!list_empty(&tr->events)))\n\t\t__trace_early_add_event_dirs(tr);\n\telse\n\t\t__trace_add_event_dirs(tr);\n\tup_write(&trace_event_sem);\n\n out:\n\treturn ret;\n}\n\n \nstatic __init int\nearly_event_add_tracer(struct dentry *parent, struct trace_array *tr)\n{\n\tint ret;\n\n\tmutex_lock(&event_mutex);\n\n\tret = create_event_toplevel_files(parent, tr);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tdown_write(&trace_event_sem);\n\t__trace_early_add_event_dirs(tr);\n\tup_write(&trace_event_sem);\n\n out_unlock:\n\tmutex_unlock(&event_mutex);\n\n\treturn ret;\n}\n\n \nint event_trace_del_tracer(struct trace_array *tr)\n{\n\tlockdep_assert_held(&event_mutex);\n\n\t \n\tclear_event_triggers(tr);\n\n\t \n\t__ftrace_clear_event_pids(tr, TRACE_PIDS | TRACE_NO_PIDS);\n\n\t \n\t__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);\n\n\t \n\ttracepoint_synchronize_unregister();\n\n\tdown_write(&trace_event_sem);\n\t__trace_remove_event_dirs(tr);\n\teventfs_remove_events_dir(tr->event_dir);\n\tup_write(&trace_event_sem);\n\n\ttr->event_dir = NULL;\n\n\treturn 0;\n}\n\nstatic __init int event_trace_memsetup(void)\n{\n\tfield_cachep = KMEM_CACHE(ftrace_event_field, SLAB_PANIC);\n\tfile_cachep = KMEM_CACHE(trace_event_file, SLAB_PANIC);\n\treturn 0;\n}\n\n__init void\nearly_enable_events(struct trace_array *tr, char *buf, bool disable_first)\n{\n\tchar *token;\n\tint ret;\n\n\twhile (true) {\n\t\ttoken = strsep(&buf, \",\");\n\n\t\tif (!token)\n\t\t\tbreak;\n\n\t\tif (*token) {\n\t\t\t \n\t\t\tif (disable_first)\n\t\t\t\tftrace_set_clr_event(tr, token, 0);\n\n\t\t\tret = ftrace_set_clr_event(tr, token, 1);\n\t\t\tif (ret)\n\t\t\t\tpr_warn(\"Failed to enable trace event: %s\\n\", token);\n\t\t}\n\n\t\t \n\t\tif (buf)\n\t\t\t*(buf - 1) = ',';\n\t}\n}\n\nstatic __init int event_trace_enable(void)\n{\n\tstruct trace_array *tr = top_trace_array();\n\tstruct trace_event_call **iter, *call;\n\tint ret;\n\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\tfor_each_event(iter, __start_ftrace_events, __stop_ftrace_events) {\n\n\t\tcall = *iter;\n\t\tret = event_init(call);\n\t\tif (!ret)\n\t\t\tlist_add(&call->list, &ftrace_events);\n\t}\n\n\tregister_trigger_cmds();\n\n\t \n\t__trace_early_add_events(tr);\n\n\tearly_enable_events(tr, bootup_event_buf, false);\n\n\ttrace_printk_start_comm();\n\n\tregister_event_cmds();\n\n\n\treturn 0;\n}\n\n \nstatic __init int event_trace_enable_again(void)\n{\n\tstruct trace_array *tr;\n\n\ttr = top_trace_array();\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\tearly_enable_events(tr, bootup_event_buf, true);\n\n\treturn 0;\n}\n\nearly_initcall(event_trace_enable_again);\n\n \nstatic __init int event_trace_init_fields(void)\n{\n\tif (trace_define_generic_fields())\n\t\tpr_warn(\"tracing: Failed to allocated generic fields\");\n\n\tif (trace_define_common_fields())\n\t\tpr_warn(\"tracing: Failed to allocate common fields\");\n\n\treturn 0;\n}\n\n__init int event_trace_init(void)\n{\n\tstruct trace_array *tr;\n\tint ret;\n\n\ttr = top_trace_array();\n\tif (!tr)\n\t\treturn -ENODEV;\n\n\ttrace_create_file(\"available_events\", TRACE_MODE_READ,\n\t\t\t  NULL, tr, &ftrace_avail_fops);\n\n\tret = early_event_add_tracer(NULL, tr);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_MODULES\n\tret = register_module_notifier(&trace_module_nb);\n\tif (ret)\n\t\tpr_warn(\"Failed to register trace events module notifier\\n\");\n#endif\n\n\teventdir_initialized = true;\n\n\treturn 0;\n}\n\nvoid __init trace_event_init(void)\n{\n\tevent_trace_memsetup();\n\tinit_ftrace_syscalls();\n\tevent_trace_enable();\n\tevent_trace_init_fields();\n}\n\n#ifdef CONFIG_EVENT_TRACE_STARTUP_TEST\n\nstatic DEFINE_SPINLOCK(test_spinlock);\nstatic DEFINE_SPINLOCK(test_spinlock_irq);\nstatic DEFINE_MUTEX(test_mutex);\n\nstatic __init void test_work(struct work_struct *dummy)\n{\n\tspin_lock(&test_spinlock);\n\tspin_lock_irq(&test_spinlock_irq);\n\tudelay(1);\n\tspin_unlock_irq(&test_spinlock_irq);\n\tspin_unlock(&test_spinlock);\n\n\tmutex_lock(&test_mutex);\n\tmsleep(1);\n\tmutex_unlock(&test_mutex);\n}\n\nstatic __init int event_test_thread(void *unused)\n{\n\tvoid *test_malloc;\n\n\ttest_malloc = kmalloc(1234, GFP_KERNEL);\n\tif (!test_malloc)\n\t\tpr_info(\"failed to kmalloc\\n\");\n\n\tschedule_on_each_cpu(test_work);\n\n\tkfree(test_malloc);\n\n\tset_current_state(TASK_INTERRUPTIBLE);\n\twhile (!kthread_should_stop()) {\n\t\tschedule();\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\treturn 0;\n}\n\n \nstatic __init void event_test_stuff(void)\n{\n\tstruct task_struct *test_thread;\n\n\ttest_thread = kthread_run(event_test_thread, NULL, \"test-events\");\n\tmsleep(1);\n\tkthread_stop(test_thread);\n}\n\n \nstatic __init void event_trace_self_tests(void)\n{\n\tstruct trace_subsystem_dir *dir;\n\tstruct trace_event_file *file;\n\tstruct trace_event_call *call;\n\tstruct event_subsystem *system;\n\tstruct trace_array *tr;\n\tint ret;\n\n\ttr = top_trace_array();\n\tif (!tr)\n\t\treturn;\n\n\tpr_info(\"Running tests on trace events:\\n\");\n\n\tlist_for_each_entry(file, &tr->events, list) {\n\n\t\tcall = file->event_call;\n\n\t\t \n\t\tif (!call->class || !call->class->probe)\n\t\t\tcontinue;\n\n \n#ifndef CONFIG_EVENT_TRACE_TEST_SYSCALLS\n\t\tif (call->class->system &&\n\t\t    strcmp(call->class->system, \"syscalls\") == 0)\n\t\t\tcontinue;\n#endif\n\n\t\tpr_info(\"Testing event %s: \", trace_event_name(call));\n\n\t\t \n\t\tif (file->flags & EVENT_FILE_FL_ENABLED) {\n\t\t\tpr_warn(\"Enabled event during self test!\\n\");\n\t\t\tWARN_ON_ONCE(1);\n\t\t\tcontinue;\n\t\t}\n\n\t\tftrace_event_enable_disable(file, 1);\n\t\tevent_test_stuff();\n\t\tftrace_event_enable_disable(file, 0);\n\n\t\tpr_cont(\"OK\\n\");\n\t}\n\n\t \n\n\tpr_info(\"Running tests on trace event systems:\\n\");\n\n\tlist_for_each_entry(dir, &tr->systems, list) {\n\n\t\tsystem = dir->subsystem;\n\n\t\t \n\t\tif (strcmp(system->name, \"ftrace\") == 0)\n\t\t\tcontinue;\n\n\t\tpr_info(\"Testing event system %s: \", system->name);\n\n\t\tret = __ftrace_set_clr_event(tr, NULL, system->name, NULL, 1);\n\t\tif (WARN_ON_ONCE(ret)) {\n\t\t\tpr_warn(\"error enabling system %s\\n\",\n\t\t\t\tsystem->name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tevent_test_stuff();\n\n\t\tret = __ftrace_set_clr_event(tr, NULL, system->name, NULL, 0);\n\t\tif (WARN_ON_ONCE(ret)) {\n\t\t\tpr_warn(\"error disabling system %s\\n\",\n\t\t\t\tsystem->name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_cont(\"OK\\n\");\n\t}\n\n\t \n\n\tpr_info(\"Running tests on all trace events:\\n\");\n\tpr_info(\"Testing all events: \");\n\n\tret = __ftrace_set_clr_event(tr, NULL, NULL, NULL, 1);\n\tif (WARN_ON_ONCE(ret)) {\n\t\tpr_warn(\"error enabling all events\\n\");\n\t\treturn;\n\t}\n\n\tevent_test_stuff();\n\n\t \n\tret = __ftrace_set_clr_event(tr, NULL, NULL, NULL, 0);\n\tif (WARN_ON_ONCE(ret)) {\n\t\tpr_warn(\"error disabling all events\\n\");\n\t\treturn;\n\t}\n\n\tpr_cont(\"OK\\n\");\n}\n\n#ifdef CONFIG_FUNCTION_TRACER\n\nstatic DEFINE_PER_CPU(atomic_t, ftrace_test_event_disable);\n\nstatic struct trace_event_file event_trace_file __initdata;\n\nstatic void __init\nfunction_test_events_call(unsigned long ip, unsigned long parent_ip,\n\t\t\t  struct ftrace_ops *op, struct ftrace_regs *regs)\n{\n\tstruct trace_buffer *buffer;\n\tstruct ring_buffer_event *event;\n\tstruct ftrace_entry *entry;\n\tunsigned int trace_ctx;\n\tlong disabled;\n\tint cpu;\n\n\ttrace_ctx = tracing_gen_ctx();\n\tpreempt_disable_notrace();\n\tcpu = raw_smp_processor_id();\n\tdisabled = atomic_inc_return(&per_cpu(ftrace_test_event_disable, cpu));\n\n\tif (disabled != 1)\n\t\tgoto out;\n\n\tevent = trace_event_buffer_lock_reserve(&buffer, &event_trace_file,\n\t\t\t\t\t\tTRACE_FN, sizeof(*entry),\n\t\t\t\t\t\ttrace_ctx);\n\tif (!event)\n\t\tgoto out;\n\tentry\t= ring_buffer_event_data(event);\n\tentry->ip\t\t\t= ip;\n\tentry->parent_ip\t\t= parent_ip;\n\n\tevent_trigger_unlock_commit(&event_trace_file, buffer, event,\n\t\t\t\t    entry, trace_ctx);\n out:\n\tatomic_dec(&per_cpu(ftrace_test_event_disable, cpu));\n\tpreempt_enable_notrace();\n}\n\nstatic struct ftrace_ops trace_ops __initdata  =\n{\n\t.func = function_test_events_call,\n};\n\nstatic __init void event_trace_self_test_with_function(void)\n{\n\tint ret;\n\n\tevent_trace_file.tr = top_trace_array();\n\tif (WARN_ON(!event_trace_file.tr))\n\t\treturn;\n\n\tret = register_ftrace_function(&trace_ops);\n\tif (WARN_ON(ret < 0)) {\n\t\tpr_info(\"Failed to enable function tracer for event tests\\n\");\n\t\treturn;\n\t}\n\tpr_info(\"Running tests again, along with the function tracer\\n\");\n\tevent_trace_self_tests();\n\tunregister_ftrace_function(&trace_ops);\n}\n#else\nstatic __init void event_trace_self_test_with_function(void)\n{\n}\n#endif\n\nstatic __init int event_trace_self_tests_init(void)\n{\n\tif (!tracing_selftest_disabled) {\n\t\tevent_trace_self_tests();\n\t\tevent_trace_self_test_with_function();\n\t}\n\n\treturn 0;\n}\n\nlate_initcall(event_trace_self_tests_init);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}