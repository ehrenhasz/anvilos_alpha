{
  "module_name": "blktrace.c",
  "hash_id": "136757d6ddadc6288e80b126b55f5138b282640bbe400977e43ceeb1f561d99d",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/blktrace.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/blkdev.h>\n#include <linux/blktrace_api.h>\n#include <linux/percpu.h>\n#include <linux/init.h>\n#include <linux/mutex.h>\n#include <linux/slab.h>\n#include <linux/debugfs.h>\n#include <linux/export.h>\n#include <linux/time.h>\n#include <linux/uaccess.h>\n#include <linux/list.h>\n#include <linux/blk-cgroup.h>\n\n#include \"../../block/blk.h\"\n\n#include <trace/events/block.h>\n\n#include \"trace_output.h\"\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\nstatic unsigned int blktrace_seq __read_mostly = 1;\n\nstatic struct trace_array *blk_tr;\nstatic bool blk_tracer_enabled __read_mostly;\n\nstatic LIST_HEAD(running_trace_list);\nstatic __cacheline_aligned_in_smp DEFINE_RAW_SPINLOCK(running_trace_lock);\n\n \n#define TRACE_BLK_OPT_CLASSIC\t0x1\n#define TRACE_BLK_OPT_CGROUP\t0x2\n#define TRACE_BLK_OPT_CGNAME\t0x4\n\nstatic struct tracer_opt blk_tracer_opts[] = {\n\t \n\t{ TRACER_OPT(blk_classic, TRACE_BLK_OPT_CLASSIC) },\n#ifdef CONFIG_BLK_CGROUP\n\t{ TRACER_OPT(blk_cgroup, TRACE_BLK_OPT_CGROUP) },\n\t{ TRACER_OPT(blk_cgname, TRACE_BLK_OPT_CGNAME) },\n#endif\n\t{ }\n};\n\nstatic struct tracer_flags blk_tracer_flags = {\n\t.val  = 0,\n\t.opts = blk_tracer_opts,\n};\n\n \nstatic DEFINE_MUTEX(blk_probe_mutex);\nstatic int blk_probes_ref;\n\nstatic void blk_register_tracepoints(void);\nstatic void blk_unregister_tracepoints(void);\n\n \nstatic void trace_note(struct blk_trace *bt, pid_t pid, int action,\n\t\t       const void *data, size_t len, u64 cgid)\n{\n\tstruct blk_io_trace *t;\n\tstruct ring_buffer_event *event = NULL;\n\tstruct trace_buffer *buffer = NULL;\n\tunsigned int trace_ctx = 0;\n\tint cpu = smp_processor_id();\n\tbool blk_tracer = blk_tracer_enabled;\n\tssize_t cgid_len = cgid ? sizeof(cgid) : 0;\n\n\tif (blk_tracer) {\n\t\tbuffer = blk_tr->array_buffer.buffer;\n\t\ttrace_ctx = tracing_gen_ctx_flags(0);\n\t\tevent = trace_buffer_lock_reserve(buffer, TRACE_BLK,\n\t\t\t\t\t\t  sizeof(*t) + len + cgid_len,\n\t\t\t\t\t\t  trace_ctx);\n\t\tif (!event)\n\t\t\treturn;\n\t\tt = ring_buffer_event_data(event);\n\t\tgoto record_it;\n\t}\n\n\tif (!bt->rchan)\n\t\treturn;\n\n\tt = relay_reserve(bt->rchan, sizeof(*t) + len + cgid_len);\n\tif (t) {\n\t\tt->magic = BLK_IO_TRACE_MAGIC | BLK_IO_TRACE_VERSION;\n\t\tt->time = ktime_to_ns(ktime_get());\nrecord_it:\n\t\tt->device = bt->dev;\n\t\tt->action = action | (cgid ? __BLK_TN_CGROUP : 0);\n\t\tt->pid = pid;\n\t\tt->cpu = cpu;\n\t\tt->pdu_len = len + cgid_len;\n\t\tif (cgid_len)\n\t\t\tmemcpy((void *)t + sizeof(*t), &cgid, cgid_len);\n\t\tmemcpy((void *) t + sizeof(*t) + cgid_len, data, len);\n\n\t\tif (blk_tracer)\n\t\t\ttrace_buffer_unlock_commit(blk_tr, buffer, event, trace_ctx);\n\t}\n}\n\n \nstatic void trace_note_tsk(struct task_struct *tsk)\n{\n\tunsigned long flags;\n\tstruct blk_trace *bt;\n\n\ttsk->btrace_seq = blktrace_seq;\n\traw_spin_lock_irqsave(&running_trace_lock, flags);\n\tlist_for_each_entry(bt, &running_trace_list, running_list) {\n\t\ttrace_note(bt, tsk->pid, BLK_TN_PROCESS, tsk->comm,\n\t\t\t   sizeof(tsk->comm), 0);\n\t}\n\traw_spin_unlock_irqrestore(&running_trace_lock, flags);\n}\n\nstatic void trace_note_time(struct blk_trace *bt)\n{\n\tstruct timespec64 now;\n\tunsigned long flags;\n\tu32 words[2];\n\n\t \n\tktime_get_real_ts64(&now);\n\twords[0] = (u32)now.tv_sec;\n\twords[1] = now.tv_nsec;\n\n\tlocal_irq_save(flags);\n\ttrace_note(bt, 0, BLK_TN_TIMESTAMP, words, sizeof(words), 0);\n\tlocal_irq_restore(flags);\n}\n\nvoid __blk_trace_note_message(struct blk_trace *bt,\n\t\tstruct cgroup_subsys_state *css, const char *fmt, ...)\n{\n\tint n;\n\tva_list args;\n\tunsigned long flags;\n\tchar *buf;\n\tu64 cgid = 0;\n\n\tif (unlikely(bt->trace_state != Blktrace_running &&\n\t\t     !blk_tracer_enabled))\n\t\treturn;\n\n\t \n\tif (!(bt->act_mask & BLK_TC_NOTIFY))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tbuf = this_cpu_ptr(bt->msg_data);\n\tva_start(args, fmt);\n\tn = vscnprintf(buf, BLK_TN_MAX_MSG, fmt, args);\n\tva_end(args);\n\n#ifdef CONFIG_BLK_CGROUP\n\tif (css && (blk_tracer_flags.val & TRACE_BLK_OPT_CGROUP))\n\t\tcgid = cgroup_id(css->cgroup);\n\telse\n\t\tcgid = 1;\n#endif\n\ttrace_note(bt, current->pid, BLK_TN_MESSAGE, buf, n, cgid);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(__blk_trace_note_message);\n\nstatic int act_log_check(struct blk_trace *bt, u32 what, sector_t sector,\n\t\t\t pid_t pid)\n{\n\tif (((bt->act_mask << BLK_TC_SHIFT) & what) == 0)\n\t\treturn 1;\n\tif (sector && (sector < bt->start_lba || sector > bt->end_lba))\n\t\treturn 1;\n\tif (bt->pid && pid != bt->pid)\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic const u32 ddir_act[2] = { BLK_TC_ACT(BLK_TC_READ),\n\t\t\t\t BLK_TC_ACT(BLK_TC_WRITE) };\n\n#define BLK_TC_RAHEAD\t\tBLK_TC_AHEAD\n#define BLK_TC_PREFLUSH\t\tBLK_TC_FLUSH\n\n \n#define MASK_TC_BIT(rw, __name) ((__force u32)(rw & REQ_ ## __name) <<\t\\\n\t  (ilog2(BLK_TC_ ## __name) + BLK_TC_SHIFT - __REQ_ ## __name))\n\n \nstatic void __blk_add_trace(struct blk_trace *bt, sector_t sector, int bytes,\n\t\t\t    const blk_opf_t opf, u32 what, int error,\n\t\t\t    int pdu_len, void *pdu_data, u64 cgid)\n{\n\tstruct task_struct *tsk = current;\n\tstruct ring_buffer_event *event = NULL;\n\tstruct trace_buffer *buffer = NULL;\n\tstruct blk_io_trace *t;\n\tunsigned long flags = 0;\n\tunsigned long *sequence;\n\tunsigned int trace_ctx = 0;\n\tpid_t pid;\n\tint cpu;\n\tbool blk_tracer = blk_tracer_enabled;\n\tssize_t cgid_len = cgid ? sizeof(cgid) : 0;\n\tconst enum req_op op = opf & REQ_OP_MASK;\n\n\tif (unlikely(bt->trace_state != Blktrace_running && !blk_tracer))\n\t\treturn;\n\n\twhat |= ddir_act[op_is_write(op) ? WRITE : READ];\n\twhat |= MASK_TC_BIT(opf, SYNC);\n\twhat |= MASK_TC_BIT(opf, RAHEAD);\n\twhat |= MASK_TC_BIT(opf, META);\n\twhat |= MASK_TC_BIT(opf, PREFLUSH);\n\twhat |= MASK_TC_BIT(opf, FUA);\n\tif (op == REQ_OP_DISCARD || op == REQ_OP_SECURE_ERASE)\n\t\twhat |= BLK_TC_ACT(BLK_TC_DISCARD);\n\tif (op == REQ_OP_FLUSH)\n\t\twhat |= BLK_TC_ACT(BLK_TC_FLUSH);\n\tif (cgid)\n\t\twhat |= __BLK_TA_CGROUP;\n\n\tpid = tsk->pid;\n\tif (act_log_check(bt, what, sector, pid))\n\t\treturn;\n\tcpu = raw_smp_processor_id();\n\n\tif (blk_tracer) {\n\t\ttracing_record_cmdline(current);\n\n\t\tbuffer = blk_tr->array_buffer.buffer;\n\t\ttrace_ctx = tracing_gen_ctx_flags(0);\n\t\tevent = trace_buffer_lock_reserve(buffer, TRACE_BLK,\n\t\t\t\t\t\t  sizeof(*t) + pdu_len + cgid_len,\n\t\t\t\t\t\t  trace_ctx);\n\t\tif (!event)\n\t\t\treturn;\n\t\tt = ring_buffer_event_data(event);\n\t\tgoto record_it;\n\t}\n\n\tif (unlikely(tsk->btrace_seq != blktrace_seq))\n\t\ttrace_note_tsk(tsk);\n\n\t \n\tlocal_irq_save(flags);\n\tt = relay_reserve(bt->rchan, sizeof(*t) + pdu_len + cgid_len);\n\tif (t) {\n\t\tsequence = per_cpu_ptr(bt->sequence, cpu);\n\n\t\tt->magic = BLK_IO_TRACE_MAGIC | BLK_IO_TRACE_VERSION;\n\t\tt->sequence = ++(*sequence);\n\t\tt->time = ktime_to_ns(ktime_get());\nrecord_it:\n\t\t \n\t\tt->cpu = cpu;\n\t\tt->pid = pid;\n\n\t\tt->sector = sector;\n\t\tt->bytes = bytes;\n\t\tt->action = what;\n\t\tt->device = bt->dev;\n\t\tt->error = error;\n\t\tt->pdu_len = pdu_len + cgid_len;\n\n\t\tif (cgid_len)\n\t\t\tmemcpy((void *)t + sizeof(*t), &cgid, cgid_len);\n\t\tif (pdu_len)\n\t\t\tmemcpy((void *)t + sizeof(*t) + cgid_len, pdu_data, pdu_len);\n\n\t\tif (blk_tracer) {\n\t\t\ttrace_buffer_unlock_commit(blk_tr, buffer, event, trace_ctx);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nstatic void blk_trace_free(struct request_queue *q, struct blk_trace *bt)\n{\n\trelay_close(bt->rchan);\n\n\t \n\tif (!bt->dir) {\n\t\tdebugfs_lookup_and_remove(\"dropped\", q->debugfs_dir);\n\t\tdebugfs_lookup_and_remove(\"msg\", q->debugfs_dir);\n\t} else {\n\t\tdebugfs_remove(bt->dir);\n\t}\n\tfree_percpu(bt->sequence);\n\tfree_percpu(bt->msg_data);\n\tkfree(bt);\n}\n\nstatic void get_probe_ref(void)\n{\n\tmutex_lock(&blk_probe_mutex);\n\tif (++blk_probes_ref == 1)\n\t\tblk_register_tracepoints();\n\tmutex_unlock(&blk_probe_mutex);\n}\n\nstatic void put_probe_ref(void)\n{\n\tmutex_lock(&blk_probe_mutex);\n\tif (!--blk_probes_ref)\n\t\tblk_unregister_tracepoints();\n\tmutex_unlock(&blk_probe_mutex);\n}\n\nstatic int blk_trace_start(struct blk_trace *bt)\n{\n\tif (bt->trace_state != Blktrace_setup &&\n\t    bt->trace_state != Blktrace_stopped)\n\t\treturn -EINVAL;\n\n\tblktrace_seq++;\n\tsmp_mb();\n\tbt->trace_state = Blktrace_running;\n\traw_spin_lock_irq(&running_trace_lock);\n\tlist_add(&bt->running_list, &running_trace_list);\n\traw_spin_unlock_irq(&running_trace_lock);\n\ttrace_note_time(bt);\n\n\treturn 0;\n}\n\nstatic int blk_trace_stop(struct blk_trace *bt)\n{\n\tif (bt->trace_state != Blktrace_running)\n\t\treturn -EINVAL;\n\n\tbt->trace_state = Blktrace_stopped;\n\traw_spin_lock_irq(&running_trace_lock);\n\tlist_del_init(&bt->running_list);\n\traw_spin_unlock_irq(&running_trace_lock);\n\trelay_flush(bt->rchan);\n\n\treturn 0;\n}\n\nstatic void blk_trace_cleanup(struct request_queue *q, struct blk_trace *bt)\n{\n\tblk_trace_stop(bt);\n\tsynchronize_rcu();\n\tblk_trace_free(q, bt);\n\tput_probe_ref();\n}\n\nstatic int __blk_trace_remove(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\tbt = rcu_replace_pointer(q->blk_trace, NULL,\n\t\t\t\t lockdep_is_held(&q->debugfs_mutex));\n\tif (!bt)\n\t\treturn -EINVAL;\n\n\tblk_trace_cleanup(q, bt);\n\n\treturn 0;\n}\n\nint blk_trace_remove(struct request_queue *q)\n{\n\tint ret;\n\n\tmutex_lock(&q->debugfs_mutex);\n\tret = __blk_trace_remove(q);\n\tmutex_unlock(&q->debugfs_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blk_trace_remove);\n\nstatic ssize_t blk_dropped_read(struct file *filp, char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct blk_trace *bt = filp->private_data;\n\tchar buf[16];\n\n\tsnprintf(buf, sizeof(buf), \"%u\\n\", atomic_read(&bt->dropped));\n\n\treturn simple_read_from_buffer(buffer, count, ppos, buf, strlen(buf));\n}\n\nstatic const struct file_operations blk_dropped_fops = {\n\t.owner =\tTHIS_MODULE,\n\t.open =\t\tsimple_open,\n\t.read =\t\tblk_dropped_read,\n\t.llseek =\tdefault_llseek,\n};\n\nstatic ssize_t blk_msg_write(struct file *filp, const char __user *buffer,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tchar *msg;\n\tstruct blk_trace *bt;\n\n\tif (count >= BLK_TN_MAX_MSG)\n\t\treturn -EINVAL;\n\n\tmsg = memdup_user_nul(buffer, count);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\n\tbt = filp->private_data;\n\t__blk_trace_note_message(bt, NULL, \"%s\", msg);\n\tkfree(msg);\n\n\treturn count;\n}\n\nstatic const struct file_operations blk_msg_fops = {\n\t.owner =\tTHIS_MODULE,\n\t.open =\t\tsimple_open,\n\t.write =\tblk_msg_write,\n\t.llseek =\tnoop_llseek,\n};\n\n \nstatic int blk_subbuf_start_callback(struct rchan_buf *buf, void *subbuf,\n\t\t\t\t     void *prev_subbuf, size_t prev_padding)\n{\n\tstruct blk_trace *bt;\n\n\tif (!relay_buf_full(buf))\n\t\treturn 1;\n\n\tbt = buf->chan->private_data;\n\tatomic_inc(&bt->dropped);\n\treturn 0;\n}\n\nstatic int blk_remove_buf_file_callback(struct dentry *dentry)\n{\n\tdebugfs_remove(dentry);\n\n\treturn 0;\n}\n\nstatic struct dentry *blk_create_buf_file_callback(const char *filename,\n\t\t\t\t\t\t   struct dentry *parent,\n\t\t\t\t\t\t   umode_t mode,\n\t\t\t\t\t\t   struct rchan_buf *buf,\n\t\t\t\t\t\t   int *is_global)\n{\n\treturn debugfs_create_file(filename, mode, parent, buf,\n\t\t\t\t\t&relay_file_operations);\n}\n\nstatic const struct rchan_callbacks blk_relay_callbacks = {\n\t.subbuf_start\t\t= blk_subbuf_start_callback,\n\t.create_buf_file\t= blk_create_buf_file_callback,\n\t.remove_buf_file\t= blk_remove_buf_file_callback,\n};\n\nstatic void blk_trace_setup_lba(struct blk_trace *bt,\n\t\t\t\tstruct block_device *bdev)\n{\n\tif (bdev) {\n\t\tbt->start_lba = bdev->bd_start_sect;\n\t\tbt->end_lba = bdev->bd_start_sect + bdev_nr_sectors(bdev);\n\t} else {\n\t\tbt->start_lba = 0;\n\t\tbt->end_lba = -1ULL;\n\t}\n}\n\n \nstatic int do_blk_trace_setup(struct request_queue *q, char *name, dev_t dev,\n\t\t\t      struct block_device *bdev,\n\t\t\t      struct blk_user_trace_setup *buts)\n{\n\tstruct blk_trace *bt = NULL;\n\tstruct dentry *dir = NULL;\n\tint ret;\n\n\tlockdep_assert_held(&q->debugfs_mutex);\n\n\tif (!buts->buf_size || !buts->buf_nr)\n\t\treturn -EINVAL;\n\n\tstrncpy(buts->name, name, BLKTRACE_BDEV_SIZE);\n\tbuts->name[BLKTRACE_BDEV_SIZE - 1] = '\\0';\n\n\t \n\tstrreplace(buts->name, '/', '_');\n\n\t \n\tif (rcu_dereference_protected(q->blk_trace,\n\t\t\t\t      lockdep_is_held(&q->debugfs_mutex))) {\n\t\tpr_warn(\"Concurrent blktraces are not allowed on %s\\n\",\n\t\t\tbuts->name);\n\t\treturn -EBUSY;\n\t}\n\n\tbt = kzalloc(sizeof(*bt), GFP_KERNEL);\n\tif (!bt)\n\t\treturn -ENOMEM;\n\n\tret = -ENOMEM;\n\tbt->sequence = alloc_percpu(unsigned long);\n\tif (!bt->sequence)\n\t\tgoto err;\n\n\tbt->msg_data = __alloc_percpu(BLK_TN_MAX_MSG, __alignof__(char));\n\tif (!bt->msg_data)\n\t\tgoto err;\n\n\t \n\tif (bdev && !bdev_is_partition(bdev))\n\t\tdir = q->debugfs_dir;\n\telse\n\t\tbt->dir = dir = debugfs_create_dir(buts->name, blk_debugfs_root);\n\n\t \n\tif (IS_ERR_OR_NULL(dir)) {\n\t\tpr_warn(\"debugfs_dir not present for %s so skipping\\n\",\n\t\t\tbuts->name);\n\t\tret = -ENOENT;\n\t\tgoto err;\n\t}\n\n\tbt->dev = dev;\n\tatomic_set(&bt->dropped, 0);\n\tINIT_LIST_HEAD(&bt->running_list);\n\n\tret = -EIO;\n\tdebugfs_create_file(\"dropped\", 0444, dir, bt, &blk_dropped_fops);\n\tdebugfs_create_file(\"msg\", 0222, dir, bt, &blk_msg_fops);\n\n\tbt->rchan = relay_open(\"trace\", dir, buts->buf_size,\n\t\t\t\tbuts->buf_nr, &blk_relay_callbacks, bt);\n\tif (!bt->rchan)\n\t\tgoto err;\n\n\tbt->act_mask = buts->act_mask;\n\tif (!bt->act_mask)\n\t\tbt->act_mask = (u16) -1;\n\n\tblk_trace_setup_lba(bt, bdev);\n\n\t \n\tif (buts->start_lba)\n\t\tbt->start_lba = buts->start_lba;\n\tif (buts->end_lba)\n\t\tbt->end_lba = buts->end_lba;\n\n\tbt->pid = buts->pid;\n\tbt->trace_state = Blktrace_setup;\n\n\trcu_assign_pointer(q->blk_trace, bt);\n\tget_probe_ref();\n\n\tret = 0;\nerr:\n\tif (ret)\n\t\tblk_trace_free(q, bt);\n\treturn ret;\n}\n\nstatic int __blk_trace_setup(struct request_queue *q, char *name, dev_t dev,\n\t\t\t     struct block_device *bdev, char __user *arg)\n{\n\tstruct blk_user_trace_setup buts;\n\tint ret;\n\n\tret = copy_from_user(&buts, arg, sizeof(buts));\n\tif (ret)\n\t\treturn -EFAULT;\n\n\tret = do_blk_trace_setup(q, name, dev, bdev, &buts);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, &buts, sizeof(buts))) {\n\t\t__blk_trace_remove(q);\n\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\nint blk_trace_setup(struct request_queue *q, char *name, dev_t dev,\n\t\t    struct block_device *bdev,\n\t\t    char __user *arg)\n{\n\tint ret;\n\n\tmutex_lock(&q->debugfs_mutex);\n\tret = __blk_trace_setup(q, name, dev, bdev, arg);\n\tmutex_unlock(&q->debugfs_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blk_trace_setup);\n\n#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)\nstatic int compat_blk_trace_setup(struct request_queue *q, char *name,\n\t\t\t\t  dev_t dev, struct block_device *bdev,\n\t\t\t\t  char __user *arg)\n{\n\tstruct blk_user_trace_setup buts;\n\tstruct compat_blk_user_trace_setup cbuts;\n\tint ret;\n\n\tif (copy_from_user(&cbuts, arg, sizeof(cbuts)))\n\t\treturn -EFAULT;\n\n\tbuts = (struct blk_user_trace_setup) {\n\t\t.act_mask = cbuts.act_mask,\n\t\t.buf_size = cbuts.buf_size,\n\t\t.buf_nr = cbuts.buf_nr,\n\t\t.start_lba = cbuts.start_lba,\n\t\t.end_lba = cbuts.end_lba,\n\t\t.pid = cbuts.pid,\n\t};\n\n\tret = do_blk_trace_setup(q, name, dev, bdev, &buts);\n\tif (ret)\n\t\treturn ret;\n\n\tif (copy_to_user(arg, &buts.name, ARRAY_SIZE(buts.name))) {\n\t\t__blk_trace_remove(q);\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic int __blk_trace_startstop(struct request_queue *q, int start)\n{\n\tstruct blk_trace *bt;\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->debugfs_mutex));\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\tif (start)\n\t\treturn blk_trace_start(bt);\n\telse\n\t\treturn blk_trace_stop(bt);\n}\n\nint blk_trace_startstop(struct request_queue *q, int start)\n{\n\tint ret;\n\n\tmutex_lock(&q->debugfs_mutex);\n\tret = __blk_trace_startstop(q, start);\n\tmutex_unlock(&q->debugfs_mutex);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blk_trace_startstop);\n\n \n\n \nint blk_trace_ioctl(struct block_device *bdev, unsigned cmd, char __user *arg)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tint ret, start = 0;\n\tchar b[BDEVNAME_SIZE];\n\n\tmutex_lock(&q->debugfs_mutex);\n\n\tswitch (cmd) {\n\tcase BLKTRACESETUP:\n\t\tsnprintf(b, sizeof(b), \"%pg\", bdev);\n\t\tret = __blk_trace_setup(q, b, bdev->bd_dev, bdev, arg);\n\t\tbreak;\n#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)\n\tcase BLKTRACESETUP32:\n\t\tsnprintf(b, sizeof(b), \"%pg\", bdev);\n\t\tret = compat_blk_trace_setup(q, b, bdev->bd_dev, bdev, arg);\n\t\tbreak;\n#endif\n\tcase BLKTRACESTART:\n\t\tstart = 1;\n\t\tfallthrough;\n\tcase BLKTRACESTOP:\n\t\tret = __blk_trace_startstop(q, start);\n\t\tbreak;\n\tcase BLKTRACETEARDOWN:\n\t\tret = __blk_trace_remove(q);\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOTTY;\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&q->debugfs_mutex);\n\treturn ret;\n}\n\n \nvoid blk_trace_shutdown(struct request_queue *q)\n{\n\tif (rcu_dereference_protected(q->blk_trace,\n\t\t\t\t      lockdep_is_held(&q->debugfs_mutex)))\n\t\t__blk_trace_remove(q);\n}\n\n#ifdef CONFIG_BLK_CGROUP\nstatic u64 blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)\n{\n\tstruct cgroup_subsys_state *blkcg_css;\n\tstruct blk_trace *bt;\n\n\t \n\tbt = rcu_dereference_protected(q->blk_trace, 1);\n\tif (!bt || !(blk_tracer_flags.val & TRACE_BLK_OPT_CGROUP))\n\t\treturn 0;\n\n\tblkcg_css = bio_blkcg_css(bio);\n\tif (!blkcg_css)\n\t\treturn 0;\n\treturn cgroup_id(blkcg_css->cgroup);\n}\n#else\nstatic u64 blk_trace_bio_get_cgid(struct request_queue *q, struct bio *bio)\n{\n\treturn 0;\n}\n#endif\n\nstatic u64\nblk_trace_request_get_cgid(struct request *rq)\n{\n\tif (!rq->bio)\n\t\treturn 0;\n\t \n\treturn blk_trace_bio_get_cgid(rq->q, rq->bio);\n}\n\n \n\n \nstatic void blk_add_trace_rq(struct request *rq, blk_status_t error,\n\t\t\t     unsigned int nr_bytes, u32 what, u64 cgid)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(rq->q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tif (blk_rq_is_passthrough(rq))\n\t\twhat |= BLK_TC_ACT(BLK_TC_PC);\n\telse\n\t\twhat |= BLK_TC_ACT(BLK_TC_FS);\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), nr_bytes, rq->cmd_flags,\n\t\t\twhat, blk_status_to_errno(error), 0, NULL, cgid);\n\trcu_read_unlock();\n}\n\nstatic void blk_add_trace_rq_insert(void *ignore, struct request *rq)\n{\n\tblk_add_trace_rq(rq, 0, blk_rq_bytes(rq), BLK_TA_INSERT,\n\t\t\t blk_trace_request_get_cgid(rq));\n}\n\nstatic void blk_add_trace_rq_issue(void *ignore, struct request *rq)\n{\n\tblk_add_trace_rq(rq, 0, blk_rq_bytes(rq), BLK_TA_ISSUE,\n\t\t\t blk_trace_request_get_cgid(rq));\n}\n\nstatic void blk_add_trace_rq_merge(void *ignore, struct request *rq)\n{\n\tblk_add_trace_rq(rq, 0, blk_rq_bytes(rq), BLK_TA_BACKMERGE,\n\t\t\t blk_trace_request_get_cgid(rq));\n}\n\nstatic void blk_add_trace_rq_requeue(void *ignore, struct request *rq)\n{\n\tblk_add_trace_rq(rq, 0, blk_rq_bytes(rq), BLK_TA_REQUEUE,\n\t\t\t blk_trace_request_get_cgid(rq));\n}\n\nstatic void blk_add_trace_rq_complete(void *ignore, struct request *rq,\n\t\t\tblk_status_t error, unsigned int nr_bytes)\n{\n\tblk_add_trace_rq(rq, error, nr_bytes, BLK_TA_COMPLETE,\n\t\t\t blk_trace_request_get_cgid(rq));\n}\n\n \nstatic void blk_add_trace_bio(struct request_queue *q, struct bio *bio,\n\t\t\t      u32 what, int error)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio->bi_opf, what, error, 0, NULL,\n\t\t\tblk_trace_bio_get_cgid(q, bio));\n\trcu_read_unlock();\n}\n\nstatic void blk_add_trace_bio_bounce(void *ignore, struct bio *bio)\n{\n\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_BOUNCE, 0);\n}\n\nstatic void blk_add_trace_bio_complete(void *ignore,\n\t\t\t\t       struct request_queue *q, struct bio *bio)\n{\n\tblk_add_trace_bio(q, bio, BLK_TA_COMPLETE,\n\t\t\t  blk_status_to_errno(bio->bi_status));\n}\n\nstatic void blk_add_trace_bio_backmerge(void *ignore, struct bio *bio)\n{\n\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_BACKMERGE,\n\t\t\t0);\n}\n\nstatic void blk_add_trace_bio_frontmerge(void *ignore, struct bio *bio)\n{\n\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_FRONTMERGE,\n\t\t\t0);\n}\n\nstatic void blk_add_trace_bio_queue(void *ignore, struct bio *bio)\n{\n\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_QUEUE, 0);\n}\n\nstatic void blk_add_trace_getrq(void *ignore, struct bio *bio)\n{\n\tblk_add_trace_bio(bio->bi_bdev->bd_disk->queue, bio, BLK_TA_GETRQ, 0);\n}\n\nstatic void blk_add_trace_plug(void *ignore, struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt)\n\t\t__blk_add_trace(bt, 0, 0, 0, BLK_TA_PLUG, 0, 0, NULL, 0);\n\trcu_read_unlock();\n}\n\nstatic void blk_add_trace_unplug(void *ignore, struct request_queue *q,\n\t\t\t\t    unsigned int depth, bool explicit)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(depth);\n\t\tu32 what;\n\n\t\tif (explicit)\n\t\t\twhat = BLK_TA_UNPLUG_IO;\n\t\telse\n\t\t\twhat = BLK_TA_UNPLUG_TIMER;\n\n\t\t__blk_add_trace(bt, 0, 0, 0, what, 0, sizeof(rpdu), &rpdu, 0);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void blk_add_trace_split(void *ignore, struct bio *bio, unsigned int pdu)\n{\n\tstruct request_queue *q = bio->bi_bdev->bd_disk->queue;\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (bt) {\n\t\t__be64 rpdu = cpu_to_be64(pdu);\n\n\t\t__blk_add_trace(bt, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_iter.bi_size, bio->bi_opf, BLK_TA_SPLIT,\n\t\t\t\tblk_status_to_errno(bio->bi_status),\n\t\t\t\tsizeof(rpdu), &rpdu,\n\t\t\t\tblk_trace_bio_get_cgid(q, bio));\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic void blk_add_trace_bio_remap(void *ignore, struct bio *bio, dev_t dev,\n\t\t\t\t    sector_t from)\n{\n\tstruct request_queue *q = bio->bi_bdev->bd_disk->queue;\n\tstruct blk_trace *bt;\n\tstruct blk_io_trace_remap r;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tr.device_from = cpu_to_be32(dev);\n\tr.device_to   = cpu_to_be32(bio_dev(bio));\n\tr.sector_from = cpu_to_be64(from);\n\n\t__blk_add_trace(bt, bio->bi_iter.bi_sector, bio->bi_iter.bi_size,\n\t\t\tbio->bi_opf, BLK_TA_REMAP,\n\t\t\tblk_status_to_errno(bio->bi_status),\n\t\t\tsizeof(r), &r, blk_trace_bio_get_cgid(q, bio));\n\trcu_read_unlock();\n}\n\n \nstatic void blk_add_trace_rq_remap(void *ignore, struct request *rq, dev_t dev,\n\t\t\t\t   sector_t from)\n{\n\tstruct blk_trace *bt;\n\tstruct blk_io_trace_remap r;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(rq->q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\tr.device_from = cpu_to_be32(dev);\n\tr.device_to   = cpu_to_be32(disk_devt(rq->q->disk));\n\tr.sector_from = cpu_to_be64(from);\n\n\t__blk_add_trace(bt, blk_rq_pos(rq), blk_rq_bytes(rq),\n\t\t\trq->cmd_flags, BLK_TA_REMAP, 0,\n\t\t\tsizeof(r), &r, blk_trace_request_get_cgid(rq));\n\trcu_read_unlock();\n}\n\n \nvoid blk_add_driver_data(struct request *rq, void *data, size_t len)\n{\n\tstruct blk_trace *bt;\n\n\trcu_read_lock();\n\tbt = rcu_dereference(rq->q->blk_trace);\n\tif (likely(!bt)) {\n\t\trcu_read_unlock();\n\t\treturn;\n\t}\n\n\t__blk_add_trace(bt, blk_rq_trace_sector(rq), blk_rq_bytes(rq), 0,\n\t\t\t\tBLK_TA_DRV_DATA, 0, len, data,\n\t\t\t\tblk_trace_request_get_cgid(rq));\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(blk_add_driver_data);\n\nstatic void blk_register_tracepoints(void)\n{\n\tint ret;\n\n\tret = register_trace_block_rq_insert(blk_add_trace_rq_insert, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_rq_issue(blk_add_trace_rq_issue, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_rq_merge(blk_add_trace_rq_merge, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_rq_requeue(blk_add_trace_rq_requeue, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_rq_complete(blk_add_trace_rq_complete, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_bio_bounce(blk_add_trace_bio_bounce, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_bio_complete(blk_add_trace_bio_complete, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_bio_backmerge(blk_add_trace_bio_backmerge, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_bio_frontmerge(blk_add_trace_bio_frontmerge, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_bio_queue(blk_add_trace_bio_queue, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_getrq(blk_add_trace_getrq, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_plug(blk_add_trace_plug, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_unplug(blk_add_trace_unplug, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_split(blk_add_trace_split, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_bio_remap(blk_add_trace_bio_remap, NULL);\n\tWARN_ON(ret);\n\tret = register_trace_block_rq_remap(blk_add_trace_rq_remap, NULL);\n\tWARN_ON(ret);\n}\n\nstatic void blk_unregister_tracepoints(void)\n{\n\tunregister_trace_block_rq_remap(blk_add_trace_rq_remap, NULL);\n\tunregister_trace_block_bio_remap(blk_add_trace_bio_remap, NULL);\n\tunregister_trace_block_split(blk_add_trace_split, NULL);\n\tunregister_trace_block_unplug(blk_add_trace_unplug, NULL);\n\tunregister_trace_block_plug(blk_add_trace_plug, NULL);\n\tunregister_trace_block_getrq(blk_add_trace_getrq, NULL);\n\tunregister_trace_block_bio_queue(blk_add_trace_bio_queue, NULL);\n\tunregister_trace_block_bio_frontmerge(blk_add_trace_bio_frontmerge, NULL);\n\tunregister_trace_block_bio_backmerge(blk_add_trace_bio_backmerge, NULL);\n\tunregister_trace_block_bio_complete(blk_add_trace_bio_complete, NULL);\n\tunregister_trace_block_bio_bounce(blk_add_trace_bio_bounce, NULL);\n\tunregister_trace_block_rq_complete(blk_add_trace_rq_complete, NULL);\n\tunregister_trace_block_rq_requeue(blk_add_trace_rq_requeue, NULL);\n\tunregister_trace_block_rq_merge(blk_add_trace_rq_merge, NULL);\n\tunregister_trace_block_rq_issue(blk_add_trace_rq_issue, NULL);\n\tunregister_trace_block_rq_insert(blk_add_trace_rq_insert, NULL);\n\n\ttracepoint_synchronize_unregister();\n}\n\n \n\nstatic void fill_rwbs(char *rwbs, const struct blk_io_trace *t)\n{\n\tint i = 0;\n\tint tc = t->action >> BLK_TC_SHIFT;\n\n\tif ((t->action & ~__BLK_TN_CGROUP) == BLK_TN_MESSAGE) {\n\t\trwbs[i++] = 'N';\n\t\tgoto out;\n\t}\n\n\tif (tc & BLK_TC_FLUSH)\n\t\trwbs[i++] = 'F';\n\n\tif (tc & BLK_TC_DISCARD)\n\t\trwbs[i++] = 'D';\n\telse if (tc & BLK_TC_WRITE)\n\t\trwbs[i++] = 'W';\n\telse if (t->bytes)\n\t\trwbs[i++] = 'R';\n\telse\n\t\trwbs[i++] = 'N';\n\n\tif (tc & BLK_TC_FUA)\n\t\trwbs[i++] = 'F';\n\tif (tc & BLK_TC_AHEAD)\n\t\trwbs[i++] = 'A';\n\tif (tc & BLK_TC_SYNC)\n\t\trwbs[i++] = 'S';\n\tif (tc & BLK_TC_META)\n\t\trwbs[i++] = 'M';\nout:\n\trwbs[i] = '\\0';\n}\n\nstatic inline\nconst struct blk_io_trace *te_blk_io_trace(const struct trace_entry *ent)\n{\n\treturn (const struct blk_io_trace *)ent;\n}\n\nstatic inline const void *pdu_start(const struct trace_entry *ent, bool has_cg)\n{\n\treturn (void *)(te_blk_io_trace(ent) + 1) + (has_cg ? sizeof(u64) : 0);\n}\n\nstatic inline u64 t_cgid(const struct trace_entry *ent)\n{\n\treturn *(u64 *)(te_blk_io_trace(ent) + 1);\n}\n\nstatic inline int pdu_real_len(const struct trace_entry *ent, bool has_cg)\n{\n\treturn te_blk_io_trace(ent)->pdu_len - (has_cg ? sizeof(u64) : 0);\n}\n\nstatic inline u32 t_action(const struct trace_entry *ent)\n{\n\treturn te_blk_io_trace(ent)->action;\n}\n\nstatic inline u32 t_bytes(const struct trace_entry *ent)\n{\n\treturn te_blk_io_trace(ent)->bytes;\n}\n\nstatic inline u32 t_sec(const struct trace_entry *ent)\n{\n\treturn te_blk_io_trace(ent)->bytes >> 9;\n}\n\nstatic inline unsigned long long t_sector(const struct trace_entry *ent)\n{\n\treturn te_blk_io_trace(ent)->sector;\n}\n\nstatic inline __u16 t_error(const struct trace_entry *ent)\n{\n\treturn te_blk_io_trace(ent)->error;\n}\n\nstatic __u64 get_pdu_int(const struct trace_entry *ent, bool has_cg)\n{\n\tconst __be64 *val = pdu_start(ent, has_cg);\n\treturn be64_to_cpu(*val);\n}\n\ntypedef void (blk_log_action_t) (struct trace_iterator *iter, const char *act,\n\tbool has_cg);\n\nstatic void blk_log_action_classic(struct trace_iterator *iter, const char *act,\n\tbool has_cg)\n{\n\tchar rwbs[RWBS_LEN];\n\tunsigned long long ts  = iter->ts;\n\tunsigned long nsec_rem = do_div(ts, NSEC_PER_SEC);\n\tunsigned secs\t       = (unsigned long)ts;\n\tconst struct blk_io_trace *t = te_blk_io_trace(iter->ent);\n\n\tfill_rwbs(rwbs, t);\n\n\ttrace_seq_printf(&iter->seq,\n\t\t\t \"%3d,%-3d %2d %5d.%09lu %5u %2s %3s \",\n\t\t\t MAJOR(t->device), MINOR(t->device), iter->cpu,\n\t\t\t secs, nsec_rem, iter->ent->pid, act, rwbs);\n}\n\nstatic void blk_log_action(struct trace_iterator *iter, const char *act,\n\tbool has_cg)\n{\n\tchar rwbs[RWBS_LEN];\n\tconst struct blk_io_trace *t = te_blk_io_trace(iter->ent);\n\n\tfill_rwbs(rwbs, t);\n\tif (has_cg) {\n\t\tu64 id = t_cgid(iter->ent);\n\n\t\tif (blk_tracer_flags.val & TRACE_BLK_OPT_CGNAME) {\n\t\t\tchar blkcg_name_buf[NAME_MAX + 1] = \"<...>\";\n\n\t\t\tcgroup_path_from_kernfs_id(id, blkcg_name_buf,\n\t\t\t\tsizeof(blkcg_name_buf));\n\t\t\ttrace_seq_printf(&iter->seq, \"%3d,%-3d %s %2s %3s \",\n\t\t\t\t MAJOR(t->device), MINOR(t->device),\n\t\t\t\t blkcg_name_buf, act, rwbs);\n\t\t} else {\n\t\t\t \n\t\t\ttrace_seq_printf(&iter->seq,\n\t\t\t\t \"%3d,%-3d %llx,%-llx %2s %3s \",\n\t\t\t\t MAJOR(t->device), MINOR(t->device),\n\t\t\t\t id & U32_MAX, id >> 32, act, rwbs);\n\t\t}\n\t} else\n\t\ttrace_seq_printf(&iter->seq, \"%3d,%-3d %2s %3s \",\n\t\t\t\t MAJOR(t->device), MINOR(t->device), act, rwbs);\n}\n\nstatic void blk_log_dump_pdu(struct trace_seq *s,\n\tconst struct trace_entry *ent, bool has_cg)\n{\n\tconst unsigned char *pdu_buf;\n\tint pdu_len;\n\tint i, end;\n\n\tpdu_buf = pdu_start(ent, has_cg);\n\tpdu_len = pdu_real_len(ent, has_cg);\n\n\tif (!pdu_len)\n\t\treturn;\n\n\t \n\tfor (end = pdu_len - 1; end >= 0; end--)\n\t\tif (pdu_buf[end])\n\t\t\tbreak;\n\tend++;\n\n\ttrace_seq_putc(s, '(');\n\n\tfor (i = 0; i < pdu_len; i++) {\n\n\t\ttrace_seq_printf(s, \"%s%02x\",\n\t\t\t\t i == 0 ? \"\" : \" \", pdu_buf[i]);\n\n\t\t \n\t\tif (i == end && end != pdu_len - 1) {\n\t\t\ttrace_seq_puts(s, \" ..) \");\n\t\t\treturn;\n\t\t}\n\t}\n\n\ttrace_seq_puts(s, \") \");\n}\n\nstatic void blk_log_generic(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)\n{\n\tchar cmd[TASK_COMM_LEN];\n\n\ttrace_find_cmdline(ent->pid, cmd);\n\n\tif (t_action(ent) & BLK_TC_ACT(BLK_TC_PC)) {\n\t\ttrace_seq_printf(s, \"%u \", t_bytes(ent));\n\t\tblk_log_dump_pdu(s, ent, has_cg);\n\t\ttrace_seq_printf(s, \"[%s]\\n\", cmd);\n\t} else {\n\t\tif (t_sec(ent))\n\t\t\ttrace_seq_printf(s, \"%llu + %u [%s]\\n\",\n\t\t\t\t\t\tt_sector(ent), t_sec(ent), cmd);\n\t\telse\n\t\t\ttrace_seq_printf(s, \"[%s]\\n\", cmd);\n\t}\n}\n\nstatic void blk_log_with_error(struct trace_seq *s,\n\t\t\t      const struct trace_entry *ent, bool has_cg)\n{\n\tif (t_action(ent) & BLK_TC_ACT(BLK_TC_PC)) {\n\t\tblk_log_dump_pdu(s, ent, has_cg);\n\t\ttrace_seq_printf(s, \"[%d]\\n\", t_error(ent));\n\t} else {\n\t\tif (t_sec(ent))\n\t\t\ttrace_seq_printf(s, \"%llu + %u [%d]\\n\",\n\t\t\t\t\t t_sector(ent),\n\t\t\t\t\t t_sec(ent), t_error(ent));\n\t\telse\n\t\t\ttrace_seq_printf(s, \"%llu [%d]\\n\",\n\t\t\t\t\t t_sector(ent), t_error(ent));\n\t}\n}\n\nstatic void blk_log_remap(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)\n{\n\tconst struct blk_io_trace_remap *__r = pdu_start(ent, has_cg);\n\n\ttrace_seq_printf(s, \"%llu + %u <- (%d,%d) %llu\\n\",\n\t\t\t t_sector(ent), t_sec(ent),\n\t\t\t MAJOR(be32_to_cpu(__r->device_from)),\n\t\t\t MINOR(be32_to_cpu(__r->device_from)),\n\t\t\t be64_to_cpu(__r->sector_from));\n}\n\nstatic void blk_log_plug(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)\n{\n\tchar cmd[TASK_COMM_LEN];\n\n\ttrace_find_cmdline(ent->pid, cmd);\n\n\ttrace_seq_printf(s, \"[%s]\\n\", cmd);\n}\n\nstatic void blk_log_unplug(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)\n{\n\tchar cmd[TASK_COMM_LEN];\n\n\ttrace_find_cmdline(ent->pid, cmd);\n\n\ttrace_seq_printf(s, \"[%s] %llu\\n\", cmd, get_pdu_int(ent, has_cg));\n}\n\nstatic void blk_log_split(struct trace_seq *s, const struct trace_entry *ent, bool has_cg)\n{\n\tchar cmd[TASK_COMM_LEN];\n\n\ttrace_find_cmdline(ent->pid, cmd);\n\n\ttrace_seq_printf(s, \"%llu / %llu [%s]\\n\", t_sector(ent),\n\t\t\t get_pdu_int(ent, has_cg), cmd);\n}\n\nstatic void blk_log_msg(struct trace_seq *s, const struct trace_entry *ent,\n\t\t\tbool has_cg)\n{\n\n\ttrace_seq_putmem(s, pdu_start(ent, has_cg),\n\t\tpdu_real_len(ent, has_cg));\n\ttrace_seq_putc(s, '\\n');\n}\n\n \n\nstatic void blk_tracer_print_header(struct seq_file *m)\n{\n\tif (!(blk_tracer_flags.val & TRACE_BLK_OPT_CLASSIC))\n\t\treturn;\n\tseq_puts(m, \"# DEV   CPU TIMESTAMP     PID ACT FLG\\n\"\n\t\t    \"#  |     |     |           |   |   |\\n\");\n}\n\nstatic void blk_tracer_start(struct trace_array *tr)\n{\n\tblk_tracer_enabled = true;\n}\n\nstatic int blk_tracer_init(struct trace_array *tr)\n{\n\tblk_tr = tr;\n\tblk_tracer_start(tr);\n\treturn 0;\n}\n\nstatic void blk_tracer_stop(struct trace_array *tr)\n{\n\tblk_tracer_enabled = false;\n}\n\nstatic void blk_tracer_reset(struct trace_array *tr)\n{\n\tblk_tracer_stop(tr);\n}\n\nstatic const struct {\n\tconst char *act[2];\n\tvoid\t   (*print)(struct trace_seq *s, const struct trace_entry *ent,\n\t\t\t    bool has_cg);\n} what2act[] = {\n\t[__BLK_TA_QUEUE]\t= {{  \"Q\", \"queue\" },\t   blk_log_generic },\n\t[__BLK_TA_BACKMERGE]\t= {{  \"M\", \"backmerge\" },  blk_log_generic },\n\t[__BLK_TA_FRONTMERGE]\t= {{  \"F\", \"frontmerge\" }, blk_log_generic },\n\t[__BLK_TA_GETRQ]\t= {{  \"G\", \"getrq\" },\t   blk_log_generic },\n\t[__BLK_TA_SLEEPRQ]\t= {{  \"S\", \"sleeprq\" },\t   blk_log_generic },\n\t[__BLK_TA_REQUEUE]\t= {{  \"R\", \"requeue\" },\t   blk_log_with_error },\n\t[__BLK_TA_ISSUE]\t= {{  \"D\", \"issue\" },\t   blk_log_generic },\n\t[__BLK_TA_COMPLETE]\t= {{  \"C\", \"complete\" },   blk_log_with_error },\n\t[__BLK_TA_PLUG]\t\t= {{  \"P\", \"plug\" },\t   blk_log_plug },\n\t[__BLK_TA_UNPLUG_IO]\t= {{  \"U\", \"unplug_io\" },  blk_log_unplug },\n\t[__BLK_TA_UNPLUG_TIMER]\t= {{ \"UT\", \"unplug_timer\" }, blk_log_unplug },\n\t[__BLK_TA_INSERT]\t= {{  \"I\", \"insert\" },\t   blk_log_generic },\n\t[__BLK_TA_SPLIT]\t= {{  \"X\", \"split\" },\t   blk_log_split },\n\t[__BLK_TA_BOUNCE]\t= {{  \"B\", \"bounce\" },\t   blk_log_generic },\n\t[__BLK_TA_REMAP]\t= {{  \"A\", \"remap\" },\t   blk_log_remap },\n};\n\nstatic enum print_line_t print_one_line(struct trace_iterator *iter,\n\t\t\t\t\tbool classic)\n{\n\tstruct trace_array *tr = iter->tr;\n\tstruct trace_seq *s = &iter->seq;\n\tconst struct blk_io_trace *t;\n\tu16 what;\n\tbool long_act;\n\tblk_log_action_t *log_action;\n\tbool has_cg;\n\n\tt\t   = te_blk_io_trace(iter->ent);\n\twhat\t   = (t->action & ((1 << BLK_TC_SHIFT) - 1)) & ~__BLK_TA_CGROUP;\n\tlong_act   = !!(tr->trace_flags & TRACE_ITER_VERBOSE);\n\tlog_action = classic ? &blk_log_action_classic : &blk_log_action;\n\thas_cg\t   = t->action & __BLK_TA_CGROUP;\n\n\tif ((t->action & ~__BLK_TN_CGROUP) == BLK_TN_MESSAGE) {\n\t\tlog_action(iter, long_act ? \"message\" : \"m\", has_cg);\n\t\tblk_log_msg(s, iter->ent, has_cg);\n\t\treturn trace_handle_return(s);\n\t}\n\n\tif (unlikely(what == 0 || what >= ARRAY_SIZE(what2act)))\n\t\ttrace_seq_printf(s, \"Unknown action %x\\n\", what);\n\telse {\n\t\tlog_action(iter, what2act[what].act[long_act], has_cg);\n\t\twhat2act[what].print(s, iter->ent, has_cg);\n\t}\n\n\treturn trace_handle_return(s);\n}\n\nstatic enum print_line_t blk_trace_event_print(struct trace_iterator *iter,\n\t\t\t\t\t       int flags, struct trace_event *event)\n{\n\treturn print_one_line(iter, false);\n}\n\nstatic void blk_trace_synthesize_old_trace(struct trace_iterator *iter)\n{\n\tstruct trace_seq *s = &iter->seq;\n\tstruct blk_io_trace *t = (struct blk_io_trace *)iter->ent;\n\tconst int offset = offsetof(struct blk_io_trace, sector);\n\tstruct blk_io_trace old = {\n\t\t.magic\t  = BLK_IO_TRACE_MAGIC | BLK_IO_TRACE_VERSION,\n\t\t.time     = iter->ts,\n\t};\n\n\ttrace_seq_putmem(s, &old, offset);\n\ttrace_seq_putmem(s, &t->sector,\n\t\t\t sizeof(old) - offset + t->pdu_len);\n}\n\nstatic enum print_line_t\nblk_trace_event_print_binary(struct trace_iterator *iter, int flags,\n\t\t\t     struct trace_event *event)\n{\n\tblk_trace_synthesize_old_trace(iter);\n\n\treturn trace_handle_return(&iter->seq);\n}\n\nstatic enum print_line_t blk_tracer_print_line(struct trace_iterator *iter)\n{\n\tif ((iter->ent->type != TRACE_BLK) ||\n\t    !(blk_tracer_flags.val & TRACE_BLK_OPT_CLASSIC))\n\t\treturn TRACE_TYPE_UNHANDLED;\n\n\treturn print_one_line(iter, true);\n}\n\nstatic int\nblk_tracer_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)\n{\n\t \n\tif (bit == TRACE_BLK_OPT_CLASSIC) {\n\t\tif (set)\n\t\t\ttr->trace_flags &= ~TRACE_ITER_CONTEXT_INFO;\n\t\telse\n\t\t\ttr->trace_flags |= TRACE_ITER_CONTEXT_INFO;\n\t}\n\treturn 0;\n}\n\nstatic struct tracer blk_tracer __read_mostly = {\n\t.name\t\t= \"blk\",\n\t.init\t\t= blk_tracer_init,\n\t.reset\t\t= blk_tracer_reset,\n\t.start\t\t= blk_tracer_start,\n\t.stop\t\t= blk_tracer_stop,\n\t.print_header\t= blk_tracer_print_header,\n\t.print_line\t= blk_tracer_print_line,\n\t.flags\t\t= &blk_tracer_flags,\n\t.set_flag\t= blk_tracer_set_flag,\n};\n\nstatic struct trace_event_functions trace_blk_event_funcs = {\n\t.trace\t\t= blk_trace_event_print,\n\t.binary\t\t= blk_trace_event_print_binary,\n};\n\nstatic struct trace_event trace_blk_event = {\n\t.type\t\t= TRACE_BLK,\n\t.funcs\t\t= &trace_blk_event_funcs,\n};\n\nstatic int __init init_blk_tracer(void)\n{\n\tif (!register_trace_event(&trace_blk_event)) {\n\t\tpr_warn(\"Warning: could not register block events\\n\");\n\t\treturn 1;\n\t}\n\n\tif (register_tracer(&blk_tracer) != 0) {\n\t\tpr_warn(\"Warning: could not register the block tracer\\n\");\n\t\tunregister_trace_event(&trace_blk_event);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\ndevice_initcall(init_blk_tracer);\n\nstatic int blk_trace_remove_queue(struct request_queue *q)\n{\n\tstruct blk_trace *bt;\n\n\tbt = rcu_replace_pointer(q->blk_trace, NULL,\n\t\t\t\t lockdep_is_held(&q->debugfs_mutex));\n\tif (bt == NULL)\n\t\treturn -EINVAL;\n\n\tblk_trace_stop(bt);\n\n\tput_probe_ref();\n\tsynchronize_rcu();\n\tblk_trace_free(q, bt);\n\treturn 0;\n}\n\n \nstatic int blk_trace_setup_queue(struct request_queue *q,\n\t\t\t\t struct block_device *bdev)\n{\n\tstruct blk_trace *bt = NULL;\n\tint ret = -ENOMEM;\n\n\tbt = kzalloc(sizeof(*bt), GFP_KERNEL);\n\tif (!bt)\n\t\treturn -ENOMEM;\n\n\tbt->msg_data = __alloc_percpu(BLK_TN_MAX_MSG, __alignof__(char));\n\tif (!bt->msg_data)\n\t\tgoto free_bt;\n\n\tbt->dev = bdev->bd_dev;\n\tbt->act_mask = (u16)-1;\n\n\tblk_trace_setup_lba(bt, bdev);\n\n\trcu_assign_pointer(q->blk_trace, bt);\n\tget_probe_ref();\n\treturn 0;\n\nfree_bt:\n\tblk_trace_free(q, bt);\n\treturn ret;\n}\n\n \n\nstatic ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf);\nstatic ssize_t sysfs_blk_trace_attr_store(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count);\n#define BLK_TRACE_DEVICE_ATTR(_name) \\\n\tDEVICE_ATTR(_name, S_IRUGO | S_IWUSR, \\\n\t\t    sysfs_blk_trace_attr_show, \\\n\t\t    sysfs_blk_trace_attr_store)\n\nstatic BLK_TRACE_DEVICE_ATTR(enable);\nstatic BLK_TRACE_DEVICE_ATTR(act_mask);\nstatic BLK_TRACE_DEVICE_ATTR(pid);\nstatic BLK_TRACE_DEVICE_ATTR(start_lba);\nstatic BLK_TRACE_DEVICE_ATTR(end_lba);\n\nstatic struct attribute *blk_trace_attrs[] = {\n\t&dev_attr_enable.attr,\n\t&dev_attr_act_mask.attr,\n\t&dev_attr_pid.attr,\n\t&dev_attr_start_lba.attr,\n\t&dev_attr_end_lba.attr,\n\tNULL\n};\n\nstruct attribute_group blk_trace_attr_group = {\n\t.name  = \"trace\",\n\t.attrs = blk_trace_attrs,\n};\n\nstatic const struct {\n\tint mask;\n\tconst char *str;\n} mask_maps[] = {\n\t{ BLK_TC_READ,\t\t\"read\"\t\t},\n\t{ BLK_TC_WRITE,\t\t\"write\"\t\t},\n\t{ BLK_TC_FLUSH,\t\t\"flush\"\t\t},\n\t{ BLK_TC_SYNC,\t\t\"sync\"\t\t},\n\t{ BLK_TC_QUEUE,\t\t\"queue\"\t\t},\n\t{ BLK_TC_REQUEUE,\t\"requeue\"\t},\n\t{ BLK_TC_ISSUE,\t\t\"issue\"\t\t},\n\t{ BLK_TC_COMPLETE,\t\"complete\"\t},\n\t{ BLK_TC_FS,\t\t\"fs\"\t\t},\n\t{ BLK_TC_PC,\t\t\"pc\"\t\t},\n\t{ BLK_TC_NOTIFY,\t\"notify\"\t},\n\t{ BLK_TC_AHEAD,\t\t\"ahead\"\t\t},\n\t{ BLK_TC_META,\t\t\"meta\"\t\t},\n\t{ BLK_TC_DISCARD,\t\"discard\"\t},\n\t{ BLK_TC_DRV_DATA,\t\"drv_data\"\t},\n\t{ BLK_TC_FUA,\t\t\"fua\"\t\t},\n};\n\nstatic int blk_trace_str2mask(const char *str)\n{\n\tint i;\n\tint mask = 0;\n\tchar *buf, *s, *token;\n\n\tbuf = kstrdup(str, GFP_KERNEL);\n\tif (buf == NULL)\n\t\treturn -ENOMEM;\n\ts = strstrip(buf);\n\n\twhile (1) {\n\t\ttoken = strsep(&s, \",\");\n\t\tif (token == NULL)\n\t\t\tbreak;\n\n\t\tif (*token == '\\0')\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(mask_maps); i++) {\n\t\t\tif (strcasecmp(token, mask_maps[i].str) == 0) {\n\t\t\t\tmask |= mask_maps[i].mask;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (i == ARRAY_SIZE(mask_maps)) {\n\t\t\tmask = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\tkfree(buf);\n\n\treturn mask;\n}\n\nstatic ssize_t blk_trace_mask2str(char *buf, int mask)\n{\n\tint i;\n\tchar *p = buf;\n\n\tfor (i = 0; i < ARRAY_SIZE(mask_maps); i++) {\n\t\tif (mask & mask_maps[i].mask) {\n\t\t\tp += sprintf(p, \"%s%s\",\n\t\t\t\t    (p == buf) ? \"\" : \",\", mask_maps[i].str);\n\t\t}\n\t}\n\t*p++ = '\\n';\n\n\treturn p - buf;\n}\n\nstatic ssize_t sysfs_blk_trace_attr_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct block_device *bdev = dev_to_bdev(dev);\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tstruct blk_trace *bt;\n\tssize_t ret = -ENXIO;\n\n\tmutex_lock(&q->debugfs_mutex);\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->debugfs_mutex));\n\tif (attr == &dev_attr_enable) {\n\t\tret = sprintf(buf, \"%u\\n\", !!bt);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tif (bt == NULL)\n\t\tret = sprintf(buf, \"disabled\\n\");\n\telse if (attr == &dev_attr_act_mask)\n\t\tret = blk_trace_mask2str(buf, bt->act_mask);\n\telse if (attr == &dev_attr_pid)\n\t\tret = sprintf(buf, \"%u\\n\", bt->pid);\n\telse if (attr == &dev_attr_start_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->start_lba);\n\telse if (attr == &dev_attr_end_lba)\n\t\tret = sprintf(buf, \"%llu\\n\", bt->end_lba);\n\nout_unlock_bdev:\n\tmutex_unlock(&q->debugfs_mutex);\n\treturn ret;\n}\n\nstatic ssize_t sysfs_blk_trace_attr_store(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  const char *buf, size_t count)\n{\n\tstruct block_device *bdev = dev_to_bdev(dev);\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tstruct blk_trace *bt;\n\tu64 value;\n\tssize_t ret = -EINVAL;\n\n\tif (count == 0)\n\t\tgoto out;\n\n\tif (attr == &dev_attr_act_mask) {\n\t\tif (kstrtoull(buf, 0, &value)) {\n\t\t\t \n\t\t\tret = blk_trace_str2mask(buf);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tvalue = ret;\n\t\t}\n\t} else {\n\t\tif (kstrtoull(buf, 0, &value))\n\t\t\tgoto out;\n\t}\n\n\tmutex_lock(&q->debugfs_mutex);\n\n\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\t       lockdep_is_held(&q->debugfs_mutex));\n\tif (attr == &dev_attr_enable) {\n\t\tif (!!value == !!bt) {\n\t\t\tret = 0;\n\t\t\tgoto out_unlock_bdev;\n\t\t}\n\t\tif (value)\n\t\t\tret = blk_trace_setup_queue(q, bdev);\n\t\telse\n\t\t\tret = blk_trace_remove_queue(q);\n\t\tgoto out_unlock_bdev;\n\t}\n\n\tret = 0;\n\tif (bt == NULL) {\n\t\tret = blk_trace_setup_queue(q, bdev);\n\t\tbt = rcu_dereference_protected(q->blk_trace,\n\t\t\t\tlockdep_is_held(&q->debugfs_mutex));\n\t}\n\n\tif (ret == 0) {\n\t\tif (attr == &dev_attr_act_mask)\n\t\t\tbt->act_mask = value;\n\t\telse if (attr == &dev_attr_pid)\n\t\t\tbt->pid = value;\n\t\telse if (attr == &dev_attr_start_lba)\n\t\t\tbt->start_lba = value;\n\t\telse if (attr == &dev_attr_end_lba)\n\t\t\tbt->end_lba = value;\n\t}\n\nout_unlock_bdev:\n\tmutex_unlock(&q->debugfs_mutex);\nout:\n\treturn ret ? ret : count;\n}\n#endif  \n\n#ifdef CONFIG_EVENT_TRACING\n\n \nvoid blk_fill_rwbs(char *rwbs, blk_opf_t opf)\n{\n\tint i = 0;\n\n\tif (opf & REQ_PREFLUSH)\n\t\trwbs[i++] = 'F';\n\n\tswitch (opf & REQ_OP_MASK) {\n\tcase REQ_OP_WRITE:\n\t\trwbs[i++] = 'W';\n\t\tbreak;\n\tcase REQ_OP_DISCARD:\n\t\trwbs[i++] = 'D';\n\t\tbreak;\n\tcase REQ_OP_SECURE_ERASE:\n\t\trwbs[i++] = 'D';\n\t\trwbs[i++] = 'E';\n\t\tbreak;\n\tcase REQ_OP_FLUSH:\n\t\trwbs[i++] = 'F';\n\t\tbreak;\n\tcase REQ_OP_READ:\n\t\trwbs[i++] = 'R';\n\t\tbreak;\n\tdefault:\n\t\trwbs[i++] = 'N';\n\t}\n\n\tif (opf & REQ_FUA)\n\t\trwbs[i++] = 'F';\n\tif (opf & REQ_RAHEAD)\n\t\trwbs[i++] = 'A';\n\tif (opf & REQ_SYNC)\n\t\trwbs[i++] = 'S';\n\tif (opf & REQ_META)\n\t\trwbs[i++] = 'M';\n\n\trwbs[i] = '\\0';\n}\nEXPORT_SYMBOL_GPL(blk_fill_rwbs);\n\n#endif  \n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}