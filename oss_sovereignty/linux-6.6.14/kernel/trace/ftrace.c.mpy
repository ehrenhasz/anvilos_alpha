{
  "module_name": "ftrace.c",
  "hash_id": "6f8581abe5962ca86948d05689dedfaf4ccf705c3003db4cf53051c8d7962fbd",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/ftrace.c",
  "human_readable_source": "\n \n\n#include <linux/stop_machine.h>\n#include <linux/clocksource.h>\n#include <linux/sched/task.h>\n#include <linux/kallsyms.h>\n#include <linux/security.h>\n#include <linux/seq_file.h>\n#include <linux/tracefs.h>\n#include <linux/hardirq.h>\n#include <linux/kthread.h>\n#include <linux/uaccess.h>\n#include <linux/bsearch.h>\n#include <linux/module.h>\n#include <linux/ftrace.h>\n#include <linux/sysctl.h>\n#include <linux/slab.h>\n#include <linux/ctype.h>\n#include <linux/sort.h>\n#include <linux/list.h>\n#include <linux/hash.h>\n#include <linux/rcupdate.h>\n#include <linux/kprobes.h>\n\n#include <trace/events/sched.h>\n\n#include <asm/sections.h>\n#include <asm/setup.h>\n\n#include \"ftrace_internal.h\"\n#include \"trace_output.h\"\n#include \"trace_stat.h\"\n\n \n#define FTRACE_NOCLEAR_FLAGS\t(FTRACE_FL_DISABLED | FTRACE_FL_TOUCHED | \\\n\t\t\t\t FTRACE_FL_MODIFIED)\n\n#define FTRACE_INVALID_FUNCTION\t\t\"__ftrace_invalid_address__\"\n\n#define FTRACE_WARN_ON(cond)\t\t\t\\\n\t({\t\t\t\t\t\\\n\t\tint ___r = cond;\t\t\\\n\t\tif (WARN_ON(___r))\t\t\\\n\t\t\tftrace_kill();\t\t\\\n\t\t___r;\t\t\t\t\\\n\t})\n\n#define FTRACE_WARN_ON_ONCE(cond)\t\t\\\n\t({\t\t\t\t\t\\\n\t\tint ___r = cond;\t\t\\\n\t\tif (WARN_ON_ONCE(___r))\t\t\\\n\t\t\tftrace_kill();\t\t\\\n\t\t___r;\t\t\t\t\\\n\t})\n\n \n#define FTRACE_HASH_DEFAULT_BITS 10\n#define FTRACE_HASH_MAX_BITS 12\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n#define INIT_OPS_HASH(opsname)\t\\\n\t.func_hash\t\t= &opsname.local_hash,\t\t\t\\\n\t.local_hash.regex_lock\t= __MUTEX_INITIALIZER(opsname.local_hash.regex_lock),\n#else\n#define INIT_OPS_HASH(opsname)\n#endif\n\nenum {\n\tFTRACE_MODIFY_ENABLE_FL\t\t= (1 << 0),\n\tFTRACE_MODIFY_MAY_SLEEP_FL\t= (1 << 1),\n};\n\nstruct ftrace_ops ftrace_list_end __read_mostly = {\n\t.func\t\t= ftrace_stub,\n\t.flags\t\t= FTRACE_OPS_FL_STUB,\n\tINIT_OPS_HASH(ftrace_list_end)\n};\n\n \nint ftrace_enabled __read_mostly;\nstatic int __maybe_unused last_ftrace_enabled;\n\n \nstruct ftrace_ops *function_trace_op __read_mostly = &ftrace_list_end;\n \nstatic struct ftrace_ops *set_function_trace_op;\n\nstatic bool ftrace_pids_enabled(struct ftrace_ops *ops)\n{\n\tstruct trace_array *tr;\n\n\tif (!(ops->flags & FTRACE_OPS_FL_PID) || !ops->private)\n\t\treturn false;\n\n\ttr = ops->private;\n\n\treturn tr->function_pids != NULL || tr->function_no_pids != NULL;\n}\n\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\n\n \nstatic int ftrace_disabled __read_mostly;\n\nDEFINE_MUTEX(ftrace_lock);\n\nstruct ftrace_ops __rcu *ftrace_ops_list __read_mostly = &ftrace_list_end;\nftrace_func_t ftrace_trace_function __read_mostly = ftrace_stub;\nstruct ftrace_ops global_ops;\n\n \nvoid ftrace_ops_list_func(unsigned long ip, unsigned long parent_ip,\n\t\t\t  struct ftrace_ops *op, struct ftrace_regs *fregs);\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_CALL_OPS\n \nconst struct ftrace_ops ftrace_list_ops = {\n\t.func\t= ftrace_ops_list_func,\n\t.flags\t= FTRACE_OPS_FL_STUB,\n};\n\nstatic void ftrace_ops_nop_func(unsigned long ip, unsigned long parent_ip,\n\t\t\t\tstruct ftrace_ops *op,\n\t\t\t\tstruct ftrace_regs *fregs)\n{\n\t \n}\n\n \nconst struct ftrace_ops ftrace_nop_ops = {\n\t.func\t= ftrace_ops_nop_func,\n\t.flags  = FTRACE_OPS_FL_STUB,\n};\n#endif\n\nstatic inline void ftrace_ops_init(struct ftrace_ops *ops)\n{\n#ifdef CONFIG_DYNAMIC_FTRACE\n\tif (!(ops->flags & FTRACE_OPS_FL_INITIALIZED)) {\n\t\tmutex_init(&ops->local_hash.regex_lock);\n\t\tops->func_hash = &ops->local_hash;\n\t\tops->flags |= FTRACE_OPS_FL_INITIALIZED;\n\t}\n#endif\n}\n\nstatic void ftrace_pid_func(unsigned long ip, unsigned long parent_ip,\n\t\t\t    struct ftrace_ops *op, struct ftrace_regs *fregs)\n{\n\tstruct trace_array *tr = op->private;\n\tint pid;\n\n\tif (tr) {\n\t\tpid = this_cpu_read(tr->array_buffer.data->ftrace_ignore_pid);\n\t\tif (pid == FTRACE_PID_IGNORE)\n\t\t\treturn;\n\t\tif (pid != FTRACE_PID_TRACE &&\n\t\t    pid != current->pid)\n\t\t\treturn;\n\t}\n\n\top->saved_func(ip, parent_ip, op, fregs);\n}\n\nstatic void ftrace_sync_ipi(void *data)\n{\n\t \n\tsmp_rmb();\n}\n\nstatic ftrace_func_t ftrace_ops_get_list_func(struct ftrace_ops *ops)\n{\n\t \n\tif (ops->flags & (FTRACE_OPS_FL_DYNAMIC | FTRACE_OPS_FL_RCU) ||\n\t    FTRACE_FORCE_LIST_FUNC)\n\t\treturn ftrace_ops_list_func;\n\n\treturn ftrace_ops_get_func(ops);\n}\n\nstatic void update_ftrace_function(void)\n{\n\tftrace_func_t func;\n\n\t \n\tset_function_trace_op = rcu_dereference_protected(ftrace_ops_list,\n\t\t\t\t\t\tlockdep_is_held(&ftrace_lock));\n\n\t \n\tif (set_function_trace_op == &ftrace_list_end) {\n\t\tfunc = ftrace_stub;\n\n\t \n\t} else if (rcu_dereference_protected(ftrace_ops_list->next,\n\t\t\tlockdep_is_held(&ftrace_lock)) == &ftrace_list_end) {\n\t\tfunc = ftrace_ops_get_list_func(ftrace_ops_list);\n\n\t} else {\n\t\t \n\t\tset_function_trace_op = &ftrace_list_end;\n\t\tfunc = ftrace_ops_list_func;\n\t}\n\n\tupdate_function_graph_func();\n\n\t \n\tif (ftrace_trace_function == func)\n\t\treturn;\n\n\t \n\tif (func == ftrace_ops_list_func) {\n\t\tftrace_trace_function = func;\n\t\t \n\t\treturn;\n\t}\n\n#ifndef CONFIG_DYNAMIC_FTRACE\n\t \n\tftrace_trace_function = ftrace_ops_list_func;\n\t \n\tsynchronize_rcu_tasks_rude();\n\t \n\tfunction_trace_op = set_function_trace_op;\n\t \n\tsmp_wmb();\n\t \n\tsmp_call_function(ftrace_sync_ipi, NULL, 1);\n\t \n#endif  \n\n\tftrace_trace_function = func;\n}\n\nstatic void add_ftrace_ops(struct ftrace_ops __rcu **list,\n\t\t\t   struct ftrace_ops *ops)\n{\n\trcu_assign_pointer(ops->next, *list);\n\n\t \n\trcu_assign_pointer(*list, ops);\n}\n\nstatic int remove_ftrace_ops(struct ftrace_ops __rcu **list,\n\t\t\t     struct ftrace_ops *ops)\n{\n\tstruct ftrace_ops **p;\n\n\t \n\tif (rcu_dereference_protected(*list,\n\t\t\tlockdep_is_held(&ftrace_lock)) == ops &&\n\t    rcu_dereference_protected(ops->next,\n\t\t\tlockdep_is_held(&ftrace_lock)) == &ftrace_list_end) {\n\t\t*list = &ftrace_list_end;\n\t\treturn 0;\n\t}\n\n\tfor (p = list; *p != &ftrace_list_end; p = &(*p)->next)\n\t\tif (*p == ops)\n\t\t\tbreak;\n\n\tif (*p != ops)\n\t\treturn -1;\n\n\t*p = (*p)->next;\n\treturn 0;\n}\n\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops);\n\nint __register_ftrace_function(struct ftrace_ops *ops)\n{\n\tif (ops->flags & FTRACE_OPS_FL_DELETED)\n\t\treturn -EINVAL;\n\n\tif (WARN_ON(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn -EBUSY;\n\n#ifndef CONFIG_DYNAMIC_FTRACE_WITH_REGS\n\t \n\tif (ops->flags & FTRACE_OPS_FL_SAVE_REGS &&\n\t    !(ops->flags & FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED))\n\t\treturn -EINVAL;\n\n\tif (ops->flags & FTRACE_OPS_FL_SAVE_REGS_IF_SUPPORTED)\n\t\tops->flags |= FTRACE_OPS_FL_SAVE_REGS;\n#endif\n\tif (!ftrace_enabled && (ops->flags & FTRACE_OPS_FL_PERMANENT))\n\t\treturn -EBUSY;\n\n\tif (!is_kernel_core_data((unsigned long)ops))\n\t\tops->flags |= FTRACE_OPS_FL_DYNAMIC;\n\n\tadd_ftrace_ops(&ftrace_ops_list, ops);\n\n\t \n\tops->saved_func = ops->func;\n\n\tif (ftrace_pids_enabled(ops))\n\t\tops->func = ftrace_pid_func;\n\n\tftrace_update_trampoline(ops);\n\n\tif (ftrace_enabled)\n\t\tupdate_ftrace_function();\n\n\treturn 0;\n}\n\nint __unregister_ftrace_function(struct ftrace_ops *ops)\n{\n\tint ret;\n\n\tif (WARN_ON(!(ops->flags & FTRACE_OPS_FL_ENABLED)))\n\t\treturn -EBUSY;\n\n\tret = remove_ftrace_ops(&ftrace_ops_list, ops);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ftrace_enabled)\n\t\tupdate_ftrace_function();\n\n\tops->func = ops->saved_func;\n\n\treturn 0;\n}\n\nstatic void ftrace_update_pid_func(void)\n{\n\tstruct ftrace_ops *op;\n\n\t \n\tif (ftrace_trace_function == ftrace_stub)\n\t\treturn;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op->flags & FTRACE_OPS_FL_PID) {\n\t\t\top->func = ftrace_pids_enabled(op) ?\n\t\t\t\tftrace_pid_func : op->saved_func;\n\t\t\tftrace_update_trampoline(op);\n\t\t}\n\t} while_for_each_ftrace_op(op);\n\n\tupdate_ftrace_function();\n}\n\n#ifdef CONFIG_FUNCTION_PROFILER\nstruct ftrace_profile {\n\tstruct hlist_node\t\tnode;\n\tunsigned long\t\t\tip;\n\tunsigned long\t\t\tcounter;\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tunsigned long long\t\ttime;\n\tunsigned long long\t\ttime_squared;\n#endif\n};\n\nstruct ftrace_profile_page {\n\tstruct ftrace_profile_page\t*next;\n\tunsigned long\t\t\tindex;\n\tstruct ftrace_profile\t\trecords[];\n};\n\nstruct ftrace_profile_stat {\n\tatomic_t\t\t\tdisabled;\n\tstruct hlist_head\t\t*hash;\n\tstruct ftrace_profile_page\t*pages;\n\tstruct ftrace_profile_page\t*start;\n\tstruct tracer_stat\t\tstat;\n};\n\n#define PROFILE_RECORDS_SIZE\t\t\t\t\t\t\\\n\t(PAGE_SIZE - offsetof(struct ftrace_profile_page, records))\n\n#define PROFILES_PER_PAGE\t\t\t\t\t\\\n\t(PROFILE_RECORDS_SIZE / sizeof(struct ftrace_profile))\n\nstatic int ftrace_profile_enabled __read_mostly;\n\n \nstatic DEFINE_MUTEX(ftrace_profile_lock);\n\nstatic DEFINE_PER_CPU(struct ftrace_profile_stat, ftrace_profile_stats);\n\n#define FTRACE_PROFILE_HASH_BITS 10\n#define FTRACE_PROFILE_HASH_SIZE (1 << FTRACE_PROFILE_HASH_BITS)\n\nstatic void *\nfunction_stat_next(void *v, int idx)\n{\n\tstruct ftrace_profile *rec = v;\n\tstruct ftrace_profile_page *pg;\n\n\tpg = (struct ftrace_profile_page *)((unsigned long)rec & PAGE_MASK);\n\n again:\n\tif (idx != 0)\n\t\trec++;\n\n\tif ((void *)rec >= (void *)&pg->records[pg->index]) {\n\t\tpg = pg->next;\n\t\tif (!pg)\n\t\t\treturn NULL;\n\t\trec = &pg->records[0];\n\t\tif (!rec->counter)\n\t\t\tgoto again;\n\t}\n\n\treturn rec;\n}\n\nstatic void *function_stat_start(struct tracer_stat *trace)\n{\n\tstruct ftrace_profile_stat *stat =\n\t\tcontainer_of(trace, struct ftrace_profile_stat, stat);\n\n\tif (!stat || !stat->start)\n\t\treturn NULL;\n\n\treturn function_stat_next(&stat->start->records[0], 0);\n}\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n \nstatic int function_stat_cmp(const void *p1, const void *p2)\n{\n\tconst struct ftrace_profile *a = p1;\n\tconst struct ftrace_profile *b = p2;\n\n\tif (a->time < b->time)\n\t\treturn -1;\n\tif (a->time > b->time)\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n#else\n \nstatic int function_stat_cmp(const void *p1, const void *p2)\n{\n\tconst struct ftrace_profile *a = p1;\n\tconst struct ftrace_profile *b = p2;\n\n\tif (a->counter < b->counter)\n\t\treturn -1;\n\tif (a->counter > b->counter)\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n#endif\n\nstatic int function_stat_headers(struct seq_file *m)\n{\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tseq_puts(m, \"  Function                               \"\n\t\t \"Hit    Time            Avg             s^2\\n\"\n\t\t    \"  --------                               \"\n\t\t \"---    ----            ---             ---\\n\");\n#else\n\tseq_puts(m, \"  Function                               Hit\\n\"\n\t\t    \"  --------                               ---\\n\");\n#endif\n\treturn 0;\n}\n\nstatic int function_stat_show(struct seq_file *m, void *v)\n{\n\tstruct ftrace_profile *rec = v;\n\tchar str[KSYM_SYMBOL_LEN];\n\tint ret = 0;\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tstatic struct trace_seq s;\n\tunsigned long long avg;\n\tunsigned long long stddev;\n#endif\n\tmutex_lock(&ftrace_profile_lock);\n\n\t \n\tif (unlikely(rec->counter == 0)) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tavg = div64_ul(rec->time, rec->counter);\n\tif (tracing_thresh && (avg < tracing_thresh))\n\t\tgoto out;\n#endif\n\n\tkallsyms_lookup(rec->ip, NULL, NULL, NULL, str);\n\tseq_printf(m, \"  %-30.30s  %10lu\", str, rec->counter);\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tseq_puts(m, \"    \");\n\n\t \n\tif (rec->counter <= 1)\n\t\tstddev = 0;\n\telse {\n\t\t \n\t\tstddev = rec->counter * rec->time_squared -\n\t\t\t rec->time * rec->time;\n\n\t\t \n\t\tstddev = div64_ul(stddev,\n\t\t\t\t  rec->counter * (rec->counter - 1) * 1000);\n\t}\n\n\ttrace_seq_init(&s);\n\ttrace_print_graph_duration(rec->time, &s);\n\ttrace_seq_puts(&s, \"    \");\n\ttrace_print_graph_duration(avg, &s);\n\ttrace_seq_puts(&s, \"    \");\n\ttrace_print_graph_duration(stddev, &s);\n\ttrace_print_seq(m, &s);\n#endif\n\tseq_putc(m, '\\n');\nout:\n\tmutex_unlock(&ftrace_profile_lock);\n\n\treturn ret;\n}\n\nstatic void ftrace_profile_reset(struct ftrace_profile_stat *stat)\n{\n\tstruct ftrace_profile_page *pg;\n\n\tpg = stat->pages = stat->start;\n\n\twhile (pg) {\n\t\tmemset(pg->records, 0, PROFILE_RECORDS_SIZE);\n\t\tpg->index = 0;\n\t\tpg = pg->next;\n\t}\n\n\tmemset(stat->hash, 0,\n\t       FTRACE_PROFILE_HASH_SIZE * sizeof(struct hlist_head));\n}\n\nstatic int ftrace_profile_pages_init(struct ftrace_profile_stat *stat)\n{\n\tstruct ftrace_profile_page *pg;\n\tint functions;\n\tint pages;\n\tint i;\n\n\t \n\tif (stat->pages)\n\t\treturn 0;\n\n\tstat->pages = (void *)get_zeroed_page(GFP_KERNEL);\n\tif (!stat->pages)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\tfunctions = ftrace_update_tot_cnt;\n#else\n\t \n\tfunctions = 20000;\n#endif\n\n\tpg = stat->start = stat->pages;\n\n\tpages = DIV_ROUND_UP(functions, PROFILES_PER_PAGE);\n\n\tfor (i = 1; i < pages; i++) {\n\t\tpg->next = (void *)get_zeroed_page(GFP_KERNEL);\n\t\tif (!pg->next)\n\t\t\tgoto out_free;\n\t\tpg = pg->next;\n\t}\n\n\treturn 0;\n\n out_free:\n\tpg = stat->start;\n\twhile (pg) {\n\t\tunsigned long tmp = (unsigned long)pg;\n\n\t\tpg = pg->next;\n\t\tfree_page(tmp);\n\t}\n\n\tstat->pages = NULL;\n\tstat->start = NULL;\n\n\treturn -ENOMEM;\n}\n\nstatic int ftrace_profile_init_cpu(int cpu)\n{\n\tstruct ftrace_profile_stat *stat;\n\tint size;\n\n\tstat = &per_cpu(ftrace_profile_stats, cpu);\n\n\tif (stat->hash) {\n\t\t \n\t\tftrace_profile_reset(stat);\n\t\treturn 0;\n\t}\n\n\t \n\tsize = FTRACE_PROFILE_HASH_SIZE;\n\n\tstat->hash = kcalloc(size, sizeof(struct hlist_head), GFP_KERNEL);\n\n\tif (!stat->hash)\n\t\treturn -ENOMEM;\n\n\t \n\tif (ftrace_profile_pages_init(stat) < 0) {\n\t\tkfree(stat->hash);\n\t\tstat->hash = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int ftrace_profile_init(void)\n{\n\tint cpu;\n\tint ret = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tret = ftrace_profile_init_cpu(cpu);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n \nstatic struct ftrace_profile *\nftrace_find_profiled_func(struct ftrace_profile_stat *stat, unsigned long ip)\n{\n\tstruct ftrace_profile *rec;\n\tstruct hlist_head *hhd;\n\tunsigned long key;\n\n\tkey = hash_long(ip, FTRACE_PROFILE_HASH_BITS);\n\thhd = &stat->hash[key];\n\n\tif (hlist_empty(hhd))\n\t\treturn NULL;\n\n\thlist_for_each_entry_rcu_notrace(rec, hhd, node) {\n\t\tif (rec->ip == ip)\n\t\t\treturn rec;\n\t}\n\n\treturn NULL;\n}\n\nstatic void ftrace_add_profile(struct ftrace_profile_stat *stat,\n\t\t\t       struct ftrace_profile *rec)\n{\n\tunsigned long key;\n\n\tkey = hash_long(rec->ip, FTRACE_PROFILE_HASH_BITS);\n\thlist_add_head_rcu(&rec->node, &stat->hash[key]);\n}\n\n \nstatic struct ftrace_profile *\nftrace_profile_alloc(struct ftrace_profile_stat *stat, unsigned long ip)\n{\n\tstruct ftrace_profile *rec = NULL;\n\n\t \n\tif (atomic_inc_return(&stat->disabled) != 1)\n\t\tgoto out;\n\n\t \n\trec = ftrace_find_profiled_func(stat, ip);\n\tif (rec)\n\t\tgoto out;\n\n\tif (stat->pages->index == PROFILES_PER_PAGE) {\n\t\tif (!stat->pages->next)\n\t\t\tgoto out;\n\t\tstat->pages = stat->pages->next;\n\t}\n\n\trec = &stat->pages->records[stat->pages->index++];\n\trec->ip = ip;\n\tftrace_add_profile(stat, rec);\n\n out:\n\tatomic_dec(&stat->disabled);\n\n\treturn rec;\n}\n\nstatic void\nfunction_profile_call(unsigned long ip, unsigned long parent_ip,\n\t\t      struct ftrace_ops *ops, struct ftrace_regs *fregs)\n{\n\tstruct ftrace_profile_stat *stat;\n\tstruct ftrace_profile *rec;\n\tunsigned long flags;\n\n\tif (!ftrace_profile_enabled)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tstat = this_cpu_ptr(&ftrace_profile_stats);\n\tif (!stat->hash || !ftrace_profile_enabled)\n\t\tgoto out;\n\n\trec = ftrace_find_profiled_func(stat, ip);\n\tif (!rec) {\n\t\trec = ftrace_profile_alloc(stat, ip);\n\t\tif (!rec)\n\t\t\tgoto out;\n\t}\n\n\trec->counter++;\n out:\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\nstatic bool fgraph_graph_time = true;\n\nvoid ftrace_graph_graph_time_control(bool enable)\n{\n\tfgraph_graph_time = enable;\n}\n\nstatic int profile_graph_entry(struct ftrace_graph_ent *trace)\n{\n\tstruct ftrace_ret_stack *ret_stack;\n\n\tfunction_profile_call(trace->func, 0, NULL, NULL);\n\n\t \n\tif (!current->ret_stack)\n\t\treturn 0;\n\n\tret_stack = ftrace_graph_get_ret_stack(current, 0);\n\tif (ret_stack)\n\t\tret_stack->subtime = 0;\n\n\treturn 1;\n}\n\nstatic void profile_graph_return(struct ftrace_graph_ret *trace)\n{\n\tstruct ftrace_ret_stack *ret_stack;\n\tstruct ftrace_profile_stat *stat;\n\tunsigned long long calltime;\n\tstruct ftrace_profile *rec;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tstat = this_cpu_ptr(&ftrace_profile_stats);\n\tif (!stat->hash || !ftrace_profile_enabled)\n\t\tgoto out;\n\n\t \n\tif (!trace->calltime)\n\t\tgoto out;\n\n\tcalltime = trace->rettime - trace->calltime;\n\n\tif (!fgraph_graph_time) {\n\n\t\t \n\t\tret_stack = ftrace_graph_get_ret_stack(current, 1);\n\t\tif (ret_stack)\n\t\t\tret_stack->subtime += calltime;\n\n\t\tret_stack = ftrace_graph_get_ret_stack(current, 0);\n\t\tif (ret_stack && ret_stack->subtime < calltime)\n\t\t\tcalltime -= ret_stack->subtime;\n\t\telse\n\t\t\tcalltime = 0;\n\t}\n\n\trec = ftrace_find_profiled_func(stat, trace->func);\n\tif (rec) {\n\t\trec->time += calltime;\n\t\trec->time_squared += calltime * calltime;\n\t}\n\n out:\n\tlocal_irq_restore(flags);\n}\n\nstatic struct fgraph_ops fprofiler_ops = {\n\t.entryfunc = &profile_graph_entry,\n\t.retfunc = &profile_graph_return,\n};\n\nstatic int register_ftrace_profiler(void)\n{\n\treturn register_ftrace_graph(&fprofiler_ops);\n}\n\nstatic void unregister_ftrace_profiler(void)\n{\n\tunregister_ftrace_graph(&fprofiler_ops);\n}\n#else\nstatic struct ftrace_ops ftrace_profile_ops __read_mostly = {\n\t.func\t\t= function_profile_call,\n\t.flags\t\t= FTRACE_OPS_FL_INITIALIZED,\n\tINIT_OPS_HASH(ftrace_profile_ops)\n};\n\nstatic int register_ftrace_profiler(void)\n{\n\treturn register_ftrace_function(&ftrace_profile_ops);\n}\n\nstatic void unregister_ftrace_profiler(void)\n{\n\tunregister_ftrace_function(&ftrace_profile_ops);\n}\n#endif  \n\nstatic ssize_t\nftrace_profile_write(struct file *filp, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tunsigned long val;\n\tint ret;\n\n\tret = kstrtoul_from_user(ubuf, cnt, 10, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tval = !!val;\n\n\tmutex_lock(&ftrace_profile_lock);\n\tif (ftrace_profile_enabled ^ val) {\n\t\tif (val) {\n\t\t\tret = ftrace_profile_init();\n\t\t\tif (ret < 0) {\n\t\t\t\tcnt = ret;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tret = register_ftrace_profiler();\n\t\t\tif (ret < 0) {\n\t\t\t\tcnt = ret;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tftrace_profile_enabled = 1;\n\t\t} else {\n\t\t\tftrace_profile_enabled = 0;\n\t\t\t \n\t\t\tunregister_ftrace_profiler();\n\t\t}\n\t}\n out:\n\tmutex_unlock(&ftrace_profile_lock);\n\n\t*ppos += cnt;\n\n\treturn cnt;\n}\n\nstatic ssize_t\nftrace_profile_read(struct file *filp, char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\tchar buf[64];\t\t \n\tint r;\n\n\tr = sprintf(buf, \"%u\\n\", ftrace_profile_enabled);\n\treturn simple_read_from_buffer(ubuf, cnt, ppos, buf, r);\n}\n\nstatic const struct file_operations ftrace_profile_fops = {\n\t.open\t\t= tracing_open_generic,\n\t.read\t\t= ftrace_profile_read,\n\t.write\t\t= ftrace_profile_write,\n\t.llseek\t\t= default_llseek,\n};\n\n \nstatic struct tracer_stat function_stats __initdata = {\n\t.name\t\t= \"functions\",\n\t.stat_start\t= function_stat_start,\n\t.stat_next\t= function_stat_next,\n\t.stat_cmp\t= function_stat_cmp,\n\t.stat_headers\t= function_stat_headers,\n\t.stat_show\t= function_stat_show\n};\n\nstatic __init void ftrace_profile_tracefs(struct dentry *d_tracer)\n{\n\tstruct ftrace_profile_stat *stat;\n\tchar *name;\n\tint ret;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstat = &per_cpu(ftrace_profile_stats, cpu);\n\n\t\tname = kasprintf(GFP_KERNEL, \"function%d\", cpu);\n\t\tif (!name) {\n\t\t\t \n\t\t\tWARN(1,\n\t\t\t     \"Could not allocate stat file for cpu %d\\n\",\n\t\t\t     cpu);\n\t\t\treturn;\n\t\t}\n\t\tstat->stat = function_stats;\n\t\tstat->stat.name = name;\n\t\tret = register_stat_tracer(&stat->stat);\n\t\tif (ret) {\n\t\t\tWARN(1,\n\t\t\t     \"Could not register function stat for cpu %d\\n\",\n\t\t\t     cpu);\n\t\t\tkfree(name);\n\t\t\treturn;\n\t\t}\n\t}\n\n\ttrace_create_file(\"function_profile_enabled\",\n\t\t\t  TRACE_MODE_WRITE, d_tracer, NULL,\n\t\t\t  &ftrace_profile_fops);\n}\n\n#else  \nstatic __init void ftrace_profile_tracefs(struct dentry *d_tracer)\n{\n}\n#endif  \n\n#ifdef CONFIG_DYNAMIC_FTRACE\n\nstatic struct ftrace_ops *removed_ops;\n\n \nstatic bool update_all_ops;\n\n#ifndef CONFIG_FTRACE_MCOUNT_RECORD\n# error Dynamic ftrace depends on MCOUNT_RECORD\n#endif\n\nstruct ftrace_func_probe {\n\tstruct ftrace_probe_ops\t*probe_ops;\n\tstruct ftrace_ops\tops;\n\tstruct trace_array\t*tr;\n\tstruct list_head\tlist;\n\tvoid\t\t\t*data;\n\tint\t\t\tref;\n};\n\n \nstatic const struct hlist_head empty_buckets[1];\nstatic const struct ftrace_hash empty_hash = {\n\t.buckets = (struct hlist_head *)empty_buckets,\n};\n#define EMPTY_HASH\t((struct ftrace_hash *)&empty_hash)\n\nstruct ftrace_ops global_ops = {\n\t.func\t\t\t\t= ftrace_stub,\n\t.local_hash.notrace_hash\t= EMPTY_HASH,\n\t.local_hash.filter_hash\t\t= EMPTY_HASH,\n\tINIT_OPS_HASH(global_ops)\n\t.flags\t\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t\t  FTRACE_OPS_FL_PID,\n};\n\n \nstruct ftrace_ops *ftrace_ops_trampoline(unsigned long addr)\n{\n\tstruct ftrace_ops *op = NULL;\n\n\t \n\tpreempt_disable_notrace();\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\t \n\t\tif (op->trampoline && op->trampoline_size)\n\t\t\tif (addr >= op->trampoline &&\n\t\t\t    addr < op->trampoline + op->trampoline_size) {\n\t\t\t\tpreempt_enable_notrace();\n\t\t\t\treturn op;\n\t\t\t}\n\t} while_for_each_ftrace_op(op);\n\tpreempt_enable_notrace();\n\n\treturn NULL;\n}\n\n \nbool is_ftrace_trampoline(unsigned long addr)\n{\n\treturn ftrace_ops_trampoline(addr) != NULL;\n}\n\nstruct ftrace_page {\n\tstruct ftrace_page\t*next;\n\tstruct dyn_ftrace\t*records;\n\tint\t\t\tindex;\n\tint\t\t\torder;\n};\n\n#define ENTRY_SIZE sizeof(struct dyn_ftrace)\n#define ENTRIES_PER_PAGE (PAGE_SIZE / ENTRY_SIZE)\n\nstatic struct ftrace_page\t*ftrace_pages_start;\nstatic struct ftrace_page\t*ftrace_pages;\n\nstatic __always_inline unsigned long\nftrace_hash_key(struct ftrace_hash *hash, unsigned long ip)\n{\n\tif (hash->size_bits > 0)\n\t\treturn hash_long(ip, hash->size_bits);\n\n\treturn 0;\n}\n\n \nstatic __always_inline struct ftrace_func_entry *\n__ftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip)\n{\n\tunsigned long key;\n\tstruct ftrace_func_entry *entry;\n\tstruct hlist_head *hhd;\n\n\tkey = ftrace_hash_key(hash, ip);\n\thhd = &hash->buckets[key];\n\n\thlist_for_each_entry_rcu_notrace(entry, hhd, hlist) {\n\t\tif (entry->ip == ip)\n\t\t\treturn entry;\n\t}\n\treturn NULL;\n}\n\n \nstruct ftrace_func_entry *\nftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip)\n{\n\tif (ftrace_hash_empty(hash))\n\t\treturn NULL;\n\n\treturn __ftrace_lookup_ip(hash, ip);\n}\n\nstatic void __add_hash_entry(struct ftrace_hash *hash,\n\t\t\t     struct ftrace_func_entry *entry)\n{\n\tstruct hlist_head *hhd;\n\tunsigned long key;\n\n\tkey = ftrace_hash_key(hash, entry->ip);\n\thhd = &hash->buckets[key];\n\thlist_add_head(&entry->hlist, hhd);\n\thash->count++;\n}\n\nstatic struct ftrace_func_entry *\nadd_hash_entry(struct ftrace_hash *hash, unsigned long ip)\n{\n\tstruct ftrace_func_entry *entry;\n\n\tentry = kmalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn NULL;\n\n\tentry->ip = ip;\n\t__add_hash_entry(hash, entry);\n\n\treturn entry;\n}\n\nstatic void\nfree_hash_entry(struct ftrace_hash *hash,\n\t\t  struct ftrace_func_entry *entry)\n{\n\thlist_del(&entry->hlist);\n\tkfree(entry);\n\thash->count--;\n}\n\nstatic void\nremove_hash_entry(struct ftrace_hash *hash,\n\t\t  struct ftrace_func_entry *entry)\n{\n\thlist_del_rcu(&entry->hlist);\n\thash->count--;\n}\n\nstatic void ftrace_hash_clear(struct ftrace_hash *hash)\n{\n\tstruct hlist_head *hhd;\n\tstruct hlist_node *tn;\n\tstruct ftrace_func_entry *entry;\n\tint size = 1 << hash->size_bits;\n\tint i;\n\n\tif (!hash->count)\n\t\treturn;\n\n\tfor (i = 0; i < size; i++) {\n\t\thhd = &hash->buckets[i];\n\t\thlist_for_each_entry_safe(entry, tn, hhd, hlist)\n\t\t\tfree_hash_entry(hash, entry);\n\t}\n\tFTRACE_WARN_ON(hash->count);\n}\n\nstatic void free_ftrace_mod(struct ftrace_mod_load *ftrace_mod)\n{\n\tlist_del(&ftrace_mod->list);\n\tkfree(ftrace_mod->module);\n\tkfree(ftrace_mod->func);\n\tkfree(ftrace_mod);\n}\n\nstatic void clear_ftrace_mod_list(struct list_head *head)\n{\n\tstruct ftrace_mod_load *p, *n;\n\n\t \n\tif (!head)\n\t\treturn;\n\n\tmutex_lock(&ftrace_lock);\n\tlist_for_each_entry_safe(p, n, head, list)\n\t\tfree_ftrace_mod(p);\n\tmutex_unlock(&ftrace_lock);\n}\n\nstatic void free_ftrace_hash(struct ftrace_hash *hash)\n{\n\tif (!hash || hash == EMPTY_HASH)\n\t\treturn;\n\tftrace_hash_clear(hash);\n\tkfree(hash->buckets);\n\tkfree(hash);\n}\n\nstatic void __free_ftrace_hash_rcu(struct rcu_head *rcu)\n{\n\tstruct ftrace_hash *hash;\n\n\thash = container_of(rcu, struct ftrace_hash, rcu);\n\tfree_ftrace_hash(hash);\n}\n\nstatic void free_ftrace_hash_rcu(struct ftrace_hash *hash)\n{\n\tif (!hash || hash == EMPTY_HASH)\n\t\treturn;\n\tcall_rcu(&hash->rcu, __free_ftrace_hash_rcu);\n}\n\n \nvoid ftrace_free_filter(struct ftrace_ops *ops)\n{\n\tftrace_ops_init(ops);\n\tfree_ftrace_hash(ops->func_hash->filter_hash);\n\tfree_ftrace_hash(ops->func_hash->notrace_hash);\n}\nEXPORT_SYMBOL_GPL(ftrace_free_filter);\n\nstatic struct ftrace_hash *alloc_ftrace_hash(int size_bits)\n{\n\tstruct ftrace_hash *hash;\n\tint size;\n\n\thash = kzalloc(sizeof(*hash), GFP_KERNEL);\n\tif (!hash)\n\t\treturn NULL;\n\n\tsize = 1 << size_bits;\n\thash->buckets = kcalloc(size, sizeof(*hash->buckets), GFP_KERNEL);\n\n\tif (!hash->buckets) {\n\t\tkfree(hash);\n\t\treturn NULL;\n\t}\n\n\thash->size_bits = size_bits;\n\n\treturn hash;\n}\n\n\nstatic int ftrace_add_mod(struct trace_array *tr,\n\t\t\t  const char *func, const char *module,\n\t\t\t  int enable)\n{\n\tstruct ftrace_mod_load *ftrace_mod;\n\tstruct list_head *mod_head = enable ? &tr->mod_trace : &tr->mod_notrace;\n\n\tftrace_mod = kzalloc(sizeof(*ftrace_mod), GFP_KERNEL);\n\tif (!ftrace_mod)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ftrace_mod->list);\n\tftrace_mod->func = kstrdup(func, GFP_KERNEL);\n\tftrace_mod->module = kstrdup(module, GFP_KERNEL);\n\tftrace_mod->enable = enable;\n\n\tif (!ftrace_mod->func || !ftrace_mod->module)\n\t\tgoto out_free;\n\n\tlist_add(&ftrace_mod->list, mod_head);\n\n\treturn 0;\n\n out_free:\n\tfree_ftrace_mod(ftrace_mod);\n\n\treturn -ENOMEM;\n}\n\nstatic struct ftrace_hash *\nalloc_and_copy_ftrace_hash(int size_bits, struct ftrace_hash *hash)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_hash *new_hash;\n\tint size;\n\tint i;\n\n\tnew_hash = alloc_ftrace_hash(size_bits);\n\tif (!new_hash)\n\t\treturn NULL;\n\n\tif (hash)\n\t\tnew_hash->flags = hash->flags;\n\n\t \n\tif (ftrace_hash_empty(hash))\n\t\treturn new_hash;\n\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tif (add_hash_entry(new_hash, entry->ip) == NULL)\n\t\t\t\tgoto free_hash;\n\t\t}\n\t}\n\n\tFTRACE_WARN_ON(new_hash->count != hash->count);\n\n\treturn new_hash;\n\n free_hash:\n\tfree_ftrace_hash(new_hash);\n\treturn NULL;\n}\n\nstatic void\nftrace_hash_rec_disable_modify(struct ftrace_ops *ops, int filter_hash);\nstatic void\nftrace_hash_rec_enable_modify(struct ftrace_ops *ops, int filter_hash);\n\nstatic int ftrace_hash_ipmodify_update(struct ftrace_ops *ops,\n\t\t\t\t       struct ftrace_hash *new_hash);\n\nstatic struct ftrace_hash *dup_hash(struct ftrace_hash *src, int size)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_hash *new_hash;\n\tstruct hlist_head *hhd;\n\tstruct hlist_node *tn;\n\tint bits = 0;\n\tint i;\n\n\t \n\tbits = fls(size / 2);\n\n\t \n\tif (bits > FTRACE_HASH_MAX_BITS)\n\t\tbits = FTRACE_HASH_MAX_BITS;\n\n\tnew_hash = alloc_ftrace_hash(bits);\n\tif (!new_hash)\n\t\treturn NULL;\n\n\tnew_hash->flags = src->flags;\n\n\tsize = 1 << src->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thhd = &src->buckets[i];\n\t\thlist_for_each_entry_safe(entry, tn, hhd, hlist) {\n\t\t\tremove_hash_entry(src, entry);\n\t\t\t__add_hash_entry(new_hash, entry);\n\t\t}\n\t}\n\treturn new_hash;\n}\n\nstatic struct ftrace_hash *\n__ftrace_hash_move(struct ftrace_hash *src)\n{\n\tint size = src->count;\n\n\t \n\tif (ftrace_hash_empty(src))\n\t\treturn EMPTY_HASH;\n\n\treturn dup_hash(src, size);\n}\n\nstatic int\nftrace_hash_move(struct ftrace_ops *ops, int enable,\n\t\t struct ftrace_hash **dst, struct ftrace_hash *src)\n{\n\tstruct ftrace_hash *new_hash;\n\tint ret;\n\n\t \n\tif (ops->flags & FTRACE_OPS_FL_IPMODIFY && !enable)\n\t\treturn -EINVAL;\n\n\tnew_hash = __ftrace_hash_move(src);\n\tif (!new_hash)\n\t\treturn -ENOMEM;\n\n\t \n\tif (enable) {\n\t\t \n\t\tret = ftrace_hash_ipmodify_update(ops, new_hash);\n\t\tif (ret < 0) {\n\t\t\tfree_ftrace_hash(new_hash);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tftrace_hash_rec_disable_modify(ops, enable);\n\n\trcu_assign_pointer(*dst, new_hash);\n\n\tftrace_hash_rec_enable_modify(ops, enable);\n\n\treturn 0;\n}\n\nstatic bool hash_contains_ip(unsigned long ip,\n\t\t\t     struct ftrace_ops_hash *hash)\n{\n\t \n\treturn (ftrace_hash_empty(hash->filter_hash) ||\n\t\t__ftrace_lookup_ip(hash->filter_hash, ip)) &&\n\t\t(ftrace_hash_empty(hash->notrace_hash) ||\n\t\t !__ftrace_lookup_ip(hash->notrace_hash, ip));\n}\n\n \nint\nftrace_ops_test(struct ftrace_ops *ops, unsigned long ip, void *regs)\n{\n\tstruct ftrace_ops_hash hash;\n\tint ret;\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_REGS\n\t \n\tif (regs == NULL && (ops->flags & FTRACE_OPS_FL_SAVE_REGS))\n\t\treturn 0;\n#endif\n\n\trcu_assign_pointer(hash.filter_hash, ops->func_hash->filter_hash);\n\trcu_assign_pointer(hash.notrace_hash, ops->func_hash->notrace_hash);\n\n\tif (hash_contains_ip(ip, &hash))\n\t\tret = 1;\n\telse\n\t\tret = 0;\n\n\treturn ret;\n}\n\n \n#define do_for_each_ftrace_rec(pg, rec)\t\t\t\t\t\\\n\tfor (pg = ftrace_pages_start; pg; pg = pg->next) {\t\t\\\n\t\tint _____i;\t\t\t\t\t\t\\\n\t\tfor (_____i = 0; _____i < pg->index; _____i++) {\t\\\n\t\t\trec = &pg->records[_____i];\n\n#define while_for_each_ftrace_rec()\t\t\\\n\t\t}\t\t\t\t\\\n\t}\n\n\nstatic int ftrace_cmp_recs(const void *a, const void *b)\n{\n\tconst struct dyn_ftrace *key = a;\n\tconst struct dyn_ftrace *rec = b;\n\n\tif (key->flags < rec->ip)\n\t\treturn -1;\n\tif (key->ip >= rec->ip + MCOUNT_INSN_SIZE)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic struct dyn_ftrace *lookup_rec(unsigned long start, unsigned long end)\n{\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec = NULL;\n\tstruct dyn_ftrace key;\n\n\tkey.ip = start;\n\tkey.flags = end;\t \n\n\tfor (pg = ftrace_pages_start; pg; pg = pg->next) {\n\t\tif (pg->index == 0 ||\n\t\t    end < pg->records[0].ip ||\n\t\t    start >= (pg->records[pg->index - 1].ip + MCOUNT_INSN_SIZE))\n\t\t\tcontinue;\n\t\trec = bsearch(&key, pg->records, pg->index,\n\t\t\t      sizeof(struct dyn_ftrace),\n\t\t\t      ftrace_cmp_recs);\n\t\tif (rec)\n\t\t\tbreak;\n\t}\n\treturn rec;\n}\n\n \nunsigned long ftrace_location_range(unsigned long start, unsigned long end)\n{\n\tstruct dyn_ftrace *rec;\n\n\trec = lookup_rec(start, end);\n\tif (rec)\n\t\treturn rec->ip;\n\n\treturn 0;\n}\n\n \nunsigned long ftrace_location(unsigned long ip)\n{\n\tstruct dyn_ftrace *rec;\n\tunsigned long offset;\n\tunsigned long size;\n\n\trec = lookup_rec(ip, ip);\n\tif (!rec) {\n\t\tif (!kallsyms_lookup_size_offset(ip, &size, &offset))\n\t\t\tgoto out;\n\n\t\t \n\t\tif (!offset)\n\t\t\trec = lookup_rec(ip, ip + size - 1);\n\t}\n\n\tif (rec)\n\t\treturn rec->ip;\n\nout:\n\treturn 0;\n}\n\n \nint ftrace_text_reserved(const void *start, const void *end)\n{\n\tunsigned long ret;\n\n\tret = ftrace_location_range((unsigned long)start,\n\t\t\t\t    (unsigned long)end);\n\n\treturn (int)!!ret;\n}\n\n \nstatic bool test_rec_ops_needs_regs(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *ops;\n\tbool keep_regs = false;\n\n\tfor (ops = ftrace_ops_list;\n\t     ops != &ftrace_list_end; ops = ops->next) {\n\t\t \n\t\tif (ftrace_ops_test(ops, rec->ip, rec)) {\n\t\t\tif (ops->flags & FTRACE_OPS_FL_SAVE_REGS) {\n\t\t\t\tkeep_regs = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn  keep_regs;\n}\n\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_any(struct dyn_ftrace *rec);\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_any_other(struct dyn_ftrace *rec, struct ftrace_ops *op_exclude);\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_next(struct dyn_ftrace *rec, struct ftrace_ops *ops);\n\nstatic bool skip_record(struct dyn_ftrace *rec)\n{\n\t \n\treturn rec->flags & FTRACE_FL_DISABLED &&\n\t\t!(rec->flags & FTRACE_FL_ENABLED);\n}\n\nstatic bool __ftrace_hash_rec_update(struct ftrace_ops *ops,\n\t\t\t\t     int filter_hash,\n\t\t\t\t     bool inc)\n{\n\tstruct ftrace_hash *hash;\n\tstruct ftrace_hash *other_hash;\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec;\n\tbool update = false;\n\tint count = 0;\n\tint all = false;\n\n\t \n\tif (!(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn false;\n\n\t \n\tif (filter_hash) {\n\t\thash = ops->func_hash->filter_hash;\n\t\tother_hash = ops->func_hash->notrace_hash;\n\t\tif (ftrace_hash_empty(hash))\n\t\t\tall = true;\n\t} else {\n\t\tinc = !inc;\n\t\thash = ops->func_hash->notrace_hash;\n\t\tother_hash = ops->func_hash->filter_hash;\n\t\t \n\t\tif (ftrace_hash_empty(hash))\n\t\t\treturn false;\n\t}\n\n\tdo_for_each_ftrace_rec(pg, rec) {\n\t\tint in_other_hash = 0;\n\t\tint in_hash = 0;\n\t\tint match = 0;\n\n\t\tif (skip_record(rec))\n\t\t\tcontinue;\n\n\t\tif (all) {\n\t\t\t \n\t\t\tif (!other_hash || !ftrace_lookup_ip(other_hash, rec->ip))\n\t\t\t\tmatch = 1;\n\t\t} else {\n\t\t\tin_hash = !!ftrace_lookup_ip(hash, rec->ip);\n\t\t\tin_other_hash = !!ftrace_lookup_ip(other_hash, rec->ip);\n\n\t\t\t \n\t\t\tif (filter_hash && in_hash && !in_other_hash)\n\t\t\t\tmatch = 1;\n\t\t\telse if (!filter_hash && in_hash &&\n\t\t\t\t (in_other_hash || ftrace_hash_empty(other_hash)))\n\t\t\t\tmatch = 1;\n\t\t}\n\t\tif (!match)\n\t\t\tcontinue;\n\n\t\tif (inc) {\n\t\t\trec->flags++;\n\t\t\tif (FTRACE_WARN_ON(ftrace_rec_count(rec) == FTRACE_REF_MAX))\n\t\t\t\treturn false;\n\n\t\t\tif (ops->flags & FTRACE_OPS_FL_DIRECT)\n\t\t\t\trec->flags |= FTRACE_FL_DIRECT;\n\n\t\t\t \n\t\t\tif (ftrace_rec_count(rec) == 1 && ops->trampoline)\n\t\t\t\trec->flags |= FTRACE_FL_TRAMP;\n\t\t\telse\n\t\t\t\t \n\t\t\t\trec->flags &= ~FTRACE_FL_TRAMP;\n\n\t\t\t \n\t\t\tif (ops->flags & FTRACE_OPS_FL_SAVE_REGS)\n\t\t\t\trec->flags |= FTRACE_FL_REGS;\n\t\t} else {\n\t\t\tif (FTRACE_WARN_ON(ftrace_rec_count(rec) == 0))\n\t\t\t\treturn false;\n\t\t\trec->flags--;\n\n\t\t\t \n\t\t\tif (ops->flags & FTRACE_OPS_FL_DIRECT)\n\t\t\t\trec->flags &= ~FTRACE_FL_DIRECT;\n\n\t\t\t \n\t\t\tif (ftrace_rec_count(rec) > 0 &&\n\t\t\t    rec->flags & FTRACE_FL_REGS &&\n\t\t\t    ops->flags & FTRACE_OPS_FL_SAVE_REGS) {\n\t\t\t\tif (!test_rec_ops_needs_regs(rec))\n\t\t\t\t\trec->flags &= ~FTRACE_FL_REGS;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (ftrace_rec_count(rec) == 1 &&\n\t\t\t    ftrace_find_tramp_ops_any_other(rec, ops))\n\t\t\t\trec->flags |= FTRACE_FL_TRAMP;\n\t\t\telse\n\t\t\t\trec->flags &= ~FTRACE_FL_TRAMP;\n\n\t\t\t \n\t\t}\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_DYNAMIC_FTRACE_WITH_CALL_OPS) &&\n\t\t    ftrace_rec_count(rec) == 1 &&\n\t\t    ftrace_ops_get_func(ops) == ops->func)\n\t\t\trec->flags |= FTRACE_FL_CALL_OPS;\n\t\telse\n\t\t\trec->flags &= ~FTRACE_FL_CALL_OPS;\n\n\t\tcount++;\n\n\t\t \n\t\tupdate |= ftrace_test_record(rec, true) != FTRACE_UPDATE_IGNORE;\n\n\t\t \n\t\tif (!all && count == hash->count)\n\t\t\treturn update;\n\t} while_for_each_ftrace_rec();\n\n\treturn update;\n}\n\nstatic bool ftrace_hash_rec_disable(struct ftrace_ops *ops,\n\t\t\t\t    int filter_hash)\n{\n\treturn __ftrace_hash_rec_update(ops, filter_hash, 0);\n}\n\nstatic bool ftrace_hash_rec_enable(struct ftrace_ops *ops,\n\t\t\t\t   int filter_hash)\n{\n\treturn __ftrace_hash_rec_update(ops, filter_hash, 1);\n}\n\nstatic void ftrace_hash_rec_update_modify(struct ftrace_ops *ops,\n\t\t\t\t\t  int filter_hash, int inc)\n{\n\tstruct ftrace_ops *op;\n\n\t__ftrace_hash_rec_update(ops, filter_hash, inc);\n\n\tif (ops->func_hash != &global_ops.local_hash)\n\t\treturn;\n\n\t \n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\t \n\t\tif (op == ops)\n\t\t\tcontinue;\n\t\tif (op->func_hash == &global_ops.local_hash)\n\t\t\t__ftrace_hash_rec_update(op, filter_hash, inc);\n\t} while_for_each_ftrace_op(op);\n}\n\nstatic void ftrace_hash_rec_disable_modify(struct ftrace_ops *ops,\n\t\t\t\t\t   int filter_hash)\n{\n\tftrace_hash_rec_update_modify(ops, filter_hash, 0);\n}\n\nstatic void ftrace_hash_rec_enable_modify(struct ftrace_ops *ops,\n\t\t\t\t\t  int filter_hash)\n{\n\tftrace_hash_rec_update_modify(ops, filter_hash, 1);\n}\n\n \nstatic int __ftrace_hash_update_ipmodify(struct ftrace_ops *ops,\n\t\t\t\t\t struct ftrace_hash *old_hash,\n\t\t\t\t\t struct ftrace_hash *new_hash)\n{\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec, *end = NULL;\n\tint in_old, in_new;\n\tbool is_ipmodify, is_direct;\n\n\t \n\tif (!(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn 0;\n\n\tis_ipmodify = ops->flags & FTRACE_OPS_FL_IPMODIFY;\n\tis_direct = ops->flags & FTRACE_OPS_FL_DIRECT;\n\n\t \n\tif (!is_ipmodify && !is_direct)\n\t\treturn 0;\n\n\tif (WARN_ON_ONCE(is_ipmodify && is_direct))\n\t\treturn 0;\n\n\t \n\tif (!new_hash || !old_hash)\n\t\treturn -EINVAL;\n\n\t \n\tdo_for_each_ftrace_rec(pg, rec) {\n\n\t\tif (rec->flags & FTRACE_FL_DISABLED)\n\t\t\tcontinue;\n\n\t\t \n\t\tin_old = !!ftrace_lookup_ip(old_hash, rec->ip);\n\t\tin_new = !!ftrace_lookup_ip(new_hash, rec->ip);\n\t\tif (in_old == in_new)\n\t\t\tcontinue;\n\n\t\tif (in_new) {\n\t\t\tif (rec->flags & FTRACE_FL_IPMODIFY) {\n\t\t\t\tint ret;\n\n\t\t\t\t \n\t\t\t\tif (is_ipmodify)\n\t\t\t\t\tgoto rollback;\n\n\t\t\t\tFTRACE_WARN_ON(rec->flags & FTRACE_FL_DIRECT);\n\n\t\t\t\t \n\t\t\t\tif (!ops->ops_func)\n\t\t\t\t\treturn -EBUSY;\n\t\t\t\tret = ops->ops_func(ops, FTRACE_OPS_CMD_ENABLE_SHARE_IPMODIFY_SELF);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t} else if (is_ipmodify) {\n\t\t\t\trec->flags |= FTRACE_FL_IPMODIFY;\n\t\t\t}\n\t\t} else if (is_ipmodify) {\n\t\t\trec->flags &= ~FTRACE_FL_IPMODIFY;\n\t\t}\n\t} while_for_each_ftrace_rec();\n\n\treturn 0;\n\nrollback:\n\tend = rec;\n\n\t \n\tdo_for_each_ftrace_rec(pg, rec) {\n\n\t\tif (rec->flags & FTRACE_FL_DISABLED)\n\t\t\tcontinue;\n\n\t\tif (rec == end)\n\t\t\tgoto err_out;\n\n\t\tin_old = !!ftrace_lookup_ip(old_hash, rec->ip);\n\t\tin_new = !!ftrace_lookup_ip(new_hash, rec->ip);\n\t\tif (in_old == in_new)\n\t\t\tcontinue;\n\n\t\tif (in_new)\n\t\t\trec->flags &= ~FTRACE_FL_IPMODIFY;\n\t\telse\n\t\t\trec->flags |= FTRACE_FL_IPMODIFY;\n\t} while_for_each_ftrace_rec();\n\nerr_out:\n\treturn -EBUSY;\n}\n\nstatic int ftrace_hash_ipmodify_enable(struct ftrace_ops *ops)\n{\n\tstruct ftrace_hash *hash = ops->func_hash->filter_hash;\n\n\tif (ftrace_hash_empty(hash))\n\t\thash = NULL;\n\n\treturn __ftrace_hash_update_ipmodify(ops, EMPTY_HASH, hash);\n}\n\n \nstatic void ftrace_hash_ipmodify_disable(struct ftrace_ops *ops)\n{\n\tstruct ftrace_hash *hash = ops->func_hash->filter_hash;\n\n\tif (ftrace_hash_empty(hash))\n\t\thash = NULL;\n\n\t__ftrace_hash_update_ipmodify(ops, hash, EMPTY_HASH);\n}\n\nstatic int ftrace_hash_ipmodify_update(struct ftrace_ops *ops,\n\t\t\t\t       struct ftrace_hash *new_hash)\n{\n\tstruct ftrace_hash *old_hash = ops->func_hash->filter_hash;\n\n\tif (ftrace_hash_empty(old_hash))\n\t\told_hash = NULL;\n\n\tif (ftrace_hash_empty(new_hash))\n\t\tnew_hash = NULL;\n\n\treturn __ftrace_hash_update_ipmodify(ops, old_hash, new_hash);\n}\n\nstatic void print_ip_ins(const char *fmt, const unsigned char *p)\n{\n\tchar ins[MCOUNT_INSN_SIZE];\n\n\tif (copy_from_kernel_nofault(ins, p, MCOUNT_INSN_SIZE)) {\n\t\tprintk(KERN_CONT \"%s[FAULT] %px\\n\", fmt, p);\n\t\treturn;\n\t}\n\n\tprintk(KERN_CONT \"%s\", fmt);\n\tpr_cont(\"%*phC\", MCOUNT_INSN_SIZE, ins);\n}\n\nenum ftrace_bug_type ftrace_bug_type;\nconst void *ftrace_expected;\n\nstatic void print_bug_type(void)\n{\n\tswitch (ftrace_bug_type) {\n\tcase FTRACE_BUG_UNKNOWN:\n\t\tbreak;\n\tcase FTRACE_BUG_INIT:\n\t\tpr_info(\"Initializing ftrace call sites\\n\");\n\t\tbreak;\n\tcase FTRACE_BUG_NOP:\n\t\tpr_info(\"Setting ftrace call site to NOP\\n\");\n\t\tbreak;\n\tcase FTRACE_BUG_CALL:\n\t\tpr_info(\"Setting ftrace call site to call ftrace function\\n\");\n\t\tbreak;\n\tcase FTRACE_BUG_UPDATE:\n\t\tpr_info(\"Updating ftrace call site to call a different ftrace function\\n\");\n\t\tbreak;\n\t}\n}\n\n \nvoid ftrace_bug(int failed, struct dyn_ftrace *rec)\n{\n\tunsigned long ip = rec ? rec->ip : 0;\n\n\tpr_info(\"------------[ ftrace bug ]------------\\n\");\n\n\tswitch (failed) {\n\tcase -EFAULT:\n\t\tpr_info(\"ftrace faulted on modifying \");\n\t\tprint_ip_sym(KERN_INFO, ip);\n\t\tbreak;\n\tcase -EINVAL:\n\t\tpr_info(\"ftrace failed to modify \");\n\t\tprint_ip_sym(KERN_INFO, ip);\n\t\tprint_ip_ins(\" actual:   \", (unsigned char *)ip);\n\t\tpr_cont(\"\\n\");\n\t\tif (ftrace_expected) {\n\t\t\tprint_ip_ins(\" expected: \", ftrace_expected);\n\t\t\tpr_cont(\"\\n\");\n\t\t}\n\t\tbreak;\n\tcase -EPERM:\n\t\tpr_info(\"ftrace faulted on writing \");\n\t\tprint_ip_sym(KERN_INFO, ip);\n\t\tbreak;\n\tdefault:\n\t\tpr_info(\"ftrace faulted on unknown error \");\n\t\tprint_ip_sym(KERN_INFO, ip);\n\t}\n\tprint_bug_type();\n\tif (rec) {\n\t\tstruct ftrace_ops *ops = NULL;\n\n\t\tpr_info(\"ftrace record flags: %lx\\n\", rec->flags);\n\t\tpr_cont(\" (%ld)%s%s\", ftrace_rec_count(rec),\n\t\t\trec->flags & FTRACE_FL_REGS ? \" R\" : \"  \",\n\t\t\trec->flags & FTRACE_FL_CALL_OPS ? \" O\" : \"  \");\n\t\tif (rec->flags & FTRACE_FL_TRAMP_EN) {\n\t\t\tops = ftrace_find_tramp_ops_any(rec);\n\t\t\tif (ops) {\n\t\t\t\tdo {\n\t\t\t\t\tpr_cont(\"\\ttramp: %pS (%pS)\",\n\t\t\t\t\t\t(void *)ops->trampoline,\n\t\t\t\t\t\t(void *)ops->func);\n\t\t\t\t\tops = ftrace_find_tramp_ops_next(rec, ops);\n\t\t\t\t} while (ops);\n\t\t\t} else\n\t\t\t\tpr_cont(\"\\ttramp: ERROR!\");\n\n\t\t}\n\t\tip = ftrace_get_addr_curr(rec);\n\t\tpr_cont(\"\\n expected tramp: %lx\\n\", ip);\n\t}\n\n\tFTRACE_WARN_ON_ONCE(1);\n}\n\nstatic int ftrace_check_record(struct dyn_ftrace *rec, bool enable, bool update)\n{\n\tunsigned long flag = 0UL;\n\n\tftrace_bug_type = FTRACE_BUG_UNKNOWN;\n\n\tif (skip_record(rec))\n\t\treturn FTRACE_UPDATE_IGNORE;\n\n\t \n\tif (enable && ftrace_rec_count(rec))\n\t\tflag = FTRACE_FL_ENABLED;\n\n\t \n\tif (flag) {\n\t\tif (!(rec->flags & FTRACE_FL_REGS) !=\n\t\t    !(rec->flags & FTRACE_FL_REGS_EN))\n\t\t\tflag |= FTRACE_FL_REGS;\n\n\t\tif (!(rec->flags & FTRACE_FL_TRAMP) !=\n\t\t    !(rec->flags & FTRACE_FL_TRAMP_EN))\n\t\t\tflag |= FTRACE_FL_TRAMP;\n\n\t\t \n\t\tif (ftrace_rec_count(rec) == 1) {\n\t\t\tif (!(rec->flags & FTRACE_FL_DIRECT) !=\n\t\t\t    !(rec->flags & FTRACE_FL_DIRECT_EN))\n\t\t\t\tflag |= FTRACE_FL_DIRECT;\n\t\t} else if (rec->flags & FTRACE_FL_DIRECT_EN) {\n\t\t\tflag |= FTRACE_FL_DIRECT;\n\t\t}\n\n\t\t \n\t\tif (ftrace_rec_count(rec) == 1) {\n\t\t\tif (!(rec->flags & FTRACE_FL_CALL_OPS) !=\n\t\t\t    !(rec->flags & FTRACE_FL_CALL_OPS_EN))\n\t\t\t\tflag |= FTRACE_FL_CALL_OPS;\n\t\t} else if (rec->flags & FTRACE_FL_CALL_OPS_EN) {\n\t\t\tflag |= FTRACE_FL_CALL_OPS;\n\t\t}\n\t}\n\n\t \n\tif ((rec->flags & FTRACE_FL_ENABLED) == flag)\n\t\treturn FTRACE_UPDATE_IGNORE;\n\n\tif (flag) {\n\t\t \n\t\tflag ^= rec->flags & FTRACE_FL_ENABLED;\n\n\t\tif (update) {\n\t\t\trec->flags |= FTRACE_FL_ENABLED | FTRACE_FL_TOUCHED;\n\t\t\tif (flag & FTRACE_FL_REGS) {\n\t\t\t\tif (rec->flags & FTRACE_FL_REGS)\n\t\t\t\t\trec->flags |= FTRACE_FL_REGS_EN;\n\t\t\t\telse\n\t\t\t\t\trec->flags &= ~FTRACE_FL_REGS_EN;\n\t\t\t}\n\t\t\tif (flag & FTRACE_FL_TRAMP) {\n\t\t\t\tif (rec->flags & FTRACE_FL_TRAMP)\n\t\t\t\t\trec->flags |= FTRACE_FL_TRAMP_EN;\n\t\t\t\telse\n\t\t\t\t\trec->flags &= ~FTRACE_FL_TRAMP_EN;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (rec->flags & (FTRACE_FL_DIRECT | FTRACE_FL_IPMODIFY))\n\t\t\t\trec->flags |= FTRACE_FL_MODIFIED;\n\n\t\t\tif (flag & FTRACE_FL_DIRECT) {\n\t\t\t\t \n\t\t\t\tif (ftrace_rec_count(rec) == 1) {\n\t\t\t\t\tif (rec->flags & FTRACE_FL_DIRECT)\n\t\t\t\t\t\trec->flags |= FTRACE_FL_DIRECT_EN;\n\t\t\t\t\telse\n\t\t\t\t\t\trec->flags &= ~FTRACE_FL_DIRECT_EN;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\trec->flags &= ~FTRACE_FL_DIRECT_EN;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (flag & FTRACE_FL_CALL_OPS) {\n\t\t\t\tif (ftrace_rec_count(rec) == 1) {\n\t\t\t\t\tif (rec->flags & FTRACE_FL_CALL_OPS)\n\t\t\t\t\t\trec->flags |= FTRACE_FL_CALL_OPS_EN;\n\t\t\t\t\telse\n\t\t\t\t\t\trec->flags &= ~FTRACE_FL_CALL_OPS_EN;\n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\trec->flags &= ~FTRACE_FL_CALL_OPS_EN;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (flag & FTRACE_FL_ENABLED) {\n\t\t\tftrace_bug_type = FTRACE_BUG_CALL;\n\t\t\treturn FTRACE_UPDATE_MAKE_CALL;\n\t\t}\n\n\t\tftrace_bug_type = FTRACE_BUG_UPDATE;\n\t\treturn FTRACE_UPDATE_MODIFY_CALL;\n\t}\n\n\tif (update) {\n\t\t \n\t\tif (!ftrace_rec_count(rec))\n\t\t\trec->flags &= FTRACE_NOCLEAR_FLAGS;\n\t\telse\n\t\t\t \n\t\t\trec->flags &= ~(FTRACE_FL_ENABLED | FTRACE_FL_TRAMP_EN |\n\t\t\t\t\tFTRACE_FL_REGS_EN | FTRACE_FL_DIRECT_EN |\n\t\t\t\t\tFTRACE_FL_CALL_OPS_EN);\n\t}\n\n\tftrace_bug_type = FTRACE_BUG_NOP;\n\treturn FTRACE_UPDATE_MAKE_NOP;\n}\n\n \nint ftrace_update_record(struct dyn_ftrace *rec, bool enable)\n{\n\treturn ftrace_check_record(rec, enable, true);\n}\n\n \nint ftrace_test_record(struct dyn_ftrace *rec, bool enable)\n{\n\treturn ftrace_check_record(rec, enable, false);\n}\n\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_any(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *op;\n\tunsigned long ip = rec->ip;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\n\t\tif (!op->trampoline)\n\t\t\tcontinue;\n\n\t\tif (hash_contains_ip(ip, op->func_hash))\n\t\t\treturn op;\n\t} while_for_each_ftrace_op(op);\n\n\treturn NULL;\n}\n\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_any_other(struct dyn_ftrace *rec, struct ftrace_ops *op_exclude)\n{\n\tstruct ftrace_ops *op;\n\tunsigned long ip = rec->ip;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\n\t\tif (op == op_exclude || !op->trampoline)\n\t\t\tcontinue;\n\n\t\tif (hash_contains_ip(ip, op->func_hash))\n\t\t\treturn op;\n\t} while_for_each_ftrace_op(op);\n\n\treturn NULL;\n}\n\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_next(struct dyn_ftrace *rec,\n\t\t\t   struct ftrace_ops *op)\n{\n\tunsigned long ip = rec->ip;\n\n\twhile_for_each_ftrace_op(op) {\n\n\t\tif (!op->trampoline)\n\t\t\tcontinue;\n\n\t\tif (hash_contains_ip(ip, op->func_hash))\n\t\t\treturn op;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_curr(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *op;\n\tunsigned long ip = rec->ip;\n\n\t \n\tif (removed_ops) {\n\t\tif (hash_contains_ip(ip, &removed_ops->old_hash))\n\t\t\treturn removed_ops;\n\t}\n\n\t \n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\n\t\tif (!op->trampoline)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (op->flags & FTRACE_OPS_FL_ADDING)\n\t\t\tcontinue;\n\n\n\t\t \n\t\tif ((op->flags & FTRACE_OPS_FL_MODIFYING) &&\n\t\t    hash_contains_ip(ip, &op->old_hash))\n\t\t\treturn op;\n\t\t \n\t\tif (!(op->flags & FTRACE_OPS_FL_MODIFYING) &&\n\t\t    hash_contains_ip(ip, op->func_hash))\n\t\t\treturn op;\n\n\t} while_for_each_ftrace_op(op);\n\n\treturn NULL;\n}\n\nstatic struct ftrace_ops *\nftrace_find_tramp_ops_new(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *op;\n\tunsigned long ip = rec->ip;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\t \n\t\tif (hash_contains_ip(ip, op->func_hash))\n\t\t\treturn op;\n\t} while_for_each_ftrace_op(op);\n\n\treturn NULL;\n}\n\nstruct ftrace_ops *\nftrace_find_unique_ops(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *op, *found = NULL;\n\tunsigned long ip = rec->ip;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\n\t\tif (hash_contains_ip(ip, op->func_hash)) {\n\t\t\tif (found)\n\t\t\t\treturn NULL;\n\t\t\tfound = op;\n\t\t}\n\n\t} while_for_each_ftrace_op(op);\n\n\treturn found;\n}\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\n \nstatic struct ftrace_hash __rcu *direct_functions = EMPTY_HASH;\nstatic DEFINE_MUTEX(direct_mutex);\nint ftrace_direct_func_count;\n\n \nunsigned long ftrace_find_rec_direct(unsigned long ip)\n{\n\tstruct ftrace_func_entry *entry;\n\n\tentry = __ftrace_lookup_ip(direct_functions, ip);\n\tif (!entry)\n\t\treturn 0;\n\n\treturn entry->direct;\n}\n\nstatic void call_direct_funcs(unsigned long ip, unsigned long pip,\n\t\t\t      struct ftrace_ops *ops, struct ftrace_regs *fregs)\n{\n\tunsigned long addr = READ_ONCE(ops->direct_call);\n\n\tif (!addr)\n\t\treturn;\n\n\tarch_ftrace_set_direct_caller(fregs, addr);\n}\n#endif  \n\n \nunsigned long ftrace_get_addr_new(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *ops;\n\tunsigned long addr;\n\n\tif ((rec->flags & FTRACE_FL_DIRECT) &&\n\t    (ftrace_rec_count(rec) == 1)) {\n\t\taddr = ftrace_find_rec_direct(rec->ip);\n\t\tif (addr)\n\t\t\treturn addr;\n\t\tWARN_ON_ONCE(1);\n\t}\n\n\t \n\tif (rec->flags & FTRACE_FL_TRAMP) {\n\t\tops = ftrace_find_tramp_ops_new(rec);\n\t\tif (FTRACE_WARN_ON(!ops || !ops->trampoline)) {\n\t\t\tpr_warn(\"Bad trampoline accounting at: %p (%pS) (%lx)\\n\",\n\t\t\t\t(void *)rec->ip, (void *)rec->ip, rec->flags);\n\t\t\t \n\t\t\treturn (unsigned long)FTRACE_ADDR;\n\t\t}\n\t\treturn ops->trampoline;\n\t}\n\n\tif (rec->flags & FTRACE_FL_REGS)\n\t\treturn (unsigned long)FTRACE_REGS_ADDR;\n\telse\n\t\treturn (unsigned long)FTRACE_ADDR;\n}\n\n \nunsigned long ftrace_get_addr_curr(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *ops;\n\tunsigned long addr;\n\n\t \n\tif (rec->flags & FTRACE_FL_DIRECT_EN) {\n\t\taddr = ftrace_find_rec_direct(rec->ip);\n\t\tif (addr)\n\t\t\treturn addr;\n\t\tWARN_ON_ONCE(1);\n\t}\n\n\t \n\tif (rec->flags & FTRACE_FL_TRAMP_EN) {\n\t\tops = ftrace_find_tramp_ops_curr(rec);\n\t\tif (FTRACE_WARN_ON(!ops)) {\n\t\t\tpr_warn(\"Bad trampoline accounting at: %p (%pS)\\n\",\n\t\t\t\t(void *)rec->ip, (void *)rec->ip);\n\t\t\t \n\t\t\treturn (unsigned long)FTRACE_ADDR;\n\t\t}\n\t\treturn ops->trampoline;\n\t}\n\n\tif (rec->flags & FTRACE_FL_REGS_EN)\n\t\treturn (unsigned long)FTRACE_REGS_ADDR;\n\telse\n\t\treturn (unsigned long)FTRACE_ADDR;\n}\n\nstatic int\n__ftrace_replace_code(struct dyn_ftrace *rec, bool enable)\n{\n\tunsigned long ftrace_old_addr;\n\tunsigned long ftrace_addr;\n\tint ret;\n\n\tftrace_addr = ftrace_get_addr_new(rec);\n\n\t \n\tftrace_old_addr = ftrace_get_addr_curr(rec);\n\n\tret = ftrace_update_record(rec, enable);\n\n\tftrace_bug_type = FTRACE_BUG_UNKNOWN;\n\n\tswitch (ret) {\n\tcase FTRACE_UPDATE_IGNORE:\n\t\treturn 0;\n\n\tcase FTRACE_UPDATE_MAKE_CALL:\n\t\tftrace_bug_type = FTRACE_BUG_CALL;\n\t\treturn ftrace_make_call(rec, ftrace_addr);\n\n\tcase FTRACE_UPDATE_MAKE_NOP:\n\t\tftrace_bug_type = FTRACE_BUG_NOP;\n\t\treturn ftrace_make_nop(NULL, rec, ftrace_old_addr);\n\n\tcase FTRACE_UPDATE_MODIFY_CALL:\n\t\tftrace_bug_type = FTRACE_BUG_UPDATE;\n\t\treturn ftrace_modify_call(rec, ftrace_old_addr, ftrace_addr);\n\t}\n\n\treturn -1;  \n}\n\nvoid __weak ftrace_replace_code(int mod_flags)\n{\n\tstruct dyn_ftrace *rec;\n\tstruct ftrace_page *pg;\n\tbool enable = mod_flags & FTRACE_MODIFY_ENABLE_FL;\n\tint schedulable = mod_flags & FTRACE_MODIFY_MAY_SLEEP_FL;\n\tint failed;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn;\n\n\tdo_for_each_ftrace_rec(pg, rec) {\n\n\t\tif (skip_record(rec))\n\t\t\tcontinue;\n\n\t\tfailed = __ftrace_replace_code(rec, enable);\n\t\tif (failed) {\n\t\t\tftrace_bug(failed, rec);\n\t\t\t \n\t\t\treturn;\n\t\t}\n\t\tif (schedulable)\n\t\t\tcond_resched();\n\t} while_for_each_ftrace_rec();\n}\n\nstruct ftrace_rec_iter {\n\tstruct ftrace_page\t*pg;\n\tint\t\t\tindex;\n};\n\n \nstruct ftrace_rec_iter *ftrace_rec_iter_start(void)\n{\n\t \n\tstatic struct ftrace_rec_iter ftrace_rec_iter;\n\tstruct ftrace_rec_iter *iter = &ftrace_rec_iter;\n\n\titer->pg = ftrace_pages_start;\n\titer->index = 0;\n\n\t \n\twhile (iter->pg && !iter->pg->index)\n\t\titer->pg = iter->pg->next;\n\n\tif (!iter->pg)\n\t\treturn NULL;\n\n\treturn iter;\n}\n\n \nstruct ftrace_rec_iter *ftrace_rec_iter_next(struct ftrace_rec_iter *iter)\n{\n\titer->index++;\n\n\tif (iter->index >= iter->pg->index) {\n\t\titer->pg = iter->pg->next;\n\t\titer->index = 0;\n\n\t\t \n\t\twhile (iter->pg && !iter->pg->index)\n\t\t\titer->pg = iter->pg->next;\n\t}\n\n\tif (!iter->pg)\n\t\treturn NULL;\n\n\treturn iter;\n}\n\n \nstruct dyn_ftrace *ftrace_rec_iter_record(struct ftrace_rec_iter *iter)\n{\n\treturn &iter->pg->records[iter->index];\n}\n\nstatic int\nftrace_nop_initialize(struct module *mod, struct dyn_ftrace *rec)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn 0;\n\n\tret = ftrace_init_nop(mod, rec);\n\tif (ret) {\n\t\tftrace_bug_type = FTRACE_BUG_INIT;\n\t\tftrace_bug(ret, rec);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nvoid __weak ftrace_arch_code_modify_prepare(void)\n{\n}\n\n \nvoid __weak ftrace_arch_code_modify_post_process(void)\n{\n}\n\nstatic int update_ftrace_func(ftrace_func_t func)\n{\n\tstatic ftrace_func_t save_func;\n\n\t \n\tif (func == save_func)\n\t\treturn 0;\n\n\tsave_func = func;\n\n\treturn ftrace_update_ftrace_func(func);\n}\n\nvoid ftrace_modify_all_code(int command)\n{\n\tint update = command & FTRACE_UPDATE_TRACE_FUNC;\n\tint mod_flags = 0;\n\tint err = 0;\n\n\tif (command & FTRACE_MAY_SLEEP)\n\t\tmod_flags = FTRACE_MODIFY_MAY_SLEEP_FL;\n\n\t \n\tif (update) {\n\t\terr = update_ftrace_func(ftrace_ops_list_func);\n\t\tif (FTRACE_WARN_ON(err))\n\t\t\treturn;\n\t}\n\n\tif (command & FTRACE_UPDATE_CALLS)\n\t\tftrace_replace_code(mod_flags | FTRACE_MODIFY_ENABLE_FL);\n\telse if (command & FTRACE_DISABLE_CALLS)\n\t\tftrace_replace_code(mod_flags);\n\n\tif (update && ftrace_trace_function != ftrace_ops_list_func) {\n\t\tfunction_trace_op = set_function_trace_op;\n\t\tsmp_wmb();\n\t\t \n\t\tif (!irqs_disabled())\n\t\t\tsmp_call_function(ftrace_sync_ipi, NULL, 1);\n\t\terr = update_ftrace_func(ftrace_trace_function);\n\t\tif (FTRACE_WARN_ON(err))\n\t\t\treturn;\n\t}\n\n\tif (command & FTRACE_START_FUNC_RET)\n\t\terr = ftrace_enable_ftrace_graph_caller();\n\telse if (command & FTRACE_STOP_FUNC_RET)\n\t\terr = ftrace_disable_ftrace_graph_caller();\n\tFTRACE_WARN_ON(err);\n}\n\nstatic int __ftrace_modify_code(void *data)\n{\n\tint *command = data;\n\n\tftrace_modify_all_code(*command);\n\n\treturn 0;\n}\n\n \nvoid ftrace_run_stop_machine(int command)\n{\n\tstop_machine(__ftrace_modify_code, &command, NULL);\n}\n\n \nvoid __weak arch_ftrace_update_code(int command)\n{\n\tftrace_run_stop_machine(command);\n}\n\nstatic void ftrace_run_update_code(int command)\n{\n\tftrace_arch_code_modify_prepare();\n\n\t \n\tarch_ftrace_update_code(command);\n\n\tftrace_arch_code_modify_post_process();\n}\n\nstatic void ftrace_run_modify_code(struct ftrace_ops *ops, int command,\n\t\t\t\t   struct ftrace_ops_hash *old_hash)\n{\n\tops->flags |= FTRACE_OPS_FL_MODIFYING;\n\tops->old_hash.filter_hash = old_hash->filter_hash;\n\tops->old_hash.notrace_hash = old_hash->notrace_hash;\n\tftrace_run_update_code(command);\n\tops->old_hash.filter_hash = NULL;\n\tops->old_hash.notrace_hash = NULL;\n\tops->flags &= ~FTRACE_OPS_FL_MODIFYING;\n}\n\nstatic ftrace_func_t saved_ftrace_func;\nstatic int ftrace_start_up;\n\nvoid __weak arch_ftrace_trampoline_free(struct ftrace_ops *ops)\n{\n}\n\n \nstatic LIST_HEAD(ftrace_ops_trampoline_list);\n\nstatic void ftrace_add_trampoline_to_kallsyms(struct ftrace_ops *ops)\n{\n\tlockdep_assert_held(&ftrace_lock);\n\tlist_add_rcu(&ops->list, &ftrace_ops_trampoline_list);\n}\n\nstatic void ftrace_remove_trampoline_from_kallsyms(struct ftrace_ops *ops)\n{\n\tlockdep_assert_held(&ftrace_lock);\n\tlist_del_rcu(&ops->list);\n\tsynchronize_rcu();\n}\n\n \n#define FTRACE_TRAMPOLINE_MOD \"__builtin__ftrace\"\n#define FTRACE_TRAMPOLINE_SYM \"ftrace_trampoline\"\n\nstatic void ftrace_trampoline_free(struct ftrace_ops *ops)\n{\n\tif (ops && (ops->flags & FTRACE_OPS_FL_ALLOC_TRAMP) &&\n\t    ops->trampoline) {\n\t\t \n\t\tperf_event_text_poke((void *)ops->trampoline,\n\t\t\t\t     (void *)ops->trampoline,\n\t\t\t\t     ops->trampoline_size, NULL, 0);\n\t\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL,\n\t\t\t\t   ops->trampoline, ops->trampoline_size,\n\t\t\t\t   true, FTRACE_TRAMPOLINE_SYM);\n\t\t \n\t\tftrace_remove_trampoline_from_kallsyms(ops);\n\t}\n\n\tarch_ftrace_trampoline_free(ops);\n}\n\nstatic void ftrace_startup_enable(int command)\n{\n\tif (saved_ftrace_func != ftrace_trace_function) {\n\t\tsaved_ftrace_func = ftrace_trace_function;\n\t\tcommand |= FTRACE_UPDATE_TRACE_FUNC;\n\t}\n\n\tif (!command || !ftrace_enabled)\n\t\treturn;\n\n\tftrace_run_update_code(command);\n}\n\nstatic void ftrace_startup_all(int command)\n{\n\tupdate_all_ops = true;\n\tftrace_startup_enable(command);\n\tupdate_all_ops = false;\n}\n\nint ftrace_startup(struct ftrace_ops *ops, int command)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tret = __register_ftrace_function(ops);\n\tif (ret)\n\t\treturn ret;\n\n\tftrace_start_up++;\n\n\t \n\tops->flags |= FTRACE_OPS_FL_ENABLED | FTRACE_OPS_FL_ADDING;\n\n\tret = ftrace_hash_ipmodify_enable(ops);\n\tif (ret < 0) {\n\t\t \n\t\t__unregister_ftrace_function(ops);\n\t\tftrace_start_up--;\n\t\tops->flags &= ~FTRACE_OPS_FL_ENABLED;\n\t\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC)\n\t\t\tftrace_trampoline_free(ops);\n\t\treturn ret;\n\t}\n\n\tif (ftrace_hash_rec_enable(ops, 1))\n\t\tcommand |= FTRACE_UPDATE_CALLS;\n\n\tftrace_startup_enable(command);\n\n\t \n\tif (unlikely(ftrace_disabled)) {\n\t\t__unregister_ftrace_function(ops);\n\t\treturn -ENODEV;\n\t}\n\n\tops->flags &= ~FTRACE_OPS_FL_ADDING;\n\n\treturn 0;\n}\n\nint ftrace_shutdown(struct ftrace_ops *ops, int command)\n{\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tret = __unregister_ftrace_function(ops);\n\tif (ret)\n\t\treturn ret;\n\n\tftrace_start_up--;\n\t \n\tWARN_ON_ONCE(ftrace_start_up < 0);\n\n\t \n\tftrace_hash_ipmodify_disable(ops);\n\n\tif (ftrace_hash_rec_disable(ops, 1))\n\t\tcommand |= FTRACE_UPDATE_CALLS;\n\n\tops->flags &= ~FTRACE_OPS_FL_ENABLED;\n\n\tif (saved_ftrace_func != ftrace_trace_function) {\n\t\tsaved_ftrace_func = ftrace_trace_function;\n\t\tcommand |= FTRACE_UPDATE_TRACE_FUNC;\n\t}\n\n\tif (!command || !ftrace_enabled)\n\t\tgoto out;\n\n\t \n\tops->flags |= FTRACE_OPS_FL_REMOVING;\n\tremoved_ops = ops;\n\n\t \n\tops->old_hash.filter_hash = ops->func_hash->filter_hash;\n\tops->old_hash.notrace_hash = ops->func_hash->notrace_hash;\n\n\tftrace_run_update_code(command);\n\n\t \n\tif (rcu_dereference_protected(ftrace_ops_list,\n\t\t\tlockdep_is_held(&ftrace_lock)) == &ftrace_list_end) {\n\t\tstruct ftrace_page *pg;\n\t\tstruct dyn_ftrace *rec;\n\n\t\tdo_for_each_ftrace_rec(pg, rec) {\n\t\t\tif (FTRACE_WARN_ON_ONCE(rec->flags & ~FTRACE_NOCLEAR_FLAGS))\n\t\t\t\tpr_warn(\"  %pS flags:%lx\\n\",\n\t\t\t\t\t(void *)rec->ip, rec->flags);\n\t\t} while_for_each_ftrace_rec();\n\t}\n\n\tops->old_hash.filter_hash = NULL;\n\tops->old_hash.notrace_hash = NULL;\n\n\tremoved_ops = NULL;\n\tops->flags &= ~FTRACE_OPS_FL_REMOVING;\n\nout:\n\t \n\tif (ops->flags & FTRACE_OPS_FL_DYNAMIC) {\n\t\t \n\t\tsynchronize_rcu_tasks_rude();\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_PREEMPTION))\n\t\t\tsynchronize_rcu_tasks();\n\n\t\tftrace_trampoline_free(ops);\n\t}\n\n\treturn 0;\n}\n\nstatic u64\t\tftrace_update_time;\nunsigned long\t\tftrace_update_tot_cnt;\nunsigned long\t\tftrace_number_of_pages;\nunsigned long\t\tftrace_number_of_groups;\n\nstatic inline int ops_traces_mod(struct ftrace_ops *ops)\n{\n\t \n\treturn ftrace_hash_empty(ops->func_hash->filter_hash) &&\n\t\tftrace_hash_empty(ops->func_hash->notrace_hash);\n}\n\nstatic int ftrace_update_code(struct module *mod, struct ftrace_page *new_pgs)\n{\n\tbool init_nop = ftrace_need_init_nop();\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *p;\n\tu64 start, stop;\n\tunsigned long update_cnt = 0;\n\tunsigned long rec_flags = 0;\n\tint i;\n\n\tstart = ftrace_now(raw_smp_processor_id());\n\n\t \n\tif (mod)\n\t\trec_flags |= FTRACE_FL_DISABLED;\n\n\tfor (pg = new_pgs; pg; pg = pg->next) {\n\n\t\tfor (i = 0; i < pg->index; i++) {\n\n\t\t\t \n\t\t\tif (unlikely(ftrace_disabled))\n\t\t\t\treturn -1;\n\n\t\t\tp = &pg->records[i];\n\t\t\tp->flags = rec_flags;\n\n\t\t\t \n\t\t\tif (init_nop && !ftrace_nop_initialize(mod, p))\n\t\t\t\tbreak;\n\n\t\t\tupdate_cnt++;\n\t\t}\n\t}\n\n\tstop = ftrace_now(raw_smp_processor_id());\n\tftrace_update_time = stop - start;\n\tftrace_update_tot_cnt += update_cnt;\n\n\treturn 0;\n}\n\nstatic int ftrace_allocate_records(struct ftrace_page *pg, int count)\n{\n\tint order;\n\tint pages;\n\tint cnt;\n\n\tif (WARN_ON(!count))\n\t\treturn -EINVAL;\n\n\t \n\tpages = DIV_ROUND_UP(count, ENTRIES_PER_PAGE);\n\torder = fls(pages) - 1;\n\n again:\n\tpg->records = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO, order);\n\n\tif (!pg->records) {\n\t\t \n\t\tif (!order)\n\t\t\treturn -ENOMEM;\n\t\torder--;\n\t\tgoto again;\n\t}\n\n\tftrace_number_of_pages += 1 << order;\n\tftrace_number_of_groups++;\n\n\tcnt = (PAGE_SIZE << order) / ENTRY_SIZE;\n\tpg->order = order;\n\n\tif (cnt > count)\n\t\tcnt = count;\n\n\treturn cnt;\n}\n\nstatic void ftrace_free_pages(struct ftrace_page *pages)\n{\n\tstruct ftrace_page *pg = pages;\n\n\twhile (pg) {\n\t\tif (pg->records) {\n\t\t\tfree_pages((unsigned long)pg->records, pg->order);\n\t\t\tftrace_number_of_pages -= 1 << pg->order;\n\t\t}\n\t\tpages = pg->next;\n\t\tkfree(pg);\n\t\tpg = pages;\n\t\tftrace_number_of_groups--;\n\t}\n}\n\nstatic struct ftrace_page *\nftrace_allocate_pages(unsigned long num_to_init)\n{\n\tstruct ftrace_page *start_pg;\n\tstruct ftrace_page *pg;\n\tint cnt;\n\n\tif (!num_to_init)\n\t\treturn NULL;\n\n\tstart_pg = pg = kzalloc(sizeof(*pg), GFP_KERNEL);\n\tif (!pg)\n\t\treturn NULL;\n\n\t \n\tfor (;;) {\n\t\tcnt = ftrace_allocate_records(pg, num_to_init);\n\t\tif (cnt < 0)\n\t\t\tgoto free_pages;\n\n\t\tnum_to_init -= cnt;\n\t\tif (!num_to_init)\n\t\t\tbreak;\n\n\t\tpg->next = kzalloc(sizeof(*pg), GFP_KERNEL);\n\t\tif (!pg->next)\n\t\t\tgoto free_pages;\n\n\t\tpg = pg->next;\n\t}\n\n\treturn start_pg;\n\n free_pages:\n\tftrace_free_pages(start_pg);\n\tpr_info(\"ftrace: FAILED to allocate memory for functions\\n\");\n\treturn NULL;\n}\n\n#define FTRACE_BUFF_MAX (KSYM_SYMBOL_LEN+4)  \n\nstruct ftrace_iterator {\n\tloff_t\t\t\t\tpos;\n\tloff_t\t\t\t\tfunc_pos;\n\tloff_t\t\t\t\tmod_pos;\n\tstruct ftrace_page\t\t*pg;\n\tstruct dyn_ftrace\t\t*func;\n\tstruct ftrace_func_probe\t*probe;\n\tstruct ftrace_func_entry\t*probe_entry;\n\tstruct trace_parser\t\tparser;\n\tstruct ftrace_hash\t\t*hash;\n\tstruct ftrace_ops\t\t*ops;\n\tstruct trace_array\t\t*tr;\n\tstruct list_head\t\t*mod_list;\n\tint\t\t\t\tpidx;\n\tint\t\t\t\tidx;\n\tunsigned\t\t\tflags;\n};\n\nstatic void *\nt_probe_next(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->ops->private;\n\tstruct list_head *func_probes;\n\tstruct ftrace_hash *hash;\n\tstruct list_head *next;\n\tstruct hlist_node *hnd = NULL;\n\tstruct hlist_head *hhd;\n\tint size;\n\n\t(*pos)++;\n\titer->pos = *pos;\n\n\tif (!tr)\n\t\treturn NULL;\n\n\tfunc_probes = &tr->func_probes;\n\tif (list_empty(func_probes))\n\t\treturn NULL;\n\n\tif (!iter->probe) {\n\t\tnext = func_probes->next;\n\t\titer->probe = list_entry(next, struct ftrace_func_probe, list);\n\t}\n\n\tif (iter->probe_entry)\n\t\thnd = &iter->probe_entry->hlist;\n\n\thash = iter->probe->ops.func_hash->filter_hash;\n\n\t \n\tif (!hash || hash == EMPTY_HASH)\n\t\treturn NULL;\n\n\tsize = 1 << hash->size_bits;\n\n retry:\n\tif (iter->pidx >= size) {\n\t\tif (iter->probe->list.next == func_probes)\n\t\t\treturn NULL;\n\t\tnext = iter->probe->list.next;\n\t\titer->probe = list_entry(next, struct ftrace_func_probe, list);\n\t\thash = iter->probe->ops.func_hash->filter_hash;\n\t\tsize = 1 << hash->size_bits;\n\t\titer->pidx = 0;\n\t}\n\n\thhd = &hash->buckets[iter->pidx];\n\n\tif (hlist_empty(hhd)) {\n\t\titer->pidx++;\n\t\thnd = NULL;\n\t\tgoto retry;\n\t}\n\n\tif (!hnd)\n\t\thnd = hhd->first;\n\telse {\n\t\thnd = hnd->next;\n\t\tif (!hnd) {\n\t\t\titer->pidx++;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (WARN_ON_ONCE(!hnd))\n\t\treturn NULL;\n\n\titer->probe_entry = hlist_entry(hnd, struct ftrace_func_entry, hlist);\n\n\treturn iter;\n}\n\nstatic void *t_probe_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tvoid *p = NULL;\n\tloff_t l;\n\n\tif (!(iter->flags & FTRACE_ITER_DO_PROBES))\n\t\treturn NULL;\n\n\tif (iter->mod_pos > *pos)\n\t\treturn NULL;\n\n\titer->probe = NULL;\n\titer->probe_entry = NULL;\n\titer->pidx = 0;\n\tfor (l = 0; l <= (*pos - iter->mod_pos); ) {\n\t\tp = t_probe_next(m, &l);\n\t\tif (!p)\n\t\t\tbreak;\n\t}\n\tif (!p)\n\t\treturn NULL;\n\n\t \n\titer->flags |= FTRACE_ITER_PROBE;\n\n\treturn iter;\n}\n\nstatic int\nt_probe_show(struct seq_file *m, struct ftrace_iterator *iter)\n{\n\tstruct ftrace_func_entry *probe_entry;\n\tstruct ftrace_probe_ops *probe_ops;\n\tstruct ftrace_func_probe *probe;\n\n\tprobe = iter->probe;\n\tprobe_entry = iter->probe_entry;\n\n\tif (WARN_ON_ONCE(!probe || !probe_entry))\n\t\treturn -EIO;\n\n\tprobe_ops = probe->probe_ops;\n\n\tif (probe_ops->print)\n\t\treturn probe_ops->print(m, probe_entry->ip, probe_ops, probe->data);\n\n\tseq_printf(m, \"%ps:%ps\\n\", (void *)probe_entry->ip,\n\t\t   (void *)probe_ops->func);\n\n\treturn 0;\n}\n\nstatic void *\nt_mod_next(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tstruct trace_array *tr = iter->tr;\n\n\t(*pos)++;\n\titer->pos = *pos;\n\n\titer->mod_list = iter->mod_list->next;\n\n\tif (iter->mod_list == &tr->mod_trace ||\n\t    iter->mod_list == &tr->mod_notrace) {\n\t\titer->flags &= ~FTRACE_ITER_MOD;\n\t\treturn NULL;\n\t}\n\n\titer->mod_pos = *pos;\n\n\treturn iter;\n}\n\nstatic void *t_mod_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tvoid *p = NULL;\n\tloff_t l;\n\n\tif (iter->func_pos > *pos)\n\t\treturn NULL;\n\n\titer->mod_pos = iter->func_pos;\n\n\t \n\tif (!iter->tr)\n\t\treturn NULL;\n\n\tfor (l = 0; l <= (*pos - iter->func_pos); ) {\n\t\tp = t_mod_next(m, &l);\n\t\tif (!p)\n\t\t\tbreak;\n\t}\n\tif (!p) {\n\t\titer->flags &= ~FTRACE_ITER_MOD;\n\t\treturn t_probe_start(m, pos);\n\t}\n\n\t \n\titer->flags |= FTRACE_ITER_MOD;\n\n\treturn iter;\n}\n\nstatic int\nt_mod_show(struct seq_file *m, struct ftrace_iterator *iter)\n{\n\tstruct ftrace_mod_load *ftrace_mod;\n\tstruct trace_array *tr = iter->tr;\n\n\tif (WARN_ON_ONCE(!iter->mod_list) ||\n\t\t\t iter->mod_list == &tr->mod_trace ||\n\t\t\t iter->mod_list == &tr->mod_notrace)\n\t\treturn -EIO;\n\n\tftrace_mod = list_entry(iter->mod_list, struct ftrace_mod_load, list);\n\n\tif (ftrace_mod->func)\n\t\tseq_printf(m, \"%s\", ftrace_mod->func);\n\telse\n\t\tseq_putc(m, '*');\n\n\tseq_printf(m, \":mod:%s\\n\", ftrace_mod->module);\n\n\treturn 0;\n}\n\nstatic void *\nt_func_next(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tstruct dyn_ftrace *rec = NULL;\n\n\t(*pos)++;\n\n retry:\n\tif (iter->idx >= iter->pg->index) {\n\t\tif (iter->pg->next) {\n\t\t\titer->pg = iter->pg->next;\n\t\t\titer->idx = 0;\n\t\t\tgoto retry;\n\t\t}\n\t} else {\n\t\trec = &iter->pg->records[iter->idx++];\n\t\tif (((iter->flags & (FTRACE_ITER_FILTER | FTRACE_ITER_NOTRACE)) &&\n\t\t     !ftrace_lookup_ip(iter->hash, rec->ip)) ||\n\n\t\t    ((iter->flags & FTRACE_ITER_ENABLED) &&\n\t\t     !(rec->flags & FTRACE_FL_ENABLED)) ||\n\n\t\t    ((iter->flags & FTRACE_ITER_TOUCHED) &&\n\t\t     !(rec->flags & FTRACE_FL_TOUCHED))) {\n\n\t\t\trec = NULL;\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (!rec)\n\t\treturn NULL;\n\n\titer->pos = iter->func_pos = *pos;\n\titer->func = rec;\n\n\treturn iter;\n}\n\nstatic void *\nt_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tloff_t l = *pos;  \n\tvoid *ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn NULL;\n\n\tif (iter->flags & FTRACE_ITER_PROBE)\n\t\treturn t_probe_next(m, pos);\n\n\tif (iter->flags & FTRACE_ITER_MOD)\n\t\treturn t_mod_next(m, pos);\n\n\tif (iter->flags & FTRACE_ITER_PRINTALL) {\n\t\t \n\t\t(*pos)++;\n\t\treturn t_mod_start(m, &l);\n\t}\n\n\tret = t_func_next(m, pos);\n\n\tif (!ret)\n\t\treturn t_mod_start(m, &l);\n\n\treturn ret;\n}\n\nstatic void reset_iter_read(struct ftrace_iterator *iter)\n{\n\titer->pos = 0;\n\titer->func_pos = 0;\n\titer->flags &= ~(FTRACE_ITER_PRINTALL | FTRACE_ITER_PROBE | FTRACE_ITER_MOD);\n}\n\nstatic void *t_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tvoid *p = NULL;\n\tloff_t l;\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn NULL;\n\n\t \n\tif (*pos < iter->pos)\n\t\treset_iter_read(iter);\n\n\t \n\tif ((iter->flags & (FTRACE_ITER_FILTER | FTRACE_ITER_NOTRACE)) &&\n\t    ftrace_hash_empty(iter->hash)) {\n\t\titer->func_pos = 1;  \n\t\tif (*pos > 0)\n\t\t\treturn t_mod_start(m, pos);\n\t\titer->flags |= FTRACE_ITER_PRINTALL;\n\t\t \n\t\titer->flags &= ~FTRACE_ITER_PROBE;\n\t\treturn iter;\n\t}\n\n\tif (iter->flags & FTRACE_ITER_MOD)\n\t\treturn t_mod_start(m, pos);\n\n\t \n\titer->pg = ftrace_pages_start;\n\titer->idx = 0;\n\tfor (l = 0; l <= *pos; ) {\n\t\tp = t_func_next(m, &l);\n\t\tif (!p)\n\t\t\tbreak;\n\t}\n\n\tif (!p)\n\t\treturn t_mod_start(m, pos);\n\n\treturn iter;\n}\n\nstatic void t_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&ftrace_lock);\n}\n\nvoid * __weak\narch_ftrace_trampoline_func(struct ftrace_ops *ops, struct dyn_ftrace *rec)\n{\n\treturn NULL;\n}\n\nstatic void add_trampoline_func(struct seq_file *m, struct ftrace_ops *ops,\n\t\t\t\tstruct dyn_ftrace *rec)\n{\n\tvoid *ptr;\n\n\tptr = arch_ftrace_trampoline_func(ops, rec);\n\tif (ptr)\n\t\tseq_printf(m, \" ->%pS\", ptr);\n}\n\n#ifdef FTRACE_MCOUNT_MAX_OFFSET\n \nstatic int test_for_valid_rec(struct dyn_ftrace *rec)\n{\n\tchar str[KSYM_SYMBOL_LEN];\n\tunsigned long offset;\n\tconst char *ret;\n\n\tret = kallsyms_lookup(rec->ip, NULL, &offset, NULL, str);\n\n\t \n\tif (!ret || offset > FTRACE_MCOUNT_MAX_OFFSET) {\n\t\trec->flags |= FTRACE_FL_DISABLED;\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic struct workqueue_struct *ftrace_check_wq __initdata;\nstatic struct work_struct ftrace_check_work __initdata;\n\n \nstatic __init void ftrace_check_work_func(struct work_struct *work)\n{\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec;\n\n\tmutex_lock(&ftrace_lock);\n\tdo_for_each_ftrace_rec(pg, rec) {\n\t\ttest_for_valid_rec(rec);\n\t} while_for_each_ftrace_rec();\n\tmutex_unlock(&ftrace_lock);\n}\n\nstatic int __init ftrace_check_for_weak_functions(void)\n{\n\tINIT_WORK(&ftrace_check_work, ftrace_check_work_func);\n\n\tftrace_check_wq = alloc_workqueue(\"ftrace_check_wq\", WQ_UNBOUND, 0);\n\n\tqueue_work(ftrace_check_wq, &ftrace_check_work);\n\treturn 0;\n}\n\nstatic int __init ftrace_check_sync(void)\n{\n\t \n\tif (ftrace_check_wq)\n\t\tdestroy_workqueue(ftrace_check_wq);\n\treturn 0;\n}\n\nlate_initcall_sync(ftrace_check_sync);\nsubsys_initcall(ftrace_check_for_weak_functions);\n\nstatic int print_rec(struct seq_file *m, unsigned long ip)\n{\n\tunsigned long offset;\n\tchar str[KSYM_SYMBOL_LEN];\n\tchar *modname;\n\tconst char *ret;\n\n\tret = kallsyms_lookup(ip, NULL, &offset, &modname, str);\n\t \n\tif (!ret || offset > FTRACE_MCOUNT_MAX_OFFSET) {\n\t\tsnprintf(str, KSYM_SYMBOL_LEN, \"%s_%ld\",\n\t\t\t FTRACE_INVALID_FUNCTION, offset);\n\t\tret = NULL;\n\t}\n\n\tseq_puts(m, str);\n\tif (modname)\n\t\tseq_printf(m, \" [%s]\", modname);\n\treturn ret == NULL ? -1 : 0;\n}\n#else\nstatic inline int test_for_valid_rec(struct dyn_ftrace *rec)\n{\n\treturn 1;\n}\n\nstatic inline int print_rec(struct seq_file *m, unsigned long ip)\n{\n\tseq_printf(m, \"%ps\", (void *)ip);\n\treturn 0;\n}\n#endif\n\nstatic int t_show(struct seq_file *m, void *v)\n{\n\tstruct ftrace_iterator *iter = m->private;\n\tstruct dyn_ftrace *rec;\n\n\tif (iter->flags & FTRACE_ITER_PROBE)\n\t\treturn t_probe_show(m, iter);\n\n\tif (iter->flags & FTRACE_ITER_MOD)\n\t\treturn t_mod_show(m, iter);\n\n\tif (iter->flags & FTRACE_ITER_PRINTALL) {\n\t\tif (iter->flags & FTRACE_ITER_NOTRACE)\n\t\t\tseq_puts(m, \"#### no functions disabled ####\\n\");\n\t\telse\n\t\t\tseq_puts(m, \"#### all functions enabled ####\\n\");\n\t\treturn 0;\n\t}\n\n\trec = iter->func;\n\n\tif (!rec)\n\t\treturn 0;\n\n\tif (iter->flags & FTRACE_ITER_ADDRS)\n\t\tseq_printf(m, \"%lx \", rec->ip);\n\n\tif (print_rec(m, rec->ip)) {\n\t\t \n\t\tWARN_ON_ONCE(!(rec->flags & FTRACE_FL_DISABLED));\n\t\tseq_putc(m, '\\n');\n\t\treturn 0;\n\t}\n\n\tif (iter->flags & (FTRACE_ITER_ENABLED | FTRACE_ITER_TOUCHED)) {\n\t\tstruct ftrace_ops *ops;\n\n\t\tseq_printf(m, \" (%ld)%s%s%s%s%s\",\n\t\t\t   ftrace_rec_count(rec),\n\t\t\t   rec->flags & FTRACE_FL_REGS ? \" R\" : \"  \",\n\t\t\t   rec->flags & FTRACE_FL_IPMODIFY ? \" I\" : \"  \",\n\t\t\t   rec->flags & FTRACE_FL_DIRECT ? \" D\" : \"  \",\n\t\t\t   rec->flags & FTRACE_FL_CALL_OPS ? \" O\" : \"  \",\n\t\t\t   rec->flags & FTRACE_FL_MODIFIED ? \" M \" : \"   \");\n\t\tif (rec->flags & FTRACE_FL_TRAMP_EN) {\n\t\t\tops = ftrace_find_tramp_ops_any(rec);\n\t\t\tif (ops) {\n\t\t\t\tdo {\n\t\t\t\t\tseq_printf(m, \"\\ttramp: %pS (%pS)\",\n\t\t\t\t\t\t   (void *)ops->trampoline,\n\t\t\t\t\t\t   (void *)ops->func);\n\t\t\t\t\tadd_trampoline_func(m, ops, rec);\n\t\t\t\t\tops = ftrace_find_tramp_ops_next(rec, ops);\n\t\t\t\t} while (ops);\n\t\t\t} else\n\t\t\t\tseq_puts(m, \"\\ttramp: ERROR!\");\n\t\t} else {\n\t\t\tadd_trampoline_func(m, NULL, rec);\n\t\t}\n\t\tif (rec->flags & FTRACE_FL_CALL_OPS_EN) {\n\t\t\tops = ftrace_find_unique_ops(rec);\n\t\t\tif (ops) {\n\t\t\t\tseq_printf(m, \"\\tops: %pS (%pS)\",\n\t\t\t\t\t   ops, ops->func);\n\t\t\t} else {\n\t\t\t\tseq_puts(m, \"\\tops: ERROR!\");\n\t\t\t}\n\t\t}\n\t\tif (rec->flags & FTRACE_FL_DIRECT) {\n\t\t\tunsigned long direct;\n\n\t\t\tdirect = ftrace_find_rec_direct(rec->ip);\n\t\t\tif (direct)\n\t\t\t\tseq_printf(m, \"\\n\\tdirect-->%pS\", (void *)direct);\n\t\t}\n\t}\n\n\tseq_putc(m, '\\n');\n\n\treturn 0;\n}\n\nstatic const struct seq_operations show_ftrace_seq_ops = {\n\t.start = t_start,\n\t.next = t_next,\n\t.stop = t_stop,\n\t.show = t_show,\n};\n\nstatic int\nftrace_avail_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_iterator *iter;\n\tint ret;\n\n\tret = security_locked_down(LOCKDOWN_TRACEFS);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\titer = __seq_open_private(file, &show_ftrace_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn -ENOMEM;\n\n\titer->pg = ftrace_pages_start;\n\titer->ops = &global_ops;\n\n\treturn 0;\n}\n\nstatic int\nftrace_enabled_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_iterator *iter;\n\n\t \n\n\titer = __seq_open_private(file, &show_ftrace_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn -ENOMEM;\n\n\titer->pg = ftrace_pages_start;\n\titer->flags = FTRACE_ITER_ENABLED;\n\titer->ops = &global_ops;\n\n\treturn 0;\n}\n\nstatic int\nftrace_touched_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_iterator *iter;\n\n\t \n\n\titer = __seq_open_private(file, &show_ftrace_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn -ENOMEM;\n\n\titer->pg = ftrace_pages_start;\n\titer->flags = FTRACE_ITER_TOUCHED;\n\titer->ops = &global_ops;\n\n\treturn 0;\n}\n\nstatic int\nftrace_avail_addrs_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_iterator *iter;\n\tint ret;\n\n\tret = security_locked_down(LOCKDOWN_TRACEFS);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\titer = __seq_open_private(file, &show_ftrace_seq_ops, sizeof(*iter));\n\tif (!iter)\n\t\treturn -ENOMEM;\n\n\titer->pg = ftrace_pages_start;\n\titer->flags = FTRACE_ITER_ADDRS;\n\titer->ops = &global_ops;\n\n\treturn 0;\n}\n\n \nint\nftrace_regex_open(struct ftrace_ops *ops, int flag,\n\t\t  struct inode *inode, struct file *file)\n{\n\tstruct ftrace_iterator *iter;\n\tstruct ftrace_hash *hash;\n\tstruct list_head *mod_head;\n\tstruct trace_array *tr = ops->private;\n\tint ret = -ENOMEM;\n\n\tftrace_ops_init(ops);\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tif (tracing_check_open_get_tr(tr))\n\t\treturn -ENODEV;\n\n\titer = kzalloc(sizeof(*iter), GFP_KERNEL);\n\tif (!iter)\n\t\tgoto out;\n\n\tif (trace_parser_get_init(&iter->parser, FTRACE_BUFF_MAX))\n\t\tgoto out;\n\n\titer->ops = ops;\n\titer->flags = flag;\n\titer->tr = tr;\n\n\tmutex_lock(&ops->func_hash->regex_lock);\n\n\tif (flag & FTRACE_ITER_NOTRACE) {\n\t\thash = ops->func_hash->notrace_hash;\n\t\tmod_head = tr ? &tr->mod_notrace : NULL;\n\t} else {\n\t\thash = ops->func_hash->filter_hash;\n\t\tmod_head = tr ? &tr->mod_trace : NULL;\n\t}\n\n\titer->mod_list = mod_head;\n\n\tif (file->f_mode & FMODE_WRITE) {\n\t\tconst int size_bits = FTRACE_HASH_DEFAULT_BITS;\n\n\t\tif (file->f_flags & O_TRUNC) {\n\t\t\titer->hash = alloc_ftrace_hash(size_bits);\n\t\t\tclear_ftrace_mod_list(mod_head);\n\t        } else {\n\t\t\titer->hash = alloc_and_copy_ftrace_hash(size_bits, hash);\n\t\t}\n\n\t\tif (!iter->hash) {\n\t\t\ttrace_parser_put(&iter->parser);\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else\n\t\titer->hash = hash;\n\n\tret = 0;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer->pg = ftrace_pages_start;\n\n\t\tret = seq_open(file, &show_ftrace_seq_ops);\n\t\tif (!ret) {\n\t\t\tstruct seq_file *m = file->private_data;\n\t\t\tm->private = iter;\n\t\t} else {\n\t\t\t \n\t\t\tfree_ftrace_hash(iter->hash);\n\t\t\ttrace_parser_put(&iter->parser);\n\t\t}\n\t} else\n\t\tfile->private_data = iter;\n\n out_unlock:\n\tmutex_unlock(&ops->func_hash->regex_lock);\n\n out:\n\tif (ret) {\n\t\tkfree(iter);\n\t\tif (tr)\n\t\t\ttrace_array_put(tr);\n\t}\n\n\treturn ret;\n}\n\nstatic int\nftrace_filter_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_ops *ops = inode->i_private;\n\n\t \n\treturn ftrace_regex_open(ops,\n\t\t\tFTRACE_ITER_FILTER | FTRACE_ITER_DO_PROBES,\n\t\t\tinode, file);\n}\n\nstatic int\nftrace_notrace_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_ops *ops = inode->i_private;\n\n\t \n\treturn ftrace_regex_open(ops, FTRACE_ITER_NOTRACE,\n\t\t\t\t inode, file);\n}\n\n \nstruct ftrace_glob {\n\tchar *search;\n\tunsigned len;\n\tint type;\n};\n\n \nchar * __weak arch_ftrace_match_adjust(char *str, const char *search)\n{\n\treturn str;\n}\n\nstatic int ftrace_match(char *str, struct ftrace_glob *g)\n{\n\tint matched = 0;\n\tint slen;\n\n\tstr = arch_ftrace_match_adjust(str, g->search);\n\n\tswitch (g->type) {\n\tcase MATCH_FULL:\n\t\tif (strcmp(str, g->search) == 0)\n\t\t\tmatched = 1;\n\t\tbreak;\n\tcase MATCH_FRONT_ONLY:\n\t\tif (strncmp(str, g->search, g->len) == 0)\n\t\t\tmatched = 1;\n\t\tbreak;\n\tcase MATCH_MIDDLE_ONLY:\n\t\tif (strstr(str, g->search))\n\t\t\tmatched = 1;\n\t\tbreak;\n\tcase MATCH_END_ONLY:\n\t\tslen = strlen(str);\n\t\tif (slen >= g->len &&\n\t\t    memcmp(str + slen - g->len, g->search, g->len) == 0)\n\t\t\tmatched = 1;\n\t\tbreak;\n\tcase MATCH_GLOB:\n\t\tif (glob_match(g->search, str))\n\t\t\tmatched = 1;\n\t\tbreak;\n\t}\n\n\treturn matched;\n}\n\nstatic int\nenter_record(struct ftrace_hash *hash, struct dyn_ftrace *rec, int clear_filter)\n{\n\tstruct ftrace_func_entry *entry;\n\tint ret = 0;\n\n\tentry = ftrace_lookup_ip(hash, rec->ip);\n\tif (clear_filter) {\n\t\t \n\t\tif (!entry)\n\t\t\treturn 0;\n\n\t\tfree_hash_entry(hash, entry);\n\t} else {\n\t\t \n\t\tif (entry)\n\t\t\treturn 0;\n\t\tif (add_hash_entry(hash, rec->ip) == NULL)\n\t\t\tret = -ENOMEM;\n\t}\n\treturn ret;\n}\n\nstatic int\nadd_rec_by_index(struct ftrace_hash *hash, struct ftrace_glob *func_g,\n\t\t int clear_filter)\n{\n\tlong index = simple_strtoul(func_g->search, NULL, 0);\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec;\n\n\t \n\tif (--index < 0)\n\t\treturn 0;\n\n\tdo_for_each_ftrace_rec(pg, rec) {\n\t\tif (pg->index <= index) {\n\t\t\tindex -= pg->index;\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\trec = &pg->records[index];\n\t\tenter_record(hash, rec, clear_filter);\n\t\treturn 1;\n\t} while_for_each_ftrace_rec();\n\treturn 0;\n}\n\n#ifdef FTRACE_MCOUNT_MAX_OFFSET\nstatic int lookup_ip(unsigned long ip, char **modname, char *str)\n{\n\tunsigned long offset;\n\n\tkallsyms_lookup(ip, NULL, &offset, modname, str);\n\tif (offset > FTRACE_MCOUNT_MAX_OFFSET)\n\t\treturn -1;\n\treturn 0;\n}\n#else\nstatic int lookup_ip(unsigned long ip, char **modname, char *str)\n{\n\tkallsyms_lookup(ip, NULL, NULL, modname, str);\n\treturn 0;\n}\n#endif\n\nstatic int\nftrace_match_record(struct dyn_ftrace *rec, struct ftrace_glob *func_g,\n\t\tstruct ftrace_glob *mod_g, int exclude_mod)\n{\n\tchar str[KSYM_SYMBOL_LEN];\n\tchar *modname;\n\n\tif (lookup_ip(rec->ip, &modname, str)) {\n\t\t \n\t\tWARN_ON_ONCE(system_state == SYSTEM_RUNNING &&\n\t\t\t     !(rec->flags & FTRACE_FL_DISABLED));\n\t\treturn 0;\n\t}\n\n\tif (mod_g) {\n\t\tint mod_matches = (modname) ? ftrace_match(modname, mod_g) : 0;\n\n\t\t \n\t\tif (!mod_g->len) {\n\t\t\t \n\t\t\tif (!exclude_mod != !modname)\n\t\t\t\tgoto func_match;\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tif (!mod_matches == !exclude_mod)\n\t\t\treturn 0;\nfunc_match:\n\t\t \n\t\tif (!func_g->len)\n\t\t\treturn 1;\n\t}\n\n\treturn ftrace_match(str, func_g);\n}\n\nstatic int\nmatch_records(struct ftrace_hash *hash, char *func, int len, char *mod)\n{\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec;\n\tstruct ftrace_glob func_g = { .type = MATCH_FULL };\n\tstruct ftrace_glob mod_g = { .type = MATCH_FULL };\n\tstruct ftrace_glob *mod_match = (mod) ? &mod_g : NULL;\n\tint exclude_mod = 0;\n\tint found = 0;\n\tint ret;\n\tint clear_filter = 0;\n\n\tif (func) {\n\t\tfunc_g.type = filter_parse_regex(func, len, &func_g.search,\n\t\t\t\t\t\t &clear_filter);\n\t\tfunc_g.len = strlen(func_g.search);\n\t}\n\n\tif (mod) {\n\t\tmod_g.type = filter_parse_regex(mod, strlen(mod),\n\t\t\t\t&mod_g.search, &exclude_mod);\n\t\tmod_g.len = strlen(mod_g.search);\n\t}\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(ftrace_disabled))\n\t\tgoto out_unlock;\n\n\tif (func_g.type == MATCH_INDEX) {\n\t\tfound = add_rec_by_index(hash, &func_g, clear_filter);\n\t\tgoto out_unlock;\n\t}\n\n\tdo_for_each_ftrace_rec(pg, rec) {\n\n\t\tif (rec->flags & FTRACE_FL_DISABLED)\n\t\t\tcontinue;\n\n\t\tif (ftrace_match_record(rec, &func_g, mod_match, exclude_mod)) {\n\t\t\tret = enter_record(hash, rec, clear_filter);\n\t\t\tif (ret < 0) {\n\t\t\t\tfound = ret;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tfound = 1;\n\t\t}\n\t\tcond_resched();\n\t} while_for_each_ftrace_rec();\n out_unlock:\n\tmutex_unlock(&ftrace_lock);\n\n\treturn found;\n}\n\nstatic int\nftrace_match_records(struct ftrace_hash *hash, char *buff, int len)\n{\n\treturn match_records(hash, buff, len, NULL);\n}\n\nstatic void ftrace_ops_update_code(struct ftrace_ops *ops,\n\t\t\t\t   struct ftrace_ops_hash *old_hash)\n{\n\tstruct ftrace_ops *op;\n\n\tif (!ftrace_enabled)\n\t\treturn;\n\n\tif (ops->flags & FTRACE_OPS_FL_ENABLED) {\n\t\tftrace_run_modify_code(ops, FTRACE_UPDATE_CALLS, old_hash);\n\t\treturn;\n\t}\n\n\t \n\tif (ops->func_hash != &global_ops.local_hash)\n\t\treturn;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op->func_hash == &global_ops.local_hash &&\n\t\t    op->flags & FTRACE_OPS_FL_ENABLED) {\n\t\t\tftrace_run_modify_code(op, FTRACE_UPDATE_CALLS, old_hash);\n\t\t\t \n\t\t\treturn;\n\t\t}\n\t} while_for_each_ftrace_op(op);\n}\n\nstatic int ftrace_hash_move_and_update_ops(struct ftrace_ops *ops,\n\t\t\t\t\t   struct ftrace_hash **orig_hash,\n\t\t\t\t\t   struct ftrace_hash *hash,\n\t\t\t\t\t   int enable)\n{\n\tstruct ftrace_ops_hash old_hash_ops;\n\tstruct ftrace_hash *old_hash;\n\tint ret;\n\n\told_hash = *orig_hash;\n\told_hash_ops.filter_hash = ops->func_hash->filter_hash;\n\told_hash_ops.notrace_hash = ops->func_hash->notrace_hash;\n\tret = ftrace_hash_move(ops, enable, orig_hash, hash);\n\tif (!ret) {\n\t\tftrace_ops_update_code(ops, &old_hash_ops);\n\t\tfree_ftrace_hash_rcu(old_hash);\n\t}\n\treturn ret;\n}\n\nstatic bool module_exists(const char *module)\n{\n\t \n\tstatic const char this_mod[] = \"__this_module\";\n\tchar modname[MAX_PARAM_PREFIX_LEN + sizeof(this_mod) + 2];\n\tunsigned long val;\n\tint n;\n\n\tn = snprintf(modname, sizeof(modname), \"%s:%s\", module, this_mod);\n\n\tif (n > sizeof(modname) - 1)\n\t\treturn false;\n\n\tval = module_kallsyms_lookup_name(modname);\n\treturn val != 0;\n}\n\nstatic int cache_mod(struct trace_array *tr,\n\t\t     const char *func, char *module, int enable)\n{\n\tstruct ftrace_mod_load *ftrace_mod, *n;\n\tstruct list_head *head = enable ? &tr->mod_trace : &tr->mod_notrace;\n\tint ret;\n\n\tmutex_lock(&ftrace_lock);\n\n\t \n\tif (func[0] == '!') {\n\t\tfunc++;\n\t\tret = -EINVAL;\n\n\t\t \n\t\tlist_for_each_entry_safe(ftrace_mod, n, head, list) {\n\t\t\tif (strcmp(ftrace_mod->module, module) != 0)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (strcmp(func, \"*\") == 0 ||\n\t\t\t    (ftrace_mod->func &&\n\t\t\t     strcmp(ftrace_mod->func, func) == 0)) {\n\t\t\t\tret = 0;\n\t\t\t\tfree_ftrace_mod(ftrace_mod);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tgoto out;\n\t}\n\n\tret = -EINVAL;\n\t \n\tif (module_exists(module))\n\t\tgoto out;\n\n\t \n\tret = ftrace_add_mod(tr, func, module, enable);\n out:\n\tmutex_unlock(&ftrace_lock);\n\n\treturn ret;\n}\n\nstatic int\nftrace_set_regex(struct ftrace_ops *ops, unsigned char *buf, int len,\n\t\t int reset, int enable);\n\n#ifdef CONFIG_MODULES\nstatic void process_mod_list(struct list_head *head, struct ftrace_ops *ops,\n\t\t\t     char *mod, bool enable)\n{\n\tstruct ftrace_mod_load *ftrace_mod, *n;\n\tstruct ftrace_hash **orig_hash, *new_hash;\n\tLIST_HEAD(process_mods);\n\tchar *func;\n\n\tmutex_lock(&ops->func_hash->regex_lock);\n\n\tif (enable)\n\t\torig_hash = &ops->func_hash->filter_hash;\n\telse\n\t\torig_hash = &ops->func_hash->notrace_hash;\n\n\tnew_hash = alloc_and_copy_ftrace_hash(FTRACE_HASH_DEFAULT_BITS,\n\t\t\t\t\t      *orig_hash);\n\tif (!new_hash)\n\t\tgoto out;  \n\n\tmutex_lock(&ftrace_lock);\n\n\tlist_for_each_entry_safe(ftrace_mod, n, head, list) {\n\n\t\tif (strcmp(ftrace_mod->module, mod) != 0)\n\t\t\tcontinue;\n\n\t\tif (ftrace_mod->func)\n\t\t\tfunc = kstrdup(ftrace_mod->func, GFP_KERNEL);\n\t\telse\n\t\t\tfunc = kstrdup(\"*\", GFP_KERNEL);\n\n\t\tif (!func)  \n\t\t\tcontinue;\n\n\t\tlist_move(&ftrace_mod->list, &process_mods);\n\n\t\t \n\t\tkfree(ftrace_mod->func);\n\t\tftrace_mod->func = func;\n\t}\n\n\tmutex_unlock(&ftrace_lock);\n\n\tlist_for_each_entry_safe(ftrace_mod, n, &process_mods, list) {\n\n\t\tfunc = ftrace_mod->func;\n\n\t\t \n\t\tmatch_records(new_hash, func, strlen(func), mod);\n\t\tfree_ftrace_mod(ftrace_mod);\n\t}\n\n\tif (enable && list_empty(head))\n\t\tnew_hash->flags &= ~FTRACE_HASH_FL_MOD;\n\n\tmutex_lock(&ftrace_lock);\n\n\tftrace_hash_move_and_update_ops(ops, orig_hash,\n\t\t\t\t\t      new_hash, enable);\n\tmutex_unlock(&ftrace_lock);\n\n out:\n\tmutex_unlock(&ops->func_hash->regex_lock);\n\n\tfree_ftrace_hash(new_hash);\n}\n\nstatic void process_cached_mods(const char *mod_name)\n{\n\tstruct trace_array *tr;\n\tchar *mod;\n\n\tmod = kstrdup(mod_name, GFP_KERNEL);\n\tif (!mod)\n\t\treturn;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!list_empty(&tr->mod_trace))\n\t\t\tprocess_mod_list(&tr->mod_trace, tr->ops, mod, true);\n\t\tif (!list_empty(&tr->mod_notrace))\n\t\t\tprocess_mod_list(&tr->mod_notrace, tr->ops, mod, false);\n\t}\n\tmutex_unlock(&trace_types_lock);\n\n\tkfree(mod);\n}\n#endif\n\n \n\nstatic int\nftrace_mod_callback(struct trace_array *tr, struct ftrace_hash *hash,\n\t\t    char *func_orig, char *cmd, char *module, int enable)\n{\n\tchar *func;\n\tint ret;\n\n\t \n\tfunc = kstrdup(func_orig, GFP_KERNEL);\n\tif (!func)\n\t\treturn -ENOMEM;\n\n\t \n\tret = match_records(hash, func, strlen(func), module);\n\tkfree(func);\n\n\tif (!ret)\n\t\treturn cache_mod(tr, func_orig, module, enable);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic struct ftrace_func_command ftrace_mod_cmd = {\n\t.name\t\t\t= \"mod\",\n\t.func\t\t\t= ftrace_mod_callback,\n};\n\nstatic int __init ftrace_mod_cmd_init(void)\n{\n\treturn register_ftrace_command(&ftrace_mod_cmd);\n}\ncore_initcall(ftrace_mod_cmd_init);\n\nstatic void function_trace_probe_call(unsigned long ip, unsigned long parent_ip,\n\t\t\t\t      struct ftrace_ops *op, struct ftrace_regs *fregs)\n{\n\tstruct ftrace_probe_ops *probe_ops;\n\tstruct ftrace_func_probe *probe;\n\n\tprobe = container_of(op, struct ftrace_func_probe, ops);\n\tprobe_ops = probe->probe_ops;\n\n\t \n\tpreempt_disable_notrace();\n\tprobe_ops->func(ip, parent_ip, probe->tr, probe_ops, probe->data);\n\tpreempt_enable_notrace();\n}\n\nstruct ftrace_func_map {\n\tstruct ftrace_func_entry\tentry;\n\tvoid\t\t\t\t*data;\n};\n\nstruct ftrace_func_mapper {\n\tstruct ftrace_hash\t\thash;\n};\n\n \nstruct ftrace_func_mapper *allocate_ftrace_func_mapper(void)\n{\n\tstruct ftrace_hash *hash;\n\n\t \n\thash = alloc_ftrace_hash(FTRACE_HASH_DEFAULT_BITS);\n\treturn (struct ftrace_func_mapper *)hash;\n}\n\n \nvoid **ftrace_func_mapper_find_ip(struct ftrace_func_mapper *mapper,\n\t\t\t\t  unsigned long ip)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_func_map *map;\n\n\tentry = ftrace_lookup_ip(&mapper->hash, ip);\n\tif (!entry)\n\t\treturn NULL;\n\n\tmap = (struct ftrace_func_map *)entry;\n\treturn &map->data;\n}\n\n \nint ftrace_func_mapper_add_ip(struct ftrace_func_mapper *mapper,\n\t\t\t      unsigned long ip, void *data)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_func_map *map;\n\n\tentry = ftrace_lookup_ip(&mapper->hash, ip);\n\tif (entry)\n\t\treturn -EBUSY;\n\n\tmap = kmalloc(sizeof(*map), GFP_KERNEL);\n\tif (!map)\n\t\treturn -ENOMEM;\n\n\tmap->entry.ip = ip;\n\tmap->data = data;\n\n\t__add_hash_entry(&mapper->hash, &map->entry);\n\n\treturn 0;\n}\n\n \nvoid *ftrace_func_mapper_remove_ip(struct ftrace_func_mapper *mapper,\n\t\t\t\t   unsigned long ip)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_func_map *map;\n\tvoid *data;\n\n\tentry = ftrace_lookup_ip(&mapper->hash, ip);\n\tif (!entry)\n\t\treturn NULL;\n\n\tmap = (struct ftrace_func_map *)entry;\n\tdata = map->data;\n\n\tremove_hash_entry(&mapper->hash, entry);\n\tkfree(entry);\n\n\treturn data;\n}\n\n \nvoid free_ftrace_func_mapper(struct ftrace_func_mapper *mapper,\n\t\t\t     ftrace_mapper_func free_func)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_func_map *map;\n\tstruct hlist_head *hhd;\n\tint size, i;\n\n\tif (!mapper)\n\t\treturn;\n\n\tif (free_func && mapper->hash.count) {\n\t\tsize = 1 << mapper->hash.size_bits;\n\t\tfor (i = 0; i < size; i++) {\n\t\t\thhd = &mapper->hash.buckets[i];\n\t\t\thlist_for_each_entry(entry, hhd, hlist) {\n\t\t\t\tmap = (struct ftrace_func_map *)entry;\n\t\t\t\tfree_func(map);\n\t\t\t}\n\t\t}\n\t}\n\tfree_ftrace_hash(&mapper->hash);\n}\n\nstatic void release_probe(struct ftrace_func_probe *probe)\n{\n\tstruct ftrace_probe_ops *probe_ops;\n\n\tmutex_lock(&ftrace_lock);\n\n\tWARN_ON(probe->ref <= 0);\n\n\t \n\tprobe->ref--;\n\n\tif (!probe->ref) {\n\t\tprobe_ops = probe->probe_ops;\n\t\t \n\t\tif (probe_ops->free)\n\t\t\tprobe_ops->free(probe_ops, probe->tr, 0, probe->data);\n\t\tlist_del(&probe->list);\n\t\tkfree(probe);\n\t}\n\tmutex_unlock(&ftrace_lock);\n}\n\nstatic void acquire_probe_locked(struct ftrace_func_probe *probe)\n{\n\t \n\tprobe->ref++;\n}\n\nint\nregister_ftrace_function_probe(char *glob, struct trace_array *tr,\n\t\t\t       struct ftrace_probe_ops *probe_ops,\n\t\t\t       void *data)\n{\n\tstruct ftrace_func_probe *probe = NULL, *iter;\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_hash **orig_hash;\n\tstruct ftrace_hash *old_hash;\n\tstruct ftrace_hash *hash;\n\tint count = 0;\n\tint size;\n\tint ret;\n\tint i;\n\n\tif (WARN_ON(!tr))\n\t\treturn -EINVAL;\n\n\t \n\tif (WARN_ON(glob[0] == '!'))\n\t\treturn -EINVAL;\n\n\n\tmutex_lock(&ftrace_lock);\n\t \n\tlist_for_each_entry(iter, &tr->func_probes, list) {\n\t\tif (iter->probe_ops == probe_ops) {\n\t\t\tprobe = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!probe) {\n\t\tprobe = kzalloc(sizeof(*probe), GFP_KERNEL);\n\t\tif (!probe) {\n\t\t\tmutex_unlock(&ftrace_lock);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tprobe->probe_ops = probe_ops;\n\t\tprobe->ops.func = function_trace_probe_call;\n\t\tprobe->tr = tr;\n\t\tftrace_ops_init(&probe->ops);\n\t\tlist_add(&probe->list, &tr->func_probes);\n\t}\n\n\tacquire_probe_locked(probe);\n\n\tmutex_unlock(&ftrace_lock);\n\n\t \n\tmutex_lock(&probe->ops.func_hash->regex_lock);\n\n\torig_hash = &probe->ops.func_hash->filter_hash;\n\told_hash = *orig_hash;\n\thash = alloc_and_copy_ftrace_hash(FTRACE_HASH_DEFAULT_BITS, old_hash);\n\n\tif (!hash) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = ftrace_match_records(hash, glob, strlen(glob));\n\n\t \n\tif (!ret)\n\t\tret = -EINVAL;\n\n\tif (ret < 0)\n\t\tgoto out;\n\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tif (ftrace_lookup_ip(old_hash, entry->ip))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (probe_ops->init) {\n\t\t\t\tret = probe_ops->init(probe_ops, tr,\n\t\t\t\t\t\t      entry->ip, data,\n\t\t\t\t\t\t      &probe->data);\n\t\t\t\tif (ret < 0) {\n\t\t\t\t\tif (probe_ops->free && count)\n\t\t\t\t\t\tprobe_ops->free(probe_ops, tr,\n\t\t\t\t\t\t\t\t0, probe->data);\n\t\t\t\t\tprobe->data = NULL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcount++;\n\t\t}\n\t}\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (!count) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tret = ftrace_hash_move_and_update_ops(&probe->ops, orig_hash,\n\t\t\t\t\t      hash, 1);\n\tif (ret < 0)\n\t\tgoto err_unlock;\n\n\t \n\tprobe->ref += count;\n\n\tif (!(probe->ops.flags & FTRACE_OPS_FL_ENABLED))\n\t\tret = ftrace_startup(&probe->ops, 0);\n\n out_unlock:\n\tmutex_unlock(&ftrace_lock);\n\n\tif (!ret)\n\t\tret = count;\n out:\n\tmutex_unlock(&probe->ops.func_hash->regex_lock);\n\tfree_ftrace_hash(hash);\n\n\trelease_probe(probe);\n\n\treturn ret;\n\n err_unlock:\n\tif (!probe_ops->free || !count)\n\t\tgoto out_unlock;\n\n\t \n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tif (ftrace_lookup_ip(old_hash, entry->ip))\n\t\t\t\tcontinue;\n\t\t\tprobe_ops->free(probe_ops, tr, entry->ip, probe->data);\n\t\t}\n\t}\n\tgoto out_unlock;\n}\n\nint\nunregister_ftrace_function_probe_func(char *glob, struct trace_array *tr,\n\t\t\t\t      struct ftrace_probe_ops *probe_ops)\n{\n\tstruct ftrace_func_probe *probe = NULL, *iter;\n\tstruct ftrace_ops_hash old_hash_ops;\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_glob func_g;\n\tstruct ftrace_hash **orig_hash;\n\tstruct ftrace_hash *old_hash;\n\tstruct ftrace_hash *hash = NULL;\n\tstruct hlist_node *tmp;\n\tstruct hlist_head hhd;\n\tchar str[KSYM_SYMBOL_LEN];\n\tint count = 0;\n\tint i, ret = -ENODEV;\n\tint size;\n\n\tif (!glob || !strlen(glob) || !strcmp(glob, \"*\"))\n\t\tfunc_g.search = NULL;\n\telse {\n\t\tint not;\n\n\t\tfunc_g.type = filter_parse_regex(glob, strlen(glob),\n\t\t\t\t\t\t &func_g.search, &not);\n\t\tfunc_g.len = strlen(func_g.search);\n\n\t\t \n\t\tif (WARN_ON(not))\n\t\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&ftrace_lock);\n\t \n\tlist_for_each_entry(iter, &tr->func_probes, list) {\n\t\tif (iter->probe_ops == probe_ops) {\n\t\t\tprobe = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!probe)\n\t\tgoto err_unlock_ftrace;\n\n\tret = -EINVAL;\n\tif (!(probe->ops.flags & FTRACE_OPS_FL_INITIALIZED))\n\t\tgoto err_unlock_ftrace;\n\n\tacquire_probe_locked(probe);\n\n\tmutex_unlock(&ftrace_lock);\n\n\tmutex_lock(&probe->ops.func_hash->regex_lock);\n\n\torig_hash = &probe->ops.func_hash->filter_hash;\n\told_hash = *orig_hash;\n\n\tif (ftrace_hash_empty(old_hash))\n\t\tgoto out_unlock;\n\n\told_hash_ops.filter_hash = old_hash;\n\t \n\told_hash_ops.notrace_hash = NULL;\n\n\tret = -ENOMEM;\n\thash = alloc_and_copy_ftrace_hash(FTRACE_HASH_DEFAULT_BITS, old_hash);\n\tif (!hash)\n\t\tgoto out_unlock;\n\n\tINIT_HLIST_HEAD(&hhd);\n\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry_safe(entry, tmp, &hash->buckets[i], hlist) {\n\n\t\t\tif (func_g.search) {\n\t\t\t\tkallsyms_lookup(entry->ip, NULL, NULL,\n\t\t\t\t\t\tNULL, str);\n\t\t\t\tif (!ftrace_match(str, &func_g))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tcount++;\n\t\t\tremove_hash_entry(hash, entry);\n\t\t\thlist_add_head(&entry->hlist, &hhd);\n\t\t}\n\t}\n\n\t \n\tif (!count) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tmutex_lock(&ftrace_lock);\n\n\tWARN_ON(probe->ref < count);\n\n\tprobe->ref -= count;\n\n\tif (ftrace_hash_empty(hash))\n\t\tftrace_shutdown(&probe->ops, 0);\n\n\tret = ftrace_hash_move_and_update_ops(&probe->ops, orig_hash,\n\t\t\t\t\t      hash, 1);\n\n\t \n\tif (ftrace_enabled && !ftrace_hash_empty(hash))\n\t\tftrace_run_modify_code(&probe->ops, FTRACE_UPDATE_CALLS,\n\t\t\t\t       &old_hash_ops);\n\tsynchronize_rcu();\n\n\thlist_for_each_entry_safe(entry, tmp, &hhd, hlist) {\n\t\thlist_del(&entry->hlist);\n\t\tif (probe_ops->free)\n\t\t\tprobe_ops->free(probe_ops, tr, entry->ip, probe->data);\n\t\tkfree(entry);\n\t}\n\tmutex_unlock(&ftrace_lock);\n\n out_unlock:\n\tmutex_unlock(&probe->ops.func_hash->regex_lock);\n\tfree_ftrace_hash(hash);\n\n\trelease_probe(probe);\n\n\treturn ret;\n\n err_unlock_ftrace:\n\tmutex_unlock(&ftrace_lock);\n\treturn ret;\n}\n\nvoid clear_ftrace_function_probes(struct trace_array *tr)\n{\n\tstruct ftrace_func_probe *probe, *n;\n\n\tlist_for_each_entry_safe(probe, n, &tr->func_probes, list)\n\t\tunregister_ftrace_function_probe_func(NULL, tr, probe->probe_ops);\n}\n\nstatic LIST_HEAD(ftrace_commands);\nstatic DEFINE_MUTEX(ftrace_cmd_mutex);\n\n \n__init int register_ftrace_command(struct ftrace_func_command *cmd)\n{\n\tstruct ftrace_func_command *p;\n\tint ret = 0;\n\n\tmutex_lock(&ftrace_cmd_mutex);\n\tlist_for_each_entry(p, &ftrace_commands, list) {\n\t\tif (strcmp(cmd->name, p->name) == 0) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\tlist_add(&cmd->list, &ftrace_commands);\n out_unlock:\n\tmutex_unlock(&ftrace_cmd_mutex);\n\n\treturn ret;\n}\n\n \n__init int unregister_ftrace_command(struct ftrace_func_command *cmd)\n{\n\tstruct ftrace_func_command *p, *n;\n\tint ret = -ENODEV;\n\n\tmutex_lock(&ftrace_cmd_mutex);\n\tlist_for_each_entry_safe(p, n, &ftrace_commands, list) {\n\t\tif (strcmp(cmd->name, p->name) == 0) {\n\t\t\tret = 0;\n\t\t\tlist_del_init(&p->list);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n out_unlock:\n\tmutex_unlock(&ftrace_cmd_mutex);\n\n\treturn ret;\n}\n\nstatic int ftrace_process_regex(struct ftrace_iterator *iter,\n\t\t\t\tchar *buff, int len, int enable)\n{\n\tstruct ftrace_hash *hash = iter->hash;\n\tstruct trace_array *tr = iter->ops->private;\n\tchar *func, *command, *next = buff;\n\tstruct ftrace_func_command *p;\n\tint ret = -EINVAL;\n\n\tfunc = strsep(&next, \":\");\n\n\tif (!next) {\n\t\tret = ftrace_match_records(hash, func, len);\n\t\tif (!ret)\n\t\t\tret = -EINVAL;\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\treturn 0;\n\t}\n\n\t \n\n\tcommand = strsep(&next, \":\");\n\n\tmutex_lock(&ftrace_cmd_mutex);\n\tlist_for_each_entry(p, &ftrace_commands, list) {\n\t\tif (strcmp(p->name, command) == 0) {\n\t\t\tret = p->func(tr, hash, func, command, next, enable);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n out_unlock:\n\tmutex_unlock(&ftrace_cmd_mutex);\n\n\treturn ret;\n}\n\nstatic ssize_t\nftrace_regex_write(struct file *file, const char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos, int enable)\n{\n\tstruct ftrace_iterator *iter;\n\tstruct trace_parser *parser;\n\tssize_t ret, read;\n\n\tif (!cnt)\n\t\treturn 0;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\tstruct seq_file *m = file->private_data;\n\t\titer = m->private;\n\t} else\n\t\titer = file->private_data;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\t \n\n\tparser = &iter->parser;\n\tread = trace_get_user(parser, ubuf, cnt, ppos);\n\n\tif (read >= 0 && trace_parser_loaded(parser) &&\n\t    !trace_parser_cont(parser)) {\n\t\tret = ftrace_process_regex(iter, parser->buffer,\n\t\t\t\t\t   parser->idx, enable);\n\t\ttrace_parser_clear(parser);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\tret = read;\n out:\n\treturn ret;\n}\n\nssize_t\nftrace_filter_write(struct file *file, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn ftrace_regex_write(file, ubuf, cnt, ppos, 1);\n}\n\nssize_t\nftrace_notrace_write(struct file *file, const char __user *ubuf,\n\t\t     size_t cnt, loff_t *ppos)\n{\n\treturn ftrace_regex_write(file, ubuf, cnt, ppos, 0);\n}\n\nstatic int\n__ftrace_match_addr(struct ftrace_hash *hash, unsigned long ip, int remove)\n{\n\tstruct ftrace_func_entry *entry;\n\n\tip = ftrace_location(ip);\n\tif (!ip)\n\t\treturn -EINVAL;\n\n\tif (remove) {\n\t\tentry = ftrace_lookup_ip(hash, ip);\n\t\tif (!entry)\n\t\t\treturn -ENOENT;\n\t\tfree_hash_entry(hash, entry);\n\t\treturn 0;\n\t}\n\n\tentry = add_hash_entry(hash, ip);\n\treturn entry ? 0 :  -ENOMEM;\n}\n\nstatic int\nftrace_match_addr(struct ftrace_hash *hash, unsigned long *ips,\n\t\t  unsigned int cnt, int remove)\n{\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\terr = __ftrace_match_addr(hash, ips[i], remove);\n\t\tif (err) {\n\t\t\t \n\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int\nftrace_set_hash(struct ftrace_ops *ops, unsigned char *buf, int len,\n\t\tunsigned long *ips, unsigned int cnt,\n\t\tint remove, int reset, int enable)\n{\n\tstruct ftrace_hash **orig_hash;\n\tstruct ftrace_hash *hash;\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tmutex_lock(&ops->func_hash->regex_lock);\n\n\tif (enable)\n\t\torig_hash = &ops->func_hash->filter_hash;\n\telse\n\t\torig_hash = &ops->func_hash->notrace_hash;\n\n\tif (reset)\n\t\thash = alloc_ftrace_hash(FTRACE_HASH_DEFAULT_BITS);\n\telse\n\t\thash = alloc_and_copy_ftrace_hash(FTRACE_HASH_DEFAULT_BITS, *orig_hash);\n\n\tif (!hash) {\n\t\tret = -ENOMEM;\n\t\tgoto out_regex_unlock;\n\t}\n\n\tif (buf && !ftrace_match_records(hash, buf, len)) {\n\t\tret = -EINVAL;\n\t\tgoto out_regex_unlock;\n\t}\n\tif (ips) {\n\t\tret = ftrace_match_addr(hash, ips, cnt, remove);\n\t\tif (ret < 0)\n\t\t\tgoto out_regex_unlock;\n\t}\n\n\tmutex_lock(&ftrace_lock);\n\tret = ftrace_hash_move_and_update_ops(ops, orig_hash, hash, enable);\n\tmutex_unlock(&ftrace_lock);\n\n out_regex_unlock:\n\tmutex_unlock(&ops->func_hash->regex_lock);\n\n\tfree_ftrace_hash(hash);\n\treturn ret;\n}\n\nstatic int\nftrace_set_addr(struct ftrace_ops *ops, unsigned long *ips, unsigned int cnt,\n\t\tint remove, int reset, int enable)\n{\n\treturn ftrace_set_hash(ops, NULL, 0, ips, cnt, remove, reset, enable);\n}\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\n\nstruct ftrace_direct_func {\n\tstruct list_head\tnext;\n\tunsigned long\t\taddr;\n\tint\t\t\tcount;\n};\n\nstatic LIST_HEAD(ftrace_direct_funcs);\n\nstatic int register_ftrace_function_nolock(struct ftrace_ops *ops);\n\n#define MULTI_FLAGS (FTRACE_OPS_FL_DIRECT | FTRACE_OPS_FL_SAVE_ARGS)\n\nstatic int check_direct_multi(struct ftrace_ops *ops)\n{\n\tif (!(ops->flags & FTRACE_OPS_FL_INITIALIZED))\n\t\treturn -EINVAL;\n\tif ((ops->flags & MULTI_FLAGS) != MULTI_FLAGS)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic void remove_direct_functions_hash(struct ftrace_hash *hash, unsigned long addr)\n{\n\tstruct ftrace_func_entry *entry, *del;\n\tint size, i;\n\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tdel = __ftrace_lookup_ip(direct_functions, entry->ip);\n\t\t\tif (del && del->direct == addr) {\n\t\t\t\tremove_hash_entry(direct_functions, del);\n\t\t\t\tkfree(del);\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nint register_ftrace_direct(struct ftrace_ops *ops, unsigned long addr)\n{\n\tstruct ftrace_hash *hash, *new_hash = NULL, *free_hash = NULL;\n\tstruct ftrace_func_entry *entry, *new;\n\tint err = -EBUSY, size, i;\n\n\tif (ops->func || ops->trampoline)\n\t\treturn -EINVAL;\n\tif (!(ops->flags & FTRACE_OPS_FL_INITIALIZED))\n\t\treturn -EINVAL;\n\tif (ops->flags & FTRACE_OPS_FL_ENABLED)\n\t\treturn -EINVAL;\n\n\thash = ops->func_hash->filter_hash;\n\tif (ftrace_hash_empty(hash))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&direct_mutex);\n\n\t \n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tif (ftrace_find_rec_direct(entry->ip))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\t \n\tsize = hash->count + direct_functions->count;\n\tif (size > 32)\n\t\tsize = 32;\n\tnew_hash = alloc_ftrace_hash(fls(size));\n\tif (!new_hash)\n\t\tgoto out_unlock;\n\n\t \n\tsize = 1 << direct_functions->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &direct_functions->buckets[i], hlist) {\n\t\t\tnew = add_hash_entry(new_hash, entry->ip);\n\t\t\tif (!new)\n\t\t\t\tgoto out_unlock;\n\t\t\tnew->direct = entry->direct;\n\t\t}\n\t}\n\n\t \n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tnew = add_hash_entry(new_hash, entry->ip);\n\t\t\tif (!new)\n\t\t\t\tgoto out_unlock;\n\t\t\t \n\t\t\tnew->direct = addr;\n\t\t\tentry->direct = addr;\n\t\t}\n\t}\n\n\tfree_hash = direct_functions;\n\trcu_assign_pointer(direct_functions, new_hash);\n\tnew_hash = NULL;\n\n\tops->func = call_direct_funcs;\n\tops->flags = MULTI_FLAGS;\n\tops->trampoline = FTRACE_REGS_ADDR;\n\tops->direct_call = addr;\n\n\terr = register_ftrace_function_nolock(ops);\n\n out_unlock:\n\tmutex_unlock(&direct_mutex);\n\n\tif (free_hash && free_hash != EMPTY_HASH) {\n\t\tsynchronize_rcu_tasks();\n\t\tfree_ftrace_hash(free_hash);\n\t}\n\n\tif (new_hash)\n\t\tfree_ftrace_hash(new_hash);\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_direct);\n\n \nint unregister_ftrace_direct(struct ftrace_ops *ops, unsigned long addr,\n\t\t\t     bool free_filters)\n{\n\tstruct ftrace_hash *hash = ops->func_hash->filter_hash;\n\tint err;\n\n\tif (check_direct_multi(ops))\n\t\treturn -EINVAL;\n\tif (!(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&direct_mutex);\n\terr = unregister_ftrace_function(ops);\n\tremove_direct_functions_hash(hash, addr);\n\tmutex_unlock(&direct_mutex);\n\n\t \n\tops->func = NULL;\n\tops->trampoline = 0;\n\n\tif (free_filters)\n\t\tftrace_free_filter(ops);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_direct);\n\nstatic int\n__modify_ftrace_direct(struct ftrace_ops *ops, unsigned long addr)\n{\n\tstruct ftrace_hash *hash;\n\tstruct ftrace_func_entry *entry, *iter;\n\tstatic struct ftrace_ops tmp_ops = {\n\t\t.func\t\t= ftrace_stub,\n\t\t.flags\t\t= FTRACE_OPS_FL_STUB,\n\t};\n\tint i, size;\n\tint err;\n\n\tlockdep_assert_held_once(&direct_mutex);\n\n\t \n\tftrace_ops_init(&tmp_ops);\n\ttmp_ops.func_hash = ops->func_hash;\n\ttmp_ops.direct_call = addr;\n\n\terr = register_ftrace_function_nolock(&tmp_ops);\n\tif (err)\n\t\treturn err;\n\n\t \n\tmutex_lock(&ftrace_lock);\n\n\thash = ops->func_hash->filter_hash;\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(iter, &hash->buckets[i], hlist) {\n\t\t\tentry = __ftrace_lookup_ip(direct_functions, iter->ip);\n\t\t\tif (!entry)\n\t\t\t\tcontinue;\n\t\t\tentry->direct = addr;\n\t\t}\n\t}\n\t \n\tWRITE_ONCE(ops->direct_call, addr);\n\n\tmutex_unlock(&ftrace_lock);\n\n\t \n\tunregister_ftrace_function(&tmp_ops);\n\n\treturn err;\n}\n\n \nint modify_ftrace_direct_nolock(struct ftrace_ops *ops, unsigned long addr)\n{\n\tif (check_direct_multi(ops))\n\t\treturn -EINVAL;\n\tif (!(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn -EINVAL;\n\n\treturn __modify_ftrace_direct(ops, addr);\n}\nEXPORT_SYMBOL_GPL(modify_ftrace_direct_nolock);\n\n \nint modify_ftrace_direct(struct ftrace_ops *ops, unsigned long addr)\n{\n\tint err;\n\n\tif (check_direct_multi(ops))\n\t\treturn -EINVAL;\n\tif (!(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&direct_mutex);\n\terr = __modify_ftrace_direct(ops, addr);\n\tmutex_unlock(&direct_mutex);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(modify_ftrace_direct);\n#endif  \n\n \nint ftrace_set_filter_ip(struct ftrace_ops *ops, unsigned long ip,\n\t\t\t int remove, int reset)\n{\n\tftrace_ops_init(ops);\n\treturn ftrace_set_addr(ops, &ip, 1, remove, reset, 1);\n}\nEXPORT_SYMBOL_GPL(ftrace_set_filter_ip);\n\n \nint ftrace_set_filter_ips(struct ftrace_ops *ops, unsigned long *ips,\n\t\t\t  unsigned int cnt, int remove, int reset)\n{\n\tftrace_ops_init(ops);\n\treturn ftrace_set_addr(ops, ips, cnt, remove, reset, 1);\n}\nEXPORT_SYMBOL_GPL(ftrace_set_filter_ips);\n\n \nvoid ftrace_ops_set_global_filter(struct ftrace_ops *ops)\n{\n\tif (ops->flags & FTRACE_OPS_FL_INITIALIZED)\n\t\treturn;\n\n\tftrace_ops_init(ops);\n\tops->func_hash = &global_ops.local_hash;\n}\nEXPORT_SYMBOL_GPL(ftrace_ops_set_global_filter);\n\nstatic int\nftrace_set_regex(struct ftrace_ops *ops, unsigned char *buf, int len,\n\t\t int reset, int enable)\n{\n\treturn ftrace_set_hash(ops, buf, len, NULL, 0, 0, reset, enable);\n}\n\n \nint ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,\n\t\t       int len, int reset)\n{\n\tftrace_ops_init(ops);\n\treturn ftrace_set_regex(ops, buf, len, reset, 1);\n}\nEXPORT_SYMBOL_GPL(ftrace_set_filter);\n\n \nint ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,\n\t\t\tint len, int reset)\n{\n\tftrace_ops_init(ops);\n\treturn ftrace_set_regex(ops, buf, len, reset, 0);\n}\nEXPORT_SYMBOL_GPL(ftrace_set_notrace);\n \nvoid ftrace_set_global_filter(unsigned char *buf, int len, int reset)\n{\n\tftrace_set_regex(&global_ops, buf, len, reset, 1);\n}\nEXPORT_SYMBOL_GPL(ftrace_set_global_filter);\n\n \nvoid ftrace_set_global_notrace(unsigned char *buf, int len, int reset)\n{\n\tftrace_set_regex(&global_ops, buf, len, reset, 0);\n}\nEXPORT_SYMBOL_GPL(ftrace_set_global_notrace);\n\n \n#define FTRACE_FILTER_SIZE\t\tCOMMAND_LINE_SIZE\nstatic char ftrace_notrace_buf[FTRACE_FILTER_SIZE] __initdata;\nstatic char ftrace_filter_buf[FTRACE_FILTER_SIZE] __initdata;\n\n \nbool ftrace_filter_param __initdata;\n\nstatic int __init set_ftrace_notrace(char *str)\n{\n\tftrace_filter_param = true;\n\tstrscpy(ftrace_notrace_buf, str, FTRACE_FILTER_SIZE);\n\treturn 1;\n}\n__setup(\"ftrace_notrace=\", set_ftrace_notrace);\n\nstatic int __init set_ftrace_filter(char *str)\n{\n\tftrace_filter_param = true;\n\tstrscpy(ftrace_filter_buf, str, FTRACE_FILTER_SIZE);\n\treturn 1;\n}\n__setup(\"ftrace_filter=\", set_ftrace_filter);\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\nstatic char ftrace_graph_buf[FTRACE_FILTER_SIZE] __initdata;\nstatic char ftrace_graph_notrace_buf[FTRACE_FILTER_SIZE] __initdata;\nstatic int ftrace_graph_set_hash(struct ftrace_hash *hash, char *buffer);\n\nstatic int __init set_graph_function(char *str)\n{\n\tstrscpy(ftrace_graph_buf, str, FTRACE_FILTER_SIZE);\n\treturn 1;\n}\n__setup(\"ftrace_graph_filter=\", set_graph_function);\n\nstatic int __init set_graph_notrace_function(char *str)\n{\n\tstrscpy(ftrace_graph_notrace_buf, str, FTRACE_FILTER_SIZE);\n\treturn 1;\n}\n__setup(\"ftrace_graph_notrace=\", set_graph_notrace_function);\n\nstatic int __init set_graph_max_depth_function(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tfgraph_max_depth = simple_strtoul(str, NULL, 0);\n\treturn 1;\n}\n__setup(\"ftrace_graph_max_depth=\", set_graph_max_depth_function);\n\nstatic void __init set_ftrace_early_graph(char *buf, int enable)\n{\n\tint ret;\n\tchar *func;\n\tstruct ftrace_hash *hash;\n\n\thash = alloc_ftrace_hash(FTRACE_HASH_DEFAULT_BITS);\n\tif (MEM_FAIL(!hash, \"Failed to allocate hash\\n\"))\n\t\treturn;\n\n\twhile (buf) {\n\t\tfunc = strsep(&buf, \",\");\n\t\t \n\t\tret = ftrace_graph_set_hash(hash, func);\n\t\tif (ret)\n\t\t\tprintk(KERN_DEBUG \"ftrace: function %s not \"\n\t\t\t\t\t  \"traceable\\n\", func);\n\t}\n\n\tif (enable)\n\t\tftrace_graph_hash = hash;\n\telse\n\t\tftrace_graph_notrace_hash = hash;\n}\n#endif  \n\nvoid __init\nftrace_set_early_filter(struct ftrace_ops *ops, char *buf, int enable)\n{\n\tchar *func;\n\n\tftrace_ops_init(ops);\n\n\twhile (buf) {\n\t\tfunc = strsep(&buf, \",\");\n\t\tftrace_set_regex(ops, func, strlen(func), 0, enable);\n\t}\n}\n\nstatic void __init set_ftrace_early_filters(void)\n{\n\tif (ftrace_filter_buf[0])\n\t\tftrace_set_early_filter(&global_ops, ftrace_filter_buf, 1);\n\tif (ftrace_notrace_buf[0])\n\t\tftrace_set_early_filter(&global_ops, ftrace_notrace_buf, 0);\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\tif (ftrace_graph_buf[0])\n\t\tset_ftrace_early_graph(ftrace_graph_buf, 1);\n\tif (ftrace_graph_notrace_buf[0])\n\t\tset_ftrace_early_graph(ftrace_graph_notrace_buf, 0);\n#endif  \n}\n\nint ftrace_regex_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *m = (struct seq_file *)file->private_data;\n\tstruct ftrace_iterator *iter;\n\tstruct ftrace_hash **orig_hash;\n\tstruct trace_parser *parser;\n\tint filter_hash;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\titer = m->private;\n\t\tseq_release(inode, file);\n\t} else\n\t\titer = file->private_data;\n\n\tparser = &iter->parser;\n\tif (trace_parser_loaded(parser)) {\n\t\tint enable = !(iter->flags & FTRACE_ITER_NOTRACE);\n\n\t\tftrace_process_regex(iter, parser->buffer,\n\t\t\t\t     parser->idx, enable);\n\t}\n\n\ttrace_parser_put(parser);\n\n\tmutex_lock(&iter->ops->func_hash->regex_lock);\n\n\tif (file->f_mode & FMODE_WRITE) {\n\t\tfilter_hash = !!(iter->flags & FTRACE_ITER_FILTER);\n\n\t\tif (filter_hash) {\n\t\t\torig_hash = &iter->ops->func_hash->filter_hash;\n\t\t\tif (iter->tr) {\n\t\t\t\tif (list_empty(&iter->tr->mod_trace))\n\t\t\t\t\titer->hash->flags &= ~FTRACE_HASH_FL_MOD;\n\t\t\t\telse\n\t\t\t\t\titer->hash->flags |= FTRACE_HASH_FL_MOD;\n\t\t\t}\n\t\t} else\n\t\t\torig_hash = &iter->ops->func_hash->notrace_hash;\n\n\t\tmutex_lock(&ftrace_lock);\n\t\tftrace_hash_move_and_update_ops(iter->ops, orig_hash,\n\t\t\t\t\t\t      iter->hash, filter_hash);\n\t\tmutex_unlock(&ftrace_lock);\n\t} else {\n\t\t \n\t\titer->hash = NULL;\n\t}\n\n\tmutex_unlock(&iter->ops->func_hash->regex_lock);\n\tfree_ftrace_hash(iter->hash);\n\tif (iter->tr)\n\t\ttrace_array_put(iter->tr);\n\tkfree(iter);\n\n\treturn 0;\n}\n\nstatic const struct file_operations ftrace_avail_fops = {\n\t.open = ftrace_avail_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release_private,\n};\n\nstatic const struct file_operations ftrace_enabled_fops = {\n\t.open = ftrace_enabled_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release_private,\n};\n\nstatic const struct file_operations ftrace_touched_fops = {\n\t.open = ftrace_touched_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release_private,\n};\n\nstatic const struct file_operations ftrace_avail_addrs_fops = {\n\t.open = ftrace_avail_addrs_open,\n\t.read = seq_read,\n\t.llseek = seq_lseek,\n\t.release = seq_release_private,\n};\n\nstatic const struct file_operations ftrace_filter_fops = {\n\t.open = ftrace_filter_open,\n\t.read = seq_read,\n\t.write = ftrace_filter_write,\n\t.llseek = tracing_lseek,\n\t.release = ftrace_regex_release,\n};\n\nstatic const struct file_operations ftrace_notrace_fops = {\n\t.open = ftrace_notrace_open,\n\t.read = seq_read,\n\t.write = ftrace_notrace_write,\n\t.llseek = tracing_lseek,\n\t.release = ftrace_regex_release,\n};\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\nstatic DEFINE_MUTEX(graph_lock);\n\nstruct ftrace_hash __rcu *ftrace_graph_hash = EMPTY_HASH;\nstruct ftrace_hash __rcu *ftrace_graph_notrace_hash = EMPTY_HASH;\n\nenum graph_filter_type {\n\tGRAPH_FILTER_NOTRACE\t= 0,\n\tGRAPH_FILTER_FUNCTION,\n};\n\n#define FTRACE_GRAPH_EMPTY\t((void *)1)\n\nstruct ftrace_graph_data {\n\tstruct ftrace_hash\t\t*hash;\n\tstruct ftrace_func_entry\t*entry;\n\tint\t\t\t\tidx;    \n\tenum graph_filter_type\t\ttype;\n\tstruct ftrace_hash\t\t*new_hash;\n\tconst struct seq_operations\t*seq_ops;\n\tstruct trace_parser\t\tparser;\n};\n\nstatic void *\n__g_next(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_graph_data *fgd = m->private;\n\tstruct ftrace_func_entry *entry = fgd->entry;\n\tstruct hlist_head *head;\n\tint i, idx = fgd->idx;\n\n\tif (*pos >= fgd->hash->count)\n\t\treturn NULL;\n\n\tif (entry) {\n\t\thlist_for_each_entry_continue(entry, hlist) {\n\t\t\tfgd->entry = entry;\n\t\t\treturn entry;\n\t\t}\n\n\t\tidx++;\n\t}\n\n\tfor (i = idx; i < 1 << fgd->hash->size_bits; i++) {\n\t\thead = &fgd->hash->buckets[i];\n\t\thlist_for_each_entry(entry, head, hlist) {\n\t\t\tfgd->entry = entry;\n\t\t\tfgd->idx = i;\n\t\t\treturn entry;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void *\ng_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\t(*pos)++;\n\treturn __g_next(m, pos);\n}\n\nstatic void *g_start(struct seq_file *m, loff_t *pos)\n{\n\tstruct ftrace_graph_data *fgd = m->private;\n\n\tmutex_lock(&graph_lock);\n\n\tif (fgd->type == GRAPH_FILTER_FUNCTION)\n\t\tfgd->hash = rcu_dereference_protected(ftrace_graph_hash,\n\t\t\t\t\tlockdep_is_held(&graph_lock));\n\telse\n\t\tfgd->hash = rcu_dereference_protected(ftrace_graph_notrace_hash,\n\t\t\t\t\tlockdep_is_held(&graph_lock));\n\n\t \n\tif (ftrace_hash_empty(fgd->hash) && !*pos)\n\t\treturn FTRACE_GRAPH_EMPTY;\n\n\tfgd->idx = 0;\n\tfgd->entry = NULL;\n\treturn __g_next(m, pos);\n}\n\nstatic void g_stop(struct seq_file *m, void *p)\n{\n\tmutex_unlock(&graph_lock);\n}\n\nstatic int g_show(struct seq_file *m, void *v)\n{\n\tstruct ftrace_func_entry *entry = v;\n\n\tif (!entry)\n\t\treturn 0;\n\n\tif (entry == FTRACE_GRAPH_EMPTY) {\n\t\tstruct ftrace_graph_data *fgd = m->private;\n\n\t\tif (fgd->type == GRAPH_FILTER_FUNCTION)\n\t\t\tseq_puts(m, \"#### all functions enabled ####\\n\");\n\t\telse\n\t\t\tseq_puts(m, \"#### no functions disabled ####\\n\");\n\t\treturn 0;\n\t}\n\n\tseq_printf(m, \"%ps\\n\", (void *)entry->ip);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations ftrace_graph_seq_ops = {\n\t.start = g_start,\n\t.next = g_next,\n\t.stop = g_stop,\n\t.show = g_show,\n};\n\nstatic int\n__ftrace_graph_open(struct inode *inode, struct file *file,\n\t\t    struct ftrace_graph_data *fgd)\n{\n\tint ret;\n\tstruct ftrace_hash *new_hash = NULL;\n\n\tret = security_locked_down(LOCKDOWN_TRACEFS);\n\tif (ret)\n\t\treturn ret;\n\n\tif (file->f_mode & FMODE_WRITE) {\n\t\tconst int size_bits = FTRACE_HASH_DEFAULT_BITS;\n\n\t\tif (trace_parser_get_init(&fgd->parser, FTRACE_BUFF_MAX))\n\t\t\treturn -ENOMEM;\n\n\t\tif (file->f_flags & O_TRUNC)\n\t\t\tnew_hash = alloc_ftrace_hash(size_bits);\n\t\telse\n\t\t\tnew_hash = alloc_and_copy_ftrace_hash(size_bits,\n\t\t\t\t\t\t\t      fgd->hash);\n\t\tif (!new_hash) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (file->f_mode & FMODE_READ) {\n\t\tret = seq_open(file, &ftrace_graph_seq_ops);\n\t\tif (!ret) {\n\t\t\tstruct seq_file *m = file->private_data;\n\t\t\tm->private = fgd;\n\t\t} else {\n\t\t\t \n\t\t\tfree_ftrace_hash(new_hash);\n\t\t\tnew_hash = NULL;\n\t\t}\n\t} else\n\t\tfile->private_data = fgd;\n\nout:\n\tif (ret < 0 && file->f_mode & FMODE_WRITE)\n\t\ttrace_parser_put(&fgd->parser);\n\n\tfgd->new_hash = new_hash;\n\n\t \n\tfgd->hash = NULL;\n\n\treturn ret;\n}\n\nstatic int\nftrace_graph_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_graph_data *fgd;\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tfgd = kmalloc(sizeof(*fgd), GFP_KERNEL);\n\tif (fgd == NULL)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&graph_lock);\n\n\tfgd->hash = rcu_dereference_protected(ftrace_graph_hash,\n\t\t\t\t\tlockdep_is_held(&graph_lock));\n\tfgd->type = GRAPH_FILTER_FUNCTION;\n\tfgd->seq_ops = &ftrace_graph_seq_ops;\n\n\tret = __ftrace_graph_open(inode, file, fgd);\n\tif (ret < 0)\n\t\tkfree(fgd);\n\n\tmutex_unlock(&graph_lock);\n\treturn ret;\n}\n\nstatic int\nftrace_graph_notrace_open(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_graph_data *fgd;\n\tint ret;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn -ENODEV;\n\n\tfgd = kmalloc(sizeof(*fgd), GFP_KERNEL);\n\tif (fgd == NULL)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&graph_lock);\n\n\tfgd->hash = rcu_dereference_protected(ftrace_graph_notrace_hash,\n\t\t\t\t\tlockdep_is_held(&graph_lock));\n\tfgd->type = GRAPH_FILTER_NOTRACE;\n\tfgd->seq_ops = &ftrace_graph_seq_ops;\n\n\tret = __ftrace_graph_open(inode, file, fgd);\n\tif (ret < 0)\n\t\tkfree(fgd);\n\n\tmutex_unlock(&graph_lock);\n\treturn ret;\n}\n\nstatic int\nftrace_graph_release(struct inode *inode, struct file *file)\n{\n\tstruct ftrace_graph_data *fgd;\n\tstruct ftrace_hash *old_hash, *new_hash;\n\tstruct trace_parser *parser;\n\tint ret = 0;\n\n\tif (file->f_mode & FMODE_READ) {\n\t\tstruct seq_file *m = file->private_data;\n\n\t\tfgd = m->private;\n\t\tseq_release(inode, file);\n\t} else {\n\t\tfgd = file->private_data;\n\t}\n\n\n\tif (file->f_mode & FMODE_WRITE) {\n\n\t\tparser = &fgd->parser;\n\n\t\tif (trace_parser_loaded((parser))) {\n\t\t\tret = ftrace_graph_set_hash(fgd->new_hash,\n\t\t\t\t\t\t    parser->buffer);\n\t\t}\n\n\t\ttrace_parser_put(parser);\n\n\t\tnew_hash = __ftrace_hash_move(fgd->new_hash);\n\t\tif (!new_hash) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmutex_lock(&graph_lock);\n\n\t\tif (fgd->type == GRAPH_FILTER_FUNCTION) {\n\t\t\told_hash = rcu_dereference_protected(ftrace_graph_hash,\n\t\t\t\t\tlockdep_is_held(&graph_lock));\n\t\t\trcu_assign_pointer(ftrace_graph_hash, new_hash);\n\t\t} else {\n\t\t\told_hash = rcu_dereference_protected(ftrace_graph_notrace_hash,\n\t\t\t\t\tlockdep_is_held(&graph_lock));\n\t\t\trcu_assign_pointer(ftrace_graph_notrace_hash, new_hash);\n\t\t}\n\n\t\tmutex_unlock(&graph_lock);\n\n\t\t \n\t\tif (old_hash != EMPTY_HASH)\n\t\t\tsynchronize_rcu_tasks_rude();\n\n\t\tfree_ftrace_hash(old_hash);\n\t}\n\n out:\n\tfree_ftrace_hash(fgd->new_hash);\n\tkfree(fgd);\n\n\treturn ret;\n}\n\nstatic int\nftrace_graph_set_hash(struct ftrace_hash *hash, char *buffer)\n{\n\tstruct ftrace_glob func_g;\n\tstruct dyn_ftrace *rec;\n\tstruct ftrace_page *pg;\n\tstruct ftrace_func_entry *entry;\n\tint fail = 1;\n\tint not;\n\n\t \n\tfunc_g.type = filter_parse_regex(buffer, strlen(buffer),\n\t\t\t\t\t &func_g.search, &not);\n\n\tfunc_g.len = strlen(func_g.search);\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(ftrace_disabled)) {\n\t\tmutex_unlock(&ftrace_lock);\n\t\treturn -ENODEV;\n\t}\n\n\tdo_for_each_ftrace_rec(pg, rec) {\n\n\t\tif (rec->flags & FTRACE_FL_DISABLED)\n\t\t\tcontinue;\n\n\t\tif (ftrace_match_record(rec, &func_g, NULL, 0)) {\n\t\t\tentry = ftrace_lookup_ip(hash, rec->ip);\n\n\t\t\tif (!not) {\n\t\t\t\tfail = 0;\n\n\t\t\t\tif (entry)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (add_hash_entry(hash, rec->ip) == NULL)\n\t\t\t\t\tgoto out;\n\t\t\t} else {\n\t\t\t\tif (entry) {\n\t\t\t\t\tfree_hash_entry(hash, entry);\n\t\t\t\t\tfail = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} while_for_each_ftrace_rec();\nout:\n\tmutex_unlock(&ftrace_lock);\n\n\tif (fail)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic ssize_t\nftrace_graph_write(struct file *file, const char __user *ubuf,\n\t\t   size_t cnt, loff_t *ppos)\n{\n\tssize_t read, ret = 0;\n\tstruct ftrace_graph_data *fgd = file->private_data;\n\tstruct trace_parser *parser;\n\n\tif (!cnt)\n\t\treturn 0;\n\n\t \n\tif (file->f_mode & FMODE_READ) {\n\t\tstruct seq_file *m = file->private_data;\n\t\tfgd = m->private;\n\t}\n\n\tparser = &fgd->parser;\n\n\tread = trace_get_user(parser, ubuf, cnt, ppos);\n\n\tif (read >= 0 && trace_parser_loaded(parser) &&\n\t    !trace_parser_cont(parser)) {\n\n\t\tret = ftrace_graph_set_hash(fgd->new_hash,\n\t\t\t\t\t    parser->buffer);\n\t\ttrace_parser_clear(parser);\n\t}\n\n\tif (!ret)\n\t\tret = read;\n\n\treturn ret;\n}\n\nstatic const struct file_operations ftrace_graph_fops = {\n\t.open\t\t= ftrace_graph_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= ftrace_graph_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= ftrace_graph_release,\n};\n\nstatic const struct file_operations ftrace_graph_notrace_fops = {\n\t.open\t\t= ftrace_graph_notrace_open,\n\t.read\t\t= seq_read,\n\t.write\t\t= ftrace_graph_write,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= ftrace_graph_release,\n};\n#endif  \n\nvoid ftrace_create_filter_files(struct ftrace_ops *ops,\n\t\t\t\tstruct dentry *parent)\n{\n\n\ttrace_create_file(\"set_ftrace_filter\", TRACE_MODE_WRITE, parent,\n\t\t\t  ops, &ftrace_filter_fops);\n\n\ttrace_create_file(\"set_ftrace_notrace\", TRACE_MODE_WRITE, parent,\n\t\t\t  ops, &ftrace_notrace_fops);\n}\n\n \nvoid ftrace_destroy_filter_files(struct ftrace_ops *ops)\n{\n\tmutex_lock(&ftrace_lock);\n\tif (ops->flags & FTRACE_OPS_FL_ENABLED)\n\t\tftrace_shutdown(ops, 0);\n\tops->flags |= FTRACE_OPS_FL_DELETED;\n\tftrace_free_filter(ops);\n\tmutex_unlock(&ftrace_lock);\n}\n\nstatic __init int ftrace_init_dyn_tracefs(struct dentry *d_tracer)\n{\n\n\ttrace_create_file(\"available_filter_functions\", TRACE_MODE_READ,\n\t\t\td_tracer, NULL, &ftrace_avail_fops);\n\n\ttrace_create_file(\"available_filter_functions_addrs\", TRACE_MODE_READ,\n\t\t\td_tracer, NULL, &ftrace_avail_addrs_fops);\n\n\ttrace_create_file(\"enabled_functions\", TRACE_MODE_READ,\n\t\t\td_tracer, NULL, &ftrace_enabled_fops);\n\n\ttrace_create_file(\"touched_functions\", TRACE_MODE_READ,\n\t\t\td_tracer, NULL, &ftrace_touched_fops);\n\n\tftrace_create_filter_files(&global_ops, d_tracer);\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\ttrace_create_file(\"set_graph_function\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t\t    NULL,\n\t\t\t\t    &ftrace_graph_fops);\n\ttrace_create_file(\"set_graph_notrace\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t\t    NULL,\n\t\t\t\t    &ftrace_graph_notrace_fops);\n#endif  \n\n\treturn 0;\n}\n\nstatic int ftrace_cmp_ips(const void *a, const void *b)\n{\n\tconst unsigned long *ipa = a;\n\tconst unsigned long *ipb = b;\n\n\tif (*ipa > *ipb)\n\t\treturn 1;\n\tif (*ipa < *ipb)\n\t\treturn -1;\n\treturn 0;\n}\n\n#ifdef CONFIG_FTRACE_SORT_STARTUP_TEST\nstatic void test_is_sorted(unsigned long *start, unsigned long count)\n{\n\tint i;\n\n\tfor (i = 1; i < count; i++) {\n\t\tif (WARN(start[i - 1] > start[i],\n\t\t\t \"[%d] %pS at %lx is not sorted with %pS at %lx\\n\", i,\n\t\t\t (void *)start[i - 1], start[i - 1],\n\t\t\t (void *)start[i], start[i]))\n\t\t\tbreak;\n\t}\n\tif (i == count)\n\t\tpr_info(\"ftrace section at %px sorted properly\\n\", start);\n}\n#else\nstatic void test_is_sorted(unsigned long *start, unsigned long count)\n{\n}\n#endif\n\nstatic int ftrace_process_locs(struct module *mod,\n\t\t\t       unsigned long *start,\n\t\t\t       unsigned long *end)\n{\n\tstruct ftrace_page *pg_unuse = NULL;\n\tstruct ftrace_page *start_pg;\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec;\n\tunsigned long skipped = 0;\n\tunsigned long count;\n\tunsigned long *p;\n\tunsigned long addr;\n\tunsigned long flags = 0;  \n\tint ret = -ENOMEM;\n\n\tcount = end - start;\n\n\tif (!count)\n\t\treturn 0;\n\n\t \n\tif (!IS_ENABLED(CONFIG_BUILDTIME_MCOUNT_SORT) || mod) {\n\t\tsort(start, count, sizeof(*start),\n\t\t     ftrace_cmp_ips, NULL);\n\t} else {\n\t\ttest_is_sorted(start, count);\n\t}\n\n\tstart_pg = ftrace_allocate_pages(count);\n\tif (!start_pg)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&ftrace_lock);\n\n\t \n\tif (!mod) {\n\t\tWARN_ON(ftrace_pages || ftrace_pages_start);\n\t\t \n\t\tftrace_pages = ftrace_pages_start = start_pg;\n\t} else {\n\t\tif (!ftrace_pages)\n\t\t\tgoto out;\n\n\t\tif (WARN_ON(ftrace_pages->next)) {\n\t\t\t \n\t\t\twhile (ftrace_pages->next)\n\t\t\t\tftrace_pages = ftrace_pages->next;\n\t\t}\n\n\t\tftrace_pages->next = start_pg;\n\t}\n\n\tp = start;\n\tpg = start_pg;\n\twhile (p < end) {\n\t\tunsigned long end_offset;\n\t\taddr = ftrace_call_adjust(*p++);\n\t\t \n\t\tif (!addr) {\n\t\t\tskipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tend_offset = (pg->index+1) * sizeof(pg->records[0]);\n\t\tif (end_offset > PAGE_SIZE << pg->order) {\n\t\t\t \n\t\t\tif (WARN_ON(!pg->next))\n\t\t\t\tbreak;\n\t\t\tpg = pg->next;\n\t\t}\n\n\t\trec = &pg->records[pg->index++];\n\t\trec->ip = addr;\n\t}\n\n\tif (pg->next) {\n\t\tpg_unuse = pg->next;\n\t\tpg->next = NULL;\n\t}\n\n\t \n\tftrace_pages = pg;\n\n\t \n\tif (!mod)\n\t\tlocal_irq_save(flags);\n\tftrace_update_code(mod, start_pg);\n\tif (!mod)\n\t\tlocal_irq_restore(flags);\n\tret = 0;\n out:\n\tmutex_unlock(&ftrace_lock);\n\n\t \n\tif (pg_unuse) {\n\t\tWARN_ON(!skipped);\n\t\tftrace_free_pages(pg_unuse);\n\t}\n\treturn ret;\n}\n\nstruct ftrace_mod_func {\n\tstruct list_head\tlist;\n\tchar\t\t\t*name;\n\tunsigned long\t\tip;\n\tunsigned int\t\tsize;\n};\n\nstruct ftrace_mod_map {\n\tstruct rcu_head\t\trcu;\n\tstruct list_head\tlist;\n\tstruct module\t\t*mod;\n\tunsigned long\t\tstart_addr;\n\tunsigned long\t\tend_addr;\n\tstruct list_head\tfuncs;\n\tunsigned int\t\tnum_funcs;\n};\n\nstatic int ftrace_get_trampoline_kallsym(unsigned int symnum,\n\t\t\t\t\t unsigned long *value, char *type,\n\t\t\t\t\t char *name, char *module_name,\n\t\t\t\t\t int *exported)\n{\n\tstruct ftrace_ops *op;\n\n\tlist_for_each_entry_rcu(op, &ftrace_ops_trampoline_list, list) {\n\t\tif (!op->trampoline || symnum--)\n\t\t\tcontinue;\n\t\t*value = op->trampoline;\n\t\t*type = 't';\n\t\tstrscpy(name, FTRACE_TRAMPOLINE_SYM, KSYM_NAME_LEN);\n\t\tstrscpy(module_name, FTRACE_TRAMPOLINE_MOD, MODULE_NAME_LEN);\n\t\t*exported = 0;\n\t\treturn 0;\n\t}\n\n\treturn -ERANGE;\n}\n\n#if defined(CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS) || defined(CONFIG_MODULES)\n \nstatic bool\nops_references_ip(struct ftrace_ops *ops, unsigned long ip)\n{\n\t \n\tif (!(ops->flags & FTRACE_OPS_FL_ENABLED))\n\t\treturn false;\n\n\t \n\tif (ops_traces_mod(ops))\n\t\treturn true;\n\n\t \n\tif (!ftrace_hash_empty(ops->func_hash->filter_hash) &&\n\t    !__ftrace_lookup_ip(ops->func_hash->filter_hash, ip))\n\t\treturn false;\n\n\t \n\tif (ftrace_lookup_ip(ops->func_hash->notrace_hash, ip))\n\t\treturn false;\n\n\treturn true;\n}\n#endif\n\n#ifdef CONFIG_MODULES\n\n#define next_to_ftrace_page(p) container_of(p, struct ftrace_page, next)\n\nstatic LIST_HEAD(ftrace_mod_maps);\n\nstatic int referenced_filters(struct dyn_ftrace *rec)\n{\n\tstruct ftrace_ops *ops;\n\tint cnt = 0;\n\n\tfor (ops = ftrace_ops_list; ops != &ftrace_list_end; ops = ops->next) {\n\t\tif (ops_references_ip(ops, rec->ip)) {\n\t\t\tif (WARN_ON_ONCE(ops->flags & FTRACE_OPS_FL_DIRECT))\n\t\t\t\tcontinue;\n\t\t\tif (WARN_ON_ONCE(ops->flags & FTRACE_OPS_FL_IPMODIFY))\n\t\t\t\tcontinue;\n\t\t\tcnt++;\n\t\t\tif (ops->flags & FTRACE_OPS_FL_SAVE_REGS)\n\t\t\t\trec->flags |= FTRACE_FL_REGS;\n\t\t\tif (cnt == 1 && ops->trampoline)\n\t\t\t\trec->flags |= FTRACE_FL_TRAMP;\n\t\t\telse\n\t\t\t\trec->flags &= ~FTRACE_FL_TRAMP;\n\t\t}\n\t}\n\n\treturn cnt;\n}\n\nstatic void\nclear_mod_from_hash(struct ftrace_page *pg, struct ftrace_hash *hash)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct dyn_ftrace *rec;\n\tint i;\n\n\tif (ftrace_hash_empty(hash))\n\t\treturn;\n\n\tfor (i = 0; i < pg->index; i++) {\n\t\trec = &pg->records[i];\n\t\tentry = __ftrace_lookup_ip(hash, rec->ip);\n\t\t \n\t\tif (entry)\n\t\t\tentry->ip = 0;\n\t}\n}\n\n \nstatic void clear_mod_from_hashes(struct ftrace_page *pg)\n{\n\tstruct trace_array *tr;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->ops || !tr->ops->func_hash)\n\t\t\tcontinue;\n\t\tmutex_lock(&tr->ops->func_hash->regex_lock);\n\t\tclear_mod_from_hash(pg, tr->ops->func_hash->filter_hash);\n\t\tclear_mod_from_hash(pg, tr->ops->func_hash->notrace_hash);\n\t\tmutex_unlock(&tr->ops->func_hash->regex_lock);\n\t}\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic void ftrace_free_mod_map(struct rcu_head *rcu)\n{\n\tstruct ftrace_mod_map *mod_map = container_of(rcu, struct ftrace_mod_map, rcu);\n\tstruct ftrace_mod_func *mod_func;\n\tstruct ftrace_mod_func *n;\n\n\t \n\tlist_for_each_entry_safe(mod_func, n, &mod_map->funcs, list) {\n\t\tkfree(mod_func->name);\n\t\tlist_del(&mod_func->list);\n\t\tkfree(mod_func);\n\t}\n\n\tkfree(mod_map);\n}\n\nvoid ftrace_release_mod(struct module *mod)\n{\n\tstruct ftrace_mod_map *mod_map;\n\tstruct ftrace_mod_map *n;\n\tstruct dyn_ftrace *rec;\n\tstruct ftrace_page **last_pg;\n\tstruct ftrace_page *tmp_page = NULL;\n\tstruct ftrace_page *pg;\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (ftrace_disabled)\n\t\tgoto out_unlock;\n\n\tlist_for_each_entry_safe(mod_map, n, &ftrace_mod_maps, list) {\n\t\tif (mod_map->mod == mod) {\n\t\t\tlist_del_rcu(&mod_map->list);\n\t\t\tcall_rcu(&mod_map->rcu, ftrace_free_mod_map);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tlast_pg = &ftrace_pages_start;\n\tfor (pg = ftrace_pages_start; pg; pg = *last_pg) {\n\t\trec = &pg->records[0];\n\t\tif (within_module(rec->ip, mod)) {\n\t\t\t \n\t\t\tif (WARN_ON(pg == ftrace_pages_start))\n\t\t\t\tgoto out_unlock;\n\n\t\t\t \n\t\t\tif (pg == ftrace_pages)\n\t\t\t\tftrace_pages = next_to_ftrace_page(last_pg);\n\n\t\t\tftrace_update_tot_cnt -= pg->index;\n\t\t\t*last_pg = pg->next;\n\n\t\t\tpg->next = tmp_page;\n\t\t\ttmp_page = pg;\n\t\t} else\n\t\t\tlast_pg = &pg->next;\n\t}\n out_unlock:\n\tmutex_unlock(&ftrace_lock);\n\n\tfor (pg = tmp_page; pg; pg = tmp_page) {\n\n\t\t \n\t\tclear_mod_from_hashes(pg);\n\n\t\tif (pg->records) {\n\t\t\tfree_pages((unsigned long)pg->records, pg->order);\n\t\t\tftrace_number_of_pages -= 1 << pg->order;\n\t\t}\n\t\ttmp_page = pg->next;\n\t\tkfree(pg);\n\t\tftrace_number_of_groups--;\n\t}\n}\n\nvoid ftrace_module_enable(struct module *mod)\n{\n\tstruct dyn_ftrace *rec;\n\tstruct ftrace_page *pg;\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (ftrace_disabled)\n\t\tgoto out_unlock;\n\n\t \n\tif (ftrace_start_up)\n\t\tftrace_arch_code_modify_prepare();\n\n\tdo_for_each_ftrace_rec(pg, rec) {\n\t\tint cnt;\n\t\t \n\t\tif (!within_module(rec->ip, mod))\n\t\t\tbreak;\n\n\t\t \n\t\tif (!test_for_valid_rec(rec)) {\n\t\t\t \n\t\t\trec->flags = FTRACE_FL_DISABLED;\n\t\t\tcontinue;\n\t\t}\n\n\t\tcnt = 0;\n\n\t\t \n\t\tif (ftrace_start_up)\n\t\t\tcnt += referenced_filters(rec);\n\n\t\trec->flags &= ~FTRACE_FL_DISABLED;\n\t\trec->flags += cnt;\n\n\t\tif (ftrace_start_up && cnt) {\n\t\t\tint failed = __ftrace_replace_code(rec, 1);\n\t\t\tif (failed) {\n\t\t\t\tftrace_bug(failed, rec);\n\t\t\t\tgoto out_loop;\n\t\t\t}\n\t\t}\n\n\t} while_for_each_ftrace_rec();\n\n out_loop:\n\tif (ftrace_start_up)\n\t\tftrace_arch_code_modify_post_process();\n\n out_unlock:\n\tmutex_unlock(&ftrace_lock);\n\n\tprocess_cached_mods(mod->name);\n}\n\nvoid ftrace_module_init(struct module *mod)\n{\n\tint ret;\n\n\tif (ftrace_disabled || !mod->num_ftrace_callsites)\n\t\treturn;\n\n\tret = ftrace_process_locs(mod, mod->ftrace_callsites,\n\t\t\t\t  mod->ftrace_callsites + mod->num_ftrace_callsites);\n\tif (ret)\n\t\tpr_warn(\"ftrace: failed to allocate entries for module '%s' functions\\n\",\n\t\t\tmod->name);\n}\n\nstatic void save_ftrace_mod_rec(struct ftrace_mod_map *mod_map,\n\t\t\t\tstruct dyn_ftrace *rec)\n{\n\tstruct ftrace_mod_func *mod_func;\n\tunsigned long symsize;\n\tunsigned long offset;\n\tchar str[KSYM_SYMBOL_LEN];\n\tchar *modname;\n\tconst char *ret;\n\n\tret = kallsyms_lookup(rec->ip, &symsize, &offset, &modname, str);\n\tif (!ret)\n\t\treturn;\n\n\tmod_func = kmalloc(sizeof(*mod_func), GFP_KERNEL);\n\tif (!mod_func)\n\t\treturn;\n\n\tmod_func->name = kstrdup(str, GFP_KERNEL);\n\tif (!mod_func->name) {\n\t\tkfree(mod_func);\n\t\treturn;\n\t}\n\n\tmod_func->ip = rec->ip - offset;\n\tmod_func->size = symsize;\n\n\tmod_map->num_funcs++;\n\n\tlist_add_rcu(&mod_func->list, &mod_map->funcs);\n}\n\nstatic struct ftrace_mod_map *\nallocate_ftrace_mod_map(struct module *mod,\n\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct ftrace_mod_map *mod_map;\n\n\tmod_map = kmalloc(sizeof(*mod_map), GFP_KERNEL);\n\tif (!mod_map)\n\t\treturn NULL;\n\n\tmod_map->mod = mod;\n\tmod_map->start_addr = start;\n\tmod_map->end_addr = end;\n\tmod_map->num_funcs = 0;\n\n\tINIT_LIST_HEAD_RCU(&mod_map->funcs);\n\n\tlist_add_rcu(&mod_map->list, &ftrace_mod_maps);\n\n\treturn mod_map;\n}\n\nstatic const char *\nftrace_func_address_lookup(struct ftrace_mod_map *mod_map,\n\t\t\t   unsigned long addr, unsigned long *size,\n\t\t\t   unsigned long *off, char *sym)\n{\n\tstruct ftrace_mod_func *found_func =  NULL;\n\tstruct ftrace_mod_func *mod_func;\n\n\tlist_for_each_entry_rcu(mod_func, &mod_map->funcs, list) {\n\t\tif (addr >= mod_func->ip &&\n\t\t    addr < mod_func->ip + mod_func->size) {\n\t\t\tfound_func = mod_func;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (found_func) {\n\t\tif (size)\n\t\t\t*size = found_func->size;\n\t\tif (off)\n\t\t\t*off = addr - found_func->ip;\n\t\tif (sym)\n\t\t\tstrscpy(sym, found_func->name, KSYM_NAME_LEN);\n\n\t\treturn found_func->name;\n\t}\n\n\treturn NULL;\n}\n\nconst char *\nftrace_mod_address_lookup(unsigned long addr, unsigned long *size,\n\t\t   unsigned long *off, char **modname, char *sym)\n{\n\tstruct ftrace_mod_map *mod_map;\n\tconst char *ret = NULL;\n\n\t \n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod_map, &ftrace_mod_maps, list) {\n\t\tret = ftrace_func_address_lookup(mod_map, addr, size, off, sym);\n\t\tif (ret) {\n\t\t\tif (modname)\n\t\t\t\t*modname = mod_map->mod->name;\n\t\t\tbreak;\n\t\t}\n\t}\n\tpreempt_enable();\n\n\treturn ret;\n}\n\nint ftrace_mod_get_kallsym(unsigned int symnum, unsigned long *value,\n\t\t\t   char *type, char *name,\n\t\t\t   char *module_name, int *exported)\n{\n\tstruct ftrace_mod_map *mod_map;\n\tstruct ftrace_mod_func *mod_func;\n\tint ret;\n\n\tpreempt_disable();\n\tlist_for_each_entry_rcu(mod_map, &ftrace_mod_maps, list) {\n\n\t\tif (symnum >= mod_map->num_funcs) {\n\t\t\tsymnum -= mod_map->num_funcs;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_for_each_entry_rcu(mod_func, &mod_map->funcs, list) {\n\t\t\tif (symnum > 1) {\n\t\t\t\tsymnum--;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t*value = mod_func->ip;\n\t\t\t*type = 'T';\n\t\t\tstrscpy(name, mod_func->name, KSYM_NAME_LEN);\n\t\t\tstrscpy(module_name, mod_map->mod->name, MODULE_NAME_LEN);\n\t\t\t*exported = 1;\n\t\t\tpreempt_enable();\n\t\t\treturn 0;\n\t\t}\n\t\tWARN_ON(1);\n\t\tbreak;\n\t}\n\tret = ftrace_get_trampoline_kallsym(symnum, value, type, name,\n\t\t\t\t\t    module_name, exported);\n\tpreempt_enable();\n\treturn ret;\n}\n\n#else\nstatic void save_ftrace_mod_rec(struct ftrace_mod_map *mod_map,\n\t\t\t\tstruct dyn_ftrace *rec) { }\nstatic inline struct ftrace_mod_map *\nallocate_ftrace_mod_map(struct module *mod,\n\t\t\tunsigned long start, unsigned long end)\n{\n\treturn NULL;\n}\nint ftrace_mod_get_kallsym(unsigned int symnum, unsigned long *value,\n\t\t\t   char *type, char *name, char *module_name,\n\t\t\t   int *exported)\n{\n\tint ret;\n\n\tpreempt_disable();\n\tret = ftrace_get_trampoline_kallsym(symnum, value, type, name,\n\t\t\t\t\t    module_name, exported);\n\tpreempt_enable();\n\treturn ret;\n}\n#endif  \n\nstruct ftrace_init_func {\n\tstruct list_head list;\n\tunsigned long ip;\n};\n\n \nstatic void\nclear_func_from_hash(struct ftrace_init_func *func, struct ftrace_hash *hash)\n{\n\tstruct ftrace_func_entry *entry;\n\n\tentry = ftrace_lookup_ip(hash, func->ip);\n\t \n\tif (entry)\n\t\tentry->ip = 0;\n}\n\nstatic void\nclear_func_from_hashes(struct ftrace_init_func *func)\n{\n\tstruct trace_array *tr;\n\n\tmutex_lock(&trace_types_lock);\n\tlist_for_each_entry(tr, &ftrace_trace_arrays, list) {\n\t\tif (!tr->ops || !tr->ops->func_hash)\n\t\t\tcontinue;\n\t\tmutex_lock(&tr->ops->func_hash->regex_lock);\n\t\tclear_func_from_hash(func, tr->ops->func_hash->filter_hash);\n\t\tclear_func_from_hash(func, tr->ops->func_hash->notrace_hash);\n\t\tmutex_unlock(&tr->ops->func_hash->regex_lock);\n\t}\n\tmutex_unlock(&trace_types_lock);\n}\n\nstatic void add_to_clear_hash_list(struct list_head *clear_list,\n\t\t\t\t   struct dyn_ftrace *rec)\n{\n\tstruct ftrace_init_func *func;\n\n\tfunc = kmalloc(sizeof(*func), GFP_KERNEL);\n\tif (!func) {\n\t\tMEM_FAIL(1, \"alloc failure, ftrace filter could be stale\\n\");\n\t\treturn;\n\t}\n\n\tfunc->ip = rec->ip;\n\tlist_add(&func->list, clear_list);\n}\n\nvoid ftrace_free_mem(struct module *mod, void *start_ptr, void *end_ptr)\n{\n\tunsigned long start = (unsigned long)(start_ptr);\n\tunsigned long end = (unsigned long)(end_ptr);\n\tstruct ftrace_page **last_pg = &ftrace_pages_start;\n\tstruct ftrace_page *pg;\n\tstruct dyn_ftrace *rec;\n\tstruct dyn_ftrace key;\n\tstruct ftrace_mod_map *mod_map = NULL;\n\tstruct ftrace_init_func *func, *func_next;\n\tLIST_HEAD(clear_hash);\n\n\tkey.ip = start;\n\tkey.flags = end;\t \n\n\tmutex_lock(&ftrace_lock);\n\n\t \n\tif (mod && ftrace_ops_list != &ftrace_list_end)\n\t\tmod_map = allocate_ftrace_mod_map(mod, start, end);\n\n\tfor (pg = ftrace_pages_start; pg; last_pg = &pg->next, pg = *last_pg) {\n\t\tif (end < pg->records[0].ip ||\n\t\t    start >= (pg->records[pg->index - 1].ip + MCOUNT_INSN_SIZE))\n\t\t\tcontinue;\n again:\n\t\trec = bsearch(&key, pg->records, pg->index,\n\t\t\t      sizeof(struct dyn_ftrace),\n\t\t\t      ftrace_cmp_recs);\n\t\tif (!rec)\n\t\t\tcontinue;\n\n\t\t \n\t\tadd_to_clear_hash_list(&clear_hash, rec);\n\n\t\tif (mod_map)\n\t\t\tsave_ftrace_mod_rec(mod_map, rec);\n\n\t\tpg->index--;\n\t\tftrace_update_tot_cnt--;\n\t\tif (!pg->index) {\n\t\t\t*last_pg = pg->next;\n\t\t\tif (pg->records) {\n\t\t\t\tfree_pages((unsigned long)pg->records, pg->order);\n\t\t\t\tftrace_number_of_pages -= 1 << pg->order;\n\t\t\t}\n\t\t\tftrace_number_of_groups--;\n\t\t\tkfree(pg);\n\t\t\tpg = container_of(last_pg, struct ftrace_page, next);\n\t\t\tif (!(*last_pg))\n\t\t\t\tftrace_pages = pg;\n\t\t\tcontinue;\n\t\t}\n\t\tmemmove(rec, rec + 1,\n\t\t\t(pg->index - (rec - pg->records)) * sizeof(*rec));\n\t\t \n\t\tgoto again;\n\t}\n\tmutex_unlock(&ftrace_lock);\n\n\tlist_for_each_entry_safe(func, func_next, &clear_hash, list) {\n\t\tclear_func_from_hashes(func);\n\t\tkfree(func);\n\t}\n}\n\nvoid __init ftrace_free_init_mem(void)\n{\n\tvoid *start = (void *)(&__init_begin);\n\tvoid *end = (void *)(&__init_end);\n\n\tftrace_boot_snapshot();\n\n\tftrace_free_mem(NULL, start, end);\n}\n\nint __init __weak ftrace_dyn_arch_init(void)\n{\n\treturn 0;\n}\n\nvoid __init ftrace_init(void)\n{\n\textern unsigned long __start_mcount_loc[];\n\textern unsigned long __stop_mcount_loc[];\n\tunsigned long count, flags;\n\tint ret;\n\n\tlocal_irq_save(flags);\n\tret = ftrace_dyn_arch_init();\n\tlocal_irq_restore(flags);\n\tif (ret)\n\t\tgoto failed;\n\n\tcount = __stop_mcount_loc - __start_mcount_loc;\n\tif (!count) {\n\t\tpr_info(\"ftrace: No functions to be traced?\\n\");\n\t\tgoto failed;\n\t}\n\n\tpr_info(\"ftrace: allocating %ld entries in %ld pages\\n\",\n\t\tcount, DIV_ROUND_UP(count, ENTRIES_PER_PAGE));\n\n\tret = ftrace_process_locs(NULL,\n\t\t\t\t  __start_mcount_loc,\n\t\t\t\t  __stop_mcount_loc);\n\tif (ret) {\n\t\tpr_warn(\"ftrace: failed to allocate entries for functions\\n\");\n\t\tgoto failed;\n\t}\n\n\tpr_info(\"ftrace: allocated %ld pages with %ld groups\\n\",\n\t\tftrace_number_of_pages, ftrace_number_of_groups);\n\n\tlast_ftrace_enabled = ftrace_enabled = 1;\n\n\tset_ftrace_early_filters();\n\n\treturn;\n failed:\n\tftrace_disabled = 1;\n}\n\n \nvoid __weak arch_ftrace_update_trampoline(struct ftrace_ops *ops)\n{\n}\n\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops)\n{\n\tunsigned long trampoline = ops->trampoline;\n\n\tarch_ftrace_update_trampoline(ops);\n\tif (ops->trampoline && ops->trampoline != trampoline &&\n\t    (ops->flags & FTRACE_OPS_FL_ALLOC_TRAMP)) {\n\t\t \n\t\tftrace_add_trampoline_to_kallsyms(ops);\n\t\tperf_event_ksymbol(PERF_RECORD_KSYMBOL_TYPE_OOL,\n\t\t\t\t   ops->trampoline, ops->trampoline_size, false,\n\t\t\t\t   FTRACE_TRAMPOLINE_SYM);\n\t\t \n\t\tperf_event_text_poke((void *)ops->trampoline, NULL, 0,\n\t\t\t\t     (void *)ops->trampoline,\n\t\t\t\t     ops->trampoline_size);\n\t}\n}\n\nvoid ftrace_init_trace_array(struct trace_array *tr)\n{\n\tINIT_LIST_HEAD(&tr->func_probes);\n\tINIT_LIST_HEAD(&tr->mod_trace);\n\tINIT_LIST_HEAD(&tr->mod_notrace);\n}\n#else\n\nstruct ftrace_ops global_ops = {\n\t.func\t\t\t= ftrace_stub,\n\t.flags\t\t\t= FTRACE_OPS_FL_INITIALIZED |\n\t\t\t\t  FTRACE_OPS_FL_PID,\n};\n\nstatic int __init ftrace_nodyn_init(void)\n{\n\tftrace_enabled = 1;\n\treturn 0;\n}\ncore_initcall(ftrace_nodyn_init);\n\nstatic inline int ftrace_init_dyn_tracefs(struct dentry *d_tracer) { return 0; }\nstatic inline void ftrace_startup_all(int command) { }\n\nstatic void ftrace_update_trampoline(struct ftrace_ops *ops)\n{\n}\n\n#endif  \n\n__init void ftrace_init_global_array_ops(struct trace_array *tr)\n{\n\ttr->ops = &global_ops;\n\ttr->ops->private = tr;\n\tftrace_init_trace_array(tr);\n}\n\nvoid ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func)\n{\n\t \n\tif (tr->flags & TRACE_ARRAY_FL_GLOBAL) {\n\t\tif (WARN_ON(tr->ops->func != ftrace_stub))\n\t\t\tprintk(\"ftrace ops had %pS for function\\n\",\n\t\t\t       tr->ops->func);\n\t}\n\ttr->ops->func = func;\n\ttr->ops->private = tr;\n}\n\nvoid ftrace_reset_array_ops(struct trace_array *tr)\n{\n\ttr->ops->func = ftrace_stub;\n}\n\nstatic nokprobe_inline void\n__ftrace_ops_list_func(unsigned long ip, unsigned long parent_ip,\n\t\t       struct ftrace_ops *ignored, struct ftrace_regs *fregs)\n{\n\tstruct pt_regs *regs = ftrace_get_regs(fregs);\n\tstruct ftrace_ops *op;\n\tint bit;\n\n\t \n\tbit = trace_test_and_set_recursion(ip, parent_ip, TRACE_LIST_START);\n\tif (bit < 0)\n\t\treturn;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\t \n\t\tif (op->flags & FTRACE_OPS_FL_STUB)\n\t\t\tcontinue;\n\t\t \n\t\tif ((!(op->flags & FTRACE_OPS_FL_RCU) || rcu_is_watching()) &&\n\t\t    ftrace_ops_test(op, ip, regs)) {\n\t\t\tif (FTRACE_WARN_ON(!op->func)) {\n\t\t\t\tpr_warn(\"op=%p %pS\\n\", op, op);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\top->func(ip, parent_ip, op, fregs);\n\t\t}\n\t} while_for_each_ftrace_op(op);\nout:\n\ttrace_clear_recursion(bit);\n}\n\n \n#if ARCH_SUPPORTS_FTRACE_OPS\nvoid arch_ftrace_ops_list_func(unsigned long ip, unsigned long parent_ip,\n\t\t\t       struct ftrace_ops *op, struct ftrace_regs *fregs)\n{\n\t__ftrace_ops_list_func(ip, parent_ip, NULL, fregs);\n}\n#else\nvoid arch_ftrace_ops_list_func(unsigned long ip, unsigned long parent_ip)\n{\n\t__ftrace_ops_list_func(ip, parent_ip, NULL, NULL);\n}\n#endif\nNOKPROBE_SYMBOL(arch_ftrace_ops_list_func);\n\n \nstatic void ftrace_ops_assist_func(unsigned long ip, unsigned long parent_ip,\n\t\t\t\t   struct ftrace_ops *op, struct ftrace_regs *fregs)\n{\n\tint bit;\n\n\tbit = trace_test_and_set_recursion(ip, parent_ip, TRACE_LIST_START);\n\tif (bit < 0)\n\t\treturn;\n\n\tif (!(op->flags & FTRACE_OPS_FL_RCU) || rcu_is_watching())\n\t\top->func(ip, parent_ip, op, fregs);\n\n\ttrace_clear_recursion(bit);\n}\nNOKPROBE_SYMBOL(ftrace_ops_assist_func);\n\n \nftrace_func_t ftrace_ops_get_func(struct ftrace_ops *ops)\n{\n\t \n\tif (ops->flags & (FTRACE_OPS_FL_RECURSION |\n\t\t\t  FTRACE_OPS_FL_RCU))\n\t\treturn ftrace_ops_assist_func;\n\n\treturn ops->func;\n}\n\nstatic void\nftrace_filter_pid_sched_switch_probe(void *data, bool preempt,\n\t\t\t\t     struct task_struct *prev,\n\t\t\t\t     struct task_struct *next,\n\t\t\t\t     unsigned int prev_state)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_pid_list *no_pid_list;\n\n\tpid_list = rcu_dereference_sched(tr->function_pids);\n\tno_pid_list = rcu_dereference_sched(tr->function_no_pids);\n\n\tif (trace_ignore_this_task(pid_list, no_pid_list, next))\n\t\tthis_cpu_write(tr->array_buffer.data->ftrace_ignore_pid,\n\t\t\t       FTRACE_PID_IGNORE);\n\telse\n\t\tthis_cpu_write(tr->array_buffer.data->ftrace_ignore_pid,\n\t\t\t       next->pid);\n}\n\nstatic void\nftrace_pid_follow_sched_process_fork(void *data,\n\t\t\t\t     struct task_struct *self,\n\t\t\t\t     struct task_struct *task)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = data;\n\n\tpid_list = rcu_dereference_sched(tr->function_pids);\n\ttrace_filter_add_remove_task(pid_list, self, task);\n\n\tpid_list = rcu_dereference_sched(tr->function_no_pids);\n\ttrace_filter_add_remove_task(pid_list, self, task);\n}\n\nstatic void\nftrace_pid_follow_sched_process_exit(void *data, struct task_struct *task)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = data;\n\n\tpid_list = rcu_dereference_sched(tr->function_pids);\n\ttrace_filter_add_remove_task(pid_list, NULL, task);\n\n\tpid_list = rcu_dereference_sched(tr->function_no_pids);\n\ttrace_filter_add_remove_task(pid_list, NULL, task);\n}\n\nvoid ftrace_pid_follow_fork(struct trace_array *tr, bool enable)\n{\n\tif (enable) {\n\t\tregister_trace_sched_process_fork(ftrace_pid_follow_sched_process_fork,\n\t\t\t\t\t\t  tr);\n\t\tregister_trace_sched_process_free(ftrace_pid_follow_sched_process_exit,\n\t\t\t\t\t\t  tr);\n\t} else {\n\t\tunregister_trace_sched_process_fork(ftrace_pid_follow_sched_process_fork,\n\t\t\t\t\t\t    tr);\n\t\tunregister_trace_sched_process_free(ftrace_pid_follow_sched_process_exit,\n\t\t\t\t\t\t    tr);\n\t}\n}\n\nstatic void clear_ftrace_pids(struct trace_array *tr, int type)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_pid_list *no_pid_list;\n\tint cpu;\n\n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     lockdep_is_held(&ftrace_lock));\n\tno_pid_list = rcu_dereference_protected(tr->function_no_pids,\n\t\t\t\t\t\tlockdep_is_held(&ftrace_lock));\n\n\t \n\tif (!pid_type_enabled(type, pid_list, no_pid_list))\n\t\treturn;\n\n\t \n\tif (!still_need_pid_events(type, pid_list, no_pid_list)) {\n\t\tunregister_trace_sched_switch(ftrace_filter_pid_sched_switch_probe, tr);\n\t\tfor_each_possible_cpu(cpu)\n\t\t\tper_cpu_ptr(tr->array_buffer.data, cpu)->ftrace_ignore_pid = FTRACE_PID_TRACE;\n\t}\n\n\tif (type & TRACE_PIDS)\n\t\trcu_assign_pointer(tr->function_pids, NULL);\n\n\tif (type & TRACE_NO_PIDS)\n\t\trcu_assign_pointer(tr->function_no_pids, NULL);\n\n\t \n\tsynchronize_rcu();\n\n\tif ((type & TRACE_PIDS) && pid_list)\n\t\ttrace_pid_list_free(pid_list);\n\n\tif ((type & TRACE_NO_PIDS) && no_pid_list)\n\t\ttrace_pid_list_free(no_pid_list);\n}\n\nvoid ftrace_clear_pids(struct trace_array *tr)\n{\n\tmutex_lock(&ftrace_lock);\n\n\tclear_ftrace_pids(tr, TRACE_PIDS | TRACE_NO_PIDS);\n\n\tmutex_unlock(&ftrace_lock);\n}\n\nstatic void ftrace_pid_reset(struct trace_array *tr, int type)\n{\n\tmutex_lock(&ftrace_lock);\n\tclear_ftrace_pids(tr, type);\n\n\tftrace_update_pid_func();\n\tftrace_startup_all(0);\n\n\tmutex_unlock(&ftrace_lock);\n}\n\n \n#define FTRACE_NO_PIDS\t\t(void *)(PID_MAX_LIMIT + 1)\n\nstatic void *fpid_start(struct seq_file *m, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&ftrace_lock);\n\trcu_read_lock_sched();\n\n\tpid_list = rcu_dereference_sched(tr->function_pids);\n\n\tif (!pid_list)\n\t\treturn !(*pos) ? FTRACE_NO_PIDS : NULL;\n\n\treturn trace_pid_start(pid_list, pos);\n}\n\nstatic void *fpid_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct trace_pid_list *pid_list = rcu_dereference_sched(tr->function_pids);\n\n\tif (v == FTRACE_NO_PIDS) {\n\t\t(*pos)++;\n\t\treturn NULL;\n\t}\n\treturn trace_pid_next(pid_list, v, pos);\n}\n\nstatic void fpid_stop(struct seq_file *m, void *p)\n\t__releases(RCU)\n{\n\trcu_read_unlock_sched();\n\tmutex_unlock(&ftrace_lock);\n}\n\nstatic int fpid_show(struct seq_file *m, void *v)\n{\n\tif (v == FTRACE_NO_PIDS) {\n\t\tseq_puts(m, \"no pid\\n\");\n\t\treturn 0;\n\t}\n\n\treturn trace_pid_show(m, v);\n}\n\nstatic const struct seq_operations ftrace_pid_sops = {\n\t.start = fpid_start,\n\t.next = fpid_next,\n\t.stop = fpid_stop,\n\t.show = fpid_show,\n};\n\nstatic void *fnpid_start(struct seq_file *m, loff_t *pos)\n\t__acquires(RCU)\n{\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_array *tr = m->private;\n\n\tmutex_lock(&ftrace_lock);\n\trcu_read_lock_sched();\n\n\tpid_list = rcu_dereference_sched(tr->function_no_pids);\n\n\tif (!pid_list)\n\t\treturn !(*pos) ? FTRACE_NO_PIDS : NULL;\n\n\treturn trace_pid_start(pid_list, pos);\n}\n\nstatic void *fnpid_next(struct seq_file *m, void *v, loff_t *pos)\n{\n\tstruct trace_array *tr = m->private;\n\tstruct trace_pid_list *pid_list = rcu_dereference_sched(tr->function_no_pids);\n\n\tif (v == FTRACE_NO_PIDS) {\n\t\t(*pos)++;\n\t\treturn NULL;\n\t}\n\treturn trace_pid_next(pid_list, v, pos);\n}\n\nstatic const struct seq_operations ftrace_no_pid_sops = {\n\t.start = fnpid_start,\n\t.next = fnpid_next,\n\t.stop = fpid_stop,\n\t.show = fpid_show,\n};\n\nstatic int pid_open(struct inode *inode, struct file *file, int type)\n{\n\tconst struct seq_operations *seq_ops;\n\tstruct trace_array *tr = inode->i_private;\n\tstruct seq_file *m;\n\tint ret = 0;\n\n\tret = tracing_check_open_get_tr(tr);\n\tif (ret)\n\t\treturn ret;\n\n\tif ((file->f_mode & FMODE_WRITE) &&\n\t    (file->f_flags & O_TRUNC))\n\t\tftrace_pid_reset(tr, type);\n\n\tswitch (type) {\n\tcase TRACE_PIDS:\n\t\tseq_ops = &ftrace_pid_sops;\n\t\tbreak;\n\tcase TRACE_NO_PIDS:\n\t\tseq_ops = &ftrace_no_pid_sops;\n\t\tbreak;\n\tdefault:\n\t\ttrace_array_put(tr);\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\n\tret = seq_open(file, seq_ops);\n\tif (ret < 0) {\n\t\ttrace_array_put(tr);\n\t} else {\n\t\tm = file->private_data;\n\t\t \n\t\tm->private = tr;\n\t}\n\n\treturn ret;\n}\n\nstatic int\nftrace_pid_open(struct inode *inode, struct file *file)\n{\n\treturn pid_open(inode, file, TRACE_PIDS);\n}\n\nstatic int\nftrace_no_pid_open(struct inode *inode, struct file *file)\n{\n\treturn pid_open(inode, file, TRACE_NO_PIDS);\n}\n\nstatic void ignore_task_cpu(void *data)\n{\n\tstruct trace_array *tr = data;\n\tstruct trace_pid_list *pid_list;\n\tstruct trace_pid_list *no_pid_list;\n\n\t \n\tpid_list = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     mutex_is_locked(&ftrace_lock));\n\tno_pid_list = rcu_dereference_protected(tr->function_no_pids,\n\t\t\t\t\t\tmutex_is_locked(&ftrace_lock));\n\n\tif (trace_ignore_this_task(pid_list, no_pid_list, current))\n\t\tthis_cpu_write(tr->array_buffer.data->ftrace_ignore_pid,\n\t\t\t       FTRACE_PID_IGNORE);\n\telse\n\t\tthis_cpu_write(tr->array_buffer.data->ftrace_ignore_pid,\n\t\t\t       current->pid);\n}\n\nstatic ssize_t\npid_write(struct file *filp, const char __user *ubuf,\n\t  size_t cnt, loff_t *ppos, int type)\n{\n\tstruct seq_file *m = filp->private_data;\n\tstruct trace_array *tr = m->private;\n\tstruct trace_pid_list *filtered_pids;\n\tstruct trace_pid_list *other_pids;\n\tstruct trace_pid_list *pid_list;\n\tssize_t ret;\n\n\tif (!cnt)\n\t\treturn 0;\n\n\tmutex_lock(&ftrace_lock);\n\n\tswitch (type) {\n\tcase TRACE_PIDS:\n\t\tfiltered_pids = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     lockdep_is_held(&ftrace_lock));\n\t\tother_pids = rcu_dereference_protected(tr->function_no_pids,\n\t\t\t\t\t     lockdep_is_held(&ftrace_lock));\n\t\tbreak;\n\tcase TRACE_NO_PIDS:\n\t\tfiltered_pids = rcu_dereference_protected(tr->function_no_pids,\n\t\t\t\t\t     lockdep_is_held(&ftrace_lock));\n\t\tother_pids = rcu_dereference_protected(tr->function_pids,\n\t\t\t\t\t     lockdep_is_held(&ftrace_lock));\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tWARN_ON_ONCE(1);\n\t\tgoto out;\n\t}\n\n\tret = trace_pid_write(filtered_pids, &pid_list, ubuf, cnt);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tswitch (type) {\n\tcase TRACE_PIDS:\n\t\trcu_assign_pointer(tr->function_pids, pid_list);\n\t\tbreak;\n\tcase TRACE_NO_PIDS:\n\t\trcu_assign_pointer(tr->function_no_pids, pid_list);\n\t\tbreak;\n\t}\n\n\n\tif (filtered_pids) {\n\t\tsynchronize_rcu();\n\t\ttrace_pid_list_free(filtered_pids);\n\t} else if (pid_list && !other_pids) {\n\t\t \n\t\tregister_trace_sched_switch(ftrace_filter_pid_sched_switch_probe, tr);\n\t}\n\n\t \n\ton_each_cpu(ignore_task_cpu, tr, 1);\n\n\tftrace_update_pid_func();\n\tftrace_startup_all(0);\n out:\n\tmutex_unlock(&ftrace_lock);\n\n\tif (ret > 0)\n\t\t*ppos += ret;\n\n\treturn ret;\n}\n\nstatic ssize_t\nftrace_pid_write(struct file *filp, const char __user *ubuf,\n\t\t size_t cnt, loff_t *ppos)\n{\n\treturn pid_write(filp, ubuf, cnt, ppos, TRACE_PIDS);\n}\n\nstatic ssize_t\nftrace_no_pid_write(struct file *filp, const char __user *ubuf,\n\t\t    size_t cnt, loff_t *ppos)\n{\n\treturn pid_write(filp, ubuf, cnt, ppos, TRACE_NO_PIDS);\n}\n\nstatic int\nftrace_pid_release(struct inode *inode, struct file *file)\n{\n\tstruct trace_array *tr = inode->i_private;\n\n\ttrace_array_put(tr);\n\n\treturn seq_release(inode, file);\n}\n\nstatic const struct file_operations ftrace_pid_fops = {\n\t.open\t\t= ftrace_pid_open,\n\t.write\t\t= ftrace_pid_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= ftrace_pid_release,\n};\n\nstatic const struct file_operations ftrace_no_pid_fops = {\n\t.open\t\t= ftrace_no_pid_open,\n\t.write\t\t= ftrace_no_pid_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= tracing_lseek,\n\t.release\t= ftrace_pid_release,\n};\n\nvoid ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer)\n{\n\ttrace_create_file(\"set_ftrace_pid\", TRACE_MODE_WRITE, d_tracer,\n\t\t\t    tr, &ftrace_pid_fops);\n\ttrace_create_file(\"set_ftrace_notrace_pid\", TRACE_MODE_WRITE,\n\t\t\t  d_tracer, tr, &ftrace_no_pid_fops);\n}\n\nvoid __init ftrace_init_tracefs_toplevel(struct trace_array *tr,\n\t\t\t\t\t struct dentry *d_tracer)\n{\n\t \n\tWARN_ON(!(tr->flags & TRACE_ARRAY_FL_GLOBAL));\n\n\tftrace_init_dyn_tracefs(d_tracer);\n\tftrace_profile_tracefs(d_tracer);\n}\n\n \nvoid ftrace_kill(void)\n{\n\tftrace_disabled = 1;\n\tftrace_enabled = 0;\n\tftrace_trace_function = ftrace_stub;\n}\n\n \nint ftrace_is_dead(void)\n{\n\treturn ftrace_disabled;\n}\n\n#ifdef CONFIG_DYNAMIC_FTRACE_WITH_DIRECT_CALLS\n \nstatic int prepare_direct_functions_for_ipmodify(struct ftrace_ops *ops)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_hash *hash;\n\tstruct ftrace_ops *op;\n\tint size, i, ret;\n\n\tlockdep_assert_held_once(&direct_mutex);\n\n\tif (!(ops->flags & FTRACE_OPS_FL_IPMODIFY))\n\t\treturn 0;\n\n\thash = ops->func_hash->filter_hash;\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tunsigned long ip = entry->ip;\n\t\t\tbool found_op = false;\n\n\t\t\tmutex_lock(&ftrace_lock);\n\t\t\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\t\t\tif (!(op->flags & FTRACE_OPS_FL_DIRECT))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ops_references_ip(op, ip)) {\n\t\t\t\t\tfound_op = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} while_for_each_ftrace_op(op);\n\t\t\tmutex_unlock(&ftrace_lock);\n\n\t\t\tif (found_op) {\n\t\t\t\tif (!op->ops_func)\n\t\t\t\t\treturn -EBUSY;\n\n\t\t\t\tret = op->ops_func(op, FTRACE_OPS_CMD_ENABLE_SHARE_IPMODIFY_PEER);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void cleanup_direct_functions_after_ipmodify(struct ftrace_ops *ops)\n{\n\tstruct ftrace_func_entry *entry;\n\tstruct ftrace_hash *hash;\n\tstruct ftrace_ops *op;\n\tint size, i;\n\n\tif (!(ops->flags & FTRACE_OPS_FL_IPMODIFY))\n\t\treturn;\n\n\tmutex_lock(&direct_mutex);\n\n\thash = ops->func_hash->filter_hash;\n\tsize = 1 << hash->size_bits;\n\tfor (i = 0; i < size; i++) {\n\t\thlist_for_each_entry(entry, &hash->buckets[i], hlist) {\n\t\t\tunsigned long ip = entry->ip;\n\t\t\tbool found_op = false;\n\n\t\t\tmutex_lock(&ftrace_lock);\n\t\t\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\t\t\tif (!(op->flags & FTRACE_OPS_FL_DIRECT))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (ops_references_ip(op, ip)) {\n\t\t\t\t\tfound_op = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} while_for_each_ftrace_op(op);\n\t\t\tmutex_unlock(&ftrace_lock);\n\n\t\t\t \n\t\t\tif (found_op && op->ops_func)\n\t\t\t\top->ops_func(op, FTRACE_OPS_CMD_DISABLE_SHARE_IPMODIFY_PEER);\n\t\t}\n\t}\n\tmutex_unlock(&direct_mutex);\n}\n\n#define lock_direct_mutex()\tmutex_lock(&direct_mutex)\n#define unlock_direct_mutex()\tmutex_unlock(&direct_mutex)\n\n#else   \n\nstatic int prepare_direct_functions_for_ipmodify(struct ftrace_ops *ops)\n{\n\treturn 0;\n}\n\nstatic void cleanup_direct_functions_after_ipmodify(struct ftrace_ops *ops)\n{\n}\n\n#define lock_direct_mutex()\tdo { } while (0)\n#define unlock_direct_mutex()\tdo { } while (0)\n\n#endif   \n\n \nstatic int register_ftrace_function_nolock(struct ftrace_ops *ops)\n{\n\tint ret;\n\n\tftrace_ops_init(ops);\n\n\tmutex_lock(&ftrace_lock);\n\n\tret = ftrace_startup(ops, 0);\n\n\tmutex_unlock(&ftrace_lock);\n\n\treturn ret;\n}\n\n \nint register_ftrace_function(struct ftrace_ops *ops)\n{\n\tint ret;\n\n\tlock_direct_mutex();\n\tret = prepare_direct_functions_for_ipmodify(ops);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\tret = register_ftrace_function_nolock(ops);\n\nout_unlock:\n\tunlock_direct_mutex();\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(register_ftrace_function);\n\n \nint unregister_ftrace_function(struct ftrace_ops *ops)\n{\n\tint ret;\n\n\tmutex_lock(&ftrace_lock);\n\tret = ftrace_shutdown(ops, 0);\n\tmutex_unlock(&ftrace_lock);\n\n\tcleanup_direct_functions_after_ipmodify(ops);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(unregister_ftrace_function);\n\nstatic int symbols_cmp(const void *a, const void *b)\n{\n\tconst char **str_a = (const char **) a;\n\tconst char **str_b = (const char **) b;\n\n\treturn strcmp(*str_a, *str_b);\n}\n\nstruct kallsyms_data {\n\tunsigned long *addrs;\n\tconst char **syms;\n\tsize_t cnt;\n\tsize_t found;\n};\n\n \nstatic int kallsyms_callback(void *data, const char *name, unsigned long addr)\n{\n\tstruct kallsyms_data *args = data;\n\tconst char **sym;\n\tint idx;\n\n\tsym = bsearch(&name, args->syms, args->cnt, sizeof(*args->syms), symbols_cmp);\n\tif (!sym)\n\t\treturn 0;\n\n\tidx = sym - args->syms;\n\tif (args->addrs[idx])\n\t\treturn 0;\n\n\tif (!ftrace_location(addr))\n\t\treturn 0;\n\n\targs->addrs[idx] = addr;\n\targs->found++;\n\treturn args->found == args->cnt ? 1 : 0;\n}\n\n \nint ftrace_lookup_symbols(const char **sorted_syms, size_t cnt, unsigned long *addrs)\n{\n\tstruct kallsyms_data args;\n\tint found_all;\n\n\tmemset(addrs, 0, sizeof(*addrs) * cnt);\n\targs.addrs = addrs;\n\targs.syms = sorted_syms;\n\targs.cnt = cnt;\n\targs.found = 0;\n\n\tfound_all = kallsyms_on_each_symbol(kallsyms_callback, &args);\n\tif (found_all)\n\t\treturn 0;\n\tfound_all = module_kallsyms_on_each_symbol(NULL, kallsyms_callback, &args);\n\treturn found_all ? 0 : -ESRCH;\n}\n\n#ifdef CONFIG_SYSCTL\n\n#ifdef CONFIG_DYNAMIC_FTRACE\nstatic void ftrace_startup_sysctl(void)\n{\n\tint command;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn;\n\n\t \n\tsaved_ftrace_func = NULL;\n\t \n\tif (ftrace_start_up) {\n\t\tcommand = FTRACE_UPDATE_CALLS;\n\t\tif (ftrace_graph_active)\n\t\t\tcommand |= FTRACE_START_FUNC_RET;\n\t\tftrace_startup_enable(command);\n\t}\n}\n\nstatic void ftrace_shutdown_sysctl(void)\n{\n\tint command;\n\n\tif (unlikely(ftrace_disabled))\n\t\treturn;\n\n\t \n\tif (ftrace_start_up) {\n\t\tcommand = FTRACE_DISABLE_CALLS;\n\t\tif (ftrace_graph_active)\n\t\t\tcommand |= FTRACE_STOP_FUNC_RET;\n\t\tftrace_run_update_code(command);\n\t}\n}\n#else\n# define ftrace_startup_sysctl()       do { } while (0)\n# define ftrace_shutdown_sysctl()      do { } while (0)\n#endif  \n\nstatic bool is_permanent_ops_registered(void)\n{\n\tstruct ftrace_ops *op;\n\n\tdo_for_each_ftrace_op(op, ftrace_ops_list) {\n\t\tif (op->flags & FTRACE_OPS_FL_PERMANENT)\n\t\t\treturn true;\n\t} while_for_each_ftrace_op(op);\n\n\treturn false;\n}\n\nstatic int\nftrace_enable_sysctl(struct ctl_table *table, int write,\n\t\t     void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret = -ENODEV;\n\n\tmutex_lock(&ftrace_lock);\n\n\tif (unlikely(ftrace_disabled))\n\t\tgoto out;\n\n\tret = proc_dointvec(table, write, buffer, lenp, ppos);\n\n\tif (ret || !write || (last_ftrace_enabled == !!ftrace_enabled))\n\t\tgoto out;\n\n\tif (ftrace_enabled) {\n\n\t\t \n\t\tif (rcu_dereference_protected(ftrace_ops_list,\n\t\t\tlockdep_is_held(&ftrace_lock)) != &ftrace_list_end)\n\t\t\tupdate_ftrace_function();\n\n\t\tftrace_startup_sysctl();\n\n\t} else {\n\t\tif (is_permanent_ops_registered()) {\n\t\t\tftrace_enabled = true;\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tftrace_trace_function = ftrace_stub;\n\n\t\tftrace_shutdown_sysctl();\n\t}\n\n\tlast_ftrace_enabled = !!ftrace_enabled;\n out:\n\tmutex_unlock(&ftrace_lock);\n\treturn ret;\n}\n\nstatic struct ctl_table ftrace_sysctls[] = {\n\t{\n\t\t.procname       = \"ftrace_enabled\",\n\t\t.data           = &ftrace_enabled,\n\t\t.maxlen         = sizeof(int),\n\t\t.mode           = 0644,\n\t\t.proc_handler   = ftrace_enable_sysctl,\n\t},\n\t{}\n};\n\nstatic int __init ftrace_sysctl_init(void)\n{\n\tregister_sysctl_init(\"kernel\", ftrace_sysctls);\n\treturn 0;\n}\nlate_initcall(ftrace_sysctl_init);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}