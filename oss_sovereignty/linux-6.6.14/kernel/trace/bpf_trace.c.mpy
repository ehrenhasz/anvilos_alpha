{
  "module_name": "bpf_trace.c",
  "hash_id": "b7adcd090b7efd7fa99202d18757b686af03ef6c9f0b2bcb59a2e267e09c1ba6",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/bpf_trace.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/bpf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/bpf_perf_event.h>\n#include <linux/btf.h>\n#include <linux/filter.h>\n#include <linux/uaccess.h>\n#include <linux/ctype.h>\n#include <linux/kprobes.h>\n#include <linux/spinlock.h>\n#include <linux/syscalls.h>\n#include <linux/error-injection.h>\n#include <linux/btf_ids.h>\n#include <linux/bpf_lsm.h>\n#include <linux/fprobe.h>\n#include <linux/bsearch.h>\n#include <linux/sort.h>\n#include <linux/key.h>\n#include <linux/verification.h>\n#include <linux/namei.h>\n\n#include <net/bpf_sk_storage.h>\n\n#include <uapi/linux/bpf.h>\n#include <uapi/linux/btf.h>\n\n#include <asm/tlb.h>\n\n#include \"trace_probe.h\"\n#include \"trace.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"bpf_trace.h\"\n\n#define bpf_event_rcu_dereference(p)\t\t\t\t\t\\\n\trcu_dereference_protected(p, lockdep_is_held(&bpf_event_mutex))\n\n#define MAX_UPROBE_MULTI_CNT (1U << 20)\n#define MAX_KPROBE_MULTI_CNT (1U << 20)\n\n#ifdef CONFIG_MODULES\nstruct bpf_trace_module {\n\tstruct module *module;\n\tstruct list_head list;\n};\n\nstatic LIST_HEAD(bpf_trace_modules);\nstatic DEFINE_MUTEX(bpf_module_mutex);\n\nstatic struct bpf_raw_event_map *bpf_get_raw_tracepoint_module(const char *name)\n{\n\tstruct bpf_raw_event_map *btp, *ret = NULL;\n\tstruct bpf_trace_module *btm;\n\tunsigned int i;\n\n\tmutex_lock(&bpf_module_mutex);\n\tlist_for_each_entry(btm, &bpf_trace_modules, list) {\n\t\tfor (i = 0; i < btm->module->num_bpf_raw_events; ++i) {\n\t\t\tbtp = &btm->module->bpf_raw_events[i];\n\t\t\tif (!strcmp(btp->tp->name, name)) {\n\t\t\t\tif (try_module_get(btm->module))\n\t\t\t\t\tret = btp;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\tmutex_unlock(&bpf_module_mutex);\n\treturn ret;\n}\n#else\nstatic struct bpf_raw_event_map *bpf_get_raw_tracepoint_module(const char *name)\n{\n\treturn NULL;\n}\n#endif  \n\nu64 bpf_get_stackid(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);\nu64 bpf_get_stack(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);\n\nstatic int bpf_btf_printf_prepare(struct btf_ptr *ptr, u32 btf_ptr_size,\n\t\t\t\t  u64 flags, const struct btf **btf,\n\t\t\t\t  s32 *btf_id);\nstatic u64 bpf_kprobe_multi_cookie(struct bpf_run_ctx *ctx);\nstatic u64 bpf_kprobe_multi_entry_ip(struct bpf_run_ctx *ctx);\n\nstatic u64 bpf_uprobe_multi_cookie(struct bpf_run_ctx *ctx);\nstatic u64 bpf_uprobe_multi_entry_ip(struct bpf_run_ctx *ctx);\n\n \nunsigned int trace_call_bpf(struct trace_event_call *call, void *ctx)\n{\n\tunsigned int ret;\n\n\tcant_sleep();\n\n\tif (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1)) {\n\t\t \n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\t \n\trcu_read_lock();\n\tret = bpf_prog_run_array(rcu_dereference(call->prog_array),\n\t\t\t\t ctx, bpf_prog_run);\n\trcu_read_unlock();\n\n out:\n\t__this_cpu_dec(bpf_prog_active);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_BPF_KPROBE_OVERRIDE\nBPF_CALL_2(bpf_override_return, struct pt_regs *, regs, unsigned long, rc)\n{\n\tregs_set_return_value(regs, rc);\n\toverride_function_with_return(regs);\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_override_return_proto = {\n\t.func\t\t= bpf_override_return,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n#endif\n\nstatic __always_inline int\nbpf_probe_read_user_common(void *dst, u32 size, const void __user *unsafe_ptr)\n{\n\tint ret;\n\n\tret = copy_from_user_nofault(dst, unsafe_ptr, size);\n\tif (unlikely(ret < 0))\n\t\tmemset(dst, 0, size);\n\treturn ret;\n}\n\nBPF_CALL_3(bpf_probe_read_user, void *, dst, u32, size,\n\t   const void __user *, unsafe_ptr)\n{\n\treturn bpf_probe_read_user_common(dst, size, unsafe_ptr);\n}\n\nconst struct bpf_func_proto bpf_probe_read_user_proto = {\n\t.func\t\t= bpf_probe_read_user,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic __always_inline int\nbpf_probe_read_user_str_common(void *dst, u32 size,\n\t\t\t       const void __user *unsafe_ptr)\n{\n\tint ret;\n\n\t \n\tret = strncpy_from_user_nofault(dst, unsafe_ptr, size);\n\tif (unlikely(ret < 0))\n\t\tmemset(dst, 0, size);\n\treturn ret;\n}\n\nBPF_CALL_3(bpf_probe_read_user_str, void *, dst, u32, size,\n\t   const void __user *, unsafe_ptr)\n{\n\treturn bpf_probe_read_user_str_common(dst, size, unsafe_ptr);\n}\n\nconst struct bpf_func_proto bpf_probe_read_user_str_proto = {\n\t.func\t\t= bpf_probe_read_user_str,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_probe_read_kernel, void *, dst, u32, size,\n\t   const void *, unsafe_ptr)\n{\n\treturn bpf_probe_read_kernel_common(dst, size, unsafe_ptr);\n}\n\nconst struct bpf_func_proto bpf_probe_read_kernel_proto = {\n\t.func\t\t= bpf_probe_read_kernel,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nstatic __always_inline int\nbpf_probe_read_kernel_str_common(void *dst, u32 size, const void *unsafe_ptr)\n{\n\tint ret;\n\n\t \n\tret = strncpy_from_kernel_nofault(dst, unsafe_ptr, size);\n\tif (unlikely(ret < 0))\n\t\tmemset(dst, 0, size);\n\treturn ret;\n}\n\nBPF_CALL_3(bpf_probe_read_kernel_str, void *, dst, u32, size,\n\t   const void *, unsafe_ptr)\n{\n\treturn bpf_probe_read_kernel_str_common(dst, size, unsafe_ptr);\n}\n\nconst struct bpf_func_proto bpf_probe_read_kernel_str_proto = {\n\t.func\t\t= bpf_probe_read_kernel_str,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\n#ifdef CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE\nBPF_CALL_3(bpf_probe_read_compat, void *, dst, u32, size,\n\t   const void *, unsafe_ptr)\n{\n\tif ((unsigned long)unsafe_ptr < TASK_SIZE) {\n\t\treturn bpf_probe_read_user_common(dst, size,\n\t\t\t\t(__force void __user *)unsafe_ptr);\n\t}\n\treturn bpf_probe_read_kernel_common(dst, size, unsafe_ptr);\n}\n\nstatic const struct bpf_func_proto bpf_probe_read_compat_proto = {\n\t.func\t\t= bpf_probe_read_compat,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_probe_read_compat_str, void *, dst, u32, size,\n\t   const void *, unsafe_ptr)\n{\n\tif ((unsigned long)unsafe_ptr < TASK_SIZE) {\n\t\treturn bpf_probe_read_user_str_common(dst, size,\n\t\t\t\t(__force void __user *)unsafe_ptr);\n\t}\n\treturn bpf_probe_read_kernel_str_common(dst, size, unsafe_ptr);\n}\n\nstatic const struct bpf_func_proto bpf_probe_read_compat_str_proto = {\n\t.func\t\t= bpf_probe_read_compat_str,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n#endif  \n\nBPF_CALL_3(bpf_probe_write_user, void __user *, unsafe_ptr, const void *, src,\n\t   u32, size)\n{\n\t \n\n\tif (unlikely(in_interrupt() ||\n\t\t     current->flags & (PF_KTHREAD | PF_EXITING)))\n\t\treturn -EPERM;\n\tif (unlikely(!nmi_uaccess_okay()))\n\t\treturn -EPERM;\n\n\treturn copy_to_user_nofault(unsafe_ptr, src, size);\n}\n\nstatic const struct bpf_func_proto bpf_probe_write_user_proto = {\n\t.func\t\t= bpf_probe_write_user,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n};\n\nstatic const struct bpf_func_proto *bpf_get_probe_write_proto(void)\n{\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn NULL;\n\n\tpr_warn_ratelimited(\"%s[%d] is installing a program with bpf_probe_write_user helper that may corrupt user memory!\",\n\t\t\t    current->comm, task_pid_nr(current));\n\n\treturn &bpf_probe_write_user_proto;\n}\n\n#define MAX_TRACE_PRINTK_VARARGS\t3\n#define BPF_TRACE_PRINTK_SIZE\t\t1024\n\nBPF_CALL_5(bpf_trace_printk, char *, fmt, u32, fmt_size, u64, arg1,\n\t   u64, arg2, u64, arg3)\n{\n\tu64 args[MAX_TRACE_PRINTK_VARARGS] = { arg1, arg2, arg3 };\n\tstruct bpf_bprintf_data data = {\n\t\t.get_bin_args\t= true,\n\t\t.get_buf\t= true,\n\t};\n\tint ret;\n\n\tret = bpf_bprintf_prepare(fmt, fmt_size, args,\n\t\t\t\t  MAX_TRACE_PRINTK_VARARGS, &data);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = bstr_printf(data.buf, MAX_BPRINTF_BUF, fmt, data.bin_args);\n\n\ttrace_bpf_trace_printk(data.buf);\n\n\tbpf_bprintf_cleanup(&data);\n\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_trace_printk_proto = {\n\t.func\t\t= bpf_trace_printk,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg2_type\t= ARG_CONST_SIZE,\n};\n\nstatic void __set_printk_clr_event(void)\n{\n\t \n\tif (trace_set_clr_event(\"bpf_trace\", \"bpf_trace_printk\", 1))\n\t\tpr_warn_ratelimited(\"could not enable bpf_trace_printk events\");\n}\n\nconst struct bpf_func_proto *bpf_get_trace_printk_proto(void)\n{\n\t__set_printk_clr_event();\n\treturn &bpf_trace_printk_proto;\n}\n\nBPF_CALL_4(bpf_trace_vprintk, char *, fmt, u32, fmt_size, const void *, args,\n\t   u32, data_len)\n{\n\tstruct bpf_bprintf_data data = {\n\t\t.get_bin_args\t= true,\n\t\t.get_buf\t= true,\n\t};\n\tint ret, num_args;\n\n\tif (data_len & 7 || data_len > MAX_BPRINTF_VARARGS * 8 ||\n\t    (data_len && !args))\n\t\treturn -EINVAL;\n\tnum_args = data_len / 8;\n\n\tret = bpf_bprintf_prepare(fmt, fmt_size, args, num_args, &data);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = bstr_printf(data.buf, MAX_BPRINTF_BUF, fmt, data.bin_args);\n\n\ttrace_bpf_trace_printk(data.buf);\n\n\tbpf_bprintf_cleanup(&data);\n\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_trace_vprintk_proto = {\n\t.func\t\t= bpf_trace_vprintk,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg2_type\t= ARG_CONST_SIZE,\n\t.arg3_type\t= ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nconst struct bpf_func_proto *bpf_get_trace_vprintk_proto(void)\n{\n\t__set_printk_clr_event();\n\treturn &bpf_trace_vprintk_proto;\n}\n\nBPF_CALL_5(bpf_seq_printf, struct seq_file *, m, char *, fmt, u32, fmt_size,\n\t   const void *, args, u32, data_len)\n{\n\tstruct bpf_bprintf_data data = {\n\t\t.get_bin_args\t= true,\n\t};\n\tint err, num_args;\n\n\tif (data_len & 7 || data_len > MAX_BPRINTF_VARARGS * 8 ||\n\t    (data_len && !args))\n\t\treturn -EINVAL;\n\tnum_args = data_len / 8;\n\n\terr = bpf_bprintf_prepare(fmt, fmt_size, args, num_args, &data);\n\tif (err < 0)\n\t\treturn err;\n\n\tseq_bprintf(m, fmt, data.bin_args);\n\n\tbpf_bprintf_cleanup(&data);\n\n\treturn seq_has_overflowed(m) ? -EOVERFLOW : 0;\n}\n\nBTF_ID_LIST_SINGLE(btf_seq_file_ids, struct, seq_file)\n\nstatic const struct bpf_func_proto bpf_seq_printf_proto = {\n\t.func\t\t= bpf_seq_printf,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_seq_file_ids[0],\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE,\n\t.arg4_type      = ARG_PTR_TO_MEM | PTR_MAYBE_NULL | MEM_RDONLY,\n\t.arg5_type      = ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_3(bpf_seq_write, struct seq_file *, m, const void *, data, u32, len)\n{\n\treturn seq_write(m, data, len) ? -EOVERFLOW : 0;\n}\n\nstatic const struct bpf_func_proto bpf_seq_write_proto = {\n\t.func\t\t= bpf_seq_write,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_seq_file_ids[0],\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_4(bpf_seq_printf_btf, struct seq_file *, m, struct btf_ptr *, ptr,\n\t   u32, btf_ptr_size, u64, flags)\n{\n\tconst struct btf *btf;\n\ts32 btf_id;\n\tint ret;\n\n\tret = bpf_btf_printf_prepare(ptr, btf_ptr_size, flags, &btf, &btf_id);\n\tif (ret)\n\t\treturn ret;\n\n\treturn btf_type_seq_show_flags(btf, btf_id, ptr->ptr, m, flags);\n}\n\nstatic const struct bpf_func_proto bpf_seq_printf_btf_proto = {\n\t.func\t\t= bpf_seq_printf_btf,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_seq_file_ids[0],\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic __always_inline int\nget_map_perf_counter(struct bpf_map *map, u64 flags,\n\t\t     u64 *value, u64 *enabled, u64 *running)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tunsigned int cpu = smp_processor_id();\n\tu64 index = flags & BPF_F_INDEX_MASK;\n\tstruct bpf_event_entry *ee;\n\n\tif (unlikely(flags & ~(BPF_F_INDEX_MASK)))\n\t\treturn -EINVAL;\n\tif (index == BPF_F_CURRENT_CPU)\n\t\tindex = cpu;\n\tif (unlikely(index >= array->map.max_entries))\n\t\treturn -E2BIG;\n\n\tee = READ_ONCE(array->ptrs[index]);\n\tif (!ee)\n\t\treturn -ENOENT;\n\n\treturn perf_event_read_local(ee->event, value, enabled, running);\n}\n\nBPF_CALL_2(bpf_perf_event_read, struct bpf_map *, map, u64, flags)\n{\n\tu64 value = 0;\n\tint err;\n\n\terr = get_map_perf_counter(map, flags, &value, NULL, NULL);\n\t \n\tif (err)\n\t\treturn err;\n\treturn value;\n}\n\nstatic const struct bpf_func_proto bpf_perf_event_read_proto = {\n\t.func\t\t= bpf_perf_event_read,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_perf_event_read_value, struct bpf_map *, map, u64, flags,\n\t   struct bpf_perf_event_value *, buf, u32, size)\n{\n\tint err = -EINVAL;\n\n\tif (unlikely(size != sizeof(struct bpf_perf_event_value)))\n\t\tgoto clear;\n\terr = get_map_perf_counter(map, flags, &buf->counter, &buf->enabled,\n\t\t\t\t   &buf->running);\n\tif (unlikely(err))\n\t\tgoto clear;\n\treturn 0;\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_perf_event_read_value_proto = {\n\t.func\t\t= bpf_perf_event_read_value,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_CONST_MAP_PTR,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg4_type\t= ARG_CONST_SIZE,\n};\n\nstatic __always_inline u64\n__bpf_perf_event_output(struct pt_regs *regs, struct bpf_map *map,\n\t\t\tu64 flags, struct perf_sample_data *sd)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tunsigned int cpu = smp_processor_id();\n\tu64 index = flags & BPF_F_INDEX_MASK;\n\tstruct bpf_event_entry *ee;\n\tstruct perf_event *event;\n\n\tif (index == BPF_F_CURRENT_CPU)\n\t\tindex = cpu;\n\tif (unlikely(index >= array->map.max_entries))\n\t\treturn -E2BIG;\n\n\tee = READ_ONCE(array->ptrs[index]);\n\tif (!ee)\n\t\treturn -ENOENT;\n\n\tevent = ee->event;\n\tif (unlikely(event->attr.type != PERF_TYPE_SOFTWARE ||\n\t\t     event->attr.config != PERF_COUNT_SW_BPF_OUTPUT))\n\t\treturn -EINVAL;\n\n\tif (unlikely(event->oncpu != cpu))\n\t\treturn -EOPNOTSUPP;\n\n\treturn perf_event_output(event, sd, regs);\n}\n\n \nstruct bpf_trace_sample_data {\n\tstruct perf_sample_data sds[3];\n};\n\nstatic DEFINE_PER_CPU(struct bpf_trace_sample_data, bpf_trace_sds);\nstatic DEFINE_PER_CPU(int, bpf_trace_nest_level);\nBPF_CALL_5(bpf_perf_event_output, struct pt_regs *, regs, struct bpf_map *, map,\n\t   u64, flags, void *, data, u64, size)\n{\n\tstruct bpf_trace_sample_data *sds;\n\tstruct perf_raw_record raw = {\n\t\t.frag = {\n\t\t\t.size = size,\n\t\t\t.data = data,\n\t\t},\n\t};\n\tstruct perf_sample_data *sd;\n\tint nest_level, err;\n\n\tpreempt_disable();\n\tsds = this_cpu_ptr(&bpf_trace_sds);\n\tnest_level = this_cpu_inc_return(bpf_trace_nest_level);\n\n\tif (WARN_ON_ONCE(nest_level > ARRAY_SIZE(sds->sds))) {\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tsd = &sds->sds[nest_level - 1];\n\n\tif (unlikely(flags & ~(BPF_F_INDEX_MASK))) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tperf_sample_data_init(sd, 0, 0);\n\tperf_sample_save_raw_data(sd, &raw);\n\n\terr = __bpf_perf_event_output(regs, map, flags, sd);\nout:\n\tthis_cpu_dec(bpf_trace_nest_level);\n\tpreempt_enable();\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_perf_event_output_proto = {\n\t.func\t\t= bpf_perf_event_output,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nstatic DEFINE_PER_CPU(int, bpf_event_output_nest_level);\nstruct bpf_nested_pt_regs {\n\tstruct pt_regs regs[3];\n};\nstatic DEFINE_PER_CPU(struct bpf_nested_pt_regs, bpf_pt_regs);\nstatic DEFINE_PER_CPU(struct bpf_trace_sample_data, bpf_misc_sds);\n\nu64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,\n\t\t     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy)\n{\n\tstruct perf_raw_frag frag = {\n\t\t.copy\t\t= ctx_copy,\n\t\t.size\t\t= ctx_size,\n\t\t.data\t\t= ctx,\n\t};\n\tstruct perf_raw_record raw = {\n\t\t.frag = {\n\t\t\t{\n\t\t\t\t.next\t= ctx_size ? &frag : NULL,\n\t\t\t},\n\t\t\t.size\t= meta_size,\n\t\t\t.data\t= meta,\n\t\t},\n\t};\n\tstruct perf_sample_data *sd;\n\tstruct pt_regs *regs;\n\tint nest_level;\n\tu64 ret;\n\n\tpreempt_disable();\n\tnest_level = this_cpu_inc_return(bpf_event_output_nest_level);\n\n\tif (WARN_ON_ONCE(nest_level > ARRAY_SIZE(bpf_misc_sds.sds))) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\tsd = this_cpu_ptr(&bpf_misc_sds.sds[nest_level - 1]);\n\tregs = this_cpu_ptr(&bpf_pt_regs.regs[nest_level - 1]);\n\n\tperf_fetch_caller_regs(regs);\n\tperf_sample_data_init(sd, 0, 0);\n\tperf_sample_save_raw_data(sd, &raw);\n\n\tret = __bpf_perf_event_output(regs, map, flags, sd);\nout:\n\tthis_cpu_dec(bpf_event_output_nest_level);\n\tpreempt_enable();\n\treturn ret;\n}\n\nBPF_CALL_0(bpf_get_current_task)\n{\n\treturn (long) current;\n}\n\nconst struct bpf_func_proto bpf_get_current_task_proto = {\n\t.func\t\t= bpf_get_current_task,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n};\n\nBPF_CALL_0(bpf_get_current_task_btf)\n{\n\treturn (unsigned long) current;\n}\n\nconst struct bpf_func_proto bpf_get_current_task_btf_proto = {\n\t.func\t\t= bpf_get_current_task_btf,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_PTR_TO_BTF_ID_TRUSTED,\n\t.ret_btf_id\t= &btf_tracing_ids[BTF_TRACING_TYPE_TASK],\n};\n\nBPF_CALL_1(bpf_task_pt_regs, struct task_struct *, task)\n{\n\treturn (unsigned long) task_pt_regs(task);\n}\n\nBTF_ID_LIST(bpf_task_pt_regs_ids)\nBTF_ID(struct, pt_regs)\n\nconst struct bpf_func_proto bpf_task_pt_regs_proto = {\n\t.func\t\t= bpf_task_pt_regs,\n\t.gpl_only\t= true,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &btf_tracing_ids[BTF_TRACING_TYPE_TASK],\n\t.ret_type\t= RET_PTR_TO_BTF_ID,\n\t.ret_btf_id\t= &bpf_task_pt_regs_ids[0],\n};\n\nBPF_CALL_2(bpf_current_task_under_cgroup, struct bpf_map *, map, u32, idx)\n{\n\tstruct bpf_array *array = container_of(map, struct bpf_array, map);\n\tstruct cgroup *cgrp;\n\n\tif (unlikely(idx >= array->map.max_entries))\n\t\treturn -E2BIG;\n\n\tcgrp = READ_ONCE(array->ptrs[idx]);\n\tif (unlikely(!cgrp))\n\t\treturn -EAGAIN;\n\n\treturn task_under_cgroup_hierarchy(current, cgrp);\n}\n\nstatic const struct bpf_func_proto bpf_current_task_under_cgroup_proto = {\n\t.func           = bpf_current_task_under_cgroup,\n\t.gpl_only       = false,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_CONST_MAP_PTR,\n\t.arg2_type      = ARG_ANYTHING,\n};\n\nstruct send_signal_irq_work {\n\tstruct irq_work irq_work;\n\tstruct task_struct *task;\n\tu32 sig;\n\tenum pid_type type;\n};\n\nstatic DEFINE_PER_CPU(struct send_signal_irq_work, send_signal_work);\n\nstatic void do_bpf_send_signal(struct irq_work *entry)\n{\n\tstruct send_signal_irq_work *work;\n\n\twork = container_of(entry, struct send_signal_irq_work, irq_work);\n\tgroup_send_sig_info(work->sig, SEND_SIG_PRIV, work->task, work->type);\n\tput_task_struct(work->task);\n}\n\nstatic int bpf_send_signal_common(u32 sig, enum pid_type type)\n{\n\tstruct send_signal_irq_work *work = NULL;\n\n\t \n\tif (unlikely(current->flags & (PF_KTHREAD | PF_EXITING)))\n\t\treturn -EPERM;\n\tif (unlikely(!nmi_uaccess_okay()))\n\t\treturn -EPERM;\n\t \n\tif (unlikely(is_global_init(current)))\n\t\treturn -EPERM;\n\n\tif (irqs_disabled()) {\n\t\t \n\t\tif (unlikely(!valid_signal(sig)))\n\t\t\treturn -EINVAL;\n\n\t\twork = this_cpu_ptr(&send_signal_work);\n\t\tif (irq_work_is_busy(&work->irq_work))\n\t\t\treturn -EBUSY;\n\n\t\t \n\t\twork->task = get_task_struct(current);\n\t\twork->sig = sig;\n\t\twork->type = type;\n\t\tirq_work_queue(&work->irq_work);\n\t\treturn 0;\n\t}\n\n\treturn group_send_sig_info(sig, SEND_SIG_PRIV, current, type);\n}\n\nBPF_CALL_1(bpf_send_signal, u32, sig)\n{\n\treturn bpf_send_signal_common(sig, PIDTYPE_TGID);\n}\n\nstatic const struct bpf_func_proto bpf_send_signal_proto = {\n\t.func\t\t= bpf_send_signal,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_send_signal_thread, u32, sig)\n{\n\treturn bpf_send_signal_common(sig, PIDTYPE_PID);\n}\n\nstatic const struct bpf_func_proto bpf_send_signal_thread_proto = {\n\t.func\t\t= bpf_send_signal_thread,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_3(bpf_d_path, struct path *, path, char *, buf, u32, sz)\n{\n\tstruct path copy;\n\tlong len;\n\tchar *p;\n\n\tif (!sz)\n\t\treturn 0;\n\n\t \n\tlen = copy_from_kernel_nofault(&copy, path, sizeof(*path));\n\tif (len < 0)\n\t\treturn len;\n\n\tp = d_path(&copy, buf, sz);\n\tif (IS_ERR(p)) {\n\t\tlen = PTR_ERR(p);\n\t} else {\n\t\tlen = buf + sz - p;\n\t\tmemmove(buf, p, len);\n\t}\n\n\treturn len;\n}\n\nBTF_SET_START(btf_allowlist_d_path)\n#ifdef CONFIG_SECURITY\nBTF_ID(func, security_file_permission)\nBTF_ID(func, security_inode_getattr)\nBTF_ID(func, security_file_open)\n#endif\n#ifdef CONFIG_SECURITY_PATH\nBTF_ID(func, security_path_truncate)\n#endif\nBTF_ID(func, vfs_truncate)\nBTF_ID(func, vfs_fallocate)\nBTF_ID(func, dentry_open)\nBTF_ID(func, vfs_getattr)\nBTF_ID(func, filp_close)\nBTF_SET_END(btf_allowlist_d_path)\n\nstatic bool bpf_d_path_allowed(const struct bpf_prog *prog)\n{\n\tif (prog->type == BPF_PROG_TYPE_TRACING &&\n\t    prog->expected_attach_type == BPF_TRACE_ITER)\n\t\treturn true;\n\n\tif (prog->type == BPF_PROG_TYPE_LSM)\n\t\treturn bpf_lsm_is_sleepable_hook(prog->aux->attach_btf_id);\n\n\treturn btf_id_set_contains(&btf_allowlist_d_path,\n\t\t\t\t   prog->aux->attach_btf_id);\n}\n\nBTF_ID_LIST_SINGLE(bpf_d_path_btf_ids, struct, path)\n\nstatic const struct bpf_func_proto bpf_d_path_proto = {\n\t.func\t\t= bpf_d_path,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_BTF_ID,\n\t.arg1_btf_id\t= &bpf_d_path_btf_ids[0],\n\t.arg2_type\t= ARG_PTR_TO_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.allowed\t= bpf_d_path_allowed,\n};\n\n#define BTF_F_ALL\t(BTF_F_COMPACT  | BTF_F_NONAME | \\\n\t\t\t BTF_F_PTR_RAW | BTF_F_ZERO)\n\nstatic int bpf_btf_printf_prepare(struct btf_ptr *ptr, u32 btf_ptr_size,\n\t\t\t\t  u64 flags, const struct btf **btf,\n\t\t\t\t  s32 *btf_id)\n{\n\tconst struct btf_type *t;\n\n\tif (unlikely(flags & ~(BTF_F_ALL)))\n\t\treturn -EINVAL;\n\n\tif (btf_ptr_size != sizeof(struct btf_ptr))\n\t\treturn -EINVAL;\n\n\t*btf = bpf_get_btf_vmlinux();\n\n\tif (IS_ERR_OR_NULL(*btf))\n\t\treturn IS_ERR(*btf) ? PTR_ERR(*btf) : -EINVAL;\n\n\tif (ptr->type_id > 0)\n\t\t*btf_id = ptr->type_id;\n\telse\n\t\treturn -EINVAL;\n\n\tif (*btf_id > 0)\n\t\tt = btf_type_by_id(*btf, *btf_id);\n\tif (*btf_id <= 0 || !t)\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\nBPF_CALL_5(bpf_snprintf_btf, char *, str, u32, str_size, struct btf_ptr *, ptr,\n\t   u32, btf_ptr_size, u64, flags)\n{\n\tconst struct btf *btf;\n\ts32 btf_id;\n\tint ret;\n\n\tret = bpf_btf_printf_prepare(ptr, btf_ptr_size, flags, &btf, &btf_id);\n\tif (ret)\n\t\treturn ret;\n\n\treturn btf_type_snprintf_show(btf, btf_id, ptr->ptr, str, str_size,\n\t\t\t\t      flags);\n}\n\nconst struct bpf_func_proto bpf_snprintf_btf_proto = {\n\t.func\t\t= bpf_snprintf_btf,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE,\n\t.arg3_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg4_type\t= ARG_CONST_SIZE,\n\t.arg5_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_1(bpf_get_func_ip_tracing, void *, ctx)\n{\n\t \n\treturn ((u64 *)ctx)[-2];\n}\n\nstatic const struct bpf_func_proto bpf_get_func_ip_proto_tracing = {\n\t.func\t\t= bpf_get_func_ip_tracing,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\n#ifdef CONFIG_X86_KERNEL_IBT\nstatic unsigned long get_entry_ip(unsigned long fentry_ip)\n{\n\tu32 instr;\n\n\t \n\tif (get_kernel_nofault(instr, (u32 *) fentry_ip - 1))\n\t\treturn fentry_ip;\n\tif (is_endbr(instr))\n\t\tfentry_ip -= ENDBR_INSN_SIZE;\n\treturn fentry_ip;\n}\n#else\n#define get_entry_ip(fentry_ip) fentry_ip\n#endif\n\nBPF_CALL_1(bpf_get_func_ip_kprobe, struct pt_regs *, regs)\n{\n\tstruct bpf_trace_run_ctx *run_ctx __maybe_unused;\n\tstruct kprobe *kp;\n\n#ifdef CONFIG_UPROBES\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_trace_run_ctx, run_ctx);\n\tif (run_ctx->is_uprobe)\n\t\treturn ((struct uprobe_dispatch_data *)current->utask->vaddr)->bp_addr;\n#endif\n\n\tkp = kprobe_running();\n\n\tif (!kp || !(kp->flags & KPROBE_FLAG_ON_FUNC_ENTRY))\n\t\treturn 0;\n\n\treturn get_entry_ip((uintptr_t)kp->addr);\n}\n\nstatic const struct bpf_func_proto bpf_get_func_ip_proto_kprobe = {\n\t.func\t\t= bpf_get_func_ip_kprobe,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_func_ip_kprobe_multi, struct pt_regs *, regs)\n{\n\treturn bpf_kprobe_multi_entry_ip(current->bpf_ctx);\n}\n\nstatic const struct bpf_func_proto bpf_get_func_ip_proto_kprobe_multi = {\n\t.func\t\t= bpf_get_func_ip_kprobe_multi,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_attach_cookie_kprobe_multi, struct pt_regs *, regs)\n{\n\treturn bpf_kprobe_multi_cookie(current->bpf_ctx);\n}\n\nstatic const struct bpf_func_proto bpf_get_attach_cookie_proto_kmulti = {\n\t.func\t\t= bpf_get_attach_cookie_kprobe_multi,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_func_ip_uprobe_multi, struct pt_regs *, regs)\n{\n\treturn bpf_uprobe_multi_entry_ip(current->bpf_ctx);\n}\n\nstatic const struct bpf_func_proto bpf_get_func_ip_proto_uprobe_multi = {\n\t.func\t\t= bpf_get_func_ip_uprobe_multi,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_attach_cookie_uprobe_multi, struct pt_regs *, regs)\n{\n\treturn bpf_uprobe_multi_cookie(current->bpf_ctx);\n}\n\nstatic const struct bpf_func_proto bpf_get_attach_cookie_proto_umulti = {\n\t.func\t\t= bpf_get_attach_cookie_uprobe_multi,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_attach_cookie_trace, void *, ctx)\n{\n\tstruct bpf_trace_run_ctx *run_ctx;\n\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_trace_run_ctx, run_ctx);\n\treturn run_ctx->bpf_cookie;\n}\n\nstatic const struct bpf_func_proto bpf_get_attach_cookie_proto_trace = {\n\t.func\t\t= bpf_get_attach_cookie_trace,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_attach_cookie_pe, struct bpf_perf_event_data_kern *, ctx)\n{\n\treturn ctx->event->bpf_cookie;\n}\n\nstatic const struct bpf_func_proto bpf_get_attach_cookie_proto_pe = {\n\t.func\t\t= bpf_get_attach_cookie_pe,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_1(bpf_get_attach_cookie_tracing, void *, ctx)\n{\n\tstruct bpf_trace_run_ctx *run_ctx;\n\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_trace_run_ctx, run_ctx);\n\treturn run_ctx->bpf_cookie;\n}\n\nstatic const struct bpf_func_proto bpf_get_attach_cookie_proto_tracing = {\n\t.func\t\t= bpf_get_attach_cookie_tracing,\n\t.gpl_only\t= false,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\nBPF_CALL_3(bpf_get_branch_snapshot, void *, buf, u32, size, u64, flags)\n{\n#ifndef CONFIG_X86\n\treturn -ENOENT;\n#else\n\tstatic const u32 br_entry_size = sizeof(struct perf_branch_entry);\n\tu32 entry_cnt = size / br_entry_size;\n\n\tentry_cnt = static_call(perf_snapshot_branch_stack)(buf, entry_cnt);\n\n\tif (unlikely(flags))\n\t\treturn -EINVAL;\n\n\tif (!entry_cnt)\n\t\treturn -ENOENT;\n\n\treturn entry_cnt * br_entry_size;\n#endif\n}\n\nstatic const struct bpf_func_proto bpf_get_branch_snapshot_proto = {\n\t.func\t\t= bpf_get_branch_snapshot,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg2_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_3(get_func_arg, void *, ctx, u32, n, u64 *, value)\n{\n\t \n\tu64 nr_args = ((u64 *)ctx)[-1];\n\n\tif ((u64) n >= nr_args)\n\t\treturn -EINVAL;\n\t*value = ((u64 *)ctx)[n];\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_get_func_arg_proto = {\n\t.func\t\t= get_func_arg,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_ANYTHING,\n\t.arg3_type\t= ARG_PTR_TO_LONG,\n};\n\nBPF_CALL_2(get_func_ret, void *, ctx, u64 *, value)\n{\n\t \n\tu64 nr_args = ((u64 *)ctx)[-1];\n\n\t*value = ((u64 *)ctx)[nr_args];\n\treturn 0;\n}\n\nstatic const struct bpf_func_proto bpf_get_func_ret_proto = {\n\t.func\t\t= get_func_ret,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_LONG,\n};\n\nBPF_CALL_1(get_func_arg_cnt, void *, ctx)\n{\n\t \n\treturn ((u64 *)ctx)[-1];\n}\n\nstatic const struct bpf_func_proto bpf_get_func_arg_cnt_proto = {\n\t.func\t\t= get_func_arg_cnt,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n};\n\n#ifdef CONFIG_KEYS\n__diag_push();\n__diag_ignore_all(\"-Wmissing-prototypes\",\n\t\t  \"kfuncs which will be used in BPF programs\");\n\n \n__bpf_kfunc struct bpf_key *bpf_lookup_user_key(u32 serial, u64 flags)\n{\n\tkey_ref_t key_ref;\n\tstruct bpf_key *bkey;\n\n\tif (flags & ~KEY_LOOKUP_ALL)\n\t\treturn NULL;\n\n\t \n\tkey_ref = lookup_user_key(serial, flags, KEY_DEFER_PERM_CHECK);\n\tif (IS_ERR(key_ref))\n\t\treturn NULL;\n\n\tbkey = kmalloc(sizeof(*bkey), GFP_KERNEL);\n\tif (!bkey) {\n\t\tkey_put(key_ref_to_ptr(key_ref));\n\t\treturn NULL;\n\t}\n\n\tbkey->key = key_ref_to_ptr(key_ref);\n\tbkey->has_ref = true;\n\n\treturn bkey;\n}\n\n \n__bpf_kfunc struct bpf_key *bpf_lookup_system_key(u64 id)\n{\n\tstruct bpf_key *bkey;\n\n\tif (system_keyring_id_check(id) < 0)\n\t\treturn NULL;\n\n\tbkey = kmalloc(sizeof(*bkey), GFP_ATOMIC);\n\tif (!bkey)\n\t\treturn NULL;\n\n\tbkey->key = (struct key *)(unsigned long)id;\n\tbkey->has_ref = false;\n\n\treturn bkey;\n}\n\n \n__bpf_kfunc void bpf_key_put(struct bpf_key *bkey)\n{\n\tif (bkey->has_ref)\n\t\tkey_put(bkey->key);\n\n\tkfree(bkey);\n}\n\n#ifdef CONFIG_SYSTEM_DATA_VERIFICATION\n \n__bpf_kfunc int bpf_verify_pkcs7_signature(struct bpf_dynptr_kern *data_ptr,\n\t\t\t       struct bpf_dynptr_kern *sig_ptr,\n\t\t\t       struct bpf_key *trusted_keyring)\n{\n\tint ret;\n\n\tif (trusted_keyring->has_ref) {\n\t\t \n\t\tret = key_validate(trusted_keyring->key);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn verify_pkcs7_signature(data_ptr->data,\n\t\t\t\t      __bpf_dynptr_size(data_ptr),\n\t\t\t\t      sig_ptr->data,\n\t\t\t\t      __bpf_dynptr_size(sig_ptr),\n\t\t\t\t      trusted_keyring->key,\n\t\t\t\t      VERIFYING_UNSPECIFIED_SIGNATURE, NULL,\n\t\t\t\t      NULL);\n}\n#endif  \n\n__diag_pop();\n\nBTF_SET8_START(key_sig_kfunc_set)\nBTF_ID_FLAGS(func, bpf_lookup_user_key, KF_ACQUIRE | KF_RET_NULL | KF_SLEEPABLE)\nBTF_ID_FLAGS(func, bpf_lookup_system_key, KF_ACQUIRE | KF_RET_NULL)\nBTF_ID_FLAGS(func, bpf_key_put, KF_RELEASE)\n#ifdef CONFIG_SYSTEM_DATA_VERIFICATION\nBTF_ID_FLAGS(func, bpf_verify_pkcs7_signature, KF_SLEEPABLE)\n#endif\nBTF_SET8_END(key_sig_kfunc_set)\n\nstatic const struct btf_kfunc_id_set bpf_key_sig_kfunc_set = {\n\t.owner = THIS_MODULE,\n\t.set = &key_sig_kfunc_set,\n};\n\nstatic int __init bpf_key_sig_kfuncs_init(void)\n{\n\treturn register_btf_kfunc_id_set(BPF_PROG_TYPE_TRACING,\n\t\t\t\t\t &bpf_key_sig_kfunc_set);\n}\n\nlate_initcall(bpf_key_sig_kfuncs_init);\n#endif  \n\nstatic const struct bpf_func_proto *\nbpf_tracing_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_map_lookup_elem:\n\t\treturn &bpf_map_lookup_elem_proto;\n\tcase BPF_FUNC_map_update_elem:\n\t\treturn &bpf_map_update_elem_proto;\n\tcase BPF_FUNC_map_delete_elem:\n\t\treturn &bpf_map_delete_elem_proto;\n\tcase BPF_FUNC_map_push_elem:\n\t\treturn &bpf_map_push_elem_proto;\n\tcase BPF_FUNC_map_pop_elem:\n\t\treturn &bpf_map_pop_elem_proto;\n\tcase BPF_FUNC_map_peek_elem:\n\t\treturn &bpf_map_peek_elem_proto;\n\tcase BPF_FUNC_map_lookup_percpu_elem:\n\t\treturn &bpf_map_lookup_percpu_elem_proto;\n\tcase BPF_FUNC_ktime_get_ns:\n\t\treturn &bpf_ktime_get_ns_proto;\n\tcase BPF_FUNC_ktime_get_boot_ns:\n\t\treturn &bpf_ktime_get_boot_ns_proto;\n\tcase BPF_FUNC_tail_call:\n\t\treturn &bpf_tail_call_proto;\n\tcase BPF_FUNC_get_current_pid_tgid:\n\t\treturn &bpf_get_current_pid_tgid_proto;\n\tcase BPF_FUNC_get_current_task:\n\t\treturn &bpf_get_current_task_proto;\n\tcase BPF_FUNC_get_current_task_btf:\n\t\treturn &bpf_get_current_task_btf_proto;\n\tcase BPF_FUNC_task_pt_regs:\n\t\treturn &bpf_task_pt_regs_proto;\n\tcase BPF_FUNC_get_current_uid_gid:\n\t\treturn &bpf_get_current_uid_gid_proto;\n\tcase BPF_FUNC_get_current_comm:\n\t\treturn &bpf_get_current_comm_proto;\n\tcase BPF_FUNC_trace_printk:\n\t\treturn bpf_get_trace_printk_proto();\n\tcase BPF_FUNC_get_smp_processor_id:\n\t\treturn &bpf_get_smp_processor_id_proto;\n\tcase BPF_FUNC_get_numa_node_id:\n\t\treturn &bpf_get_numa_node_id_proto;\n\tcase BPF_FUNC_perf_event_read:\n\t\treturn &bpf_perf_event_read_proto;\n\tcase BPF_FUNC_current_task_under_cgroup:\n\t\treturn &bpf_current_task_under_cgroup_proto;\n\tcase BPF_FUNC_get_prandom_u32:\n\t\treturn &bpf_get_prandom_u32_proto;\n\tcase BPF_FUNC_probe_write_user:\n\t\treturn security_locked_down(LOCKDOWN_BPF_WRITE_USER) < 0 ?\n\t\t       NULL : bpf_get_probe_write_proto();\n\tcase BPF_FUNC_probe_read_user:\n\t\treturn &bpf_probe_read_user_proto;\n\tcase BPF_FUNC_probe_read_kernel:\n\t\treturn security_locked_down(LOCKDOWN_BPF_READ_KERNEL) < 0 ?\n\t\t       NULL : &bpf_probe_read_kernel_proto;\n\tcase BPF_FUNC_probe_read_user_str:\n\t\treturn &bpf_probe_read_user_str_proto;\n\tcase BPF_FUNC_probe_read_kernel_str:\n\t\treturn security_locked_down(LOCKDOWN_BPF_READ_KERNEL) < 0 ?\n\t\t       NULL : &bpf_probe_read_kernel_str_proto;\n#ifdef CONFIG_ARCH_HAS_NON_OVERLAPPING_ADDRESS_SPACE\n\tcase BPF_FUNC_probe_read:\n\t\treturn security_locked_down(LOCKDOWN_BPF_READ_KERNEL) < 0 ?\n\t\t       NULL : &bpf_probe_read_compat_proto;\n\tcase BPF_FUNC_probe_read_str:\n\t\treturn security_locked_down(LOCKDOWN_BPF_READ_KERNEL) < 0 ?\n\t\t       NULL : &bpf_probe_read_compat_str_proto;\n#endif\n#ifdef CONFIG_CGROUPS\n\tcase BPF_FUNC_cgrp_storage_get:\n\t\treturn &bpf_cgrp_storage_get_proto;\n\tcase BPF_FUNC_cgrp_storage_delete:\n\t\treturn &bpf_cgrp_storage_delete_proto;\n#endif\n\tcase BPF_FUNC_send_signal:\n\t\treturn &bpf_send_signal_proto;\n\tcase BPF_FUNC_send_signal_thread:\n\t\treturn &bpf_send_signal_thread_proto;\n\tcase BPF_FUNC_perf_event_read_value:\n\t\treturn &bpf_perf_event_read_value_proto;\n\tcase BPF_FUNC_get_ns_current_pid_tgid:\n\t\treturn &bpf_get_ns_current_pid_tgid_proto;\n\tcase BPF_FUNC_ringbuf_output:\n\t\treturn &bpf_ringbuf_output_proto;\n\tcase BPF_FUNC_ringbuf_reserve:\n\t\treturn &bpf_ringbuf_reserve_proto;\n\tcase BPF_FUNC_ringbuf_submit:\n\t\treturn &bpf_ringbuf_submit_proto;\n\tcase BPF_FUNC_ringbuf_discard:\n\t\treturn &bpf_ringbuf_discard_proto;\n\tcase BPF_FUNC_ringbuf_query:\n\t\treturn &bpf_ringbuf_query_proto;\n\tcase BPF_FUNC_jiffies64:\n\t\treturn &bpf_jiffies64_proto;\n\tcase BPF_FUNC_get_task_stack:\n\t\treturn &bpf_get_task_stack_proto;\n\tcase BPF_FUNC_copy_from_user:\n\t\treturn &bpf_copy_from_user_proto;\n\tcase BPF_FUNC_copy_from_user_task:\n\t\treturn &bpf_copy_from_user_task_proto;\n\tcase BPF_FUNC_snprintf_btf:\n\t\treturn &bpf_snprintf_btf_proto;\n\tcase BPF_FUNC_per_cpu_ptr:\n\t\treturn &bpf_per_cpu_ptr_proto;\n\tcase BPF_FUNC_this_cpu_ptr:\n\t\treturn &bpf_this_cpu_ptr_proto;\n\tcase BPF_FUNC_task_storage_get:\n\t\tif (bpf_prog_check_recur(prog))\n\t\t\treturn &bpf_task_storage_get_recur_proto;\n\t\treturn &bpf_task_storage_get_proto;\n\tcase BPF_FUNC_task_storage_delete:\n\t\tif (bpf_prog_check_recur(prog))\n\t\t\treturn &bpf_task_storage_delete_recur_proto;\n\t\treturn &bpf_task_storage_delete_proto;\n\tcase BPF_FUNC_for_each_map_elem:\n\t\treturn &bpf_for_each_map_elem_proto;\n\tcase BPF_FUNC_snprintf:\n\t\treturn &bpf_snprintf_proto;\n\tcase BPF_FUNC_get_func_ip:\n\t\treturn &bpf_get_func_ip_proto_tracing;\n\tcase BPF_FUNC_get_branch_snapshot:\n\t\treturn &bpf_get_branch_snapshot_proto;\n\tcase BPF_FUNC_find_vma:\n\t\treturn &bpf_find_vma_proto;\n\tcase BPF_FUNC_trace_vprintk:\n\t\treturn bpf_get_trace_vprintk_proto();\n\tdefault:\n\t\treturn bpf_base_func_proto(func_id);\n\t}\n}\n\nstatic const struct bpf_func_proto *\nkprobe_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_perf_event_output_proto;\n\tcase BPF_FUNC_get_stackid:\n\t\treturn &bpf_get_stackid_proto;\n\tcase BPF_FUNC_get_stack:\n\t\treturn &bpf_get_stack_proto;\n#ifdef CONFIG_BPF_KPROBE_OVERRIDE\n\tcase BPF_FUNC_override_return:\n\t\treturn &bpf_override_return_proto;\n#endif\n\tcase BPF_FUNC_get_func_ip:\n\t\tif (prog->expected_attach_type == BPF_TRACE_KPROBE_MULTI)\n\t\t\treturn &bpf_get_func_ip_proto_kprobe_multi;\n\t\tif (prog->expected_attach_type == BPF_TRACE_UPROBE_MULTI)\n\t\t\treturn &bpf_get_func_ip_proto_uprobe_multi;\n\t\treturn &bpf_get_func_ip_proto_kprobe;\n\tcase BPF_FUNC_get_attach_cookie:\n\t\tif (prog->expected_attach_type == BPF_TRACE_KPROBE_MULTI)\n\t\t\treturn &bpf_get_attach_cookie_proto_kmulti;\n\t\tif (prog->expected_attach_type == BPF_TRACE_UPROBE_MULTI)\n\t\t\treturn &bpf_get_attach_cookie_proto_umulti;\n\t\treturn &bpf_get_attach_cookie_proto_trace;\n\tdefault:\n\t\treturn bpf_tracing_func_proto(func_id, prog);\n\t}\n}\n\n \nstatic bool kprobe_prog_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\tif (off < 0 || off >= sizeof(struct pt_regs))\n\t\treturn false;\n\tif (type != BPF_READ)\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\t \n\tif (off + size > sizeof(struct pt_regs))\n\t\treturn false;\n\n\treturn true;\n}\n\nconst struct bpf_verifier_ops kprobe_verifier_ops = {\n\t.get_func_proto  = kprobe_prog_func_proto,\n\t.is_valid_access = kprobe_prog_is_valid_access,\n};\n\nconst struct bpf_prog_ops kprobe_prog_ops = {\n};\n\nBPF_CALL_5(bpf_perf_event_output_tp, void *, tp_buff, struct bpf_map *, map,\n\t   u64, flags, void *, data, u64, size)\n{\n\tstruct pt_regs *regs = *(struct pt_regs **)tp_buff;\n\n\t \n\treturn ____bpf_perf_event_output(regs, map, flags, data, size);\n}\n\nstatic const struct bpf_func_proto bpf_perf_event_output_proto_tp = {\n\t.func\t\t= bpf_perf_event_output_tp,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nBPF_CALL_3(bpf_get_stackid_tp, void *, tp_buff, struct bpf_map *, map,\n\t   u64, flags)\n{\n\tstruct pt_regs *regs = *(struct pt_regs **)tp_buff;\n\n\t \n\treturn bpf_get_stackid((unsigned long) regs, (unsigned long) map,\n\t\t\t       flags, 0, 0);\n}\n\nstatic const struct bpf_func_proto bpf_get_stackid_proto_tp = {\n\t.func\t\t= bpf_get_stackid_tp,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_stack_tp, void *, tp_buff, void *, buf, u32, size,\n\t   u64, flags)\n{\n\tstruct pt_regs *regs = *(struct pt_regs **)tp_buff;\n\n\treturn bpf_get_stack((unsigned long) regs, (unsigned long) buf,\n\t\t\t     (unsigned long) size, flags, 0);\n}\n\nstatic const struct bpf_func_proto bpf_get_stack_proto_tp = {\n\t.func\t\t= bpf_get_stack_tp,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_UNINIT_MEM,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto *\ntp_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_perf_event_output_proto_tp;\n\tcase BPF_FUNC_get_stackid:\n\t\treturn &bpf_get_stackid_proto_tp;\n\tcase BPF_FUNC_get_stack:\n\t\treturn &bpf_get_stack_proto_tp;\n\tcase BPF_FUNC_get_attach_cookie:\n\t\treturn &bpf_get_attach_cookie_proto_trace;\n\tdefault:\n\t\treturn bpf_tracing_func_proto(func_id, prog);\n\t}\n}\n\nstatic bool tp_prog_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t    const struct bpf_prog *prog,\n\t\t\t\t    struct bpf_insn_access_aux *info)\n{\n\tif (off < sizeof(void *) || off >= PERF_MAX_TRACE_SIZE)\n\t\treturn false;\n\tif (type != BPF_READ)\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\n\tBUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(__u64));\n\treturn true;\n}\n\nconst struct bpf_verifier_ops tracepoint_verifier_ops = {\n\t.get_func_proto  = tp_prog_func_proto,\n\t.is_valid_access = tp_prog_is_valid_access,\n};\n\nconst struct bpf_prog_ops tracepoint_prog_ops = {\n};\n\nBPF_CALL_3(bpf_perf_prog_read_value, struct bpf_perf_event_data_kern *, ctx,\n\t   struct bpf_perf_event_value *, buf, u32, size)\n{\n\tint err = -EINVAL;\n\n\tif (unlikely(size != sizeof(struct bpf_perf_event_value)))\n\t\tgoto clear;\n\terr = perf_event_read_local(ctx->event, &buf->counter, &buf->enabled,\n\t\t\t\t    &buf->running);\n\tif (unlikely(err))\n\t\tgoto clear;\n\treturn 0;\nclear:\n\tmemset(buf, 0, size);\n\treturn err;\n}\n\nstatic const struct bpf_func_proto bpf_perf_prog_read_value_proto = {\n         .func           = bpf_perf_prog_read_value,\n         .gpl_only       = true,\n         .ret_type       = RET_INTEGER,\n         .arg1_type      = ARG_PTR_TO_CTX,\n         .arg2_type      = ARG_PTR_TO_UNINIT_MEM,\n         .arg3_type      = ARG_CONST_SIZE,\n};\n\nBPF_CALL_4(bpf_read_branch_records, struct bpf_perf_event_data_kern *, ctx,\n\t   void *, buf, u32, size, u64, flags)\n{\n\tstatic const u32 br_entry_size = sizeof(struct perf_branch_entry);\n\tstruct perf_branch_stack *br_stack = ctx->data->br_stack;\n\tu32 to_copy;\n\n\tif (unlikely(flags & ~BPF_F_GET_BRANCH_RECORDS_SIZE))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!(ctx->data->sample_flags & PERF_SAMPLE_BRANCH_STACK)))\n\t\treturn -ENOENT;\n\n\tif (unlikely(!br_stack))\n\t\treturn -ENOENT;\n\n\tif (flags & BPF_F_GET_BRANCH_RECORDS_SIZE)\n\t\treturn br_stack->nr * br_entry_size;\n\n\tif (!buf || (size % br_entry_size != 0))\n\t\treturn -EINVAL;\n\n\tto_copy = min_t(u32, br_stack->nr * br_entry_size, size);\n\tmemcpy(buf, br_stack->entries, to_copy);\n\n\treturn to_copy;\n}\n\nstatic const struct bpf_func_proto bpf_read_branch_records_proto = {\n\t.func           = bpf_read_branch_records,\n\t.gpl_only       = true,\n\t.ret_type       = RET_INTEGER,\n\t.arg1_type      = ARG_PTR_TO_CTX,\n\t.arg2_type      = ARG_PTR_TO_MEM_OR_NULL,\n\t.arg3_type      = ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type      = ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto *\npe_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_perf_event_output_proto_tp;\n\tcase BPF_FUNC_get_stackid:\n\t\treturn &bpf_get_stackid_proto_pe;\n\tcase BPF_FUNC_get_stack:\n\t\treturn &bpf_get_stack_proto_pe;\n\tcase BPF_FUNC_perf_prog_read_value:\n\t\treturn &bpf_perf_prog_read_value_proto;\n\tcase BPF_FUNC_read_branch_records:\n\t\treturn &bpf_read_branch_records_proto;\n\tcase BPF_FUNC_get_attach_cookie:\n\t\treturn &bpf_get_attach_cookie_proto_pe;\n\tdefault:\n\t\treturn bpf_tracing_func_proto(func_id, prog);\n\t}\n}\n\n \nstruct bpf_raw_tp_regs {\n\tstruct pt_regs regs[3];\n};\nstatic DEFINE_PER_CPU(struct bpf_raw_tp_regs, bpf_raw_tp_regs);\nstatic DEFINE_PER_CPU(int, bpf_raw_tp_nest_level);\nstatic struct pt_regs *get_bpf_raw_tp_regs(void)\n{\n\tstruct bpf_raw_tp_regs *tp_regs = this_cpu_ptr(&bpf_raw_tp_regs);\n\tint nest_level = this_cpu_inc_return(bpf_raw_tp_nest_level);\n\n\tif (WARN_ON_ONCE(nest_level > ARRAY_SIZE(tp_regs->regs))) {\n\t\tthis_cpu_dec(bpf_raw_tp_nest_level);\n\t\treturn ERR_PTR(-EBUSY);\n\t}\n\n\treturn &tp_regs->regs[nest_level - 1];\n}\n\nstatic void put_bpf_raw_tp_regs(void)\n{\n\tthis_cpu_dec(bpf_raw_tp_nest_level);\n}\n\nBPF_CALL_5(bpf_perf_event_output_raw_tp, struct bpf_raw_tracepoint_args *, args,\n\t   struct bpf_map *, map, u64, flags, void *, data, u64, size)\n{\n\tstruct pt_regs *regs = get_bpf_raw_tp_regs();\n\tint ret;\n\n\tif (IS_ERR(regs))\n\t\treturn PTR_ERR(regs);\n\n\tperf_fetch_caller_regs(regs);\n\tret = ____bpf_perf_event_output(regs, map, flags, data, size);\n\n\tput_bpf_raw_tp_regs();\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_perf_event_output_proto_raw_tp = {\n\t.func\t\t= bpf_perf_event_output_raw_tp,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n\t.arg4_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg5_type\t= ARG_CONST_SIZE_OR_ZERO,\n};\n\nextern const struct bpf_func_proto bpf_skb_output_proto;\nextern const struct bpf_func_proto bpf_xdp_output_proto;\nextern const struct bpf_func_proto bpf_xdp_get_buff_len_trace_proto;\n\nBPF_CALL_3(bpf_get_stackid_raw_tp, struct bpf_raw_tracepoint_args *, args,\n\t   struct bpf_map *, map, u64, flags)\n{\n\tstruct pt_regs *regs = get_bpf_raw_tp_regs();\n\tint ret;\n\n\tif (IS_ERR(regs))\n\t\treturn PTR_ERR(regs);\n\n\tperf_fetch_caller_regs(regs);\n\t \n\tret = bpf_get_stackid((unsigned long) regs, (unsigned long) map,\n\t\t\t      flags, 0, 0);\n\tput_bpf_raw_tp_regs();\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_get_stackid_proto_raw_tp = {\n\t.func\t\t= bpf_get_stackid_raw_tp,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_CONST_MAP_PTR,\n\t.arg3_type\t= ARG_ANYTHING,\n};\n\nBPF_CALL_4(bpf_get_stack_raw_tp, struct bpf_raw_tracepoint_args *, args,\n\t   void *, buf, u32, size, u64, flags)\n{\n\tstruct pt_regs *regs = get_bpf_raw_tp_regs();\n\tint ret;\n\n\tif (IS_ERR(regs))\n\t\treturn PTR_ERR(regs);\n\n\tperf_fetch_caller_regs(regs);\n\tret = bpf_get_stack((unsigned long) regs, (unsigned long) buf,\n\t\t\t    (unsigned long) size, flags, 0);\n\tput_bpf_raw_tp_regs();\n\treturn ret;\n}\n\nstatic const struct bpf_func_proto bpf_get_stack_proto_raw_tp = {\n\t.func\t\t= bpf_get_stack_raw_tp,\n\t.gpl_only\t= true,\n\t.ret_type\t= RET_INTEGER,\n\t.arg1_type\t= ARG_PTR_TO_CTX,\n\t.arg2_type\t= ARG_PTR_TO_MEM | MEM_RDONLY,\n\t.arg3_type\t= ARG_CONST_SIZE_OR_ZERO,\n\t.arg4_type\t= ARG_ANYTHING,\n};\n\nstatic const struct bpf_func_proto *\nraw_tp_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tswitch (func_id) {\n\tcase BPF_FUNC_perf_event_output:\n\t\treturn &bpf_perf_event_output_proto_raw_tp;\n\tcase BPF_FUNC_get_stackid:\n\t\treturn &bpf_get_stackid_proto_raw_tp;\n\tcase BPF_FUNC_get_stack:\n\t\treturn &bpf_get_stack_proto_raw_tp;\n\tdefault:\n\t\treturn bpf_tracing_func_proto(func_id, prog);\n\t}\n}\n\nconst struct bpf_func_proto *\ntracing_prog_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)\n{\n\tconst struct bpf_func_proto *fn;\n\n\tswitch (func_id) {\n#ifdef CONFIG_NET\n\tcase BPF_FUNC_skb_output:\n\t\treturn &bpf_skb_output_proto;\n\tcase BPF_FUNC_xdp_output:\n\t\treturn &bpf_xdp_output_proto;\n\tcase BPF_FUNC_skc_to_tcp6_sock:\n\t\treturn &bpf_skc_to_tcp6_sock_proto;\n\tcase BPF_FUNC_skc_to_tcp_sock:\n\t\treturn &bpf_skc_to_tcp_sock_proto;\n\tcase BPF_FUNC_skc_to_tcp_timewait_sock:\n\t\treturn &bpf_skc_to_tcp_timewait_sock_proto;\n\tcase BPF_FUNC_skc_to_tcp_request_sock:\n\t\treturn &bpf_skc_to_tcp_request_sock_proto;\n\tcase BPF_FUNC_skc_to_udp6_sock:\n\t\treturn &bpf_skc_to_udp6_sock_proto;\n\tcase BPF_FUNC_skc_to_unix_sock:\n\t\treturn &bpf_skc_to_unix_sock_proto;\n\tcase BPF_FUNC_skc_to_mptcp_sock:\n\t\treturn &bpf_skc_to_mptcp_sock_proto;\n\tcase BPF_FUNC_sk_storage_get:\n\t\treturn &bpf_sk_storage_get_tracing_proto;\n\tcase BPF_FUNC_sk_storage_delete:\n\t\treturn &bpf_sk_storage_delete_tracing_proto;\n\tcase BPF_FUNC_sock_from_file:\n\t\treturn &bpf_sock_from_file_proto;\n\tcase BPF_FUNC_get_socket_cookie:\n\t\treturn &bpf_get_socket_ptr_cookie_proto;\n\tcase BPF_FUNC_xdp_get_buff_len:\n\t\treturn &bpf_xdp_get_buff_len_trace_proto;\n#endif\n\tcase BPF_FUNC_seq_printf:\n\t\treturn prog->expected_attach_type == BPF_TRACE_ITER ?\n\t\t       &bpf_seq_printf_proto :\n\t\t       NULL;\n\tcase BPF_FUNC_seq_write:\n\t\treturn prog->expected_attach_type == BPF_TRACE_ITER ?\n\t\t       &bpf_seq_write_proto :\n\t\t       NULL;\n\tcase BPF_FUNC_seq_printf_btf:\n\t\treturn prog->expected_attach_type == BPF_TRACE_ITER ?\n\t\t       &bpf_seq_printf_btf_proto :\n\t\t       NULL;\n\tcase BPF_FUNC_d_path:\n\t\treturn &bpf_d_path_proto;\n\tcase BPF_FUNC_get_func_arg:\n\t\treturn bpf_prog_has_trampoline(prog) ? &bpf_get_func_arg_proto : NULL;\n\tcase BPF_FUNC_get_func_ret:\n\t\treturn bpf_prog_has_trampoline(prog) ? &bpf_get_func_ret_proto : NULL;\n\tcase BPF_FUNC_get_func_arg_cnt:\n\t\treturn bpf_prog_has_trampoline(prog) ? &bpf_get_func_arg_cnt_proto : NULL;\n\tcase BPF_FUNC_get_attach_cookie:\n\t\treturn bpf_prog_has_trampoline(prog) ? &bpf_get_attach_cookie_proto_tracing : NULL;\n\tdefault:\n\t\tfn = raw_tp_prog_func_proto(func_id, prog);\n\t\tif (!fn && prog->expected_attach_type == BPF_TRACE_ITER)\n\t\t\tfn = bpf_iter_get_func_proto(func_id, prog);\n\t\treturn fn;\n\t}\n}\n\nstatic bool raw_tp_prog_is_valid_access(int off, int size,\n\t\t\t\t\tenum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\treturn bpf_tracing_ctx_access(off, size, type);\n}\n\nstatic bool tracing_prog_is_valid_access(int off, int size,\n\t\t\t\t\t enum bpf_access_type type,\n\t\t\t\t\t const struct bpf_prog *prog,\n\t\t\t\t\t struct bpf_insn_access_aux *info)\n{\n\treturn bpf_tracing_btf_ctx_access(off, size, type, prog, info);\n}\n\nint __weak bpf_prog_test_run_tracing(struct bpf_prog *prog,\n\t\t\t\t     const union bpf_attr *kattr,\n\t\t\t\t     union bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\nconst struct bpf_verifier_ops raw_tracepoint_verifier_ops = {\n\t.get_func_proto  = raw_tp_prog_func_proto,\n\t.is_valid_access = raw_tp_prog_is_valid_access,\n};\n\nconst struct bpf_prog_ops raw_tracepoint_prog_ops = {\n#ifdef CONFIG_NET\n\t.test_run = bpf_prog_test_run_raw_tp,\n#endif\n};\n\nconst struct bpf_verifier_ops tracing_verifier_ops = {\n\t.get_func_proto  = tracing_prog_func_proto,\n\t.is_valid_access = tracing_prog_is_valid_access,\n};\n\nconst struct bpf_prog_ops tracing_prog_ops = {\n\t.test_run = bpf_prog_test_run_tracing,\n};\n\nstatic bool raw_tp_writable_prog_is_valid_access(int off, int size,\n\t\t\t\t\t\t enum bpf_access_type type,\n\t\t\t\t\t\t const struct bpf_prog *prog,\n\t\t\t\t\t\t struct bpf_insn_access_aux *info)\n{\n\tif (off == 0) {\n\t\tif (size != sizeof(u64) || type != BPF_READ)\n\t\t\treturn false;\n\t\tinfo->reg_type = PTR_TO_TP_BUFFER;\n\t}\n\treturn raw_tp_prog_is_valid_access(off, size, type, prog, info);\n}\n\nconst struct bpf_verifier_ops raw_tracepoint_writable_verifier_ops = {\n\t.get_func_proto  = raw_tp_prog_func_proto,\n\t.is_valid_access = raw_tp_writable_prog_is_valid_access,\n};\n\nconst struct bpf_prog_ops raw_tracepoint_writable_prog_ops = {\n};\n\nstatic bool pe_prog_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t    const struct bpf_prog *prog,\n\t\t\t\t    struct bpf_insn_access_aux *info)\n{\n\tconst int size_u64 = sizeof(u64);\n\n\tif (off < 0 || off >= sizeof(struct bpf_perf_event_data))\n\t\treturn false;\n\tif (type != BPF_READ)\n\t\treturn false;\n\tif (off % size != 0) {\n\t\tif (sizeof(unsigned long) != 4)\n\t\t\treturn false;\n\t\tif (size != 8)\n\t\t\treturn false;\n\t\tif (off % size != 4)\n\t\t\treturn false;\n\t}\n\n\tswitch (off) {\n\tcase bpf_ctx_range(struct bpf_perf_event_data, sample_period):\n\t\tbpf_ctx_record_field_size(info, size_u64);\n\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_u64))\n\t\t\treturn false;\n\t\tbreak;\n\tcase bpf_ctx_range(struct bpf_perf_event_data, addr):\n\t\tbpf_ctx_record_field_size(info, size_u64);\n\t\tif (!bpf_ctx_narrow_access_ok(off, size, size_u64))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\tif (size != sizeof(long))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic u32 pe_prog_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t      const struct bpf_insn *si,\n\t\t\t\t      struct bpf_insn *insn_buf,\n\t\t\t\t      struct bpf_prog *prog, u32 *target_size)\n{\n\tstruct bpf_insn *insn = insn_buf;\n\n\tswitch (si->off) {\n\tcase offsetof(struct bpf_perf_event_data, sample_period):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_perf_event_data_kern,\n\t\t\t\t\t\t       data), si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_perf_event_data_kern, data));\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct perf_sample_data, period, 8,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tcase offsetof(struct bpf_perf_event_data, addr):\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_perf_event_data_kern,\n\t\t\t\t\t\t       data), si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_perf_event_data_kern, data));\n\t\t*insn++ = BPF_LDX_MEM(BPF_DW, si->dst_reg, si->dst_reg,\n\t\t\t\t      bpf_target_off(struct perf_sample_data, addr, 8,\n\t\t\t\t\t\t     target_size));\n\t\tbreak;\n\tdefault:\n\t\t*insn++ = BPF_LDX_MEM(BPF_FIELD_SIZEOF(struct bpf_perf_event_data_kern,\n\t\t\t\t\t\t       regs), si->dst_reg, si->src_reg,\n\t\t\t\t      offsetof(struct bpf_perf_event_data_kern, regs));\n\t\t*insn++ = BPF_LDX_MEM(BPF_SIZEOF(long), si->dst_reg, si->dst_reg,\n\t\t\t\t      si->off);\n\t\tbreak;\n\t}\n\n\treturn insn - insn_buf;\n}\n\nconst struct bpf_verifier_ops perf_event_verifier_ops = {\n\t.get_func_proto\t\t= pe_prog_func_proto,\n\t.is_valid_access\t= pe_prog_is_valid_access,\n\t.convert_ctx_access\t= pe_prog_convert_ctx_access,\n};\n\nconst struct bpf_prog_ops perf_event_prog_ops = {\n};\n\nstatic DEFINE_MUTEX(bpf_event_mutex);\n\n#define BPF_TRACE_MAX_PROGS 64\n\nint perf_event_attach_bpf_prog(struct perf_event *event,\n\t\t\t       struct bpf_prog *prog,\n\t\t\t       u64 bpf_cookie)\n{\n\tstruct bpf_prog_array *old_array;\n\tstruct bpf_prog_array *new_array;\n\tint ret = -EEXIST;\n\n\t \n\tif (prog->kprobe_override &&\n\t    (!trace_kprobe_on_func_entry(event->tp_event) ||\n\t     !trace_kprobe_error_injectable(event->tp_event)))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&bpf_event_mutex);\n\n\tif (event->prog)\n\t\tgoto unlock;\n\n\told_array = bpf_event_rcu_dereference(event->tp_event->prog_array);\n\tif (old_array &&\n\t    bpf_prog_array_length(old_array) >= BPF_TRACE_MAX_PROGS) {\n\t\tret = -E2BIG;\n\t\tgoto unlock;\n\t}\n\n\tret = bpf_prog_array_copy(old_array, NULL, prog, bpf_cookie, &new_array);\n\tif (ret < 0)\n\t\tgoto unlock;\n\n\t \n\tevent->prog = prog;\n\tevent->bpf_cookie = bpf_cookie;\n\trcu_assign_pointer(event->tp_event->prog_array, new_array);\n\tbpf_prog_array_free_sleepable(old_array);\n\nunlock:\n\tmutex_unlock(&bpf_event_mutex);\n\treturn ret;\n}\n\nvoid perf_event_detach_bpf_prog(struct perf_event *event)\n{\n\tstruct bpf_prog_array *old_array;\n\tstruct bpf_prog_array *new_array;\n\tint ret;\n\n\tmutex_lock(&bpf_event_mutex);\n\n\tif (!event->prog)\n\t\tgoto unlock;\n\n\told_array = bpf_event_rcu_dereference(event->tp_event->prog_array);\n\tret = bpf_prog_array_copy(old_array, event->prog, NULL, 0, &new_array);\n\tif (ret == -ENOENT)\n\t\tgoto unlock;\n\tif (ret < 0) {\n\t\tbpf_prog_array_delete_safe(old_array, event->prog);\n\t} else {\n\t\trcu_assign_pointer(event->tp_event->prog_array, new_array);\n\t\tbpf_prog_array_free_sleepable(old_array);\n\t}\n\n\tbpf_prog_put(event->prog);\n\tevent->prog = NULL;\n\nunlock:\n\tmutex_unlock(&bpf_event_mutex);\n}\n\nint perf_event_query_prog_array(struct perf_event *event, void __user *info)\n{\n\tstruct perf_event_query_bpf __user *uquery = info;\n\tstruct perf_event_query_bpf query = {};\n\tstruct bpf_prog_array *progs;\n\tu32 *ids, prog_cnt, ids_len;\n\tint ret;\n\n\tif (!perfmon_capable())\n\t\treturn -EPERM;\n\tif (event->attr.type != PERF_TYPE_TRACEPOINT)\n\t\treturn -EINVAL;\n\tif (copy_from_user(&query, uquery, sizeof(query)))\n\t\treturn -EFAULT;\n\n\tids_len = query.ids_len;\n\tif (ids_len > BPF_TRACE_MAX_PROGS)\n\t\treturn -E2BIG;\n\tids = kcalloc(ids_len, sizeof(u32), GFP_USER | __GFP_NOWARN);\n\tif (!ids)\n\t\treturn -ENOMEM;\n\t \n\n\tmutex_lock(&bpf_event_mutex);\n\tprogs = bpf_event_rcu_dereference(event->tp_event->prog_array);\n\tret = bpf_prog_array_copy_info(progs, ids, ids_len, &prog_cnt);\n\tmutex_unlock(&bpf_event_mutex);\n\n\tif (copy_to_user(&uquery->prog_cnt, &prog_cnt, sizeof(prog_cnt)) ||\n\t    copy_to_user(uquery->ids, ids, ids_len * sizeof(u32)))\n\t\tret = -EFAULT;\n\n\tkfree(ids);\n\treturn ret;\n}\n\nextern struct bpf_raw_event_map __start__bpf_raw_tp[];\nextern struct bpf_raw_event_map __stop__bpf_raw_tp[];\n\nstruct bpf_raw_event_map *bpf_get_raw_tracepoint(const char *name)\n{\n\tstruct bpf_raw_event_map *btp = __start__bpf_raw_tp;\n\n\tfor (; btp < __stop__bpf_raw_tp; btp++) {\n\t\tif (!strcmp(btp->tp->name, name))\n\t\t\treturn btp;\n\t}\n\n\treturn bpf_get_raw_tracepoint_module(name);\n}\n\nvoid bpf_put_raw_tracepoint(struct bpf_raw_event_map *btp)\n{\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tmod = __module_address((unsigned long)btp);\n\tmodule_put(mod);\n\tpreempt_enable();\n}\n\nstatic __always_inline\nvoid __bpf_trace_run(struct bpf_prog *prog, u64 *args)\n{\n\tcant_sleep();\n\tif (unlikely(this_cpu_inc_return(*(prog->active)) != 1)) {\n\t\tbpf_prog_inc_misses_counter(prog);\n\t\tgoto out;\n\t}\n\trcu_read_lock();\n\t(void) bpf_prog_run(prog, args);\n\trcu_read_unlock();\nout:\n\tthis_cpu_dec(*(prog->active));\n}\n\n#define UNPACK(...)\t\t\t__VA_ARGS__\n#define REPEAT_1(FN, DL, X, ...)\tFN(X)\n#define REPEAT_2(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_1(FN, DL, __VA_ARGS__)\n#define REPEAT_3(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_2(FN, DL, __VA_ARGS__)\n#define REPEAT_4(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_3(FN, DL, __VA_ARGS__)\n#define REPEAT_5(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_4(FN, DL, __VA_ARGS__)\n#define REPEAT_6(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_5(FN, DL, __VA_ARGS__)\n#define REPEAT_7(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_6(FN, DL, __VA_ARGS__)\n#define REPEAT_8(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_7(FN, DL, __VA_ARGS__)\n#define REPEAT_9(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_8(FN, DL, __VA_ARGS__)\n#define REPEAT_10(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_9(FN, DL, __VA_ARGS__)\n#define REPEAT_11(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_10(FN, DL, __VA_ARGS__)\n#define REPEAT_12(FN, DL, X, ...)\tFN(X) UNPACK DL REPEAT_11(FN, DL, __VA_ARGS__)\n#define REPEAT(X, FN, DL, ...)\t\tREPEAT_##X(FN, DL, __VA_ARGS__)\n\n#define SARG(X)\t\tu64 arg##X\n#define COPY(X)\t\targs[X] = arg##X\n\n#define __DL_COM\t(,)\n#define __DL_SEM\t(;)\n\n#define __SEQ_0_11\t0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11\n\n#define BPF_TRACE_DEFN_x(x)\t\t\t\t\t\t\\\n\tvoid bpf_trace_run##x(struct bpf_prog *prog,\t\t\t\\\n\t\t\t      REPEAT(x, SARG, __DL_COM, __SEQ_0_11))\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tu64 args[x];\t\t\t\t\t\t\\\n\t\tREPEAT(x, COPY, __DL_SEM, __SEQ_0_11);\t\t\t\\\n\t\t__bpf_trace_run(prog, args);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tEXPORT_SYMBOL_GPL(bpf_trace_run##x)\nBPF_TRACE_DEFN_x(1);\nBPF_TRACE_DEFN_x(2);\nBPF_TRACE_DEFN_x(3);\nBPF_TRACE_DEFN_x(4);\nBPF_TRACE_DEFN_x(5);\nBPF_TRACE_DEFN_x(6);\nBPF_TRACE_DEFN_x(7);\nBPF_TRACE_DEFN_x(8);\nBPF_TRACE_DEFN_x(9);\nBPF_TRACE_DEFN_x(10);\nBPF_TRACE_DEFN_x(11);\nBPF_TRACE_DEFN_x(12);\n\nstatic int __bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *prog)\n{\n\tstruct tracepoint *tp = btp->tp;\n\n\t \n\tif (prog->aux->max_ctx_offset > btp->num_args * sizeof(u64))\n\t\treturn -EINVAL;\n\n\tif (prog->aux->max_tp_access > btp->writable_size)\n\t\treturn -EINVAL;\n\n\treturn tracepoint_probe_register_may_exist(tp, (void *)btp->bpf_func,\n\t\t\t\t\t\t   prog);\n}\n\nint bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *prog)\n{\n\treturn __bpf_probe_register(btp, prog);\n}\n\nint bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *prog)\n{\n\treturn tracepoint_probe_unregister(btp->tp, (void *)btp->bpf_func, prog);\n}\n\nint bpf_get_perf_event_info(const struct perf_event *event, u32 *prog_id,\n\t\t\t    u32 *fd_type, const char **buf,\n\t\t\t    u64 *probe_offset, u64 *probe_addr)\n{\n\tbool is_tracepoint, is_syscall_tp;\n\tstruct bpf_prog *prog;\n\tint flags, err = 0;\n\n\tprog = event->prog;\n\tif (!prog)\n\t\treturn -ENOENT;\n\n\t \n\tif (prog->type == BPF_PROG_TYPE_PERF_EVENT)\n\t\treturn -EOPNOTSUPP;\n\n\t*prog_id = prog->aux->id;\n\tflags = event->tp_event->flags;\n\tis_tracepoint = flags & TRACE_EVENT_FL_TRACEPOINT;\n\tis_syscall_tp = is_syscall_trace_event(event->tp_event);\n\n\tif (is_tracepoint || is_syscall_tp) {\n\t\t*buf = is_tracepoint ? event->tp_event->tp->name\n\t\t\t\t     : event->tp_event->name;\n\t\t \n\t\tif (fd_type)\n\t\t\t*fd_type = BPF_FD_TYPE_TRACEPOINT;\n\t\tif (probe_offset)\n\t\t\t*probe_offset = 0x0;\n\t\tif (probe_addr)\n\t\t\t*probe_addr = 0x0;\n\t} else {\n\t\t \n\t\terr = -EOPNOTSUPP;\n#ifdef CONFIG_KPROBE_EVENTS\n\t\tif (flags & TRACE_EVENT_FL_KPROBE)\n\t\t\terr = bpf_get_kprobe_info(event, fd_type, buf,\n\t\t\t\t\t\t  probe_offset, probe_addr,\n\t\t\t\t\t\t  event->attr.type == PERF_TYPE_TRACEPOINT);\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\n\t\tif (flags & TRACE_EVENT_FL_UPROBE)\n\t\t\terr = bpf_get_uprobe_info(event, fd_type, buf,\n\t\t\t\t\t\t  probe_offset, probe_addr,\n\t\t\t\t\t\t  event->attr.type == PERF_TYPE_TRACEPOINT);\n#endif\n\t}\n\n\treturn err;\n}\n\nstatic int __init send_signal_irq_work_init(void)\n{\n\tint cpu;\n\tstruct send_signal_irq_work *work;\n\n\tfor_each_possible_cpu(cpu) {\n\t\twork = per_cpu_ptr(&send_signal_work, cpu);\n\t\tinit_irq_work(&work->irq_work, do_bpf_send_signal);\n\t}\n\treturn 0;\n}\n\nsubsys_initcall(send_signal_irq_work_init);\n\n#ifdef CONFIG_MODULES\nstatic int bpf_event_notify(struct notifier_block *nb, unsigned long op,\n\t\t\t    void *module)\n{\n\tstruct bpf_trace_module *btm, *tmp;\n\tstruct module *mod = module;\n\tint ret = 0;\n\n\tif (mod->num_bpf_raw_events == 0 ||\n\t    (op != MODULE_STATE_COMING && op != MODULE_STATE_GOING))\n\t\tgoto out;\n\n\tmutex_lock(&bpf_module_mutex);\n\n\tswitch (op) {\n\tcase MODULE_STATE_COMING:\n\t\tbtm = kzalloc(sizeof(*btm), GFP_KERNEL);\n\t\tif (btm) {\n\t\t\tbtm->module = module;\n\t\t\tlist_add(&btm->list, &bpf_trace_modules);\n\t\t} else {\n\t\t\tret = -ENOMEM;\n\t\t}\n\t\tbreak;\n\tcase MODULE_STATE_GOING:\n\t\tlist_for_each_entry_safe(btm, tmp, &bpf_trace_modules, list) {\n\t\t\tif (btm->module == module) {\n\t\t\t\tlist_del(&btm->list);\n\t\t\t\tkfree(btm);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&bpf_module_mutex);\n\nout:\n\treturn notifier_from_errno(ret);\n}\n\nstatic struct notifier_block bpf_module_nb = {\n\t.notifier_call = bpf_event_notify,\n};\n\nstatic int __init bpf_event_init(void)\n{\n\tregister_module_notifier(&bpf_module_nb);\n\treturn 0;\n}\n\nfs_initcall(bpf_event_init);\n#endif  \n\n#ifdef CONFIG_FPROBE\nstruct bpf_kprobe_multi_link {\n\tstruct bpf_link link;\n\tstruct fprobe fp;\n\tunsigned long *addrs;\n\tu64 *cookies;\n\tu32 cnt;\n\tu32 mods_cnt;\n\tstruct module **mods;\n\tu32 flags;\n};\n\nstruct bpf_kprobe_multi_run_ctx {\n\tstruct bpf_run_ctx run_ctx;\n\tstruct bpf_kprobe_multi_link *link;\n\tunsigned long entry_ip;\n};\n\nstruct user_syms {\n\tconst char **syms;\n\tchar *buf;\n};\n\nstatic int copy_user_syms(struct user_syms *us, unsigned long __user *usyms, u32 cnt)\n{\n\tunsigned long __user usymbol;\n\tconst char **syms = NULL;\n\tchar *buf = NULL, *p;\n\tint err = -ENOMEM;\n\tunsigned int i;\n\n\tsyms = kvmalloc_array(cnt, sizeof(*syms), GFP_KERNEL);\n\tif (!syms)\n\t\tgoto error;\n\n\tbuf = kvmalloc_array(cnt, KSYM_NAME_LEN, GFP_KERNEL);\n\tif (!buf)\n\t\tgoto error;\n\n\tfor (p = buf, i = 0; i < cnt; i++) {\n\t\tif (__get_user(usymbol, usyms + i)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto error;\n\t\t}\n\t\terr = strncpy_from_user(p, (const char __user *) usymbol, KSYM_NAME_LEN);\n\t\tif (err == KSYM_NAME_LEN)\n\t\t\terr = -E2BIG;\n\t\tif (err < 0)\n\t\t\tgoto error;\n\t\tsyms[i] = p;\n\t\tp += err + 1;\n\t}\n\n\tus->syms = syms;\n\tus->buf = buf;\n\treturn 0;\n\nerror:\n\tif (err) {\n\t\tkvfree(syms);\n\t\tkvfree(buf);\n\t}\n\treturn err;\n}\n\nstatic void kprobe_multi_put_modules(struct module **mods, u32 cnt)\n{\n\tu32 i;\n\n\tfor (i = 0; i < cnt; i++)\n\t\tmodule_put(mods[i]);\n}\n\nstatic void free_user_syms(struct user_syms *us)\n{\n\tkvfree(us->syms);\n\tkvfree(us->buf);\n}\n\nstatic void bpf_kprobe_multi_link_release(struct bpf_link *link)\n{\n\tstruct bpf_kprobe_multi_link *kmulti_link;\n\n\tkmulti_link = container_of(link, struct bpf_kprobe_multi_link, link);\n\tunregister_fprobe(&kmulti_link->fp);\n\tkprobe_multi_put_modules(kmulti_link->mods, kmulti_link->mods_cnt);\n}\n\nstatic void bpf_kprobe_multi_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_kprobe_multi_link *kmulti_link;\n\n\tkmulti_link = container_of(link, struct bpf_kprobe_multi_link, link);\n\tkvfree(kmulti_link->addrs);\n\tkvfree(kmulti_link->cookies);\n\tkfree(kmulti_link->mods);\n\tkfree(kmulti_link);\n}\n\nstatic int bpf_kprobe_multi_link_fill_link_info(const struct bpf_link *link,\n\t\t\t\t\t\tstruct bpf_link_info *info)\n{\n\tu64 __user *uaddrs = u64_to_user_ptr(info->kprobe_multi.addrs);\n\tstruct bpf_kprobe_multi_link *kmulti_link;\n\tu32 ucount = info->kprobe_multi.count;\n\tint err = 0, i;\n\n\tif (!uaddrs ^ !ucount)\n\t\treturn -EINVAL;\n\n\tkmulti_link = container_of(link, struct bpf_kprobe_multi_link, link);\n\tinfo->kprobe_multi.count = kmulti_link->cnt;\n\tinfo->kprobe_multi.flags = kmulti_link->flags;\n\n\tif (!uaddrs)\n\t\treturn 0;\n\tif (ucount < kmulti_link->cnt)\n\t\terr = -ENOSPC;\n\telse\n\t\tucount = kmulti_link->cnt;\n\n\tif (kallsyms_show_value(current_cred())) {\n\t\tif (copy_to_user(uaddrs, kmulti_link->addrs, ucount * sizeof(u64)))\n\t\t\treturn -EFAULT;\n\t} else {\n\t\tfor (i = 0; i < ucount; i++) {\n\t\t\tif (put_user(0, uaddrs + i))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic const struct bpf_link_ops bpf_kprobe_multi_link_lops = {\n\t.release = bpf_kprobe_multi_link_release,\n\t.dealloc = bpf_kprobe_multi_link_dealloc,\n\t.fill_link_info = bpf_kprobe_multi_link_fill_link_info,\n};\n\nstatic void bpf_kprobe_multi_cookie_swap(void *a, void *b, int size, const void *priv)\n{\n\tconst struct bpf_kprobe_multi_link *link = priv;\n\tunsigned long *addr_a = a, *addr_b = b;\n\tu64 *cookie_a, *cookie_b;\n\n\tcookie_a = link->cookies + (addr_a - link->addrs);\n\tcookie_b = link->cookies + (addr_b - link->addrs);\n\n\t \n\tswap(*addr_a, *addr_b);\n\tswap(*cookie_a, *cookie_b);\n}\n\nstatic int bpf_kprobe_multi_addrs_cmp(const void *a, const void *b)\n{\n\tconst unsigned long *addr_a = a, *addr_b = b;\n\n\tif (*addr_a == *addr_b)\n\t\treturn 0;\n\treturn *addr_a < *addr_b ? -1 : 1;\n}\n\nstatic int bpf_kprobe_multi_cookie_cmp(const void *a, const void *b, const void *priv)\n{\n\treturn bpf_kprobe_multi_addrs_cmp(a, b);\n}\n\nstatic u64 bpf_kprobe_multi_cookie(struct bpf_run_ctx *ctx)\n{\n\tstruct bpf_kprobe_multi_run_ctx *run_ctx;\n\tstruct bpf_kprobe_multi_link *link;\n\tu64 *cookie, entry_ip;\n\tunsigned long *addr;\n\n\tif (WARN_ON_ONCE(!ctx))\n\t\treturn 0;\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_kprobe_multi_run_ctx, run_ctx);\n\tlink = run_ctx->link;\n\tif (!link->cookies)\n\t\treturn 0;\n\tentry_ip = run_ctx->entry_ip;\n\taddr = bsearch(&entry_ip, link->addrs, link->cnt, sizeof(entry_ip),\n\t\t       bpf_kprobe_multi_addrs_cmp);\n\tif (!addr)\n\t\treturn 0;\n\tcookie = link->cookies + (addr - link->addrs);\n\treturn *cookie;\n}\n\nstatic u64 bpf_kprobe_multi_entry_ip(struct bpf_run_ctx *ctx)\n{\n\tstruct bpf_kprobe_multi_run_ctx *run_ctx;\n\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_kprobe_multi_run_ctx, run_ctx);\n\treturn run_ctx->entry_ip;\n}\n\nstatic int\nkprobe_multi_link_prog_run(struct bpf_kprobe_multi_link *link,\n\t\t\t   unsigned long entry_ip, struct pt_regs *regs)\n{\n\tstruct bpf_kprobe_multi_run_ctx run_ctx = {\n\t\t.link = link,\n\t\t.entry_ip = entry_ip,\n\t};\n\tstruct bpf_run_ctx *old_run_ctx;\n\tint err;\n\n\tif (unlikely(__this_cpu_inc_return(bpf_prog_active) != 1)) {\n\t\terr = 0;\n\t\tgoto out;\n\t}\n\n\tmigrate_disable();\n\trcu_read_lock();\n\told_run_ctx = bpf_set_run_ctx(&run_ctx.run_ctx);\n\terr = bpf_prog_run(link->link.prog, regs);\n\tbpf_reset_run_ctx(old_run_ctx);\n\trcu_read_unlock();\n\tmigrate_enable();\n\n out:\n\t__this_cpu_dec(bpf_prog_active);\n\treturn err;\n}\n\nstatic int\nkprobe_multi_link_handler(struct fprobe *fp, unsigned long fentry_ip,\n\t\t\t  unsigned long ret_ip, struct pt_regs *regs,\n\t\t\t  void *data)\n{\n\tstruct bpf_kprobe_multi_link *link;\n\n\tlink = container_of(fp, struct bpf_kprobe_multi_link, fp);\n\tkprobe_multi_link_prog_run(link, get_entry_ip(fentry_ip), regs);\n\treturn 0;\n}\n\nstatic void\nkprobe_multi_link_exit_handler(struct fprobe *fp, unsigned long fentry_ip,\n\t\t\t       unsigned long ret_ip, struct pt_regs *regs,\n\t\t\t       void *data)\n{\n\tstruct bpf_kprobe_multi_link *link;\n\n\tlink = container_of(fp, struct bpf_kprobe_multi_link, fp);\n\tkprobe_multi_link_prog_run(link, get_entry_ip(fentry_ip), regs);\n}\n\nstatic int symbols_cmp_r(const void *a, const void *b, const void *priv)\n{\n\tconst char **str_a = (const char **) a;\n\tconst char **str_b = (const char **) b;\n\n\treturn strcmp(*str_a, *str_b);\n}\n\nstruct multi_symbols_sort {\n\tconst char **funcs;\n\tu64 *cookies;\n};\n\nstatic void symbols_swap_r(void *a, void *b, int size, const void *priv)\n{\n\tconst struct multi_symbols_sort *data = priv;\n\tconst char **name_a = a, **name_b = b;\n\n\tswap(*name_a, *name_b);\n\n\t \n\tif (data->cookies) {\n\t\tu64 *cookie_a, *cookie_b;\n\n\t\tcookie_a = data->cookies + (name_a - data->funcs);\n\t\tcookie_b = data->cookies + (name_b - data->funcs);\n\t\tswap(*cookie_a, *cookie_b);\n\t}\n}\n\nstruct modules_array {\n\tstruct module **mods;\n\tint mods_cnt;\n\tint mods_cap;\n};\n\nstatic int add_module(struct modules_array *arr, struct module *mod)\n{\n\tstruct module **mods;\n\n\tif (arr->mods_cnt == arr->mods_cap) {\n\t\tarr->mods_cap = max(16, arr->mods_cap * 3 / 2);\n\t\tmods = krealloc_array(arr->mods, arr->mods_cap, sizeof(*mods), GFP_KERNEL);\n\t\tif (!mods)\n\t\t\treturn -ENOMEM;\n\t\tarr->mods = mods;\n\t}\n\n\tarr->mods[arr->mods_cnt] = mod;\n\tarr->mods_cnt++;\n\treturn 0;\n}\n\nstatic bool has_module(struct modules_array *arr, struct module *mod)\n{\n\tint i;\n\n\tfor (i = arr->mods_cnt - 1; i >= 0; i--) {\n\t\tif (arr->mods[i] == mod)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int get_modules_for_addrs(struct module ***mods, unsigned long *addrs, u32 addrs_cnt)\n{\n\tstruct modules_array arr = {};\n\tu32 i, err = 0;\n\n\tfor (i = 0; i < addrs_cnt; i++) {\n\t\tstruct module *mod;\n\n\t\tpreempt_disable();\n\t\tmod = __module_address(addrs[i]);\n\t\t \n\t\tif (!mod || has_module(&arr, mod)) {\n\t\t\tpreempt_enable();\n\t\t\tcontinue;\n\t\t}\n\t\tif (!try_module_get(mod))\n\t\t\terr = -EINVAL;\n\t\tpreempt_enable();\n\t\tif (err)\n\t\t\tbreak;\n\t\terr = add_module(&arr, mod);\n\t\tif (err) {\n\t\t\tmodule_put(mod);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (err) {\n\t\tkprobe_multi_put_modules(arr.mods, arr.mods_cnt);\n\t\tkfree(arr.mods);\n\t\treturn err;\n\t}\n\n\t \n\t*mods = arr.mods;\n\treturn arr.mods_cnt;\n}\n\nstatic int addrs_check_error_injection_list(unsigned long *addrs, u32 cnt)\n{\n\tu32 i;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tif (!within_error_injection_list(addrs[i]))\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nint bpf_kprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct bpf_kprobe_multi_link *link = NULL;\n\tstruct bpf_link_primer link_primer;\n\tvoid __user *ucookies;\n\tunsigned long *addrs;\n\tu32 flags, cnt, size;\n\tvoid __user *uaddrs;\n\tu64 *cookies = NULL;\n\tvoid __user *usyms;\n\tint err;\n\n\t \n\tif (sizeof(u64) != sizeof(void *))\n\t\treturn -EOPNOTSUPP;\n\n\tif (prog->expected_attach_type != BPF_TRACE_KPROBE_MULTI)\n\t\treturn -EINVAL;\n\n\tflags = attr->link_create.kprobe_multi.flags;\n\tif (flags & ~BPF_F_KPROBE_MULTI_RETURN)\n\t\treturn -EINVAL;\n\n\tuaddrs = u64_to_user_ptr(attr->link_create.kprobe_multi.addrs);\n\tusyms = u64_to_user_ptr(attr->link_create.kprobe_multi.syms);\n\tif (!!uaddrs == !!usyms)\n\t\treturn -EINVAL;\n\n\tcnt = attr->link_create.kprobe_multi.cnt;\n\tif (!cnt)\n\t\treturn -EINVAL;\n\tif (cnt > MAX_KPROBE_MULTI_CNT)\n\t\treturn -E2BIG;\n\n\tsize = cnt * sizeof(*addrs);\n\taddrs = kvmalloc_array(cnt, sizeof(*addrs), GFP_KERNEL);\n\tif (!addrs)\n\t\treturn -ENOMEM;\n\n\tucookies = u64_to_user_ptr(attr->link_create.kprobe_multi.cookies);\n\tif (ucookies) {\n\t\tcookies = kvmalloc_array(cnt, sizeof(*addrs), GFP_KERNEL);\n\t\tif (!cookies) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto error;\n\t\t}\n\t\tif (copy_from_user(cookies, ucookies, size)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tif (uaddrs) {\n\t\tif (copy_from_user(addrs, uaddrs, size)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\tstruct multi_symbols_sort data = {\n\t\t\t.cookies = cookies,\n\t\t};\n\t\tstruct user_syms us;\n\n\t\terr = copy_user_syms(&us, usyms, cnt);\n\t\tif (err)\n\t\t\tgoto error;\n\n\t\tif (cookies)\n\t\t\tdata.funcs = us.syms;\n\n\t\tsort_r(us.syms, cnt, sizeof(*us.syms), symbols_cmp_r,\n\t\t       symbols_swap_r, &data);\n\n\t\terr = ftrace_lookup_symbols(us.syms, cnt, addrs);\n\t\tfree_user_syms(&us);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\n\tif (prog->kprobe_override && addrs_check_error_injection_list(addrs, cnt)) {\n\t\terr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tlink = kzalloc(sizeof(*link), GFP_KERNEL);\n\tif (!link) {\n\t\terr = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_KPROBE_MULTI,\n\t\t      &bpf_kprobe_multi_link_lops, prog);\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err)\n\t\tgoto error;\n\n\tif (flags & BPF_F_KPROBE_MULTI_RETURN)\n\t\tlink->fp.exit_handler = kprobe_multi_link_exit_handler;\n\telse\n\t\tlink->fp.entry_handler = kprobe_multi_link_handler;\n\n\tlink->addrs = addrs;\n\tlink->cookies = cookies;\n\tlink->cnt = cnt;\n\tlink->flags = flags;\n\n\tif (cookies) {\n\t\t \n\t\tsort_r(addrs, cnt, sizeof(*addrs),\n\t\t       bpf_kprobe_multi_cookie_cmp,\n\t\t       bpf_kprobe_multi_cookie_swap,\n\t\t       link);\n\t}\n\n\terr = get_modules_for_addrs(&link->mods, addrs, cnt);\n\tif (err < 0) {\n\t\tbpf_link_cleanup(&link_primer);\n\t\treturn err;\n\t}\n\tlink->mods_cnt = err;\n\n\terr = register_fprobe_ips(&link->fp, addrs, cnt);\n\tif (err) {\n\t\tkprobe_multi_put_modules(link->mods, link->mods_cnt);\n\t\tbpf_link_cleanup(&link_primer);\n\t\treturn err;\n\t}\n\n\treturn bpf_link_settle(&link_primer);\n\nerror:\n\tkfree(link);\n\tkvfree(addrs);\n\tkvfree(cookies);\n\treturn err;\n}\n#else  \nint bpf_kprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic u64 bpf_kprobe_multi_cookie(struct bpf_run_ctx *ctx)\n{\n\treturn 0;\n}\nstatic u64 bpf_kprobe_multi_entry_ip(struct bpf_run_ctx *ctx)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_UPROBES\nstruct bpf_uprobe_multi_link;\n\nstruct bpf_uprobe {\n\tstruct bpf_uprobe_multi_link *link;\n\tloff_t offset;\n\tu64 cookie;\n\tstruct uprobe_consumer consumer;\n};\n\nstruct bpf_uprobe_multi_link {\n\tstruct path path;\n\tstruct bpf_link link;\n\tu32 cnt;\n\tstruct bpf_uprobe *uprobes;\n\tstruct task_struct *task;\n};\n\nstruct bpf_uprobe_multi_run_ctx {\n\tstruct bpf_run_ctx run_ctx;\n\tunsigned long entry_ip;\n\tstruct bpf_uprobe *uprobe;\n};\n\nstatic void bpf_uprobe_unregister(struct path *path, struct bpf_uprobe *uprobes,\n\t\t\t\t  u32 cnt)\n{\n\tu32 i;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tuprobe_unregister(d_real_inode(path->dentry), uprobes[i].offset,\n\t\t\t\t  &uprobes[i].consumer);\n\t}\n}\n\nstatic void bpf_uprobe_multi_link_release(struct bpf_link *link)\n{\n\tstruct bpf_uprobe_multi_link *umulti_link;\n\n\tumulti_link = container_of(link, struct bpf_uprobe_multi_link, link);\n\tbpf_uprobe_unregister(&umulti_link->path, umulti_link->uprobes, umulti_link->cnt);\n}\n\nstatic void bpf_uprobe_multi_link_dealloc(struct bpf_link *link)\n{\n\tstruct bpf_uprobe_multi_link *umulti_link;\n\n\tumulti_link = container_of(link, struct bpf_uprobe_multi_link, link);\n\tif (umulti_link->task)\n\t\tput_task_struct(umulti_link->task);\n\tpath_put(&umulti_link->path);\n\tkvfree(umulti_link->uprobes);\n\tkfree(umulti_link);\n}\n\nstatic const struct bpf_link_ops bpf_uprobe_multi_link_lops = {\n\t.release = bpf_uprobe_multi_link_release,\n\t.dealloc = bpf_uprobe_multi_link_dealloc,\n};\n\nstatic int uprobe_prog_run(struct bpf_uprobe *uprobe,\n\t\t\t   unsigned long entry_ip,\n\t\t\t   struct pt_regs *regs)\n{\n\tstruct bpf_uprobe_multi_link *link = uprobe->link;\n\tstruct bpf_uprobe_multi_run_ctx run_ctx = {\n\t\t.entry_ip = entry_ip,\n\t\t.uprobe = uprobe,\n\t};\n\tstruct bpf_prog *prog = link->link.prog;\n\tbool sleepable = prog->aux->sleepable;\n\tstruct bpf_run_ctx *old_run_ctx;\n\tint err = 0;\n\n\tif (link->task && current != link->task)\n\t\treturn 0;\n\n\tif (sleepable)\n\t\trcu_read_lock_trace();\n\telse\n\t\trcu_read_lock();\n\n\tmigrate_disable();\n\n\told_run_ctx = bpf_set_run_ctx(&run_ctx.run_ctx);\n\terr = bpf_prog_run(link->link.prog, regs);\n\tbpf_reset_run_ctx(old_run_ctx);\n\n\tmigrate_enable();\n\n\tif (sleepable)\n\t\trcu_read_unlock_trace();\n\telse\n\t\trcu_read_unlock();\n\treturn err;\n}\n\nstatic bool\nuprobe_multi_link_filter(struct uprobe_consumer *con, enum uprobe_filter_ctx ctx,\n\t\t\t struct mm_struct *mm)\n{\n\tstruct bpf_uprobe *uprobe;\n\n\tuprobe = container_of(con, struct bpf_uprobe, consumer);\n\treturn uprobe->link->task->mm == mm;\n}\n\nstatic int\nuprobe_multi_link_handler(struct uprobe_consumer *con, struct pt_regs *regs)\n{\n\tstruct bpf_uprobe *uprobe;\n\n\tuprobe = container_of(con, struct bpf_uprobe, consumer);\n\treturn uprobe_prog_run(uprobe, instruction_pointer(regs), regs);\n}\n\nstatic int\nuprobe_multi_link_ret_handler(struct uprobe_consumer *con, unsigned long func, struct pt_regs *regs)\n{\n\tstruct bpf_uprobe *uprobe;\n\n\tuprobe = container_of(con, struct bpf_uprobe, consumer);\n\treturn uprobe_prog_run(uprobe, func, regs);\n}\n\nstatic u64 bpf_uprobe_multi_entry_ip(struct bpf_run_ctx *ctx)\n{\n\tstruct bpf_uprobe_multi_run_ctx *run_ctx;\n\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_uprobe_multi_run_ctx, run_ctx);\n\treturn run_ctx->entry_ip;\n}\n\nstatic u64 bpf_uprobe_multi_cookie(struct bpf_run_ctx *ctx)\n{\n\tstruct bpf_uprobe_multi_run_ctx *run_ctx;\n\n\trun_ctx = container_of(current->bpf_ctx, struct bpf_uprobe_multi_run_ctx, run_ctx);\n\treturn run_ctx->uprobe->cookie;\n}\n\nint bpf_uprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\tstruct bpf_uprobe_multi_link *link = NULL;\n\tunsigned long __user *uref_ctr_offsets;\n\tunsigned long *ref_ctr_offsets = NULL;\n\tstruct bpf_link_primer link_primer;\n\tstruct bpf_uprobe *uprobes = NULL;\n\tstruct task_struct *task = NULL;\n\tunsigned long __user *uoffsets;\n\tu64 __user *ucookies;\n\tvoid __user *upath;\n\tu32 flags, cnt, i;\n\tstruct path path;\n\tchar *name;\n\tpid_t pid;\n\tint err;\n\n\t \n\tif (sizeof(u64) != sizeof(void *))\n\t\treturn -EOPNOTSUPP;\n\n\tif (prog->expected_attach_type != BPF_TRACE_UPROBE_MULTI)\n\t\treturn -EINVAL;\n\n\tflags = attr->link_create.uprobe_multi.flags;\n\tif (flags & ~BPF_F_UPROBE_MULTI_RETURN)\n\t\treturn -EINVAL;\n\n\t \n\tupath = u64_to_user_ptr(attr->link_create.uprobe_multi.path);\n\tuoffsets = u64_to_user_ptr(attr->link_create.uprobe_multi.offsets);\n\tcnt = attr->link_create.uprobe_multi.cnt;\n\n\tif (!upath || !uoffsets || !cnt)\n\t\treturn -EINVAL;\n\tif (cnt > MAX_UPROBE_MULTI_CNT)\n\t\treturn -E2BIG;\n\n\turef_ctr_offsets = u64_to_user_ptr(attr->link_create.uprobe_multi.ref_ctr_offsets);\n\tucookies = u64_to_user_ptr(attr->link_create.uprobe_multi.cookies);\n\n\tname = strndup_user(upath, PATH_MAX);\n\tif (IS_ERR(name)) {\n\t\terr = PTR_ERR(name);\n\t\treturn err;\n\t}\n\n\terr = kern_path(name, LOOKUP_FOLLOW, &path);\n\tkfree(name);\n\tif (err)\n\t\treturn err;\n\n\tif (!d_is_reg(path.dentry)) {\n\t\terr = -EBADF;\n\t\tgoto error_path_put;\n\t}\n\n\tpid = attr->link_create.uprobe_multi.pid;\n\tif (pid) {\n\t\trcu_read_lock();\n\t\ttask = get_pid_task(find_vpid(pid), PIDTYPE_PID);\n\t\trcu_read_unlock();\n\t\tif (!task) {\n\t\t\terr = -ESRCH;\n\t\t\tgoto error_path_put;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\n\tlink = kzalloc(sizeof(*link), GFP_KERNEL);\n\tuprobes = kvcalloc(cnt, sizeof(*uprobes), GFP_KERNEL);\n\n\tif (!uprobes || !link)\n\t\tgoto error_free;\n\n\tif (uref_ctr_offsets) {\n\t\tref_ctr_offsets = kvcalloc(cnt, sizeof(*ref_ctr_offsets), GFP_KERNEL);\n\t\tif (!ref_ctr_offsets)\n\t\t\tgoto error_free;\n\t}\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tif (ucookies && __get_user(uprobes[i].cookie, ucookies + i)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto error_free;\n\t\t}\n\t\tif (uref_ctr_offsets && __get_user(ref_ctr_offsets[i], uref_ctr_offsets + i)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto error_free;\n\t\t}\n\t\tif (__get_user(uprobes[i].offset, uoffsets + i)) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto error_free;\n\t\t}\n\n\t\tuprobes[i].link = link;\n\n\t\tif (flags & BPF_F_UPROBE_MULTI_RETURN)\n\t\t\tuprobes[i].consumer.ret_handler = uprobe_multi_link_ret_handler;\n\t\telse\n\t\t\tuprobes[i].consumer.handler = uprobe_multi_link_handler;\n\n\t\tif (pid)\n\t\t\tuprobes[i].consumer.filter = uprobe_multi_link_filter;\n\t}\n\n\tlink->cnt = cnt;\n\tlink->uprobes = uprobes;\n\tlink->path = path;\n\tlink->task = task;\n\n\tbpf_link_init(&link->link, BPF_LINK_TYPE_UPROBE_MULTI,\n\t\t      &bpf_uprobe_multi_link_lops, prog);\n\n\tfor (i = 0; i < cnt; i++) {\n\t\terr = uprobe_register_refctr(d_real_inode(link->path.dentry),\n\t\t\t\t\t     uprobes[i].offset,\n\t\t\t\t\t     ref_ctr_offsets ? ref_ctr_offsets[i] : 0,\n\t\t\t\t\t     &uprobes[i].consumer);\n\t\tif (err) {\n\t\t\tbpf_uprobe_unregister(&path, uprobes, i);\n\t\t\tgoto error_free;\n\t\t}\n\t}\n\n\terr = bpf_link_prime(&link->link, &link_primer);\n\tif (err)\n\t\tgoto error_free;\n\n\tkvfree(ref_ctr_offsets);\n\treturn bpf_link_settle(&link_primer);\n\nerror_free:\n\tkvfree(ref_ctr_offsets);\n\tkvfree(uprobes);\n\tkfree(link);\n\tif (task)\n\t\tput_task_struct(task);\nerror_path_put:\n\tpath_put(&path);\n\treturn err;\n}\n#else  \nint bpf_uprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic u64 bpf_uprobe_multi_cookie(struct bpf_run_ctx *ctx)\n{\n\treturn 0;\n}\nstatic u64 bpf_uprobe_multi_entry_ip(struct bpf_run_ctx *ctx)\n{\n\treturn 0;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}