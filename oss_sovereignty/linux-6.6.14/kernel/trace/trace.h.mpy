{
  "module_name": "trace.h",
  "hash_id": "52bcd9310fdb12a2dac13981e041f234f0edd10dfcda5fa92248b2a7bd5db42f",
  "original_prompt": "Ingested from linux-6.6.14/kernel/trace/trace.h",
  "human_readable_source": "\n\n#ifndef _LINUX_KERNEL_TRACE_H\n#define _LINUX_KERNEL_TRACE_H\n\n#include <linux/fs.h>\n#include <linux/atomic.h>\n#include <linux/sched.h>\n#include <linux/clocksource.h>\n#include <linux/ring_buffer.h>\n#include <linux/mmiotrace.h>\n#include <linux/tracepoint.h>\n#include <linux/ftrace.h>\n#include <linux/trace.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/trace_seq.h>\n#include <linux/trace_events.h>\n#include <linux/compiler.h>\n#include <linux/glob.h>\n#include <linux/irq_work.h>\n#include <linux/workqueue.h>\n#include <linux/ctype.h>\n#include <linux/once_lite.h>\n\n#include \"pid_list.h\"\n\n#ifdef CONFIG_FTRACE_SYSCALLS\n#include <asm/unistd.h>\t\t \n#include <asm/syscall.h>\t \n#endif\n\n#define TRACE_MODE_WRITE\t0640\n#define TRACE_MODE_READ\t\t0440\n\nenum trace_type {\n\t__TRACE_FIRST_TYPE = 0,\n\n\tTRACE_FN,\n\tTRACE_CTX,\n\tTRACE_WAKE,\n\tTRACE_STACK,\n\tTRACE_PRINT,\n\tTRACE_BPRINT,\n\tTRACE_MMIO_RW,\n\tTRACE_MMIO_MAP,\n\tTRACE_BRANCH,\n\tTRACE_GRAPH_RET,\n\tTRACE_GRAPH_ENT,\n\tTRACE_USER_STACK,\n\tTRACE_BLK,\n\tTRACE_BPUTS,\n\tTRACE_HWLAT,\n\tTRACE_OSNOISE,\n\tTRACE_TIMERLAT,\n\tTRACE_RAW_DATA,\n\tTRACE_FUNC_REPEATS,\n\n\t__TRACE_LAST_TYPE,\n};\n\n\n#undef __field\n#define __field(type, item)\t\ttype\titem;\n\n#undef __field_fn\n#define __field_fn(type, item)\t\ttype\titem;\n\n#undef __field_struct\n#define __field_struct(type, item)\t__field(type, item)\n\n#undef __field_desc\n#define __field_desc(type, container, item)\n\n#undef __field_packed\n#define __field_packed(type, container, item)\n\n#undef __array\n#define __array(type, item, size)\ttype\titem[size];\n\n \n#undef __stack_array\n#define __stack_array(type, item, size, field)\t\ttype item[] __counted_by(field);\n\n#undef __array_desc\n#define __array_desc(type, container, item, size)\n\n#undef __dynamic_array\n#define __dynamic_array(type, item)\ttype\titem[];\n\n#undef __rel_dynamic_array\n#define __rel_dynamic_array(type, item)\ttype\titem[];\n\n#undef F_STRUCT\n#define F_STRUCT(args...)\t\targs\n\n#undef FTRACE_ENTRY\n#define FTRACE_ENTRY(name, struct_name, id, tstruct, print)\t\t\\\n\tstruct struct_name {\t\t\t\t\t\t\\\n\t\tstruct trace_entry\tent;\t\t\t\t\\\n\t\ttstruct\t\t\t\t\t\t\t\\\n\t}\n\n#undef FTRACE_ENTRY_DUP\n#define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk)\n\n#undef FTRACE_ENTRY_REG\n#define FTRACE_ENTRY_REG(name, struct_name, id, tstruct, print,\tregfn)\t\\\n\tFTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print))\n\n#undef FTRACE_ENTRY_PACKED\n#define FTRACE_ENTRY_PACKED(name, struct_name, id, tstruct, print)\t\\\n\tFTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print)) __packed\n\n#include \"trace_entries.h\"\n\n \n#define MEM_FAIL(condition, fmt, ...)\t\t\t\t\t\\\n\tDO_ONCE_LITE_IF(condition, pr_err, \"ERROR: \" fmt, ##__VA_ARGS__)\n\n#define FAULT_STRING \"(fault)\"\n\n#define HIST_STACKTRACE_DEPTH\t16\n#define HIST_STACKTRACE_SIZE\t(HIST_STACKTRACE_DEPTH * sizeof(unsigned long))\n#define HIST_STACKTRACE_SKIP\t5\n\n \nstruct syscall_trace_enter {\n\tstruct trace_entry\tent;\n\tint\t\t\tnr;\n\tunsigned long\t\targs[];\n};\n\nstruct syscall_trace_exit {\n\tstruct trace_entry\tent;\n\tint\t\t\tnr;\n\tlong\t\t\tret;\n};\n\nstruct kprobe_trace_entry_head {\n\tstruct trace_entry\tent;\n\tunsigned long\t\tip;\n};\n\nstruct eprobe_trace_entry_head {\n\tstruct trace_entry\tent;\n};\n\nstruct kretprobe_trace_entry_head {\n\tstruct trace_entry\tent;\n\tunsigned long\t\tfunc;\n\tunsigned long\t\tret_ip;\n};\n\nstruct fentry_trace_entry_head {\n\tstruct trace_entry\tent;\n\tunsigned long\t\tip;\n};\n\nstruct fexit_trace_entry_head {\n\tstruct trace_entry\tent;\n\tunsigned long\t\tfunc;\n\tunsigned long\t\tret_ip;\n};\n\n#define TRACE_BUF_SIZE\t\t1024\n\nstruct trace_array;\n\n \nstruct trace_array_cpu {\n\tatomic_t\t\tdisabled;\n\tvoid\t\t\t*buffer_page;\t \n\n\tunsigned long\t\tentries;\n\tunsigned long\t\tsaved_latency;\n\tunsigned long\t\tcritical_start;\n\tunsigned long\t\tcritical_end;\n\tunsigned long\t\tcritical_sequence;\n\tunsigned long\t\tnice;\n\tunsigned long\t\tpolicy;\n\tunsigned long\t\trt_priority;\n\tunsigned long\t\tskipped_entries;\n\tu64\t\t\tpreempt_timestamp;\n\tpid_t\t\t\tpid;\n\tkuid_t\t\t\tuid;\n\tchar\t\t\tcomm[TASK_COMM_LEN];\n\n#ifdef CONFIG_FUNCTION_TRACER\n\tint\t\t\tftrace_ignore_pid;\n#endif\n\tbool\t\t\tignore_pid;\n};\n\nstruct tracer;\nstruct trace_option_dentry;\n\nstruct array_buffer {\n\tstruct trace_array\t\t*tr;\n\tstruct trace_buffer\t\t*buffer;\n\tstruct trace_array_cpu __percpu\t*data;\n\tu64\t\t\t\ttime_start;\n\tint\t\t\t\tcpu;\n};\n\n#define TRACE_FLAGS_MAX_SIZE\t\t32\n\nstruct trace_options {\n\tstruct tracer\t\t\t*tracer;\n\tstruct trace_option_dentry\t*topts;\n};\n\nstruct trace_pid_list *trace_pid_list_alloc(void);\nvoid trace_pid_list_free(struct trace_pid_list *pid_list);\nbool trace_pid_list_is_set(struct trace_pid_list *pid_list, unsigned int pid);\nint trace_pid_list_set(struct trace_pid_list *pid_list, unsigned int pid);\nint trace_pid_list_clear(struct trace_pid_list *pid_list, unsigned int pid);\nint trace_pid_list_first(struct trace_pid_list *pid_list, unsigned int *pid);\nint trace_pid_list_next(struct trace_pid_list *pid_list, unsigned int pid,\n\t\t\tunsigned int *next);\n\nenum {\n\tTRACE_PIDS\t\t= BIT(0),\n\tTRACE_NO_PIDS\t\t= BIT(1),\n};\n\nstatic inline bool pid_type_enabled(int type, struct trace_pid_list *pid_list,\n\t\t\t\t    struct trace_pid_list *no_pid_list)\n{\n\t \n\treturn ((type & TRACE_PIDS) && pid_list) ||\n\t\t((type & TRACE_NO_PIDS) && no_pid_list);\n}\n\nstatic inline bool still_need_pid_events(int type, struct trace_pid_list *pid_list,\n\t\t\t\t\t struct trace_pid_list *no_pid_list)\n{\n\t \n\treturn (!(type & TRACE_PIDS) && pid_list) ||\n\t\t(!(type & TRACE_NO_PIDS) && no_pid_list);\n}\n\ntypedef bool (*cond_update_fn_t)(struct trace_array *tr, void *cond_data);\n\n \nstruct cond_snapshot {\n\tvoid\t\t\t\t*cond_data;\n\tcond_update_fn_t\t\tupdate;\n};\n\n \nstruct trace_func_repeats {\n\tunsigned long\tip;\n\tunsigned long\tparent_ip;\n\tunsigned long\tcount;\n\tu64\t\tts_last_call;\n};\n\n \nstruct trace_array {\n\tstruct list_head\tlist;\n\tchar\t\t\t*name;\n\tstruct array_buffer\tarray_buffer;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\t \n\tstruct array_buffer\tmax_buffer;\n\tbool\t\t\tallocated_snapshot;\n#endif\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tunsigned long\t\tmax_latency;\n#ifdef CONFIG_FSNOTIFY\n\tstruct dentry\t\t*d_max_latency;\n\tstruct work_struct\tfsnotify_work;\n\tstruct irq_work\t\tfsnotify_irqwork;\n#endif\n#endif\n\tstruct trace_pid_list\t__rcu *filtered_pids;\n\tstruct trace_pid_list\t__rcu *filtered_no_pids;\n\t \n\tarch_spinlock_t\t\tmax_lock;\n\tint\t\t\tbuffer_disabled;\n#ifdef CONFIG_FTRACE_SYSCALLS\n\tint\t\t\tsys_refcount_enter;\n\tint\t\t\tsys_refcount_exit;\n\tstruct trace_event_file __rcu *enter_syscall_files[NR_syscalls];\n\tstruct trace_event_file __rcu *exit_syscall_files[NR_syscalls];\n#endif\n\tint\t\t\tstop_count;\n\tint\t\t\tclock_id;\n\tint\t\t\tnr_topts;\n\tbool\t\t\tclear_trace;\n\tint\t\t\tbuffer_percent;\n\tunsigned int\t\tn_err_log_entries;\n\tstruct tracer\t\t*current_trace;\n\tunsigned int\t\ttrace_flags;\n\tunsigned char\t\ttrace_flags_index[TRACE_FLAGS_MAX_SIZE];\n\tunsigned int\t\tflags;\n\traw_spinlock_t\t\tstart_lock;\n\tstruct list_head\terr_log;\n\tstruct dentry\t\t*dir;\n\tstruct dentry\t\t*options;\n\tstruct dentry\t\t*percpu_dir;\n\tstruct dentry\t\t*event_dir;\n\tstruct trace_options\t*topts;\n\tstruct list_head\tsystems;\n\tstruct list_head\tevents;\n\tstruct trace_event_file *trace_marker_file;\n\tcpumask_var_t\t\ttracing_cpumask;  \n\t \n\tcpumask_var_t\t\tpipe_cpumask;\n\tint\t\t\tref;\n\tint\t\t\ttrace_ref;\n#ifdef CONFIG_FUNCTION_TRACER\n\tstruct ftrace_ops\t*ops;\n\tstruct trace_pid_list\t__rcu *function_pids;\n\tstruct trace_pid_list\t__rcu *function_no_pids;\n#ifdef CONFIG_DYNAMIC_FTRACE\n\t \n\tstruct list_head\tfunc_probes;\n\tstruct list_head\tmod_trace;\n\tstruct list_head\tmod_notrace;\n#endif\n\t \n\tint\t\t\tfunction_enabled;\n#endif\n\tint\t\t\tno_filter_buffering_ref;\n\tstruct list_head\thist_vars;\n#ifdef CONFIG_TRACER_SNAPSHOT\n\tstruct cond_snapshot\t*cond_snapshot;\n#endif\n\tstruct trace_func_repeats\t__percpu *last_func_repeats;\n};\n\nenum {\n\tTRACE_ARRAY_FL_GLOBAL\t= (1 << 0)\n};\n\nextern struct list_head ftrace_trace_arrays;\n\nextern struct mutex trace_types_lock;\n\nextern int trace_array_get(struct trace_array *tr);\nextern int tracing_check_open_get_tr(struct trace_array *tr);\nextern struct trace_array *trace_array_find(const char *instance);\nextern struct trace_array *trace_array_find_get(const char *instance);\n\nextern u64 tracing_event_time_stamp(struct trace_buffer *buffer, struct ring_buffer_event *rbe);\nextern int tracing_set_filter_buffering(struct trace_array *tr, bool set);\nextern int tracing_set_clock(struct trace_array *tr, const char *clockstr);\n\nextern bool trace_clock_in_ns(struct trace_array *tr);\n\n \nstatic inline struct trace_array *top_trace_array(void)\n{\n\tstruct trace_array *tr;\n\n\tif (list_empty(&ftrace_trace_arrays))\n\t\treturn NULL;\n\n\ttr = list_entry(ftrace_trace_arrays.prev,\n\t\t\ttypeof(*tr), list);\n\tWARN_ON(!(tr->flags & TRACE_ARRAY_FL_GLOBAL));\n\treturn tr;\n}\n\n#define FTRACE_CMP_TYPE(var, type) \\\n\t__builtin_types_compatible_p(typeof(var), type *)\n\n#undef IF_ASSIGN\n#define IF_ASSIGN(var, entry, etype, id)\t\t\t\\\n\tif (FTRACE_CMP_TYPE(var, etype)) {\t\t\t\\\n\t\tvar = (typeof(var))(entry);\t\t\t\\\n\t\tWARN_ON(id != 0 && (entry)->type != id);\t\\\n\t\tbreak;\t\t\t\t\t\t\\\n\t}\n\n \nextern void __ftrace_bad_type(void);\n\n \n#define trace_assign_type(var, ent)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tIF_ASSIGN(var, ent, struct ftrace_entry, TRACE_FN);\t\\\n\t\tIF_ASSIGN(var, ent, struct ctx_switch_entry, 0);\t\\\n\t\tIF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);\t\\\n\t\tIF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\\\n\t\tIF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);\t\\\n\t\tIF_ASSIGN(var, ent, struct bprint_entry, TRACE_BPRINT);\t\\\n\t\tIF_ASSIGN(var, ent, struct bputs_entry, TRACE_BPUTS);\t\\\n\t\tIF_ASSIGN(var, ent, struct hwlat_entry, TRACE_HWLAT);\t\\\n\t\tIF_ASSIGN(var, ent, struct osnoise_entry, TRACE_OSNOISE);\\\n\t\tIF_ASSIGN(var, ent, struct timerlat_entry, TRACE_TIMERLAT);\\\n\t\tIF_ASSIGN(var, ent, struct raw_data_entry, TRACE_RAW_DATA);\\\n\t\tIF_ASSIGN(var, ent, struct trace_mmiotrace_rw,\t\t\\\n\t\t\t  TRACE_MMIO_RW);\t\t\t\t\\\n\t\tIF_ASSIGN(var, ent, struct trace_mmiotrace_map,\t\t\\\n\t\t\t  TRACE_MMIO_MAP);\t\t\t\t\\\n\t\tIF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \\\n\t\tIF_ASSIGN(var, ent, struct ftrace_graph_ent_entry,\t\\\n\t\t\t  TRACE_GRAPH_ENT);\t\t\\\n\t\tIF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,\t\\\n\t\t\t  TRACE_GRAPH_RET);\t\t\\\n\t\tIF_ASSIGN(var, ent, struct func_repeats_entry,\t\t\\\n\t\t\t  TRACE_FUNC_REPEATS);\t\t\t\t\\\n\t\t__ftrace_bad_type();\t\t\t\t\t\\\n\t} while (0)\n\n \nstruct tracer_opt {\n\tconst char\t*name;  \n\tu32\t\tbit;  \n};\n\n \nstruct tracer_flags {\n\tu32\t\t\tval;\n\tstruct tracer_opt\t*opts;\n\tstruct tracer\t\t*trace;\n};\n\n \n#define TRACER_OPT(s, b)\t.name = #s, .bit = b\n\n\nstruct trace_option_dentry {\n\tstruct tracer_opt\t\t*opt;\n\tstruct tracer_flags\t\t*flags;\n\tstruct trace_array\t\t*tr;\n\tstruct dentry\t\t\t*entry;\n};\n\n \nstruct tracer {\n\tconst char\t\t*name;\n\tint\t\t\t(*init)(struct trace_array *tr);\n\tvoid\t\t\t(*reset)(struct trace_array *tr);\n\tvoid\t\t\t(*start)(struct trace_array *tr);\n\tvoid\t\t\t(*stop)(struct trace_array *tr);\n\tint\t\t\t(*update_thresh)(struct trace_array *tr);\n\tvoid\t\t\t(*open)(struct trace_iterator *iter);\n\tvoid\t\t\t(*pipe_open)(struct trace_iterator *iter);\n\tvoid\t\t\t(*close)(struct trace_iterator *iter);\n\tvoid\t\t\t(*pipe_close)(struct trace_iterator *iter);\n\tssize_t\t\t\t(*read)(struct trace_iterator *iter,\n\t\t\t\t\tstruct file *filp, char __user *ubuf,\n\t\t\t\t\tsize_t cnt, loff_t *ppos);\n\tssize_t\t\t\t(*splice_read)(struct trace_iterator *iter,\n\t\t\t\t\t       struct file *filp,\n\t\t\t\t\t       loff_t *ppos,\n\t\t\t\t\t       struct pipe_inode_info *pipe,\n\t\t\t\t\t       size_t len,\n\t\t\t\t\t       unsigned int flags);\n#ifdef CONFIG_FTRACE_STARTUP_TEST\n\tint\t\t\t(*selftest)(struct tracer *trace,\n\t\t\t\t\t    struct trace_array *tr);\n#endif\n\tvoid\t\t\t(*print_header)(struct seq_file *m);\n\tenum print_line_t\t(*print_line)(struct trace_iterator *iter);\n\t \n\tint\t\t\t(*set_flag)(struct trace_array *tr,\n\t\t\t\t\t    u32 old_flags, u32 bit, int set);\n\t \n\tint\t\t\t(*flag_changed)(struct trace_array *tr,\n\t\t\t\t\t\tu32 mask, int set);\n\tstruct tracer\t\t*next;\n\tstruct tracer_flags\t*flags;\n\tint\t\t\tenabled;\n\tbool\t\t\tprint_max;\n\tbool\t\t\tallow_instances;\n#ifdef CONFIG_TRACER_MAX_TRACE\n\tbool\t\t\tuse_max_tr;\n#endif\n\t \n\tbool\t\t\tnoboot;\n};\n\nstatic inline struct ring_buffer_iter *\ntrace_buffer_iter(struct trace_iterator *iter, int cpu)\n{\n\treturn iter->buffer_iter ? iter->buffer_iter[cpu] : NULL;\n}\n\nint tracer_init(struct tracer *t, struct trace_array *tr);\nint tracing_is_enabled(void);\nvoid tracing_reset_online_cpus(struct array_buffer *buf);\nvoid tracing_reset_all_online_cpus(void);\nvoid tracing_reset_all_online_cpus_unlocked(void);\nint tracing_open_generic(struct inode *inode, struct file *filp);\nint tracing_open_generic_tr(struct inode *inode, struct file *filp);\nint tracing_open_file_tr(struct inode *inode, struct file *filp);\nint tracing_release_file_tr(struct inode *inode, struct file *filp);\nint tracing_single_release_file_tr(struct inode *inode, struct file *filp);\nbool tracing_is_disabled(void);\nbool tracer_tracing_is_on(struct trace_array *tr);\nvoid tracer_tracing_on(struct trace_array *tr);\nvoid tracer_tracing_off(struct trace_array *tr);\nstruct dentry *trace_create_file(const char *name,\n\t\t\t\t umode_t mode,\n\t\t\t\t struct dentry *parent,\n\t\t\t\t void *data,\n\t\t\t\t const struct file_operations *fops);\n\nint tracing_init_dentry(void);\n\nstruct ring_buffer_event;\n\nstruct ring_buffer_event *\ntrace_buffer_lock_reserve(struct trace_buffer *buffer,\n\t\t\t  int type,\n\t\t\t  unsigned long len,\n\t\t\t  unsigned int trace_ctx);\n\nstruct trace_entry *tracing_get_trace_entry(struct trace_array *tr,\n\t\t\t\t\t\tstruct trace_array_cpu *data);\n\nstruct trace_entry *trace_find_next_entry(struct trace_iterator *iter,\n\t\t\t\t\t  int *ent_cpu, u64 *ent_ts);\n\nvoid trace_buffer_unlock_commit_nostack(struct trace_buffer *buffer,\n\t\t\t\t\tstruct ring_buffer_event *event);\n\nbool trace_is_tracepoint_string(const char *str);\nconst char *trace_event_format(struct trace_iterator *iter, const char *fmt);\nvoid trace_check_vprintf(struct trace_iterator *iter, const char *fmt,\n\t\t\t va_list ap) __printf(2, 0);\nchar *trace_iter_expand_format(struct trace_iterator *iter);\n\nint trace_empty(struct trace_iterator *iter);\n\nvoid *trace_find_next_entry_inc(struct trace_iterator *iter);\n\nvoid trace_init_global_iter(struct trace_iterator *iter);\n\nvoid tracing_iter_reset(struct trace_iterator *iter, int cpu);\n\nunsigned long trace_total_entries_cpu(struct trace_array *tr, int cpu);\nunsigned long trace_total_entries(struct trace_array *tr);\n\nvoid trace_function(struct trace_array *tr,\n\t\t    unsigned long ip,\n\t\t    unsigned long parent_ip,\n\t\t    unsigned int trace_ctx);\nvoid trace_graph_function(struct trace_array *tr,\n\t\t    unsigned long ip,\n\t\t    unsigned long parent_ip,\n\t\t    unsigned int trace_ctx);\nvoid trace_latency_header(struct seq_file *m);\nvoid trace_default_header(struct seq_file *m);\nvoid print_trace_header(struct seq_file *m, struct trace_iterator *iter);\n\nvoid trace_graph_return(struct ftrace_graph_ret *trace);\nint trace_graph_entry(struct ftrace_graph_ent *trace);\nvoid set_graph_array(struct trace_array *tr);\n\nvoid tracing_start_cmdline_record(void);\nvoid tracing_stop_cmdline_record(void);\nvoid tracing_start_tgid_record(void);\nvoid tracing_stop_tgid_record(void);\n\nint register_tracer(struct tracer *type);\nint is_tracing_stopped(void);\n\nloff_t tracing_lseek(struct file *file, loff_t offset, int whence);\n\nextern cpumask_var_t __read_mostly tracing_buffer_mask;\n\n#define for_each_tracing_cpu(cpu)\t\\\n\tfor_each_cpu(cpu, tracing_buffer_mask)\n\nextern unsigned long nsecs_to_usecs(unsigned long nsecs);\n\nextern unsigned long tracing_thresh;\n\n \n\nextern int pid_max;\n\nbool trace_find_filtered_pid(struct trace_pid_list *filtered_pids,\n\t\t\t     pid_t search_pid);\nbool trace_ignore_this_task(struct trace_pid_list *filtered_pids,\n\t\t\t    struct trace_pid_list *filtered_no_pids,\n\t\t\t    struct task_struct *task);\nvoid trace_filter_add_remove_task(struct trace_pid_list *pid_list,\n\t\t\t\t  struct task_struct *self,\n\t\t\t\t  struct task_struct *task);\nvoid *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos);\nvoid *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos);\nint trace_pid_show(struct seq_file *m, void *v);\nint trace_pid_write(struct trace_pid_list *filtered_pids,\n\t\t    struct trace_pid_list **new_pid_list,\n\t\t    const char __user *ubuf, size_t cnt);\n\n#ifdef CONFIG_TRACER_MAX_TRACE\nvoid update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu,\n\t\t   void *cond_data);\nvoid update_max_tr_single(struct trace_array *tr,\n\t\t\t  struct task_struct *tsk, int cpu);\n\n#ifdef CONFIG_FSNOTIFY\n#define LATENCY_FS_NOTIFY\n#endif\n#endif  \n\n#ifdef LATENCY_FS_NOTIFY\nvoid latency_fsnotify(struct trace_array *tr);\n#else\nstatic inline void latency_fsnotify(struct trace_array *tr) { }\n#endif\n\n#ifdef CONFIG_STACKTRACE\nvoid __trace_stack(struct trace_array *tr, unsigned int trace_ctx, int skip);\n#else\nstatic inline void __trace_stack(struct trace_array *tr, unsigned int trace_ctx,\n\t\t\t\t int skip)\n{\n}\n#endif  \n\nvoid trace_last_func_repeats(struct trace_array *tr,\n\t\t\t     struct trace_func_repeats *last_info,\n\t\t\t     unsigned int trace_ctx);\n\nextern u64 ftrace_now(int cpu);\n\nextern void trace_find_cmdline(int pid, char comm[]);\nextern int trace_find_tgid(int pid);\nextern void trace_event_follow_fork(struct trace_array *tr, bool enable);\n\n#ifdef CONFIG_DYNAMIC_FTRACE\nextern unsigned long ftrace_update_tot_cnt;\nextern unsigned long ftrace_number_of_pages;\nextern unsigned long ftrace_number_of_groups;\nvoid ftrace_init_trace_array(struct trace_array *tr);\n#else\nstatic inline void ftrace_init_trace_array(struct trace_array *tr) { }\n#endif\n#define DYN_FTRACE_TEST_NAME trace_selftest_dynamic_test_func\nextern int DYN_FTRACE_TEST_NAME(void);\n#define DYN_FTRACE_TEST_NAME2 trace_selftest_dynamic_test_func2\nextern int DYN_FTRACE_TEST_NAME2(void);\n\nextern bool ring_buffer_expanded;\nextern bool tracing_selftest_disabled;\n\n#ifdef CONFIG_FTRACE_STARTUP_TEST\nextern void __init disable_tracing_selftest(const char *reason);\n\nextern int trace_selftest_startup_function(struct tracer *trace,\n\t\t\t\t\t   struct trace_array *tr);\nextern int trace_selftest_startup_function_graph(struct tracer *trace,\n\t\t\t\t\t\t struct trace_array *tr);\nextern int trace_selftest_startup_irqsoff(struct tracer *trace,\n\t\t\t\t\t  struct trace_array *tr);\nextern int trace_selftest_startup_preemptoff(struct tracer *trace,\n\t\t\t\t\t     struct trace_array *tr);\nextern int trace_selftest_startup_preemptirqsoff(struct tracer *trace,\n\t\t\t\t\t\t struct trace_array *tr);\nextern int trace_selftest_startup_wakeup(struct tracer *trace,\n\t\t\t\t\t struct trace_array *tr);\nextern int trace_selftest_startup_nop(struct tracer *trace,\n\t\t\t\t\t struct trace_array *tr);\nextern int trace_selftest_startup_branch(struct tracer *trace,\n\t\t\t\t\t struct trace_array *tr);\n \n#define __tracer_data\t\t__refdata\n#else\nstatic inline void __init disable_tracing_selftest(const char *reason)\n{\n}\n \n#define __tracer_data\t\t__read_mostly\n#endif  \n\nextern void *head_page(struct trace_array_cpu *data);\nextern unsigned long long ns2usecs(u64 nsec);\nextern int\ntrace_vbprintk(unsigned long ip, const char *fmt, va_list args);\nextern int\ntrace_vprintk(unsigned long ip, const char *fmt, va_list args);\nextern int\ntrace_array_vprintk(struct trace_array *tr,\n\t\t    unsigned long ip, const char *fmt, va_list args);\nint trace_array_printk_buf(struct trace_buffer *buffer,\n\t\t\t   unsigned long ip, const char *fmt, ...);\nvoid trace_printk_seq(struct trace_seq *s);\nenum print_line_t print_trace_line(struct trace_iterator *iter);\n\nextern char trace_find_mark(unsigned long long duration);\n\nstruct ftrace_hash;\n\nstruct ftrace_mod_load {\n\tstruct list_head\tlist;\n\tchar\t\t\t*func;\n\tchar\t\t\t*module;\n\tint\t\t\t enable;\n};\n\nenum {\n\tFTRACE_HASH_FL_MOD\t= (1 << 0),\n};\n\nstruct ftrace_hash {\n\tunsigned long\t\tsize_bits;\n\tstruct hlist_head\t*buckets;\n\tunsigned long\t\tcount;\n\tunsigned long\t\tflags;\n\tstruct rcu_head\t\trcu;\n};\n\nstruct ftrace_func_entry *\nftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip);\n\nstatic __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)\n{\n\treturn !hash || !(hash->count || (hash->flags & FTRACE_HASH_FL_MOD));\n}\n\n \n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\n \n#define TRACE_GRAPH_PRINT_OVERRUN       0x1\n#define TRACE_GRAPH_PRINT_CPU           0x2\n#define TRACE_GRAPH_PRINT_OVERHEAD      0x4\n#define TRACE_GRAPH_PRINT_PROC          0x8\n#define TRACE_GRAPH_PRINT_DURATION      0x10\n#define TRACE_GRAPH_PRINT_ABS_TIME      0x20\n#define TRACE_GRAPH_PRINT_REL_TIME      0x40\n#define TRACE_GRAPH_PRINT_IRQS          0x80\n#define TRACE_GRAPH_PRINT_TAIL          0x100\n#define TRACE_GRAPH_SLEEP_TIME          0x200\n#define TRACE_GRAPH_GRAPH_TIME          0x400\n#define TRACE_GRAPH_PRINT_RETVAL        0x800\n#define TRACE_GRAPH_PRINT_RETVAL_HEX    0x1000\n#define TRACE_GRAPH_PRINT_FILL_SHIFT\t28\n#define TRACE_GRAPH_PRINT_FILL_MASK\t(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)\n\nextern void ftrace_graph_sleep_time_control(bool enable);\n\n#ifdef CONFIG_FUNCTION_PROFILER\nextern void ftrace_graph_graph_time_control(bool enable);\n#else\nstatic inline void ftrace_graph_graph_time_control(bool enable) { }\n#endif\n\nextern enum print_line_t\nprint_graph_function_flags(struct trace_iterator *iter, u32 flags);\nextern void print_graph_headers_flags(struct seq_file *s, u32 flags);\nextern void\ntrace_print_graph_duration(unsigned long long duration, struct trace_seq *s);\nextern void graph_trace_open(struct trace_iterator *iter);\nextern void graph_trace_close(struct trace_iterator *iter);\nextern int __trace_graph_entry(struct trace_array *tr,\n\t\t\t       struct ftrace_graph_ent *trace,\n\t\t\t       unsigned int trace_ctx);\nextern void __trace_graph_return(struct trace_array *tr,\n\t\t\t\t struct ftrace_graph_ret *trace,\n\t\t\t\t unsigned int trace_ctx);\n\n#ifdef CONFIG_DYNAMIC_FTRACE\nextern struct ftrace_hash __rcu *ftrace_graph_hash;\nextern struct ftrace_hash __rcu *ftrace_graph_notrace_hash;\n\nstatic inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)\n{\n\tunsigned long addr = trace->func;\n\tint ret = 0;\n\tstruct ftrace_hash *hash;\n\n\tpreempt_disable_notrace();\n\n\t \n\thash = rcu_dereference_protected(ftrace_graph_hash, !preemptible());\n\n\tif (ftrace_hash_empty(hash)) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tif (ftrace_lookup_ip(hash, addr)) {\n\n\t\t \n\t\ttrace_recursion_set(TRACE_GRAPH_BIT);\n\t\ttrace_recursion_set_depth(trace->depth);\n\n\t\t \n\t\tif (in_hardirq())\n\t\t\ttrace_recursion_set(TRACE_IRQ_BIT);\n\t\telse\n\t\t\ttrace_recursion_clear(TRACE_IRQ_BIT);\n\t\tret = 1;\n\t}\n\nout:\n\tpreempt_enable_notrace();\n\treturn ret;\n}\n\nstatic inline void ftrace_graph_addr_finish(struct ftrace_graph_ret *trace)\n{\n\tif (trace_recursion_test(TRACE_GRAPH_BIT) &&\n\t    trace->depth == trace_recursion_depth())\n\t\ttrace_recursion_clear(TRACE_GRAPH_BIT);\n}\n\nstatic inline int ftrace_graph_notrace_addr(unsigned long addr)\n{\n\tint ret = 0;\n\tstruct ftrace_hash *notrace_hash;\n\n\tpreempt_disable_notrace();\n\n\t \n\tnotrace_hash = rcu_dereference_protected(ftrace_graph_notrace_hash,\n\t\t\t\t\t\t !preemptible());\n\n\tif (ftrace_lookup_ip(notrace_hash, addr))\n\t\tret = 1;\n\n\tpreempt_enable_notrace();\n\treturn ret;\n}\n#else\nstatic inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)\n{\n\treturn 1;\n}\n\nstatic inline int ftrace_graph_notrace_addr(unsigned long addr)\n{\n\treturn 0;\n}\nstatic inline void ftrace_graph_addr_finish(struct ftrace_graph_ret *trace)\n{ }\n#endif  \n\nextern unsigned int fgraph_max_depth;\n\nstatic inline bool ftrace_graph_ignore_func(struct ftrace_graph_ent *trace)\n{\n\t \n\treturn !(trace_recursion_test(TRACE_GRAPH_BIT) ||\n\t\t ftrace_graph_addr(trace)) ||\n\t\t(trace->depth < 0) ||\n\t\t(fgraph_max_depth && trace->depth >= fgraph_max_depth);\n}\n\n#else  \nstatic inline enum print_line_t\nprint_graph_function_flags(struct trace_iterator *iter, u32 flags)\n{\n\treturn TRACE_TYPE_UNHANDLED;\n}\n#endif  \n\nextern struct list_head ftrace_pids;\n\n#ifdef CONFIG_FUNCTION_TRACER\n\n#define FTRACE_PID_IGNORE\t-1\n#define FTRACE_PID_TRACE\t-2\n\nstruct ftrace_func_command {\n\tstruct list_head\tlist;\n\tchar\t\t\t*name;\n\tint\t\t\t(*func)(struct trace_array *tr,\n\t\t\t\t\tstruct ftrace_hash *hash,\n\t\t\t\t\tchar *func, char *cmd,\n\t\t\t\t\tchar *params, int enable);\n};\nextern bool ftrace_filter_param __initdata;\nstatic inline int ftrace_trace_task(struct trace_array *tr)\n{\n\treturn this_cpu_read(tr->array_buffer.data->ftrace_ignore_pid) !=\n\t\tFTRACE_PID_IGNORE;\n}\nextern int ftrace_is_dead(void);\nint ftrace_create_function_files(struct trace_array *tr,\n\t\t\t\t struct dentry *parent);\nvoid ftrace_destroy_function_files(struct trace_array *tr);\nint ftrace_allocate_ftrace_ops(struct trace_array *tr);\nvoid ftrace_free_ftrace_ops(struct trace_array *tr);\nvoid ftrace_init_global_array_ops(struct trace_array *tr);\nvoid ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func);\nvoid ftrace_reset_array_ops(struct trace_array *tr);\nvoid ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);\nvoid ftrace_init_tracefs_toplevel(struct trace_array *tr,\n\t\t\t\t  struct dentry *d_tracer);\nvoid ftrace_clear_pids(struct trace_array *tr);\nint init_function_trace(void);\nvoid ftrace_pid_follow_fork(struct trace_array *tr, bool enable);\n#else\nstatic inline int ftrace_trace_task(struct trace_array *tr)\n{\n\treturn 1;\n}\nstatic inline int ftrace_is_dead(void) { return 0; }\nstatic inline int\nftrace_create_function_files(struct trace_array *tr,\n\t\t\t     struct dentry *parent)\n{\n\treturn 0;\n}\nstatic inline int ftrace_allocate_ftrace_ops(struct trace_array *tr)\n{\n\treturn 0;\n}\nstatic inline void ftrace_free_ftrace_ops(struct trace_array *tr) { }\nstatic inline void ftrace_destroy_function_files(struct trace_array *tr) { }\nstatic inline __init void\nftrace_init_global_array_ops(struct trace_array *tr) { }\nstatic inline void ftrace_reset_array_ops(struct trace_array *tr) { }\nstatic inline void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d) { }\nstatic inline void ftrace_init_tracefs_toplevel(struct trace_array *tr, struct dentry *d) { }\nstatic inline void ftrace_clear_pids(struct trace_array *tr) { }\nstatic inline int init_function_trace(void) { return 0; }\nstatic inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) { }\n \n#define ftrace_init_array_ops(tr, func) do { } while (0)\n#endif  \n\n#if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)\n\nstruct ftrace_probe_ops {\n\tvoid\t\t\t(*func)(unsigned long ip,\n\t\t\t\t\tunsigned long parent_ip,\n\t\t\t\t\tstruct trace_array *tr,\n\t\t\t\t\tstruct ftrace_probe_ops *ops,\n\t\t\t\t\tvoid *data);\n\tint\t\t\t(*init)(struct ftrace_probe_ops *ops,\n\t\t\t\t\tstruct trace_array *tr,\n\t\t\t\t\tunsigned long ip, void *init_data,\n\t\t\t\t\tvoid **data);\n\tvoid\t\t\t(*free)(struct ftrace_probe_ops *ops,\n\t\t\t\t\tstruct trace_array *tr,\n\t\t\t\t\tunsigned long ip, void *data);\n\tint\t\t\t(*print)(struct seq_file *m,\n\t\t\t\t\t unsigned long ip,\n\t\t\t\t\t struct ftrace_probe_ops *ops,\n\t\t\t\t\t void *data);\n};\n\nstruct ftrace_func_mapper;\ntypedef int (*ftrace_mapper_func)(void *data);\n\nstruct ftrace_func_mapper *allocate_ftrace_func_mapper(void);\nvoid **ftrace_func_mapper_find_ip(struct ftrace_func_mapper *mapper,\n\t\t\t\t\t   unsigned long ip);\nint ftrace_func_mapper_add_ip(struct ftrace_func_mapper *mapper,\n\t\t\t       unsigned long ip, void *data);\nvoid *ftrace_func_mapper_remove_ip(struct ftrace_func_mapper *mapper,\n\t\t\t\t   unsigned long ip);\nvoid free_ftrace_func_mapper(struct ftrace_func_mapper *mapper,\n\t\t\t     ftrace_mapper_func free_func);\n\nextern int\nregister_ftrace_function_probe(char *glob, struct trace_array *tr,\n\t\t\t       struct ftrace_probe_ops *ops, void *data);\nextern int\nunregister_ftrace_function_probe_func(char *glob, struct trace_array *tr,\n\t\t\t\t      struct ftrace_probe_ops *ops);\nextern void clear_ftrace_function_probes(struct trace_array *tr);\n\nint register_ftrace_command(struct ftrace_func_command *cmd);\nint unregister_ftrace_command(struct ftrace_func_command *cmd);\n\nvoid ftrace_create_filter_files(struct ftrace_ops *ops,\n\t\t\t\tstruct dentry *parent);\nvoid ftrace_destroy_filter_files(struct ftrace_ops *ops);\n\nextern int ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,\n\t\t\t     int len, int reset);\nextern int ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,\n\t\t\t      int len, int reset);\n#else\nstruct ftrace_func_command;\n\nstatic inline __init int register_ftrace_command(struct ftrace_func_command *cmd)\n{\n\treturn -EINVAL;\n}\nstatic inline __init int unregister_ftrace_command(char *cmd_name)\n{\n\treturn -EINVAL;\n}\nstatic inline void clear_ftrace_function_probes(struct trace_array *tr)\n{\n}\n\n \n#define ftrace_create_filter_files(ops, parent) do { } while (0)\n#define ftrace_destroy_filter_files(ops) do { } while (0)\n#endif  \n\nbool ftrace_event_is_function(struct trace_event_call *call);\n\n \nstruct trace_parser {\n\tbool\t\tcont;\n\tchar\t\t*buffer;\n\tunsigned\tidx;\n\tunsigned\tsize;\n};\n\nstatic inline bool trace_parser_loaded(struct trace_parser *parser)\n{\n\treturn (parser->idx != 0);\n}\n\nstatic inline bool trace_parser_cont(struct trace_parser *parser)\n{\n\treturn parser->cont;\n}\n\nstatic inline void trace_parser_clear(struct trace_parser *parser)\n{\n\tparser->cont = false;\n\tparser->idx = 0;\n}\n\nextern int trace_parser_get_init(struct trace_parser *parser, int size);\nextern void trace_parser_put(struct trace_parser *parser);\nextern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,\n\tsize_t cnt, loff_t *ppos);\n\n \n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n# define FGRAPH_FLAGS\t\t\t\t\t\t\\\n\t\tC(DISPLAY_GRAPH,\t\"display-graph\"),\n#else\n# define FGRAPH_FLAGS\n#endif\n\n#ifdef CONFIG_BRANCH_TRACER\n# define BRANCH_FLAGS\t\t\t\t\t\\\n\t\tC(BRANCH,\t\t\"branch\"),\n#else\n# define BRANCH_FLAGS\n#endif\n\n#ifdef CONFIG_FUNCTION_TRACER\n# define FUNCTION_FLAGS\t\t\t\t\t\t\\\n\t\tC(FUNCTION,\t\t\"function-trace\"),\t\\\n\t\tC(FUNC_FORK,\t\t\"function-fork\"),\n# define FUNCTION_DEFAULT_FLAGS\t\tTRACE_ITER_FUNCTION\n#else\n# define FUNCTION_FLAGS\n# define FUNCTION_DEFAULT_FLAGS\t\t0UL\n# define TRACE_ITER_FUNC_FORK\t\t0UL\n#endif\n\n#ifdef CONFIG_STACKTRACE\n# define STACK_FLAGS\t\t\t\t\\\n\t\tC(STACKTRACE,\t\t\"stacktrace\"),\n#else\n# define STACK_FLAGS\n#endif\n\n \n#define TRACE_FLAGS\t\t\t\t\t\t\\\n\t\tC(PRINT_PARENT,\t\t\"print-parent\"),\t\\\n\t\tC(SYM_OFFSET,\t\t\"sym-offset\"),\t\t\\\n\t\tC(SYM_ADDR,\t\t\"sym-addr\"),\t\t\\\n\t\tC(VERBOSE,\t\t\"verbose\"),\t\t\\\n\t\tC(RAW,\t\t\t\"raw\"),\t\t\t\\\n\t\tC(HEX,\t\t\t\"hex\"),\t\t\t\\\n\t\tC(BIN,\t\t\t\"bin\"),\t\t\t\\\n\t\tC(BLOCK,\t\t\"block\"),\t\t\\\n\t\tC(FIELDS,\t\t\"fields\"),\t\t\\\n\t\tC(PRINTK,\t\t\"trace_printk\"),\t\\\n\t\tC(ANNOTATE,\t\t\"annotate\"),\t\t\\\n\t\tC(USERSTACKTRACE,\t\"userstacktrace\"),\t\\\n\t\tC(SYM_USEROBJ,\t\t\"sym-userobj\"),\t\t\\\n\t\tC(PRINTK_MSGONLY,\t\"printk-msg-only\"),\t\\\n\t\tC(CONTEXT_INFO,\t\t\"context-info\"),     \\\n\t\tC(LATENCY_FMT,\t\t\"latency-format\"),\t\\\n\t\tC(RECORD_CMD,\t\t\"record-cmd\"),\t\t\\\n\t\tC(RECORD_TGID,\t\t\"record-tgid\"),\t\t\\\n\t\tC(OVERWRITE,\t\t\"overwrite\"),\t\t\\\n\t\tC(STOP_ON_FREE,\t\t\"disable_on_free\"),\t\\\n\t\tC(IRQ_INFO,\t\t\"irq-info\"),\t\t\\\n\t\tC(MARKERS,\t\t\"markers\"),\t\t\\\n\t\tC(EVENT_FORK,\t\t\"event-fork\"),\t\t\\\n\t\tC(PAUSE_ON_TRACE,\t\"pause-on-trace\"),\t\\\n\t\tC(HASH_PTR,\t\t\"hash-ptr\"),\t  \\\n\t\tFUNCTION_FLAGS\t\t\t\t\t\\\n\t\tFGRAPH_FLAGS\t\t\t\t\t\\\n\t\tSTACK_FLAGS\t\t\t\t\t\\\n\t\tBRANCH_FLAGS\n\n \n#undef C\n#define C(a, b) TRACE_ITER_##a##_BIT\n\nenum trace_iterator_bits {\n\tTRACE_FLAGS\n\t \n\tTRACE_ITER_LAST_BIT\n};\n\n \n#undef C\n#define C(a, b) TRACE_ITER_##a = (1 << TRACE_ITER_##a##_BIT)\n\nenum trace_iterator_flags { TRACE_FLAGS };\n\n \n#define TRACE_ITER_SYM_MASK \\\n\t(TRACE_ITER_PRINT_PARENT|TRACE_ITER_SYM_OFFSET|TRACE_ITER_SYM_ADDR)\n\nextern struct tracer nop_trace;\n\n#ifdef CONFIG_BRANCH_TRACER\nextern int enable_branch_tracing(struct trace_array *tr);\nextern void disable_branch_tracing(void);\nstatic inline int trace_branch_enable(struct trace_array *tr)\n{\n\tif (tr->trace_flags & TRACE_ITER_BRANCH)\n\t\treturn enable_branch_tracing(tr);\n\treturn 0;\n}\nstatic inline void trace_branch_disable(void)\n{\n\t \n\tdisable_branch_tracing();\n}\n#else\nstatic inline int trace_branch_enable(struct trace_array *tr)\n{\n\treturn 0;\n}\nstatic inline void trace_branch_disable(void)\n{\n}\n#endif  \n\n \nint tracing_update_buffers(void);\n\nunion trace_synth_field {\n\tu8\t\t\t\tas_u8;\n\tu16\t\t\t\tas_u16;\n\tu32\t\t\t\tas_u32;\n\tu64\t\t\t\tas_u64;\n\tstruct trace_dynamic_info\tas_dynamic;\n};\n\nstruct ftrace_event_field {\n\tstruct list_head\tlink;\n\tconst char\t\t*name;\n\tconst char\t\t*type;\n\tint\t\t\tfilter_type;\n\tint\t\t\toffset;\n\tint\t\t\tsize;\n\tint\t\t\tis_signed;\n\tint\t\t\tlen;\n};\n\nstruct prog_entry;\n\nstruct event_filter {\n\tstruct prog_entry __rcu\t*prog;\n\tchar\t\t\t*filter_string;\n};\n\nstruct event_subsystem {\n\tstruct list_head\tlist;\n\tconst char\t\t*name;\n\tstruct event_filter\t*filter;\n\tint\t\t\tref_count;\n};\n\nstruct trace_subsystem_dir {\n\tstruct list_head\t\tlist;\n\tstruct event_subsystem\t\t*subsystem;\n\tstruct trace_array\t\t*tr;\n\tstruct eventfs_file             *ef;\n\tint\t\t\t\tref_count;\n\tint\t\t\t\tnr_events;\n};\n\nextern int call_filter_check_discard(struct trace_event_call *call, void *rec,\n\t\t\t\t     struct trace_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event);\n\nvoid trace_buffer_unlock_commit_regs(struct trace_array *tr,\n\t\t\t\t     struct trace_buffer *buffer,\n\t\t\t\t     struct ring_buffer_event *event,\n\t\t\t\t     unsigned int trcace_ctx,\n\t\t\t\t     struct pt_regs *regs);\n\nstatic inline void trace_buffer_unlock_commit(struct trace_array *tr,\n\t\t\t\t\t      struct trace_buffer *buffer,\n\t\t\t\t\t      struct ring_buffer_event *event,\n\t\t\t\t\t      unsigned int trace_ctx)\n{\n\ttrace_buffer_unlock_commit_regs(tr, buffer, event, trace_ctx, NULL);\n}\n\nDECLARE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);\nDECLARE_PER_CPU(int, trace_buffered_event_cnt);\nvoid trace_buffered_event_disable(void);\nvoid trace_buffered_event_enable(void);\n\nvoid early_enable_events(struct trace_array *tr, char *buf, bool disable_first);\n\nstatic inline void\n__trace_event_discard_commit(struct trace_buffer *buffer,\n\t\t\t     struct ring_buffer_event *event)\n{\n\tif (this_cpu_read(trace_buffered_event) == event) {\n\t\t \n\t\tthis_cpu_dec(trace_buffered_event_cnt);\n\t\tpreempt_enable_notrace();\n\t\treturn;\n\t}\n\t \n\tring_buffer_discard_commit(buffer, event);\n}\n\n \nstatic inline bool\n__event_trigger_test_discard(struct trace_event_file *file,\n\t\t\t     struct trace_buffer *buffer,\n\t\t\t     struct ring_buffer_event *event,\n\t\t\t     void *entry,\n\t\t\t     enum event_trigger_type *tt)\n{\n\tunsigned long eflags = file->flags;\n\n\tif (eflags & EVENT_FILE_FL_TRIGGER_COND)\n\t\t*tt = event_triggers_call(file, buffer, entry, event);\n\n\tif (likely(!(file->flags & (EVENT_FILE_FL_SOFT_DISABLED |\n\t\t\t\t    EVENT_FILE_FL_FILTERED |\n\t\t\t\t    EVENT_FILE_FL_PID_FILTER))))\n\t\treturn false;\n\n\tif (file->flags & EVENT_FILE_FL_SOFT_DISABLED)\n\t\tgoto discard;\n\n\tif (file->flags & EVENT_FILE_FL_FILTERED &&\n\t    !filter_match_preds(file->filter, entry))\n\t\tgoto discard;\n\n\tif ((file->flags & EVENT_FILE_FL_PID_FILTER) &&\n\t    trace_event_ignore_this_pid(file))\n\t\tgoto discard;\n\n\treturn false;\n discard:\n\t__trace_event_discard_commit(buffer, event);\n\treturn true;\n}\n\n \nstatic inline void\nevent_trigger_unlock_commit(struct trace_event_file *file,\n\t\t\t    struct trace_buffer *buffer,\n\t\t\t    struct ring_buffer_event *event,\n\t\t\t    void *entry, unsigned int trace_ctx)\n{\n\tenum event_trigger_type tt = ETT_NONE;\n\n\tif (!__event_trigger_test_discard(file, buffer, event, entry, &tt))\n\t\ttrace_buffer_unlock_commit(file->tr, buffer, event, trace_ctx);\n\n\tif (tt)\n\t\tevent_triggers_post_call(file, tt);\n}\n\n#define FILTER_PRED_INVALID\t((unsigned short)-1)\n#define FILTER_PRED_IS_RIGHT\t(1 << 15)\n#define FILTER_PRED_FOLD\t(1 << 15)\n\n \n#define MAX_FILTER_PRED\t\t16384\n\nstruct filter_pred;\nstruct regex;\n\ntypedef int (*regex_match_func)(char *str, struct regex *r, int len);\n\nenum regex_type {\n\tMATCH_FULL = 0,\n\tMATCH_FRONT_ONLY,\n\tMATCH_MIDDLE_ONLY,\n\tMATCH_END_ONLY,\n\tMATCH_GLOB,\n\tMATCH_INDEX,\n};\n\nstruct regex {\n\tchar\t\t\tpattern[MAX_FILTER_STR_VAL];\n\tint\t\t\tlen;\n\tint\t\t\tfield_len;\n\tregex_match_func\tmatch;\n};\n\nstatic inline bool is_string_field(struct ftrace_event_field *field)\n{\n\treturn field->filter_type == FILTER_DYN_STRING ||\n\t       field->filter_type == FILTER_RDYN_STRING ||\n\t       field->filter_type == FILTER_STATIC_STRING ||\n\t       field->filter_type == FILTER_PTR_STRING ||\n\t       field->filter_type == FILTER_COMM;\n}\n\nstatic inline bool is_function_field(struct ftrace_event_field *field)\n{\n\treturn field->filter_type == FILTER_TRACE_FN;\n}\n\nextern enum regex_type\nfilter_parse_regex(char *buff, int len, char **search, int *not);\nextern void print_event_filter(struct trace_event_file *file,\n\t\t\t       struct trace_seq *s);\nextern int apply_event_filter(struct trace_event_file *file,\n\t\t\t      char *filter_string);\nextern int apply_subsystem_event_filter(struct trace_subsystem_dir *dir,\n\t\t\t\t\tchar *filter_string);\nextern void print_subsystem_event_filter(struct event_subsystem *system,\n\t\t\t\t\t struct trace_seq *s);\nextern int filter_assign_type(const char *type);\nextern int create_event_filter(struct trace_array *tr,\n\t\t\t       struct trace_event_call *call,\n\t\t\t       char *filter_str, bool set_str,\n\t\t\t       struct event_filter **filterp);\nextern void free_event_filter(struct event_filter *filter);\n\nstruct ftrace_event_field *\ntrace_find_event_field(struct trace_event_call *call, char *name);\n\nextern void trace_event_enable_cmd_record(bool enable);\nextern void trace_event_enable_tgid_record(bool enable);\n\nextern int event_trace_init(void);\nextern int init_events(void);\nextern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);\nextern int event_trace_del_tracer(struct trace_array *tr);\nextern void __trace_early_add_events(struct trace_array *tr);\n\nextern struct trace_event_file *__find_event_file(struct trace_array *tr,\n\t\t\t\t\t\t  const char *system,\n\t\t\t\t\t\t  const char *event);\nextern struct trace_event_file *find_event_file(struct trace_array *tr,\n\t\t\t\t\t\tconst char *system,\n\t\t\t\t\t\tconst char *event);\n\nstatic inline void *event_file_data(struct file *filp)\n{\n\treturn READ_ONCE(file_inode(filp)->i_private);\n}\n\nextern struct mutex event_mutex;\nextern struct list_head ftrace_events;\n\nextern const struct file_operations event_trigger_fops;\nextern const struct file_operations event_hist_fops;\nextern const struct file_operations event_hist_debug_fops;\nextern const struct file_operations event_inject_fops;\n\n#ifdef CONFIG_HIST_TRIGGERS\nextern int register_trigger_hist_cmd(void);\nextern int register_trigger_hist_enable_disable_cmds(void);\n#else\nstatic inline int register_trigger_hist_cmd(void) { return 0; }\nstatic inline int register_trigger_hist_enable_disable_cmds(void) { return 0; }\n#endif\n\nextern int register_trigger_cmds(void);\nextern void clear_event_triggers(struct trace_array *tr);\n\nenum {\n\tEVENT_TRIGGER_FL_PROBE\t\t= BIT(0),\n};\n\nstruct event_trigger_data {\n\tunsigned long\t\t\tcount;\n\tint\t\t\t\tref;\n\tint\t\t\t\tflags;\n\tstruct event_trigger_ops\t*ops;\n\tstruct event_command\t\t*cmd_ops;\n\tstruct event_filter __rcu\t*filter;\n\tchar\t\t\t\t*filter_str;\n\tvoid\t\t\t\t*private_data;\n\tbool\t\t\t\tpaused;\n\tbool\t\t\t\tpaused_tmp;\n\tstruct list_head\t\tlist;\n\tchar\t\t\t\t*name;\n\tstruct list_head\t\tnamed_list;\n\tstruct event_trigger_data\t*named_data;\n};\n\n \n#define ENABLE_EVENT_STR\t\"enable_event\"\n#define DISABLE_EVENT_STR\t\"disable_event\"\n#define ENABLE_HIST_STR\t\t\"enable_hist\"\n#define DISABLE_HIST_STR\t\"disable_hist\"\n\nstruct enable_trigger_data {\n\tstruct trace_event_file\t\t*file;\n\tbool\t\t\t\tenable;\n\tbool\t\t\t\thist;\n};\n\nextern int event_enable_trigger_print(struct seq_file *m,\n\t\t\t\t      struct event_trigger_data *data);\nextern void event_enable_trigger_free(struct event_trigger_data *data);\nextern int event_enable_trigger_parse(struct event_command *cmd_ops,\n\t\t\t\t      struct trace_event_file *file,\n\t\t\t\t      char *glob, char *cmd,\n\t\t\t\t      char *param_and_filter);\nextern int event_enable_register_trigger(char *glob,\n\t\t\t\t\t struct event_trigger_data *data,\n\t\t\t\t\t struct trace_event_file *file);\nextern void event_enable_unregister_trigger(char *glob,\n\t\t\t\t\t    struct event_trigger_data *test,\n\t\t\t\t\t    struct trace_event_file *file);\nextern void trigger_data_free(struct event_trigger_data *data);\nextern int event_trigger_init(struct event_trigger_data *data);\nextern int trace_event_trigger_enable_disable(struct trace_event_file *file,\n\t\t\t\t\t      int trigger_enable);\nextern void update_cond_flag(struct trace_event_file *file);\nextern int set_trigger_filter(char *filter_str,\n\t\t\t      struct event_trigger_data *trigger_data,\n\t\t\t      struct trace_event_file *file);\nextern struct event_trigger_data *find_named_trigger(const char *name);\nextern bool is_named_trigger(struct event_trigger_data *test);\nextern int save_named_trigger(const char *name,\n\t\t\t      struct event_trigger_data *data);\nextern void del_named_trigger(struct event_trigger_data *data);\nextern void pause_named_trigger(struct event_trigger_data *data);\nextern void unpause_named_trigger(struct event_trigger_data *data);\nextern void set_named_trigger_data(struct event_trigger_data *data,\n\t\t\t\t   struct event_trigger_data *named_data);\nextern struct event_trigger_data *\nget_named_trigger_data(struct event_trigger_data *data);\nextern int register_event_command(struct event_command *cmd);\nextern int unregister_event_command(struct event_command *cmd);\nextern int register_trigger_hist_enable_disable_cmds(void);\nextern bool event_trigger_check_remove(const char *glob);\nextern bool event_trigger_empty_param(const char *param);\nextern int event_trigger_separate_filter(char *param_and_filter, char **param,\n\t\t\t\t\t char **filter, bool param_required);\nextern struct event_trigger_data *\nevent_trigger_alloc(struct event_command *cmd_ops,\n\t\t    char *cmd,\n\t\t    char *param,\n\t\t    void *private_data);\nextern int event_trigger_parse_num(char *trigger,\n\t\t\t\t   struct event_trigger_data *trigger_data);\nextern int event_trigger_set_filter(struct event_command *cmd_ops,\n\t\t\t\t    struct trace_event_file *file,\n\t\t\t\t    char *param,\n\t\t\t\t    struct event_trigger_data *trigger_data);\nextern void event_trigger_reset_filter(struct event_command *cmd_ops,\n\t\t\t\t       struct event_trigger_data *trigger_data);\nextern int event_trigger_register(struct event_command *cmd_ops,\n\t\t\t\t  struct trace_event_file *file,\n\t\t\t\t  char *glob,\n\t\t\t\t  struct event_trigger_data *trigger_data);\nextern void event_trigger_unregister(struct event_command *cmd_ops,\n\t\t\t\t     struct trace_event_file *file,\n\t\t\t\t     char *glob,\n\t\t\t\t     struct event_trigger_data *trigger_data);\n\nextern void event_file_get(struct trace_event_file *file);\nextern void event_file_put(struct trace_event_file *file);\n\n \nstruct event_trigger_ops {\n\tvoid\t\t\t(*trigger)(struct event_trigger_data *data,\n\t\t\t\t\t   struct trace_buffer *buffer,\n\t\t\t\t\t   void *rec,\n\t\t\t\t\t   struct ring_buffer_event *rbe);\n\tint\t\t\t(*init)(struct event_trigger_data *data);\n\tvoid\t\t\t(*free)(struct event_trigger_data *data);\n\tint\t\t\t(*print)(struct seq_file *m,\n\t\t\t\t\t struct event_trigger_data *data);\n};\n\n \nstruct event_command {\n\tstruct list_head\tlist;\n\tchar\t\t\t*name;\n\tenum event_trigger_type\ttrigger_type;\n\tint\t\t\tflags;\n\tint\t\t\t(*parse)(struct event_command *cmd_ops,\n\t\t\t\t\t struct trace_event_file *file,\n\t\t\t\t\t char *glob, char *cmd,\n\t\t\t\t\t char *param_and_filter);\n\tint\t\t\t(*reg)(char *glob,\n\t\t\t\t       struct event_trigger_data *data,\n\t\t\t\t       struct trace_event_file *file);\n\tvoid\t\t\t(*unreg)(char *glob,\n\t\t\t\t\t struct event_trigger_data *data,\n\t\t\t\t\t struct trace_event_file *file);\n\tvoid\t\t\t(*unreg_all)(struct trace_event_file *file);\n\tint\t\t\t(*set_filter)(char *filter_str,\n\t\t\t\t\t      struct event_trigger_data *data,\n\t\t\t\t\t      struct trace_event_file *file);\n\tstruct event_trigger_ops *(*get_trigger_ops)(char *cmd, char *param);\n};\n\n \nenum event_command_flags {\n\tEVENT_CMD_FL_POST_TRIGGER\t= 1,\n\tEVENT_CMD_FL_NEEDS_REC\t\t= 2,\n};\n\nstatic inline bool event_command_post_trigger(struct event_command *cmd_ops)\n{\n\treturn cmd_ops->flags & EVENT_CMD_FL_POST_TRIGGER;\n}\n\nstatic inline bool event_command_needs_rec(struct event_command *cmd_ops)\n{\n\treturn cmd_ops->flags & EVENT_CMD_FL_NEEDS_REC;\n}\n\nextern int trace_event_enable_disable(struct trace_event_file *file,\n\t\t\t\t      int enable, int soft_disable);\nextern int tracing_alloc_snapshot(void);\nextern void tracing_snapshot_cond(struct trace_array *tr, void *cond_data);\nextern int tracing_snapshot_cond_enable(struct trace_array *tr, void *cond_data, cond_update_fn_t update);\n\nextern int tracing_snapshot_cond_disable(struct trace_array *tr);\nextern void *tracing_cond_snapshot_data(struct trace_array *tr);\n\nextern const char *__start___trace_bprintk_fmt[];\nextern const char *__stop___trace_bprintk_fmt[];\n\nextern const char *__start___tracepoint_str[];\nextern const char *__stop___tracepoint_str[];\n\nvoid trace_printk_control(bool enabled);\nvoid trace_printk_start_comm(void);\nint trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);\nint set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);\n\n \nextern int trace_set_options(struct trace_array *tr, char *option);\nextern int tracing_set_tracer(struct trace_array *tr, const char *buf);\nextern ssize_t tracing_resize_ring_buffer(struct trace_array *tr,\n\t\t\t\t\t  unsigned long size, int cpu_id);\nextern int tracing_set_cpumask(struct trace_array *tr,\n\t\t\t\tcpumask_var_t tracing_cpumask_new);\n\n\n#define MAX_EVENT_NAME_LEN\t64\n\nextern ssize_t trace_parse_run_command(struct file *file,\n\t\tconst char __user *buffer, size_t count, loff_t *ppos,\n\t\tint (*createfn)(const char *));\n\nextern unsigned int err_pos(char *cmd, const char *str);\nextern void tracing_log_err(struct trace_array *tr,\n\t\t\t    const char *loc, const char *cmd,\n\t\t\t    const char **errs, u8 type, u16 pos);\n\n \n#define internal_trace_puts(str) __trace_puts(_THIS_IP_, str, strlen(str))\n\n#undef FTRACE_ENTRY\n#define FTRACE_ENTRY(call, struct_name, id, tstruct, print)\t\\\n\textern struct trace_event_call\t\t\t\t\t\\\n\t__aligned(4) event_##call;\n#undef FTRACE_ENTRY_DUP\n#define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print)\t\\\n\tFTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))\n#undef FTRACE_ENTRY_PACKED\n#define FTRACE_ENTRY_PACKED(call, struct_name, id, tstruct, print) \\\n\tFTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))\n\n#include \"trace_entries.h\"\n\n#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_FUNCTION_TRACER)\nint perf_ftrace_event_register(struct trace_event_call *call,\n\t\t\t       enum trace_reg type, void *data);\n#else\n#define perf_ftrace_event_register NULL\n#endif\n\n#ifdef CONFIG_FTRACE_SYSCALLS\nvoid init_ftrace_syscalls(void);\nconst char *get_syscall_name(int syscall);\n#else\nstatic inline void init_ftrace_syscalls(void) { }\nstatic inline const char *get_syscall_name(int syscall)\n{\n\treturn NULL;\n}\n#endif\n\n#ifdef CONFIG_EVENT_TRACING\nvoid trace_event_init(void);\nvoid trace_event_eval_update(struct trace_eval_map **map, int len);\n \nextern int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set);\nextern int trigger_process_regex(struct trace_event_file *file, char *buff);\n#else\nstatic inline void __init trace_event_init(void) { }\nstatic inline void trace_event_eval_update(struct trace_eval_map **map, int len) { }\n#endif\n\n#ifdef CONFIG_TRACER_SNAPSHOT\nvoid tracing_snapshot_instance(struct trace_array *tr);\nint tracing_alloc_snapshot_instance(struct trace_array *tr);\n#else\nstatic inline void tracing_snapshot_instance(struct trace_array *tr) { }\nstatic inline int tracing_alloc_snapshot_instance(struct trace_array *tr)\n{\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_PREEMPT_TRACER\nvoid tracer_preempt_on(unsigned long a0, unsigned long a1);\nvoid tracer_preempt_off(unsigned long a0, unsigned long a1);\n#else\nstatic inline void tracer_preempt_on(unsigned long a0, unsigned long a1) { }\nstatic inline void tracer_preempt_off(unsigned long a0, unsigned long a1) { }\n#endif\n#ifdef CONFIG_IRQSOFF_TRACER\nvoid tracer_hardirqs_on(unsigned long a0, unsigned long a1);\nvoid tracer_hardirqs_off(unsigned long a0, unsigned long a1);\n#else\nstatic inline void tracer_hardirqs_on(unsigned long a0, unsigned long a1) { }\nstatic inline void tracer_hardirqs_off(unsigned long a0, unsigned long a1) { }\n#endif\n\n \nstatic __always_inline void trace_iterator_reset(struct trace_iterator *iter)\n{\n\tmemset_startat(iter, 0, seq);\n\titer->pos = -1;\n}\n\n \nstatic inline bool __is_good_name(const char *name, bool hash_ok)\n{\n\tif (!isalpha(*name) && *name != '_' && (!hash_ok || *name != '-'))\n\t\treturn false;\n\twhile (*++name != '\\0') {\n\t\tif (!isalpha(*name) && !isdigit(*name) && *name != '_' &&\n\t\t    (!hash_ok || *name != '-'))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic inline bool is_good_name(const char *name)\n{\n\treturn __is_good_name(name, false);\n}\n\n \nstatic inline bool is_good_system_name(const char *name)\n{\n\treturn __is_good_name(name, true);\n}\n\n \nstatic inline void sanitize_event_name(char *name)\n{\n\twhile (*name++ != '\\0')\n\t\tif (*name == ':' || *name == '.')\n\t\t\t*name = '_';\n}\n\n \nstruct trace_min_max_param {\n\tstruct mutex\t*lock;\n\tu64\t\t*val;\n\tu64\t\t*min;\n\tu64\t\t*max;\n};\n\n#define U64_STR_SIZE\t\t24\t \n\nextern const struct file_operations trace_min_max_fops;\n\n#ifdef CONFIG_RV\nextern int rv_init_interface(void);\n#else\nstatic inline int rv_init_interface(void)\n{\n\treturn 0;\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}