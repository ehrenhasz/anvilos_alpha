{
  "module_name": "softirq.c",
  "hash_id": "9e52458e76c4e0b184a99d17405cfdc0a36e6f906806442e76cc92abe75df2fd",
  "original_prompt": "Ingested from linux-6.6.14/kernel/softirq.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/export.h>\n#include <linux/kernel_stat.h>\n#include <linux/interrupt.h>\n#include <linux/init.h>\n#include <linux/local_lock.h>\n#include <linux/mm.h>\n#include <linux/notifier.h>\n#include <linux/percpu.h>\n#include <linux/cpu.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/rcupdate.h>\n#include <linux/ftrace.h>\n#include <linux/smp.h>\n#include <linux/smpboot.h>\n#include <linux/tick.h>\n#include <linux/irq.h>\n#include <linux/wait_bit.h>\n\n#include <asm/softirq_stack.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/irq.h>\n\n \n\n#ifndef __ARCH_IRQ_STAT\nDEFINE_PER_CPU_ALIGNED(irq_cpustat_t, irq_stat);\nEXPORT_PER_CPU_SYMBOL(irq_stat);\n#endif\n\nstatic struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;\n\nDEFINE_PER_CPU(struct task_struct *, ksoftirqd);\n\nconst char * const softirq_to_name[NR_SOFTIRQS] = {\n\t\"HI\", \"TIMER\", \"NET_TX\", \"NET_RX\", \"BLOCK\", \"IRQ_POLL\",\n\t\"TASKLET\", \"SCHED\", \"HRTIMER\", \"RCU\"\n};\n\n \nstatic void wakeup_softirqd(void)\n{\n\t \n\tstruct task_struct *tsk = __this_cpu_read(ksoftirqd);\n\n\tif (tsk)\n\t\twake_up_process(tsk);\n}\n\n#ifdef CONFIG_TRACE_IRQFLAGS\nDEFINE_PER_CPU(int, hardirqs_enabled);\nDEFINE_PER_CPU(int, hardirq_context);\nEXPORT_PER_CPU_SYMBOL_GPL(hardirqs_enabled);\nEXPORT_PER_CPU_SYMBOL_GPL(hardirq_context);\n#endif\n\n \n#ifdef CONFIG_PREEMPT_RT\n\n \nstruct softirq_ctrl {\n\tlocal_lock_t\tlock;\n\tint\t\tcnt;\n};\n\nstatic DEFINE_PER_CPU(struct softirq_ctrl, softirq_ctrl) = {\n\t.lock\t= INIT_LOCAL_LOCK(softirq_ctrl.lock),\n};\n\n \nbool local_bh_blocked(void)\n{\n\treturn __this_cpu_read(softirq_ctrl.cnt) != 0;\n}\n\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\n{\n\tunsigned long flags;\n\tint newcnt;\n\n\tWARN_ON_ONCE(in_hardirq());\n\n\t \n\tif (!current->softirq_disable_cnt) {\n\t\tif (preemptible()) {\n\t\t\tlocal_lock(&softirq_ctrl.lock);\n\t\t\t \n\t\t\trcu_read_lock();\n\t\t} else {\n\t\t\tDEBUG_LOCKS_WARN_ON(this_cpu_read(softirq_ctrl.cnt));\n\t\t}\n\t}\n\n\t \n\tnewcnt = __this_cpu_add_return(softirq_ctrl.cnt, cnt);\n\t \n\tcurrent->softirq_disable_cnt = newcnt;\n\n\tif (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && newcnt == cnt) {\n\t\traw_local_irq_save(flags);\n\t\tlockdep_softirqs_off(ip);\n\t\traw_local_irq_restore(flags);\n\t}\n}\nEXPORT_SYMBOL(__local_bh_disable_ip);\n\nstatic void __local_bh_enable(unsigned int cnt, bool unlock)\n{\n\tunsigned long flags;\n\tint newcnt;\n\n\tDEBUG_LOCKS_WARN_ON(current->softirq_disable_cnt !=\n\t\t\t    this_cpu_read(softirq_ctrl.cnt));\n\n\tif (IS_ENABLED(CONFIG_TRACE_IRQFLAGS) && softirq_count() == cnt) {\n\t\traw_local_irq_save(flags);\n\t\tlockdep_softirqs_on(_RET_IP_);\n\t\traw_local_irq_restore(flags);\n\t}\n\n\tnewcnt = __this_cpu_sub_return(softirq_ctrl.cnt, cnt);\n\tcurrent->softirq_disable_cnt = newcnt;\n\n\tif (!newcnt && unlock) {\n\t\trcu_read_unlock();\n\t\tlocal_unlock(&softirq_ctrl.lock);\n\t}\n}\n\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\n{\n\tbool preempt_on = preemptible();\n\tunsigned long flags;\n\tu32 pending;\n\tint curcnt;\n\n\tWARN_ON_ONCE(in_hardirq());\n\tlockdep_assert_irqs_enabled();\n\n\tlocal_irq_save(flags);\n\tcurcnt = __this_cpu_read(softirq_ctrl.cnt);\n\n\t \n\tif (curcnt != cnt)\n\t\tgoto out;\n\n\tpending = local_softirq_pending();\n\tif (!pending)\n\t\tgoto out;\n\n\t \n\tif (!preempt_on) {\n\t\twakeup_softirqd();\n\t\tgoto out;\n\t}\n\n\t \n\tcnt = SOFTIRQ_OFFSET;\n\t__local_bh_enable(cnt, false);\n\t__do_softirq();\n\nout:\n\t__local_bh_enable(cnt, preempt_on);\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL(__local_bh_enable_ip);\n\n \nstatic inline void ksoftirqd_run_begin(void)\n{\n\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\n\tlocal_irq_disable();\n}\n\n \nstatic inline void ksoftirqd_run_end(void)\n{\n\t__local_bh_enable(SOFTIRQ_OFFSET, true);\n\tWARN_ON_ONCE(in_interrupt());\n\tlocal_irq_enable();\n}\n\nstatic inline void softirq_handle_begin(void) { }\nstatic inline void softirq_handle_end(void) { }\n\nstatic inline bool should_wake_ksoftirqd(void)\n{\n\treturn !this_cpu_read(softirq_ctrl.cnt);\n}\n\nstatic inline void invoke_softirq(void)\n{\n\tif (should_wake_ksoftirqd())\n\t\twakeup_softirqd();\n}\n\n \nvoid do_softirq_post_smp_call_flush(unsigned int was_pending)\n{\n\tif (WARN_ON_ONCE(was_pending != local_softirq_pending()))\n\t\tinvoke_softirq();\n}\n\n#else  \n\n \n#ifdef CONFIG_TRACE_IRQFLAGS\nvoid __local_bh_disable_ip(unsigned long ip, unsigned int cnt)\n{\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(in_hardirq());\n\n\traw_local_irq_save(flags);\n\t \n\t__preempt_count_add(cnt);\n\t \n\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\n\t\tlockdep_softirqs_off(ip);\n\traw_local_irq_restore(flags);\n\n\tif (preempt_count() == cnt) {\n#ifdef CONFIG_DEBUG_PREEMPT\n\t\tcurrent->preempt_disable_ip = get_lock_parent_ip();\n#endif\n\t\ttrace_preempt_off(CALLER_ADDR0, get_lock_parent_ip());\n\t}\n}\nEXPORT_SYMBOL(__local_bh_disable_ip);\n#endif  \n\nstatic void __local_bh_enable(unsigned int cnt)\n{\n\tlockdep_assert_irqs_disabled();\n\n\tif (preempt_count() == cnt)\n\t\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());\n\n\tif (softirq_count() == (cnt & SOFTIRQ_MASK))\n\t\tlockdep_softirqs_on(_RET_IP_);\n\n\t__preempt_count_sub(cnt);\n}\n\n \nvoid _local_bh_enable(void)\n{\n\tWARN_ON_ONCE(in_hardirq());\n\t__local_bh_enable(SOFTIRQ_DISABLE_OFFSET);\n}\nEXPORT_SYMBOL(_local_bh_enable);\n\nvoid __local_bh_enable_ip(unsigned long ip, unsigned int cnt)\n{\n\tWARN_ON_ONCE(in_hardirq());\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tlocal_irq_disable();\n#endif\n\t \n\tif (softirq_count() == SOFTIRQ_DISABLE_OFFSET)\n\t\tlockdep_softirqs_on(ip);\n\t \n\t__preempt_count_sub(cnt - 1);\n\n\tif (unlikely(!in_interrupt() && local_softirq_pending())) {\n\t\t \n\t\tdo_softirq();\n\t}\n\n\tpreempt_count_dec();\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tlocal_irq_enable();\n#endif\n\tpreempt_check_resched();\n}\nEXPORT_SYMBOL(__local_bh_enable_ip);\n\nstatic inline void softirq_handle_begin(void)\n{\n\t__local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET);\n}\n\nstatic inline void softirq_handle_end(void)\n{\n\t__local_bh_enable(SOFTIRQ_OFFSET);\n\tWARN_ON_ONCE(in_interrupt());\n}\n\nstatic inline void ksoftirqd_run_begin(void)\n{\n\tlocal_irq_disable();\n}\n\nstatic inline void ksoftirqd_run_end(void)\n{\n\tlocal_irq_enable();\n}\n\nstatic inline bool should_wake_ksoftirqd(void)\n{\n\treturn true;\n}\n\nstatic inline void invoke_softirq(void)\n{\n\tif (!force_irqthreads() || !__this_cpu_read(ksoftirqd)) {\n#ifdef CONFIG_HAVE_IRQ_EXIT_ON_IRQ_STACK\n\t\t \n\t\t__do_softirq();\n#else\n\t\t \n\t\tdo_softirq_own_stack();\n#endif\n\t} else {\n\t\twakeup_softirqd();\n\t}\n}\n\nasmlinkage __visible void do_softirq(void)\n{\n\t__u32 pending;\n\tunsigned long flags;\n\n\tif (in_interrupt())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tpending = local_softirq_pending();\n\n\tif (pending)\n\t\tdo_softirq_own_stack();\n\n\tlocal_irq_restore(flags);\n}\n\n#endif  \n\n \n#define MAX_SOFTIRQ_TIME  msecs_to_jiffies(2)\n#define MAX_SOFTIRQ_RESTART 10\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n \n\nstatic inline bool lockdep_softirq_start(void)\n{\n\tbool in_hardirq = false;\n\n\tif (lockdep_hardirq_context()) {\n\t\tin_hardirq = true;\n\t\tlockdep_hardirq_exit();\n\t}\n\n\tlockdep_softirq_enter();\n\n\treturn in_hardirq;\n}\n\nstatic inline void lockdep_softirq_end(bool in_hardirq)\n{\n\tlockdep_softirq_exit();\n\n\tif (in_hardirq)\n\t\tlockdep_hardirq_enter();\n}\n#else\nstatic inline bool lockdep_softirq_start(void) { return false; }\nstatic inline void lockdep_softirq_end(bool in_hardirq) { }\n#endif\n\nasmlinkage __visible void __softirq_entry __do_softirq(void)\n{\n\tunsigned long end = jiffies + MAX_SOFTIRQ_TIME;\n\tunsigned long old_flags = current->flags;\n\tint max_restart = MAX_SOFTIRQ_RESTART;\n\tstruct softirq_action *h;\n\tbool in_hardirq;\n\t__u32 pending;\n\tint softirq_bit;\n\n\t \n\tcurrent->flags &= ~PF_MEMALLOC;\n\n\tpending = local_softirq_pending();\n\n\tsoftirq_handle_begin();\n\tin_hardirq = lockdep_softirq_start();\n\taccount_softirq_enter(current);\n\nrestart:\n\t \n\tset_softirq_pending(0);\n\n\tlocal_irq_enable();\n\n\th = softirq_vec;\n\n\twhile ((softirq_bit = ffs(pending))) {\n\t\tunsigned int vec_nr;\n\t\tint prev_count;\n\n\t\th += softirq_bit - 1;\n\n\t\tvec_nr = h - softirq_vec;\n\t\tprev_count = preempt_count();\n\n\t\tkstat_incr_softirqs_this_cpu(vec_nr);\n\n\t\ttrace_softirq_entry(vec_nr);\n\t\th->action(h);\n\t\ttrace_softirq_exit(vec_nr);\n\t\tif (unlikely(prev_count != preempt_count())) {\n\t\t\tpr_err(\"huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\n\",\n\t\t\t       vec_nr, softirq_to_name[vec_nr], h->action,\n\t\t\t       prev_count, preempt_count());\n\t\t\tpreempt_count_set(prev_count);\n\t\t}\n\t\th++;\n\t\tpending >>= softirq_bit;\n\t}\n\n\tif (!IS_ENABLED(CONFIG_PREEMPT_RT) &&\n\t    __this_cpu_read(ksoftirqd) == current)\n\t\trcu_softirq_qs();\n\n\tlocal_irq_disable();\n\n\tpending = local_softirq_pending();\n\tif (pending) {\n\t\tif (time_before(jiffies, end) && !need_resched() &&\n\t\t    --max_restart)\n\t\t\tgoto restart;\n\n\t\twakeup_softirqd();\n\t}\n\n\taccount_softirq_exit(current);\n\tlockdep_softirq_end(in_hardirq);\n\tsoftirq_handle_end();\n\tcurrent_restore_flags(old_flags, PF_MEMALLOC);\n}\n\n \nvoid irq_enter_rcu(void)\n{\n\t__irq_enter_raw();\n\n\tif (tick_nohz_full_cpu(smp_processor_id()) ||\n\t    (is_idle_task(current) && (irq_count() == HARDIRQ_OFFSET)))\n\t\ttick_irq_enter();\n\n\taccount_hardirq_enter(current);\n}\n\n \nvoid irq_enter(void)\n{\n\tct_irq_enter();\n\tirq_enter_rcu();\n}\n\nstatic inline void tick_irq_exit(void)\n{\n#ifdef CONFIG_NO_HZ_COMMON\n\tint cpu = smp_processor_id();\n\n\t \n\tif ((sched_core_idle_cpu(cpu) && !need_resched()) || tick_nohz_full_cpu(cpu)) {\n\t\tif (!in_hardirq())\n\t\t\ttick_nohz_irq_exit();\n\t}\n#endif\n}\n\nstatic inline void __irq_exit_rcu(void)\n{\n#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED\n\tlocal_irq_disable();\n#else\n\tlockdep_assert_irqs_disabled();\n#endif\n\taccount_hardirq_exit(current);\n\tpreempt_count_sub(HARDIRQ_OFFSET);\n\tif (!in_interrupt() && local_softirq_pending())\n\t\tinvoke_softirq();\n\n\ttick_irq_exit();\n}\n\n \nvoid irq_exit_rcu(void)\n{\n\t__irq_exit_rcu();\n\t  \n\tlockdep_hardirq_exit();\n}\n\n \nvoid irq_exit(void)\n{\n\t__irq_exit_rcu();\n\tct_irq_exit();\n\t  \n\tlockdep_hardirq_exit();\n}\n\n \ninline void raise_softirq_irqoff(unsigned int nr)\n{\n\t__raise_softirq_irqoff(nr);\n\n\t \n\tif (!in_interrupt() && should_wake_ksoftirqd())\n\t\twakeup_softirqd();\n}\n\nvoid raise_softirq(unsigned int nr)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\traise_softirq_irqoff(nr);\n\tlocal_irq_restore(flags);\n}\n\nvoid __raise_softirq_irqoff(unsigned int nr)\n{\n\tlockdep_assert_irqs_disabled();\n\ttrace_softirq_raise(nr);\n\tor_softirq_pending(1UL << nr);\n}\n\nvoid open_softirq(int nr, void (*action)(struct softirq_action *))\n{\n\tsoftirq_vec[nr].action = action;\n}\n\n \nstruct tasklet_head {\n\tstruct tasklet_struct *head;\n\tstruct tasklet_struct **tail;\n};\n\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_vec);\nstatic DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec);\n\nstatic void __tasklet_schedule_common(struct tasklet_struct *t,\n\t\t\t\t      struct tasklet_head __percpu *headp,\n\t\t\t\t      unsigned int softirq_nr)\n{\n\tstruct tasklet_head *head;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\thead = this_cpu_ptr(headp);\n\tt->next = NULL;\n\t*head->tail = t;\n\thead->tail = &(t->next);\n\traise_softirq_irqoff(softirq_nr);\n\tlocal_irq_restore(flags);\n}\n\nvoid __tasklet_schedule(struct tasklet_struct *t)\n{\n\t__tasklet_schedule_common(t, &tasklet_vec,\n\t\t\t\t  TASKLET_SOFTIRQ);\n}\nEXPORT_SYMBOL(__tasklet_schedule);\n\nvoid __tasklet_hi_schedule(struct tasklet_struct *t)\n{\n\t__tasklet_schedule_common(t, &tasklet_hi_vec,\n\t\t\t\t  HI_SOFTIRQ);\n}\nEXPORT_SYMBOL(__tasklet_hi_schedule);\n\nstatic bool tasklet_clear_sched(struct tasklet_struct *t)\n{\n\tif (test_and_clear_bit(TASKLET_STATE_SCHED, &t->state)) {\n\t\twake_up_var(&t->state);\n\t\treturn true;\n\t}\n\n\tWARN_ONCE(1, \"tasklet SCHED state not set: %s %pS\\n\",\n\t\t  t->use_callback ? \"callback\" : \"func\",\n\t\t  t->use_callback ? (void *)t->callback : (void *)t->func);\n\n\treturn false;\n}\n\nstatic void tasklet_action_common(struct softirq_action *a,\n\t\t\t\t  struct tasklet_head *tl_head,\n\t\t\t\t  unsigned int softirq_nr)\n{\n\tstruct tasklet_struct *list;\n\n\tlocal_irq_disable();\n\tlist = tl_head->head;\n\ttl_head->head = NULL;\n\ttl_head->tail = &tl_head->head;\n\tlocal_irq_enable();\n\n\twhile (list) {\n\t\tstruct tasklet_struct *t = list;\n\n\t\tlist = list->next;\n\n\t\tif (tasklet_trylock(t)) {\n\t\t\tif (!atomic_read(&t->count)) {\n\t\t\t\tif (tasklet_clear_sched(t)) {\n\t\t\t\t\tif (t->use_callback) {\n\t\t\t\t\t\ttrace_tasklet_entry(t, t->callback);\n\t\t\t\t\t\tt->callback(t);\n\t\t\t\t\t\ttrace_tasklet_exit(t, t->callback);\n\t\t\t\t\t} else {\n\t\t\t\t\t\ttrace_tasklet_entry(t, t->func);\n\t\t\t\t\t\tt->func(t->data);\n\t\t\t\t\t\ttrace_tasklet_exit(t, t->func);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttasklet_unlock(t);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\ttasklet_unlock(t);\n\t\t}\n\n\t\tlocal_irq_disable();\n\t\tt->next = NULL;\n\t\t*tl_head->tail = t;\n\t\ttl_head->tail = &t->next;\n\t\t__raise_softirq_irqoff(softirq_nr);\n\t\tlocal_irq_enable();\n\t}\n}\n\nstatic __latent_entropy void tasklet_action(struct softirq_action *a)\n{\n\ttasklet_action_common(a, this_cpu_ptr(&tasklet_vec), TASKLET_SOFTIRQ);\n}\n\nstatic __latent_entropy void tasklet_hi_action(struct softirq_action *a)\n{\n\ttasklet_action_common(a, this_cpu_ptr(&tasklet_hi_vec), HI_SOFTIRQ);\n}\n\nvoid tasklet_setup(struct tasklet_struct *t,\n\t\t   void (*callback)(struct tasklet_struct *))\n{\n\tt->next = NULL;\n\tt->state = 0;\n\tatomic_set(&t->count, 0);\n\tt->callback = callback;\n\tt->use_callback = true;\n\tt->data = 0;\n}\nEXPORT_SYMBOL(tasklet_setup);\n\nvoid tasklet_init(struct tasklet_struct *t,\n\t\t  void (*func)(unsigned long), unsigned long data)\n{\n\tt->next = NULL;\n\tt->state = 0;\n\tatomic_set(&t->count, 0);\n\tt->func = func;\n\tt->use_callback = false;\n\tt->data = data;\n}\nEXPORT_SYMBOL(tasklet_init);\n\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\n \nvoid tasklet_unlock_spin_wait(struct tasklet_struct *t)\n{\n\twhile (test_bit(TASKLET_STATE_RUN, &(t)->state)) {\n\t\tif (IS_ENABLED(CONFIG_PREEMPT_RT)) {\n\t\t\t \n\t\t\tlocal_bh_disable();\n\t\t\tlocal_bh_enable();\n\t\t} else {\n\t\t\tcpu_relax();\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(tasklet_unlock_spin_wait);\n#endif\n\nvoid tasklet_kill(struct tasklet_struct *t)\n{\n\tif (in_interrupt())\n\t\tpr_notice(\"Attempt to kill tasklet from interrupt\\n\");\n\n\twhile (test_and_set_bit(TASKLET_STATE_SCHED, &t->state))\n\t\twait_var_event(&t->state, !test_bit(TASKLET_STATE_SCHED, &t->state));\n\n\ttasklet_unlock_wait(t);\n\ttasklet_clear_sched(t);\n}\nEXPORT_SYMBOL(tasklet_kill);\n\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\nvoid tasklet_unlock(struct tasklet_struct *t)\n{\n\tsmp_mb__before_atomic();\n\tclear_bit(TASKLET_STATE_RUN, &t->state);\n\tsmp_mb__after_atomic();\n\twake_up_var(&t->state);\n}\nEXPORT_SYMBOL_GPL(tasklet_unlock);\n\nvoid tasklet_unlock_wait(struct tasklet_struct *t)\n{\n\twait_var_event(&t->state, !test_bit(TASKLET_STATE_RUN, &t->state));\n}\nEXPORT_SYMBOL_GPL(tasklet_unlock_wait);\n#endif\n\nvoid __init softirq_init(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tper_cpu(tasklet_vec, cpu).tail =\n\t\t\t&per_cpu(tasklet_vec, cpu).head;\n\t\tper_cpu(tasklet_hi_vec, cpu).tail =\n\t\t\t&per_cpu(tasklet_hi_vec, cpu).head;\n\t}\n\n\topen_softirq(TASKLET_SOFTIRQ, tasklet_action);\n\topen_softirq(HI_SOFTIRQ, tasklet_hi_action);\n}\n\nstatic int ksoftirqd_should_run(unsigned int cpu)\n{\n\treturn local_softirq_pending();\n}\n\nstatic void run_ksoftirqd(unsigned int cpu)\n{\n\tksoftirqd_run_begin();\n\tif (local_softirq_pending()) {\n\t\t \n\t\t__do_softirq();\n\t\tksoftirqd_run_end();\n\t\tcond_resched();\n\t\treturn;\n\t}\n\tksoftirqd_run_end();\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic int takeover_tasklets(unsigned int cpu)\n{\n\t \n\tlocal_irq_disable();\n\n\t \n\tif (&per_cpu(tasklet_vec, cpu).head != per_cpu(tasklet_vec, cpu).tail) {\n\t\t*__this_cpu_read(tasklet_vec.tail) = per_cpu(tasklet_vec, cpu).head;\n\t\t__this_cpu_write(tasklet_vec.tail, per_cpu(tasklet_vec, cpu).tail);\n\t\tper_cpu(tasklet_vec, cpu).head = NULL;\n\t\tper_cpu(tasklet_vec, cpu).tail = &per_cpu(tasklet_vec, cpu).head;\n\t}\n\traise_softirq_irqoff(TASKLET_SOFTIRQ);\n\n\tif (&per_cpu(tasklet_hi_vec, cpu).head != per_cpu(tasklet_hi_vec, cpu).tail) {\n\t\t*__this_cpu_read(tasklet_hi_vec.tail) = per_cpu(tasklet_hi_vec, cpu).head;\n\t\t__this_cpu_write(tasklet_hi_vec.tail, per_cpu(tasklet_hi_vec, cpu).tail);\n\t\tper_cpu(tasklet_hi_vec, cpu).head = NULL;\n\t\tper_cpu(tasklet_hi_vec, cpu).tail = &per_cpu(tasklet_hi_vec, cpu).head;\n\t}\n\traise_softirq_irqoff(HI_SOFTIRQ);\n\n\tlocal_irq_enable();\n\treturn 0;\n}\n#else\n#define takeover_tasklets\tNULL\n#endif  \n\nstatic struct smp_hotplug_thread softirq_threads = {\n\t.store\t\t\t= &ksoftirqd,\n\t.thread_should_run\t= ksoftirqd_should_run,\n\t.thread_fn\t\t= run_ksoftirqd,\n\t.thread_comm\t\t= \"ksoftirqd/%u\",\n};\n\nstatic __init int spawn_ksoftirqd(void)\n{\n\tcpuhp_setup_state_nocalls(CPUHP_SOFTIRQ_DEAD, \"softirq:dead\", NULL,\n\t\t\t\t  takeover_tasklets);\n\tBUG_ON(smpboot_register_percpu_thread(&softirq_threads));\n\n\treturn 0;\n}\nearly_initcall(spawn_ksoftirqd);\n\n \n\nint __init __weak early_irq_init(void)\n{\n\treturn 0;\n}\n\nint __init __weak arch_probe_nr_irqs(void)\n{\n\treturn NR_IRQS_LEGACY;\n}\n\nint __init __weak arch_early_irq_init(void)\n{\n\treturn 0;\n}\n\nunsigned int __weak arch_dynirq_lower_bound(unsigned int from)\n{\n\treturn from;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}