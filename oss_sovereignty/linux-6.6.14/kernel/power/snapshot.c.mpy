{
  "module_name": "snapshot.c",
  "hash_id": "156dfa639b4b6c9b036e92d8aa2b9c75097be7995f89ac7ce1f0e52d0206083c",
  "original_prompt": "Ingested from linux-6.6.14/kernel/power/snapshot.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"PM: hibernation: \" fmt\n\n#include <linux/version.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/suspend.h>\n#include <linux/delay.h>\n#include <linux/bitops.h>\n#include <linux/spinlock.h>\n#include <linux/kernel.h>\n#include <linux/pm.h>\n#include <linux/device.h>\n#include <linux/init.h>\n#include <linux/memblock.h>\n#include <linux/nmi.h>\n#include <linux/syscalls.h>\n#include <linux/console.h>\n#include <linux/highmem.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/compiler.h>\n#include <linux/ktime.h>\n#include <linux/set_memory.h>\n\n#include <linux/uaccess.h>\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/io.h>\n\n#include \"power.h\"\n\n#if defined(CONFIG_STRICT_KERNEL_RWX) && defined(CONFIG_ARCH_HAS_SET_MEMORY)\nstatic bool hibernate_restore_protection;\nstatic bool hibernate_restore_protection_active;\n\nvoid enable_restore_image_protection(void)\n{\n\thibernate_restore_protection = true;\n}\n\nstatic inline void hibernate_restore_protection_begin(void)\n{\n\thibernate_restore_protection_active = hibernate_restore_protection;\n}\n\nstatic inline void hibernate_restore_protection_end(void)\n{\n\thibernate_restore_protection_active = false;\n}\n\nstatic inline void hibernate_restore_protect_page(void *page_address)\n{\n\tif (hibernate_restore_protection_active)\n\t\tset_memory_ro((unsigned long)page_address, 1);\n}\n\nstatic inline void hibernate_restore_unprotect_page(void *page_address)\n{\n\tif (hibernate_restore_protection_active)\n\t\tset_memory_rw((unsigned long)page_address, 1);\n}\n#else\nstatic inline void hibernate_restore_protection_begin(void) {}\nstatic inline void hibernate_restore_protection_end(void) {}\nstatic inline void hibernate_restore_protect_page(void *page_address) {}\nstatic inline void hibernate_restore_unprotect_page(void *page_address) {}\n#endif  \n\n\n \nstatic inline void hibernate_map_page(struct page *page)\n{\n\tif (IS_ENABLED(CONFIG_ARCH_HAS_SET_DIRECT_MAP)) {\n\t\tint ret = set_direct_map_default_noflush(page);\n\n\t\tif (ret)\n\t\t\tpr_warn_once(\"Failed to remap page\\n\");\n\t} else {\n\t\tdebug_pagealloc_map_pages(page, 1);\n\t}\n}\n\nstatic inline void hibernate_unmap_page(struct page *page)\n{\n\tif (IS_ENABLED(CONFIG_ARCH_HAS_SET_DIRECT_MAP)) {\n\t\tunsigned long addr = (unsigned long)page_address(page);\n\t\tint ret  = set_direct_map_invalid_noflush(page);\n\n\t\tif (ret)\n\t\t\tpr_warn_once(\"Failed to remap page\\n\");\n\n\t\tflush_tlb_kernel_range(addr, addr + PAGE_SIZE);\n\t} else {\n\t\tdebug_pagealloc_unmap_pages(page, 1);\n\t}\n}\n\nstatic int swsusp_page_is_free(struct page *);\nstatic void swsusp_set_page_forbidden(struct page *);\nstatic void swsusp_unset_page_forbidden(struct page *);\n\n \nunsigned long reserved_size;\n\nvoid __init hibernate_reserved_size_init(void)\n{\n\treserved_size = SPARE_PAGES * PAGE_SIZE;\n}\n\n \nunsigned long image_size;\n\nvoid __init hibernate_image_size_init(void)\n{\n\timage_size = ((totalram_pages() * 2) / 5) * PAGE_SIZE;\n}\n\n \nstruct pbe *restore_pblist;\n\n \n\n#define LINKED_PAGE_DATA_SIZE\t(PAGE_SIZE - sizeof(void *))\n\nstruct linked_page {\n\tstruct linked_page *next;\n\tchar data[LINKED_PAGE_DATA_SIZE];\n} __packed;\n\n \nstatic struct linked_page *safe_pages_list;\n\n \nstatic void *buffer;\n\n#define PG_ANY\t\t0\n#define PG_SAFE\t\t1\n#define PG_UNSAFE_CLEAR\t1\n#define PG_UNSAFE_KEEP\t0\n\nstatic unsigned int allocated_unsafe_pages;\n\n \nstatic void *get_image_page(gfp_t gfp_mask, int safe_needed)\n{\n\tvoid *res;\n\n\tres = (void *)get_zeroed_page(gfp_mask);\n\tif (safe_needed)\n\t\twhile (res && swsusp_page_is_free(virt_to_page(res))) {\n\t\t\t \n\t\t\tswsusp_set_page_forbidden(virt_to_page(res));\n\t\t\tallocated_unsafe_pages++;\n\t\t\tres = (void *)get_zeroed_page(gfp_mask);\n\t\t}\n\tif (res) {\n\t\tswsusp_set_page_forbidden(virt_to_page(res));\n\t\tswsusp_set_page_free(virt_to_page(res));\n\t}\n\treturn res;\n}\n\nstatic void *__get_safe_page(gfp_t gfp_mask)\n{\n\tif (safe_pages_list) {\n\t\tvoid *ret = safe_pages_list;\n\n\t\tsafe_pages_list = safe_pages_list->next;\n\t\tmemset(ret, 0, PAGE_SIZE);\n\t\treturn ret;\n\t}\n\treturn get_image_page(gfp_mask, PG_SAFE);\n}\n\nunsigned long get_safe_page(gfp_t gfp_mask)\n{\n\treturn (unsigned long)__get_safe_page(gfp_mask);\n}\n\nstatic struct page *alloc_image_page(gfp_t gfp_mask)\n{\n\tstruct page *page;\n\n\tpage = alloc_page(gfp_mask);\n\tif (page) {\n\t\tswsusp_set_page_forbidden(page);\n\t\tswsusp_set_page_free(page);\n\t}\n\treturn page;\n}\n\nstatic void recycle_safe_page(void *page_address)\n{\n\tstruct linked_page *lp = page_address;\n\n\tlp->next = safe_pages_list;\n\tsafe_pages_list = lp;\n}\n\n \nstatic inline void free_image_page(void *addr, int clear_nosave_free)\n{\n\tstruct page *page;\n\n\tBUG_ON(!virt_addr_valid(addr));\n\n\tpage = virt_to_page(addr);\n\n\tswsusp_unset_page_forbidden(page);\n\tif (clear_nosave_free)\n\t\tswsusp_unset_page_free(page);\n\n\t__free_page(page);\n}\n\nstatic inline void free_list_of_pages(struct linked_page *list,\n\t\t\t\t      int clear_page_nosave)\n{\n\twhile (list) {\n\t\tstruct linked_page *lp = list->next;\n\n\t\tfree_image_page(list, clear_page_nosave);\n\t\tlist = lp;\n\t}\n}\n\n \nstruct chain_allocator {\n\tstruct linked_page *chain;\t \n\tunsigned int used_space;\t \n\tgfp_t gfp_mask;\t\t \n\tint safe_needed;\t \n};\n\nstatic void chain_init(struct chain_allocator *ca, gfp_t gfp_mask,\n\t\t       int safe_needed)\n{\n\tca->chain = NULL;\n\tca->used_space = LINKED_PAGE_DATA_SIZE;\n\tca->gfp_mask = gfp_mask;\n\tca->safe_needed = safe_needed;\n}\n\nstatic void *chain_alloc(struct chain_allocator *ca, unsigned int size)\n{\n\tvoid *ret;\n\n\tif (LINKED_PAGE_DATA_SIZE - ca->used_space < size) {\n\t\tstruct linked_page *lp;\n\n\t\tlp = ca->safe_needed ? __get_safe_page(ca->gfp_mask) :\n\t\t\t\t\tget_image_page(ca->gfp_mask, PG_ANY);\n\t\tif (!lp)\n\t\t\treturn NULL;\n\n\t\tlp->next = ca->chain;\n\t\tca->chain = lp;\n\t\tca->used_space = 0;\n\t}\n\tret = ca->chain->data + ca->used_space;\n\tca->used_space += size;\n\treturn ret;\n}\n\n \n\n#define BM_END_OF_MAP\t(~0UL)\n\n#define BM_BITS_PER_BLOCK\t(PAGE_SIZE * BITS_PER_BYTE)\n#define BM_BLOCK_SHIFT\t\t(PAGE_SHIFT + 3)\n#define BM_BLOCK_MASK\t\t((1UL << BM_BLOCK_SHIFT) - 1)\n\n \nstruct rtree_node {\n\tstruct list_head list;\n\tunsigned long *data;\n};\n\n \nstruct mem_zone_bm_rtree {\n\tstruct list_head list;\t\t \n\tstruct list_head nodes;\t\t \n\tstruct list_head leaves;\t \n\tunsigned long start_pfn;\t \n\tunsigned long end_pfn;\t\t \n\tstruct rtree_node *rtree;\t \n\tint levels;\t\t\t \n\tunsigned int blocks;\t\t \n};\n\n \n\nstruct bm_position {\n\tstruct mem_zone_bm_rtree *zone;\n\tstruct rtree_node *node;\n\tunsigned long node_pfn;\n\tunsigned long cur_pfn;\n\tint node_bit;\n};\n\nstruct memory_bitmap {\n\tstruct list_head zones;\n\tstruct linked_page *p_list;\t \n\tstruct bm_position cur;\t \n};\n\n \n\n#define BM_ENTRIES_PER_LEVEL\t(PAGE_SIZE / sizeof(unsigned long))\n#if BITS_PER_LONG == 32\n#define BM_RTREE_LEVEL_SHIFT\t(PAGE_SHIFT - 2)\n#else\n#define BM_RTREE_LEVEL_SHIFT\t(PAGE_SHIFT - 3)\n#endif\n#define BM_RTREE_LEVEL_MASK\t((1UL << BM_RTREE_LEVEL_SHIFT) - 1)\n\n \nstatic struct rtree_node *alloc_rtree_node(gfp_t gfp_mask, int safe_needed,\n\t\t\t\t\t   struct chain_allocator *ca,\n\t\t\t\t\t   struct list_head *list)\n{\n\tstruct rtree_node *node;\n\n\tnode = chain_alloc(ca, sizeof(struct rtree_node));\n\tif (!node)\n\t\treturn NULL;\n\n\tnode->data = get_image_page(gfp_mask, safe_needed);\n\tif (!node->data)\n\t\treturn NULL;\n\n\tlist_add_tail(&node->list, list);\n\n\treturn node;\n}\n\n \nstatic int add_rtree_block(struct mem_zone_bm_rtree *zone, gfp_t gfp_mask,\n\t\t\t   int safe_needed, struct chain_allocator *ca)\n{\n\tstruct rtree_node *node, *block, **dst;\n\tunsigned int levels_needed, block_nr;\n\tint i;\n\n\tblock_nr = zone->blocks;\n\tlevels_needed = 0;\n\n\t \n\twhile (block_nr) {\n\t\tlevels_needed += 1;\n\t\tblock_nr >>= BM_RTREE_LEVEL_SHIFT;\n\t}\n\n\t \n\tfor (i = zone->levels; i < levels_needed; i++) {\n\t\tnode = alloc_rtree_node(gfp_mask, safe_needed, ca,\n\t\t\t\t\t&zone->nodes);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\n\t\tnode->data[0] = (unsigned long)zone->rtree;\n\t\tzone->rtree = node;\n\t\tzone->levels += 1;\n\t}\n\n\t \n\tblock = alloc_rtree_node(gfp_mask, safe_needed, ca, &zone->leaves);\n\tif (!block)\n\t\treturn -ENOMEM;\n\n\t \n\tnode = zone->rtree;\n\tdst = &zone->rtree;\n\tblock_nr = zone->blocks;\n\tfor (i = zone->levels; i > 0; i--) {\n\t\tint index;\n\n\t\tif (!node) {\n\t\t\tnode = alloc_rtree_node(gfp_mask, safe_needed, ca,\n\t\t\t\t\t\t&zone->nodes);\n\t\t\tif (!node)\n\t\t\t\treturn -ENOMEM;\n\t\t\t*dst = node;\n\t\t}\n\n\t\tindex = block_nr >> ((i - 1) * BM_RTREE_LEVEL_SHIFT);\n\t\tindex &= BM_RTREE_LEVEL_MASK;\n\t\tdst = (struct rtree_node **)&((*dst)->data[index]);\n\t\tnode = *dst;\n\t}\n\n\tzone->blocks += 1;\n\t*dst = block;\n\n\treturn 0;\n}\n\nstatic void free_zone_bm_rtree(struct mem_zone_bm_rtree *zone,\n\t\t\t       int clear_nosave_free);\n\n \nstatic struct mem_zone_bm_rtree *create_zone_bm_rtree(gfp_t gfp_mask,\n\t\t\t\t\t\t      int safe_needed,\n\t\t\t\t\t\t      struct chain_allocator *ca,\n\t\t\t\t\t\t      unsigned long start,\n\t\t\t\t\t\t      unsigned long end)\n{\n\tstruct mem_zone_bm_rtree *zone;\n\tunsigned int i, nr_blocks;\n\tunsigned long pages;\n\n\tpages = end - start;\n\tzone  = chain_alloc(ca, sizeof(struct mem_zone_bm_rtree));\n\tif (!zone)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&zone->nodes);\n\tINIT_LIST_HEAD(&zone->leaves);\n\tzone->start_pfn = start;\n\tzone->end_pfn = end;\n\tnr_blocks = DIV_ROUND_UP(pages, BM_BITS_PER_BLOCK);\n\n\tfor (i = 0; i < nr_blocks; i++) {\n\t\tif (add_rtree_block(zone, gfp_mask, safe_needed, ca)) {\n\t\t\tfree_zone_bm_rtree(zone, PG_UNSAFE_CLEAR);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn zone;\n}\n\n \nstatic void free_zone_bm_rtree(struct mem_zone_bm_rtree *zone,\n\t\t\t       int clear_nosave_free)\n{\n\tstruct rtree_node *node;\n\n\tlist_for_each_entry(node, &zone->nodes, list)\n\t\tfree_image_page(node->data, clear_nosave_free);\n\n\tlist_for_each_entry(node, &zone->leaves, list)\n\t\tfree_image_page(node->data, clear_nosave_free);\n}\n\nstatic void memory_bm_position_reset(struct memory_bitmap *bm)\n{\n\tbm->cur.zone = list_entry(bm->zones.next, struct mem_zone_bm_rtree,\n\t\t\t\t  list);\n\tbm->cur.node = list_entry(bm->cur.zone->leaves.next,\n\t\t\t\t  struct rtree_node, list);\n\tbm->cur.node_pfn = 0;\n\tbm->cur.cur_pfn = BM_END_OF_MAP;\n\tbm->cur.node_bit = 0;\n}\n\nstatic void memory_bm_free(struct memory_bitmap *bm, int clear_nosave_free);\n\nstruct mem_extent {\n\tstruct list_head hook;\n\tunsigned long start;\n\tunsigned long end;\n};\n\n \nstatic void free_mem_extents(struct list_head *list)\n{\n\tstruct mem_extent *ext, *aux;\n\n\tlist_for_each_entry_safe(ext, aux, list, hook) {\n\t\tlist_del(&ext->hook);\n\t\tkfree(ext);\n\t}\n}\n\n \nstatic int create_mem_extents(struct list_head *list, gfp_t gfp_mask)\n{\n\tstruct zone *zone;\n\n\tINIT_LIST_HEAD(list);\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned long zone_start, zone_end;\n\t\tstruct mem_extent *ext, *cur, *aux;\n\n\t\tzone_start = zone->zone_start_pfn;\n\t\tzone_end = zone_end_pfn(zone);\n\n\t\tlist_for_each_entry(ext, list, hook)\n\t\t\tif (zone_start <= ext->end)\n\t\t\t\tbreak;\n\n\t\tif (&ext->hook == list || zone_end < ext->start) {\n\t\t\t \n\t\t\tstruct mem_extent *new_ext;\n\n\t\t\tnew_ext = kzalloc(sizeof(struct mem_extent), gfp_mask);\n\t\t\tif (!new_ext) {\n\t\t\t\tfree_mem_extents(list);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tnew_ext->start = zone_start;\n\t\t\tnew_ext->end = zone_end;\n\t\t\tlist_add_tail(&new_ext->hook, &ext->hook);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (zone_start < ext->start)\n\t\t\text->start = zone_start;\n\t\tif (zone_end > ext->end)\n\t\t\text->end = zone_end;\n\n\t\t \n\t\tcur = ext;\n\t\tlist_for_each_entry_safe_continue(cur, aux, list, hook) {\n\t\t\tif (zone_end < cur->start)\n\t\t\t\tbreak;\n\t\t\tif (zone_end < cur->end)\n\t\t\t\text->end = cur->end;\n\t\t\tlist_del(&cur->hook);\n\t\t\tkfree(cur);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int memory_bm_create(struct memory_bitmap *bm, gfp_t gfp_mask,\n\t\t\t    int safe_needed)\n{\n\tstruct chain_allocator ca;\n\tstruct list_head mem_extents;\n\tstruct mem_extent *ext;\n\tint error;\n\n\tchain_init(&ca, gfp_mask, safe_needed);\n\tINIT_LIST_HEAD(&bm->zones);\n\n\terror = create_mem_extents(&mem_extents, gfp_mask);\n\tif (error)\n\t\treturn error;\n\n\tlist_for_each_entry(ext, &mem_extents, hook) {\n\t\tstruct mem_zone_bm_rtree *zone;\n\n\t\tzone = create_zone_bm_rtree(gfp_mask, safe_needed, &ca,\n\t\t\t\t\t    ext->start, ext->end);\n\t\tif (!zone) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto Error;\n\t\t}\n\t\tlist_add_tail(&zone->list, &bm->zones);\n\t}\n\n\tbm->p_list = ca.chain;\n\tmemory_bm_position_reset(bm);\n Exit:\n\tfree_mem_extents(&mem_extents);\n\treturn error;\n\n Error:\n\tbm->p_list = ca.chain;\n\tmemory_bm_free(bm, PG_UNSAFE_CLEAR);\n\tgoto Exit;\n}\n\n \nstatic void memory_bm_free(struct memory_bitmap *bm, int clear_nosave_free)\n{\n\tstruct mem_zone_bm_rtree *zone;\n\n\tlist_for_each_entry(zone, &bm->zones, list)\n\t\tfree_zone_bm_rtree(zone, clear_nosave_free);\n\n\tfree_list_of_pages(bm->p_list, clear_nosave_free);\n\n\tINIT_LIST_HEAD(&bm->zones);\n}\n\n \nstatic int memory_bm_find_bit(struct memory_bitmap *bm, unsigned long pfn,\n\t\t\t      void **addr, unsigned int *bit_nr)\n{\n\tstruct mem_zone_bm_rtree *curr, *zone;\n\tstruct rtree_node *node;\n\tint i, block_nr;\n\n\tzone = bm->cur.zone;\n\n\tif (pfn >= zone->start_pfn && pfn < zone->end_pfn)\n\t\tgoto zone_found;\n\n\tzone = NULL;\n\n\t \n\tlist_for_each_entry(curr, &bm->zones, list) {\n\t\tif (pfn >= curr->start_pfn && pfn < curr->end_pfn) {\n\t\t\tzone = curr;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!zone)\n\t\treturn -EFAULT;\n\nzone_found:\n\t \n\n\t \n\tnode = bm->cur.node;\n\tif (zone == bm->cur.zone &&\n\t    ((pfn - zone->start_pfn) & ~BM_BLOCK_MASK) == bm->cur.node_pfn)\n\t\tgoto node_found;\n\n\tnode      = zone->rtree;\n\tblock_nr  = (pfn - zone->start_pfn) >> BM_BLOCK_SHIFT;\n\n\tfor (i = zone->levels; i > 0; i--) {\n\t\tint index;\n\n\t\tindex = block_nr >> ((i - 1) * BM_RTREE_LEVEL_SHIFT);\n\t\tindex &= BM_RTREE_LEVEL_MASK;\n\t\tBUG_ON(node->data[index] == 0);\n\t\tnode = (struct rtree_node *)node->data[index];\n\t}\n\nnode_found:\n\t \n\tbm->cur.zone = zone;\n\tbm->cur.node = node;\n\tbm->cur.node_pfn = (pfn - zone->start_pfn) & ~BM_BLOCK_MASK;\n\tbm->cur.cur_pfn = pfn;\n\n\t \n\t*addr = node->data;\n\t*bit_nr = (pfn - zone->start_pfn) & BM_BLOCK_MASK;\n\n\treturn 0;\n}\n\nstatic void memory_bm_set_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\tset_bit(bit, addr);\n}\n\nstatic int mem_bm_set_bit_check(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tif (!error)\n\t\tset_bit(bit, addr);\n\n\treturn error;\n}\n\nstatic void memory_bm_clear_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\tclear_bit(bit, addr);\n}\n\nstatic void memory_bm_clear_current(struct memory_bitmap *bm)\n{\n\tint bit;\n\n\tbit = max(bm->cur.node_bit - 1, 0);\n\tclear_bit(bit, bm->cur.node->data);\n}\n\nstatic unsigned long memory_bm_get_current(struct memory_bitmap *bm)\n{\n\treturn bm->cur.cur_pfn;\n}\n\nstatic int memory_bm_test_bit(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\tint error;\n\n\terror = memory_bm_find_bit(bm, pfn, &addr, &bit);\n\tBUG_ON(error);\n\treturn test_bit(bit, addr);\n}\n\nstatic bool memory_bm_pfn_present(struct memory_bitmap *bm, unsigned long pfn)\n{\n\tvoid *addr;\n\tunsigned int bit;\n\n\treturn !memory_bm_find_bit(bm, pfn, &addr, &bit);\n}\n\n \nstatic bool rtree_next_node(struct memory_bitmap *bm)\n{\n\tif (!list_is_last(&bm->cur.node->list, &bm->cur.zone->leaves)) {\n\t\tbm->cur.node = list_entry(bm->cur.node->list.next,\n\t\t\t\t\t  struct rtree_node, list);\n\t\tbm->cur.node_pfn += BM_BITS_PER_BLOCK;\n\t\tbm->cur.node_bit  = 0;\n\t\ttouch_softlockup_watchdog();\n\t\treturn true;\n\t}\n\n\t \n\tif (!list_is_last(&bm->cur.zone->list, &bm->zones)) {\n\t\tbm->cur.zone = list_entry(bm->cur.zone->list.next,\n\t\t\t\t  struct mem_zone_bm_rtree, list);\n\t\tbm->cur.node = list_entry(bm->cur.zone->leaves.next,\n\t\t\t\t\t  struct rtree_node, list);\n\t\tbm->cur.node_pfn = 0;\n\t\tbm->cur.node_bit = 0;\n\t\treturn true;\n\t}\n\n\t \n\treturn false;\n}\n\n \nstatic unsigned long memory_bm_next_pfn(struct memory_bitmap *bm)\n{\n\tunsigned long bits, pfn, pages;\n\tint bit;\n\n\tdo {\n\t\tpages\t  = bm->cur.zone->end_pfn - bm->cur.zone->start_pfn;\n\t\tbits      = min(pages - bm->cur.node_pfn, BM_BITS_PER_BLOCK);\n\t\tbit\t  = find_next_bit(bm->cur.node->data, bits,\n\t\t\t\t\t  bm->cur.node_bit);\n\t\tif (bit < bits) {\n\t\t\tpfn = bm->cur.zone->start_pfn + bm->cur.node_pfn + bit;\n\t\t\tbm->cur.node_bit = bit + 1;\n\t\t\tbm->cur.cur_pfn = pfn;\n\t\t\treturn pfn;\n\t\t}\n\t} while (rtree_next_node(bm));\n\n\tbm->cur.cur_pfn = BM_END_OF_MAP;\n\treturn BM_END_OF_MAP;\n}\n\n \nstruct nosave_region {\n\tstruct list_head list;\n\tunsigned long start_pfn;\n\tunsigned long end_pfn;\n};\n\nstatic LIST_HEAD(nosave_regions);\n\nstatic void recycle_zone_bm_rtree(struct mem_zone_bm_rtree *zone)\n{\n\tstruct rtree_node *node;\n\n\tlist_for_each_entry(node, &zone->nodes, list)\n\t\trecycle_safe_page(node->data);\n\n\tlist_for_each_entry(node, &zone->leaves, list)\n\t\trecycle_safe_page(node->data);\n}\n\nstatic void memory_bm_recycle(struct memory_bitmap *bm)\n{\n\tstruct mem_zone_bm_rtree *zone;\n\tstruct linked_page *p_list;\n\n\tlist_for_each_entry(zone, &bm->zones, list)\n\t\trecycle_zone_bm_rtree(zone);\n\n\tp_list = bm->p_list;\n\twhile (p_list) {\n\t\tstruct linked_page *lp = p_list;\n\n\t\tp_list = lp->next;\n\t\trecycle_safe_page(lp);\n\t}\n}\n\n \nvoid __init register_nosave_region(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tstruct nosave_region *region;\n\n\tif (start_pfn >= end_pfn)\n\t\treturn;\n\n\tif (!list_empty(&nosave_regions)) {\n\t\t \n\t\tregion = list_entry(nosave_regions.prev,\n\t\t\t\t\tstruct nosave_region, list);\n\t\tif (region->end_pfn == start_pfn) {\n\t\t\tregion->end_pfn = end_pfn;\n\t\t\tgoto Report;\n\t\t}\n\t}\n\t \n\tregion = memblock_alloc(sizeof(struct nosave_region),\n\t\t\t\tSMP_CACHE_BYTES);\n\tif (!region)\n\t\tpanic(\"%s: Failed to allocate %zu bytes\\n\", __func__,\n\t\t      sizeof(struct nosave_region));\n\tregion->start_pfn = start_pfn;\n\tregion->end_pfn = end_pfn;\n\tlist_add_tail(&region->list, &nosave_regions);\n Report:\n\tpr_info(\"Registered nosave memory: [mem %#010llx-%#010llx]\\n\",\n\t\t(unsigned long long) start_pfn << PAGE_SHIFT,\n\t\t((unsigned long long) end_pfn << PAGE_SHIFT) - 1);\n}\n\n \nstatic struct memory_bitmap *forbidden_pages_map;\n\n \nstatic struct memory_bitmap *free_pages_map;\n\n \n\nvoid swsusp_set_page_free(struct page *page)\n{\n\tif (free_pages_map)\n\t\tmemory_bm_set_bit(free_pages_map, page_to_pfn(page));\n}\n\nstatic int swsusp_page_is_free(struct page *page)\n{\n\treturn free_pages_map ?\n\t\tmemory_bm_test_bit(free_pages_map, page_to_pfn(page)) : 0;\n}\n\nvoid swsusp_unset_page_free(struct page *page)\n{\n\tif (free_pages_map)\n\t\tmemory_bm_clear_bit(free_pages_map, page_to_pfn(page));\n}\n\nstatic void swsusp_set_page_forbidden(struct page *page)\n{\n\tif (forbidden_pages_map)\n\t\tmemory_bm_set_bit(forbidden_pages_map, page_to_pfn(page));\n}\n\nint swsusp_page_is_forbidden(struct page *page)\n{\n\treturn forbidden_pages_map ?\n\t\tmemory_bm_test_bit(forbidden_pages_map, page_to_pfn(page)) : 0;\n}\n\nstatic void swsusp_unset_page_forbidden(struct page *page)\n{\n\tif (forbidden_pages_map)\n\t\tmemory_bm_clear_bit(forbidden_pages_map, page_to_pfn(page));\n}\n\n \nstatic void mark_nosave_pages(struct memory_bitmap *bm)\n{\n\tstruct nosave_region *region;\n\n\tif (list_empty(&nosave_regions))\n\t\treturn;\n\n\tlist_for_each_entry(region, &nosave_regions, list) {\n\t\tunsigned long pfn;\n\n\t\tpr_debug(\"Marking nosave pages: [mem %#010llx-%#010llx]\\n\",\n\t\t\t (unsigned long long) region->start_pfn << PAGE_SHIFT,\n\t\t\t ((unsigned long long) region->end_pfn << PAGE_SHIFT)\n\t\t\t\t- 1);\n\n\t\tfor (pfn = region->start_pfn; pfn < region->end_pfn; pfn++)\n\t\t\tif (pfn_valid(pfn)) {\n\t\t\t\t \n\t\t\t\tmem_bm_set_bit_check(bm, pfn);\n\t\t\t}\n\t}\n}\n\n \nint create_basic_memory_bitmaps(void)\n{\n\tstruct memory_bitmap *bm1, *bm2;\n\tint error = 0;\n\n\tif (forbidden_pages_map && free_pages_map)\n\t\treturn 0;\n\telse\n\t\tBUG_ON(forbidden_pages_map || free_pages_map);\n\n\tbm1 = kzalloc(sizeof(struct memory_bitmap), GFP_KERNEL);\n\tif (!bm1)\n\t\treturn -ENOMEM;\n\n\terror = memory_bm_create(bm1, GFP_KERNEL, PG_ANY);\n\tif (error)\n\t\tgoto Free_first_object;\n\n\tbm2 = kzalloc(sizeof(struct memory_bitmap), GFP_KERNEL);\n\tif (!bm2)\n\t\tgoto Free_first_bitmap;\n\n\terror = memory_bm_create(bm2, GFP_KERNEL, PG_ANY);\n\tif (error)\n\t\tgoto Free_second_object;\n\n\tforbidden_pages_map = bm1;\n\tfree_pages_map = bm2;\n\tmark_nosave_pages(forbidden_pages_map);\n\n\tpr_debug(\"Basic memory bitmaps created\\n\");\n\n\treturn 0;\n\n Free_second_object:\n\tkfree(bm2);\n Free_first_bitmap:\n\tmemory_bm_free(bm1, PG_UNSAFE_CLEAR);\n Free_first_object:\n\tkfree(bm1);\n\treturn -ENOMEM;\n}\n\n \nvoid free_basic_memory_bitmaps(void)\n{\n\tstruct memory_bitmap *bm1, *bm2;\n\n\tif (WARN_ON(!(forbidden_pages_map && free_pages_map)))\n\t\treturn;\n\n\tbm1 = forbidden_pages_map;\n\tbm2 = free_pages_map;\n\tforbidden_pages_map = NULL;\n\tfree_pages_map = NULL;\n\tmemory_bm_free(bm1, PG_UNSAFE_CLEAR);\n\tkfree(bm1);\n\tmemory_bm_free(bm2, PG_UNSAFE_CLEAR);\n\tkfree(bm2);\n\n\tpr_debug(\"Basic memory bitmaps freed\\n\");\n}\n\nstatic void clear_or_poison_free_page(struct page *page)\n{\n\tif (page_poisoning_enabled_static())\n\t\t__kernel_poison_pages(page, 1);\n\telse if (want_init_on_free())\n\t\tclear_highpage(page);\n}\n\nvoid clear_or_poison_free_pages(void)\n{\n\tstruct memory_bitmap *bm = free_pages_map;\n\tunsigned long pfn;\n\n\tif (WARN_ON(!(free_pages_map)))\n\t\treturn;\n\n\tif (page_poisoning_enabled() || want_init_on_free()) {\n\t\tmemory_bm_position_reset(bm);\n\t\tpfn = memory_bm_next_pfn(bm);\n\t\twhile (pfn != BM_END_OF_MAP) {\n\t\t\tif (pfn_valid(pfn))\n\t\t\t\tclear_or_poison_free_page(pfn_to_page(pfn));\n\n\t\t\tpfn = memory_bm_next_pfn(bm);\n\t\t}\n\t\tmemory_bm_position_reset(bm);\n\t\tpr_info(\"free pages cleared after restore\\n\");\n\t}\n}\n\n \nunsigned int snapshot_additional_pages(struct zone *zone)\n{\n\tunsigned int rtree, nodes;\n\n\trtree = nodes = DIV_ROUND_UP(zone->spanned_pages, BM_BITS_PER_BLOCK);\n\trtree += DIV_ROUND_UP(rtree * sizeof(struct rtree_node),\n\t\t\t      LINKED_PAGE_DATA_SIZE);\n\twhile (nodes > 1) {\n\t\tnodes = DIV_ROUND_UP(nodes, BM_ENTRIES_PER_LEVEL);\n\t\trtree += nodes;\n\t}\n\n\treturn 2 * rtree;\n}\n\n \n#define WD_PAGE_COUNT\t(128*1024)\n\nstatic void mark_free_pages(struct zone *zone)\n{\n\tunsigned long pfn, max_zone_pfn, page_count = WD_PAGE_COUNT;\n\tunsigned long flags;\n\tunsigned int order, t;\n\tstruct page *page;\n\n\tif (zone_is_empty(zone))\n\t\treturn;\n\n\tspin_lock_irqsave(&zone->lock, flags);\n\n\tmax_zone_pfn = zone_end_pfn(zone);\n\tfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\n\t\tif (pfn_valid(pfn)) {\n\t\t\tpage = pfn_to_page(pfn);\n\n\t\t\tif (!--page_count) {\n\t\t\t\ttouch_nmi_watchdog();\n\t\t\t\tpage_count = WD_PAGE_COUNT;\n\t\t\t}\n\n\t\t\tif (page_zone(page) != zone)\n\t\t\t\tcontinue;\n\n\t\t\tif (!swsusp_page_is_forbidden(page))\n\t\t\t\tswsusp_unset_page_free(page);\n\t\t}\n\n\tfor_each_migratetype_order(order, t) {\n\t\tlist_for_each_entry(page,\n\t\t\t\t&zone->free_area[order].free_list[t], buddy_list) {\n\t\t\tunsigned long i;\n\n\t\t\tpfn = page_to_pfn(page);\n\t\t\tfor (i = 0; i < (1UL << order); i++) {\n\t\t\t\tif (!--page_count) {\n\t\t\t\t\ttouch_nmi_watchdog();\n\t\t\t\t\tpage_count = WD_PAGE_COUNT;\n\t\t\t\t}\n\t\t\t\tswsusp_set_page_free(pfn_to_page(pfn + i));\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&zone->lock, flags);\n}\n\n#ifdef CONFIG_HIGHMEM\n \nstatic unsigned int count_free_highmem_pages(void)\n{\n\tstruct zone *zone;\n\tunsigned int cnt = 0;\n\n\tfor_each_populated_zone(zone)\n\t\tif (is_highmem(zone))\n\t\t\tcnt += zone_page_state(zone, NR_FREE_PAGES);\n\n\treturn cnt;\n}\n\n \nstatic struct page *saveable_highmem_page(struct zone *zone, unsigned long pfn)\n{\n\tstruct page *page;\n\n\tif (!pfn_valid(pfn))\n\t\treturn NULL;\n\n\tpage = pfn_to_online_page(pfn);\n\tif (!page || page_zone(page) != zone)\n\t\treturn NULL;\n\n\tBUG_ON(!PageHighMem(page));\n\n\tif (swsusp_page_is_forbidden(page) ||  swsusp_page_is_free(page))\n\t\treturn NULL;\n\n\tif (PageReserved(page) || PageOffline(page))\n\t\treturn NULL;\n\n\tif (page_is_guard(page))\n\t\treturn NULL;\n\n\treturn page;\n}\n\n \nstatic unsigned int count_highmem_pages(void)\n{\n\tstruct zone *zone;\n\tunsigned int n = 0;\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned long pfn, max_zone_pfn;\n\n\t\tif (!is_highmem(zone))\n\t\t\tcontinue;\n\n\t\tmark_free_pages(zone);\n\t\tmax_zone_pfn = zone_end_pfn(zone);\n\t\tfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\n\t\t\tif (saveable_highmem_page(zone, pfn))\n\t\t\t\tn++;\n\t}\n\treturn n;\n}\n#else\nstatic inline void *saveable_highmem_page(struct zone *z, unsigned long p)\n{\n\treturn NULL;\n}\n#endif  \n\n \nstatic struct page *saveable_page(struct zone *zone, unsigned long pfn)\n{\n\tstruct page *page;\n\n\tif (!pfn_valid(pfn))\n\t\treturn NULL;\n\n\tpage = pfn_to_online_page(pfn);\n\tif (!page || page_zone(page) != zone)\n\t\treturn NULL;\n\n\tBUG_ON(PageHighMem(page));\n\n\tif (swsusp_page_is_forbidden(page) || swsusp_page_is_free(page))\n\t\treturn NULL;\n\n\tif (PageOffline(page))\n\t\treturn NULL;\n\n\tif (PageReserved(page)\n\t    && (!kernel_page_present(page) || pfn_is_nosave(pfn)))\n\t\treturn NULL;\n\n\tif (page_is_guard(page))\n\t\treturn NULL;\n\n\treturn page;\n}\n\n \nstatic unsigned int count_data_pages(void)\n{\n\tstruct zone *zone;\n\tunsigned long pfn, max_zone_pfn;\n\tunsigned int n = 0;\n\n\tfor_each_populated_zone(zone) {\n\t\tif (is_highmem(zone))\n\t\t\tcontinue;\n\n\t\tmark_free_pages(zone);\n\t\tmax_zone_pfn = zone_end_pfn(zone);\n\t\tfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\n\t\t\tif (saveable_page(zone, pfn))\n\t\t\t\tn++;\n\t}\n\treturn n;\n}\n\n \nstatic inline bool do_copy_page(long *dst, long *src)\n{\n\tlong z = 0;\n\tint n;\n\n\tfor (n = PAGE_SIZE / sizeof(long); n; n--) {\n\t\tz |= *src;\n\t\t*dst++ = *src++;\n\t}\n\treturn !z;\n}\n\n \nstatic bool safe_copy_page(void *dst, struct page *s_page)\n{\n\tbool zeros_only;\n\n\tif (kernel_page_present(s_page)) {\n\t\tzeros_only = do_copy_page(dst, page_address(s_page));\n\t} else {\n\t\thibernate_map_page(s_page);\n\t\tzeros_only = do_copy_page(dst, page_address(s_page));\n\t\thibernate_unmap_page(s_page);\n\t}\n\treturn zeros_only;\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic inline struct page *page_is_saveable(struct zone *zone, unsigned long pfn)\n{\n\treturn is_highmem(zone) ?\n\t\tsaveable_highmem_page(zone, pfn) : saveable_page(zone, pfn);\n}\n\nstatic bool copy_data_page(unsigned long dst_pfn, unsigned long src_pfn)\n{\n\tstruct page *s_page, *d_page;\n\tvoid *src, *dst;\n\tbool zeros_only;\n\n\ts_page = pfn_to_page(src_pfn);\n\td_page = pfn_to_page(dst_pfn);\n\tif (PageHighMem(s_page)) {\n\t\tsrc = kmap_atomic(s_page);\n\t\tdst = kmap_atomic(d_page);\n\t\tzeros_only = do_copy_page(dst, src);\n\t\tkunmap_atomic(dst);\n\t\tkunmap_atomic(src);\n\t} else {\n\t\tif (PageHighMem(d_page)) {\n\t\t\t \n\t\t\tzeros_only = safe_copy_page(buffer, s_page);\n\t\t\tdst = kmap_atomic(d_page);\n\t\t\tcopy_page(dst, buffer);\n\t\t\tkunmap_atomic(dst);\n\t\t} else {\n\t\t\tzeros_only = safe_copy_page(page_address(d_page), s_page);\n\t\t}\n\t}\n\treturn zeros_only;\n}\n#else\n#define page_is_saveable(zone, pfn)\tsaveable_page(zone, pfn)\n\nstatic inline int copy_data_page(unsigned long dst_pfn, unsigned long src_pfn)\n{\n\treturn safe_copy_page(page_address(pfn_to_page(dst_pfn)),\n\t\t\t\tpfn_to_page(src_pfn));\n}\n#endif  \n\n \nstatic unsigned long copy_data_pages(struct memory_bitmap *copy_bm,\n\t\t\t    struct memory_bitmap *orig_bm,\n\t\t\t    struct memory_bitmap *zero_bm)\n{\n\tunsigned long copied_pages = 0;\n\tstruct zone *zone;\n\tunsigned long pfn, copy_pfn;\n\n\tfor_each_populated_zone(zone) {\n\t\tunsigned long max_zone_pfn;\n\n\t\tmark_free_pages(zone);\n\t\tmax_zone_pfn = zone_end_pfn(zone);\n\t\tfor (pfn = zone->zone_start_pfn; pfn < max_zone_pfn; pfn++)\n\t\t\tif (page_is_saveable(zone, pfn))\n\t\t\t\tmemory_bm_set_bit(orig_bm, pfn);\n\t}\n\tmemory_bm_position_reset(orig_bm);\n\tmemory_bm_position_reset(copy_bm);\n\tcopy_pfn = memory_bm_next_pfn(copy_bm);\n\tfor(;;) {\n\t\tpfn = memory_bm_next_pfn(orig_bm);\n\t\tif (unlikely(pfn == BM_END_OF_MAP))\n\t\t\tbreak;\n\t\tif (copy_data_page(copy_pfn, pfn)) {\n\t\t\tmemory_bm_set_bit(zero_bm, pfn);\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\tcopied_pages++;\n\t\tcopy_pfn = memory_bm_next_pfn(copy_bm);\n\t}\n\treturn copied_pages;\n}\n\n \nstatic unsigned int nr_copy_pages;\n \nstatic unsigned int nr_meta_pages;\n \nstatic unsigned int nr_zero_pages;\n\n \nstatic unsigned int alloc_normal, alloc_highmem;\n \nstatic struct memory_bitmap orig_bm;\n \nstatic struct memory_bitmap copy_bm;\n\n \nstatic struct memory_bitmap zero_bm;\n\n \nvoid swsusp_free(void)\n{\n\tunsigned long fb_pfn, fr_pfn;\n\n\tif (!forbidden_pages_map || !free_pages_map)\n\t\tgoto out;\n\n\tmemory_bm_position_reset(forbidden_pages_map);\n\tmemory_bm_position_reset(free_pages_map);\n\nloop:\n\tfr_pfn = memory_bm_next_pfn(free_pages_map);\n\tfb_pfn = memory_bm_next_pfn(forbidden_pages_map);\n\n\t \n\tdo {\n\t\tif (fb_pfn < fr_pfn)\n\t\t\tfb_pfn = memory_bm_next_pfn(forbidden_pages_map);\n\t\tif (fr_pfn < fb_pfn)\n\t\t\tfr_pfn = memory_bm_next_pfn(free_pages_map);\n\t} while (fb_pfn != fr_pfn);\n\n\tif (fr_pfn != BM_END_OF_MAP && pfn_valid(fr_pfn)) {\n\t\tstruct page *page = pfn_to_page(fr_pfn);\n\n\t\tmemory_bm_clear_current(forbidden_pages_map);\n\t\tmemory_bm_clear_current(free_pages_map);\n\t\thibernate_restore_unprotect_page(page_address(page));\n\t\t__free_page(page);\n\t\tgoto loop;\n\t}\n\nout:\n\tnr_copy_pages = 0;\n\tnr_meta_pages = 0;\n\tnr_zero_pages = 0;\n\trestore_pblist = NULL;\n\tbuffer = NULL;\n\talloc_normal = 0;\n\talloc_highmem = 0;\n\thibernate_restore_protection_end();\n}\n\n \n\n#define GFP_IMAGE\t(GFP_KERNEL | __GFP_NOWARN)\n\n \nstatic unsigned long preallocate_image_pages(unsigned long nr_pages, gfp_t mask)\n{\n\tunsigned long nr_alloc = 0;\n\n\twhile (nr_pages > 0) {\n\t\tstruct page *page;\n\n\t\tpage = alloc_image_page(mask);\n\t\tif (!page)\n\t\t\tbreak;\n\t\tmemory_bm_set_bit(&copy_bm, page_to_pfn(page));\n\t\tif (PageHighMem(page))\n\t\t\talloc_highmem++;\n\t\telse\n\t\t\talloc_normal++;\n\t\tnr_pages--;\n\t\tnr_alloc++;\n\t}\n\n\treturn nr_alloc;\n}\n\nstatic unsigned long preallocate_image_memory(unsigned long nr_pages,\n\t\t\t\t\t      unsigned long avail_normal)\n{\n\tunsigned long alloc;\n\n\tif (avail_normal <= alloc_normal)\n\t\treturn 0;\n\n\talloc = avail_normal - alloc_normal;\n\tif (nr_pages < alloc)\n\t\talloc = nr_pages;\n\n\treturn preallocate_image_pages(alloc, GFP_IMAGE);\n}\n\n#ifdef CONFIG_HIGHMEM\nstatic unsigned long preallocate_image_highmem(unsigned long nr_pages)\n{\n\treturn preallocate_image_pages(nr_pages, GFP_IMAGE | __GFP_HIGHMEM);\n}\n\n \nstatic unsigned long __fraction(u64 x, u64 multiplier, u64 base)\n{\n\treturn div64_u64(x * multiplier, base);\n}\n\nstatic unsigned long preallocate_highmem_fraction(unsigned long nr_pages,\n\t\t\t\t\t\t  unsigned long highmem,\n\t\t\t\t\t\t  unsigned long total)\n{\n\tunsigned long alloc = __fraction(nr_pages, highmem, total);\n\n\treturn preallocate_image_pages(alloc, GFP_IMAGE | __GFP_HIGHMEM);\n}\n#else  \nstatic inline unsigned long preallocate_image_highmem(unsigned long nr_pages)\n{\n\treturn 0;\n}\n\nstatic inline unsigned long preallocate_highmem_fraction(unsigned long nr_pages,\n\t\t\t\t\t\t\t unsigned long highmem,\n\t\t\t\t\t\t\t unsigned long total)\n{\n\treturn 0;\n}\n#endif  \n\n \nstatic unsigned long free_unnecessary_pages(void)\n{\n\tunsigned long save, to_free_normal, to_free_highmem, free;\n\n\tsave = count_data_pages();\n\tif (alloc_normal >= save) {\n\t\tto_free_normal = alloc_normal - save;\n\t\tsave = 0;\n\t} else {\n\t\tto_free_normal = 0;\n\t\tsave -= alloc_normal;\n\t}\n\tsave += count_highmem_pages();\n\tif (alloc_highmem >= save) {\n\t\tto_free_highmem = alloc_highmem - save;\n\t} else {\n\t\tto_free_highmem = 0;\n\t\tsave -= alloc_highmem;\n\t\tif (to_free_normal > save)\n\t\t\tto_free_normal -= save;\n\t\telse\n\t\t\tto_free_normal = 0;\n\t}\n\tfree = to_free_normal + to_free_highmem;\n\n\tmemory_bm_position_reset(&copy_bm);\n\n\twhile (to_free_normal > 0 || to_free_highmem > 0) {\n\t\tunsigned long pfn = memory_bm_next_pfn(&copy_bm);\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tif (PageHighMem(page)) {\n\t\t\tif (!to_free_highmem)\n\t\t\t\tcontinue;\n\t\t\tto_free_highmem--;\n\t\t\talloc_highmem--;\n\t\t} else {\n\t\t\tif (!to_free_normal)\n\t\t\t\tcontinue;\n\t\t\tto_free_normal--;\n\t\t\talloc_normal--;\n\t\t}\n\t\tmemory_bm_clear_bit(&copy_bm, pfn);\n\t\tswsusp_unset_page_forbidden(page);\n\t\tswsusp_unset_page_free(page);\n\t\t__free_page(page);\n\t}\n\n\treturn free;\n}\n\n \nstatic unsigned long minimum_image_size(unsigned long saveable)\n{\n\tunsigned long size;\n\n\tsize = global_node_page_state_pages(NR_SLAB_RECLAIMABLE_B)\n\t\t+ global_node_page_state(NR_ACTIVE_ANON)\n\t\t+ global_node_page_state(NR_INACTIVE_ANON)\n\t\t+ global_node_page_state(NR_ACTIVE_FILE)\n\t\t+ global_node_page_state(NR_INACTIVE_FILE);\n\n\treturn saveable <= size ? 0 : saveable - size;\n}\n\n \nint hibernate_preallocate_memory(void)\n{\n\tstruct zone *zone;\n\tunsigned long saveable, size, max_size, count, highmem, pages = 0;\n\tunsigned long alloc, save_highmem, pages_highmem, avail_normal;\n\tktime_t start, stop;\n\tint error;\n\n\tpr_info(\"Preallocating image memory\\n\");\n\tstart = ktime_get();\n\n\terror = memory_bm_create(&orig_bm, GFP_IMAGE, PG_ANY);\n\tif (error) {\n\t\tpr_err(\"Cannot allocate original bitmap\\n\");\n\t\tgoto err_out;\n\t}\n\n\terror = memory_bm_create(&copy_bm, GFP_IMAGE, PG_ANY);\n\tif (error) {\n\t\tpr_err(\"Cannot allocate copy bitmap\\n\");\n\t\tgoto err_out;\n\t}\n\n\terror = memory_bm_create(&zero_bm, GFP_IMAGE, PG_ANY);\n\tif (error) {\n\t\tpr_err(\"Cannot allocate zero bitmap\\n\");\n\t\tgoto err_out;\n\t}\n\n\talloc_normal = 0;\n\talloc_highmem = 0;\n\tnr_zero_pages = 0;\n\n\t \n\tsave_highmem = count_highmem_pages();\n\tsaveable = count_data_pages();\n\n\t \n\tcount = saveable;\n\tsaveable += save_highmem;\n\thighmem = save_highmem;\n\tsize = 0;\n\tfor_each_populated_zone(zone) {\n\t\tsize += snapshot_additional_pages(zone);\n\t\tif (is_highmem(zone))\n\t\t\thighmem += zone_page_state(zone, NR_FREE_PAGES);\n\t\telse\n\t\t\tcount += zone_page_state(zone, NR_FREE_PAGES);\n\t}\n\tavail_normal = count;\n\tcount += highmem;\n\tcount -= totalreserve_pages;\n\n\t \n\tmax_size = (count - (size + PAGES_FOR_IO)) / 2\n\t\t\t- 2 * DIV_ROUND_UP(reserved_size, PAGE_SIZE);\n\t \n\tsize = DIV_ROUND_UP(image_size, PAGE_SIZE);\n\tif (size > max_size)\n\t\tsize = max_size;\n\t \n\tif (size >= saveable) {\n\t\tpages = preallocate_image_highmem(save_highmem);\n\t\tpages += preallocate_image_memory(saveable - pages, avail_normal);\n\t\tgoto out;\n\t}\n\n\t \n\tpages = minimum_image_size(saveable);\n\t \n\tif (avail_normal > pages)\n\t\tavail_normal -= pages;\n\telse\n\t\tavail_normal = 0;\n\tif (size < pages)\n\t\tsize = min_t(unsigned long, pages, max_size);\n\n\t \n\tshrink_all_memory(saveable - size);\n\n\t \n\tpages_highmem = preallocate_image_highmem(highmem / 2);\n\talloc = count - max_size;\n\tif (alloc > pages_highmem)\n\t\talloc -= pages_highmem;\n\telse\n\t\talloc = 0;\n\tpages = preallocate_image_memory(alloc, avail_normal);\n\tif (pages < alloc) {\n\t\t \n\t\talloc -= pages;\n\t\tpages += pages_highmem;\n\t\tpages_highmem = preallocate_image_highmem(alloc);\n\t\tif (pages_highmem < alloc) {\n\t\t\tpr_err(\"Image allocation is %lu pages short\\n\",\n\t\t\t\talloc - pages_highmem);\n\t\t\tgoto err_out;\n\t\t}\n\t\tpages += pages_highmem;\n\t\t \n\t\talloc = (count - pages) - size;\n\t\tpages += preallocate_image_highmem(alloc);\n\t} else {\n\t\t \n\t\talloc = max_size - size;\n\t\tsize = preallocate_highmem_fraction(alloc, highmem, count);\n\t\tpages_highmem += size;\n\t\talloc -= size;\n\t\tsize = preallocate_image_memory(alloc, avail_normal);\n\t\tpages_highmem += preallocate_image_highmem(alloc - size);\n\t\tpages += pages_highmem + size;\n\t}\n\n\t \n\tpages -= free_unnecessary_pages();\n\n out:\n\tstop = ktime_get();\n\tpr_info(\"Allocated %lu pages for snapshot\\n\", pages);\n\tswsusp_show_speed(start, stop, pages, \"Allocated\");\n\n\treturn 0;\n\n err_out:\n\tswsusp_free();\n\treturn -ENOMEM;\n}\n\n#ifdef CONFIG_HIGHMEM\n \nstatic unsigned int count_pages_for_highmem(unsigned int nr_highmem)\n{\n\tunsigned int free_highmem = count_free_highmem_pages() + alloc_highmem;\n\n\tif (free_highmem >= nr_highmem)\n\t\tnr_highmem = 0;\n\telse\n\t\tnr_highmem -= free_highmem;\n\n\treturn nr_highmem;\n}\n#else\nstatic unsigned int count_pages_for_highmem(unsigned int nr_highmem) { return 0; }\n#endif  \n\n \nstatic int enough_free_mem(unsigned int nr_pages, unsigned int nr_highmem)\n{\n\tstruct zone *zone;\n\tunsigned int free = alloc_normal;\n\n\tfor_each_populated_zone(zone)\n\t\tif (!is_highmem(zone))\n\t\t\tfree += zone_page_state(zone, NR_FREE_PAGES);\n\n\tnr_pages += count_pages_for_highmem(nr_highmem);\n\tpr_debug(\"Normal pages needed: %u + %u, available pages: %u\\n\",\n\t\t nr_pages, PAGES_FOR_IO, free);\n\n\treturn free > nr_pages + PAGES_FOR_IO;\n}\n\n#ifdef CONFIG_HIGHMEM\n \nstatic inline int get_highmem_buffer(int safe_needed)\n{\n\tbuffer = get_image_page(GFP_ATOMIC, safe_needed);\n\treturn buffer ? 0 : -ENOMEM;\n}\n\n \nstatic inline unsigned int alloc_highmem_pages(struct memory_bitmap *bm,\n\t\t\t\t\t       unsigned int nr_highmem)\n{\n\tunsigned int to_alloc = count_free_highmem_pages();\n\n\tif (to_alloc > nr_highmem)\n\t\tto_alloc = nr_highmem;\n\n\tnr_highmem -= to_alloc;\n\twhile (to_alloc-- > 0) {\n\t\tstruct page *page;\n\n\t\tpage = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);\n\t\tmemory_bm_set_bit(bm, page_to_pfn(page));\n\t}\n\treturn nr_highmem;\n}\n#else\nstatic inline int get_highmem_buffer(int safe_needed) { return 0; }\n\nstatic inline unsigned int alloc_highmem_pages(struct memory_bitmap *bm,\n\t\t\t\t\t       unsigned int n) { return 0; }\n#endif  \n\n \nstatic int swsusp_alloc(struct memory_bitmap *copy_bm,\n\t\t\tunsigned int nr_pages, unsigned int nr_highmem)\n{\n\tif (nr_highmem > 0) {\n\t\tif (get_highmem_buffer(PG_ANY))\n\t\t\tgoto err_out;\n\t\tif (nr_highmem > alloc_highmem) {\n\t\t\tnr_highmem -= alloc_highmem;\n\t\t\tnr_pages += alloc_highmem_pages(copy_bm, nr_highmem);\n\t\t}\n\t}\n\tif (nr_pages > alloc_normal) {\n\t\tnr_pages -= alloc_normal;\n\t\twhile (nr_pages-- > 0) {\n\t\t\tstruct page *page;\n\n\t\t\tpage = alloc_image_page(GFP_ATOMIC);\n\t\t\tif (!page)\n\t\t\t\tgoto err_out;\n\t\t\tmemory_bm_set_bit(copy_bm, page_to_pfn(page));\n\t\t}\n\t}\n\n\treturn 0;\n\n err_out:\n\tswsusp_free();\n\treturn -ENOMEM;\n}\n\nasmlinkage __visible int swsusp_save(void)\n{\n\tunsigned int nr_pages, nr_highmem;\n\n\tpr_info(\"Creating image:\\n\");\n\n\tdrain_local_pages(NULL);\n\tnr_pages = count_data_pages();\n\tnr_highmem = count_highmem_pages();\n\tpr_info(\"Need to copy %u pages\\n\", nr_pages + nr_highmem);\n\n\tif (!enough_free_mem(nr_pages, nr_highmem)) {\n\t\tpr_err(\"Not enough free memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (swsusp_alloc(&copy_bm, nr_pages, nr_highmem)) {\n\t\tpr_err(\"Memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tdrain_local_pages(NULL);\n\tnr_copy_pages = copy_data_pages(&copy_bm, &orig_bm, &zero_bm);\n\n\t \n\tnr_pages += nr_highmem;\n\t \n\tnr_zero_pages = nr_pages - nr_copy_pages;\n\tnr_meta_pages = DIV_ROUND_UP(nr_pages * sizeof(long), PAGE_SIZE);\n\n\tpr_info(\"Image created (%d pages copied, %d zero pages)\\n\", nr_copy_pages, nr_zero_pages);\n\n\treturn 0;\n}\n\n#ifndef CONFIG_ARCH_HIBERNATION_HEADER\nstatic int init_header_complete(struct swsusp_info *info)\n{\n\tmemcpy(&info->uts, init_utsname(), sizeof(struct new_utsname));\n\tinfo->version_code = LINUX_VERSION_CODE;\n\treturn 0;\n}\n\nstatic const char *check_image_kernel(struct swsusp_info *info)\n{\n\tif (info->version_code != LINUX_VERSION_CODE)\n\t\treturn \"kernel version\";\n\tif (strcmp(info->uts.sysname,init_utsname()->sysname))\n\t\treturn \"system type\";\n\tif (strcmp(info->uts.release,init_utsname()->release))\n\t\treturn \"kernel release\";\n\tif (strcmp(info->uts.version,init_utsname()->version))\n\t\treturn \"version\";\n\tif (strcmp(info->uts.machine,init_utsname()->machine))\n\t\treturn \"machine\";\n\treturn NULL;\n}\n#endif  \n\nunsigned long snapshot_get_image_size(void)\n{\n\treturn nr_copy_pages + nr_meta_pages + 1;\n}\n\nstatic int init_header(struct swsusp_info *info)\n{\n\tmemset(info, 0, sizeof(struct swsusp_info));\n\tinfo->num_physpages = get_num_physpages();\n\tinfo->image_pages = nr_copy_pages;\n\tinfo->pages = snapshot_get_image_size();\n\tinfo->size = info->pages;\n\tinfo->size <<= PAGE_SHIFT;\n\treturn init_header_complete(info);\n}\n\n#define ENCODED_PFN_ZERO_FLAG ((unsigned long)1 << (BITS_PER_LONG - 1))\n#define ENCODED_PFN_MASK (~ENCODED_PFN_ZERO_FLAG)\n\n \nstatic inline void pack_pfns(unsigned long *buf, struct memory_bitmap *bm,\n\t\tstruct memory_bitmap *zero_bm)\n{\n\tint j;\n\n\tfor (j = 0; j < PAGE_SIZE / sizeof(long); j++) {\n\t\tbuf[j] = memory_bm_next_pfn(bm);\n\t\tif (unlikely(buf[j] == BM_END_OF_MAP))\n\t\t\tbreak;\n\t\tif (memory_bm_test_bit(zero_bm, buf[j]))\n\t\t\tbuf[j] |= ENCODED_PFN_ZERO_FLAG;\n\t}\n}\n\n \nint snapshot_read_next(struct snapshot_handle *handle)\n{\n\tif (handle->cur > nr_meta_pages + nr_copy_pages)\n\t\treturn 0;\n\n\tif (!buffer) {\n\t\t \n\t\tbuffer = get_image_page(GFP_ATOMIC, PG_ANY);\n\t\tif (!buffer)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (!handle->cur) {\n\t\tint error;\n\n\t\terror = init_header((struct swsusp_info *)buffer);\n\t\tif (error)\n\t\t\treturn error;\n\t\thandle->buffer = buffer;\n\t\tmemory_bm_position_reset(&orig_bm);\n\t\tmemory_bm_position_reset(&copy_bm);\n\t} else if (handle->cur <= nr_meta_pages) {\n\t\tclear_page(buffer);\n\t\tpack_pfns(buffer, &orig_bm, &zero_bm);\n\t} else {\n\t\tstruct page *page;\n\n\t\tpage = pfn_to_page(memory_bm_next_pfn(&copy_bm));\n\t\tif (PageHighMem(page)) {\n\t\t\t \n\t\t\tvoid *kaddr;\n\n\t\t\tkaddr = kmap_atomic(page);\n\t\t\tcopy_page(buffer, kaddr);\n\t\t\tkunmap_atomic(kaddr);\n\t\t\thandle->buffer = buffer;\n\t\t} else {\n\t\t\thandle->buffer = page_address(page);\n\t\t}\n\t}\n\thandle->cur++;\n\treturn PAGE_SIZE;\n}\n\nstatic void duplicate_memory_bitmap(struct memory_bitmap *dst,\n\t\t\t\t    struct memory_bitmap *src)\n{\n\tunsigned long pfn;\n\n\tmemory_bm_position_reset(src);\n\tpfn = memory_bm_next_pfn(src);\n\twhile (pfn != BM_END_OF_MAP) {\n\t\tmemory_bm_set_bit(dst, pfn);\n\t\tpfn = memory_bm_next_pfn(src);\n\t}\n}\n\n \nstatic void mark_unsafe_pages(struct memory_bitmap *bm)\n{\n\tunsigned long pfn;\n\n\t \n\tmemory_bm_position_reset(free_pages_map);\n\tpfn = memory_bm_next_pfn(free_pages_map);\n\twhile (pfn != BM_END_OF_MAP) {\n\t\tmemory_bm_clear_current(free_pages_map);\n\t\tpfn = memory_bm_next_pfn(free_pages_map);\n\t}\n\n\t \n\tduplicate_memory_bitmap(free_pages_map, bm);\n\n\tallocated_unsafe_pages = 0;\n}\n\nstatic int check_header(struct swsusp_info *info)\n{\n\tconst char *reason;\n\n\treason = check_image_kernel(info);\n\tif (!reason && info->num_physpages != get_num_physpages())\n\t\treason = \"memory size\";\n\tif (reason) {\n\t\tpr_err(\"Image mismatch: %s\\n\", reason);\n\t\treturn -EPERM;\n\t}\n\treturn 0;\n}\n\n \nstatic int load_header(struct swsusp_info *info)\n{\n\tint error;\n\n\trestore_pblist = NULL;\n\terror = check_header(info);\n\tif (!error) {\n\t\tnr_copy_pages = info->image_pages;\n\t\tnr_meta_pages = info->pages - info->image_pages - 1;\n\t}\n\treturn error;\n}\n\n \nstatic int unpack_orig_pfns(unsigned long *buf, struct memory_bitmap *bm,\n\t\tstruct memory_bitmap *zero_bm)\n{\n\tunsigned long decoded_pfn;\n        bool zero;\n\tint j;\n\n\tfor (j = 0; j < PAGE_SIZE / sizeof(long); j++) {\n\t\tif (unlikely(buf[j] == BM_END_OF_MAP))\n\t\t\tbreak;\n\n\t\tzero = !!(buf[j] & ENCODED_PFN_ZERO_FLAG);\n\t\tdecoded_pfn = buf[j] & ENCODED_PFN_MASK;\n\t\tif (pfn_valid(decoded_pfn) && memory_bm_pfn_present(bm, decoded_pfn)) {\n\t\t\tmemory_bm_set_bit(bm, decoded_pfn);\n\t\t\tif (zero) {\n\t\t\t\tmemory_bm_set_bit(zero_bm, decoded_pfn);\n\t\t\t\tnr_zero_pages++;\n\t\t\t}\n\t\t} else {\n\t\t\tif (!pfn_valid(decoded_pfn))\n\t\t\t\tpr_err(FW_BUG \"Memory map mismatch at 0x%llx after hibernation\\n\",\n\t\t\t\t       (unsigned long long)PFN_PHYS(decoded_pfn));\n\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_HIGHMEM\n \nstruct highmem_pbe {\n\tstruct page *copy_page;\t \n\tstruct page *orig_page;\t \n\tstruct highmem_pbe *next;\n};\n\n \nstatic struct highmem_pbe *highmem_pblist;\n\n \nstatic unsigned int count_highmem_image_pages(struct memory_bitmap *bm)\n{\n\tunsigned long pfn;\n\tunsigned int cnt = 0;\n\n\tmemory_bm_position_reset(bm);\n\tpfn = memory_bm_next_pfn(bm);\n\twhile (pfn != BM_END_OF_MAP) {\n\t\tif (PageHighMem(pfn_to_page(pfn)))\n\t\t\tcnt++;\n\n\t\tpfn = memory_bm_next_pfn(bm);\n\t}\n\treturn cnt;\n}\n\nstatic unsigned int safe_highmem_pages;\n\nstatic struct memory_bitmap *safe_highmem_bm;\n\n \nstatic int prepare_highmem_image(struct memory_bitmap *bm,\n\t\t\t\t unsigned int *nr_highmem_p)\n{\n\tunsigned int to_alloc;\n\n\tif (memory_bm_create(bm, GFP_ATOMIC, PG_SAFE))\n\t\treturn -ENOMEM;\n\n\tif (get_highmem_buffer(PG_SAFE))\n\t\treturn -ENOMEM;\n\n\tto_alloc = count_free_highmem_pages();\n\tif (to_alloc > *nr_highmem_p)\n\t\tto_alloc = *nr_highmem_p;\n\telse\n\t\t*nr_highmem_p = to_alloc;\n\n\tsafe_highmem_pages = 0;\n\twhile (to_alloc-- > 0) {\n\t\tstruct page *page;\n\n\t\tpage = alloc_page(__GFP_HIGHMEM);\n\t\tif (!swsusp_page_is_free(page)) {\n\t\t\t \n\t\t\tmemory_bm_set_bit(bm, page_to_pfn(page));\n\t\t\tsafe_highmem_pages++;\n\t\t}\n\t\t \n\t\tswsusp_set_page_forbidden(page);\n\t\tswsusp_set_page_free(page);\n\t}\n\tmemory_bm_position_reset(bm);\n\tsafe_highmem_bm = bm;\n\treturn 0;\n}\n\nstatic struct page *last_highmem_page;\n\n \nstatic void *get_highmem_page_buffer(struct page *page,\n\t\t\t\t     struct chain_allocator *ca)\n{\n\tstruct highmem_pbe *pbe;\n\tvoid *kaddr;\n\n\tif (swsusp_page_is_forbidden(page) && swsusp_page_is_free(page)) {\n\t\t \n\t\tlast_highmem_page = page;\n\t\treturn buffer;\n\t}\n\t \n\tpbe = chain_alloc(ca, sizeof(struct highmem_pbe));\n\tif (!pbe) {\n\t\tswsusp_free();\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tpbe->orig_page = page;\n\tif (safe_highmem_pages > 0) {\n\t\tstruct page *tmp;\n\n\t\t \n\t\tkaddr = buffer;\n\t\ttmp = pfn_to_page(memory_bm_next_pfn(safe_highmem_bm));\n\t\tsafe_highmem_pages--;\n\t\tlast_highmem_page = tmp;\n\t\tpbe->copy_page = tmp;\n\t} else {\n\t\t \n\t\tkaddr = __get_safe_page(ca->gfp_mask);\n\t\tif (!kaddr)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tpbe->copy_page = virt_to_page(kaddr);\n\t}\n\tpbe->next = highmem_pblist;\n\thighmem_pblist = pbe;\n\treturn kaddr;\n}\n\n \nstatic void copy_last_highmem_page(void)\n{\n\tif (last_highmem_page) {\n\t\tvoid *dst;\n\n\t\tdst = kmap_atomic(last_highmem_page);\n\t\tcopy_page(dst, buffer);\n\t\tkunmap_atomic(dst);\n\t\tlast_highmem_page = NULL;\n\t}\n}\n\nstatic inline int last_highmem_page_copied(void)\n{\n\treturn !last_highmem_page;\n}\n\nstatic inline void free_highmem_data(void)\n{\n\tif (safe_highmem_bm)\n\t\tmemory_bm_free(safe_highmem_bm, PG_UNSAFE_CLEAR);\n\n\tif (buffer)\n\t\tfree_image_page(buffer, PG_UNSAFE_CLEAR);\n}\n#else\nstatic unsigned int count_highmem_image_pages(struct memory_bitmap *bm) { return 0; }\n\nstatic inline int prepare_highmem_image(struct memory_bitmap *bm,\n\t\t\t\t\tunsigned int *nr_highmem_p) { return 0; }\n\nstatic inline void *get_highmem_page_buffer(struct page *page,\n\t\t\t\t\t    struct chain_allocator *ca)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic inline void copy_last_highmem_page(void) {}\nstatic inline int last_highmem_page_copied(void) { return 1; }\nstatic inline void free_highmem_data(void) {}\n#endif  \n\n#define PBES_PER_LINKED_PAGE\t(LINKED_PAGE_DATA_SIZE / sizeof(struct pbe))\n\n \nstatic int prepare_image(struct memory_bitmap *new_bm, struct memory_bitmap *bm,\n\t\tstruct memory_bitmap *zero_bm)\n{\n\tunsigned int nr_pages, nr_highmem;\n\tstruct memory_bitmap tmp;\n\tstruct linked_page *lp;\n\tint error;\n\n\t \n\tfree_image_page(buffer, PG_UNSAFE_CLEAR);\n\tbuffer = NULL;\n\n\tnr_highmem = count_highmem_image_pages(bm);\n\tmark_unsafe_pages(bm);\n\n\terror = memory_bm_create(new_bm, GFP_ATOMIC, PG_SAFE);\n\tif (error)\n\t\tgoto Free;\n\n\tduplicate_memory_bitmap(new_bm, bm);\n\tmemory_bm_free(bm, PG_UNSAFE_KEEP);\n\n\t \n\terror = memory_bm_create(&tmp, GFP_ATOMIC, PG_SAFE);\n\tif (error)\n\t\tgoto Free;\n\n\tduplicate_memory_bitmap(&tmp, zero_bm);\n\tmemory_bm_free(zero_bm, PG_UNSAFE_KEEP);\n\n\t \n\terror = memory_bm_create(zero_bm, GFP_ATOMIC, PG_SAFE);\n\tif (error)\n\t\tgoto Free;\n\n\tduplicate_memory_bitmap(zero_bm, &tmp);\n\tmemory_bm_free(&tmp, PG_UNSAFE_CLEAR);\n\t \n\n\tif (nr_highmem > 0) {\n\t\terror = prepare_highmem_image(bm, &nr_highmem);\n\t\tif (error)\n\t\t\tgoto Free;\n\t}\n\t \n\tnr_pages = (nr_zero_pages + nr_copy_pages) - nr_highmem - allocated_unsafe_pages;\n\tnr_pages = DIV_ROUND_UP(nr_pages, PBES_PER_LINKED_PAGE);\n\twhile (nr_pages > 0) {\n\t\tlp = get_image_page(GFP_ATOMIC, PG_SAFE);\n\t\tif (!lp) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto Free;\n\t\t}\n\t\tlp->next = safe_pages_list;\n\t\tsafe_pages_list = lp;\n\t\tnr_pages--;\n\t}\n\t \n\tnr_pages = (nr_zero_pages + nr_copy_pages) - nr_highmem - allocated_unsafe_pages;\n\twhile (nr_pages > 0) {\n\t\tlp = (struct linked_page *)get_zeroed_page(GFP_ATOMIC);\n\t\tif (!lp) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto Free;\n\t\t}\n\t\tif (!swsusp_page_is_free(virt_to_page(lp))) {\n\t\t\t \n\t\t\tlp->next = safe_pages_list;\n\t\t\tsafe_pages_list = lp;\n\t\t}\n\t\t \n\t\tswsusp_set_page_forbidden(virt_to_page(lp));\n\t\tswsusp_set_page_free(virt_to_page(lp));\n\t\tnr_pages--;\n\t}\n\treturn 0;\n\n Free:\n\tswsusp_free();\n\treturn error;\n}\n\n \nstatic void *get_buffer(struct memory_bitmap *bm, struct chain_allocator *ca)\n{\n\tstruct pbe *pbe;\n\tstruct page *page;\n\tunsigned long pfn = memory_bm_next_pfn(bm);\n\n\tif (pfn == BM_END_OF_MAP)\n\t\treturn ERR_PTR(-EFAULT);\n\n\tpage = pfn_to_page(pfn);\n\tif (PageHighMem(page))\n\t\treturn get_highmem_page_buffer(page, ca);\n\n\tif (swsusp_page_is_forbidden(page) && swsusp_page_is_free(page))\n\t\t \n\t\treturn page_address(page);\n\n\t \n\tpbe = chain_alloc(ca, sizeof(struct pbe));\n\tif (!pbe) {\n\t\tswsusp_free();\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tpbe->orig_address = page_address(page);\n\tpbe->address = __get_safe_page(ca->gfp_mask);\n\tif (!pbe->address)\n\t\treturn ERR_PTR(-ENOMEM);\n\tpbe->next = restore_pblist;\n\trestore_pblist = pbe;\n\treturn pbe->address;\n}\n\n \nint snapshot_write_next(struct snapshot_handle *handle)\n{\n\tstatic struct chain_allocator ca;\n\tint error = 0;\n\nnext:\n\t \n\tif (handle->cur > 1 && handle->cur > nr_meta_pages + nr_copy_pages + nr_zero_pages)\n\t\treturn 0;\n\n\tif (!handle->cur) {\n\t\tif (!buffer)\n\t\t\t \n\t\t\tbuffer = get_image_page(GFP_ATOMIC, PG_ANY);\n\n\t\tif (!buffer)\n\t\t\treturn -ENOMEM;\n\n\t\thandle->buffer = buffer;\n\t} else if (handle->cur == 1) {\n\t\terror = load_header(buffer);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\tsafe_pages_list = NULL;\n\n\t\terror = memory_bm_create(&copy_bm, GFP_ATOMIC, PG_ANY);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\terror = memory_bm_create(&zero_bm, GFP_ATOMIC, PG_ANY);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\tnr_zero_pages = 0;\n\n\t\thibernate_restore_protection_begin();\n\t} else if (handle->cur <= nr_meta_pages + 1) {\n\t\terror = unpack_orig_pfns(buffer, &copy_bm, &zero_bm);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\tif (handle->cur == nr_meta_pages + 1) {\n\t\t\terror = prepare_image(&orig_bm, &copy_bm, &zero_bm);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\n\t\t\tchain_init(&ca, GFP_ATOMIC, PG_SAFE);\n\t\t\tmemory_bm_position_reset(&orig_bm);\n\t\t\tmemory_bm_position_reset(&zero_bm);\n\t\t\trestore_pblist = NULL;\n\t\t\thandle->buffer = get_buffer(&orig_bm, &ca);\n\t\t\tif (IS_ERR(handle->buffer))\n\t\t\t\treturn PTR_ERR(handle->buffer);\n\t\t}\n\t} else {\n\t\tcopy_last_highmem_page();\n\t\thibernate_restore_protect_page(handle->buffer);\n\t\thandle->buffer = get_buffer(&orig_bm, &ca);\n\t\tif (IS_ERR(handle->buffer))\n\t\t\treturn PTR_ERR(handle->buffer);\n\t}\n\thandle->sync_read = (handle->buffer == buffer);\n\thandle->cur++;\n\n\t \n\tif (handle->cur > nr_meta_pages + 1 &&\n\t    memory_bm_test_bit(&zero_bm, memory_bm_get_current(&orig_bm))) {\n\t\tmemset(handle->buffer, 0, PAGE_SIZE);\n\t\tgoto next;\n\t}\n\n\treturn PAGE_SIZE;\n}\n\n \nvoid snapshot_write_finalize(struct snapshot_handle *handle)\n{\n\tcopy_last_highmem_page();\n\thibernate_restore_protect_page(handle->buffer);\n\t \n\tif (handle->cur > 1 && handle->cur > nr_meta_pages + nr_copy_pages + nr_zero_pages) {\n\t\tmemory_bm_recycle(&orig_bm);\n\t\tfree_highmem_data();\n\t}\n}\n\nint snapshot_image_loaded(struct snapshot_handle *handle)\n{\n\treturn !(!nr_copy_pages || !last_highmem_page_copied() ||\n\t\t\thandle->cur <= nr_meta_pages + nr_copy_pages + nr_zero_pages);\n}\n\n#ifdef CONFIG_HIGHMEM\n \nstatic inline void swap_two_pages_data(struct page *p1, struct page *p2,\n\t\t\t\t       void *buf)\n{\n\tvoid *kaddr1, *kaddr2;\n\n\tkaddr1 = kmap_atomic(p1);\n\tkaddr2 = kmap_atomic(p2);\n\tcopy_page(buf, kaddr1);\n\tcopy_page(kaddr1, kaddr2);\n\tcopy_page(kaddr2, buf);\n\tkunmap_atomic(kaddr2);\n\tkunmap_atomic(kaddr1);\n}\n\n \nint restore_highmem(void)\n{\n\tstruct highmem_pbe *pbe = highmem_pblist;\n\tvoid *buf;\n\n\tif (!pbe)\n\t\treturn 0;\n\n\tbuf = get_image_page(GFP_ATOMIC, PG_SAFE);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\twhile (pbe) {\n\t\tswap_two_pages_data(pbe->copy_page, pbe->orig_page, buf);\n\t\tpbe = pbe->next;\n\t}\n\tfree_image_page(buf, PG_UNSAFE_CLEAR);\n\treturn 0;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}