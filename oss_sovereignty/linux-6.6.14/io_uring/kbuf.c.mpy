{
  "module_name": "kbuf.c",
  "hash_id": "76c5e80a906031afdf27dfcaeea1ce5ace8440aaddaaa68b222aba1436080b29",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/kbuf.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/namei.h>\n#include <linux/poll.h>\n#include <linux/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"opdef.h\"\n#include \"kbuf.h\"\n\n#define IO_BUFFER_LIST_BUF_PER_PAGE (PAGE_SIZE / sizeof(struct io_uring_buf))\n\n#define BGID_ARRAY\t64\n\n \n#define MAX_BIDS_PER_BGID (1 << 16)\n\nstruct io_provide_buf {\n\tstruct file\t\t\t*file;\n\t__u64\t\t\t\taddr;\n\t__u32\t\t\t\tlen;\n\t__u32\t\t\t\tbgid;\n\t__u32\t\t\t\tnbufs;\n\t__u16\t\t\t\tbid;\n};\n\nstatic struct io_buffer_list *__io_buffer_get_list(struct io_ring_ctx *ctx,\n\t\t\t\t\t\t   struct io_buffer_list *bl,\n\t\t\t\t\t\t   unsigned int bgid)\n{\n\tif (bl && bgid < BGID_ARRAY)\n\t\treturn &bl[bgid];\n\n\treturn xa_load(&ctx->io_bl_xa, bgid);\n}\n\nstruct io_buf_free {\n\tstruct hlist_node\t\tlist;\n\tvoid\t\t\t\t*mem;\n\tsize_t\t\t\t\tsize;\n\tint\t\t\t\tinuse;\n};\n\nstatic inline struct io_buffer_list *io_buffer_get_list(struct io_ring_ctx *ctx,\n\t\t\t\t\t\t\tunsigned int bgid)\n{\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\treturn __io_buffer_get_list(ctx, ctx->io_bl, bgid);\n}\n\nstatic int io_buffer_add_list(struct io_ring_ctx *ctx,\n\t\t\t      struct io_buffer_list *bl, unsigned int bgid)\n{\n\t \n\tbl->bgid = bgid;\n\tsmp_store_release(&bl->is_ready, 1);\n\n\tif (bgid < BGID_ARRAY)\n\t\treturn 0;\n\n\treturn xa_err(xa_store(&ctx->io_bl_xa, bgid, bl, GFP_KERNEL));\n}\n\nvoid io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer_list *bl;\n\tstruct io_buffer *buf;\n\n\t \n\tif (req->flags & REQ_F_PARTIAL_IO)\n\t\treturn;\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\n\tbuf = req->kbuf;\n\tbl = io_buffer_get_list(ctx, buf->bgid);\n\tlist_add(&buf->list, &bl->buf_list);\n\treq->flags &= ~REQ_F_BUFFER_SELECTED;\n\treq->buf_index = buf->bgid;\n\n\tio_ring_submit_unlock(ctx, issue_flags);\n\treturn;\n}\n\nunsigned int __io_put_kbuf(struct io_kiocb *req, unsigned issue_flags)\n{\n\tunsigned int cflags;\n\n\t \n\tif (req->flags & REQ_F_BUFFER_RING) {\n\t\t \n\t\tcflags = __io_put_kbuf_list(req, NULL);\n\t} else if (issue_flags & IO_URING_F_UNLOCKED) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\tspin_lock(&ctx->completion_lock);\n\t\tcflags = __io_put_kbuf_list(req, &ctx->io_buffers_comp);\n\t\tspin_unlock(&ctx->completion_lock);\n\t} else {\n\t\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\t\tcflags = __io_put_kbuf_list(req, &req->ctx->io_buffers_cache);\n\t}\n\treturn cflags;\n}\n\nstatic void __user *io_provided_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\t      struct io_buffer_list *bl)\n{\n\tif (!list_empty(&bl->buf_list)) {\n\t\tstruct io_buffer *kbuf;\n\n\t\tkbuf = list_first_entry(&bl->buf_list, struct io_buffer, list);\n\t\tlist_del(&kbuf->list);\n\t\tif (*len == 0 || *len > kbuf->len)\n\t\t\t*len = kbuf->len;\n\t\treq->flags |= REQ_F_BUFFER_SELECTED;\n\t\treq->kbuf = kbuf;\n\t\treq->buf_index = kbuf->bid;\n\t\treturn u64_to_user_ptr(kbuf->addr);\n\t}\n\treturn NULL;\n}\n\nstatic void __user *io_ring_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t\t\t  struct io_buffer_list *bl,\n\t\t\t\t\t  unsigned int issue_flags)\n{\n\tstruct io_uring_buf_ring *br = bl->buf_ring;\n\tstruct io_uring_buf *buf;\n\t__u16 head = bl->head;\n\n\tif (unlikely(smp_load_acquire(&br->tail) == head))\n\t\treturn NULL;\n\n\thead &= bl->mask;\n\t \n\tif (bl->is_mmap || head < IO_BUFFER_LIST_BUF_PER_PAGE) {\n\t\tbuf = &br->bufs[head];\n\t} else {\n\t\tint off = head & (IO_BUFFER_LIST_BUF_PER_PAGE - 1);\n\t\tint index = head / IO_BUFFER_LIST_BUF_PER_PAGE;\n\t\tbuf = page_address(bl->buf_pages[index]);\n\t\tbuf += off;\n\t}\n\tif (*len == 0 || *len > buf->len)\n\t\t*len = buf->len;\n\treq->flags |= REQ_F_BUFFER_RING;\n\treq->buf_list = bl;\n\treq->buf_index = buf->bid;\n\n\tif (issue_flags & IO_URING_F_UNLOCKED || !file_can_poll(req->file)) {\n\t\t \n\t\treq->buf_list = NULL;\n\t\tbl->head++;\n\t}\n\treturn u64_to_user_ptr(buf->addr);\n}\n\nvoid __user *io_buffer_select(struct io_kiocb *req, size_t *len,\n\t\t\t      unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer_list *bl;\n\tvoid __user *ret = NULL;\n\n\tio_ring_submit_lock(req->ctx, issue_flags);\n\n\tbl = io_buffer_get_list(ctx, req->buf_index);\n\tif (likely(bl)) {\n\t\tif (bl->is_mapped)\n\t\t\tret = io_ring_buffer_select(req, len, bl, issue_flags);\n\t\telse\n\t\t\tret = io_provided_buffer_select(req, len, bl);\n\t}\n\tio_ring_submit_unlock(req->ctx, issue_flags);\n\treturn ret;\n}\n\nstatic __cold int io_init_bl_list(struct io_ring_ctx *ctx)\n{\n\tstruct io_buffer_list *bl;\n\tint i;\n\n\tbl = kcalloc(BGID_ARRAY, sizeof(struct io_buffer_list), GFP_KERNEL);\n\tif (!bl)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < BGID_ARRAY; i++) {\n\t\tINIT_LIST_HEAD(&bl[i].buf_list);\n\t\tbl[i].bgid = i;\n\t}\n\n\tsmp_store_release(&ctx->io_bl, bl);\n\treturn 0;\n}\n\n \nstatic void io_kbuf_mark_free(struct io_ring_ctx *ctx, struct io_buffer_list *bl)\n{\n\tstruct io_buf_free *ibf;\n\n\thlist_for_each_entry(ibf, &ctx->io_buf_list, list) {\n\t\tif (bl->buf_ring == ibf->mem) {\n\t\t\tibf->inuse = 0;\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tWARN_ON_ONCE(1);\n}\n\nstatic int __io_remove_buffers(struct io_ring_ctx *ctx,\n\t\t\t       struct io_buffer_list *bl, unsigned nbufs)\n{\n\tunsigned i = 0;\n\n\t \n\tif (!nbufs)\n\t\treturn 0;\n\n\tif (bl->is_mapped) {\n\t\ti = bl->buf_ring->tail - bl->head;\n\t\tif (bl->is_mmap) {\n\t\t\t \n\t\t\tio_kbuf_mark_free(ctx, bl);\n\t\t\tbl->buf_ring = NULL;\n\t\t\tbl->is_mmap = 0;\n\t\t} else if (bl->buf_nr_pages) {\n\t\t\tint j;\n\n\t\t\tfor (j = 0; j < bl->buf_nr_pages; j++)\n\t\t\t\tunpin_user_page(bl->buf_pages[j]);\n\t\t\tkvfree(bl->buf_pages);\n\t\t\tbl->buf_pages = NULL;\n\t\t\tbl->buf_nr_pages = 0;\n\t\t}\n\t\t \n\t\tINIT_LIST_HEAD(&bl->buf_list);\n\t\tbl->is_mapped = 0;\n\t\treturn i;\n\t}\n\n\t \n\tlockdep_assert_held(&ctx->uring_lock);\n\n\twhile (!list_empty(&bl->buf_list)) {\n\t\tstruct io_buffer *nxt;\n\n\t\tnxt = list_first_entry(&bl->buf_list, struct io_buffer, list);\n\t\tlist_move(&nxt->list, &ctx->io_buffers_cache);\n\t\tif (++i == nbufs)\n\t\t\treturn i;\n\t\tcond_resched();\n\t}\n\n\treturn i;\n}\n\nvoid io_destroy_buffers(struct io_ring_ctx *ctx)\n{\n\tstruct io_buffer_list *bl;\n\tunsigned long index;\n\tint i;\n\n\tfor (i = 0; i < BGID_ARRAY; i++) {\n\t\tif (!ctx->io_bl)\n\t\t\tbreak;\n\t\t__io_remove_buffers(ctx, &ctx->io_bl[i], -1U);\n\t}\n\n\txa_for_each(&ctx->io_bl_xa, index, bl) {\n\t\txa_erase(&ctx->io_bl_xa, bl->bgid);\n\t\t__io_remove_buffers(ctx, bl, -1U);\n\t\tkfree_rcu(bl, rcu);\n\t}\n\n\twhile (!list_empty(&ctx->io_buffers_pages)) {\n\t\tstruct page *page;\n\n\t\tpage = list_first_entry(&ctx->io_buffers_pages, struct page, lru);\n\t\tlist_del_init(&page->lru);\n\t\t__free_page(page);\n\t}\n}\n\nint io_remove_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);\n\tu64 tmp;\n\n\tif (sqe->rw_flags || sqe->addr || sqe->len || sqe->off ||\n\t    sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > MAX_BIDS_PER_BGID)\n\t\treturn -EINVAL;\n\n\tmemset(p, 0, sizeof(*p));\n\tp->nbufs = tmp;\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\treturn 0;\n}\n\nint io_remove_buffers(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer_list *bl;\n\tint ret = 0;\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\n\tret = -ENOENT;\n\tbl = io_buffer_get_list(ctx, p->bgid);\n\tif (bl) {\n\t\tret = -EINVAL;\n\t\t \n\t\tif (!bl->is_mapped)\n\t\t\tret = __io_remove_buffers(ctx, bl, p->nbufs);\n\t}\n\tio_ring_submit_unlock(ctx, issue_flags);\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}\n\nint io_provide_buffers_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tunsigned long size, tmp_check;\n\tstruct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);\n\tu64 tmp;\n\n\tif (sqe->rw_flags || sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\n\ttmp = READ_ONCE(sqe->fd);\n\tif (!tmp || tmp > MAX_BIDS_PER_BGID)\n\t\treturn -E2BIG;\n\tp->nbufs = tmp;\n\tp->addr = READ_ONCE(sqe->addr);\n\tp->len = READ_ONCE(sqe->len);\n\n\tif (check_mul_overflow((unsigned long)p->len, (unsigned long)p->nbufs,\n\t\t\t\t&size))\n\t\treturn -EOVERFLOW;\n\tif (check_add_overflow((unsigned long)p->addr, size, &tmp_check))\n\t\treturn -EOVERFLOW;\n\n\tsize = (unsigned long)p->len * p->nbufs;\n\tif (!access_ok(u64_to_user_ptr(p->addr), size))\n\t\treturn -EFAULT;\n\n\tp->bgid = READ_ONCE(sqe->buf_group);\n\ttmp = READ_ONCE(sqe->off);\n\tif (tmp > USHRT_MAX)\n\t\treturn -E2BIG;\n\tif (tmp + p->nbufs > MAX_BIDS_PER_BGID)\n\t\treturn -EINVAL;\n\tp->bid = tmp;\n\treturn 0;\n}\n\nstatic int io_refill_buffer_cache(struct io_ring_ctx *ctx)\n{\n\tstruct io_buffer *buf;\n\tstruct page *page;\n\tint bufs_in_page;\n\n\t \n\tif (!list_empty_careful(&ctx->io_buffers_comp)) {\n\t\tspin_lock(&ctx->completion_lock);\n\t\tif (!list_empty(&ctx->io_buffers_comp)) {\n\t\t\tlist_splice_init(&ctx->io_buffers_comp,\n\t\t\t\t\t\t&ctx->io_buffers_cache);\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tspin_unlock(&ctx->completion_lock);\n\t}\n\n\t \n\tpage = alloc_page(GFP_KERNEL_ACCOUNT);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tlist_add(&page->lru, &ctx->io_buffers_pages);\n\n\tbuf = page_address(page);\n\tbufs_in_page = PAGE_SIZE / sizeof(*buf);\n\twhile (bufs_in_page) {\n\t\tlist_add_tail(&buf->list, &ctx->io_buffers_cache);\n\t\tbuf++;\n\t\tbufs_in_page--;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_add_buffers(struct io_ring_ctx *ctx, struct io_provide_buf *pbuf,\n\t\t\t  struct io_buffer_list *bl)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tif (list_empty(&ctx->io_buffers_cache) &&\n\t\t    io_refill_buffer_cache(ctx))\n\t\t\tbreak;\n\t\tbuf = list_first_entry(&ctx->io_buffers_cache, struct io_buffer,\n\t\t\t\t\tlist);\n\t\tlist_move_tail(&buf->list, &bl->buf_list);\n\t\tbuf->addr = addr;\n\t\tbuf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);\n\t\tbuf->bid = bid;\n\t\tbuf->bgid = pbuf->bgid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tcond_resched();\n\t}\n\n\treturn i ? 0 : -ENOMEM;\n}\n\nint io_provide_buffers(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_provide_buf *p = io_kiocb_to_cmd(req, struct io_provide_buf);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_buffer_list *bl;\n\tint ret = 0;\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\n\tif (unlikely(p->bgid < BGID_ARRAY && !ctx->io_bl)) {\n\t\tret = io_init_bl_list(ctx);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tbl = io_buffer_get_list(ctx, p->bgid);\n\tif (unlikely(!bl)) {\n\t\tbl = kzalloc(sizeof(*bl), GFP_KERNEL_ACCOUNT);\n\t\tif (!bl) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\t\tINIT_LIST_HEAD(&bl->buf_list);\n\t\tret = io_buffer_add_list(ctx, bl, p->bgid);\n\t\tif (ret) {\n\t\t\t \n\t\t\tif (p->bgid >= BGID_ARRAY)\n\t\t\t\tkfree_rcu(bl, rcu);\n\t\t\telse\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\tgoto err;\n\t\t}\n\t}\n\t \n\tif (bl->is_mapped) {\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tret = io_add_buffers(ctx, p, bl);\nerr:\n\tio_ring_submit_unlock(ctx, issue_flags);\n\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}\n\nstatic int io_pin_pbuf_ring(struct io_uring_buf_reg *reg,\n\t\t\t    struct io_buffer_list *bl)\n{\n\tstruct io_uring_buf_ring *br;\n\tstruct page **pages;\n\tint i, nr_pages;\n\n\tpages = io_pin_pages(reg->ring_addr,\n\t\t\t     flex_array_size(br, bufs, reg->ring_entries),\n\t\t\t     &nr_pages);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\t \n\tfor (i = 0; i < nr_pages; i++)\n\t\tif (PageHighMem(pages[i]))\n\t\t\tgoto error_unpin;\n\n\tbr = page_address(pages[0]);\n#ifdef SHM_COLOUR\n\t \n\tif ((reg->ring_addr | (unsigned long) br) & (SHM_COLOUR - 1))\n\t\tgoto error_unpin;\n#endif\n\tbl->buf_pages = pages;\n\tbl->buf_nr_pages = nr_pages;\n\tbl->buf_ring = br;\n\tbl->is_mapped = 1;\n\tbl->is_mmap = 0;\n\treturn 0;\nerror_unpin:\n\tfor (i = 0; i < nr_pages; i++)\n\t\tunpin_user_page(pages[i]);\n\tkvfree(pages);\n\treturn -EINVAL;\n}\n\n \nstatic struct io_buf_free *io_lookup_buf_free_entry(struct io_ring_ctx *ctx,\n\t\t\t\t\t\t    size_t ring_size)\n{\n\tstruct io_buf_free *ibf, *best = NULL;\n\tsize_t best_dist;\n\n\thlist_for_each_entry(ibf, &ctx->io_buf_list, list) {\n\t\tsize_t dist;\n\n\t\tif (ibf->inuse || ibf->size < ring_size)\n\t\t\tcontinue;\n\t\tdist = ibf->size - ring_size;\n\t\tif (!best || dist < best_dist) {\n\t\t\tbest = ibf;\n\t\t\tif (!dist)\n\t\t\t\tbreak;\n\t\t\tbest_dist = dist;\n\t\t}\n\t}\n\n\treturn best;\n}\n\nstatic int io_alloc_pbuf_ring(struct io_ring_ctx *ctx,\n\t\t\t      struct io_uring_buf_reg *reg,\n\t\t\t      struct io_buffer_list *bl)\n{\n\tstruct io_buf_free *ibf;\n\tsize_t ring_size;\n\tvoid *ptr;\n\n\tring_size = reg->ring_entries * sizeof(struct io_uring_buf_ring);\n\n\t \n\tibf = io_lookup_buf_free_entry(ctx, ring_size);\n\tif (!ibf) {\n\t\tptr = io_mem_alloc(ring_size);\n\t\tif (IS_ERR(ptr))\n\t\t\treturn PTR_ERR(ptr);\n\n\t\t \n\t\tibf = kmalloc(sizeof(*ibf), GFP_KERNEL_ACCOUNT);\n\t\tif (!ibf) {\n\t\t\tio_mem_free(ptr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tibf->mem = ptr;\n\t\tibf->size = ring_size;\n\t\thlist_add_head(&ibf->list, &ctx->io_buf_list);\n\t}\n\tibf->inuse = 1;\n\tbl->buf_ring = ibf->mem;\n\tbl->is_mapped = 1;\n\tbl->is_mmap = 1;\n\treturn 0;\n}\n\nint io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n{\n\tstruct io_uring_buf_reg reg;\n\tstruct io_buffer_list *bl, *free_bl = NULL;\n\tint ret;\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tif (copy_from_user(&reg, arg, sizeof(reg)))\n\t\treturn -EFAULT;\n\n\tif (reg.resv[0] || reg.resv[1] || reg.resv[2])\n\t\treturn -EINVAL;\n\tif (reg.flags & ~IOU_PBUF_RING_MMAP)\n\t\treturn -EINVAL;\n\tif (!(reg.flags & IOU_PBUF_RING_MMAP)) {\n\t\tif (!reg.ring_addr)\n\t\t\treturn -EFAULT;\n\t\tif (reg.ring_addr & ~PAGE_MASK)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (reg.ring_addr)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!is_power_of_2(reg.ring_entries))\n\t\treturn -EINVAL;\n\n\t \n\tif (reg.ring_entries >= 65536)\n\t\treturn -EINVAL;\n\n\tif (unlikely(reg.bgid < BGID_ARRAY && !ctx->io_bl)) {\n\t\tint ret = io_init_bl_list(ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbl = io_buffer_get_list(ctx, reg.bgid);\n\tif (bl) {\n\t\t \n\t\tif (bl->is_mapped || !list_empty(&bl->buf_list))\n\t\t\treturn -EEXIST;\n\t} else {\n\t\tfree_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL);\n\t\tif (!bl)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (!(reg.flags & IOU_PBUF_RING_MMAP))\n\t\tret = io_pin_pbuf_ring(&reg, bl);\n\telse\n\t\tret = io_alloc_pbuf_ring(ctx, &reg, bl);\n\n\tif (!ret) {\n\t\tbl->nr_entries = reg.ring_entries;\n\t\tbl->mask = reg.ring_entries - 1;\n\n\t\tio_buffer_add_list(ctx, bl, reg.bgid);\n\t\treturn 0;\n\t}\n\n\tkfree_rcu(free_bl, rcu);\n\treturn ret;\n}\n\nint io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg)\n{\n\tstruct io_uring_buf_reg reg;\n\tstruct io_buffer_list *bl;\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tif (copy_from_user(&reg, arg, sizeof(reg)))\n\t\treturn -EFAULT;\n\tif (reg.resv[0] || reg.resv[1] || reg.resv[2])\n\t\treturn -EINVAL;\n\tif (reg.flags)\n\t\treturn -EINVAL;\n\n\tbl = io_buffer_get_list(ctx, reg.bgid);\n\tif (!bl)\n\t\treturn -ENOENT;\n\tif (!bl->is_mapped)\n\t\treturn -EINVAL;\n\n\t__io_remove_buffers(ctx, bl, -1U);\n\tif (bl->bgid >= BGID_ARRAY) {\n\t\txa_erase(&ctx->io_bl_xa, bl->bgid);\n\t\tkfree_rcu(bl, rcu);\n\t}\n\treturn 0;\n}\n\nvoid *io_pbuf_get_address(struct io_ring_ctx *ctx, unsigned long bgid)\n{\n\tstruct io_buffer_list *bl;\n\n\tbl = __io_buffer_get_list(ctx, smp_load_acquire(&ctx->io_bl), bgid);\n\n\tif (!bl || !bl->is_mmap)\n\t\treturn NULL;\n\t \n\tif (!smp_load_acquire(&bl->is_ready))\n\t\treturn NULL;\n\n\treturn bl->buf_ring;\n}\n\n \nvoid io_kbuf_mmap_list_free(struct io_ring_ctx *ctx)\n{\n\tstruct io_buf_free *ibf;\n\tstruct hlist_node *tmp;\n\n\thlist_for_each_entry_safe(ibf, tmp, &ctx->io_buf_list, list) {\n\t\thlist_del(&ibf->list);\n\t\tio_mem_free(ibf->mem);\n\t\tkfree(ibf);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}