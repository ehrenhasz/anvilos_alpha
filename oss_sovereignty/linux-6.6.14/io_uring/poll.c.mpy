{
  "module_name": "poll.c",
  "hash_id": "37e146d920bb755d17caec7032b33f3f7bda456811dd37ff4970896cd8fe4add",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/poll.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/hashtable.h>\n#include <linux/io_uring.h>\n\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"refs.h\"\n#include \"opdef.h\"\n#include \"kbuf.h\"\n#include \"poll.h\"\n#include \"cancel.h\"\n\nstruct io_poll_update {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\told_user_data;\n\tu64\t\t\t\tnew_user_data;\n\t__poll_t\t\t\tevents;\n\tbool\t\t\t\tupdate_events;\n\tbool\t\t\t\tupdate_user_data;\n};\n\nstruct io_poll_table {\n\tstruct poll_table_struct pt;\n\tstruct io_kiocb *req;\n\tint nr_entries;\n\tint error;\n\tbool owning;\n\t \n\t__poll_t result_mask;\n};\n\n#define IO_POLL_CANCEL_FLAG\tBIT(31)\n#define IO_POLL_RETRY_FLAG\tBIT(30)\n#define IO_POLL_REF_MASK\tGENMASK(29, 0)\n\n \n#define IO_POLL_REF_BIAS\t128\n\n#define IO_WQE_F_DOUBLE\t\t1\n\nstatic int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key);\n\nstatic inline struct io_kiocb *wqe_to_req(struct wait_queue_entry *wqe)\n{\n\tunsigned long priv = (unsigned long)wqe->private;\n\n\treturn (struct io_kiocb *)(priv & ~IO_WQE_F_DOUBLE);\n}\n\nstatic inline bool wqe_is_double(struct wait_queue_entry *wqe)\n{\n\tunsigned long priv = (unsigned long)wqe->private;\n\n\treturn priv & IO_WQE_F_DOUBLE;\n}\n\nstatic bool io_poll_get_ownership_slowpath(struct io_kiocb *req)\n{\n\tint v;\n\n\t \n\tv = atomic_fetch_or(IO_POLL_RETRY_FLAG, &req->poll_refs);\n\tif (v & IO_POLL_REF_MASK)\n\t\treturn false;\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}\n\n \nstatic inline bool io_poll_get_ownership(struct io_kiocb *req)\n{\n\tif (unlikely(atomic_read(&req->poll_refs) >= IO_POLL_REF_BIAS))\n\t\treturn io_poll_get_ownership_slowpath(req);\n\treturn !(atomic_fetch_inc(&req->poll_refs) & IO_POLL_REF_MASK);\n}\n\nstatic void io_poll_mark_cancelled(struct io_kiocb *req)\n{\n\tatomic_or(IO_POLL_CANCEL_FLAG, &req->poll_refs);\n}\n\nstatic struct io_poll *io_poll_get_double(struct io_kiocb *req)\n{\n\t \n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn req->async_data;\n\treturn req->apoll->double_poll;\n}\n\nstatic struct io_poll *io_poll_get_single(struct io_kiocb *req)\n{\n\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\treturn io_kiocb_to_cmd(req, struct io_poll);\n\treturn &req->apoll->poll;\n}\n\nstatic void io_poll_req_insert(struct io_kiocb *req)\n{\n\tstruct io_hash_table *table = &req->ctx->cancel_table;\n\tu32 index = hash_long(req->cqe.user_data, table->hash_bits);\n\tstruct io_hash_bucket *hb = &table->hbs[index];\n\n\tspin_lock(&hb->lock);\n\thlist_add_head(&req->hash_node, &hb->list);\n\tspin_unlock(&hb->lock);\n}\n\nstatic void io_poll_req_delete(struct io_kiocb *req, struct io_ring_ctx *ctx)\n{\n\tstruct io_hash_table *table = &req->ctx->cancel_table;\n\tu32 index = hash_long(req->cqe.user_data, table->hash_bits);\n\tspinlock_t *lock = &table->hbs[index].lock;\n\n\tspin_lock(lock);\n\thash_del(&req->hash_node);\n\tspin_unlock(lock);\n}\n\nstatic void io_poll_req_insert_locked(struct io_kiocb *req)\n{\n\tstruct io_hash_table *table = &req->ctx->cancel_table_locked;\n\tu32 index = hash_long(req->cqe.user_data, table->hash_bits);\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\thlist_add_head(&req->hash_node, &table->hbs[index].list);\n}\n\nstatic void io_poll_tw_hash_eject(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (req->flags & REQ_F_HASH_LOCKED) {\n\t\t \n\t\tio_tw_lock(ctx, ts);\n\t\thash_del(&req->hash_node);\n\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n\t} else {\n\t\tio_poll_req_delete(req, ctx);\n\t}\n}\n\nstatic void io_init_poll_iocb(struct io_poll *poll, __poll_t events)\n{\n\tpoll->head = NULL;\n#define IO_POLL_UNMASK\t(EPOLLERR|EPOLLHUP|EPOLLNVAL|EPOLLRDHUP)\n\t \n\tpoll->events = events | IO_POLL_UNMASK;\n\tINIT_LIST_HEAD(&poll->wait.entry);\n\tinit_waitqueue_func_entry(&poll->wait, io_poll_wake);\n}\n\nstatic inline void io_poll_remove_entry(struct io_poll *poll)\n{\n\tstruct wait_queue_head *head = smp_load_acquire(&poll->head);\n\n\tif (head) {\n\t\tspin_lock_irq(&head->lock);\n\t\tlist_del_init(&poll->wait.entry);\n\t\tpoll->head = NULL;\n\t\tspin_unlock_irq(&head->lock);\n\t}\n}\n\nstatic void io_poll_remove_entries(struct io_kiocb *req)\n{\n\t \n\tif (!(req->flags & (REQ_F_SINGLE_POLL | REQ_F_DOUBLE_POLL)))\n\t\treturn;\n\n\t \n\trcu_read_lock();\n\tif (req->flags & REQ_F_SINGLE_POLL)\n\t\tio_poll_remove_entry(io_poll_get_single(req));\n\tif (req->flags & REQ_F_DOUBLE_POLL)\n\t\tio_poll_remove_entry(io_poll_get_double(req));\n\trcu_read_unlock();\n}\n\nenum {\n\tIOU_POLL_DONE = 0,\n\tIOU_POLL_NO_ACTION = 1,\n\tIOU_POLL_REMOVE_POLL_USE_RES = 2,\n\tIOU_POLL_REISSUE = 3,\n};\n\n \nstatic int io_poll_check_events(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tint v;\n\n\t \n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\treturn -ECANCELED;\n\n\tdo {\n\t\tv = atomic_read(&req->poll_refs);\n\n\t\tif (unlikely(v != 1)) {\n\t\t\t \n\t\t\tif (WARN_ON_ONCE(!(v & IO_POLL_REF_MASK)))\n\t\t\t\treturn IOU_POLL_NO_ACTION;\n\t\t\tif (v & IO_POLL_CANCEL_FLAG)\n\t\t\t\treturn -ECANCELED;\n\t\t\t \n\t\t\tif ((v & IO_POLL_REF_MASK) != 1)\n\t\t\t\treq->cqe.res = 0;\n\n\t\t\tif (v & IO_POLL_RETRY_FLAG) {\n\t\t\t\treq->cqe.res = 0;\n\t\t\t\t \n\t\t\t\tatomic_andnot(IO_POLL_RETRY_FLAG, &req->poll_refs);\n\t\t\t\tv &= ~IO_POLL_RETRY_FLAG;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!req->cqe.res) {\n\t\t\tstruct poll_table_struct pt = { ._key = req->apoll_events };\n\t\t\treq->cqe.res = vfs_poll(req->file, &pt) & req->apoll_events;\n\t\t\t \n\t\t\tif (unlikely(!req->cqe.res)) {\n\t\t\t\t \n\t\t\t\tif (!(req->apoll_events & EPOLLONESHOT))\n\t\t\t\t\tcontinue;\n\t\t\t\treturn IOU_POLL_REISSUE;\n\t\t\t}\n\t\t}\n\t\tif (req->apoll_events & EPOLLONESHOT)\n\t\t\treturn IOU_POLL_DONE;\n\n\t\t \n\t\tif (!(req->flags & REQ_F_APOLL_MULTISHOT)) {\n\t\t\t__poll_t mask = mangle_poll(req->cqe.res &\n\t\t\t\t\t\t    req->apoll_events);\n\n\t\t\tif (!io_fill_cqe_req_aux(req, ts->locked, mask,\n\t\t\t\t\t\t IORING_CQE_F_MORE)) {\n\t\t\t\tio_req_set_res(req, mask, 0);\n\t\t\t\treturn IOU_POLL_REMOVE_POLL_USE_RES;\n\t\t\t}\n\t\t} else {\n\t\t\tint ret = io_poll_issue(req, ts);\n\t\t\tif (ret == IOU_STOP_MULTISHOT)\n\t\t\t\treturn IOU_POLL_REMOVE_POLL_USE_RES;\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\n\t\t \n\t\treq->cqe.res = 0;\n\n\t\t \n\t} while (atomic_sub_return(v & IO_POLL_REF_MASK, &req->poll_refs) &\n\t\t\t\t\tIO_POLL_REF_MASK);\n\n\treturn IOU_POLL_NO_ACTION;\n}\n\nvoid io_poll_task_func(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tint ret;\n\n\tret = io_poll_check_events(req, ts);\n\tif (ret == IOU_POLL_NO_ACTION)\n\t\treturn;\n\tio_poll_remove_entries(req);\n\tio_poll_tw_hash_eject(req, ts);\n\n\tif (req->opcode == IORING_OP_POLL_ADD) {\n\t\tif (ret == IOU_POLL_DONE) {\n\t\t\tstruct io_poll *poll;\n\n\t\t\tpoll = io_kiocb_to_cmd(req, struct io_poll);\n\t\t\treq->cqe.res = mangle_poll(req->cqe.res & poll->events);\n\t\t} else if (ret == IOU_POLL_REISSUE) {\n\t\t\tio_req_task_submit(req, ts);\n\t\t\treturn;\n\t\t} else if (ret != IOU_POLL_REMOVE_POLL_USE_RES) {\n\t\t\treq->cqe.res = ret;\n\t\t\treq_set_fail(req);\n\t\t}\n\n\t\tio_req_set_res(req, req->cqe.res, 0);\n\t\tio_req_task_complete(req, ts);\n\t} else {\n\t\tio_tw_lock(req->ctx, ts);\n\n\t\tif (ret == IOU_POLL_REMOVE_POLL_USE_RES)\n\t\t\tio_req_task_complete(req, ts);\n\t\telse if (ret == IOU_POLL_DONE || ret == IOU_POLL_REISSUE)\n\t\t\tio_req_task_submit(req, ts);\n\t\telse\n\t\t\tio_req_defer_failed(req, ret);\n\t}\n}\n\nstatic void __io_poll_execute(struct io_kiocb *req, int mask)\n{\n\tio_req_set_res(req, mask, 0);\n\treq->io_task_work.func = io_poll_task_func;\n\n\ttrace_io_uring_task_add(req, mask);\n\tio_req_task_work_add(req);\n}\n\nstatic inline void io_poll_execute(struct io_kiocb *req, int res)\n{\n\tif (io_poll_get_ownership(req))\n\t\t__io_poll_execute(req, res);\n}\n\nstatic void io_poll_cancel_req(struct io_kiocb *req)\n{\n\tio_poll_mark_cancelled(req);\n\t \n\tio_poll_execute(req, 0);\n}\n\n#define IO_ASYNC_POLL_COMMON\t(EPOLLONESHOT | EPOLLPRI)\n\nstatic __cold int io_pollfree_wake(struct io_kiocb *req, struct io_poll *poll)\n{\n\tio_poll_mark_cancelled(req);\n\t \n\tio_poll_execute(req, 0);\n\n\t \n\tlist_del_init(&poll->wait.entry);\n\n\t \n\tsmp_store_release(&poll->head, NULL);\n\treturn 1;\n}\n\nstatic int io_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\t\tvoid *key)\n{\n\tstruct io_kiocb *req = wqe_to_req(wait);\n\tstruct io_poll *poll = container_of(wait, struct io_poll, wait);\n\t__poll_t mask = key_to_poll(key);\n\n\tif (unlikely(mask & POLLFREE))\n\t\treturn io_pollfree_wake(req, poll);\n\n\t \n\tif (mask && !(mask & (poll->events & ~IO_ASYNC_POLL_COMMON)))\n\t\treturn 0;\n\n\tif (io_poll_get_ownership(req)) {\n\t\t \n\t\tif (mask & EPOLL_URING_WAKE)\n\t\t\tpoll->events |= EPOLLONESHOT;\n\n\t\t \n\t\tif (mask && poll->events & EPOLLONESHOT) {\n\t\t\tlist_del_init(&poll->wait.entry);\n\t\t\tpoll->head = NULL;\n\t\t\tif (wqe_is_double(wait))\n\t\t\t\treq->flags &= ~REQ_F_DOUBLE_POLL;\n\t\t\telse\n\t\t\t\treq->flags &= ~REQ_F_SINGLE_POLL;\n\t\t}\n\t\t__io_poll_execute(req, mask);\n\t}\n\treturn 1;\n}\n\n \nstatic bool io_poll_double_prepare(struct io_kiocb *req)\n{\n\tstruct wait_queue_head *head;\n\tstruct io_poll *poll = io_poll_get_single(req);\n\n\t \n\trcu_read_lock();\n\thead = smp_load_acquire(&poll->head);\n\t \n\tif (head) {\n\t\tspin_lock_irq(&head->lock);\n\t\treq->flags |= REQ_F_DOUBLE_POLL;\n\t\tif (req->opcode == IORING_OP_POLL_ADD)\n\t\t\treq->flags |= REQ_F_ASYNC_DATA;\n\t\tspin_unlock_irq(&head->lock);\n\t}\n\trcu_read_unlock();\n\treturn !!head;\n}\n\nstatic void __io_queue_proc(struct io_poll *poll, struct io_poll_table *pt,\n\t\t\t    struct wait_queue_head *head,\n\t\t\t    struct io_poll **poll_ptr)\n{\n\tstruct io_kiocb *req = pt->req;\n\tunsigned long wqe_private = (unsigned long) req;\n\n\t \n\tif (unlikely(pt->nr_entries)) {\n\t\tstruct io_poll *first = poll;\n\n\t\t \n\t\tif (first->head == head)\n\t\t\treturn;\n\t\t \n\t\tif (*poll_ptr) {\n\t\t\tif ((*poll_ptr)->head == head)\n\t\t\t\treturn;\n\t\t\tpt->error = -EINVAL;\n\t\t\treturn;\n\t\t}\n\n\t\tpoll = kmalloc(sizeof(*poll), GFP_ATOMIC);\n\t\tif (!poll) {\n\t\t\tpt->error = -ENOMEM;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\twqe_private |= IO_WQE_F_DOUBLE;\n\t\tio_init_poll_iocb(poll, first->events);\n\t\tif (!io_poll_double_prepare(req)) {\n\t\t\t \n\t\t\tkfree(poll);\n\t\t\treturn;\n\t\t}\n\t\t*poll_ptr = poll;\n\t} else {\n\t\t \n\t\treq->flags |= REQ_F_SINGLE_POLL;\n\t}\n\n\tpt->nr_entries++;\n\tpoll->head = head;\n\tpoll->wait.private = (void *) wqe_private;\n\n\tif (poll->events & EPOLLEXCLUSIVE)\n\t\tadd_wait_queue_exclusive(head, &poll->wait);\n\telse\n\t\tadd_wait_queue(head, &poll->wait);\n}\n\nstatic void io_poll_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\tstruct io_poll *poll = io_kiocb_to_cmd(pt->req, struct io_poll);\n\n\t__io_queue_proc(poll, pt, head,\n\t\t\t(struct io_poll **) &pt->req->async_data);\n}\n\nstatic bool io_poll_can_finish_inline(struct io_kiocb *req,\n\t\t\t\t      struct io_poll_table *pt)\n{\n\treturn pt->owning || io_poll_get_ownership(req);\n}\n\nstatic void io_poll_add_hash(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_HASH_LOCKED)\n\t\tio_poll_req_insert_locked(req);\n\telse\n\t\tio_poll_req_insert(req);\n}\n\n \nstatic int __io_arm_poll_handler(struct io_kiocb *req,\n\t\t\t\t struct io_poll *poll,\n\t\t\t\t struct io_poll_table *ipt, __poll_t mask,\n\t\t\t\t unsigned issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tINIT_HLIST_NODE(&req->hash_node);\n\treq->work.cancel_seq = atomic_read(&ctx->cancel_seq);\n\tio_init_poll_iocb(poll, mask);\n\tpoll->file = req->file;\n\treq->apoll_events = poll->events;\n\n\tipt->pt._key = mask;\n\tipt->req = req;\n\tipt->error = 0;\n\tipt->nr_entries = 0;\n\t \n\tipt->owning = issue_flags & IO_URING_F_UNLOCKED;\n\tatomic_set(&req->poll_refs, (int)ipt->owning);\n\n\t \n\tif (issue_flags & IO_URING_F_UNLOCKED)\n\t\treq->flags &= ~REQ_F_HASH_LOCKED;\n\n\tmask = vfs_poll(req->file, &ipt->pt) & poll->events;\n\n\tif (unlikely(ipt->error || !ipt->nr_entries)) {\n\t\tio_poll_remove_entries(req);\n\n\t\tif (!io_poll_can_finish_inline(req, ipt)) {\n\t\t\tio_poll_mark_cancelled(req);\n\t\t\treturn 0;\n\t\t} else if (mask && (poll->events & EPOLLET)) {\n\t\t\tipt->result_mask = mask;\n\t\t\treturn 1;\n\t\t}\n\t\treturn ipt->error ?: -EINVAL;\n\t}\n\n\tif (mask &&\n\t   ((poll->events & (EPOLLET|EPOLLONESHOT)) == (EPOLLET|EPOLLONESHOT))) {\n\t\tif (!io_poll_can_finish_inline(req, ipt)) {\n\t\t\tio_poll_add_hash(req);\n\t\t\treturn 0;\n\t\t}\n\t\tio_poll_remove_entries(req);\n\t\tipt->result_mask = mask;\n\t\t \n\t\treturn 1;\n\t}\n\n\tio_poll_add_hash(req);\n\n\tif (mask && (poll->events & EPOLLET) &&\n\t    io_poll_can_finish_inline(req, ipt)) {\n\t\t__io_poll_execute(req, mask);\n\t\treturn 0;\n\t}\n\n\tif (ipt->owning) {\n\t\t \n\t\tif (atomic_cmpxchg(&req->poll_refs, 1, 0) != 1)\n\t\t\t__io_poll_execute(req, 0);\n\t}\n\treturn 0;\n}\n\nstatic void io_async_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\t\t       struct poll_table_struct *p)\n{\n\tstruct io_poll_table *pt = container_of(p, struct io_poll_table, pt);\n\tstruct async_poll *apoll = pt->req->apoll;\n\n\t__io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);\n}\n\n \n#define APOLL_MAX_RETRY\t\t128\n\nstatic struct async_poll *io_req_alloc_apoll(struct io_kiocb *req,\n\t\t\t\t\t     unsigned issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_cache_entry *entry;\n\tstruct async_poll *apoll;\n\n\tif (req->flags & REQ_F_POLLED) {\n\t\tapoll = req->apoll;\n\t\tkfree(apoll->double_poll);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED)) {\n\t\tentry = io_alloc_cache_get(&ctx->apoll_cache);\n\t\tif (entry == NULL)\n\t\t\tgoto alloc_apoll;\n\t\tapoll = container_of(entry, struct async_poll, cache);\n\t\tapoll->poll.retries = APOLL_MAX_RETRY;\n\t} else {\nalloc_apoll:\n\t\tapoll = kmalloc(sizeof(*apoll), GFP_ATOMIC);\n\t\tif (unlikely(!apoll))\n\t\t\treturn NULL;\n\t\tapoll->poll.retries = APOLL_MAX_RETRY;\n\t}\n\tapoll->double_poll = NULL;\n\treq->apoll = apoll;\n\tif (unlikely(!--apoll->poll.retries))\n\t\treturn NULL;\n\treturn apoll;\n}\n\nint io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)\n{\n\tconst struct io_issue_def *def = &io_issue_defs[req->opcode];\n\tstruct async_poll *apoll;\n\tstruct io_poll_table ipt;\n\t__poll_t mask = POLLPRI | POLLERR | EPOLLET;\n\tint ret;\n\n\t \n\treq->flags |= REQ_F_HASH_LOCKED;\n\n\tif (!def->pollin && !def->pollout)\n\t\treturn IO_APOLL_ABORTED;\n\tif (!file_can_poll(req->file))\n\t\treturn IO_APOLL_ABORTED;\n\tif (!(req->flags & REQ_F_APOLL_MULTISHOT))\n\t\tmask |= EPOLLONESHOT;\n\n\tif (def->pollin) {\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\t\t \n\t\tif (req->flags & REQ_F_CLEAR_POLLIN)\n\t\t\tmask &= ~EPOLLIN;\n\t} else {\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\t}\n\tif (def->poll_exclusive)\n\t\tmask |= EPOLLEXCLUSIVE;\n\n\tapoll = io_req_alloc_apoll(req, issue_flags);\n\tif (!apoll)\n\t\treturn IO_APOLL_ABORTED;\n\treq->flags &= ~(REQ_F_SINGLE_POLL | REQ_F_DOUBLE_POLL);\n\treq->flags |= REQ_F_POLLED;\n\tipt.pt._qproc = io_async_queue_proc;\n\n\tio_kbuf_recycle(req, issue_flags);\n\n\tret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask, issue_flags);\n\tif (ret)\n\t\treturn ret > 0 ? IO_APOLL_READY : IO_APOLL_ABORTED;\n\ttrace_io_uring_poll_arm(req, mask, apoll->poll.events);\n\treturn IO_APOLL_OK;\n}\n\nstatic __cold bool io_poll_remove_all_table(struct task_struct *tsk,\n\t\t\t\t\t    struct io_hash_table *table,\n\t\t\t\t\t    bool cancel_all)\n{\n\tunsigned nr_buckets = 1U << table->hash_bits;\n\tstruct hlist_node *tmp;\n\tstruct io_kiocb *req;\n\tbool found = false;\n\tint i;\n\n\tfor (i = 0; i < nr_buckets; i++) {\n\t\tstruct io_hash_bucket *hb = &table->hbs[i];\n\n\t\tspin_lock(&hb->lock);\n\t\thlist_for_each_entry_safe(req, tmp, &hb->list, hash_node) {\n\t\t\tif (io_match_task_safe(req, tsk, cancel_all)) {\n\t\t\t\thlist_del_init(&req->hash_node);\n\t\t\t\tio_poll_cancel_req(req);\n\t\t\t\tfound = true;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&hb->lock);\n\t}\n\treturn found;\n}\n\n \n__cold bool io_poll_remove_all(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t       bool cancel_all)\n\t__must_hold(&ctx->uring_lock)\n{\n\tbool ret;\n\n\tret = io_poll_remove_all_table(tsk, &ctx->cancel_table, cancel_all);\n\tret |= io_poll_remove_all_table(tsk, &ctx->cancel_table_locked, cancel_all);\n\treturn ret;\n}\n\nstatic struct io_kiocb *io_poll_find(struct io_ring_ctx *ctx, bool poll_only,\n\t\t\t\t     struct io_cancel_data *cd,\n\t\t\t\t     struct io_hash_table *table,\n\t\t\t\t     struct io_hash_bucket **out_bucket)\n{\n\tstruct io_kiocb *req;\n\tu32 index = hash_long(cd->data, table->hash_bits);\n\tstruct io_hash_bucket *hb = &table->hbs[index];\n\n\t*out_bucket = NULL;\n\n\tspin_lock(&hb->lock);\n\thlist_for_each_entry(req, &hb->list, hash_node) {\n\t\tif (cd->data != req->cqe.user_data)\n\t\t\tcontinue;\n\t\tif (poll_only && req->opcode != IORING_OP_POLL_ADD)\n\t\t\tcontinue;\n\t\tif (cd->flags & IORING_ASYNC_CANCEL_ALL) {\n\t\t\tif (cd->seq == req->work.cancel_seq)\n\t\t\t\tcontinue;\n\t\t\treq->work.cancel_seq = cd->seq;\n\t\t}\n\t\t*out_bucket = hb;\n\t\treturn req;\n\t}\n\tspin_unlock(&hb->lock);\n\treturn NULL;\n}\n\nstatic struct io_kiocb *io_poll_file_find(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct io_cancel_data *cd,\n\t\t\t\t\t  struct io_hash_table *table,\n\t\t\t\t\t  struct io_hash_bucket **out_bucket)\n{\n\tunsigned nr_buckets = 1U << table->hash_bits;\n\tstruct io_kiocb *req;\n\tint i;\n\n\t*out_bucket = NULL;\n\n\tfor (i = 0; i < nr_buckets; i++) {\n\t\tstruct io_hash_bucket *hb = &table->hbs[i];\n\n\t\tspin_lock(&hb->lock);\n\t\thlist_for_each_entry(req, &hb->list, hash_node) {\n\t\t\tif (io_cancel_req_match(req, cd)) {\n\t\t\t\t*out_bucket = hb;\n\t\t\t\treturn req;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&hb->lock);\n\t}\n\treturn NULL;\n}\n\nstatic int io_poll_disarm(struct io_kiocb *req)\n{\n\tif (!req)\n\t\treturn -ENOENT;\n\tif (!io_poll_get_ownership(req))\n\t\treturn -EALREADY;\n\tio_poll_remove_entries(req);\n\thash_del(&req->hash_node);\n\treturn 0;\n}\n\nstatic int __io_poll_cancel(struct io_ring_ctx *ctx, struct io_cancel_data *cd,\n\t\t\t    struct io_hash_table *table)\n{\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *req;\n\n\tif (cd->flags & (IORING_ASYNC_CANCEL_FD | IORING_ASYNC_CANCEL_OP |\n\t\t\t IORING_ASYNC_CANCEL_ANY))\n\t\treq = io_poll_file_find(ctx, cd, table, &bucket);\n\telse\n\t\treq = io_poll_find(ctx, false, cd, table, &bucket);\n\n\tif (req)\n\t\tio_poll_cancel_req(req);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\treturn req ? 0 : -ENOENT;\n}\n\nint io_poll_cancel(struct io_ring_ctx *ctx, struct io_cancel_data *cd,\n\t\t   unsigned issue_flags)\n{\n\tint ret;\n\n\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table);\n\tif (ret != -ENOENT)\n\t\treturn ret;\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\tret = __io_poll_cancel(ctx, cd, &ctx->cancel_table_locked);\n\tio_ring_submit_unlock(ctx, issue_flags);\n\treturn ret;\n}\n\nstatic __poll_t io_poll_parse_events(const struct io_uring_sqe *sqe,\n\t\t\t\t     unsigned int flags)\n{\n\tu32 events;\n\n\tevents = READ_ONCE(sqe->poll32_events);\n#ifdef __BIG_ENDIAN\n\tevents = swahw32(events);\n#endif\n\tif (!(flags & IORING_POLL_ADD_MULTI))\n\t\tevents |= EPOLLONESHOT;\n\tif (!(flags & IORING_POLL_ADD_LEVEL))\n\t\tevents |= EPOLLET;\n\treturn demangle_poll(events) |\n\t\t(events & (EPOLLEXCLUSIVE|EPOLLONESHOT|EPOLLET));\n}\n\nint io_poll_remove_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_poll_update *upd = io_kiocb_to_cmd(req, struct io_poll_update);\n\tu32 flags;\n\n\tif (sqe->buf_index || sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->len);\n\tif (flags & ~(IORING_POLL_UPDATE_EVENTS | IORING_POLL_UPDATE_USER_DATA |\n\t\t      IORING_POLL_ADD_MULTI))\n\t\treturn -EINVAL;\n\t \n\tif (flags == IORING_POLL_ADD_MULTI)\n\t\treturn -EINVAL;\n\n\tupd->old_user_data = READ_ONCE(sqe->addr);\n\tupd->update_events = flags & IORING_POLL_UPDATE_EVENTS;\n\tupd->update_user_data = flags & IORING_POLL_UPDATE_USER_DATA;\n\n\tupd->new_user_data = READ_ONCE(sqe->off);\n\tif (!upd->update_user_data && upd->new_user_data)\n\t\treturn -EINVAL;\n\tif (upd->update_events)\n\t\tupd->events = io_poll_parse_events(sqe, flags);\n\telse if (sqe->poll32_events)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint io_poll_add_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_poll *poll = io_kiocb_to_cmd(req, struct io_poll);\n\tu32 flags;\n\n\tif (sqe->buf_index || sqe->off || sqe->addr)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->len);\n\tif (flags & ~IORING_POLL_ADD_MULTI)\n\t\treturn -EINVAL;\n\tif ((flags & IORING_POLL_ADD_MULTI) && (req->flags & REQ_F_CQE_SKIP))\n\t\treturn -EINVAL;\n\n\tpoll->events = io_poll_parse_events(sqe, flags);\n\treturn 0;\n}\n\nint io_poll_add(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll *poll = io_kiocb_to_cmd(req, struct io_poll);\n\tstruct io_poll_table ipt;\n\tint ret;\n\n\tipt.pt._qproc = io_poll_queue_proc;\n\n\t \n\tif (req->ctx->flags & (IORING_SETUP_SQPOLL|IORING_SETUP_SINGLE_ISSUER))\n\t\treq->flags |= REQ_F_HASH_LOCKED;\n\n\tret = __io_arm_poll_handler(req, poll, &ipt, poll->events, issue_flags);\n\tif (ret > 0) {\n\t\tio_req_set_res(req, ipt.result_mask, 0);\n\t\treturn IOU_OK;\n\t}\n\treturn ret ?: IOU_ISSUE_SKIP_COMPLETE;\n}\n\nint io_poll_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_poll_update *poll_update = io_kiocb_to_cmd(req, struct io_poll_update);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_cancel_data cd = { .ctx = ctx, .data = poll_update->old_user_data, };\n\tstruct io_hash_bucket *bucket;\n\tstruct io_kiocb *preq;\n\tint ret2, ret = 0;\n\tstruct io_tw_state ts = { .locked = true };\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tif (!ret2)\n\t\tgoto found;\n\tif (ret2 != -ENOENT) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\n\tpreq = io_poll_find(ctx, true, &cd, &ctx->cancel_table_locked, &bucket);\n\tret2 = io_poll_disarm(preq);\n\tif (bucket)\n\t\tspin_unlock(&bucket->lock);\n\tif (ret2) {\n\t\tret = ret2;\n\t\tgoto out;\n\t}\n\nfound:\n\tif (WARN_ON_ONCE(preq->opcode != IORING_OP_POLL_ADD)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tif (poll_update->update_events || poll_update->update_user_data) {\n\t\t \n\t\tif (poll_update->update_events) {\n\t\t\tstruct io_poll *poll = io_kiocb_to_cmd(preq, struct io_poll);\n\n\t\t\tpoll->events &= ~0xffff;\n\t\t\tpoll->events |= poll_update->events & 0xffff;\n\t\t\tpoll->events |= IO_POLL_UNMASK;\n\t\t}\n\t\tif (poll_update->update_user_data)\n\t\t\tpreq->cqe.user_data = poll_update->new_user_data;\n\n\t\tret2 = io_poll_add(preq, issue_flags & ~IO_URING_F_UNLOCKED);\n\t\t \n\t\tif (!ret2 || ret2 == -EIOCBQUEUED)\n\t\t\tgoto out;\n\t}\n\n\treq_set_fail(preq);\n\tio_req_set_res(preq, -ECANCELED, 0);\n\tio_req_task_complete(preq, &ts);\nout:\n\tio_ring_submit_unlock(ctx, issue_flags);\n\tif (ret < 0) {\n\t\treq_set_fail(req);\n\t\treturn ret;\n\t}\n\t \n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}\n\nvoid io_apoll_cache_free(struct io_cache_entry *entry)\n{\n\tkfree(container_of(entry, struct async_poll, cache));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}