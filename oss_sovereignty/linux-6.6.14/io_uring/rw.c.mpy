{
  "module_name": "rw.c",
  "hash_id": "4efbba21b19ebfb38ace1581511fe8f72a38c1b0eda4fea7ea12c3eee363fe93",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/rw.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/blk-mq.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/fsnotify.h>\n#include <linux/poll.h>\n#include <linux/nospec.h>\n#include <linux/compat.h>\n#include <linux/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"opdef.h\"\n#include \"kbuf.h\"\n#include \"rsrc.h\"\n#include \"rw.h\"\n\nstruct io_rw {\n\t \n\tstruct kiocb\t\t\tkiocb;\n\tu64\t\t\t\taddr;\n\tu32\t\t\t\tlen;\n\trwf_t\t\t\t\tflags;\n};\n\nstatic inline bool io_file_supports_nowait(struct io_kiocb *req)\n{\n\treturn req->flags & REQ_F_SUPPORT_NOWAIT;\n}\n\n#ifdef CONFIG_COMPAT\nstatic int io_iov_compat_buffer_select_prep(struct io_rw *rw)\n{\n\tstruct compat_iovec __user *uiov;\n\tcompat_ssize_t clen;\n\n\tuiov = u64_to_user_ptr(rw->addr);\n\tif (!access_ok(uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\tif (__get_user(clen, &uiov->iov_len))\n\t\treturn -EFAULT;\n\tif (clen < 0)\n\t\treturn -EINVAL;\n\n\trw->len = clen;\n\treturn 0;\n}\n#endif\n\nstatic int io_iov_buffer_select_prep(struct io_kiocb *req)\n{\n\tstruct iovec __user *uiov;\n\tstruct iovec iov;\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\n\tif (rw->len != 1)\n\t\treturn -EINVAL;\n\n#ifdef CONFIG_COMPAT\n\tif (req->ctx->compat)\n\t\treturn io_iov_compat_buffer_select_prep(rw);\n#endif\n\n\tuiov = u64_to_user_ptr(rw->addr);\n\tif (copy_from_user(&iov, uiov, sizeof(*uiov)))\n\t\treturn -EFAULT;\n\trw->len = iov.iov_len;\n\treturn 0;\n}\n\nint io_prep_rw(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tunsigned ioprio;\n\tint ret;\n\n\trw->kiocb.ki_pos = READ_ONCE(sqe->off);\n\t \n\treq->buf_index = READ_ONCE(sqe->buf_index);\n\n\tif (req->opcode == IORING_OP_READ_FIXED ||\n\t    req->opcode == IORING_OP_WRITE_FIXED) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\t\tu16 index;\n\n\t\tif (unlikely(req->buf_index >= ctx->nr_user_bufs))\n\t\t\treturn -EFAULT;\n\t\tindex = array_index_nospec(req->buf_index, ctx->nr_user_bufs);\n\t\treq->imu = ctx->user_bufs[index];\n\t\tio_req_set_rsrc_node(req, ctx, 0);\n\t}\n\n\tioprio = READ_ONCE(sqe->ioprio);\n\tif (ioprio) {\n\t\tret = ioprio_check_cap(ioprio);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\trw->kiocb.ki_ioprio = ioprio;\n\t} else {\n\t\trw->kiocb.ki_ioprio = get_current_ioprio();\n\t}\n\trw->kiocb.dio_complete = NULL;\n\n\trw->addr = READ_ONCE(sqe->addr);\n\trw->len = READ_ONCE(sqe->len);\n\trw->flags = READ_ONCE(sqe->rw_flags);\n\n\t \n\tif (req->opcode == IORING_OP_READV && req->flags & REQ_F_BUFFER_SELECT) {\n\t\tret = io_iov_buffer_select_prep(req);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nvoid io_readv_writev_cleanup(struct io_kiocb *req)\n{\n\tstruct io_async_rw *io = req->async_data;\n\n\tkfree(io->free_iovec);\n}\n\nstatic inline void io_rw_done(struct kiocb *kiocb, ssize_t ret)\n{\n\tswitch (ret) {\n\tcase -EIOCBQUEUED:\n\t\tbreak;\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\tcase -ERESTART_RESTARTBLOCK:\n\t\t \n\t\tret = -EINTR;\n\t\tfallthrough;\n\tdefault:\n\t\tkiocb->ki_complete(kiocb, ret);\n\t}\n}\n\nstatic inline loff_t *io_kiocb_update_pos(struct io_kiocb *req)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\n\tif (rw->kiocb.ki_pos != -1)\n\t\treturn &rw->kiocb.ki_pos;\n\n\tif (!(req->file->f_mode & FMODE_STREAM)) {\n\t\treq->flags |= REQ_F_CUR_POS;\n\t\trw->kiocb.ki_pos = req->file->f_pos;\n\t\treturn &rw->kiocb.ki_pos;\n\t}\n\n\trw->kiocb.ki_pos = 0;\n\treturn NULL;\n}\n\nstatic void io_req_task_queue_reissue(struct io_kiocb *req)\n{\n\treq->io_task_work.func = io_queue_iowq;\n\tio_req_task_work_add(req);\n}\n\n#ifdef CONFIG_BLOCK\nstatic bool io_resubmit_prep(struct io_kiocb *req)\n{\n\tstruct io_async_rw *io = req->async_data;\n\n\tif (!req_has_async_data(req))\n\t\treturn !io_req_prep_async(req);\n\tiov_iter_restore(&io->s.iter, &io->s.iter_state);\n\treturn true;\n}\n\nstatic bool io_rw_should_reissue(struct io_kiocb *req)\n{\n\tumode_t mode = file_inode(req->file)->i_mode;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!S_ISBLK(mode) && !S_ISREG(mode))\n\t\treturn false;\n\tif ((req->flags & REQ_F_NOWAIT) || (io_wq_current_is_worker() &&\n\t    !(ctx->flags & IORING_SETUP_IOPOLL)))\n\t\treturn false;\n\t \n\tif (percpu_ref_is_dying(&ctx->refs))\n\t\treturn false;\n\t \n\tif (!same_thread_group(req->task, current) || !in_task())\n\t\treturn false;\n\treturn true;\n}\n#else\nstatic bool io_resubmit_prep(struct io_kiocb *req)\n{\n\treturn false;\n}\nstatic bool io_rw_should_reissue(struct io_kiocb *req)\n{\n\treturn false;\n}\n#endif\n\nstatic void io_req_end_write(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_ISREG) {\n\t\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\n\t\tkiocb_end_write(&rw->kiocb);\n\t}\n}\n\n \nstatic void io_req_io_end(struct io_kiocb *req)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\n\tif (rw->kiocb.ki_flags & IOCB_WRITE) {\n\t\tio_req_end_write(req);\n\t\tfsnotify_modify(req->file);\n\t} else {\n\t\tfsnotify_access(req->file);\n\t}\n}\n\nstatic bool __io_complete_rw_common(struct io_kiocb *req, long res)\n{\n\tif (unlikely(res != req->cqe.res)) {\n\t\tif ((res == -EAGAIN || res == -EOPNOTSUPP) &&\n\t\t    io_rw_should_reissue(req)) {\n\t\t\t \n\t\t\tio_req_io_end(req);\n\t\t\treq->flags |= REQ_F_REISSUE | REQ_F_PARTIAL_IO;\n\t\t\treturn true;\n\t\t}\n\t\treq_set_fail(req);\n\t\treq->cqe.res = res;\n\t}\n\treturn false;\n}\n\nstatic inline int io_fixup_rw_res(struct io_kiocb *req, long res)\n{\n\tstruct io_async_rw *io = req->async_data;\n\n\t \n\tif (req_has_async_data(req) && io->bytes_done > 0) {\n\t\tif (res < 0)\n\t\t\tres = io->bytes_done;\n\t\telse\n\t\t\tres += io->bytes_done;\n\t}\n\treturn res;\n}\n\nvoid io_req_rw_complete(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct kiocb *kiocb = &rw->kiocb;\n\n\tif ((kiocb->ki_flags & IOCB_DIO_CALLER_COMP) && kiocb->dio_complete) {\n\t\tlong res = kiocb->dio_complete(rw->kiocb.private);\n\n\t\tio_req_set_res(req, io_fixup_rw_res(req, res), 0);\n\t}\n\n\tio_req_io_end(req);\n\n\tif (req->flags & (REQ_F_BUFFER_SELECTED|REQ_F_BUFFER_RING)) {\n\t\tunsigned issue_flags = ts->locked ? 0 : IO_URING_F_UNLOCKED;\n\n\t\treq->cqe.flags |= io_put_kbuf(req, issue_flags);\n\t}\n\tio_req_task_complete(req, ts);\n}\n\nstatic void io_complete_rw(struct kiocb *kiocb, long res)\n{\n\tstruct io_rw *rw = container_of(kiocb, struct io_rw, kiocb);\n\tstruct io_kiocb *req = cmd_to_io_kiocb(rw);\n\n\tif (!kiocb->dio_complete || !(kiocb->ki_flags & IOCB_DIO_CALLER_COMP)) {\n\t\tif (__io_complete_rw_common(req, res))\n\t\t\treturn;\n\t\tio_req_set_res(req, io_fixup_rw_res(req, res), 0);\n\t}\n\treq->io_task_work.func = io_req_rw_complete;\n\t__io_req_task_work_add(req, IOU_F_TWQ_LAZY_WAKE);\n}\n\nstatic void io_complete_rw_iopoll(struct kiocb *kiocb, long res)\n{\n\tstruct io_rw *rw = container_of(kiocb, struct io_rw, kiocb);\n\tstruct io_kiocb *req = cmd_to_io_kiocb(rw);\n\n\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\tio_req_end_write(req);\n\tif (unlikely(res != req->cqe.res)) {\n\t\tif (res == -EAGAIN && io_rw_should_reissue(req)) {\n\t\t\treq->flags |= REQ_F_REISSUE | REQ_F_PARTIAL_IO;\n\t\t\treturn;\n\t\t}\n\t\treq->cqe.res = res;\n\t}\n\n\t \n\tsmp_store_release(&req->iopoll_completed, 1);\n}\n\nstatic int kiocb_done(struct io_kiocb *req, ssize_t ret,\n\t\t       unsigned int issue_flags)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tunsigned final_ret = io_fixup_rw_res(req, ret);\n\n\tif (ret >= 0 && req->flags & REQ_F_CUR_POS)\n\t\treq->file->f_pos = rw->kiocb.ki_pos;\n\tif (ret >= 0 && (rw->kiocb.ki_complete == io_complete_rw)) {\n\t\tif (!__io_complete_rw_common(req, ret)) {\n\t\t\t \n\t\t\tio_req_io_end(req);\n\t\t\tio_req_set_res(req, final_ret,\n\t\t\t\t       io_put_kbuf(req, issue_flags));\n\t\t\treturn IOU_OK;\n\t\t}\n\t} else {\n\t\tio_rw_done(&rw->kiocb, ret);\n\t}\n\n\tif (req->flags & REQ_F_REISSUE) {\n\t\treq->flags &= ~REQ_F_REISSUE;\n\t\tif (io_resubmit_prep(req))\n\t\t\tio_req_task_queue_reissue(req);\n\t\telse\n\t\t\tio_req_task_queue_fail(req, final_ret);\n\t}\n\treturn IOU_ISSUE_SKIP_COMPLETE;\n}\n\nstatic struct iovec *__io_import_iovec(int ddir, struct io_kiocb *req,\n\t\t\t\t       struct io_rw_state *s,\n\t\t\t\t       unsigned int issue_flags)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct iov_iter *iter = &s->iter;\n\tu8 opcode = req->opcode;\n\tstruct iovec *iovec;\n\tvoid __user *buf;\n\tsize_t sqe_len;\n\tssize_t ret;\n\n\tif (opcode == IORING_OP_READ_FIXED || opcode == IORING_OP_WRITE_FIXED) {\n\t\tret = io_import_fixed(ddir, iter, req->imu, rw->addr, rw->len);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\tbuf = u64_to_user_ptr(rw->addr);\n\tsqe_len = rw->len;\n\n\tif (opcode == IORING_OP_READ || opcode == IORING_OP_WRITE ||\n\t    (req->flags & REQ_F_BUFFER_SELECT)) {\n\t\tif (io_do_buffer_select(req)) {\n\t\t\tbuf = io_buffer_select(req, &sqe_len, issue_flags);\n\t\t\tif (!buf)\n\t\t\t\treturn ERR_PTR(-ENOBUFS);\n\t\t\trw->addr = (unsigned long) buf;\n\t\t\trw->len = sqe_len;\n\t\t}\n\n\t\tret = import_ubuf(ddir, buf, sqe_len, iter);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\tiovec = s->fast_iov;\n\tret = __import_iovec(ddir, buf, sqe_len, UIO_FASTIOV, &iovec, iter,\n\t\t\t      req->ctx->compat);\n\tif (unlikely(ret < 0))\n\t\treturn ERR_PTR(ret);\n\treturn iovec;\n}\n\nstatic inline int io_import_iovec(int rw, struct io_kiocb *req,\n\t\t\t\t  struct iovec **iovec, struct io_rw_state *s,\n\t\t\t\t  unsigned int issue_flags)\n{\n\t*iovec = __io_import_iovec(rw, req, s, issue_flags);\n\tif (IS_ERR(*iovec))\n\t\treturn PTR_ERR(*iovec);\n\n\tiov_iter_save_state(&s->iter, &s->iter_state);\n\treturn 0;\n}\n\nstatic inline loff_t *io_kiocb_ppos(struct kiocb *kiocb)\n{\n\treturn (kiocb->ki_filp->f_mode & FMODE_STREAM) ? NULL : &kiocb->ki_pos;\n}\n\n \nstatic ssize_t loop_rw_iter(int ddir, struct io_rw *rw, struct iov_iter *iter)\n{\n\tstruct kiocb *kiocb = &rw->kiocb;\n\tstruct file *file = kiocb->ki_filp;\n\tssize_t ret = 0;\n\tloff_t *ppos;\n\n\t \n\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\treturn -EOPNOTSUPP;\n\tif ((kiocb->ki_flags & IOCB_NOWAIT) &&\n\t    !(kiocb->ki_filp->f_flags & O_NONBLOCK))\n\t\treturn -EAGAIN;\n\n\tppos = io_kiocb_ppos(kiocb);\n\n\twhile (iov_iter_count(iter)) {\n\t\tvoid __user *addr;\n\t\tsize_t len;\n\t\tssize_t nr;\n\n\t\tif (iter_is_ubuf(iter)) {\n\t\t\taddr = iter->ubuf + iter->iov_offset;\n\t\t\tlen = iov_iter_count(iter);\n\t\t} else if (!iov_iter_is_bvec(iter)) {\n\t\t\taddr = iter_iov_addr(iter);\n\t\t\tlen = iter_iov_len(iter);\n\t\t} else {\n\t\t\taddr = u64_to_user_ptr(rw->addr);\n\t\t\tlen = rw->len;\n\t\t}\n\n\t\tif (ddir == READ)\n\t\t\tnr = file->f_op->read(file, addr, len, ppos);\n\t\telse\n\t\t\tnr = file->f_op->write(file, addr, len, ppos);\n\n\t\tif (nr < 0) {\n\t\t\tif (!ret)\n\t\t\t\tret = nr;\n\t\t\tbreak;\n\t\t}\n\t\tret += nr;\n\t\tif (!iov_iter_is_bvec(iter)) {\n\t\t\tiov_iter_advance(iter, nr);\n\t\t} else {\n\t\t\trw->addr += nr;\n\t\t\trw->len -= nr;\n\t\t\tif (!rw->len)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (nr != len)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void io_req_map_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t  const struct iovec *fast_iov, struct iov_iter *iter)\n{\n\tstruct io_async_rw *io = req->async_data;\n\n\tmemcpy(&io->s.iter, iter, sizeof(*iter));\n\tio->free_iovec = iovec;\n\tio->bytes_done = 0;\n\t \n\tif (iov_iter_is_bvec(iter) || iter_is_ubuf(iter))\n\t\treturn;\n\tif (!iovec) {\n\t\tunsigned iov_off = 0;\n\n\t\tio->s.iter.__iov = io->s.fast_iov;\n\t\tif (iter->__iov != fast_iov) {\n\t\t\tiov_off = iter_iov(iter) - fast_iov;\n\t\t\tio->s.iter.__iov += iov_off;\n\t\t}\n\t\tif (io->s.fast_iov != fast_iov)\n\t\t\tmemcpy(io->s.fast_iov + iov_off, fast_iov + iov_off,\n\t\t\t       sizeof(struct iovec) * iter->nr_segs);\n\t} else {\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\t}\n}\n\nstatic int io_setup_async_rw(struct io_kiocb *req, const struct iovec *iovec,\n\t\t\t     struct io_rw_state *s, bool force)\n{\n\tif (!force && !io_cold_defs[req->opcode].prep_async)\n\t\treturn 0;\n\tif (!req_has_async_data(req)) {\n\t\tstruct io_async_rw *iorw;\n\n\t\tif (io_alloc_async_data(req)) {\n\t\t\tkfree(iovec);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tio_req_map_rw(req, iovec, s->fast_iov, &s->iter);\n\t\tiorw = req->async_data;\n\t\t \n\t\tiov_iter_save_state(&iorw->s.iter, &iorw->s.iter_state);\n\t}\n\treturn 0;\n}\n\nstatic inline int io_rw_prep_async(struct io_kiocb *req, int rw)\n{\n\tstruct io_async_rw *iorw = req->async_data;\n\tstruct iovec *iov;\n\tint ret;\n\n\tiorw->bytes_done = 0;\n\tiorw->free_iovec = NULL;\n\n\t \n\tret = io_import_iovec(rw, req, &iov, &iorw->s, 0);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (iov) {\n\t\tiorw->free_iovec = iov;\n\t\treq->flags |= REQ_F_NEED_CLEANUP;\n\t}\n\n\treturn 0;\n}\n\nint io_readv_prep_async(struct io_kiocb *req)\n{\n\treturn io_rw_prep_async(req, ITER_DEST);\n}\n\nint io_writev_prep_async(struct io_kiocb *req)\n{\n\treturn io_rw_prep_async(req, ITER_SOURCE);\n}\n\n \nstatic int io_async_buf_func(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t     int sync, void *arg)\n{\n\tstruct wait_page_queue *wpq;\n\tstruct io_kiocb *req = wait->private;\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct wait_page_key *key = arg;\n\n\twpq = container_of(wait, struct wait_page_queue, wait);\n\n\tif (!wake_page_match(wpq, key))\n\t\treturn 0;\n\n\trw->kiocb.ki_flags &= ~IOCB_WAITQ;\n\tlist_del_init(&wait->entry);\n\tio_req_task_queue(req);\n\treturn 1;\n}\n\n \nstatic bool io_rw_should_retry(struct io_kiocb *req)\n{\n\tstruct io_async_rw *io = req->async_data;\n\tstruct wait_page_queue *wait = &io->wpq;\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct kiocb *kiocb = &rw->kiocb;\n\n\t \n\tif (req->flags & REQ_F_NOWAIT)\n\t\treturn false;\n\n\t \n\tif (kiocb->ki_flags & (IOCB_DIRECT | IOCB_HIPRI))\n\t\treturn false;\n\n\t \n\tif (file_can_poll(req->file) || !(req->file->f_mode & FMODE_BUF_RASYNC))\n\t\treturn false;\n\n\twait->wait.func = io_async_buf_func;\n\twait->wait.private = req;\n\twait->wait.flags = 0;\n\tINIT_LIST_HEAD(&wait->wait.entry);\n\tkiocb->ki_flags |= IOCB_WAITQ;\n\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\tkiocb->ki_waitq = wait;\n\treturn true;\n}\n\nstatic inline int io_iter_do_read(struct io_rw *rw, struct iov_iter *iter)\n{\n\tstruct file *file = rw->kiocb.ki_filp;\n\n\tif (likely(file->f_op->read_iter))\n\t\treturn call_read_iter(file, &rw->kiocb, iter);\n\telse if (file->f_op->read)\n\t\treturn loop_rw_iter(READ, rw, iter);\n\telse\n\t\treturn -EINVAL;\n}\n\nstatic bool need_complete_io(struct io_kiocb *req)\n{\n\treturn req->flags & REQ_F_ISREG ||\n\t\tS_ISBLK(file_inode(req->file)->i_mode);\n}\n\nstatic int io_rw_init_file(struct io_kiocb *req, fmode_t mode)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct kiocb *kiocb = &rw->kiocb;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct file *file = req->file;\n\tint ret;\n\n\tif (unlikely(!file || !(file->f_mode & mode)))\n\t\treturn -EBADF;\n\n\tif (!(req->flags & REQ_F_FIXED_FILE))\n\t\treq->flags |= io_file_get_flags(file);\n\n\tkiocb->ki_flags = file->f_iocb_flags;\n\tret = kiocb_set_rw_flags(kiocb, rw->flags);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tkiocb->ki_flags |= IOCB_ALLOC_CACHE;\n\n\t \n\tif ((kiocb->ki_flags & IOCB_NOWAIT) ||\n\t    ((file->f_flags & O_NONBLOCK) && !io_file_supports_nowait(req)))\n\t\treq->flags |= REQ_F_NOWAIT;\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tif (!(kiocb->ki_flags & IOCB_DIRECT) || !file->f_op->iopoll)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tkiocb->private = NULL;\n\t\tkiocb->ki_flags |= IOCB_HIPRI;\n\t\tkiocb->ki_complete = io_complete_rw_iopoll;\n\t\treq->iopoll_completed = 0;\n\t} else {\n\t\tif (kiocb->ki_flags & IOCB_HIPRI)\n\t\t\treturn -EINVAL;\n\t\tkiocb->ki_complete = io_complete_rw;\n\t}\n\n\treturn 0;\n}\n\nint io_read(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct io_rw_state __s, *s = &__s;\n\tstruct iovec *iovec;\n\tstruct kiocb *kiocb = &rw->kiocb;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\tstruct io_async_rw *io;\n\tssize_t ret, ret2;\n\tloff_t *ppos;\n\n\tif (!req_has_async_data(req)) {\n\t\tret = io_import_iovec(ITER_DEST, req, &iovec, s, issue_flags);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\t} else {\n\t\tio = req->async_data;\n\t\ts = &io->s;\n\n\t\t \n\t\tif (io_do_buffer_select(req)) {\n\t\t\tret = io_import_iovec(ITER_DEST, req, &iovec, s, issue_flags);\n\t\t\tif (unlikely(ret < 0))\n\t\t\t\treturn ret;\n\t\t}\n\n\t\t \n\t\tiov_iter_restore(&s->iter, &s->iter_state);\n\t\tiovec = NULL;\n\t}\n\tret = io_rw_init_file(req, FMODE_READ);\n\tif (unlikely(ret)) {\n\t\tkfree(iovec);\n\t\treturn ret;\n\t}\n\treq->cqe.res = iov_iter_count(&s->iter);\n\n\tif (force_nonblock) {\n\t\t \n\t\tif (unlikely(!io_file_supports_nowait(req))) {\n\t\t\tret = io_setup_async_rw(req, iovec, s, true);\n\t\t\treturn ret ?: -EAGAIN;\n\t\t}\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\t} else {\n\t\t \n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\t}\n\n\tppos = io_kiocb_update_pos(req);\n\n\tret = rw_verify_area(READ, req->file, ppos, req->cqe.res);\n\tif (unlikely(ret)) {\n\t\tkfree(iovec);\n\t\treturn ret;\n\t}\n\n\tret = io_iter_do_read(rw, &s->iter);\n\n\tif (ret == -EAGAIN || (req->flags & REQ_F_REISSUE)) {\n\t\treq->flags &= ~REQ_F_REISSUE;\n\t\t \n\t\tif (req->opcode == IORING_OP_READ && file_can_poll(req->file))\n\t\t\treturn -EAGAIN;\n\t\t \n\t\tif (!force_nonblock && !(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tgoto done;\n\t\t \n\t\tif (req->flags & REQ_F_NOWAIT)\n\t\t\tgoto done;\n\t\tret = 0;\n\t} else if (ret == -EIOCBQUEUED) {\n\t\tif (iovec)\n\t\t\tkfree(iovec);\n\t\treturn IOU_ISSUE_SKIP_COMPLETE;\n\t} else if (ret == req->cqe.res || ret <= 0 || !force_nonblock ||\n\t\t   (req->flags & REQ_F_NOWAIT) || !need_complete_io(req)) {\n\t\t \n\t\tgoto done;\n\t}\n\n\t \n\tiov_iter_restore(&s->iter, &s->iter_state);\n\n\tret2 = io_setup_async_rw(req, iovec, s, true);\n\tiovec = NULL;\n\tif (ret2) {\n\t\tret = ret > 0 ? ret : ret2;\n\t\tgoto done;\n\t}\n\n\tio = req->async_data;\n\ts = &io->s;\n\t \n\n\tdo {\n\t\t \n\t\tiov_iter_advance(&s->iter, ret);\n\t\tif (!iov_iter_count(&s->iter))\n\t\t\tbreak;\n\t\tio->bytes_done += ret;\n\t\tiov_iter_save_state(&s->iter, &s->iter_state);\n\n\t\t \n\t\tif (!io_rw_should_retry(req)) {\n\t\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\treq->cqe.res = iov_iter_count(&s->iter);\n\t\t \n\t\tret = io_iter_do_read(rw, &s->iter);\n\t\tif (ret == -EIOCBQUEUED)\n\t\t\treturn IOU_ISSUE_SKIP_COMPLETE;\n\t\t \n\t\tkiocb->ki_flags &= ~IOCB_WAITQ;\n\t\tiov_iter_restore(&s->iter, &s->iter_state);\n\t} while (ret > 0);\ndone:\n\t \n\tif (iovec)\n\t\tkfree(iovec);\n\treturn kiocb_done(req, ret, issue_flags);\n}\n\nint io_write(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\tstruct io_rw_state __s, *s = &__s;\n\tstruct iovec *iovec;\n\tstruct kiocb *kiocb = &rw->kiocb;\n\tbool force_nonblock = issue_flags & IO_URING_F_NONBLOCK;\n\tssize_t ret, ret2;\n\tloff_t *ppos;\n\n\tif (!req_has_async_data(req)) {\n\t\tret = io_import_iovec(ITER_SOURCE, req, &iovec, s, issue_flags);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\t} else {\n\t\tstruct io_async_rw *io = req->async_data;\n\n\t\ts = &io->s;\n\t\tiov_iter_restore(&s->iter, &s->iter_state);\n\t\tiovec = NULL;\n\t}\n\tret = io_rw_init_file(req, FMODE_WRITE);\n\tif (unlikely(ret)) {\n\t\tkfree(iovec);\n\t\treturn ret;\n\t}\n\treq->cqe.res = iov_iter_count(&s->iter);\n\n\tif (force_nonblock) {\n\t\t \n\t\tif (unlikely(!io_file_supports_nowait(req)))\n\t\t\tgoto copy_iov;\n\n\t\t \n\t\tif (!(kiocb->ki_flags & IOCB_DIRECT) &&\n\t\t\t!(kiocb->ki_filp->f_mode & FMODE_BUF_WASYNC) &&\n\t\t\t(req->flags & REQ_F_ISREG))\n\t\t\tgoto copy_iov;\n\n\t\tkiocb->ki_flags |= IOCB_NOWAIT;\n\t} else {\n\t\t \n\t\tkiocb->ki_flags &= ~IOCB_NOWAIT;\n\t}\n\n\tppos = io_kiocb_update_pos(req);\n\n\tret = rw_verify_area(WRITE, req->file, ppos, req->cqe.res);\n\tif (unlikely(ret)) {\n\t\tkfree(iovec);\n\t\treturn ret;\n\t}\n\n\tif (req->flags & REQ_F_ISREG)\n\t\tkiocb_start_write(kiocb);\n\tkiocb->ki_flags |= IOCB_WRITE;\n\n\tif (likely(req->file->f_op->write_iter))\n\t\tret2 = call_write_iter(req->file, kiocb, &s->iter);\n\telse if (req->file->f_op->write)\n\t\tret2 = loop_rw_iter(WRITE, rw, &s->iter);\n\telse\n\t\tret2 = -EINVAL;\n\n\tif (req->flags & REQ_F_REISSUE) {\n\t\treq->flags &= ~REQ_F_REISSUE;\n\t\tret2 = -EAGAIN;\n\t}\n\n\t \n\tif (ret2 == -EOPNOTSUPP && (kiocb->ki_flags & IOCB_NOWAIT))\n\t\tret2 = -EAGAIN;\n\t \n\tif (ret2 == -EAGAIN && (req->flags & REQ_F_NOWAIT))\n\t\tgoto done;\n\tif (!force_nonblock || ret2 != -EAGAIN) {\n\t\t \n\t\tif (ret2 == -EAGAIN && (req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tgoto copy_iov;\n\n\t\tif (ret2 != req->cqe.res && ret2 >= 0 && need_complete_io(req)) {\n\t\t\tstruct io_async_rw *io;\n\n\t\t\ttrace_io_uring_short_write(req->ctx, kiocb->ki_pos - ret2,\n\t\t\t\t\t\treq->cqe.res, ret2);\n\n\t\t\t \n\t\t\tiov_iter_save_state(&s->iter, &s->iter_state);\n\t\t\tret = io_setup_async_rw(req, iovec, s, true);\n\n\t\t\tio = req->async_data;\n\t\t\tif (io)\n\t\t\t\tio->bytes_done += ret2;\n\n\t\t\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\t\t\tio_req_end_write(req);\n\t\t\treturn ret ? ret : -EAGAIN;\n\t\t}\ndone:\n\t\tret = kiocb_done(req, ret2, issue_flags);\n\t} else {\ncopy_iov:\n\t\tiov_iter_restore(&s->iter, &s->iter_state);\n\t\tret = io_setup_async_rw(req, iovec, s, false);\n\t\tif (!ret) {\n\t\t\tif (kiocb->ki_flags & IOCB_WRITE)\n\t\t\t\tio_req_end_write(req);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\treturn ret;\n\t}\n\t \n\tif (iovec)\n\t\tkfree(iovec);\n\treturn ret;\n}\n\nvoid io_rw_fail(struct io_kiocb *req)\n{\n\tint res;\n\n\tres = io_fixup_rw_res(req, req->cqe.res);\n\tio_req_set_res(req, res, req->cqe.flags);\n}\n\nint io_do_iopoll(struct io_ring_ctx *ctx, bool force_nonspin)\n{\n\tstruct io_wq_work_node *pos, *start, *prev;\n\tunsigned int poll_flags = 0;\n\tDEFINE_IO_COMP_BATCH(iob);\n\tint nr_events = 0;\n\n\t \n\tif (ctx->poll_multi_queue || force_nonspin)\n\t\tpoll_flags |= BLK_POLL_ONESHOT;\n\n\twq_list_for_each(pos, start, &ctx->iopoll_list) {\n\t\tstruct io_kiocb *req = container_of(pos, struct io_kiocb, comp_list);\n\t\tstruct file *file = req->file;\n\t\tint ret;\n\n\t\t \n\t\tif (READ_ONCE(req->iopoll_completed))\n\t\t\tbreak;\n\n\t\tif (req->opcode == IORING_OP_URING_CMD) {\n\t\t\tstruct io_uring_cmd *ioucmd;\n\n\t\t\tioucmd = io_kiocb_to_cmd(req, struct io_uring_cmd);\n\t\t\tret = file->f_op->uring_cmd_iopoll(ioucmd, &iob,\n\t\t\t\t\t\t\t\tpoll_flags);\n\t\t} else {\n\t\t\tstruct io_rw *rw = io_kiocb_to_cmd(req, struct io_rw);\n\n\t\t\tret = file->f_op->iopoll(&rw->kiocb, &iob, poll_flags);\n\t\t}\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\t\telse if (ret)\n\t\t\tpoll_flags |= BLK_POLL_ONESHOT;\n\n\t\t \n\t\tif (!rq_list_empty(iob.req_list) ||\n\t\t    READ_ONCE(req->iopoll_completed))\n\t\t\tbreak;\n\t}\n\n\tif (!rq_list_empty(iob.req_list))\n\t\tiob.complete(&iob);\n\telse if (!pos)\n\t\treturn 0;\n\n\tprev = start;\n\twq_list_for_each_resume(pos, prev) {\n\t\tstruct io_kiocb *req = container_of(pos, struct io_kiocb, comp_list);\n\n\t\t \n\t\tif (!smp_load_acquire(&req->iopoll_completed))\n\t\t\tbreak;\n\t\tnr_events++;\n\t\treq->cqe.flags = io_put_kbuf(req, 0);\n\t}\n\tif (unlikely(!nr_events))\n\t\treturn 0;\n\n\tpos = start ? start->next : ctx->iopoll_list.first;\n\twq_list_cut(&ctx->iopoll_list, prev, start);\n\n\tif (WARN_ON_ONCE(!wq_list_empty(&ctx->submit_state.compl_reqs)))\n\t\treturn 0;\n\tctx->submit_state.compl_reqs.first = pos;\n\t__io_submit_flush_completions(ctx);\n\treturn nr_events;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}