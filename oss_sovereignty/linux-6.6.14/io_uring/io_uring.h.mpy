{
  "module_name": "io_uring.h",
  "hash_id": "4e5b32a2a63e27d8823eb67e895d0df99218969c159b1a5d436637ed074b2e14",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/io_uring.h",
  "human_readable_source": "#ifndef IOU_CORE_H\n#define IOU_CORE_H\n\n#include <linux/errno.h>\n#include <linux/lockdep.h>\n#include <linux/resume_user_mode.h>\n#include <linux/kasan.h>\n#include <linux/io_uring_types.h>\n#include <uapi/linux/eventpoll.h>\n#include \"io-wq.h\"\n#include \"slist.h\"\n#include \"filetable.h\"\n\n#ifndef CREATE_TRACE_POINTS\n#include <trace/events/io_uring.h>\n#endif\n\nenum {\n\t \n\tIOU_F_TWQ_LAZY_WAKE\t\t\t= 1,\n};\n\nenum {\n\tIOU_OK\t\t\t= 0,\n\tIOU_ISSUE_SKIP_COMPLETE\t= -EIOCBQUEUED,\n\n\t \n\tIOU_STOP_MULTISHOT\t= -ECANCELED,\n};\n\nbool io_cqe_cache_refill(struct io_ring_ctx *ctx, bool overflow);\nvoid io_req_cqe_overflow(struct io_kiocb *req);\nint io_run_task_work_sig(struct io_ring_ctx *ctx);\nvoid io_req_defer_failed(struct io_kiocb *req, s32 res);\nvoid io_req_complete_post(struct io_kiocb *req, unsigned issue_flags);\nbool io_post_aux_cqe(struct io_ring_ctx *ctx, u64 user_data, s32 res, u32 cflags);\nbool io_fill_cqe_req_aux(struct io_kiocb *req, bool defer, s32 res, u32 cflags);\nvoid __io_commit_cqring_flush(struct io_ring_ctx *ctx);\n\nstruct page **io_pin_pages(unsigned long ubuf, unsigned long len, int *npages);\n\nstruct file *io_file_get_normal(struct io_kiocb *req, int fd);\nstruct file *io_file_get_fixed(struct io_kiocb *req, int fd,\n\t\t\t       unsigned issue_flags);\n\nvoid __io_req_task_work_add(struct io_kiocb *req, unsigned flags);\nbool io_is_uring_fops(struct file *file);\nbool io_alloc_async_data(struct io_kiocb *req);\nvoid io_req_task_queue(struct io_kiocb *req);\nvoid io_queue_iowq(struct io_kiocb *req, struct io_tw_state *ts_dont_use);\nvoid io_req_task_complete(struct io_kiocb *req, struct io_tw_state *ts);\nvoid io_req_task_queue_fail(struct io_kiocb *req, int ret);\nvoid io_req_task_submit(struct io_kiocb *req, struct io_tw_state *ts);\nvoid tctx_task_work(struct callback_head *cb);\n__cold void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd);\nint io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\tstruct io_ring_ctx *ctx);\n\nint io_ring_add_registered_file(struct io_uring_task *tctx, struct file *file,\n\t\t\t\t     int start, int end);\n\nint io_poll_issue(struct io_kiocb *req, struct io_tw_state *ts);\nint io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr);\nint io_do_iopoll(struct io_ring_ctx *ctx, bool force_nonspin);\nvoid __io_submit_flush_completions(struct io_ring_ctx *ctx);\nint io_req_prep_async(struct io_kiocb *req);\n\nstruct io_wq_work *io_wq_free_work(struct io_wq_work *work);\nvoid io_wq_submit_work(struct io_wq_work *work);\n\nvoid io_free_req(struct io_kiocb *req);\nvoid io_queue_next(struct io_kiocb *req);\nvoid io_task_refs_refill(struct io_uring_task *tctx);\nbool __io_alloc_req_refill(struct io_ring_ctx *ctx);\n\nbool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,\n\t\t\tbool cancel_all);\n\nvoid *io_mem_alloc(size_t size);\nvoid io_mem_free(void *ptr);\n\n#if defined(CONFIG_PROVE_LOCKING)\nstatic inline void io_lockdep_assert_cq_locked(struct io_ring_ctx *ctx)\n{\n\tlockdep_assert(in_task());\n\n\tif (ctx->flags & IORING_SETUP_IOPOLL) {\n\t\tlockdep_assert_held(&ctx->uring_lock);\n\t} else if (!ctx->task_complete) {\n\t\tlockdep_assert_held(&ctx->completion_lock);\n\t} else if (ctx->submitter_task) {\n\t\t \n\t\tif (ctx->submitter_task->flags & PF_EXITING)\n\t\t\tlockdep_assert(current_work());\n\t\telse\n\t\t\tlockdep_assert(current == ctx->submitter_task);\n\t}\n}\n#else\nstatic inline void io_lockdep_assert_cq_locked(struct io_ring_ctx *ctx)\n{\n}\n#endif\n\nstatic inline void io_req_task_work_add(struct io_kiocb *req)\n{\n\t__io_req_task_work_add(req, 0);\n}\n\n#define io_for_each_link(pos, head) \\\n\tfor (pos = (head); pos; pos = pos->link)\n\nstatic inline bool io_get_cqe_overflow(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_uring_cqe **ret,\n\t\t\t\t\tbool overflow)\n{\n\tio_lockdep_assert_cq_locked(ctx);\n\n\tif (unlikely(ctx->cqe_cached >= ctx->cqe_sentinel)) {\n\t\tif (unlikely(!io_cqe_cache_refill(ctx, overflow)))\n\t\t\treturn false;\n\t}\n\t*ret = ctx->cqe_cached;\n\tctx->cached_cq_tail++;\n\tctx->cqe_cached++;\n\tif (ctx->flags & IORING_SETUP_CQE32)\n\t\tctx->cqe_cached++;\n\treturn true;\n}\n\nstatic inline bool io_get_cqe(struct io_ring_ctx *ctx, struct io_uring_cqe **ret)\n{\n\treturn io_get_cqe_overflow(ctx, ret, false);\n}\n\nstatic __always_inline bool io_fill_cqe_req(struct io_ring_ctx *ctx,\n\t\t\t\t\t    struct io_kiocb *req)\n{\n\tstruct io_uring_cqe *cqe;\n\n\t \n\tif (unlikely(!io_get_cqe(ctx, &cqe)))\n\t\treturn false;\n\n\tif (trace_io_uring_complete_enabled())\n\t\ttrace_io_uring_complete(req->ctx, req, req->cqe.user_data,\n\t\t\t\t\treq->cqe.res, req->cqe.flags,\n\t\t\t\t\treq->big_cqe.extra1, req->big_cqe.extra2);\n\n\tmemcpy(cqe, &req->cqe, sizeof(*cqe));\n\tif (ctx->flags & IORING_SETUP_CQE32) {\n\t\tmemcpy(cqe->big_cqe, &req->big_cqe, sizeof(*cqe));\n\t\tmemset(&req->big_cqe, 0, sizeof(req->big_cqe));\n\t}\n\treturn true;\n}\n\nstatic inline void req_set_fail(struct io_kiocb *req)\n{\n\treq->flags |= REQ_F_FAIL;\n\tif (req->flags & REQ_F_CQE_SKIP) {\n\t\treq->flags &= ~REQ_F_CQE_SKIP;\n\t\treq->flags |= REQ_F_SKIP_LINK_CQES;\n\t}\n}\n\nstatic inline void io_req_set_res(struct io_kiocb *req, s32 res, u32 cflags)\n{\n\treq->cqe.res = res;\n\treq->cqe.flags = cflags;\n}\n\nstatic inline bool req_has_async_data(struct io_kiocb *req)\n{\n\treturn req->flags & REQ_F_ASYNC_DATA;\n}\n\nstatic inline void io_put_file(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_FIXED_FILE) && req->file)\n\t\tfput(req->file);\n}\n\nstatic inline void io_ring_submit_unlock(struct io_ring_ctx *ctx,\n\t\t\t\t\t unsigned issue_flags)\n{\n\tlockdep_assert_held(&ctx->uring_lock);\n\tif (issue_flags & IO_URING_F_UNLOCKED)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic inline void io_ring_submit_lock(struct io_ring_ctx *ctx,\n\t\t\t\t       unsigned issue_flags)\n{\n\t \n\tif (issue_flags & IO_URING_F_UNLOCKED)\n\t\tmutex_lock(&ctx->uring_lock);\n\tlockdep_assert_held(&ctx->uring_lock);\n}\n\nstatic inline void io_commit_cqring(struct io_ring_ctx *ctx)\n{\n\t \n\tsmp_store_release(&ctx->rings->cq.tail, ctx->cached_cq_tail);\n}\n\nstatic inline void io_poll_wq_wake(struct io_ring_ctx *ctx)\n{\n\tif (wq_has_sleeper(&ctx->poll_wq))\n\t\t__wake_up(&ctx->poll_wq, TASK_NORMAL, 0,\n\t\t\t\tpoll_to_key(EPOLL_URING_WAKE | EPOLLIN));\n}\n\nstatic inline void io_cqring_wake(struct io_ring_ctx *ctx)\n{\n\t \n\tif (wq_has_sleeper(&ctx->cq_wait))\n\t\t__wake_up(&ctx->cq_wait, TASK_NORMAL, 0,\n\t\t\t\tpoll_to_key(EPOLL_URING_WAKE | EPOLLIN));\n}\n\nstatic inline bool io_sqring_full(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *r = ctx->rings;\n\n\treturn READ_ONCE(r->sq.tail) - ctx->cached_sq_head == ctx->sq_entries;\n}\n\nstatic inline unsigned int io_sqring_entries(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned int entries;\n\n\t \n\tentries = smp_load_acquire(&rings->sq.tail) - ctx->cached_sq_head;\n\treturn min(entries, ctx->sq_entries);\n}\n\nstatic inline int io_run_task_work(void)\n{\n\t \n\tif (test_thread_flag(TIF_NOTIFY_SIGNAL))\n\t\tclear_notify_signal();\n\t \n\tif (current->flags & PF_IO_WORKER &&\n\t    test_thread_flag(TIF_NOTIFY_RESUME)) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tresume_user_mode_work(NULL);\n\t}\n\tif (task_work_pending(current)) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\ttask_work_run();\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool io_task_work_pending(struct io_ring_ctx *ctx)\n{\n\treturn task_work_pending(current) || !wq_list_empty(&ctx->work_llist);\n}\n\nstatic inline void io_tw_lock(struct io_ring_ctx *ctx, struct io_tw_state *ts)\n{\n\tif (!ts->locked) {\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tts->locked = true;\n\t}\n}\n\n \nstatic inline void io_req_complete_defer(struct io_kiocb *req)\n\t__must_hold(&req->ctx->uring_lock)\n{\n\tstruct io_submit_state *state = &req->ctx->submit_state;\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\twq_list_add_tail(&req->comp_list, &state->compl_reqs);\n}\n\nstatic inline void io_commit_cqring_flush(struct io_ring_ctx *ctx)\n{\n\tif (unlikely(ctx->off_timeout_used || ctx->drain_active ||\n\t\t     ctx->has_evfd || ctx->poll_activated))\n\t\t__io_commit_cqring_flush(ctx);\n}\n\nstatic inline void io_get_task_refs(int nr)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\ttctx->cached_refs -= nr;\n\tif (unlikely(tctx->cached_refs < 0))\n\t\tio_task_refs_refill(tctx);\n}\n\nstatic inline bool io_req_cache_empty(struct io_ring_ctx *ctx)\n{\n\treturn !ctx->submit_state.free_list.next;\n}\n\nextern struct kmem_cache *req_cachep;\n\nstatic inline struct io_kiocb *io_extract_req(struct io_ring_ctx *ctx)\n{\n\tstruct io_kiocb *req;\n\n\treq = container_of(ctx->submit_state.free_list.next, struct io_kiocb, comp_list);\n\twq_stack_extract(&ctx->submit_state.free_list);\n\treturn req;\n}\n\nstatic inline bool io_alloc_req(struct io_ring_ctx *ctx, struct io_kiocb **req)\n{\n\tif (unlikely(io_req_cache_empty(ctx))) {\n\t\tif (!__io_alloc_req_refill(ctx))\n\t\t\treturn false;\n\t}\n\t*req = io_extract_req(ctx);\n\treturn true;\n}\n\nstatic inline bool io_allowed_defer_tw_run(struct io_ring_ctx *ctx)\n{\n\treturn likely(ctx->submitter_task == current);\n}\n\nstatic inline bool io_allowed_run_tw(struct io_ring_ctx *ctx)\n{\n\treturn likely(!(ctx->flags & IORING_SETUP_DEFER_TASKRUN) ||\n\t\t      ctx->submitter_task == current);\n}\n\nstatic inline void io_req_queue_tw_complete(struct io_kiocb *req, s32 res)\n{\n\tio_req_set_res(req, res, 0);\n\treq->io_task_work.func = io_req_task_complete;\n\tio_req_task_work_add(req);\n}\n\n \nstatic inline size_t uring_sqe_size(struct io_ring_ctx *ctx)\n{\n\tif (ctx->flags & IORING_SETUP_SQE128)\n\t\treturn 2 * sizeof(struct io_uring_sqe);\n\treturn sizeof(struct io_uring_sqe);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}