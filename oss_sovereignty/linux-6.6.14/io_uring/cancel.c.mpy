{
  "module_name": "cancel.c",
  "hash_id": "1ba3197c636cd424a16e25162ffa5d18070985833ed10c4b756d5496485f4720",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/cancel.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/namei.h>\n#include <linux/nospec.h>\n#include <linux/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"tctx.h\"\n#include \"poll.h\"\n#include \"timeout.h\"\n#include \"cancel.h\"\n\nstruct io_cancel {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\tu32\t\t\t\tflags;\n\ts32\t\t\t\tfd;\n\tu8\t\t\t\topcode;\n};\n\n#define CANCEL_FLAGS\t(IORING_ASYNC_CANCEL_ALL | IORING_ASYNC_CANCEL_FD | \\\n\t\t\t IORING_ASYNC_CANCEL_ANY | IORING_ASYNC_CANCEL_FD_FIXED | \\\n\t\t\t IORING_ASYNC_CANCEL_USERDATA | IORING_ASYNC_CANCEL_OP)\n\n \nbool io_cancel_req_match(struct io_kiocb *req, struct io_cancel_data *cd)\n{\n\tbool match_user_data = cd->flags & IORING_ASYNC_CANCEL_USERDATA;\n\n\tif (req->ctx != cd->ctx)\n\t\treturn false;\n\n\tif (!(cd->flags & (IORING_ASYNC_CANCEL_FD | IORING_ASYNC_CANCEL_OP)))\n\t\tmatch_user_data = true;\n\n\tif (cd->flags & IORING_ASYNC_CANCEL_ANY)\n\t\tgoto check_seq;\n\tif (cd->flags & IORING_ASYNC_CANCEL_FD) {\n\t\tif (req->file != cd->file)\n\t\t\treturn false;\n\t}\n\tif (cd->flags & IORING_ASYNC_CANCEL_OP) {\n\t\tif (req->opcode != cd->opcode)\n\t\t\treturn false;\n\t}\n\tif (match_user_data && req->cqe.user_data != cd->data)\n\t\treturn false;\n\tif (cd->flags & IORING_ASYNC_CANCEL_ALL) {\ncheck_seq:\n\t\tif (cd->seq == req->work.cancel_seq)\n\t\t\treturn false;\n\t\treq->work.cancel_seq = cd->seq;\n\t}\n\n\treturn true;\n}\n\nstatic bool io_cancel_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_cancel_data *cd = data;\n\n\treturn io_cancel_req_match(req, cd);\n}\n\nstatic int io_async_cancel_one(struct io_uring_task *tctx,\n\t\t\t       struct io_cancel_data *cd)\n{\n\tenum io_wq_cancel cancel_ret;\n\tint ret = 0;\n\tbool all;\n\n\tif (!tctx || !tctx->io_wq)\n\t\treturn -ENOENT;\n\n\tall = cd->flags & (IORING_ASYNC_CANCEL_ALL|IORING_ASYNC_CANCEL_ANY);\n\tcancel_ret = io_wq_cancel_cb(tctx->io_wq, io_cancel_cb, cd, all);\n\tswitch (cancel_ret) {\n\tcase IO_WQ_CANCEL_OK:\n\t\tret = 0;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_RUNNING:\n\t\tret = -EALREADY;\n\t\tbreak;\n\tcase IO_WQ_CANCEL_NOTFOUND:\n\t\tret = -ENOENT;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint io_try_cancel(struct io_uring_task *tctx, struct io_cancel_data *cd,\n\t\t  unsigned issue_flags)\n{\n\tstruct io_ring_ctx *ctx = cd->ctx;\n\tint ret;\n\n\tWARN_ON_ONCE(!io_wq_current_is_worker() && tctx != current->io_uring);\n\n\tret = io_async_cancel_one(tctx, cd);\n\t \n\tif (!ret)\n\t\treturn 0;\n\n\tret = io_poll_cancel(ctx, cd, issue_flags);\n\tif (ret != -ENOENT)\n\t\treturn ret;\n\n\tspin_lock(&ctx->completion_lock);\n\tif (!(cd->flags & IORING_ASYNC_CANCEL_FD))\n\t\tret = io_timeout_cancel(ctx, cd);\n\tspin_unlock(&ctx->completion_lock);\n\treturn ret;\n}\n\nint io_async_cancel_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_cancel *cancel = io_kiocb_to_cmd(req, struct io_cancel);\n\n\tif (unlikely(req->flags & REQ_F_BUFFER_SELECT))\n\t\treturn -EINVAL;\n\tif (sqe->off || sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\n\tcancel->addr = READ_ONCE(sqe->addr);\n\tcancel->flags = READ_ONCE(sqe->cancel_flags);\n\tif (cancel->flags & ~CANCEL_FLAGS)\n\t\treturn -EINVAL;\n\tif (cancel->flags & IORING_ASYNC_CANCEL_FD) {\n\t\tif (cancel->flags & IORING_ASYNC_CANCEL_ANY)\n\t\t\treturn -EINVAL;\n\t\tcancel->fd = READ_ONCE(sqe->fd);\n\t}\n\tif (cancel->flags & IORING_ASYNC_CANCEL_OP) {\n\t\tif (cancel->flags & IORING_ASYNC_CANCEL_ANY)\n\t\t\treturn -EINVAL;\n\t\tcancel->opcode = READ_ONCE(sqe->len);\n\t}\n\n\treturn 0;\n}\n\nstatic int __io_async_cancel(struct io_cancel_data *cd,\n\t\t\t     struct io_uring_task *tctx,\n\t\t\t     unsigned int issue_flags)\n{\n\tbool all = cd->flags & (IORING_ASYNC_CANCEL_ALL|IORING_ASYNC_CANCEL_ANY);\n\tstruct io_ring_ctx *ctx = cd->ctx;\n\tstruct io_tctx_node *node;\n\tint ret, nr = 0;\n\n\tdo {\n\t\tret = io_try_cancel(tctx, cd, issue_flags);\n\t\tif (ret == -ENOENT)\n\t\t\tbreak;\n\t\tif (!all)\n\t\t\treturn ret;\n\t\tnr++;\n\t} while (1);\n\n\t \n\tio_ring_submit_lock(ctx, issue_flags);\n\tret = -ENOENT;\n\tlist_for_each_entry(node, &ctx->tctx_list, ctx_node) {\n\t\tstruct io_uring_task *tctx = node->task->io_uring;\n\n\t\tret = io_async_cancel_one(tctx, cd);\n\t\tif (ret != -ENOENT) {\n\t\t\tif (!all)\n\t\t\t\tbreak;\n\t\t\tnr++;\n\t\t}\n\t}\n\tio_ring_submit_unlock(ctx, issue_flags);\n\treturn all ? nr : ret;\n}\n\nint io_async_cancel(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_cancel *cancel = io_kiocb_to_cmd(req, struct io_cancel);\n\tstruct io_cancel_data cd = {\n\t\t.ctx\t= req->ctx,\n\t\t.data\t= cancel->addr,\n\t\t.flags\t= cancel->flags,\n\t\t.opcode\t= cancel->opcode,\n\t\t.seq\t= atomic_inc_return(&req->ctx->cancel_seq),\n\t};\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tint ret;\n\n\tif (cd.flags & IORING_ASYNC_CANCEL_FD) {\n\t\tif (req->flags & REQ_F_FIXED_FILE ||\n\t\t    cd.flags & IORING_ASYNC_CANCEL_FD_FIXED) {\n\t\t\treq->flags |= REQ_F_FIXED_FILE;\n\t\t\treq->file = io_file_get_fixed(req, cancel->fd,\n\t\t\t\t\t\t\tissue_flags);\n\t\t} else {\n\t\t\treq->file = io_file_get_normal(req, cancel->fd);\n\t\t}\n\t\tif (!req->file) {\n\t\t\tret = -EBADF;\n\t\t\tgoto done;\n\t\t}\n\t\tcd.file = req->file;\n\t}\n\n\tret = __io_async_cancel(&cd, tctx, issue_flags);\ndone:\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}\n\nvoid init_hash_table(struct io_hash_table *table, unsigned size)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < size; i++) {\n\t\tspin_lock_init(&table->hbs[i].lock);\n\t\tINIT_HLIST_HEAD(&table->hbs[i].list);\n\t}\n}\n\nstatic int __io_sync_cancel(struct io_uring_task *tctx,\n\t\t\t    struct io_cancel_data *cd, int fd)\n{\n\tstruct io_ring_ctx *ctx = cd->ctx;\n\n\t \n\tif ((cd->flags & IORING_ASYNC_CANCEL_FD) &&\n\t    (cd->flags & IORING_ASYNC_CANCEL_FD_FIXED)) {\n\t\tif (unlikely(fd >= ctx->nr_user_files))\n\t\t\treturn -EBADF;\n\t\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\t\tcd->file = io_file_from_index(&ctx->file_table, fd);\n\t\tif (!cd->file)\n\t\t\treturn -EBADF;\n\t}\n\n\treturn __io_async_cancel(cd, tctx, 0);\n}\n\nint io_sync_cancel(struct io_ring_ctx *ctx, void __user *arg)\n\t__must_hold(&ctx->uring_lock)\n{\n\tstruct io_cancel_data cd = {\n\t\t.ctx\t= ctx,\n\t\t.seq\t= atomic_inc_return(&ctx->cancel_seq),\n\t};\n\tktime_t timeout = KTIME_MAX;\n\tstruct io_uring_sync_cancel_reg sc;\n\tstruct file *file = NULL;\n\tDEFINE_WAIT(wait);\n\tint ret, i;\n\n\tif (copy_from_user(&sc, arg, sizeof(sc)))\n\t\treturn -EFAULT;\n\tif (sc.flags & ~CANCEL_FLAGS)\n\t\treturn -EINVAL;\n\tfor (i = 0; i < ARRAY_SIZE(sc.pad); i++)\n\t\tif (sc.pad[i])\n\t\t\treturn -EINVAL;\n\tfor (i = 0; i < ARRAY_SIZE(sc.pad2); i++)\n\t\tif (sc.pad2[i])\n\t\t\treturn -EINVAL;\n\n\tcd.data = sc.addr;\n\tcd.flags = sc.flags;\n\tcd.opcode = sc.opcode;\n\n\t \n\tif ((cd.flags & IORING_ASYNC_CANCEL_FD) &&\n\t   !(cd.flags & IORING_ASYNC_CANCEL_FD_FIXED)) {\n\t\tfile = fget(sc.fd);\n\t\tif (!file)\n\t\t\treturn -EBADF;\n\t\tcd.file = file;\n\t}\n\n\tret = __io_sync_cancel(current->io_uring, &cd, sc.fd);\n\n\t \n\tif (ret != -EALREADY)\n\t\tgoto out;\n\n\tif (sc.timeout.tv_sec != -1UL || sc.timeout.tv_nsec != -1UL) {\n\t\tstruct timespec64 ts = {\n\t\t\t.tv_sec\t\t= sc.timeout.tv_sec,\n\t\t\t.tv_nsec\t= sc.timeout.tv_nsec\n\t\t};\n\n\t\ttimeout = ktime_add_ns(timespec64_to_ktime(ts), ktime_get_ns());\n\t}\n\n\t \n\tdo {\n\t\tcd.seq = atomic_inc_return(&ctx->cancel_seq);\n\n\t\tprepare_to_wait(&ctx->cq_wait, &wait, TASK_INTERRUPTIBLE);\n\n\t\tret = __io_sync_cancel(current->io_uring, &cd, sc.fd);\n\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tif (ret != -EALREADY)\n\t\t\tbreak;\n\n\t\tret = io_run_task_work_sig(ctx);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tret = schedule_hrtimeout(&timeout, HRTIMER_MODE_ABS);\n\t\tif (!ret) {\n\t\t\tret = -ETIME;\n\t\t\tbreak;\n\t\t}\n\t\tmutex_lock(&ctx->uring_lock);\n\t} while (1);\n\n\tfinish_wait(&ctx->cq_wait, &wait);\n\tmutex_lock(&ctx->uring_lock);\n\n\tif (ret == -ENOENT || ret > 0)\n\t\tret = 0;\nout:\n\tif (file)\n\t\tfput(file);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}