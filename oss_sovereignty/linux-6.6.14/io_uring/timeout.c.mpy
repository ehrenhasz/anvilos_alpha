{
  "module_name": "timeout.c",
  "hash_id": "63125e5b34e04712b92090cbac855f4a491162b4cec9fa63f3f09a5754b1fd98",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/timeout.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/file.h>\n#include <linux/io_uring.h>\n\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"refs.h\"\n#include \"cancel.h\"\n#include \"timeout.h\"\n\nstruct io_timeout {\n\tstruct file\t\t\t*file;\n\tu32\t\t\t\toff;\n\tu32\t\t\t\ttarget_seq;\n\tu32\t\t\t\trepeats;\n\tstruct list_head\t\tlist;\n\t \n\tstruct io_kiocb\t\t\t*head;\n\t \n\tstruct io_kiocb\t\t\t*prev;\n};\n\nstruct io_timeout_rem {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\taddr;\n\n\t \n\tstruct timespec64\t\tts;\n\tu32\t\t\t\tflags;\n\tbool\t\t\t\tltimeout;\n};\n\nstatic inline bool io_is_timeout_noseq(struct io_kiocb *req)\n{\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_timeout_data *data = req->async_data;\n\n\treturn !timeout->off || data->flags & IORING_TIMEOUT_MULTISHOT;\n}\n\nstatic inline void io_put_req(struct io_kiocb *req)\n{\n\tif (req_ref_put_and_test(req)) {\n\t\tio_queue_next(req);\n\t\tio_free_req(req);\n\t}\n}\n\nstatic inline bool io_timeout_finish(struct io_timeout *timeout,\n\t\t\t\t     struct io_timeout_data *data)\n{\n\tif (!(data->flags & IORING_TIMEOUT_MULTISHOT))\n\t\treturn true;\n\n\tif (!timeout->off || (timeout->repeats && --timeout->repeats))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic enum hrtimer_restart io_timeout_fn(struct hrtimer *timer);\n\nstatic void io_timeout_complete(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_timeout_data *data = req->async_data;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!io_timeout_finish(timeout, data)) {\n\t\tbool filled;\n\t\tfilled = io_fill_cqe_req_aux(req, ts->locked, -ETIME,\n\t\t\t\t\t     IORING_CQE_F_MORE);\n\t\tif (filled) {\n\t\t\t \n\t\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\t\tlist_add(&timeout->list, ctx->timeout_list.prev);\n\t\t\tdata->timer.function = io_timeout_fn;\n\t\t\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);\n\t\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tio_req_task_complete(req, ts);\n}\n\nstatic bool io_kill_timeout(struct io_kiocb *req, int status)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tstruct io_timeout_data *io = req->async_data;\n\n\tif (hrtimer_try_to_cancel(&io->timer) != -1) {\n\t\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\n\t\tif (status)\n\t\t\treq_set_fail(req);\n\t\tatomic_set(&req->ctx->cq_timeouts,\n\t\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\t\tlist_del_init(&timeout->list);\n\t\tio_req_queue_tw_complete(req, status);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n__cold void io_flush_timeouts(struct io_ring_ctx *ctx)\n{\n\tu32 seq;\n\tstruct io_timeout *timeout, *tmp;\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\tseq = ctx->cached_cq_tail - atomic_read(&ctx->cq_timeouts);\n\n\tlist_for_each_entry_safe(timeout, tmp, &ctx->timeout_list, list) {\n\t\tstruct io_kiocb *req = cmd_to_io_kiocb(timeout);\n\t\tu32 events_needed, events_got;\n\n\t\tif (io_is_timeout_noseq(req))\n\t\t\tbreak;\n\n\t\t \n\t\tevents_needed = timeout->target_seq - ctx->cq_last_tm_flush;\n\t\tevents_got = seq - ctx->cq_last_tm_flush;\n\t\tif (events_got < events_needed)\n\t\t\tbreak;\n\n\t\tio_kill_timeout(req, 0);\n\t}\n\tctx->cq_last_tm_flush = seq;\n\tspin_unlock_irq(&ctx->timeout_lock);\n}\n\nstatic void io_req_tw_fail_links(struct io_kiocb *link, struct io_tw_state *ts)\n{\n\tio_tw_lock(link->ctx, ts);\n\twhile (link) {\n\t\tstruct io_kiocb *nxt = link->link;\n\t\tlong res = -ECANCELED;\n\n\t\tif (link->flags & REQ_F_FAIL)\n\t\t\tres = link->cqe.res;\n\t\tlink->link = NULL;\n\t\tio_req_set_res(link, res, 0);\n\t\tio_req_task_complete(link, ts);\n\t\tlink = nxt;\n\t}\n}\n\nstatic void io_fail_links(struct io_kiocb *req)\n\t__must_hold(&req->ctx->completion_lock)\n{\n\tstruct io_kiocb *link = req->link;\n\tbool ignore_cqes = req->flags & REQ_F_SKIP_LINK_CQES;\n\n\tif (!link)\n\t\treturn;\n\n\twhile (link) {\n\t\tif (ignore_cqes)\n\t\t\tlink->flags |= REQ_F_CQE_SKIP;\n\t\telse\n\t\t\tlink->flags &= ~REQ_F_CQE_SKIP;\n\t\ttrace_io_uring_fail_link(req, link);\n\t\tlink = link->link;\n\t}\n\n\tlink = req->link;\n\tlink->io_task_work.func = io_req_tw_fail_links;\n\tio_req_task_work_add(link);\n\treq->link = NULL;\n}\n\nstatic inline void io_remove_next_linked(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = req->link;\n\n\treq->link = nxt->link;\n\tnxt->link = NULL;\n}\n\nvoid io_disarm_next(struct io_kiocb *req)\n\t__must_hold(&req->ctx->completion_lock)\n{\n\tstruct io_kiocb *link = NULL;\n\n\tif (req->flags & REQ_F_ARM_LTIMEOUT) {\n\t\tlink = req->link;\n\t\treq->flags &= ~REQ_F_ARM_LTIMEOUT;\n\t\tif (link && link->opcode == IORING_OP_LINK_TIMEOUT) {\n\t\t\tio_remove_next_linked(req);\n\t\t\tio_req_queue_tw_complete(link, -ECANCELED);\n\t\t}\n\t} else if (req->flags & REQ_F_LINK_TIMEOUT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\tlink = io_disarm_linked_timeout(req);\n\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t\tif (link)\n\t\t\tio_req_queue_tw_complete(link, -ECANCELED);\n\t}\n\tif (unlikely((req->flags & REQ_F_FAIL) &&\n\t\t     !(req->flags & REQ_F_HARDLINK)))\n\t\tio_fail_links(req);\n}\n\nstruct io_kiocb *__io_disarm_linked_timeout(struct io_kiocb *req,\n\t\t\t\t\t    struct io_kiocb *link)\n\t__must_hold(&req->ctx->completion_lock)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tstruct io_timeout_data *io = link->async_data;\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(link, struct io_timeout);\n\n\tio_remove_next_linked(req);\n\ttimeout->head = NULL;\n\tif (hrtimer_try_to_cancel(&io->timer) != -1) {\n\t\tlist_del(&timeout->list);\n\t\treturn link;\n\t}\n\n\treturn NULL;\n}\n\nstatic enum hrtimer_restart io_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *req = data->req;\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->timeout_lock, flags);\n\tlist_del_init(&timeout->list);\n\tatomic_set(&req->ctx->cq_timeouts,\n\t\tatomic_read(&req->ctx->cq_timeouts) + 1);\n\tspin_unlock_irqrestore(&ctx->timeout_lock, flags);\n\n\tif (!(data->flags & IORING_TIMEOUT_ETIME_SUCCESS))\n\t\treq_set_fail(req);\n\n\tio_req_set_res(req, -ETIME, 0);\n\treq->io_task_work.func = io_timeout_complete;\n\tio_req_task_work_add(req);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic struct io_kiocb *io_timeout_extract(struct io_ring_ctx *ctx,\n\t\t\t\t\t   struct io_cancel_data *cd)\n\t__must_hold(&ctx->timeout_lock)\n{\n\tstruct io_timeout *timeout;\n\tstruct io_timeout_data *io;\n\tstruct io_kiocb *req = NULL;\n\n\tlist_for_each_entry(timeout, &ctx->timeout_list, list) {\n\t\tstruct io_kiocb *tmp = cmd_to_io_kiocb(timeout);\n\n\t\tif (io_cancel_req_match(tmp, cd)) {\n\t\t\treq = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!req)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tio = req->async_data;\n\tif (hrtimer_try_to_cancel(&io->timer) == -1)\n\t\treturn ERR_PTR(-EALREADY);\n\ttimeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tlist_del_init(&timeout->list);\n\treturn req;\n}\n\nint io_timeout_cancel(struct io_ring_ctx *ctx, struct io_cancel_data *cd)\n\t__must_hold(&ctx->completion_lock)\n{\n\tstruct io_kiocb *req;\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\treq = io_timeout_extract(ctx, cd);\n\tspin_unlock_irq(&ctx->timeout_lock);\n\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\tio_req_task_queue_fail(req, -ECANCELED);\n\treturn 0;\n}\n\nstatic void io_req_task_link_timeout(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tunsigned issue_flags = ts->locked ? 0 : IO_URING_F_UNLOCKED;\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_kiocb *prev = timeout->prev;\n\tint ret = -ENOENT;\n\n\tif (prev) {\n\t\tif (!(req->task->flags & PF_EXITING)) {\n\t\t\tstruct io_cancel_data cd = {\n\t\t\t\t.ctx\t\t= req->ctx,\n\t\t\t\t.data\t\t= prev->cqe.user_data,\n\t\t\t};\n\n\t\t\tret = io_try_cancel(req->task->io_uring, &cd, issue_flags);\n\t\t}\n\t\tio_req_set_res(req, ret ?: -ETIME, 0);\n\t\tio_req_task_complete(req, ts);\n\t\tio_put_req(prev);\n\t} else {\n\t\tio_req_set_res(req, -ETIME, 0);\n\t\tio_req_task_complete(req, ts);\n\t}\n}\n\nstatic enum hrtimer_restart io_link_timeout_fn(struct hrtimer *timer)\n{\n\tstruct io_timeout_data *data = container_of(timer,\n\t\t\t\t\t\tstruct io_timeout_data, timer);\n\tstruct io_kiocb *prev, *req = data->req;\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->timeout_lock, flags);\n\tprev = timeout->head;\n\ttimeout->head = NULL;\n\n\t \n\tif (prev) {\n\t\tio_remove_next_linked(prev);\n\t\tif (!req_ref_inc_not_zero(prev))\n\t\t\tprev = NULL;\n\t}\n\tlist_del(&timeout->list);\n\ttimeout->prev = prev;\n\tspin_unlock_irqrestore(&ctx->timeout_lock, flags);\n\n\treq->io_task_work.func = io_req_task_link_timeout;\n\tio_req_task_work_add(req);\n\treturn HRTIMER_NORESTART;\n}\n\nstatic clockid_t io_timeout_get_clock(struct io_timeout_data *data)\n{\n\tswitch (data->flags & IORING_TIMEOUT_CLOCK_MASK) {\n\tcase IORING_TIMEOUT_BOOTTIME:\n\t\treturn CLOCK_BOOTTIME;\n\tcase IORING_TIMEOUT_REALTIME:\n\t\treturn CLOCK_REALTIME;\n\tdefault:\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t\tfallthrough;\n\tcase 0:\n\t\treturn CLOCK_MONOTONIC;\n\t}\n}\n\nstatic int io_linked_timeout_update(struct io_ring_ctx *ctx, __u64 user_data,\n\t\t\t\t    struct timespec64 *ts, enum hrtimer_mode mode)\n\t__must_hold(&ctx->timeout_lock)\n{\n\tstruct io_timeout_data *io;\n\tstruct io_timeout *timeout;\n\tstruct io_kiocb *req = NULL;\n\n\tlist_for_each_entry(timeout, &ctx->ltimeout_list, list) {\n\t\tstruct io_kiocb *tmp = cmd_to_io_kiocb(timeout);\n\n\t\tif (user_data == tmp->cqe.user_data) {\n\t\t\treq = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!req)\n\t\treturn -ENOENT;\n\n\tio = req->async_data;\n\tif (hrtimer_try_to_cancel(&io->timer) == -1)\n\t\treturn -EALREADY;\n\thrtimer_init(&io->timer, io_timeout_get_clock(io), mode);\n\tio->timer.function = io_link_timeout_fn;\n\thrtimer_start(&io->timer, timespec64_to_ktime(*ts), mode);\n\treturn 0;\n}\n\nstatic int io_timeout_update(struct io_ring_ctx *ctx, __u64 user_data,\n\t\t\t     struct timespec64 *ts, enum hrtimer_mode mode)\n\t__must_hold(&ctx->timeout_lock)\n{\n\tstruct io_cancel_data cd = { .ctx = ctx, .data = user_data, };\n\tstruct io_kiocb *req = io_timeout_extract(ctx, &cd);\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_timeout_data *data;\n\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\n\ttimeout->off = 0;  \n\tdata = req->async_data;\n\tlist_add_tail(&timeout->list, &ctx->timeout_list);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), mode);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(*ts), mode);\n\treturn 0;\n}\n\nint io_timeout_remove_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_timeout_rem *tr = io_kiocb_to_cmd(req, struct io_timeout_rem);\n\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->buf_index || sqe->len || sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\n\ttr->ltimeout = false;\n\ttr->addr = READ_ONCE(sqe->addr);\n\ttr->flags = READ_ONCE(sqe->timeout_flags);\n\tif (tr->flags & IORING_TIMEOUT_UPDATE_MASK) {\n\t\tif (hweight32(tr->flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\t\treturn -EINVAL;\n\t\tif (tr->flags & IORING_LINK_TIMEOUT_UPDATE)\n\t\t\ttr->ltimeout = true;\n\t\tif (tr->flags & ~(IORING_TIMEOUT_UPDATE_MASK|IORING_TIMEOUT_ABS))\n\t\t\treturn -EINVAL;\n\t\tif (get_timespec64(&tr->ts, u64_to_user_ptr(sqe->addr2)))\n\t\t\treturn -EFAULT;\n\t\tif (tr->ts.tv_sec < 0 || tr->ts.tv_nsec < 0)\n\t\t\treturn -EINVAL;\n\t} else if (tr->flags) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic inline enum hrtimer_mode io_translate_timeout_mode(unsigned int flags)\n{\n\treturn (flags & IORING_TIMEOUT_ABS) ? HRTIMER_MODE_ABS\n\t\t\t\t\t    : HRTIMER_MODE_REL;\n}\n\n \nint io_timeout_remove(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_timeout_rem *tr = io_kiocb_to_cmd(req, struct io_timeout_rem);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tint ret;\n\n\tif (!(tr->flags & IORING_TIMEOUT_UPDATE)) {\n\t\tstruct io_cancel_data cd = { .ctx = ctx, .data = tr->addr, };\n\n\t\tspin_lock(&ctx->completion_lock);\n\t\tret = io_timeout_cancel(ctx, &cd);\n\t\tspin_unlock(&ctx->completion_lock);\n\t} else {\n\t\tenum hrtimer_mode mode = io_translate_timeout_mode(tr->flags);\n\n\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\tif (tr->ltimeout)\n\t\t\tret = io_linked_timeout_update(ctx, tr->addr, &tr->ts, mode);\n\t\telse\n\t\t\tret = io_timeout_update(ctx, tr->addr, &tr->ts, mode);\n\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t}\n\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}\n\nstatic int __io_timeout_prep(struct io_kiocb *req,\n\t\t\t     const struct io_uring_sqe *sqe,\n\t\t\t     bool is_timeout_link)\n{\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_timeout_data *data;\n\tunsigned flags;\n\tu32 off = READ_ONCE(sqe->off);\n\n\tif (sqe->buf_index || sqe->len != 1 || sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\tif (off && is_timeout_link)\n\t\treturn -EINVAL;\n\tflags = READ_ONCE(sqe->timeout_flags);\n\tif (flags & ~(IORING_TIMEOUT_ABS | IORING_TIMEOUT_CLOCK_MASK |\n\t\t      IORING_TIMEOUT_ETIME_SUCCESS |\n\t\t      IORING_TIMEOUT_MULTISHOT))\n\t\treturn -EINVAL;\n\t \n\tif (hweight32(flags & IORING_TIMEOUT_CLOCK_MASK) > 1)\n\t\treturn -EINVAL;\n\t \n\tif (!(~flags & (IORING_TIMEOUT_MULTISHOT | IORING_TIMEOUT_ABS)))\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&timeout->list);\n\ttimeout->off = off;\n\tif (unlikely(off && !req->ctx->off_timeout_used))\n\t\treq->ctx->off_timeout_used = true;\n\t \n\ttimeout->repeats = 0;\n\tif ((flags & IORING_TIMEOUT_MULTISHOT) && off > 0)\n\t\ttimeout->repeats = off;\n\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (io_alloc_async_data(req))\n\t\treturn -ENOMEM;\n\n\tdata = req->async_data;\n\tdata->req = req;\n\tdata->flags = flags;\n\n\tif (get_timespec64(&data->ts, u64_to_user_ptr(sqe->addr)))\n\t\treturn -EFAULT;\n\n\tif (data->ts.tv_sec < 0 || data->ts.tv_nsec < 0)\n\t\treturn -EINVAL;\n\n\tINIT_LIST_HEAD(&timeout->list);\n\tdata->mode = io_translate_timeout_mode(flags);\n\thrtimer_init(&data->timer, io_timeout_get_clock(data), data->mode);\n\n\tif (is_timeout_link) {\n\t\tstruct io_submit_link *link = &req->ctx->submit_state.link;\n\n\t\tif (!link->head)\n\t\t\treturn -EINVAL;\n\t\tif (link->last->opcode == IORING_OP_LINK_TIMEOUT)\n\t\t\treturn -EINVAL;\n\t\ttimeout->head = link->last;\n\t\tlink->last->flags |= REQ_F_ARM_LTIMEOUT;\n\t}\n\treturn 0;\n}\n\nint io_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn __io_timeout_prep(req, sqe, false);\n}\n\nint io_link_timeout_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\treturn __io_timeout_prep(req, sqe, true);\n}\n\nint io_timeout(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_timeout_data *data = req->async_data;\n\tstruct list_head *entry;\n\tu32 tail, off = timeout->off;\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\n\t \n\tif (io_is_timeout_noseq(req)) {\n\t\tentry = ctx->timeout_list.prev;\n\t\tgoto add;\n\t}\n\n\ttail = data_race(ctx->cached_cq_tail) - atomic_read(&ctx->cq_timeouts);\n\ttimeout->target_seq = tail + off;\n\n\t \n\tctx->cq_last_tm_flush = tail;\n\n\t \n\tlist_for_each_prev(entry, &ctx->timeout_list) {\n\t\tstruct io_timeout *nextt = list_entry(entry, struct io_timeout, list);\n\t\tstruct io_kiocb *nxt = cmd_to_io_kiocb(nextt);\n\n\t\tif (io_is_timeout_noseq(nxt))\n\t\t\tcontinue;\n\t\t \n\t\tif (off >= nextt->target_seq - tail)\n\t\t\tbreak;\n\t}\nadd:\n\tlist_add(&timeout->list, entry);\n\tdata->timer.function = io_timeout_fn;\n\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts), data->mode);\n\tspin_unlock_irq(&ctx->timeout_lock);\n\treturn IOU_ISSUE_SKIP_COMPLETE;\n}\n\nvoid io_queue_linked_timeout(struct io_kiocb *req)\n{\n\tstruct io_timeout *timeout = io_kiocb_to_cmd(req, struct io_timeout);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock_irq(&ctx->timeout_lock);\n\t \n\tif (timeout->head) {\n\t\tstruct io_timeout_data *data = req->async_data;\n\n\t\tdata->timer.function = io_link_timeout_fn;\n\t\thrtimer_start(&data->timer, timespec64_to_ktime(data->ts),\n\t\t\t\tdata->mode);\n\t\tlist_add_tail(&timeout->list, &ctx->ltimeout_list);\n\t}\n\tspin_unlock_irq(&ctx->timeout_lock);\n\t \n\tio_put_req(req);\n}\n\nstatic bool io_match_task(struct io_kiocb *head, struct task_struct *task,\n\t\t\t  bool cancel_all)\n\t__must_hold(&req->ctx->timeout_lock)\n{\n\tstruct io_kiocb *req;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (cancel_all)\n\t\treturn true;\n\n\tio_for_each_link(req, head) {\n\t\tif (req->flags & REQ_F_INFLIGHT)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \n__cold bool io_kill_timeouts(struct io_ring_ctx *ctx, struct task_struct *tsk,\n\t\t\t     bool cancel_all)\n{\n\tstruct io_timeout *timeout, *tmp;\n\tint canceled = 0;\n\n\t \n\tspin_lock(&ctx->completion_lock);\n\tspin_lock_irq(&ctx->timeout_lock);\n\tlist_for_each_entry_safe(timeout, tmp, &ctx->timeout_list, list) {\n\t\tstruct io_kiocb *req = cmd_to_io_kiocb(timeout);\n\n\t\tif (io_match_task(req, tsk, cancel_all) &&\n\t\t    io_kill_timeout(req, -ECANCELED))\n\t\t\tcanceled++;\n\t}\n\tspin_unlock_irq(&ctx->timeout_lock);\n\tspin_unlock(&ctx->completion_lock);\n\treturn canceled != 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}