{
  "module_name": "tctx.c",
  "hash_id": "4fdb0e47f06db871c47b33f54b4174da541feb8d9636fb0c576dbfd7b43a4586",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/tctx.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/nospec.h>\n#include <linux/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"tctx.h\"\n\nstatic struct io_wq *io_init_wq_offload(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct task_struct *task)\n{\n\tstruct io_wq_hash *hash;\n\tstruct io_wq_data data;\n\tunsigned int concurrency;\n\n\tmutex_lock(&ctx->uring_lock);\n\thash = ctx->hash_map;\n\tif (!hash) {\n\t\thash = kzalloc(sizeof(*hash), GFP_KERNEL);\n\t\tif (!hash) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\t\trefcount_set(&hash->refs, 1);\n\t\tinit_waitqueue_head(&hash->wait);\n\t\tctx->hash_map = hash;\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n\n\tdata.hash = hash;\n\tdata.task = task;\n\tdata.free_work = io_wq_free_work;\n\tdata.do_work = io_wq_submit_work;\n\n\t \n\tconcurrency = min(ctx->sq_entries, 4 * num_online_cpus());\n\n\treturn io_wq_create(concurrency, &data);\n}\n\nvoid __io_uring_free(struct task_struct *tsk)\n{\n\tstruct io_uring_task *tctx = tsk->io_uring;\n\n\tWARN_ON_ONCE(!xa_empty(&tctx->xa));\n\tWARN_ON_ONCE(tctx->io_wq);\n\tWARN_ON_ONCE(tctx->cached_refs);\n\n\tpercpu_counter_destroy(&tctx->inflight);\n\tkfree(tctx);\n\ttsk->io_uring = NULL;\n}\n\n__cold int io_uring_alloc_task_context(struct task_struct *task,\n\t\t\t\t       struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx;\n\tint ret;\n\n\ttctx = kzalloc(sizeof(*tctx), GFP_KERNEL);\n\tif (unlikely(!tctx))\n\t\treturn -ENOMEM;\n\n\tret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);\n\tif (unlikely(ret)) {\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\ttctx->io_wq = io_init_wq_offload(ctx, task);\n\tif (IS_ERR(tctx->io_wq)) {\n\t\tret = PTR_ERR(tctx->io_wq);\n\t\tpercpu_counter_destroy(&tctx->inflight);\n\t\tkfree(tctx);\n\t\treturn ret;\n\t}\n\n\txa_init(&tctx->xa);\n\tinit_waitqueue_head(&tctx->wait);\n\tatomic_set(&tctx->in_cancel, 0);\n\tatomic_set(&tctx->inflight_tracked, 0);\n\ttask->io_uring = tctx;\n\tinit_llist_head(&tctx->task_list);\n\tinit_task_work(&tctx->task_work, tctx_task_work);\n\treturn 0;\n}\n\nint __io_uring_add_tctx_node(struct io_ring_ctx *ctx)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_node *node;\n\tint ret;\n\n\tif (unlikely(!tctx)) {\n\t\tret = io_uring_alloc_task_context(current, ctx);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\n\t\ttctx = current->io_uring;\n\t\tif (ctx->iowq_limits_set) {\n\t\t\tunsigned int limits[2] = { ctx->iowq_limits[0],\n\t\t\t\t\t\t   ctx->iowq_limits[1], };\n\n\t\t\tret = io_wq_max_workers(tctx->io_wq, limits);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\tif (!xa_load(&tctx->xa, (unsigned long)ctx)) {\n\t\tnode = kmalloc(sizeof(*node), GFP_KERNEL);\n\t\tif (!node)\n\t\t\treturn -ENOMEM;\n\t\tnode->ctx = ctx;\n\t\tnode->task = current;\n\n\t\tret = xa_err(xa_store(&tctx->xa, (unsigned long)ctx,\n\t\t\t\t\tnode, GFP_KERNEL));\n\t\tif (ret) {\n\t\t\tkfree(node);\n\t\t\treturn ret;\n\t\t}\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tlist_add(&node->ctx_node, &ctx->tctx_list);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\treturn 0;\n}\n\nint __io_uring_add_tctx_node_from_submit(struct io_ring_ctx *ctx)\n{\n\tint ret;\n\n\tif (ctx->flags & IORING_SETUP_SINGLE_ISSUER\n\t    && ctx->submitter_task != current)\n\t\treturn -EEXIST;\n\n\tret = __io_uring_add_tctx_node(ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tcurrent->io_uring->last = ctx;\n\treturn 0;\n}\n\n \n__cold void io_uring_del_tctx_node(unsigned long index)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_node *node;\n\n\tif (!tctx)\n\t\treturn;\n\tnode = xa_erase(&tctx->xa, index);\n\tif (!node)\n\t\treturn;\n\n\tWARN_ON_ONCE(current != node->task);\n\tWARN_ON_ONCE(list_empty(&node->ctx_node));\n\n\tmutex_lock(&node->ctx->uring_lock);\n\tlist_del(&node->ctx_node);\n\tmutex_unlock(&node->ctx->uring_lock);\n\n\tif (tctx->last == node->ctx)\n\t\ttctx->last = NULL;\n\tkfree(node);\n}\n\n__cold void io_uring_clean_tctx(struct io_uring_task *tctx)\n{\n\tstruct io_wq *wq = tctx->io_wq;\n\tstruct io_tctx_node *node;\n\tunsigned long index;\n\n\txa_for_each(&tctx->xa, index, node) {\n\t\tio_uring_del_tctx_node(index);\n\t\tcond_resched();\n\t}\n\tif (wq) {\n\t\t \n\t\tio_wq_put_and_exit(wq);\n\t\ttctx->io_wq = NULL;\n\t}\n}\n\nvoid io_uring_unreg_ringfd(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tint i;\n\n\tfor (i = 0; i < IO_RINGFD_REG_MAX; i++) {\n\t\tif (tctx->registered_rings[i]) {\n\t\t\tfput(tctx->registered_rings[i]);\n\t\t\ttctx->registered_rings[i] = NULL;\n\t\t}\n\t}\n}\n\nint io_ring_add_registered_file(struct io_uring_task *tctx, struct file *file,\n\t\t\t\t     int start, int end)\n{\n\tint offset;\n\tfor (offset = start; offset < end; offset++) {\n\t\toffset = array_index_nospec(offset, IO_RINGFD_REG_MAX);\n\t\tif (tctx->registered_rings[offset])\n\t\t\tcontinue;\n\n\t\ttctx->registered_rings[offset] = file;\n\t\treturn offset;\n\t}\n\treturn -EBUSY;\n}\n\nstatic int io_ring_add_registered_fd(struct io_uring_task *tctx, int fd,\n\t\t\t\t     int start, int end)\n{\n\tstruct file *file;\n\tint offset;\n\n\tfile = fget(fd);\n\tif (!file) {\n\t\treturn -EBADF;\n\t} else if (!io_is_uring_fops(file)) {\n\t\tfput(file);\n\t\treturn -EOPNOTSUPP;\n\t}\n\toffset = io_ring_add_registered_file(tctx, file, start, end);\n\tif (offset < 0)\n\t\tfput(file);\n\treturn offset;\n}\n\n \nint io_ringfd_register(struct io_ring_ctx *ctx, void __user *__arg,\n\t\t       unsigned nr_args)\n{\n\tstruct io_uring_rsrc_update __user *arg = __arg;\n\tstruct io_uring_rsrc_update reg;\n\tstruct io_uring_task *tctx;\n\tint ret, i;\n\n\tif (!nr_args || nr_args > IO_RINGFD_REG_MAX)\n\t\treturn -EINVAL;\n\n\tmutex_unlock(&ctx->uring_lock);\n\tret = __io_uring_add_tctx_node(ctx);\n\tmutex_lock(&ctx->uring_lock);\n\tif (ret)\n\t\treturn ret;\n\n\ttctx = current->io_uring;\n\tfor (i = 0; i < nr_args; i++) {\n\t\tint start, end;\n\n\t\tif (copy_from_user(&reg, &arg[i], sizeof(reg))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (reg.resv) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (reg.offset == -1U) {\n\t\t\tstart = 0;\n\t\t\tend = IO_RINGFD_REG_MAX;\n\t\t} else {\n\t\t\tif (reg.offset >= IO_RINGFD_REG_MAX) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstart = reg.offset;\n\t\t\tend = start + 1;\n\t\t}\n\n\t\tret = io_ring_add_registered_fd(tctx, reg.data, start, end);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\treg.offset = ret;\n\t\tif (copy_to_user(&arg[i], &reg, sizeof(reg))) {\n\t\t\tfput(tctx->registered_rings[reg.offset]);\n\t\t\ttctx->registered_rings[reg.offset] = NULL;\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn i ? i : ret;\n}\n\nint io_ringfd_unregister(struct io_ring_ctx *ctx, void __user *__arg,\n\t\t\t unsigned nr_args)\n{\n\tstruct io_uring_rsrc_update __user *arg = __arg;\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_uring_rsrc_update reg;\n\tint ret = 0, i;\n\n\tif (!nr_args || nr_args > IO_RINGFD_REG_MAX)\n\t\treturn -EINVAL;\n\tif (!tctx)\n\t\treturn 0;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tif (copy_from_user(&reg, &arg[i], sizeof(reg))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (reg.resv || reg.data || reg.offset >= IO_RINGFD_REG_MAX) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\treg.offset = array_index_nospec(reg.offset, IO_RINGFD_REG_MAX);\n\t\tif (tctx->registered_rings[reg.offset]) {\n\t\t\tfput(tctx->registered_rings[reg.offset]);\n\t\t\ttctx->registered_rings[reg.offset] = NULL;\n\t\t}\n\t}\n\n\treturn i ? i : ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}