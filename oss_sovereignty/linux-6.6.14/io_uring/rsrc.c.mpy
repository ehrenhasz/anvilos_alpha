{
  "module_name": "rsrc.c",
  "hash_id": "45d0167789fce77872394e9826ddd068c3aed9a5eb4af2dcbcdce40cae3795e6",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/rsrc.c",
  "human_readable_source": "\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/nospec.h>\n#include <linux/hugetlb.h>\n#include <linux/compat.h>\n#include <linux/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io_uring.h\"\n#include \"openclose.h\"\n#include \"rsrc.h\"\n\nstruct io_rsrc_update {\n\tstruct file\t\t\t*file;\n\tu64\t\t\t\targ;\n\tu32\t\t\t\tnr_args;\n\tu32\t\t\t\toffset;\n};\n\nstatic void io_rsrc_buf_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc);\nstatic void io_rsrc_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc);\nstatic int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\n\t\t\t\t  struct io_mapped_ubuf **pimu,\n\t\t\t\t  struct page **last_hpage);\n\n \n#define IORING_MAX_FIXED_FILES\t(1U << 20)\n#define IORING_MAX_REG_BUFFERS\t(1U << 14)\n\nstatic const struct io_mapped_ubuf dummy_ubuf = {\n\t \n\t.ubuf = -1UL,\n\t.ubuf_end = 0,\n};\n\nint __io_account_mem(struct user_struct *user, unsigned long nr_pages)\n{\n\tunsigned long page_limit, cur_pages, new_pages;\n\n\tif (!nr_pages)\n\t\treturn 0;\n\n\t \n\tpage_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\n\tcur_pages = atomic_long_read(&user->locked_vm);\n\tdo {\n\t\tnew_pages = cur_pages + nr_pages;\n\t\tif (new_pages > page_limit)\n\t\t\treturn -ENOMEM;\n\t} while (!atomic_long_try_cmpxchg(&user->locked_vm,\n\t\t\t\t\t  &cur_pages, new_pages));\n\treturn 0;\n}\n\nstatic void io_unaccount_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)\n{\n\tif (ctx->user)\n\t\t__io_unaccount_mem(ctx->user, nr_pages);\n\n\tif (ctx->mm_account)\n\t\tatomic64_sub(nr_pages, &ctx->mm_account->pinned_vm);\n}\n\nstatic int io_account_mem(struct io_ring_ctx *ctx, unsigned long nr_pages)\n{\n\tint ret;\n\n\tif (ctx->user) {\n\t\tret = __io_account_mem(ctx->user, nr_pages);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (ctx->mm_account)\n\t\tatomic64_add(nr_pages, &ctx->mm_account->pinned_vm);\n\n\treturn 0;\n}\n\nstatic int io_copy_iov(struct io_ring_ctx *ctx, struct iovec *dst,\n\t\t       void __user *arg, unsigned index)\n{\n\tstruct iovec __user *src;\n\n#ifdef CONFIG_COMPAT\n\tif (ctx->compat) {\n\t\tstruct compat_iovec __user *ciovs;\n\t\tstruct compat_iovec ciov;\n\n\t\tciovs = (struct compat_iovec __user *) arg;\n\t\tif (copy_from_user(&ciov, &ciovs[index], sizeof(ciov)))\n\t\t\treturn -EFAULT;\n\n\t\tdst->iov_base = u64_to_user_ptr((u64)ciov.iov_base);\n\t\tdst->iov_len = ciov.iov_len;\n\t\treturn 0;\n\t}\n#endif\n\tsrc = (struct iovec __user *) arg;\n\tif (copy_from_user(dst, &src[index], sizeof(*dst)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic int io_buffer_validate(struct iovec *iov)\n{\n\tunsigned long tmp, acct_len = iov->iov_len + (PAGE_SIZE - 1);\n\n\t \n\tif (!iov->iov_base)\n\t\treturn iov->iov_len ? -EFAULT : 0;\n\tif (!iov->iov_len)\n\t\treturn -EFAULT;\n\n\t \n\tif (iov->iov_len > SZ_1G)\n\t\treturn -EFAULT;\n\n\tif (check_add_overflow((unsigned long)iov->iov_base, acct_len, &tmp))\n\t\treturn -EOVERFLOW;\n\n\treturn 0;\n}\n\nstatic void io_buffer_unmap(struct io_ring_ctx *ctx, struct io_mapped_ubuf **slot)\n{\n\tstruct io_mapped_ubuf *imu = *slot;\n\tunsigned int i;\n\n\tif (imu != &dummy_ubuf) {\n\t\tfor (i = 0; i < imu->nr_bvecs; i++)\n\t\t\tunpin_user_page(imu->bvec[i].bv_page);\n\t\tif (imu->acct_pages)\n\t\t\tio_unaccount_mem(ctx, imu->acct_pages);\n\t\tkvfree(imu);\n\t}\n\t*slot = NULL;\n}\n\nstatic void io_rsrc_put_work(struct io_rsrc_node *node)\n{\n\tstruct io_rsrc_put *prsrc = &node->item;\n\n\tif (prsrc->tag)\n\t\tio_post_aux_cqe(node->ctx, prsrc->tag, 0, 0);\n\n\tswitch (node->type) {\n\tcase IORING_RSRC_FILE:\n\t\tio_rsrc_file_put(node->ctx, prsrc);\n\t\tbreak;\n\tcase IORING_RSRC_BUFFER:\n\t\tio_rsrc_buf_put(node->ctx, prsrc);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\tbreak;\n\t}\n}\n\nvoid io_rsrc_node_destroy(struct io_ring_ctx *ctx, struct io_rsrc_node *node)\n{\n\tif (!io_alloc_cache_put(&ctx->rsrc_node_cache, &node->cache))\n\t\tkfree(node);\n}\n\nvoid io_rsrc_node_ref_zero(struct io_rsrc_node *node)\n\t__must_hold(&node->ctx->uring_lock)\n{\n\tstruct io_ring_ctx *ctx = node->ctx;\n\n\twhile (!list_empty(&ctx->rsrc_ref_list)) {\n\t\tnode = list_first_entry(&ctx->rsrc_ref_list,\n\t\t\t\t\t    struct io_rsrc_node, node);\n\t\t \n\t\tif (node->refs)\n\t\t\tbreak;\n\t\tlist_del(&node->node);\n\n\t\tif (likely(!node->empty))\n\t\t\tio_rsrc_put_work(node);\n\t\tio_rsrc_node_destroy(ctx, node);\n\t}\n\tif (list_empty(&ctx->rsrc_ref_list) && unlikely(ctx->rsrc_quiesce))\n\t\twake_up_all(&ctx->rsrc_quiesce_wq);\n}\n\nstruct io_rsrc_node *io_rsrc_node_alloc(struct io_ring_ctx *ctx)\n{\n\tstruct io_rsrc_node *ref_node;\n\tstruct io_cache_entry *entry;\n\n\tentry = io_alloc_cache_get(&ctx->rsrc_node_cache);\n\tif (entry) {\n\t\tref_node = container_of(entry, struct io_rsrc_node, cache);\n\t} else {\n\t\tref_node = kzalloc(sizeof(*ref_node), GFP_KERNEL);\n\t\tif (!ref_node)\n\t\t\treturn NULL;\n\t}\n\n\tref_node->ctx = ctx;\n\tref_node->empty = 0;\n\tref_node->refs = 1;\n\treturn ref_node;\n}\n\n__cold static int io_rsrc_ref_quiesce(struct io_rsrc_data *data,\n\t\t\t\t      struct io_ring_ctx *ctx)\n{\n\tstruct io_rsrc_node *backup;\n\tDEFINE_WAIT(we);\n\tint ret;\n\n\t \n\tif (data->quiesce)\n\t\treturn -ENXIO;\n\n\tbackup = io_rsrc_node_alloc(ctx);\n\tif (!backup)\n\t\treturn -ENOMEM;\n\tctx->rsrc_node->empty = true;\n\tctx->rsrc_node->type = -1;\n\tlist_add_tail(&ctx->rsrc_node->node, &ctx->rsrc_ref_list);\n\tio_put_rsrc_node(ctx, ctx->rsrc_node);\n\tctx->rsrc_node = backup;\n\n\tif (list_empty(&ctx->rsrc_ref_list))\n\t\treturn 0;\n\n\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN) {\n\t\tatomic_set(&ctx->cq_wait_nr, 1);\n\t\tsmp_mb();\n\t}\n\n\tctx->rsrc_quiesce++;\n\tdata->quiesce = true;\n\tdo {\n\t\tprepare_to_wait(&ctx->rsrc_quiesce_wq, &we, TASK_INTERRUPTIBLE);\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\t\tret = io_run_task_work_sig(ctx);\n\t\tif (ret < 0) {\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t\tif (list_empty(&ctx->rsrc_ref_list))\n\t\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\t\t__set_current_state(TASK_RUNNING);\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tret = 0;\n\t} while (!list_empty(&ctx->rsrc_ref_list));\n\n\tfinish_wait(&ctx->rsrc_quiesce_wq, &we);\n\tdata->quiesce = false;\n\tctx->rsrc_quiesce--;\n\n\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN) {\n\t\tatomic_set(&ctx->cq_wait_nr, 0);\n\t\tsmp_mb();\n\t}\n\treturn ret;\n}\n\nstatic void io_free_page_table(void **table, size_t size)\n{\n\tunsigned i, nr_tables = DIV_ROUND_UP(size, PAGE_SIZE);\n\n\tfor (i = 0; i < nr_tables; i++)\n\t\tkfree(table[i]);\n\tkfree(table);\n}\n\nstatic void io_rsrc_data_free(struct io_rsrc_data *data)\n{\n\tsize_t size = data->nr * sizeof(data->tags[0][0]);\n\n\tif (data->tags)\n\t\tio_free_page_table((void **)data->tags, size);\n\tkfree(data);\n}\n\nstatic __cold void **io_alloc_page_table(size_t size)\n{\n\tunsigned i, nr_tables = DIV_ROUND_UP(size, PAGE_SIZE);\n\tsize_t init_size = size;\n\tvoid **table;\n\n\ttable = kcalloc(nr_tables, sizeof(*table), GFP_KERNEL_ACCOUNT);\n\tif (!table)\n\t\treturn NULL;\n\n\tfor (i = 0; i < nr_tables; i++) {\n\t\tunsigned int this_size = min_t(size_t, size, PAGE_SIZE);\n\n\t\ttable[i] = kzalloc(this_size, GFP_KERNEL_ACCOUNT);\n\t\tif (!table[i]) {\n\t\t\tio_free_page_table(table, init_size);\n\t\t\treturn NULL;\n\t\t}\n\t\tsize -= this_size;\n\t}\n\treturn table;\n}\n\n__cold static int io_rsrc_data_alloc(struct io_ring_ctx *ctx, int type,\n\t\t\t\t     u64 __user *utags,\n\t\t\t\t     unsigned nr, struct io_rsrc_data **pdata)\n{\n\tstruct io_rsrc_data *data;\n\tint ret = 0;\n\tunsigned i;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\tdata->tags = (u64 **)io_alloc_page_table(nr * sizeof(data->tags[0][0]));\n\tif (!data->tags) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\n\tdata->nr = nr;\n\tdata->ctx = ctx;\n\tdata->rsrc_type = type;\n\tif (utags) {\n\t\tret = -EFAULT;\n\t\tfor (i = 0; i < nr; i++) {\n\t\t\tu64 *tag_slot = io_get_tag_slot(data, i);\n\n\t\t\tif (copy_from_user(tag_slot, &utags[i],\n\t\t\t\t\t   sizeof(*tag_slot)))\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\t*pdata = data;\n\treturn 0;\nfail:\n\tio_rsrc_data_free(data);\n\treturn ret;\n}\n\nstatic int __io_sqe_files_update(struct io_ring_ctx *ctx,\n\t\t\t\t struct io_uring_rsrc_update2 *up,\n\t\t\t\t unsigned nr_args)\n{\n\tu64 __user *tags = u64_to_user_ptr(up->tags);\n\t__s32 __user *fds = u64_to_user_ptr(up->data);\n\tstruct io_rsrc_data *data = ctx->file_data;\n\tstruct io_fixed_file *file_slot;\n\tint fd, i, err = 0;\n\tunsigned int done;\n\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\tif (up->offset + nr_args > ctx->nr_user_files)\n\t\treturn -EINVAL;\n\n\tfor (done = 0; done < nr_args; done++) {\n\t\tu64 tag = 0;\n\n\t\tif ((tags && copy_from_user(&tag, &tags[done], sizeof(tag))) ||\n\t\t    copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif ((fd == IORING_REGISTER_FILES_SKIP || fd == -1) && tag) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (fd == IORING_REGISTER_FILES_SKIP)\n\t\t\tcontinue;\n\n\t\ti = array_index_nospec(up->offset + done, ctx->nr_user_files);\n\t\tfile_slot = io_fixed_file_slot(&ctx->file_table, i);\n\n\t\tif (file_slot->file_ptr) {\n\t\t\terr = io_queue_rsrc_removal(data, i,\n\t\t\t\t\t\t    io_slot_file(file_slot));\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tfile_slot->file_ptr = 0;\n\t\t\tio_file_bitmap_clear(&ctx->file_table, i);\n\t\t}\n\t\tif (fd != -1) {\n\t\t\tstruct file *file = fget(fd);\n\n\t\t\tif (!file) {\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\tif (io_is_uring_fops(file)) {\n\t\t\t\tfput(file);\n\t\t\t\terr = -EBADF;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\terr = io_scm_file_account(ctx, file);\n\t\t\tif (err) {\n\t\t\t\tfput(file);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t*io_get_tag_slot(data, i) = tag;\n\t\t\tio_fixed_file_set(file_slot, file);\n\t\t\tio_file_bitmap_set(&ctx->file_table, i);\n\t\t}\n\t}\n\treturn done ? done : err;\n}\n\nstatic int __io_sqe_buffers_update(struct io_ring_ctx *ctx,\n\t\t\t\t   struct io_uring_rsrc_update2 *up,\n\t\t\t\t   unsigned int nr_args)\n{\n\tu64 __user *tags = u64_to_user_ptr(up->tags);\n\tstruct iovec iov, __user *iovs = u64_to_user_ptr(up->data);\n\tstruct page *last_hpage = NULL;\n\t__u32 done;\n\tint i, err;\n\n\tif (!ctx->buf_data)\n\t\treturn -ENXIO;\n\tif (up->offset + nr_args > ctx->nr_user_bufs)\n\t\treturn -EINVAL;\n\n\tfor (done = 0; done < nr_args; done++) {\n\t\tstruct io_mapped_ubuf *imu;\n\t\tu64 tag = 0;\n\n\t\terr = io_copy_iov(ctx, &iov, iovs, done);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (tags && copy_from_user(&tag, &tags[done], sizeof(tag))) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\terr = io_buffer_validate(&iov);\n\t\tif (err)\n\t\t\tbreak;\n\t\tif (!iov.iov_base && tag) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\terr = io_sqe_buffer_register(ctx, &iov, &imu, &last_hpage);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\ti = array_index_nospec(up->offset + done, ctx->nr_user_bufs);\n\t\tif (ctx->user_bufs[i] != &dummy_ubuf) {\n\t\t\terr = io_queue_rsrc_removal(ctx->buf_data, i,\n\t\t\t\t\t\t    ctx->user_bufs[i]);\n\t\t\tif (unlikely(err)) {\n\t\t\t\tio_buffer_unmap(ctx, &imu);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tctx->user_bufs[i] = (struct io_mapped_ubuf *)&dummy_ubuf;\n\t\t}\n\n\t\tctx->user_bufs[i] = imu;\n\t\t*io_get_tag_slot(ctx->buf_data, i) = tag;\n\t}\n\treturn done ? done : err;\n}\n\nstatic int __io_register_rsrc_update(struct io_ring_ctx *ctx, unsigned type,\n\t\t\t\t     struct io_uring_rsrc_update2 *up,\n\t\t\t\t     unsigned nr_args)\n{\n\t__u32 tmp;\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tif (check_add_overflow(up->offset, nr_args, &tmp))\n\t\treturn -EOVERFLOW;\n\n\tswitch (type) {\n\tcase IORING_RSRC_FILE:\n\t\treturn __io_sqe_files_update(ctx, up, nr_args);\n\tcase IORING_RSRC_BUFFER:\n\t\treturn __io_sqe_buffers_update(ctx, up, nr_args);\n\t}\n\treturn -EINVAL;\n}\n\nint io_register_files_update(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t     unsigned nr_args)\n{\n\tstruct io_uring_rsrc_update2 up;\n\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tmemset(&up, 0, sizeof(up));\n\tif (copy_from_user(&up, arg, sizeof(struct io_uring_rsrc_update)))\n\t\treturn -EFAULT;\n\tif (up.resv || up.resv2)\n\t\treturn -EINVAL;\n\treturn __io_register_rsrc_update(ctx, IORING_RSRC_FILE, &up, nr_args);\n}\n\nint io_register_rsrc_update(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t    unsigned size, unsigned type)\n{\n\tstruct io_uring_rsrc_update2 up;\n\n\tif (size != sizeof(up))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&up, arg, sizeof(up)))\n\t\treturn -EFAULT;\n\tif (!up.nr || up.resv || up.resv2)\n\t\treturn -EINVAL;\n\treturn __io_register_rsrc_update(ctx, type, &up, up.nr);\n}\n\n__cold int io_register_rsrc(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t    unsigned int size, unsigned int type)\n{\n\tstruct io_uring_rsrc_register rr;\n\n\t \n\tif (size != sizeof(rr))\n\t\treturn -EINVAL;\n\n\tmemset(&rr, 0, sizeof(rr));\n\tif (copy_from_user(&rr, arg, size))\n\t\treturn -EFAULT;\n\tif (!rr.nr || rr.resv2)\n\t\treturn -EINVAL;\n\tif (rr.flags & ~IORING_RSRC_REGISTER_SPARSE)\n\t\treturn -EINVAL;\n\n\tswitch (type) {\n\tcase IORING_RSRC_FILE:\n\t\tif (rr.flags & IORING_RSRC_REGISTER_SPARSE && rr.data)\n\t\t\tbreak;\n\t\treturn io_sqe_files_register(ctx, u64_to_user_ptr(rr.data),\n\t\t\t\t\t     rr.nr, u64_to_user_ptr(rr.tags));\n\tcase IORING_RSRC_BUFFER:\n\t\tif (rr.flags & IORING_RSRC_REGISTER_SPARSE && rr.data)\n\t\t\tbreak;\n\t\treturn io_sqe_buffers_register(ctx, u64_to_user_ptr(rr.data),\n\t\t\t\t\t       rr.nr, u64_to_user_ptr(rr.tags));\n\t}\n\treturn -EINVAL;\n}\n\nint io_files_update_prep(struct io_kiocb *req, const struct io_uring_sqe *sqe)\n{\n\tstruct io_rsrc_update *up = io_kiocb_to_cmd(req, struct io_rsrc_update);\n\n\tif (unlikely(req->flags & (REQ_F_FIXED_FILE | REQ_F_BUFFER_SELECT)))\n\t\treturn -EINVAL;\n\tif (sqe->rw_flags || sqe->splice_fd_in)\n\t\treturn -EINVAL;\n\n\tup->offset = READ_ONCE(sqe->off);\n\tup->nr_args = READ_ONCE(sqe->len);\n\tif (!up->nr_args)\n\t\treturn -EINVAL;\n\tup->arg = READ_ONCE(sqe->addr);\n\treturn 0;\n}\n\nstatic int io_files_update_with_index_alloc(struct io_kiocb *req,\n\t\t\t\t\t    unsigned int issue_flags)\n{\n\tstruct io_rsrc_update *up = io_kiocb_to_cmd(req, struct io_rsrc_update);\n\t__s32 __user *fds = u64_to_user_ptr(up->arg);\n\tunsigned int done;\n\tstruct file *file;\n\tint ret, fd;\n\n\tif (!req->ctx->file_data)\n\t\treturn -ENXIO;\n\n\tfor (done = 0; done < up->nr_args; done++) {\n\t\tif (copy_from_user(&fd, &fds[done], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tfile = fget(fd);\n\t\tif (!file) {\n\t\t\tret = -EBADF;\n\t\t\tbreak;\n\t\t}\n\t\tret = io_fixed_fd_install(req, issue_flags, file,\n\t\t\t\t\t  IORING_FILE_INDEX_ALLOC);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tif (copy_to_user(&fds[done], &ret, sizeof(ret))) {\n\t\t\t__io_close_fixed(req->ctx, issue_flags, ret);\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (done)\n\t\treturn done;\n\treturn ret;\n}\n\nint io_files_update(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_rsrc_update *up = io_kiocb_to_cmd(req, struct io_rsrc_update);\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_uring_rsrc_update2 up2;\n\tint ret;\n\n\tup2.offset = up->offset;\n\tup2.data = up->arg;\n\tup2.nr = 0;\n\tup2.tags = 0;\n\tup2.resv = 0;\n\tup2.resv2 = 0;\n\n\tif (up->offset == IORING_FILE_INDEX_ALLOC) {\n\t\tret = io_files_update_with_index_alloc(req, issue_flags);\n\t} else {\n\t\tio_ring_submit_lock(ctx, issue_flags);\n\t\tret = __io_register_rsrc_update(ctx, IORING_RSRC_FILE,\n\t\t\t\t\t\t&up2, up->nr_args);\n\t\tio_ring_submit_unlock(ctx, issue_flags);\n\t}\n\n\tif (ret < 0)\n\t\treq_set_fail(req);\n\tio_req_set_res(req, ret, 0);\n\treturn IOU_OK;\n}\n\nint io_queue_rsrc_removal(struct io_rsrc_data *data, unsigned idx, void *rsrc)\n{\n\tstruct io_ring_ctx *ctx = data->ctx;\n\tstruct io_rsrc_node *node = ctx->rsrc_node;\n\tu64 *tag_slot = io_get_tag_slot(data, idx);\n\n\tctx->rsrc_node = io_rsrc_node_alloc(ctx);\n\tif (unlikely(!ctx->rsrc_node)) {\n\t\tctx->rsrc_node = node;\n\t\treturn -ENOMEM;\n\t}\n\n\tnode->item.rsrc = rsrc;\n\tnode->type = data->rsrc_type;\n\tnode->item.tag = *tag_slot;\n\t*tag_slot = 0;\n\tlist_add_tail(&node->node, &ctx->rsrc_ref_list);\n\tio_put_rsrc_node(ctx, node);\n\treturn 0;\n}\n\nvoid __io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->nr_user_files; i++) {\n\t\tstruct file *file = io_file_from_index(&ctx->file_table, i);\n\n\t\t \n\t\tif (!file || io_file_need_scm(file))\n\t\t\tcontinue;\n\t\tio_file_bitmap_clear(&ctx->file_table, i);\n\t\tfput(file);\n\t}\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tstruct sock *sock = ctx->ring_sock->sk;\n\t\tstruct sk_buff *skb;\n\n\t\twhile ((skb = skb_dequeue(&sock->sk_receive_queue)) != NULL)\n\t\t\tkfree_skb(skb);\n\t}\n#endif\n\tio_free_file_tables(&ctx->file_table);\n\tio_file_table_set_alloc_range(ctx, 0, 0);\n\tio_rsrc_data_free(ctx->file_data);\n\tctx->file_data = NULL;\n\tctx->nr_user_files = 0;\n}\n\nint io_sqe_files_unregister(struct io_ring_ctx *ctx)\n{\n\tunsigned nr = ctx->nr_user_files;\n\tint ret;\n\n\tif (!ctx->file_data)\n\t\treturn -ENXIO;\n\n\t \n\tctx->nr_user_files = 0;\n\tret = io_rsrc_ref_quiesce(ctx->file_data, ctx);\n\tctx->nr_user_files = nr;\n\tif (!ret)\n\t\t__io_sqe_files_unregister(ctx);\n\treturn ret;\n}\n\n \nint __io_scm_file_account(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sk = ctx->ring_sock->sk;\n\tstruct sk_buff_head *head = &sk->sk_receive_queue;\n\tstruct scm_fp_list *fpl;\n\tstruct sk_buff *skb;\n\n\tif (likely(!io_file_need_scm(file)))\n\t\treturn 0;\n\n\t \n\tspin_lock_irq(&head->lock);\n\tskb = skb_peek(head);\n\tif (skb && UNIXCB(skb).fp->count < SCM_MAX_FD)\n\t\t__skb_unlink(skb, head);\n\telse\n\t\tskb = NULL;\n\tspin_unlock_irq(&head->lock);\n\n\tif (!skb) {\n\t\tfpl = kzalloc(sizeof(*fpl), GFP_KERNEL);\n\t\tif (!fpl)\n\t\t\treturn -ENOMEM;\n\n\t\tskb = alloc_skb(0, GFP_KERNEL);\n\t\tif (!skb) {\n\t\t\tkfree(fpl);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tfpl->user = get_uid(current_user());\n\t\tfpl->max = SCM_MAX_FD;\n\t\tfpl->count = 0;\n\n\t\tUNIXCB(skb).fp = fpl;\n\t\tskb->sk = sk;\n\t\tskb->destructor = io_uring_destruct_scm;\n\t\trefcount_add(skb->truesize, &sk->sk_wmem_alloc);\n\t}\n\n\tfpl = UNIXCB(skb).fp;\n\tfpl->fp[fpl->count++] = get_file(file);\n\tunix_inflight(fpl->user, file);\n\tskb_queue_head(head, skb);\n\tfput(file);\n#endif\n\treturn 0;\n}\n\nstatic __cold void io_rsrc_file_scm_put(struct io_ring_ctx *ctx, struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tstruct sock *sock = ctx->ring_sock->sk;\n\tstruct sk_buff_head list, *head = &sock->sk_receive_queue;\n\tstruct sk_buff *skb;\n\tint i;\n\n\t__skb_queue_head_init(&list);\n\n\t \n\tskb = skb_dequeue(head);\n\twhile (skb) {\n\t\tstruct scm_fp_list *fp;\n\n\t\tfp = UNIXCB(skb).fp;\n\t\tfor (i = 0; i < fp->count; i++) {\n\t\t\tint left;\n\n\t\t\tif (fp->fp[i] != file)\n\t\t\t\tcontinue;\n\n\t\t\tunix_notinflight(fp->user, fp->fp[i]);\n\t\t\tleft = fp->count - 1 - i;\n\t\t\tif (left) {\n\t\t\t\tmemmove(&fp->fp[i], &fp->fp[i + 1],\n\t\t\t\t\t\tleft * sizeof(struct file *));\n\t\t\t}\n\t\t\tfp->count--;\n\t\t\tif (!fp->count) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tskb = NULL;\n\t\t\t} else {\n\t\t\t\t__skb_queue_tail(&list, skb);\n\t\t\t}\n\t\t\tfput(file);\n\t\t\tfile = NULL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!file)\n\t\t\tbreak;\n\n\t\t__skb_queue_tail(&list, skb);\n\n\t\tskb = skb_dequeue(head);\n\t}\n\n\tif (skb_peek(&list)) {\n\t\tspin_lock_irq(&head->lock);\n\t\twhile ((skb = __skb_dequeue(&list)) != NULL)\n\t\t\t__skb_queue_tail(head, skb);\n\t\tspin_unlock_irq(&head->lock);\n\t}\n#endif\n}\n\nstatic void io_rsrc_file_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc)\n{\n\tstruct file *file = prsrc->file;\n\n\tif (likely(!io_file_need_scm(file)))\n\t\tfput(file);\n\telse\n\t\tio_rsrc_file_scm_put(ctx, file);\n}\n\nint io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t  unsigned nr_args, u64 __user *tags)\n{\n\t__s32 __user *fds = (__s32 __user *) arg;\n\tstruct file *file;\n\tint fd, ret;\n\tunsigned i;\n\n\tif (ctx->file_data)\n\t\treturn -EBUSY;\n\tif (!nr_args)\n\t\treturn -EINVAL;\n\tif (nr_args > IORING_MAX_FIXED_FILES)\n\t\treturn -EMFILE;\n\tif (nr_args > rlimit(RLIMIT_NOFILE))\n\t\treturn -EMFILE;\n\tret = io_rsrc_data_alloc(ctx, IORING_RSRC_FILE, tags, nr_args,\n\t\t\t\t &ctx->file_data);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!io_alloc_file_tables(&ctx->file_table, nr_args)) {\n\t\tio_rsrc_data_free(ctx->file_data);\n\t\tctx->file_data = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < nr_args; i++, ctx->nr_user_files++) {\n\t\tstruct io_fixed_file *file_slot;\n\n\t\tif (fds && copy_from_user(&fd, &fds[i], sizeof(fd))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto fail;\n\t\t}\n\t\t \n\t\tif (!fds || fd == -1) {\n\t\t\tret = -EINVAL;\n\t\t\tif (unlikely(*io_get_tag_slot(ctx->file_data, i)))\n\t\t\t\tgoto fail;\n\t\t\tcontinue;\n\t\t}\n\n\t\tfile = fget(fd);\n\t\tret = -EBADF;\n\t\tif (unlikely(!file))\n\t\t\tgoto fail;\n\n\t\t \n\t\tif (io_is_uring_fops(file)) {\n\t\t\tfput(file);\n\t\t\tgoto fail;\n\t\t}\n\t\tret = io_scm_file_account(ctx, file);\n\t\tif (ret) {\n\t\t\tfput(file);\n\t\t\tgoto fail;\n\t\t}\n\t\tfile_slot = io_fixed_file_slot(&ctx->file_table, i);\n\t\tio_fixed_file_set(file_slot, file);\n\t\tio_file_bitmap_set(&ctx->file_table, i);\n\t}\n\n\t \n\tio_file_table_set_alloc_range(ctx, 0, ctx->nr_user_files);\n\treturn 0;\nfail:\n\t__io_sqe_files_unregister(ctx);\n\treturn ret;\n}\n\nstatic void io_rsrc_buf_put(struct io_ring_ctx *ctx, struct io_rsrc_put *prsrc)\n{\n\tio_buffer_unmap(ctx, &prsrc->buf);\n\tprsrc->buf = NULL;\n}\n\nvoid __io_sqe_buffers_unregister(struct io_ring_ctx *ctx)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ctx->nr_user_bufs; i++)\n\t\tio_buffer_unmap(ctx, &ctx->user_bufs[i]);\n\tkfree(ctx->user_bufs);\n\tio_rsrc_data_free(ctx->buf_data);\n\tctx->user_bufs = NULL;\n\tctx->buf_data = NULL;\n\tctx->nr_user_bufs = 0;\n}\n\nint io_sqe_buffers_unregister(struct io_ring_ctx *ctx)\n{\n\tunsigned nr = ctx->nr_user_bufs;\n\tint ret;\n\n\tif (!ctx->buf_data)\n\t\treturn -ENXIO;\n\n\t \n\tctx->nr_user_bufs = 0;\n\tret = io_rsrc_ref_quiesce(ctx->buf_data, ctx);\n\tctx->nr_user_bufs = nr;\n\tif (!ret)\n\t\t__io_sqe_buffers_unregister(ctx);\n\treturn ret;\n}\n\n \nstatic bool headpage_already_acct(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t  int nr_pages, struct page *hpage)\n{\n\tint i, j;\n\n\t \n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i]))\n\t\t\tcontinue;\n\t\tif (compound_head(pages[i]) == hpage)\n\t\t\treturn true;\n\t}\n\n\t \n\tfor (i = 0; i < ctx->nr_user_bufs; i++) {\n\t\tstruct io_mapped_ubuf *imu = ctx->user_bufs[i];\n\n\t\tfor (j = 0; j < imu->nr_bvecs; j++) {\n\t\t\tif (!PageCompound(imu->bvec[j].bv_page))\n\t\t\t\tcontinue;\n\t\t\tif (compound_head(imu->bvec[j].bv_page) == hpage)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int io_buffer_account_pin(struct io_ring_ctx *ctx, struct page **pages,\n\t\t\t\t int nr_pages, struct io_mapped_ubuf *imu,\n\t\t\t\t struct page **last_hpage)\n{\n\tint i, ret;\n\n\timu->acct_pages = 0;\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (!PageCompound(pages[i])) {\n\t\t\timu->acct_pages++;\n\t\t} else {\n\t\t\tstruct page *hpage;\n\n\t\t\thpage = compound_head(pages[i]);\n\t\t\tif (hpage == *last_hpage)\n\t\t\t\tcontinue;\n\t\t\t*last_hpage = hpage;\n\t\t\tif (headpage_already_acct(ctx, pages, i, hpage))\n\t\t\t\tcontinue;\n\t\t\timu->acct_pages += page_size(hpage) >> PAGE_SHIFT;\n\t\t}\n\t}\n\n\tif (!imu->acct_pages)\n\t\treturn 0;\n\n\tret = io_account_mem(ctx, imu->acct_pages);\n\tif (ret)\n\t\timu->acct_pages = 0;\n\treturn ret;\n}\n\nstruct page **io_pin_pages(unsigned long ubuf, unsigned long len, int *npages)\n{\n\tunsigned long start, end, nr_pages;\n\tstruct page **pages = NULL;\n\tint pret, ret = -ENOMEM;\n\n\tend = (ubuf + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstart = ubuf >> PAGE_SHIFT;\n\tnr_pages = end - start;\n\n\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto done;\n\n\tret = 0;\n\tmmap_read_lock(current->mm);\n\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t      pages);\n\tif (pret == nr_pages)\n\t\t*npages = nr_pages;\n\telse\n\t\tret = pret < 0 ? pret : -EFAULT;\n\n\tmmap_read_unlock(current->mm);\n\tif (ret) {\n\t\t \n\t\tif (pret > 0)\n\t\t\tunpin_user_pages(pages, pret);\n\t\tgoto done;\n\t}\n\tret = 0;\ndone:\n\tif (ret < 0) {\n\t\tkvfree(pages);\n\t\tpages = ERR_PTR(ret);\n\t}\n\treturn pages;\n}\n\nstatic int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\n\t\t\t\t  struct io_mapped_ubuf **pimu,\n\t\t\t\t  struct page **last_hpage)\n{\n\tstruct io_mapped_ubuf *imu = NULL;\n\tstruct page **pages = NULL;\n\tunsigned long off;\n\tsize_t size;\n\tint ret, nr_pages, i;\n\tstruct folio *folio = NULL;\n\n\t*pimu = (struct io_mapped_ubuf *)&dummy_ubuf;\n\tif (!iov->iov_base)\n\t\treturn 0;\n\n\tret = -ENOMEM;\n\tpages = io_pin_pages((unsigned long) iov->iov_base, iov->iov_len,\n\t\t\t\t&nr_pages);\n\tif (IS_ERR(pages)) {\n\t\tret = PTR_ERR(pages);\n\t\tpages = NULL;\n\t\tgoto done;\n\t}\n\n\t \n\tif (nr_pages > 1) {\n\t\tfolio = page_folio(pages[0]);\n\t\tfor (i = 1; i < nr_pages; i++) {\n\t\t\t \n\t\t\tif (page_folio(pages[i]) != folio ||\n\t\t\t    pages[i] != pages[i - 1] + 1) {\n\t\t\t\tfolio = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (folio) {\n\t\t\t \n\t\t\tunpin_user_pages(&pages[1], nr_pages - 1);\n\t\t\tnr_pages = 1;\n\t\t}\n\t}\n\n\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);\n\tif (!imu)\n\t\tgoto done;\n\n\tret = io_buffer_account_pin(ctx, pages, nr_pages, imu, last_hpage);\n\tif (ret) {\n\t\tunpin_user_pages(pages, nr_pages);\n\t\tgoto done;\n\t}\n\n\toff = (unsigned long) iov->iov_base & ~PAGE_MASK;\n\tsize = iov->iov_len;\n\t \n\timu->ubuf = (unsigned long) iov->iov_base;\n\timu->ubuf_end = imu->ubuf + iov->iov_len;\n\timu->nr_bvecs = nr_pages;\n\t*pimu = imu;\n\tret = 0;\n\n\tif (folio) {\n\t\tbvec_set_page(&imu->bvec[0], pages[0], size, off);\n\t\tgoto done;\n\t}\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t vec_len;\n\n\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\tbvec_set_page(&imu->bvec[i], pages[i], vec_len, off);\n\t\toff = 0;\n\t\tsize -= vec_len;\n\t}\ndone:\n\tif (ret)\n\t\tkvfree(imu);\n\tkvfree(pages);\n\treturn ret;\n}\n\nstatic int io_buffers_map_alloc(struct io_ring_ctx *ctx, unsigned int nr_args)\n{\n\tctx->user_bufs = kcalloc(nr_args, sizeof(*ctx->user_bufs), GFP_KERNEL);\n\treturn ctx->user_bufs ? 0 : -ENOMEM;\n}\n\nint io_sqe_buffers_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t    unsigned int nr_args, u64 __user *tags)\n{\n\tstruct page *last_hpage = NULL;\n\tstruct io_rsrc_data *data;\n\tint i, ret;\n\tstruct iovec iov;\n\n\tBUILD_BUG_ON(IORING_MAX_REG_BUFFERS >= (1u << 16));\n\n\tif (ctx->user_bufs)\n\t\treturn -EBUSY;\n\tif (!nr_args || nr_args > IORING_MAX_REG_BUFFERS)\n\t\treturn -EINVAL;\n\tret = io_rsrc_data_alloc(ctx, IORING_RSRC_BUFFER, tags, nr_args, &data);\n\tif (ret)\n\t\treturn ret;\n\tret = io_buffers_map_alloc(ctx, nr_args);\n\tif (ret) {\n\t\tio_rsrc_data_free(data);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < nr_args; i++, ctx->nr_user_bufs++) {\n\t\tif (arg) {\n\t\t\tret = io_copy_iov(ctx, &iov, arg, i);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\tret = io_buffer_validate(&iov);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tmemset(&iov, 0, sizeof(iov));\n\t\t}\n\n\t\tif (!iov.iov_base && *io_get_tag_slot(data, i)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_sqe_buffer_register(ctx, &iov, &ctx->user_bufs[i],\n\t\t\t\t\t     &last_hpage);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tWARN_ON_ONCE(ctx->buf_data);\n\n\tctx->buf_data = data;\n\tif (ret)\n\t\t__io_sqe_buffers_unregister(ctx);\n\treturn ret;\n}\n\nint io_import_fixed(int ddir, struct iov_iter *iter,\n\t\t\t   struct io_mapped_ubuf *imu,\n\t\t\t   u64 buf_addr, size_t len)\n{\n\tu64 buf_end;\n\tsize_t offset;\n\n\tif (WARN_ON_ONCE(!imu))\n\t\treturn -EFAULT;\n\tif (unlikely(check_add_overflow(buf_addr, (u64)len, &buf_end)))\n\t\treturn -EFAULT;\n\t \n\tif (unlikely(buf_addr < imu->ubuf || buf_end > imu->ubuf_end))\n\t\treturn -EFAULT;\n\n\t \n\toffset = buf_addr - imu->ubuf;\n\tiov_iter_bvec(iter, ddir, imu->bvec, imu->nr_bvecs, offset + len);\n\n\tif (offset) {\n\t\t \n\t\tconst struct bio_vec *bvec = imu->bvec;\n\n\t\tif (offset < bvec->bv_len) {\n\t\t\t \n\t\t\titer->bvec = bvec;\n\t\t\titer->nr_segs = bvec->bv_len;\n\t\t\titer->count -= offset;\n\t\t\titer->iov_offset = offset;\n\t\t} else {\n\t\t\tunsigned long seg_skip;\n\n\t\t\t \n\t\t\toffset -= bvec->bv_len;\n\t\t\tseg_skip = 1 + (offset >> PAGE_SHIFT);\n\n\t\t\titer->bvec = bvec + seg_skip;\n\t\t\titer->nr_segs -= seg_skip;\n\t\t\titer->count -= bvec->bv_len + offset;\n\t\t\titer->iov_offset = offset & ~PAGE_MASK;\n\t\t}\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}