{
  "module_name": "io-wq.c",
  "hash_id": "1413f4eabc942e3cfc8202857b29d96e9b0c6df0f2d67bbd31bb9c54b8e0d5a3",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/io-wq.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/sched/signal.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/rculist_nulls.h>\n#include <linux/cpu.h>\n#include <linux/task_work.h>\n#include <linux/audit.h>\n#include <linux/mmu_context.h>\n#include <uapi/linux/io_uring.h>\n\n#include \"io-wq.h\"\n#include \"slist.h\"\n#include \"io_uring.h\"\n\n#define WORKER_IDLE_TIMEOUT\t(5 * HZ)\n\nenum {\n\tIO_WORKER_F_UP\t\t= 1,\t \n\tIO_WORKER_F_RUNNING\t= 2,\t \n\tIO_WORKER_F_FREE\t= 4,\t \n\tIO_WORKER_F_BOUND\t= 8,\t \n};\n\nenum {\n\tIO_WQ_BIT_EXIT\t\t= 0,\t \n};\n\nenum {\n\tIO_ACCT_STALLED_BIT\t= 0,\t \n};\n\n \nstruct io_worker {\n\trefcount_t ref;\n\tunsigned flags;\n\tstruct hlist_nulls_node nulls_node;\n\tstruct list_head all_list;\n\tstruct task_struct *task;\n\tstruct io_wq *wq;\n\n\tstruct io_wq_work *cur_work;\n\tstruct io_wq_work *next_work;\n\traw_spinlock_t lock;\n\n\tstruct completion ref_done;\n\n\tunsigned long create_state;\n\tstruct callback_head create_work;\n\tint create_index;\n\n\tunion {\n\t\tstruct rcu_head rcu;\n\t\tstruct work_struct work;\n\t};\n};\n\n#if BITS_PER_LONG == 64\n#define IO_WQ_HASH_ORDER\t6\n#else\n#define IO_WQ_HASH_ORDER\t5\n#endif\n\n#define IO_WQ_NR_HASH_BUCKETS\t(1u << IO_WQ_HASH_ORDER)\n\nstruct io_wq_acct {\n\tunsigned nr_workers;\n\tunsigned max_workers;\n\tint index;\n\tatomic_t nr_running;\n\traw_spinlock_t lock;\n\tstruct io_wq_work_list work_list;\n\tunsigned long flags;\n};\n\nenum {\n\tIO_WQ_ACCT_BOUND,\n\tIO_WQ_ACCT_UNBOUND,\n\tIO_WQ_ACCT_NR,\n};\n\n \nstruct io_wq {\n\tunsigned long state;\n\n\tfree_work_fn *free_work;\n\tio_wq_work_fn *do_work;\n\n\tstruct io_wq_hash *hash;\n\n\tatomic_t worker_refs;\n\tstruct completion worker_done;\n\n\tstruct hlist_node cpuhp_node;\n\n\tstruct task_struct *task;\n\n\tstruct io_wq_acct acct[IO_WQ_ACCT_NR];\n\n\t \n\traw_spinlock_t lock;\n\n\tstruct hlist_nulls_head free_list;\n\tstruct list_head all_list;\n\n\tstruct wait_queue_entry wait;\n\n\tstruct io_wq_work *hash_tail[IO_WQ_NR_HASH_BUCKETS];\n\n\tcpumask_var_t cpu_mask;\n};\n\nstatic enum cpuhp_state io_wq_online;\n\nstruct io_cb_cancel_data {\n\twork_cancel_fn *fn;\n\tvoid *data;\n\tint nr_running;\n\tint nr_pending;\n\tbool cancel_all;\n};\n\nstatic bool create_io_worker(struct io_wq *wq, int index);\nstatic void io_wq_dec_running(struct io_worker *worker);\nstatic bool io_acct_cancel_pending_work(struct io_wq *wq,\n\t\t\t\t\tstruct io_wq_acct *acct,\n\t\t\t\t\tstruct io_cb_cancel_data *match);\nstatic void create_worker_cb(struct callback_head *cb);\nstatic void io_wq_cancel_tw_create(struct io_wq *wq);\n\nstatic bool io_worker_get(struct io_worker *worker)\n{\n\treturn refcount_inc_not_zero(&worker->ref);\n}\n\nstatic void io_worker_release(struct io_worker *worker)\n{\n\tif (refcount_dec_and_test(&worker->ref))\n\t\tcomplete(&worker->ref_done);\n}\n\nstatic inline struct io_wq_acct *io_get_acct(struct io_wq *wq, bool bound)\n{\n\treturn &wq->acct[bound ? IO_WQ_ACCT_BOUND : IO_WQ_ACCT_UNBOUND];\n}\n\nstatic inline struct io_wq_acct *io_work_get_acct(struct io_wq *wq,\n\t\t\t\t\t\t  struct io_wq_work *work)\n{\n\treturn io_get_acct(wq, !(work->flags & IO_WQ_WORK_UNBOUND));\n}\n\nstatic inline struct io_wq_acct *io_wq_get_acct(struct io_worker *worker)\n{\n\treturn io_get_acct(worker->wq, worker->flags & IO_WORKER_F_BOUND);\n}\n\nstatic void io_worker_ref_put(struct io_wq *wq)\n{\n\tif (atomic_dec_and_test(&wq->worker_refs))\n\t\tcomplete(&wq->worker_done);\n}\n\nbool io_wq_worker_stopped(void)\n{\n\tstruct io_worker *worker = current->worker_private;\n\n\tif (WARN_ON_ONCE(!io_wq_current_is_worker()))\n\t\treturn true;\n\n\treturn test_bit(IO_WQ_BIT_EXIT, &worker->wq->state);\n}\n\nstatic void io_worker_cancel_cb(struct io_worker *worker)\n{\n\tstruct io_wq_acct *acct = io_wq_get_acct(worker);\n\tstruct io_wq *wq = worker->wq;\n\n\tatomic_dec(&acct->nr_running);\n\traw_spin_lock(&wq->lock);\n\tacct->nr_workers--;\n\traw_spin_unlock(&wq->lock);\n\tio_worker_ref_put(wq);\n\tclear_bit_unlock(0, &worker->create_state);\n\tio_worker_release(worker);\n}\n\nstatic bool io_task_worker_match(struct callback_head *cb, void *data)\n{\n\tstruct io_worker *worker;\n\n\tif (cb->func != create_worker_cb)\n\t\treturn false;\n\tworker = container_of(cb, struct io_worker, create_work);\n\treturn worker == data;\n}\n\nstatic void io_worker_exit(struct io_worker *worker)\n{\n\tstruct io_wq *wq = worker->wq;\n\n\twhile (1) {\n\t\tstruct callback_head *cb = task_work_cancel_match(wq->task,\n\t\t\t\t\t\tio_task_worker_match, worker);\n\n\t\tif (!cb)\n\t\t\tbreak;\n\t\tio_worker_cancel_cb(worker);\n\t}\n\n\tio_worker_release(worker);\n\twait_for_completion(&worker->ref_done);\n\n\traw_spin_lock(&wq->lock);\n\tif (worker->flags & IO_WORKER_F_FREE)\n\t\thlist_nulls_del_rcu(&worker->nulls_node);\n\tlist_del_rcu(&worker->all_list);\n\traw_spin_unlock(&wq->lock);\n\tio_wq_dec_running(worker);\n\t \n\tcurrent->worker_private = NULL;\n\n\tkfree_rcu(worker, rcu);\n\tio_worker_ref_put(wq);\n\tdo_exit(0);\n}\n\nstatic inline bool __io_acct_run_queue(struct io_wq_acct *acct)\n{\n\treturn !test_bit(IO_ACCT_STALLED_BIT, &acct->flags) &&\n\t\t!wq_list_empty(&acct->work_list);\n}\n\n \nstatic inline bool io_acct_run_queue(struct io_wq_acct *acct)\n\t__acquires(&acct->lock)\n{\n\traw_spin_lock(&acct->lock);\n\tif (__io_acct_run_queue(acct))\n\t\treturn true;\n\n\traw_spin_unlock(&acct->lock);\n\treturn false;\n}\n\n \nstatic bool io_wq_activate_free_worker(struct io_wq *wq,\n\t\t\t\t\tstruct io_wq_acct *acct)\n\t__must_hold(RCU)\n{\n\tstruct hlist_nulls_node *n;\n\tstruct io_worker *worker;\n\n\t \n\thlist_nulls_for_each_entry_rcu(worker, n, &wq->free_list, nulls_node) {\n\t\tif (!io_worker_get(worker))\n\t\t\tcontinue;\n\t\tif (io_wq_get_acct(worker) != acct) {\n\t\t\tio_worker_release(worker);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\twake_up_process(worker->task);\n\t\tio_worker_release(worker);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool io_wq_create_worker(struct io_wq *wq, struct io_wq_acct *acct)\n{\n\t \n\tif (unlikely(!acct->max_workers))\n\t\tpr_warn_once(\"io-wq is not configured for unbound workers\");\n\n\traw_spin_lock(&wq->lock);\n\tif (acct->nr_workers >= acct->max_workers) {\n\t\traw_spin_unlock(&wq->lock);\n\t\treturn true;\n\t}\n\tacct->nr_workers++;\n\traw_spin_unlock(&wq->lock);\n\tatomic_inc(&acct->nr_running);\n\tatomic_inc(&wq->worker_refs);\n\treturn create_io_worker(wq, acct->index);\n}\n\nstatic void io_wq_inc_running(struct io_worker *worker)\n{\n\tstruct io_wq_acct *acct = io_wq_get_acct(worker);\n\n\tatomic_inc(&acct->nr_running);\n}\n\nstatic void create_worker_cb(struct callback_head *cb)\n{\n\tstruct io_worker *worker;\n\tstruct io_wq *wq;\n\n\tstruct io_wq_acct *acct;\n\tbool do_create = false;\n\n\tworker = container_of(cb, struct io_worker, create_work);\n\twq = worker->wq;\n\tacct = &wq->acct[worker->create_index];\n\traw_spin_lock(&wq->lock);\n\n\tif (acct->nr_workers < acct->max_workers) {\n\t\tacct->nr_workers++;\n\t\tdo_create = true;\n\t}\n\traw_spin_unlock(&wq->lock);\n\tif (do_create) {\n\t\tcreate_io_worker(wq, worker->create_index);\n\t} else {\n\t\tatomic_dec(&acct->nr_running);\n\t\tio_worker_ref_put(wq);\n\t}\n\tclear_bit_unlock(0, &worker->create_state);\n\tio_worker_release(worker);\n}\n\nstatic bool io_queue_worker_create(struct io_worker *worker,\n\t\t\t\t   struct io_wq_acct *acct,\n\t\t\t\t   task_work_func_t func)\n{\n\tstruct io_wq *wq = worker->wq;\n\n\t \n\tif (test_bit(IO_WQ_BIT_EXIT, &wq->state))\n\t\tgoto fail;\n\tif (!io_worker_get(worker))\n\t\tgoto fail;\n\t \n\tif (test_bit(0, &worker->create_state) ||\n\t    test_and_set_bit_lock(0, &worker->create_state))\n\t\tgoto fail_release;\n\n\tatomic_inc(&wq->worker_refs);\n\tinit_task_work(&worker->create_work, func);\n\tworker->create_index = acct->index;\n\tif (!task_work_add(wq->task, &worker->create_work, TWA_SIGNAL)) {\n\t\t \n\t\tif (test_bit(IO_WQ_BIT_EXIT, &wq->state))\n\t\t\tio_wq_cancel_tw_create(wq);\n\t\tio_worker_ref_put(wq);\n\t\treturn true;\n\t}\n\tio_worker_ref_put(wq);\n\tclear_bit_unlock(0, &worker->create_state);\nfail_release:\n\tio_worker_release(worker);\nfail:\n\tatomic_dec(&acct->nr_running);\n\tio_worker_ref_put(wq);\n\treturn false;\n}\n\nstatic void io_wq_dec_running(struct io_worker *worker)\n{\n\tstruct io_wq_acct *acct = io_wq_get_acct(worker);\n\tstruct io_wq *wq = worker->wq;\n\n\tif (!(worker->flags & IO_WORKER_F_UP))\n\t\treturn;\n\n\tif (!atomic_dec_and_test(&acct->nr_running))\n\t\treturn;\n\tif (!io_acct_run_queue(acct))\n\t\treturn;\n\n\traw_spin_unlock(&acct->lock);\n\tatomic_inc(&acct->nr_running);\n\tatomic_inc(&wq->worker_refs);\n\tio_queue_worker_create(worker, acct, create_worker_cb);\n}\n\n \nstatic void __io_worker_busy(struct io_wq *wq, struct io_worker *worker)\n{\n\tif (worker->flags & IO_WORKER_F_FREE) {\n\t\tworker->flags &= ~IO_WORKER_F_FREE;\n\t\traw_spin_lock(&wq->lock);\n\t\thlist_nulls_del_init_rcu(&worker->nulls_node);\n\t\traw_spin_unlock(&wq->lock);\n\t}\n}\n\n \nstatic void __io_worker_idle(struct io_wq *wq, struct io_worker *worker)\n\t__must_hold(wq->lock)\n{\n\tif (!(worker->flags & IO_WORKER_F_FREE)) {\n\t\tworker->flags |= IO_WORKER_F_FREE;\n\t\thlist_nulls_add_head_rcu(&worker->nulls_node, &wq->free_list);\n\t}\n}\n\nstatic inline unsigned int io_get_work_hash(struct io_wq_work *work)\n{\n\treturn work->flags >> IO_WQ_HASH_SHIFT;\n}\n\nstatic bool io_wait_on_hash(struct io_wq *wq, unsigned int hash)\n{\n\tbool ret = false;\n\n\tspin_lock_irq(&wq->hash->wait.lock);\n\tif (list_empty(&wq->wait.entry)) {\n\t\t__add_wait_queue(&wq->hash->wait, &wq->wait);\n\t\tif (!test_bit(hash, &wq->hash->map)) {\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\tlist_del_init(&wq->wait.entry);\n\t\t\tret = true;\n\t\t}\n\t}\n\tspin_unlock_irq(&wq->hash->wait.lock);\n\treturn ret;\n}\n\nstatic struct io_wq_work *io_get_next_work(struct io_wq_acct *acct,\n\t\t\t\t\t   struct io_worker *worker)\n\t__must_hold(acct->lock)\n{\n\tstruct io_wq_work_node *node, *prev;\n\tstruct io_wq_work *work, *tail;\n\tunsigned int stall_hash = -1U;\n\tstruct io_wq *wq = worker->wq;\n\n\twq_list_for_each(node, prev, &acct->work_list) {\n\t\tunsigned int hash;\n\n\t\twork = container_of(node, struct io_wq_work, list);\n\n\t\t \n\t\tif (!io_wq_is_hashed(work)) {\n\t\t\twq_list_del(&acct->work_list, node, prev);\n\t\t\treturn work;\n\t\t}\n\n\t\thash = io_get_work_hash(work);\n\t\t \n\t\ttail = wq->hash_tail[hash];\n\n\t\t \n\t\tif (!test_and_set_bit(hash, &wq->hash->map)) {\n\t\t\twq->hash_tail[hash] = NULL;\n\t\t\twq_list_cut(&acct->work_list, &tail->list, prev);\n\t\t\treturn work;\n\t\t}\n\t\tif (stall_hash == -1U)\n\t\t\tstall_hash = hash;\n\t\t \n\t\tnode = &tail->list;\n\t}\n\n\tif (stall_hash != -1U) {\n\t\tbool unstalled;\n\n\t\t \n\t\tset_bit(IO_ACCT_STALLED_BIT, &acct->flags);\n\t\traw_spin_unlock(&acct->lock);\n\t\tunstalled = io_wait_on_hash(wq, stall_hash);\n\t\traw_spin_lock(&acct->lock);\n\t\tif (unstalled) {\n\t\t\tclear_bit(IO_ACCT_STALLED_BIT, &acct->flags);\n\t\t\tif (wq_has_sleeper(&wq->hash->wait))\n\t\t\t\twake_up(&wq->hash->wait);\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic void io_assign_current_work(struct io_worker *worker,\n\t\t\t\t   struct io_wq_work *work)\n{\n\tif (work) {\n\t\tio_run_task_work();\n\t\tcond_resched();\n\t}\n\n\traw_spin_lock(&worker->lock);\n\tworker->cur_work = work;\n\tworker->next_work = NULL;\n\traw_spin_unlock(&worker->lock);\n}\n\n \nstatic void io_worker_handle_work(struct io_wq_acct *acct,\n\t\t\t\t  struct io_worker *worker)\n\t__releases(&acct->lock)\n{\n\tstruct io_wq *wq = worker->wq;\n\tbool do_kill = test_bit(IO_WQ_BIT_EXIT, &wq->state);\n\n\tdo {\n\t\tstruct io_wq_work *work;\n\n\t\t \n\t\twork = io_get_next_work(acct, worker);\n\t\traw_spin_unlock(&acct->lock);\n\t\tif (work) {\n\t\t\t__io_worker_busy(wq, worker);\n\n\t\t\t \n\t\t\traw_spin_lock(&worker->lock);\n\t\t\tworker->next_work = work;\n\t\t\traw_spin_unlock(&worker->lock);\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t\tio_assign_current_work(worker, work);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\t \n\t\tdo {\n\t\t\tstruct io_wq_work *next_hashed, *linked;\n\t\t\tunsigned int hash = io_get_work_hash(work);\n\n\t\t\tnext_hashed = wq_next_work(work);\n\n\t\t\tif (unlikely(do_kill) && (work->flags & IO_WQ_WORK_UNBOUND))\n\t\t\t\twork->flags |= IO_WQ_WORK_CANCEL;\n\t\t\twq->do_work(work);\n\t\t\tio_assign_current_work(worker, NULL);\n\n\t\t\tlinked = wq->free_work(work);\n\t\t\twork = next_hashed;\n\t\t\tif (!work && linked && !io_wq_is_hashed(linked)) {\n\t\t\t\twork = linked;\n\t\t\t\tlinked = NULL;\n\t\t\t}\n\t\t\tio_assign_current_work(worker, work);\n\t\t\tif (linked)\n\t\t\t\tio_wq_enqueue(wq, linked);\n\n\t\t\tif (hash != -1U && !next_hashed) {\n\t\t\t\t \n\t\t\t\tspin_lock_irq(&wq->hash->wait.lock);\n\t\t\t\tclear_bit(hash, &wq->hash->map);\n\t\t\t\tclear_bit(IO_ACCT_STALLED_BIT, &acct->flags);\n\t\t\t\tspin_unlock_irq(&wq->hash->wait.lock);\n\t\t\t\tif (wq_has_sleeper(&wq->hash->wait))\n\t\t\t\t\twake_up(&wq->hash->wait);\n\t\t\t}\n\t\t} while (work);\n\n\t\tif (!__io_acct_run_queue(acct))\n\t\t\tbreak;\n\t\traw_spin_lock(&acct->lock);\n\t} while (1);\n}\n\nstatic int io_wq_worker(void *data)\n{\n\tstruct io_worker *worker = data;\n\tstruct io_wq_acct *acct = io_wq_get_acct(worker);\n\tstruct io_wq *wq = worker->wq;\n\tbool exit_mask = false, last_timeout = false;\n\tchar buf[TASK_COMM_LEN];\n\n\tworker->flags |= (IO_WORKER_F_UP | IO_WORKER_F_RUNNING);\n\n\tsnprintf(buf, sizeof(buf), \"iou-wrk-%d\", wq->task->pid);\n\tset_task_comm(current, buf);\n\n\twhile (!test_bit(IO_WQ_BIT_EXIT, &wq->state)) {\n\t\tlong ret;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t\t \n\t\twhile (io_acct_run_queue(acct))\n\t\t\tio_worker_handle_work(acct, worker);\n\n\t\traw_spin_lock(&wq->lock);\n\t\t \n\t\tif (last_timeout && (exit_mask || acct->nr_workers > 1)) {\n\t\t\tacct->nr_workers--;\n\t\t\traw_spin_unlock(&wq->lock);\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\tbreak;\n\t\t}\n\t\tlast_timeout = false;\n\t\t__io_worker_idle(wq, worker);\n\t\traw_spin_unlock(&wq->lock);\n\t\tif (io_run_task_work())\n\t\t\tcontinue;\n\t\tret = schedule_timeout(WORKER_IDLE_TIMEOUT);\n\t\tif (signal_pending(current)) {\n\t\t\tstruct ksignal ksig;\n\n\t\t\tif (!get_signal(&ksig))\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\tif (!ret) {\n\t\t\tlast_timeout = true;\n\t\t\texit_mask = !cpumask_test_cpu(raw_smp_processor_id(),\n\t\t\t\t\t\t\twq->cpu_mask);\n\t\t}\n\t}\n\n\tif (test_bit(IO_WQ_BIT_EXIT, &wq->state) && io_acct_run_queue(acct))\n\t\tio_worker_handle_work(acct, worker);\n\n\tio_worker_exit(worker);\n\treturn 0;\n}\n\n \nvoid io_wq_worker_running(struct task_struct *tsk)\n{\n\tstruct io_worker *worker = tsk->worker_private;\n\n\tif (!worker)\n\t\treturn;\n\tif (!(worker->flags & IO_WORKER_F_UP))\n\t\treturn;\n\tif (worker->flags & IO_WORKER_F_RUNNING)\n\t\treturn;\n\tworker->flags |= IO_WORKER_F_RUNNING;\n\tio_wq_inc_running(worker);\n}\n\n \nvoid io_wq_worker_sleeping(struct task_struct *tsk)\n{\n\tstruct io_worker *worker = tsk->worker_private;\n\n\tif (!worker)\n\t\treturn;\n\tif (!(worker->flags & IO_WORKER_F_UP))\n\t\treturn;\n\tif (!(worker->flags & IO_WORKER_F_RUNNING))\n\t\treturn;\n\n\tworker->flags &= ~IO_WORKER_F_RUNNING;\n\tio_wq_dec_running(worker);\n}\n\nstatic void io_init_new_worker(struct io_wq *wq, struct io_worker *worker,\n\t\t\t       struct task_struct *tsk)\n{\n\ttsk->worker_private = worker;\n\tworker->task = tsk;\n\tset_cpus_allowed_ptr(tsk, wq->cpu_mask);\n\n\traw_spin_lock(&wq->lock);\n\thlist_nulls_add_head_rcu(&worker->nulls_node, &wq->free_list);\n\tlist_add_tail_rcu(&worker->all_list, &wq->all_list);\n\tworker->flags |= IO_WORKER_F_FREE;\n\traw_spin_unlock(&wq->lock);\n\twake_up_new_task(tsk);\n}\n\nstatic bool io_wq_work_match_all(struct io_wq_work *work, void *data)\n{\n\treturn true;\n}\n\nstatic inline bool io_should_retry_thread(long err)\n{\n\t \n\tif (fatal_signal_pending(current))\n\t\treturn false;\n\n\tswitch (err) {\n\tcase -EAGAIN:\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void create_worker_cont(struct callback_head *cb)\n{\n\tstruct io_worker *worker;\n\tstruct task_struct *tsk;\n\tstruct io_wq *wq;\n\n\tworker = container_of(cb, struct io_worker, create_work);\n\tclear_bit_unlock(0, &worker->create_state);\n\twq = worker->wq;\n\ttsk = create_io_thread(io_wq_worker, worker, NUMA_NO_NODE);\n\tif (!IS_ERR(tsk)) {\n\t\tio_init_new_worker(wq, worker, tsk);\n\t\tio_worker_release(worker);\n\t\treturn;\n\t} else if (!io_should_retry_thread(PTR_ERR(tsk))) {\n\t\tstruct io_wq_acct *acct = io_wq_get_acct(worker);\n\n\t\tatomic_dec(&acct->nr_running);\n\t\traw_spin_lock(&wq->lock);\n\t\tacct->nr_workers--;\n\t\tif (!acct->nr_workers) {\n\t\t\tstruct io_cb_cancel_data match = {\n\t\t\t\t.fn\t\t= io_wq_work_match_all,\n\t\t\t\t.cancel_all\t= true,\n\t\t\t};\n\n\t\t\traw_spin_unlock(&wq->lock);\n\t\t\twhile (io_acct_cancel_pending_work(wq, acct, &match))\n\t\t\t\t;\n\t\t} else {\n\t\t\traw_spin_unlock(&wq->lock);\n\t\t}\n\t\tio_worker_ref_put(wq);\n\t\tkfree(worker);\n\t\treturn;\n\t}\n\n\t \n\tio_worker_release(worker);\n\tschedule_work(&worker->work);\n}\n\nstatic void io_workqueue_create(struct work_struct *work)\n{\n\tstruct io_worker *worker = container_of(work, struct io_worker, work);\n\tstruct io_wq_acct *acct = io_wq_get_acct(worker);\n\n\tif (!io_queue_worker_create(worker, acct, create_worker_cont))\n\t\tkfree(worker);\n}\n\nstatic bool create_io_worker(struct io_wq *wq, int index)\n{\n\tstruct io_wq_acct *acct = &wq->acct[index];\n\tstruct io_worker *worker;\n\tstruct task_struct *tsk;\n\n\t__set_current_state(TASK_RUNNING);\n\n\tworker = kzalloc(sizeof(*worker), GFP_KERNEL);\n\tif (!worker) {\nfail:\n\t\tatomic_dec(&acct->nr_running);\n\t\traw_spin_lock(&wq->lock);\n\t\tacct->nr_workers--;\n\t\traw_spin_unlock(&wq->lock);\n\t\tio_worker_ref_put(wq);\n\t\treturn false;\n\t}\n\n\trefcount_set(&worker->ref, 1);\n\tworker->wq = wq;\n\traw_spin_lock_init(&worker->lock);\n\tinit_completion(&worker->ref_done);\n\n\tif (index == IO_WQ_ACCT_BOUND)\n\t\tworker->flags |= IO_WORKER_F_BOUND;\n\n\ttsk = create_io_thread(io_wq_worker, worker, NUMA_NO_NODE);\n\tif (!IS_ERR(tsk)) {\n\t\tio_init_new_worker(wq, worker, tsk);\n\t} else if (!io_should_retry_thread(PTR_ERR(tsk))) {\n\t\tkfree(worker);\n\t\tgoto fail;\n\t} else {\n\t\tINIT_WORK(&worker->work, io_workqueue_create);\n\t\tschedule_work(&worker->work);\n\t}\n\n\treturn true;\n}\n\n \nstatic bool io_wq_for_each_worker(struct io_wq *wq,\n\t\t\t\t  bool (*func)(struct io_worker *, void *),\n\t\t\t\t  void *data)\n{\n\tstruct io_worker *worker;\n\tbool ret = false;\n\n\tlist_for_each_entry_rcu(worker, &wq->all_list, all_list) {\n\t\tif (io_worker_get(worker)) {\n\t\t\t \n\t\t\tif (worker->task)\n\t\t\t\tret = func(worker, data);\n\t\t\tio_worker_release(worker);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic bool io_wq_worker_wake(struct io_worker *worker, void *data)\n{\n\t__set_notify_signal(worker->task);\n\twake_up_process(worker->task);\n\treturn false;\n}\n\nstatic void io_run_cancel(struct io_wq_work *work, struct io_wq *wq)\n{\n\tdo {\n\t\twork->flags |= IO_WQ_WORK_CANCEL;\n\t\twq->do_work(work);\n\t\twork = wq->free_work(work);\n\t} while (work);\n}\n\nstatic void io_wq_insert_work(struct io_wq *wq, struct io_wq_work *work)\n{\n\tstruct io_wq_acct *acct = io_work_get_acct(wq, work);\n\tunsigned int hash;\n\tstruct io_wq_work *tail;\n\n\tif (!io_wq_is_hashed(work)) {\nappend:\n\t\twq_list_add_tail(&work->list, &acct->work_list);\n\t\treturn;\n\t}\n\n\thash = io_get_work_hash(work);\n\ttail = wq->hash_tail[hash];\n\twq->hash_tail[hash] = work;\n\tif (!tail)\n\t\tgoto append;\n\n\twq_list_add_after(&work->list, &tail->list, &acct->work_list);\n}\n\nstatic bool io_wq_work_match_item(struct io_wq_work *work, void *data)\n{\n\treturn work == data;\n}\n\nvoid io_wq_enqueue(struct io_wq *wq, struct io_wq_work *work)\n{\n\tstruct io_wq_acct *acct = io_work_get_acct(wq, work);\n\tstruct io_cb_cancel_data match;\n\tunsigned work_flags = work->flags;\n\tbool do_create;\n\n\t \n\tif (test_bit(IO_WQ_BIT_EXIT, &wq->state) ||\n\t    (work->flags & IO_WQ_WORK_CANCEL)) {\n\t\tio_run_cancel(work, wq);\n\t\treturn;\n\t}\n\n\traw_spin_lock(&acct->lock);\n\tio_wq_insert_work(wq, work);\n\tclear_bit(IO_ACCT_STALLED_BIT, &acct->flags);\n\traw_spin_unlock(&acct->lock);\n\n\trcu_read_lock();\n\tdo_create = !io_wq_activate_free_worker(wq, acct);\n\trcu_read_unlock();\n\n\tif (do_create && ((work_flags & IO_WQ_WORK_CONCURRENT) ||\n\t    !atomic_read(&acct->nr_running))) {\n\t\tbool did_create;\n\n\t\tdid_create = io_wq_create_worker(wq, acct);\n\t\tif (likely(did_create))\n\t\t\treturn;\n\n\t\traw_spin_lock(&wq->lock);\n\t\tif (acct->nr_workers) {\n\t\t\traw_spin_unlock(&wq->lock);\n\t\t\treturn;\n\t\t}\n\t\traw_spin_unlock(&wq->lock);\n\n\t\t \n\t\tmatch.fn\t\t= io_wq_work_match_item,\n\t\tmatch.data\t\t= work,\n\t\tmatch.cancel_all\t= false,\n\n\t\tio_acct_cancel_pending_work(wq, acct, &match);\n\t}\n}\n\n \nvoid io_wq_hash_work(struct io_wq_work *work, void *val)\n{\n\tunsigned int bit;\n\n\tbit = hash_ptr(val, IO_WQ_HASH_ORDER);\n\twork->flags |= (IO_WQ_WORK_HASHED | (bit << IO_WQ_HASH_SHIFT));\n}\n\nstatic bool __io_wq_worker_cancel(struct io_worker *worker,\n\t\t\t\t  struct io_cb_cancel_data *match,\n\t\t\t\t  struct io_wq_work *work)\n{\n\tif (work && match->fn(work, match->data)) {\n\t\twork->flags |= IO_WQ_WORK_CANCEL;\n\t\t__set_notify_signal(worker->task);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool io_wq_worker_cancel(struct io_worker *worker, void *data)\n{\n\tstruct io_cb_cancel_data *match = data;\n\n\t \n\traw_spin_lock(&worker->lock);\n\tif (__io_wq_worker_cancel(worker, match, worker->cur_work) ||\n\t    __io_wq_worker_cancel(worker, match, worker->next_work))\n\t\tmatch->nr_running++;\n\traw_spin_unlock(&worker->lock);\n\n\treturn match->nr_running && !match->cancel_all;\n}\n\nstatic inline void io_wq_remove_pending(struct io_wq *wq,\n\t\t\t\t\t struct io_wq_work *work,\n\t\t\t\t\t struct io_wq_work_node *prev)\n{\n\tstruct io_wq_acct *acct = io_work_get_acct(wq, work);\n\tunsigned int hash = io_get_work_hash(work);\n\tstruct io_wq_work *prev_work = NULL;\n\n\tif (io_wq_is_hashed(work) && work == wq->hash_tail[hash]) {\n\t\tif (prev)\n\t\t\tprev_work = container_of(prev, struct io_wq_work, list);\n\t\tif (prev_work && io_get_work_hash(prev_work) == hash)\n\t\t\twq->hash_tail[hash] = prev_work;\n\t\telse\n\t\t\twq->hash_tail[hash] = NULL;\n\t}\n\twq_list_del(&acct->work_list, &work->list, prev);\n}\n\nstatic bool io_acct_cancel_pending_work(struct io_wq *wq,\n\t\t\t\t\tstruct io_wq_acct *acct,\n\t\t\t\t\tstruct io_cb_cancel_data *match)\n{\n\tstruct io_wq_work_node *node, *prev;\n\tstruct io_wq_work *work;\n\n\traw_spin_lock(&acct->lock);\n\twq_list_for_each(node, prev, &acct->work_list) {\n\t\twork = container_of(node, struct io_wq_work, list);\n\t\tif (!match->fn(work, match->data))\n\t\t\tcontinue;\n\t\tio_wq_remove_pending(wq, work, prev);\n\t\traw_spin_unlock(&acct->lock);\n\t\tio_run_cancel(work, wq);\n\t\tmatch->nr_pending++;\n\t\t \n\t\treturn true;\n\t}\n\traw_spin_unlock(&acct->lock);\n\n\treturn false;\n}\n\nstatic void io_wq_cancel_pending_work(struct io_wq *wq,\n\t\t\t\t      struct io_cb_cancel_data *match)\n{\n\tint i;\nretry:\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++) {\n\t\tstruct io_wq_acct *acct = io_get_acct(wq, i == 0);\n\n\t\tif (io_acct_cancel_pending_work(wq, acct, match)) {\n\t\t\tif (match->cancel_all)\n\t\t\t\tgoto retry;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void io_wq_cancel_running_work(struct io_wq *wq,\n\t\t\t\t       struct io_cb_cancel_data *match)\n{\n\trcu_read_lock();\n\tio_wq_for_each_worker(wq, io_wq_worker_cancel, match);\n\trcu_read_unlock();\n}\n\nenum io_wq_cancel io_wq_cancel_cb(struct io_wq *wq, work_cancel_fn *cancel,\n\t\t\t\t  void *data, bool cancel_all)\n{\n\tstruct io_cb_cancel_data match = {\n\t\t.fn\t\t= cancel,\n\t\t.data\t\t= data,\n\t\t.cancel_all\t= cancel_all,\n\t};\n\n\t \n\tio_wq_cancel_pending_work(wq, &match);\n\tif (match.nr_pending && !match.cancel_all)\n\t\treturn IO_WQ_CANCEL_OK;\n\n\traw_spin_lock(&wq->lock);\n\tio_wq_cancel_running_work(wq, &match);\n\traw_spin_unlock(&wq->lock);\n\tif (match.nr_running && !match.cancel_all)\n\t\treturn IO_WQ_CANCEL_RUNNING;\n\n\tif (match.nr_running)\n\t\treturn IO_WQ_CANCEL_RUNNING;\n\tif (match.nr_pending)\n\t\treturn IO_WQ_CANCEL_OK;\n\treturn IO_WQ_CANCEL_NOTFOUND;\n}\n\nstatic int io_wq_hash_wake(struct wait_queue_entry *wait, unsigned mode,\n\t\t\t    int sync, void *key)\n{\n\tstruct io_wq *wq = container_of(wait, struct io_wq, wait);\n\tint i;\n\n\tlist_del_init(&wait->entry);\n\n\trcu_read_lock();\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++) {\n\t\tstruct io_wq_acct *acct = &wq->acct[i];\n\n\t\tif (test_and_clear_bit(IO_ACCT_STALLED_BIT, &acct->flags))\n\t\t\tio_wq_activate_free_worker(wq, acct);\n\t}\n\trcu_read_unlock();\n\treturn 1;\n}\n\nstruct io_wq *io_wq_create(unsigned bounded, struct io_wq_data *data)\n{\n\tint ret, i;\n\tstruct io_wq *wq;\n\n\tif (WARN_ON_ONCE(!data->free_work || !data->do_work))\n\t\treturn ERR_PTR(-EINVAL);\n\tif (WARN_ON_ONCE(!bounded))\n\t\treturn ERR_PTR(-EINVAL);\n\n\twq = kzalloc(sizeof(struct io_wq), GFP_KERNEL);\n\tif (!wq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trefcount_inc(&data->hash->refs);\n\twq->hash = data->hash;\n\twq->free_work = data->free_work;\n\twq->do_work = data->do_work;\n\n\tret = -ENOMEM;\n\n\tif (!alloc_cpumask_var(&wq->cpu_mask, GFP_KERNEL))\n\t\tgoto err;\n\tcpumask_copy(wq->cpu_mask, cpu_possible_mask);\n\twq->acct[IO_WQ_ACCT_BOUND].max_workers = bounded;\n\twq->acct[IO_WQ_ACCT_UNBOUND].max_workers =\n\t\t\t\ttask_rlimit(current, RLIMIT_NPROC);\n\tINIT_LIST_HEAD(&wq->wait.entry);\n\twq->wait.func = io_wq_hash_wake;\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++) {\n\t\tstruct io_wq_acct *acct = &wq->acct[i];\n\n\t\tacct->index = i;\n\t\tatomic_set(&acct->nr_running, 0);\n\t\tINIT_WQ_LIST(&acct->work_list);\n\t\traw_spin_lock_init(&acct->lock);\n\t}\n\n\traw_spin_lock_init(&wq->lock);\n\tINIT_HLIST_NULLS_HEAD(&wq->free_list, 0);\n\tINIT_LIST_HEAD(&wq->all_list);\n\n\twq->task = get_task_struct(data->task);\n\tatomic_set(&wq->worker_refs, 1);\n\tinit_completion(&wq->worker_done);\n\tret = cpuhp_state_add_instance_nocalls(io_wq_online, &wq->cpuhp_node);\n\tif (ret)\n\t\tgoto err;\n\n\treturn wq;\nerr:\n\tio_wq_put_hash(data->hash);\n\tfree_cpumask_var(wq->cpu_mask);\n\tkfree(wq);\n\treturn ERR_PTR(ret);\n}\n\nstatic bool io_task_work_match(struct callback_head *cb, void *data)\n{\n\tstruct io_worker *worker;\n\n\tif (cb->func != create_worker_cb && cb->func != create_worker_cont)\n\t\treturn false;\n\tworker = container_of(cb, struct io_worker, create_work);\n\treturn worker->wq == data;\n}\n\nvoid io_wq_exit_start(struct io_wq *wq)\n{\n\tset_bit(IO_WQ_BIT_EXIT, &wq->state);\n}\n\nstatic void io_wq_cancel_tw_create(struct io_wq *wq)\n{\n\tstruct callback_head *cb;\n\n\twhile ((cb = task_work_cancel_match(wq->task, io_task_work_match, wq)) != NULL) {\n\t\tstruct io_worker *worker;\n\n\t\tworker = container_of(cb, struct io_worker, create_work);\n\t\tio_worker_cancel_cb(worker);\n\t\t \n\t\tif (cb->func == create_worker_cont)\n\t\t\tkfree(worker);\n\t}\n}\n\nstatic void io_wq_exit_workers(struct io_wq *wq)\n{\n\tif (!wq->task)\n\t\treturn;\n\n\tio_wq_cancel_tw_create(wq);\n\n\trcu_read_lock();\n\tio_wq_for_each_worker(wq, io_wq_worker_wake, NULL);\n\trcu_read_unlock();\n\tio_worker_ref_put(wq);\n\twait_for_completion(&wq->worker_done);\n\n\tspin_lock_irq(&wq->hash->wait.lock);\n\tlist_del_init(&wq->wait.entry);\n\tspin_unlock_irq(&wq->hash->wait.lock);\n\n\tput_task_struct(wq->task);\n\twq->task = NULL;\n}\n\nstatic void io_wq_destroy(struct io_wq *wq)\n{\n\tstruct io_cb_cancel_data match = {\n\t\t.fn\t\t= io_wq_work_match_all,\n\t\t.cancel_all\t= true,\n\t};\n\n\tcpuhp_state_remove_instance_nocalls(io_wq_online, &wq->cpuhp_node);\n\tio_wq_cancel_pending_work(wq, &match);\n\tfree_cpumask_var(wq->cpu_mask);\n\tio_wq_put_hash(wq->hash);\n\tkfree(wq);\n}\n\nvoid io_wq_put_and_exit(struct io_wq *wq)\n{\n\tWARN_ON_ONCE(!test_bit(IO_WQ_BIT_EXIT, &wq->state));\n\n\tio_wq_exit_workers(wq);\n\tio_wq_destroy(wq);\n}\n\nstruct online_data {\n\tunsigned int cpu;\n\tbool online;\n};\n\nstatic bool io_wq_worker_affinity(struct io_worker *worker, void *data)\n{\n\tstruct online_data *od = data;\n\n\tif (od->online)\n\t\tcpumask_set_cpu(od->cpu, worker->wq->cpu_mask);\n\telse\n\t\tcpumask_clear_cpu(od->cpu, worker->wq->cpu_mask);\n\treturn false;\n}\n\nstatic int __io_wq_cpu_online(struct io_wq *wq, unsigned int cpu, bool online)\n{\n\tstruct online_data od = {\n\t\t.cpu = cpu,\n\t\t.online = online\n\t};\n\n\trcu_read_lock();\n\tio_wq_for_each_worker(wq, io_wq_worker_affinity, &od);\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic int io_wq_cpu_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct io_wq *wq = hlist_entry_safe(node, struct io_wq, cpuhp_node);\n\n\treturn __io_wq_cpu_online(wq, cpu, true);\n}\n\nstatic int io_wq_cpu_offline(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct io_wq *wq = hlist_entry_safe(node, struct io_wq, cpuhp_node);\n\n\treturn __io_wq_cpu_online(wq, cpu, false);\n}\n\nint io_wq_cpu_affinity(struct io_uring_task *tctx, cpumask_var_t mask)\n{\n\tif (!tctx || !tctx->io_wq)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\tif (mask)\n\t\tcpumask_copy(tctx->io_wq->cpu_mask, mask);\n\telse\n\t\tcpumask_copy(tctx->io_wq->cpu_mask, cpu_possible_mask);\n\trcu_read_unlock();\n\n\treturn 0;\n}\n\n \nint io_wq_max_workers(struct io_wq *wq, int *new_count)\n{\n\tstruct io_wq_acct *acct;\n\tint prev[IO_WQ_ACCT_NR];\n\tint i;\n\n\tBUILD_BUG_ON((int) IO_WQ_ACCT_BOUND   != (int) IO_WQ_BOUND);\n\tBUILD_BUG_ON((int) IO_WQ_ACCT_UNBOUND != (int) IO_WQ_UNBOUND);\n\tBUILD_BUG_ON((int) IO_WQ_ACCT_NR      != 2);\n\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++) {\n\t\tif (new_count[i] > task_rlimit(current, RLIMIT_NPROC))\n\t\t\tnew_count[i] = task_rlimit(current, RLIMIT_NPROC);\n\t}\n\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++)\n\t\tprev[i] = 0;\n\n\trcu_read_lock();\n\n\traw_spin_lock(&wq->lock);\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++) {\n\t\tacct = &wq->acct[i];\n\t\tprev[i] = max_t(int, acct->max_workers, prev[i]);\n\t\tif (new_count[i])\n\t\t\tacct->max_workers = new_count[i];\n\t}\n\traw_spin_unlock(&wq->lock);\n\trcu_read_unlock();\n\n\tfor (i = 0; i < IO_WQ_ACCT_NR; i++)\n\t\tnew_count[i] = prev[i];\n\n\treturn 0;\n}\n\nstatic __init int io_wq_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, \"io-wq/online\",\n\t\t\t\t\tio_wq_cpu_online, io_wq_cpu_offline);\n\tif (ret < 0)\n\t\treturn ret;\n\tio_wq_online = ret;\n\treturn 0;\n}\nsubsys_initcall(io_wq_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}