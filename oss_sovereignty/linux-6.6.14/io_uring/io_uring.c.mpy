{
  "module_name": "io_uring.c",
  "hash_id": "27c0b33c13de5b2f92713ccfc2ebc116e88cbd303d0c59cd5dc3c3621331b5f0",
  "original_prompt": "Ingested from linux-6.6.14/io_uring/io_uring.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/syscalls.h>\n#include <net/compat.h>\n#include <linux/refcount.h>\n#include <linux/uio.h>\n#include <linux/bits.h>\n\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/fdtable.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/bvec.h>\n#include <linux/net.h>\n#include <net/sock.h>\n#include <net/af_unix.h>\n#include <net/scm.h>\n#include <linux/anon_inodes.h>\n#include <linux/sched/mm.h>\n#include <linux/uaccess.h>\n#include <linux/nospec.h>\n#include <linux/highmem.h>\n#include <linux/fsnotify.h>\n#include <linux/fadvise.h>\n#include <linux/task_work.h>\n#include <linux/io_uring.h>\n#include <linux/audit.h>\n#include <linux/security.h>\n#include <asm/shmparam.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/io_uring.h>\n\n#include <uapi/linux/io_uring.h>\n\n#include \"io-wq.h\"\n\n#include \"io_uring.h\"\n#include \"opdef.h\"\n#include \"refs.h\"\n#include \"tctx.h\"\n#include \"sqpoll.h\"\n#include \"fdinfo.h\"\n#include \"kbuf.h\"\n#include \"rsrc.h\"\n#include \"cancel.h\"\n#include \"net.h\"\n#include \"notif.h\"\n\n#include \"timeout.h\"\n#include \"poll.h\"\n#include \"rw.h\"\n#include \"alloc_cache.h\"\n\n#define IORING_MAX_ENTRIES\t32768\n#define IORING_MAX_CQ_ENTRIES\t(2 * IORING_MAX_ENTRIES)\n\n#define IORING_MAX_RESTRICTIONS\t(IORING_RESTRICTION_LAST + \\\n\t\t\t\t IORING_REGISTER_LAST + IORING_OP_LAST)\n\n#define SQE_COMMON_FLAGS (IOSQE_FIXED_FILE | IOSQE_IO_LINK | \\\n\t\t\t  IOSQE_IO_HARDLINK | IOSQE_ASYNC)\n\n#define SQE_VALID_FLAGS\t(SQE_COMMON_FLAGS | IOSQE_BUFFER_SELECT | \\\n\t\t\tIOSQE_IO_DRAIN | IOSQE_CQE_SKIP_SUCCESS)\n\n#define IO_REQ_CLEAN_FLAGS (REQ_F_BUFFER_SELECTED | REQ_F_NEED_CLEANUP | \\\n\t\t\t\tREQ_F_POLLED | REQ_F_INFLIGHT | REQ_F_CREDS | \\\n\t\t\t\tREQ_F_ASYNC_DATA)\n\n#define IO_REQ_CLEAN_SLOW_FLAGS (REQ_F_REFCOUNT | REQ_F_LINK | REQ_F_HARDLINK |\\\n\t\t\t\t IO_REQ_CLEAN_FLAGS)\n\n#define IO_TCTX_REFS_CACHE_NR\t(1U << 10)\n\n#define IO_COMPL_BATCH\t\t\t32\n#define IO_REQ_ALLOC_BATCH\t\t8\n\nenum {\n\tIO_CHECK_CQ_OVERFLOW_BIT,\n\tIO_CHECK_CQ_DROPPED_BIT,\n};\n\nenum {\n\tIO_EVENTFD_OP_SIGNAL_BIT,\n\tIO_EVENTFD_OP_FREE_BIT,\n};\n\nstruct io_defer_entry {\n\tstruct list_head\tlist;\n\tstruct io_kiocb\t\t*req;\n\tu32\t\t\tseq;\n};\n\n \n#define IO_DISARM_MASK (REQ_F_ARM_LTIMEOUT | REQ_F_LINK_TIMEOUT | REQ_F_FAIL)\n#define IO_REQ_LINK_FLAGS (REQ_F_LINK | REQ_F_HARDLINK)\n\nstatic bool io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct task_struct *task,\n\t\t\t\t\t bool cancel_all);\n\nstatic void io_queue_sqe(struct io_kiocb *req);\n\nstruct kmem_cache *req_cachep;\n\nstatic int __read_mostly sysctl_io_uring_disabled;\nstatic int __read_mostly sysctl_io_uring_group = -1;\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table kernel_io_uring_disabled_table[] = {\n\t{\n\t\t.procname\t= \"io_uring_disabled\",\n\t\t.data\t\t= &sysctl_io_uring_disabled,\n\t\t.maxlen\t\t= sizeof(sysctl_io_uring_disabled),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_TWO,\n\t},\n\t{\n\t\t.procname\t= \"io_uring_group\",\n\t\t.data\t\t= &sysctl_io_uring_group,\n\t\t.maxlen\t\t= sizeof(gid_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{},\n};\n#endif\n\nstruct sock *io_uring_get_socket(struct file *file)\n{\n#if defined(CONFIG_UNIX)\n\tif (io_is_uring_fops(file)) {\n\t\tstruct io_ring_ctx *ctx = file->private_data;\n\n\t\treturn ctx->ring_sock->sk;\n\t}\n#endif\n\treturn NULL;\n}\nEXPORT_SYMBOL(io_uring_get_socket);\n\nstatic inline void io_submit_flush_completions(struct io_ring_ctx *ctx)\n{\n\tif (!wq_list_empty(&ctx->submit_state.compl_reqs) ||\n\t    ctx->submit_state.cqes_count)\n\t\t__io_submit_flush_completions(ctx);\n}\n\nstatic inline unsigned int __io_cqring_events(struct io_ring_ctx *ctx)\n{\n\treturn ctx->cached_cq_tail - READ_ONCE(ctx->rings->cq.head);\n}\n\nstatic inline unsigned int __io_cqring_events_user(struct io_ring_ctx *ctx)\n{\n\treturn READ_ONCE(ctx->rings->cq.tail) - READ_ONCE(ctx->rings->cq.head);\n}\n\nstatic bool io_match_linked(struct io_kiocb *head)\n{\n\tstruct io_kiocb *req;\n\n\tio_for_each_link(req, head) {\n\t\tif (req->flags & REQ_F_INFLIGHT)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nbool io_match_task_safe(struct io_kiocb *head, struct task_struct *task,\n\t\t\tbool cancel_all)\n{\n\tbool matched;\n\n\tif (task && head->task != task)\n\t\treturn false;\n\tif (cancel_all)\n\t\treturn true;\n\n\tif (head->flags & REQ_F_LINK_TIMEOUT) {\n\t\tstruct io_ring_ctx *ctx = head->ctx;\n\n\t\t \n\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\tmatched = io_match_linked(head);\n\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t} else {\n\t\tmatched = io_match_linked(head);\n\t}\n\treturn matched;\n}\n\nstatic inline void req_fail_link_node(struct io_kiocb *req, int res)\n{\n\treq_set_fail(req);\n\tio_req_set_res(req, res, 0);\n}\n\nstatic inline void io_req_add_to_cache(struct io_kiocb *req, struct io_ring_ctx *ctx)\n{\n\twq_stack_add_head(&req->comp_list, &ctx->submit_state.free_list);\n}\n\nstatic __cold void io_ring_ctx_ref_free(struct percpu_ref *ref)\n{\n\tstruct io_ring_ctx *ctx = container_of(ref, struct io_ring_ctx, refs);\n\n\tcomplete(&ctx->ref_comp);\n}\n\nstatic __cold void io_fallback_req_func(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx,\n\t\t\t\t\t\tfallback_work.work);\n\tstruct llist_node *node = llist_del_all(&ctx->fallback_llist);\n\tstruct io_kiocb *req, *tmp;\n\tstruct io_tw_state ts = { .locked = true, };\n\n\tpercpu_ref_get(&ctx->refs);\n\tmutex_lock(&ctx->uring_lock);\n\tllist_for_each_entry_safe(req, tmp, node, io_task_work.node)\n\t\treq->io_task_work.func(req, &ts);\n\tif (WARN_ON_ONCE(!ts.locked))\n\t\treturn;\n\tio_submit_flush_completions(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic int io_alloc_hash_table(struct io_hash_table *table, unsigned bits)\n{\n\tunsigned hash_buckets = 1U << bits;\n\tsize_t hash_size = hash_buckets * sizeof(table->hbs[0]);\n\n\ttable->hbs = kmalloc(hash_size, GFP_KERNEL);\n\tif (!table->hbs)\n\t\treturn -ENOMEM;\n\n\ttable->hash_bits = bits;\n\tinit_hash_table(table, hash_buckets);\n\treturn 0;\n}\n\nstatic __cold struct io_ring_ctx *io_ring_ctx_alloc(struct io_uring_params *p)\n{\n\tstruct io_ring_ctx *ctx;\n\tint hash_bits;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn NULL;\n\n\txa_init(&ctx->io_bl_xa);\n\n\t \n\thash_bits = ilog2(p->cq_entries) - 5;\n\thash_bits = clamp(hash_bits, 1, 8);\n\tif (io_alloc_hash_table(&ctx->cancel_table, hash_bits))\n\t\tgoto err;\n\tif (io_alloc_hash_table(&ctx->cancel_table_locked, hash_bits))\n\t\tgoto err;\n\tif (percpu_ref_init(&ctx->refs, io_ring_ctx_ref_free,\n\t\t\t    0, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->flags = p->flags;\n\tinit_waitqueue_head(&ctx->sqo_sq_wait);\n\tINIT_LIST_HEAD(&ctx->sqd_list);\n\tINIT_LIST_HEAD(&ctx->cq_overflow_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_cache);\n\tINIT_HLIST_HEAD(&ctx->io_buf_list);\n\tio_alloc_cache_init(&ctx->rsrc_node_cache, IO_NODE_ALLOC_CACHE_MAX,\n\t\t\t    sizeof(struct io_rsrc_node));\n\tio_alloc_cache_init(&ctx->apoll_cache, IO_ALLOC_CACHE_MAX,\n\t\t\t    sizeof(struct async_poll));\n\tio_alloc_cache_init(&ctx->netmsg_cache, IO_ALLOC_CACHE_MAX,\n\t\t\t    sizeof(struct io_async_msghdr));\n\tinit_completion(&ctx->ref_comp);\n\txa_init_flags(&ctx->personalities, XA_FLAGS_ALLOC1);\n\tmutex_init(&ctx->uring_lock);\n\tinit_waitqueue_head(&ctx->cq_wait);\n\tinit_waitqueue_head(&ctx->poll_wq);\n\tinit_waitqueue_head(&ctx->rsrc_quiesce_wq);\n\tspin_lock_init(&ctx->completion_lock);\n\tspin_lock_init(&ctx->timeout_lock);\n\tINIT_WQ_LIST(&ctx->iopoll_list);\n\tINIT_LIST_HEAD(&ctx->io_buffers_pages);\n\tINIT_LIST_HEAD(&ctx->io_buffers_comp);\n\tINIT_LIST_HEAD(&ctx->defer_list);\n\tINIT_LIST_HEAD(&ctx->timeout_list);\n\tINIT_LIST_HEAD(&ctx->ltimeout_list);\n\tINIT_LIST_HEAD(&ctx->rsrc_ref_list);\n\tinit_llist_head(&ctx->work_llist);\n\tINIT_LIST_HEAD(&ctx->tctx_list);\n\tctx->submit_state.free_list.next = NULL;\n\tINIT_WQ_LIST(&ctx->locked_free_list);\n\tINIT_DELAYED_WORK(&ctx->fallback_work, io_fallback_req_func);\n\tINIT_WQ_LIST(&ctx->submit_state.compl_reqs);\n\treturn ctx;\nerr:\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->cancel_table_locked.hbs);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n\treturn NULL;\n}\n\nstatic void io_account_cq_overflow(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *r = ctx->rings;\n\n\tWRITE_ONCE(r->cq_overflow, READ_ONCE(r->cq_overflow) + 1);\n\tctx->cq_extra--;\n}\n\nstatic bool req_need_defer(struct io_kiocb *req, u32 seq)\n{\n\tif (unlikely(req->flags & REQ_F_IO_DRAIN)) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\treturn seq + READ_ONCE(ctx->cq_extra) != ctx->cached_cq_tail;\n\t}\n\n\treturn false;\n}\n\nstatic void io_clean_op(struct io_kiocb *req)\n{\n\tif (req->flags & REQ_F_BUFFER_SELECTED) {\n\t\tspin_lock(&req->ctx->completion_lock);\n\t\tio_put_kbuf_comp(req);\n\t\tspin_unlock(&req->ctx->completion_lock);\n\t}\n\n\tif (req->flags & REQ_F_NEED_CLEANUP) {\n\t\tconst struct io_cold_def *def = &io_cold_defs[req->opcode];\n\n\t\tif (def->cleanup)\n\t\t\tdef->cleanup(req);\n\t}\n\tif ((req->flags & REQ_F_POLLED) && req->apoll) {\n\t\tkfree(req->apoll->double_poll);\n\t\tkfree(req->apoll);\n\t\treq->apoll = NULL;\n\t}\n\tif (req->flags & REQ_F_INFLIGHT) {\n\t\tstruct io_uring_task *tctx = req->task->io_uring;\n\n\t\tatomic_dec(&tctx->inflight_tracked);\n\t}\n\tif (req->flags & REQ_F_CREDS)\n\t\tput_cred(req->creds);\n\tif (req->flags & REQ_F_ASYNC_DATA) {\n\t\tkfree(req->async_data);\n\t\treq->async_data = NULL;\n\t}\n\treq->flags &= ~IO_REQ_CLEAN_FLAGS;\n}\n\nstatic inline void io_req_track_inflight(struct io_kiocb *req)\n{\n\tif (!(req->flags & REQ_F_INFLIGHT)) {\n\t\treq->flags |= REQ_F_INFLIGHT;\n\t\tatomic_inc(&req->task->io_uring->inflight_tracked);\n\t}\n}\n\nstatic struct io_kiocb *__io_prep_linked_timeout(struct io_kiocb *req)\n{\n\tif (WARN_ON_ONCE(!req->link))\n\t\treturn NULL;\n\n\treq->flags &= ~REQ_F_ARM_LTIMEOUT;\n\treq->flags |= REQ_F_LINK_TIMEOUT;\n\n\t \n\tio_req_set_refcount(req);\n\t__io_req_set_refcount(req->link, 2);\n\treturn req->link;\n}\n\nstatic inline struct io_kiocb *io_prep_linked_timeout(struct io_kiocb *req)\n{\n\tif (likely(!(req->flags & REQ_F_ARM_LTIMEOUT)))\n\t\treturn NULL;\n\treturn __io_prep_linked_timeout(req);\n}\n\nstatic noinline void __io_arm_ltimeout(struct io_kiocb *req)\n{\n\tio_queue_linked_timeout(__io_prep_linked_timeout(req));\n}\n\nstatic inline void io_arm_ltimeout(struct io_kiocb *req)\n{\n\tif (unlikely(req->flags & REQ_F_ARM_LTIMEOUT))\n\t\t__io_arm_ltimeout(req);\n}\n\nstatic void io_prep_async_work(struct io_kiocb *req)\n{\n\tconst struct io_issue_def *def = &io_issue_defs[req->opcode];\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tif (!(req->flags & REQ_F_CREDS)) {\n\t\treq->flags |= REQ_F_CREDS;\n\t\treq->creds = get_current_cred();\n\t}\n\n\treq->work.list.next = NULL;\n\treq->work.flags = 0;\n\treq->work.cancel_seq = atomic_read(&ctx->cancel_seq);\n\tif (req->flags & REQ_F_FORCE_ASYNC)\n\t\treq->work.flags |= IO_WQ_WORK_CONCURRENT;\n\n\tif (req->file && !(req->flags & REQ_F_FIXED_FILE))\n\t\treq->flags |= io_file_get_flags(req->file);\n\n\tif (req->file && (req->flags & REQ_F_ISREG)) {\n\t\tbool should_hash = def->hash_reg_file;\n\n\t\t \n\t\tif (should_hash && (req->file->f_flags & O_DIRECT) &&\n\t\t    (req->file->f_mode & FMODE_DIO_PARALLEL_WRITE))\n\t\t\tshould_hash = false;\n\t\tif (should_hash || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\tio_wq_hash_work(&req->work, file_inode(req->file));\n\t} else if (!req->file || !S_ISBLK(file_inode(req->file)->i_mode)) {\n\t\tif (def->unbound_nonreg_file)\n\t\t\treq->work.flags |= IO_WQ_WORK_UNBOUND;\n\t}\n}\n\nstatic void io_prep_async_link(struct io_kiocb *req)\n{\n\tstruct io_kiocb *cur;\n\n\tif (req->flags & REQ_F_LINK_TIMEOUT) {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\tspin_lock_irq(&ctx->timeout_lock);\n\t\tio_for_each_link(cur, req)\n\t\t\tio_prep_async_work(cur);\n\t\tspin_unlock_irq(&ctx->timeout_lock);\n\t} else {\n\t\tio_for_each_link(cur, req)\n\t\t\tio_prep_async_work(cur);\n\t}\n}\n\nvoid io_queue_iowq(struct io_kiocb *req, struct io_tw_state *ts_dont_use)\n{\n\tstruct io_kiocb *link = io_prep_linked_timeout(req);\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\n\tBUG_ON(!tctx);\n\tBUG_ON(!tctx->io_wq);\n\n\t \n\tio_prep_async_link(req);\n\n\t \n\tif (WARN_ON_ONCE(!same_thread_group(req->task, current)))\n\t\treq->work.flags |= IO_WQ_WORK_CANCEL;\n\n\ttrace_io_uring_queue_async_work(req, io_wq_is_hashed(&req->work));\n\tio_wq_enqueue(tctx->io_wq, &req->work);\n\tif (link)\n\t\tio_queue_linked_timeout(link);\n}\n\nstatic __cold void io_queue_deferred(struct io_ring_ctx *ctx)\n{\n\twhile (!list_empty(&ctx->defer_list)) {\n\t\tstruct io_defer_entry *de = list_first_entry(&ctx->defer_list,\n\t\t\t\t\t\tstruct io_defer_entry, list);\n\n\t\tif (req_need_defer(de->req, de->seq))\n\t\t\tbreak;\n\t\tlist_del_init(&de->list);\n\t\tio_req_task_queue(de->req);\n\t\tkfree(de);\n\t}\n}\n\n\nstatic void io_eventfd_ops(struct rcu_head *rcu)\n{\n\tstruct io_ev_fd *ev_fd = container_of(rcu, struct io_ev_fd, rcu);\n\tint ops = atomic_xchg(&ev_fd->ops, 0);\n\n\tif (ops & BIT(IO_EVENTFD_OP_SIGNAL_BIT))\n\t\teventfd_signal_mask(ev_fd->cq_ev_fd, 1, EPOLL_URING_WAKE);\n\n\t \n\tif (atomic_dec_and_test(&ev_fd->refs)) {\n\t\teventfd_ctx_put(ev_fd->cq_ev_fd);\n\t\tkfree(ev_fd);\n\t}\n}\n\nstatic void io_eventfd_signal(struct io_ring_ctx *ctx)\n{\n\tstruct io_ev_fd *ev_fd = NULL;\n\n\trcu_read_lock();\n\t \n\tev_fd = rcu_dereference(ctx->io_ev_fd);\n\n\t \n\tif (unlikely(!ev_fd))\n\t\tgoto out;\n\tif (READ_ONCE(ctx->rings->cq_flags) & IORING_CQ_EVENTFD_DISABLED)\n\t\tgoto out;\n\tif (ev_fd->eventfd_async && !io_wq_current_is_worker())\n\t\tgoto out;\n\n\tif (likely(eventfd_signal_allowed())) {\n\t\teventfd_signal_mask(ev_fd->cq_ev_fd, 1, EPOLL_URING_WAKE);\n\t} else {\n\t\tatomic_inc(&ev_fd->refs);\n\t\tif (!atomic_fetch_or(BIT(IO_EVENTFD_OP_SIGNAL_BIT), &ev_fd->ops))\n\t\t\tcall_rcu_hurry(&ev_fd->rcu, io_eventfd_ops);\n\t\telse\n\t\t\tatomic_dec(&ev_fd->refs);\n\t}\n\nout:\n\trcu_read_unlock();\n}\n\nstatic void io_eventfd_flush_signal(struct io_ring_ctx *ctx)\n{\n\tbool skip;\n\n\tspin_lock(&ctx->completion_lock);\n\n\t \n\tskip = ctx->cached_cq_tail == ctx->evfd_last_cq_tail;\n\tctx->evfd_last_cq_tail = ctx->cached_cq_tail;\n\tspin_unlock(&ctx->completion_lock);\n\tif (skip)\n\t\treturn;\n\n\tio_eventfd_signal(ctx);\n}\n\nvoid __io_commit_cqring_flush(struct io_ring_ctx *ctx)\n{\n\tif (ctx->poll_activated)\n\t\tio_poll_wq_wake(ctx);\n\tif (ctx->off_timeout_used)\n\t\tio_flush_timeouts(ctx);\n\tif (ctx->drain_active) {\n\t\tspin_lock(&ctx->completion_lock);\n\t\tio_queue_deferred(ctx);\n\t\tspin_unlock(&ctx->completion_lock);\n\t}\n\tif (ctx->has_evfd)\n\t\tio_eventfd_flush_signal(ctx);\n}\n\nstatic inline void __io_cq_lock(struct io_ring_ctx *ctx)\n{\n\tif (!ctx->lockless_cq)\n\t\tspin_lock(&ctx->completion_lock);\n}\n\nstatic inline void io_cq_lock(struct io_ring_ctx *ctx)\n\t__acquires(ctx->completion_lock)\n{\n\tspin_lock(&ctx->completion_lock);\n}\n\nstatic inline void __io_cq_unlock_post(struct io_ring_ctx *ctx)\n{\n\tio_commit_cqring(ctx);\n\tif (!ctx->task_complete) {\n\t\tif (!ctx->lockless_cq)\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t \n\t\tif (!ctx->syscall_iopoll)\n\t\t\tio_cqring_wake(ctx);\n\t}\n\tio_commit_cqring_flush(ctx);\n}\n\nstatic void io_cq_unlock_post(struct io_ring_ctx *ctx)\n\t__releases(ctx->completion_lock)\n{\n\tio_commit_cqring(ctx);\n\tspin_unlock(&ctx->completion_lock);\n\tio_cqring_wake(ctx);\n\tio_commit_cqring_flush(ctx);\n}\n\n \nstatic void io_cqring_overflow_kill(struct io_ring_ctx *ctx)\n{\n\tstruct io_overflow_cqe *ocqe;\n\tLIST_HEAD(list);\n\n\tspin_lock(&ctx->completion_lock);\n\tlist_splice_init(&ctx->cq_overflow_list, &list);\n\tclear_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq);\n\tspin_unlock(&ctx->completion_lock);\n\n\twhile (!list_empty(&list)) {\n\t\tocqe = list_first_entry(&list, struct io_overflow_cqe, list);\n\t\tlist_del(&ocqe->list);\n\t\tkfree(ocqe);\n\t}\n}\n\nstatic void __io_cqring_overflow_flush(struct io_ring_ctx *ctx)\n{\n\tsize_t cqe_size = sizeof(struct io_uring_cqe);\n\n\tif (__io_cqring_events(ctx) == ctx->cq_entries)\n\t\treturn;\n\n\tif (ctx->flags & IORING_SETUP_CQE32)\n\t\tcqe_size <<= 1;\n\n\tio_cq_lock(ctx);\n\twhile (!list_empty(&ctx->cq_overflow_list)) {\n\t\tstruct io_uring_cqe *cqe;\n\t\tstruct io_overflow_cqe *ocqe;\n\n\t\tif (!io_get_cqe_overflow(ctx, &cqe, true))\n\t\t\tbreak;\n\t\tocqe = list_first_entry(&ctx->cq_overflow_list,\n\t\t\t\t\tstruct io_overflow_cqe, list);\n\t\tmemcpy(cqe, &ocqe->cqe, cqe_size);\n\t\tlist_del(&ocqe->list);\n\t\tkfree(ocqe);\n\t}\n\n\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\tclear_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq);\n\t\tatomic_andnot(IORING_SQ_CQ_OVERFLOW, &ctx->rings->sq_flags);\n\t}\n\tio_cq_unlock_post(ctx);\n}\n\nstatic void io_cqring_do_overflow_flush(struct io_ring_ctx *ctx)\n{\n\t \n\tif (ctx->flags & IORING_SETUP_IOPOLL)\n\t\tmutex_lock(&ctx->uring_lock);\n\t__io_cqring_overflow_flush(ctx);\n\tif (ctx->flags & IORING_SETUP_IOPOLL)\n\t\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_cqring_overflow_flush(struct io_ring_ctx *ctx)\n{\n\tif (test_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq))\n\t\tio_cqring_do_overflow_flush(ctx);\n}\n\n \nstatic void io_put_task_remote(struct task_struct *task)\n{\n\tstruct io_uring_task *tctx = task->io_uring;\n\n\tpercpu_counter_sub(&tctx->inflight, 1);\n\tif (unlikely(atomic_read(&tctx->in_cancel)))\n\t\twake_up(&tctx->wait);\n\tput_task_struct(task);\n}\n\n \nstatic void io_put_task_local(struct task_struct *task)\n{\n\ttask->io_uring->cached_refs++;\n}\n\n \nstatic inline void io_put_task(struct task_struct *task)\n{\n\tif (likely(task == current))\n\t\tio_put_task_local(task);\n\telse\n\t\tio_put_task_remote(task);\n}\n\nvoid io_task_refs_refill(struct io_uring_task *tctx)\n{\n\tunsigned int refill = -tctx->cached_refs + IO_TCTX_REFS_CACHE_NR;\n\n\tpercpu_counter_add(&tctx->inflight, refill);\n\trefcount_add(refill, &current->usage);\n\ttctx->cached_refs += refill;\n}\n\nstatic __cold void io_uring_drop_tctx_refs(struct task_struct *task)\n{\n\tstruct io_uring_task *tctx = task->io_uring;\n\tunsigned int refs = tctx->cached_refs;\n\n\tif (refs) {\n\t\ttctx->cached_refs = 0;\n\t\tpercpu_counter_sub(&tctx->inflight, refs);\n\t\tput_task_struct_many(task, refs);\n\t}\n}\n\nstatic bool io_cqring_event_overflow(struct io_ring_ctx *ctx, u64 user_data,\n\t\t\t\t     s32 res, u32 cflags, u64 extra1, u64 extra2)\n{\n\tstruct io_overflow_cqe *ocqe;\n\tsize_t ocq_size = sizeof(struct io_overflow_cqe);\n\tbool is_cqe32 = (ctx->flags & IORING_SETUP_CQE32);\n\n\tlockdep_assert_held(&ctx->completion_lock);\n\n\tif (is_cqe32)\n\t\tocq_size += sizeof(struct io_uring_cqe);\n\n\tocqe = kmalloc(ocq_size, GFP_ATOMIC | __GFP_ACCOUNT);\n\ttrace_io_uring_cqe_overflow(ctx, user_data, res, cflags, ocqe);\n\tif (!ocqe) {\n\t\t \n\t\tio_account_cq_overflow(ctx);\n\t\tset_bit(IO_CHECK_CQ_DROPPED_BIT, &ctx->check_cq);\n\t\treturn false;\n\t}\n\tif (list_empty(&ctx->cq_overflow_list)) {\n\t\tset_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq);\n\t\tatomic_or(IORING_SQ_CQ_OVERFLOW, &ctx->rings->sq_flags);\n\n\t}\n\tocqe->cqe.user_data = user_data;\n\tocqe->cqe.res = res;\n\tocqe->cqe.flags = cflags;\n\tif (is_cqe32) {\n\t\tocqe->cqe.big_cqe[0] = extra1;\n\t\tocqe->cqe.big_cqe[1] = extra2;\n\t}\n\tlist_add_tail(&ocqe->list, &ctx->cq_overflow_list);\n\treturn true;\n}\n\nvoid io_req_cqe_overflow(struct io_kiocb *req)\n{\n\tio_cqring_event_overflow(req->ctx, req->cqe.user_data,\n\t\t\t\treq->cqe.res, req->cqe.flags,\n\t\t\t\treq->big_cqe.extra1, req->big_cqe.extra2);\n\tmemset(&req->big_cqe, 0, sizeof(req->big_cqe));\n}\n\n \nbool io_cqe_cache_refill(struct io_ring_ctx *ctx, bool overflow)\n{\n\tstruct io_rings *rings = ctx->rings;\n\tunsigned int off = ctx->cached_cq_tail & (ctx->cq_entries - 1);\n\tunsigned int free, queued, len;\n\n\t \n\tif (!overflow && (ctx->check_cq & BIT(IO_CHECK_CQ_OVERFLOW_BIT)))\n\t\treturn false;\n\n\t \n\tqueued = min(__io_cqring_events(ctx), ctx->cq_entries);\n\tfree = ctx->cq_entries - queued;\n\t \n\tlen = min(free, ctx->cq_entries - off);\n\tif (!len)\n\t\treturn false;\n\n\tif (ctx->flags & IORING_SETUP_CQE32) {\n\t\toff <<= 1;\n\t\tlen <<= 1;\n\t}\n\n\tctx->cqe_cached = &rings->cqes[off];\n\tctx->cqe_sentinel = ctx->cqe_cached + len;\n\treturn true;\n}\n\nstatic bool io_fill_cqe_aux(struct io_ring_ctx *ctx, u64 user_data, s32 res,\n\t\t\t      u32 cflags)\n{\n\tstruct io_uring_cqe *cqe;\n\n\tctx->cq_extra++;\n\n\t \n\tif (likely(io_get_cqe(ctx, &cqe))) {\n\t\ttrace_io_uring_complete(ctx, NULL, user_data, res, cflags, 0, 0);\n\n\t\tWRITE_ONCE(cqe->user_data, user_data);\n\t\tWRITE_ONCE(cqe->res, res);\n\t\tWRITE_ONCE(cqe->flags, cflags);\n\n\t\tif (ctx->flags & IORING_SETUP_CQE32) {\n\t\t\tWRITE_ONCE(cqe->big_cqe[0], 0);\n\t\t\tWRITE_ONCE(cqe->big_cqe[1], 0);\n\t\t}\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void __io_flush_post_cqes(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->uring_lock)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\tunsigned int i;\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\tfor (i = 0; i < state->cqes_count; i++) {\n\t\tstruct io_uring_cqe *cqe = &ctx->completion_cqes[i];\n\n\t\tif (!io_fill_cqe_aux(ctx, cqe->user_data, cqe->res, cqe->flags)) {\n\t\t\tif (ctx->lockless_cq) {\n\t\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\t\tio_cqring_event_overflow(ctx, cqe->user_data,\n\t\t\t\t\t\t\tcqe->res, cqe->flags, 0, 0);\n\t\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\t} else {\n\t\t\t\tio_cqring_event_overflow(ctx, cqe->user_data,\n\t\t\t\t\t\t\tcqe->res, cqe->flags, 0, 0);\n\t\t\t}\n\t\t}\n\t}\n\tstate->cqes_count = 0;\n}\n\nstatic bool __io_post_aux_cqe(struct io_ring_ctx *ctx, u64 user_data, s32 res, u32 cflags,\n\t\t\t      bool allow_overflow)\n{\n\tbool filled;\n\n\tio_cq_lock(ctx);\n\tfilled = io_fill_cqe_aux(ctx, user_data, res, cflags);\n\tif (!filled && allow_overflow)\n\t\tfilled = io_cqring_event_overflow(ctx, user_data, res, cflags, 0, 0);\n\n\tio_cq_unlock_post(ctx);\n\treturn filled;\n}\n\nbool io_post_aux_cqe(struct io_ring_ctx *ctx, u64 user_data, s32 res, u32 cflags)\n{\n\treturn __io_post_aux_cqe(ctx, user_data, res, cflags, true);\n}\n\n \nbool io_fill_cqe_req_aux(struct io_kiocb *req, bool defer, s32 res, u32 cflags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tu64 user_data = req->cqe.user_data;\n\tstruct io_uring_cqe *cqe;\n\n\tif (!defer)\n\t\treturn __io_post_aux_cqe(ctx, user_data, res, cflags, false);\n\n\tlockdep_assert_held(&ctx->uring_lock);\n\n\tif (ctx->submit_state.cqes_count == ARRAY_SIZE(ctx->completion_cqes)) {\n\t\t__io_cq_lock(ctx);\n\t\t__io_flush_post_cqes(ctx);\n\t\t \n\t\t__io_cq_unlock_post(ctx);\n\t}\n\n\t \n\tif (test_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq))\n\t\treturn false;\n\n\tcqe = &ctx->completion_cqes[ctx->submit_state.cqes_count++];\n\tcqe->user_data = user_data;\n\tcqe->res = res;\n\tcqe->flags = cflags;\n\treturn true;\n}\n\nstatic void __io_req_complete_post(struct io_kiocb *req, unsigned issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_rsrc_node *rsrc_node = NULL;\n\n\tio_cq_lock(ctx);\n\tif (!(req->flags & REQ_F_CQE_SKIP)) {\n\t\tif (!io_fill_cqe_req(ctx, req))\n\t\t\tio_req_cqe_overflow(req);\n\t}\n\n\t \n\tif (req_ref_put_and_test(req)) {\n\t\tif (req->flags & IO_REQ_LINK_FLAGS) {\n\t\t\tif (req->flags & IO_DISARM_MASK)\n\t\t\t\tio_disarm_next(req);\n\t\t\tif (req->link) {\n\t\t\t\tio_req_task_queue(req->link);\n\t\t\t\treq->link = NULL;\n\t\t\t}\n\t\t}\n\t\tio_put_kbuf_comp(req);\n\t\tif (unlikely(req->flags & IO_REQ_CLEAN_FLAGS))\n\t\t\tio_clean_op(req);\n\t\tio_put_file(req);\n\n\t\trsrc_node = req->rsrc_node;\n\t\t \n\t\tio_put_task_remote(req->task);\n\t\twq_list_add_head(&req->comp_list, &ctx->locked_free_list);\n\t\tctx->locked_free_nr++;\n\t}\n\tio_cq_unlock_post(ctx);\n\n\tif (rsrc_node) {\n\t\tio_ring_submit_lock(ctx, issue_flags);\n\t\tio_put_rsrc_node(ctx, rsrc_node);\n\t\tio_ring_submit_unlock(ctx, issue_flags);\n\t}\n}\n\nvoid io_req_complete_post(struct io_kiocb *req, unsigned issue_flags)\n{\n\tif (req->ctx->task_complete && req->ctx->submitter_task != current) {\n\t\treq->io_task_work.func = io_req_task_complete;\n\t\tio_req_task_work_add(req);\n\t} else if (!(issue_flags & IO_URING_F_UNLOCKED) ||\n\t\t   !(req->ctx->flags & IORING_SETUP_IOPOLL)) {\n\t\t__io_req_complete_post(req, issue_flags);\n\t} else {\n\t\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\t__io_req_complete_post(req, issue_flags & ~IO_URING_F_UNLOCKED);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n}\n\nvoid io_req_defer_failed(struct io_kiocb *req, s32 res)\n\t__must_hold(&ctx->uring_lock)\n{\n\tconst struct io_cold_def *def = &io_cold_defs[req->opcode];\n\n\tlockdep_assert_held(&req->ctx->uring_lock);\n\n\treq_set_fail(req);\n\tio_req_set_res(req, res, io_put_kbuf(req, IO_URING_F_UNLOCKED));\n\tif (def->fail)\n\t\tdef->fail(req);\n\tio_req_complete_defer(req);\n}\n\n \nstatic void io_preinit_req(struct io_kiocb *req, struct io_ring_ctx *ctx)\n{\n\treq->ctx = ctx;\n\treq->link = NULL;\n\treq->async_data = NULL;\n\t \n\tmemset(&req->cqe, 0, sizeof(req->cqe));\n\tmemset(&req->big_cqe, 0, sizeof(req->big_cqe));\n}\n\nstatic void io_flush_cached_locked_reqs(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_submit_state *state)\n{\n\tspin_lock(&ctx->completion_lock);\n\twq_list_splice(&ctx->locked_free_list, &state->free_list);\n\tctx->locked_free_nr = 0;\n\tspin_unlock(&ctx->completion_lock);\n}\n\n \n__cold bool __io_alloc_req_refill(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->uring_lock)\n{\n\tgfp_t gfp = GFP_KERNEL | __GFP_NOWARN;\n\tvoid *reqs[IO_REQ_ALLOC_BATCH];\n\tint ret, i;\n\n\t \n\tif (data_race(ctx->locked_free_nr) > IO_COMPL_BATCH) {\n\t\tio_flush_cached_locked_reqs(ctx, &ctx->submit_state);\n\t\tif (!io_req_cache_empty(ctx))\n\t\t\treturn true;\n\t}\n\n\tret = kmem_cache_alloc_bulk(req_cachep, gfp, ARRAY_SIZE(reqs), reqs);\n\n\t \n\tif (unlikely(ret <= 0)) {\n\t\treqs[0] = kmem_cache_alloc(req_cachep, gfp);\n\t\tif (!reqs[0])\n\t\t\treturn false;\n\t\tret = 1;\n\t}\n\n\tpercpu_ref_get_many(&ctx->refs, ret);\n\tfor (i = 0; i < ret; i++) {\n\t\tstruct io_kiocb *req = reqs[i];\n\n\t\tio_preinit_req(req, ctx);\n\t\tio_req_add_to_cache(req, ctx);\n\t}\n\treturn true;\n}\n\n__cold void io_free_req(struct io_kiocb *req)\n{\n\t \n\treq->flags &= ~REQ_F_REFCOUNT;\n\t \n\treq->flags |= REQ_F_CQE_SKIP;\n\treq->io_task_work.func = io_req_task_complete;\n\tio_req_task_work_add(req);\n}\n\nstatic void __io_req_find_next_prep(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\tspin_lock(&ctx->completion_lock);\n\tio_disarm_next(req);\n\tspin_unlock(&ctx->completion_lock);\n}\n\nstatic inline struct io_kiocb *io_req_find_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt;\n\n\t \n\tif (unlikely(req->flags & IO_DISARM_MASK))\n\t\t__io_req_find_next_prep(req);\n\tnxt = req->link;\n\treq->link = NULL;\n\treturn nxt;\n}\n\nstatic void ctx_flush_and_put(struct io_ring_ctx *ctx, struct io_tw_state *ts)\n{\n\tif (!ctx)\n\t\treturn;\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_andnot(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\tif (ts->locked) {\n\t\tio_submit_flush_completions(ctx);\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tts->locked = false;\n\t}\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic unsigned int handle_tw_list(struct llist_node *node,\n\t\t\t\t   struct io_ring_ctx **ctx,\n\t\t\t\t   struct io_tw_state *ts,\n\t\t\t\t   struct llist_node *last)\n{\n\tunsigned int count = 0;\n\n\twhile (node && node != last) {\n\t\tstruct llist_node *next = node->next;\n\t\tstruct io_kiocb *req = container_of(node, struct io_kiocb,\n\t\t\t\t\t\t    io_task_work.node);\n\n\t\tprefetch(container_of(next, struct io_kiocb, io_task_work.node));\n\n\t\tif (req->ctx != *ctx) {\n\t\t\tctx_flush_and_put(*ctx, ts);\n\t\t\t*ctx = req->ctx;\n\t\t\t \n\t\t\tts->locked = mutex_trylock(&(*ctx)->uring_lock);\n\t\t\tpercpu_ref_get(&(*ctx)->refs);\n\t\t}\n\t\tINDIRECT_CALL_2(req->io_task_work.func,\n\t\t\t\tio_poll_task_func, io_req_rw_complete,\n\t\t\t\treq, ts);\n\t\tnode = next;\n\t\tcount++;\n\t\tif (unlikely(need_resched())) {\n\t\t\tctx_flush_and_put(*ctx, ts);\n\t\t\t*ctx = NULL;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\treturn count;\n}\n\n \nstatic inline struct llist_node *io_llist_xchg(struct llist_head *head,\n\t\t\t\t\t       struct llist_node *new)\n{\n\treturn xchg(&head->first, new);\n}\n\n \n\nstatic inline struct llist_node *io_llist_cmpxchg(struct llist_head *head,\n\t\t\t\t\t\t  struct llist_node *old,\n\t\t\t\t\t\t  struct llist_node *new)\n{\n\treturn cmpxchg(&head->first, old, new);\n}\n\nstatic __cold void io_fallback_tw(struct io_uring_task *tctx, bool sync)\n{\n\tstruct llist_node *node = llist_del_all(&tctx->task_list);\n\tstruct io_ring_ctx *last_ctx = NULL;\n\tstruct io_kiocb *req;\n\n\twhile (node) {\n\t\treq = container_of(node, struct io_kiocb, io_task_work.node);\n\t\tnode = node->next;\n\t\tif (sync && last_ctx != req->ctx) {\n\t\t\tif (last_ctx) {\n\t\t\t\tflush_delayed_work(&last_ctx->fallback_work);\n\t\t\t\tpercpu_ref_put(&last_ctx->refs);\n\t\t\t}\n\t\t\tlast_ctx = req->ctx;\n\t\t\tpercpu_ref_get(&last_ctx->refs);\n\t\t}\n\t\tif (llist_add(&req->io_task_work.node,\n\t\t\t      &req->ctx->fallback_llist))\n\t\t\tschedule_delayed_work(&req->ctx->fallback_work, 1);\n\t}\n\n\tif (last_ctx) {\n\t\tflush_delayed_work(&last_ctx->fallback_work);\n\t\tpercpu_ref_put(&last_ctx->refs);\n\t}\n}\n\nvoid tctx_task_work(struct callback_head *cb)\n{\n\tstruct io_tw_state ts = {};\n\tstruct io_ring_ctx *ctx = NULL;\n\tstruct io_uring_task *tctx = container_of(cb, struct io_uring_task,\n\t\t\t\t\t\t  task_work);\n\tstruct llist_node fake = {};\n\tstruct llist_node *node;\n\tunsigned int loops = 0;\n\tunsigned int count = 0;\n\n\tif (unlikely(current->flags & PF_EXITING)) {\n\t\tio_fallback_tw(tctx, true);\n\t\treturn;\n\t}\n\n\tdo {\n\t\tloops++;\n\t\tnode = io_llist_xchg(&tctx->task_list, &fake);\n\t\tcount += handle_tw_list(node, &ctx, &ts, &fake);\n\n\t\t \n\t\tif (READ_ONCE(tctx->task_list.first) != &fake)\n\t\t\tcontinue;\n\t\tif (ts.locked && !wq_list_empty(&ctx->submit_state.compl_reqs)) {\n\t\t\tio_submit_flush_completions(ctx);\n\t\t\tif (READ_ONCE(tctx->task_list.first) != &fake)\n\t\t\t\tcontinue;\n\t\t}\n\t\tnode = io_llist_cmpxchg(&tctx->task_list, &fake, NULL);\n\t} while (node != &fake);\n\n\tctx_flush_and_put(ctx, &ts);\n\n\t \n\tif (unlikely(atomic_read(&tctx->in_cancel)))\n\t\tio_uring_drop_tctx_refs(current);\n\n\ttrace_io_uring_task_work_run(tctx, count, loops);\n}\n\nstatic inline void io_req_local_work_add(struct io_kiocb *req, unsigned flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tunsigned nr_wait, nr_tw, nr_tw_prev;\n\tstruct llist_node *first;\n\n\tif (req->flags & (REQ_F_LINK | REQ_F_HARDLINK))\n\t\tflags &= ~IOU_F_TWQ_LAZY_WAKE;\n\n\tfirst = READ_ONCE(ctx->work_llist.first);\n\tdo {\n\t\tnr_tw_prev = 0;\n\t\tif (first) {\n\t\t\tstruct io_kiocb *first_req = container_of(first,\n\t\t\t\t\t\t\tstruct io_kiocb,\n\t\t\t\t\t\t\tio_task_work.node);\n\t\t\t \n\t\t\tnr_tw_prev = READ_ONCE(first_req->nr_tw);\n\t\t}\n\t\tnr_tw = nr_tw_prev + 1;\n\t\t \n\t\tif (!(flags & IOU_F_TWQ_LAZY_WAKE))\n\t\t\tnr_tw = INT_MAX;\n\n\t\treq->nr_tw = nr_tw;\n\t\treq->io_task_work.node.next = first;\n\t} while (!try_cmpxchg(&ctx->work_llist.first, &first,\n\t\t\t      &req->io_task_work.node));\n\n\tif (!first) {\n\t\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\t\tatomic_or(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\t\tif (ctx->has_evfd)\n\t\t\tio_eventfd_signal(ctx);\n\t}\n\n\tnr_wait = atomic_read(&ctx->cq_wait_nr);\n\t \n\tif (!nr_wait)\n\t\treturn;\n\t \n\tif (nr_wait > nr_tw || nr_tw_prev >= nr_wait)\n\t\treturn;\n\t \n\tsmp_mb__after_atomic();\n\twake_up_state(ctx->submitter_task, TASK_INTERRUPTIBLE);\n}\n\nstatic void io_req_normal_work_add(struct io_kiocb *req)\n{\n\tstruct io_uring_task *tctx = req->task->io_uring;\n\tstruct io_ring_ctx *ctx = req->ctx;\n\n\t \n\tif (!llist_add(&req->io_task_work.node, &tctx->task_list))\n\t\treturn;\n\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_or(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\n\n\tif (likely(!task_work_add(req->task, &tctx->task_work, ctx->notify_method)))\n\t\treturn;\n\n\tio_fallback_tw(tctx, false);\n}\n\nvoid __io_req_task_work_add(struct io_kiocb *req, unsigned flags)\n{\n\tif (req->ctx->flags & IORING_SETUP_DEFER_TASKRUN) {\n\t\trcu_read_lock();\n\t\tio_req_local_work_add(req, flags);\n\t\trcu_read_unlock();\n\t} else {\n\t\tio_req_normal_work_add(req);\n\t}\n}\n\nstatic void __cold io_move_task_work_from_local(struct io_ring_ctx *ctx)\n{\n\tstruct llist_node *node;\n\n\tnode = llist_del_all(&ctx->work_llist);\n\twhile (node) {\n\t\tstruct io_kiocb *req = container_of(node, struct io_kiocb,\n\t\t\t\t\t\t    io_task_work.node);\n\n\t\tnode = node->next;\n\t\tio_req_normal_work_add(req);\n\t}\n}\n\nstatic int __io_run_local_work(struct io_ring_ctx *ctx, struct io_tw_state *ts)\n{\n\tstruct llist_node *node;\n\tunsigned int loops = 0;\n\tint ret = 0;\n\n\tif (WARN_ON_ONCE(ctx->submitter_task != current))\n\t\treturn -EEXIST;\n\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG)\n\t\tatomic_andnot(IORING_SQ_TASKRUN, &ctx->rings->sq_flags);\nagain:\n\t \n\tnode = llist_reverse_order(io_llist_xchg(&ctx->work_llist, NULL));\n\twhile (node) {\n\t\tstruct llist_node *next = node->next;\n\t\tstruct io_kiocb *req = container_of(node, struct io_kiocb,\n\t\t\t\t\t\t    io_task_work.node);\n\t\tprefetch(container_of(next, struct io_kiocb, io_task_work.node));\n\t\tINDIRECT_CALL_2(req->io_task_work.func,\n\t\t\t\tio_poll_task_func, io_req_rw_complete,\n\t\t\t\treq, ts);\n\t\tret++;\n\t\tnode = next;\n\t}\n\tloops++;\n\n\tif (!llist_empty(&ctx->work_llist))\n\t\tgoto again;\n\tif (ts->locked) {\n\t\tio_submit_flush_completions(ctx);\n\t\tif (!llist_empty(&ctx->work_llist))\n\t\t\tgoto again;\n\t}\n\ttrace_io_uring_local_work_run(ctx, ret, loops);\n\treturn ret;\n}\n\nstatic inline int io_run_local_work_locked(struct io_ring_ctx *ctx)\n{\n\tstruct io_tw_state ts = { .locked = true, };\n\tint ret;\n\n\tif (llist_empty(&ctx->work_llist))\n\t\treturn 0;\n\n\tret = __io_run_local_work(ctx, &ts);\n\t \n\tif (WARN_ON_ONCE(!ts.locked))\n\t\tmutex_lock(&ctx->uring_lock);\n\treturn ret;\n}\n\nstatic int io_run_local_work(struct io_ring_ctx *ctx)\n{\n\tstruct io_tw_state ts = {};\n\tint ret;\n\n\tts.locked = mutex_trylock(&ctx->uring_lock);\n\tret = __io_run_local_work(ctx, &ts);\n\tif (ts.locked)\n\t\tmutex_unlock(&ctx->uring_lock);\n\n\treturn ret;\n}\n\nstatic void io_req_task_cancel(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tio_tw_lock(req->ctx, ts);\n\tio_req_defer_failed(req, req->cqe.res);\n}\n\nvoid io_req_task_submit(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tio_tw_lock(req->ctx, ts);\n\t \n\tif (unlikely(req->task->flags & PF_EXITING))\n\t\tio_req_defer_failed(req, -EFAULT);\n\telse if (req->flags & REQ_F_FORCE_ASYNC)\n\t\tio_queue_iowq(req, ts);\n\telse\n\t\tio_queue_sqe(req);\n}\n\nvoid io_req_task_queue_fail(struct io_kiocb *req, int ret)\n{\n\tio_req_set_res(req, ret, 0);\n\treq->io_task_work.func = io_req_task_cancel;\n\tio_req_task_work_add(req);\n}\n\nvoid io_req_task_queue(struct io_kiocb *req)\n{\n\treq->io_task_work.func = io_req_task_submit;\n\tio_req_task_work_add(req);\n}\n\nvoid io_queue_next(struct io_kiocb *req)\n{\n\tstruct io_kiocb *nxt = io_req_find_next(req);\n\n\tif (nxt)\n\t\tio_req_task_queue(nxt);\n}\n\nstatic void io_free_batch_list(struct io_ring_ctx *ctx,\n\t\t\t       struct io_wq_work_node *node)\n\t__must_hold(&ctx->uring_lock)\n{\n\tdo {\n\t\tstruct io_kiocb *req = container_of(node, struct io_kiocb,\n\t\t\t\t\t\t    comp_list);\n\n\t\tif (unlikely(req->flags & IO_REQ_CLEAN_SLOW_FLAGS)) {\n\t\t\tif (req->flags & REQ_F_REFCOUNT) {\n\t\t\t\tnode = req->comp_list.next;\n\t\t\t\tif (!req_ref_put_and_test(req))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif ((req->flags & REQ_F_POLLED) && req->apoll) {\n\t\t\t\tstruct async_poll *apoll = req->apoll;\n\n\t\t\t\tif (apoll->double_poll)\n\t\t\t\t\tkfree(apoll->double_poll);\n\t\t\t\tif (!io_alloc_cache_put(&ctx->apoll_cache, &apoll->cache))\n\t\t\t\t\tkfree(apoll);\n\t\t\t\treq->flags &= ~REQ_F_POLLED;\n\t\t\t}\n\t\t\tif (req->flags & IO_REQ_LINK_FLAGS)\n\t\t\t\tio_queue_next(req);\n\t\t\tif (unlikely(req->flags & IO_REQ_CLEAN_FLAGS))\n\t\t\t\tio_clean_op(req);\n\t\t}\n\t\tio_put_file(req);\n\n\t\tio_req_put_rsrc_locked(req, ctx);\n\n\t\tio_put_task(req->task);\n\t\tnode = req->comp_list.next;\n\t\tio_req_add_to_cache(req, ctx);\n\t} while (node);\n}\n\nvoid __io_submit_flush_completions(struct io_ring_ctx *ctx)\n\t__must_hold(&ctx->uring_lock)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\tstruct io_wq_work_node *node;\n\n\t__io_cq_lock(ctx);\n\t \n\tif (state->cqes_count)\n\t\t__io_flush_post_cqes(ctx);\n\t__wq_list_for_each(node, &state->compl_reqs) {\n\t\tstruct io_kiocb *req = container_of(node, struct io_kiocb,\n\t\t\t\t\t    comp_list);\n\n\t\tif (!(req->flags & REQ_F_CQE_SKIP) &&\n\t\t    unlikely(!io_fill_cqe_req(ctx, req))) {\n\t\t\tif (ctx->lockless_cq) {\n\t\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\t\tio_req_cqe_overflow(req);\n\t\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\t} else {\n\t\t\t\tio_req_cqe_overflow(req);\n\t\t\t}\n\t\t}\n\t}\n\t__io_cq_unlock_post(ctx);\n\n\tif (!wq_list_empty(&ctx->submit_state.compl_reqs)) {\n\t\tio_free_batch_list(ctx, state->compl_reqs.first);\n\t\tINIT_WQ_LIST(&state->compl_reqs);\n\t}\n}\n\nstatic unsigned io_cqring_events(struct io_ring_ctx *ctx)\n{\n\t \n\tsmp_rmb();\n\treturn __io_cqring_events(ctx);\n}\n\n \nstatic __cold void io_iopoll_try_reap_events(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn;\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!wq_list_empty(&ctx->iopoll_list)) {\n\t\t \n\t\tif (io_do_iopoll(ctx, true) == 0)\n\t\t\tbreak;\n\t\t \n\t\tif (need_resched()) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tcond_resched();\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t}\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic int io_iopoll_check(struct io_ring_ctx *ctx, long min)\n{\n\tunsigned int nr_events = 0;\n\tunsigned long check_cq;\n\n\tif (!io_allowed_run_tw(ctx))\n\t\treturn -EEXIST;\n\n\tcheck_cq = READ_ONCE(ctx->check_cq);\n\tif (unlikely(check_cq)) {\n\t\tif (check_cq & BIT(IO_CHECK_CQ_OVERFLOW_BIT))\n\t\t\t__io_cqring_overflow_flush(ctx);\n\t\t \n\t\tif (check_cq & BIT(IO_CHECK_CQ_DROPPED_BIT))\n\t\t\treturn -EBADR;\n\t}\n\t \n\tif (io_cqring_events(ctx))\n\t\treturn 0;\n\n\tdo {\n\t\tint ret = 0;\n\n\t\t \n\t\tif (wq_list_empty(&ctx->iopoll_list) ||\n\t\t    io_task_work_pending(ctx)) {\n\t\t\tu32 tail = ctx->cached_cq_tail;\n\n\t\t\t(void) io_run_local_work_locked(ctx);\n\n\t\t\tif (task_work_pending(current) ||\n\t\t\t    wq_list_empty(&ctx->iopoll_list)) {\n\t\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\t\tio_run_task_work();\n\t\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t\t}\n\t\t\t \n\t\t\tif (tail != ctx->cached_cq_tail ||\n\t\t\t    wq_list_empty(&ctx->iopoll_list))\n\t\t\t\tbreak;\n\t\t}\n\t\tret = io_do_iopoll(ctx, !min);\n\t\tif (unlikely(ret < 0))\n\t\t\treturn ret;\n\n\t\tif (task_sigpending(current))\n\t\t\treturn -EINTR;\n\t\tif (need_resched())\n\t\t\tbreak;\n\n\t\tnr_events += ret;\n\t} while (nr_events < min);\n\n\treturn 0;\n}\n\nvoid io_req_task_complete(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tif (ts->locked)\n\t\tio_req_complete_defer(req);\n\telse\n\t\tio_req_complete_post(req, IO_URING_F_UNLOCKED);\n}\n\n \nstatic void io_iopoll_req_issued(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tconst bool needs_lock = issue_flags & IO_URING_F_UNLOCKED;\n\n\t \n\tif (unlikely(needs_lock))\n\t\tmutex_lock(&ctx->uring_lock);\n\n\t \n\tif (wq_list_empty(&ctx->iopoll_list)) {\n\t\tctx->poll_multi_queue = false;\n\t} else if (!ctx->poll_multi_queue) {\n\t\tstruct io_kiocb *list_req;\n\n\t\tlist_req = container_of(ctx->iopoll_list.first, struct io_kiocb,\n\t\t\t\t\tcomp_list);\n\t\tif (list_req->file != req->file)\n\t\t\tctx->poll_multi_queue = true;\n\t}\n\n\t \n\tif (READ_ONCE(req->iopoll_completed))\n\t\twq_list_add_head(&req->comp_list, &ctx->iopoll_list);\n\telse\n\t\twq_list_add_tail(&req->comp_list, &ctx->iopoll_list);\n\n\tif (unlikely(needs_lock)) {\n\t\t \n\t\tif ((ctx->flags & IORING_SETUP_SQPOLL) &&\n\t\t    wq_has_sleeper(&ctx->sq_data->wait))\n\t\t\twake_up(&ctx->sq_data->wait);\n\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n}\n\nunsigned int io_file_get_flags(struct file *file)\n{\n\tunsigned int res = 0;\n\n\tif (S_ISREG(file_inode(file)->i_mode))\n\t\tres |= REQ_F_ISREG;\n\tif ((file->f_flags & O_NONBLOCK) || (file->f_mode & FMODE_NOWAIT))\n\t\tres |= REQ_F_SUPPORT_NOWAIT;\n\treturn res;\n}\n\nbool io_alloc_async_data(struct io_kiocb *req)\n{\n\tWARN_ON_ONCE(!io_cold_defs[req->opcode].async_size);\n\treq->async_data = kmalloc(io_cold_defs[req->opcode].async_size, GFP_KERNEL);\n\tif (req->async_data) {\n\t\treq->flags |= REQ_F_ASYNC_DATA;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nint io_req_prep_async(struct io_kiocb *req)\n{\n\tconst struct io_cold_def *cdef = &io_cold_defs[req->opcode];\n\tconst struct io_issue_def *def = &io_issue_defs[req->opcode];\n\n\t \n\tif (def->needs_file && !(req->flags & REQ_F_FIXED_FILE) && !req->file)\n\t\treq->file = io_file_get_normal(req, req->cqe.fd);\n\tif (!cdef->prep_async)\n\t\treturn 0;\n\tif (WARN_ON_ONCE(req_has_async_data(req)))\n\t\treturn -EFAULT;\n\tif (!def->manual_alloc) {\n\t\tif (io_alloc_async_data(req))\n\t\t\treturn -EAGAIN;\n\t}\n\treturn cdef->prep_async(req);\n}\n\nstatic u32 io_get_sequence(struct io_kiocb *req)\n{\n\tu32 seq = req->ctx->cached_sq_head;\n\tstruct io_kiocb *cur;\n\n\t \n\tio_for_each_link(cur, req)\n\t\tseq--;\n\treturn seq;\n}\n\nstatic __cold void io_drain_req(struct io_kiocb *req)\n\t__must_hold(&ctx->uring_lock)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_defer_entry *de;\n\tint ret;\n\tu32 seq = io_get_sequence(req);\n\n\t \n\tspin_lock(&ctx->completion_lock);\n\tif (!req_need_defer(req, seq) && list_empty_careful(&ctx->defer_list)) {\n\t\tspin_unlock(&ctx->completion_lock);\nqueue:\n\t\tctx->drain_active = false;\n\t\tio_req_task_queue(req);\n\t\treturn;\n\t}\n\tspin_unlock(&ctx->completion_lock);\n\n\tio_prep_async_link(req);\n\tde = kmalloc(sizeof(*de), GFP_KERNEL);\n\tif (!de) {\n\t\tret = -ENOMEM;\n\t\tio_req_defer_failed(req, ret);\n\t\treturn;\n\t}\n\n\tspin_lock(&ctx->completion_lock);\n\tif (!req_need_defer(req, seq) && list_empty(&ctx->defer_list)) {\n\t\tspin_unlock(&ctx->completion_lock);\n\t\tkfree(de);\n\t\tgoto queue;\n\t}\n\n\ttrace_io_uring_defer(req);\n\tde->req = req;\n\tde->seq = seq;\n\tlist_add_tail(&de->list, &ctx->defer_list);\n\tspin_unlock(&ctx->completion_lock);\n}\n\nstatic bool io_assign_file(struct io_kiocb *req, const struct io_issue_def *def,\n\t\t\t   unsigned int issue_flags)\n{\n\tif (req->file || !def->needs_file)\n\t\treturn true;\n\n\tif (req->flags & REQ_F_FIXED_FILE)\n\t\treq->file = io_file_get_fixed(req, req->cqe.fd, issue_flags);\n\telse\n\t\treq->file = io_file_get_normal(req, req->cqe.fd);\n\n\treturn !!req->file;\n}\n\nstatic int io_issue_sqe(struct io_kiocb *req, unsigned int issue_flags)\n{\n\tconst struct io_issue_def *def = &io_issue_defs[req->opcode];\n\tconst struct cred *creds = NULL;\n\tint ret;\n\n\tif (unlikely(!io_assign_file(req, def, issue_flags)))\n\t\treturn -EBADF;\n\n\tif (unlikely((req->flags & REQ_F_CREDS) && req->creds != current_cred()))\n\t\tcreds = override_creds(req->creds);\n\n\tif (!def->audit_skip)\n\t\taudit_uring_entry(req->opcode);\n\n\tret = def->issue(req, issue_flags);\n\n\tif (!def->audit_skip)\n\t\taudit_uring_exit(!ret, ret);\n\n\tif (creds)\n\t\trevert_creds(creds);\n\n\tif (ret == IOU_OK) {\n\t\tif (issue_flags & IO_URING_F_COMPLETE_DEFER)\n\t\t\tio_req_complete_defer(req);\n\t\telse\n\t\t\tio_req_complete_post(req, issue_flags);\n\n\t\treturn 0;\n\t}\n\n\tif (ret != IOU_ISSUE_SKIP_COMPLETE)\n\t\treturn ret;\n\n\t \n\tif ((req->ctx->flags & IORING_SETUP_IOPOLL) && def->iopoll_queue)\n\t\tio_iopoll_req_issued(req, issue_flags);\n\n\treturn 0;\n}\n\nint io_poll_issue(struct io_kiocb *req, struct io_tw_state *ts)\n{\n\tio_tw_lock(req->ctx, ts);\n\treturn io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_MULTISHOT|\n\t\t\t\t IO_URING_F_COMPLETE_DEFER);\n}\n\nstruct io_wq_work *io_wq_free_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_kiocb *nxt = NULL;\n\n\tif (req_ref_put_and_test(req)) {\n\t\tif (req->flags & IO_REQ_LINK_FLAGS)\n\t\t\tnxt = io_req_find_next(req);\n\t\tio_free_req(req);\n\t}\n\treturn nxt ? &nxt->work : NULL;\n}\n\nvoid io_wq_submit_work(struct io_wq_work *work)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tconst struct io_issue_def *def = &io_issue_defs[req->opcode];\n\tunsigned int issue_flags = IO_URING_F_UNLOCKED | IO_URING_F_IOWQ;\n\tbool needs_poll = false;\n\tint ret = 0, err = -ECANCELED;\n\n\t \n\tif (!(req->flags & REQ_F_REFCOUNT))\n\t\t__io_req_set_refcount(req, 2);\n\telse\n\t\treq_ref_get(req);\n\n\tio_arm_ltimeout(req);\n\n\t \n\tif (work->flags & IO_WQ_WORK_CANCEL) {\nfail:\n\t\tio_req_task_queue_fail(req, err);\n\t\treturn;\n\t}\n\tif (!io_assign_file(req, def, issue_flags)) {\n\t\terr = -EBADF;\n\t\twork->flags |= IO_WQ_WORK_CANCEL;\n\t\tgoto fail;\n\t}\n\n\tif (req->flags & REQ_F_FORCE_ASYNC) {\n\t\tbool opcode_poll = def->pollin || def->pollout;\n\n\t\tif (opcode_poll && file_can_poll(req->file)) {\n\t\t\tneeds_poll = true;\n\t\t\tissue_flags |= IO_URING_F_NONBLOCK;\n\t\t}\n\t}\n\n\tdo {\n\t\tret = io_issue_sqe(req, issue_flags);\n\t\tif (ret != -EAGAIN)\n\t\t\tbreak;\n\n\t\t \n\t\tif (req->flags & REQ_F_NOWAIT)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!needs_poll) {\n\t\t\tif (!(req->ctx->flags & IORING_SETUP_IOPOLL))\n\t\t\t\tbreak;\n\t\t\tif (io_wq_worker_stopped())\n\t\t\t\tbreak;\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (io_arm_poll_handler(req, issue_flags) == IO_APOLL_OK)\n\t\t\treturn;\n\t\t \n\t\tneeds_poll = false;\n\t\tissue_flags &= ~IO_URING_F_NONBLOCK;\n\t} while (1);\n\n\t \n\tif (ret < 0)\n\t\tio_req_task_queue_fail(req, ret);\n}\n\ninline struct file *io_file_get_fixed(struct io_kiocb *req, int fd,\n\t\t\t\t      unsigned int issue_flags)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_fixed_file *slot;\n\tstruct file *file = NULL;\n\n\tio_ring_submit_lock(ctx, issue_flags);\n\n\tif (unlikely((unsigned int)fd >= ctx->nr_user_files))\n\t\tgoto out;\n\tfd = array_index_nospec(fd, ctx->nr_user_files);\n\tslot = io_fixed_file_slot(&ctx->file_table, fd);\n\tfile = io_slot_file(slot);\n\treq->flags |= io_slot_flags(slot);\n\tio_req_set_rsrc_node(req, ctx, 0);\nout:\n\tio_ring_submit_unlock(ctx, issue_flags);\n\treturn file;\n}\n\nstruct file *io_file_get_normal(struct io_kiocb *req, int fd)\n{\n\tstruct file *file = fget(fd);\n\n\ttrace_io_uring_file_get(req, fd);\n\n\t \n\tif (file && io_is_uring_fops(file))\n\t\tio_req_track_inflight(req);\n\treturn file;\n}\n\nstatic void io_queue_async(struct io_kiocb *req, int ret)\n\t__must_hold(&req->ctx->uring_lock)\n{\n\tstruct io_kiocb *linked_timeout;\n\n\tif (ret != -EAGAIN || (req->flags & REQ_F_NOWAIT)) {\n\t\tio_req_defer_failed(req, ret);\n\t\treturn;\n\t}\n\n\tlinked_timeout = io_prep_linked_timeout(req);\n\n\tswitch (io_arm_poll_handler(req, 0)) {\n\tcase IO_APOLL_READY:\n\t\tio_kbuf_recycle(req, 0);\n\t\tio_req_task_queue(req);\n\t\tbreak;\n\tcase IO_APOLL_ABORTED:\n\t\tio_kbuf_recycle(req, 0);\n\t\tio_queue_iowq(req, NULL);\n\t\tbreak;\n\tcase IO_APOLL_OK:\n\t\tbreak;\n\t}\n\n\tif (linked_timeout)\n\t\tio_queue_linked_timeout(linked_timeout);\n}\n\nstatic inline void io_queue_sqe(struct io_kiocb *req)\n\t__must_hold(&req->ctx->uring_lock)\n{\n\tint ret;\n\n\tret = io_issue_sqe(req, IO_URING_F_NONBLOCK|IO_URING_F_COMPLETE_DEFER);\n\n\t \n\tif (likely(!ret))\n\t\tio_arm_ltimeout(req);\n\telse\n\t\tio_queue_async(req, ret);\n}\n\nstatic void io_queue_sqe_fallback(struct io_kiocb *req)\n\t__must_hold(&req->ctx->uring_lock)\n{\n\tif (unlikely(req->flags & REQ_F_FAIL)) {\n\t\t \n\t\treq->flags &= ~REQ_F_HARDLINK;\n\t\treq->flags |= REQ_F_LINK;\n\t\tio_req_defer_failed(req, req->cqe.res);\n\t} else {\n\t\tint ret = io_req_prep_async(req);\n\n\t\tif (unlikely(ret)) {\n\t\t\tio_req_defer_failed(req, ret);\n\t\t\treturn;\n\t\t}\n\n\t\tif (unlikely(req->ctx->drain_active))\n\t\t\tio_drain_req(req);\n\t\telse\n\t\t\tio_queue_iowq(req, NULL);\n\t}\n}\n\n \nstatic inline bool io_check_restriction(struct io_ring_ctx *ctx,\n\t\t\t\t\tstruct io_kiocb *req,\n\t\t\t\t\tunsigned int sqe_flags)\n{\n\tif (!test_bit(req->opcode, ctx->restrictions.sqe_op))\n\t\treturn false;\n\n\tif ((sqe_flags & ctx->restrictions.sqe_flags_required) !=\n\t    ctx->restrictions.sqe_flags_required)\n\t\treturn false;\n\n\tif (sqe_flags & ~(ctx->restrictions.sqe_flags_allowed |\n\t\t\t  ctx->restrictions.sqe_flags_required))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void io_init_req_drain(struct io_kiocb *req)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_kiocb *head = ctx->submit_state.link.head;\n\n\tctx->drain_active = true;\n\tif (head) {\n\t\t \n\t\thead->flags |= REQ_F_IO_DRAIN | REQ_F_FORCE_ASYNC;\n\t\tctx->drain_next = true;\n\t}\n}\n\nstatic int io_init_req(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t       const struct io_uring_sqe *sqe)\n\t__must_hold(&ctx->uring_lock)\n{\n\tconst struct io_issue_def *def;\n\tunsigned int sqe_flags;\n\tint personality;\n\tu8 opcode;\n\n\t \n\treq->opcode = opcode = READ_ONCE(sqe->opcode);\n\t \n\treq->flags = sqe_flags = READ_ONCE(sqe->flags);\n\treq->cqe.user_data = READ_ONCE(sqe->user_data);\n\treq->file = NULL;\n\treq->rsrc_node = NULL;\n\treq->task = current;\n\n\tif (unlikely(opcode >= IORING_OP_LAST)) {\n\t\treq->opcode = 0;\n\t\treturn -EINVAL;\n\t}\n\tdef = &io_issue_defs[opcode];\n\tif (unlikely(sqe_flags & ~SQE_COMMON_FLAGS)) {\n\t\t \n\t\tif (sqe_flags & ~SQE_VALID_FLAGS)\n\t\t\treturn -EINVAL;\n\t\tif (sqe_flags & IOSQE_BUFFER_SELECT) {\n\t\t\tif (!def->buffer_select)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\treq->buf_index = READ_ONCE(sqe->buf_group);\n\t\t}\n\t\tif (sqe_flags & IOSQE_CQE_SKIP_SUCCESS)\n\t\t\tctx->drain_disabled = true;\n\t\tif (sqe_flags & IOSQE_IO_DRAIN) {\n\t\t\tif (ctx->drain_disabled)\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\tio_init_req_drain(req);\n\t\t}\n\t}\n\tif (unlikely(ctx->restricted || ctx->drain_active || ctx->drain_next)) {\n\t\tif (ctx->restricted && !io_check_restriction(ctx, req, sqe_flags))\n\t\t\treturn -EACCES;\n\t\t \n\t\tif (ctx->drain_active)\n\t\t\treq->flags |= REQ_F_FORCE_ASYNC;\n\t\t \n\t\tif (unlikely(ctx->drain_next) && !ctx->submit_state.link.head) {\n\t\t\tctx->drain_next = false;\n\t\t\tctx->drain_active = true;\n\t\t\treq->flags |= REQ_F_IO_DRAIN | REQ_F_FORCE_ASYNC;\n\t\t}\n\t}\n\n\tif (!def->ioprio && sqe->ioprio)\n\t\treturn -EINVAL;\n\tif (!def->iopoll && (ctx->flags & IORING_SETUP_IOPOLL))\n\t\treturn -EINVAL;\n\n\tif (def->needs_file) {\n\t\tstruct io_submit_state *state = &ctx->submit_state;\n\n\t\treq->cqe.fd = READ_ONCE(sqe->fd);\n\n\t\t \n\t\tif (state->need_plug && def->plug) {\n\t\t\tstate->plug_started = true;\n\t\t\tstate->need_plug = false;\n\t\t\tblk_start_plug_nr_ios(&state->plug, state->submit_nr);\n\t\t}\n\t}\n\n\tpersonality = READ_ONCE(sqe->personality);\n\tif (personality) {\n\t\tint ret;\n\n\t\treq->creds = xa_load(&ctx->personalities, personality);\n\t\tif (!req->creds)\n\t\t\treturn -EINVAL;\n\t\tget_cred(req->creds);\n\t\tret = security_uring_override_creds(req->creds);\n\t\tif (ret) {\n\t\t\tput_cred(req->creds);\n\t\t\treturn ret;\n\t\t}\n\t\treq->flags |= REQ_F_CREDS;\n\t}\n\n\treturn def->prep(req, sqe);\n}\n\nstatic __cold int io_submit_fail_init(const struct io_uring_sqe *sqe,\n\t\t\t\t      struct io_kiocb *req, int ret)\n{\n\tstruct io_ring_ctx *ctx = req->ctx;\n\tstruct io_submit_link *link = &ctx->submit_state.link;\n\tstruct io_kiocb *head = link->head;\n\n\ttrace_io_uring_req_failed(sqe, req, ret);\n\n\t \n\treq_fail_link_node(req, ret);\n\tif (head && !(head->flags & REQ_F_FAIL))\n\t\treq_fail_link_node(head, -ECANCELED);\n\n\tif (!(req->flags & IO_REQ_LINK_FLAGS)) {\n\t\tif (head) {\n\t\t\tlink->last->link = req;\n\t\t\tlink->head = NULL;\n\t\t\treq = head;\n\t\t}\n\t\tio_queue_sqe_fallback(req);\n\t\treturn ret;\n\t}\n\n\tif (head)\n\t\tlink->last->link = req;\n\telse\n\t\tlink->head = req;\n\tlink->last = req;\n\treturn 0;\n}\n\nstatic inline int io_submit_sqe(struct io_ring_ctx *ctx, struct io_kiocb *req,\n\t\t\t const struct io_uring_sqe *sqe)\n\t__must_hold(&ctx->uring_lock)\n{\n\tstruct io_submit_link *link = &ctx->submit_state.link;\n\tint ret;\n\n\tret = io_init_req(ctx, req, sqe);\n\tif (unlikely(ret))\n\t\treturn io_submit_fail_init(sqe, req, ret);\n\n\ttrace_io_uring_submit_req(req);\n\n\t \n\tif (unlikely(link->head)) {\n\t\tret = io_req_prep_async(req);\n\t\tif (unlikely(ret))\n\t\t\treturn io_submit_fail_init(sqe, req, ret);\n\n\t\ttrace_io_uring_link(req, link->head);\n\t\tlink->last->link = req;\n\t\tlink->last = req;\n\n\t\tif (req->flags & IO_REQ_LINK_FLAGS)\n\t\t\treturn 0;\n\t\t \n\t\treq = link->head;\n\t\tlink->head = NULL;\n\t\tif (req->flags & (REQ_F_FORCE_ASYNC | REQ_F_FAIL))\n\t\t\tgoto fallback;\n\n\t} else if (unlikely(req->flags & (IO_REQ_LINK_FLAGS |\n\t\t\t\t\t  REQ_F_FORCE_ASYNC | REQ_F_FAIL))) {\n\t\tif (req->flags & IO_REQ_LINK_FLAGS) {\n\t\t\tlink->head = req;\n\t\t\tlink->last = req;\n\t\t} else {\nfallback:\n\t\t\tio_queue_sqe_fallback(req);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tio_queue_sqe(req);\n\treturn 0;\n}\n\n \nstatic void io_submit_state_end(struct io_ring_ctx *ctx)\n{\n\tstruct io_submit_state *state = &ctx->submit_state;\n\n\tif (unlikely(state->link.head))\n\t\tio_queue_sqe_fallback(state->link.head);\n\t \n\tio_submit_flush_completions(ctx);\n\tif (state->plug_started)\n\t\tblk_finish_plug(&state->plug);\n}\n\n \nstatic void io_submit_state_start(struct io_submit_state *state,\n\t\t\t\t  unsigned int max_ios)\n{\n\tstate->plug_started = false;\n\tstate->need_plug = max_ios > 2;\n\tstate->submit_nr = max_ios;\n\t \n\tstate->link.head = NULL;\n}\n\nstatic void io_commit_sqring(struct io_ring_ctx *ctx)\n{\n\tstruct io_rings *rings = ctx->rings;\n\n\t \n\tsmp_store_release(&rings->sq.head, ctx->cached_sq_head);\n}\n\n \nstatic bool io_get_sqe(struct io_ring_ctx *ctx, const struct io_uring_sqe **sqe)\n{\n\tunsigned mask = ctx->sq_entries - 1;\n\tunsigned head = ctx->cached_sq_head++ & mask;\n\n\tif (!(ctx->flags & IORING_SETUP_NO_SQARRAY)) {\n\t\thead = READ_ONCE(ctx->sq_array[head]);\n\t\tif (unlikely(head >= ctx->sq_entries)) {\n\t\t\t \n\t\t\tspin_lock(&ctx->completion_lock);\n\t\t\tctx->cq_extra--;\n\t\t\tspin_unlock(&ctx->completion_lock);\n\t\t\tWRITE_ONCE(ctx->rings->sq_dropped,\n\t\t\t\t   READ_ONCE(ctx->rings->sq_dropped) + 1);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\t \n\n\t \n\tif (ctx->flags & IORING_SETUP_SQE128)\n\t\thead <<= 1;\n\t*sqe = &ctx->sq_sqes[head];\n\treturn true;\n}\n\nint io_submit_sqes(struct io_ring_ctx *ctx, unsigned int nr)\n\t__must_hold(&ctx->uring_lock)\n{\n\tunsigned int entries = io_sqring_entries(ctx);\n\tunsigned int left;\n\tint ret;\n\n\tif (unlikely(!entries))\n\t\treturn 0;\n\t \n\tret = left = min(nr, entries);\n\tio_get_task_refs(left);\n\tio_submit_state_start(&ctx->submit_state, left);\n\n\tdo {\n\t\tconst struct io_uring_sqe *sqe;\n\t\tstruct io_kiocb *req;\n\n\t\tif (unlikely(!io_alloc_req(ctx, &req)))\n\t\t\tbreak;\n\t\tif (unlikely(!io_get_sqe(ctx, &sqe))) {\n\t\t\tio_req_add_to_cache(req, ctx);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (unlikely(io_submit_sqe(ctx, req, sqe)) &&\n\t\t    !(ctx->flags & IORING_SETUP_SUBMIT_ALL)) {\n\t\t\tleft--;\n\t\t\tbreak;\n\t\t}\n\t} while (--left);\n\n\tif (unlikely(left)) {\n\t\tret -= left;\n\t\t \n\t\tif (!ret && io_req_cache_empty(ctx))\n\t\t\tret = -EAGAIN;\n\t\tcurrent->io_uring->cached_refs += left;\n\t}\n\n\tio_submit_state_end(ctx);\n\t  \n\tio_commit_sqring(ctx);\n\treturn ret;\n}\n\nstruct io_wait_queue {\n\tstruct wait_queue_entry wq;\n\tstruct io_ring_ctx *ctx;\n\tunsigned cq_tail;\n\tunsigned nr_timeouts;\n\tktime_t timeout;\n};\n\nstatic inline bool io_has_work(struct io_ring_ctx *ctx)\n{\n\treturn test_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq) ||\n\t       !llist_empty(&ctx->work_llist);\n}\n\nstatic inline bool io_should_wake(struct io_wait_queue *iowq)\n{\n\tstruct io_ring_ctx *ctx = iowq->ctx;\n\tint dist = READ_ONCE(ctx->rings->cq.tail) - (int) iowq->cq_tail;\n\n\t \n\treturn dist >= 0 || atomic_read(&ctx->cq_timeouts) != iowq->nr_timeouts;\n}\n\nstatic int io_wake_function(struct wait_queue_entry *curr, unsigned int mode,\n\t\t\t    int wake_flags, void *key)\n{\n\tstruct io_wait_queue *iowq = container_of(curr, struct io_wait_queue, wq);\n\n\t \n\tif (io_should_wake(iowq) || io_has_work(iowq->ctx))\n\t\treturn autoremove_wake_function(curr, mode, wake_flags, key);\n\treturn -1;\n}\n\nint io_run_task_work_sig(struct io_ring_ctx *ctx)\n{\n\tif (!llist_empty(&ctx->work_llist)) {\n\t\t__set_current_state(TASK_RUNNING);\n\t\tif (io_run_local_work(ctx) > 0)\n\t\t\treturn 0;\n\t}\n\tif (io_run_task_work() > 0)\n\t\treturn 0;\n\tif (task_sigpending(current))\n\t\treturn -EINTR;\n\treturn 0;\n}\n\nstatic bool current_pending_io(void)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\n\tif (!tctx)\n\t\treturn false;\n\treturn percpu_counter_read_positive(&tctx->inflight);\n}\n\n \nstatic inline int io_cqring_wait_schedule(struct io_ring_ctx *ctx,\n\t\t\t\t\t  struct io_wait_queue *iowq)\n{\n\tint io_wait, ret;\n\n\tif (unlikely(READ_ONCE(ctx->check_cq)))\n\t\treturn 1;\n\tif (unlikely(!llist_empty(&ctx->work_llist)))\n\t\treturn 1;\n\tif (unlikely(test_thread_flag(TIF_NOTIFY_SIGNAL)))\n\t\treturn 1;\n\tif (unlikely(task_sigpending(current)))\n\t\treturn -EINTR;\n\tif (unlikely(io_should_wake(iowq)))\n\t\treturn 0;\n\n\t \n\tio_wait = current->in_iowait;\n\tif (current_pending_io())\n\t\tcurrent->in_iowait = 1;\n\tret = 0;\n\tif (iowq->timeout == KTIME_MAX)\n\t\tschedule();\n\telse if (!schedule_hrtimeout(&iowq->timeout, HRTIMER_MODE_ABS))\n\t\tret = -ETIME;\n\tcurrent->in_iowait = io_wait;\n\treturn ret;\n}\n\n \nstatic int io_cqring_wait(struct io_ring_ctx *ctx, int min_events,\n\t\t\t  const sigset_t __user *sig, size_t sigsz,\n\t\t\t  struct __kernel_timespec __user *uts)\n{\n\tstruct io_wait_queue iowq;\n\tstruct io_rings *rings = ctx->rings;\n\tint ret;\n\n\tif (!io_allowed_run_tw(ctx))\n\t\treturn -EEXIST;\n\tif (!llist_empty(&ctx->work_llist))\n\t\tio_run_local_work(ctx);\n\tio_run_task_work();\n\tio_cqring_overflow_flush(ctx);\n\t \n\tif (__io_cqring_events_user(ctx) >= min_events)\n\t\treturn 0;\n\n\tif (sig) {\n#ifdef CONFIG_COMPAT\n\t\tif (in_compat_syscall())\n\t\t\tret = set_compat_user_sigmask((const compat_sigset_t __user *)sig,\n\t\t\t\t\t\t      sigsz);\n\t\telse\n#endif\n\t\t\tret = set_user_sigmask(sig, sigsz);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tinit_waitqueue_func_entry(&iowq.wq, io_wake_function);\n\tiowq.wq.private = current;\n\tINIT_LIST_HEAD(&iowq.wq.entry);\n\tiowq.ctx = ctx;\n\tiowq.nr_timeouts = atomic_read(&ctx->cq_timeouts);\n\tiowq.cq_tail = READ_ONCE(ctx->rings->cq.head) + min_events;\n\tiowq.timeout = KTIME_MAX;\n\n\tif (uts) {\n\t\tstruct timespec64 ts;\n\n\t\tif (get_timespec64(&ts, uts))\n\t\t\treturn -EFAULT;\n\t\tiowq.timeout = ktime_add_ns(timespec64_to_ktime(ts), ktime_get_ns());\n\t}\n\n\ttrace_io_uring_cqring_wait(ctx, min_events);\n\tdo {\n\t\tunsigned long check_cq;\n\n\t\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN) {\n\t\t\tint nr_wait = (int) iowq.cq_tail - READ_ONCE(ctx->rings->cq.tail);\n\n\t\t\tatomic_set(&ctx->cq_wait_nr, nr_wait);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t} else {\n\t\t\tprepare_to_wait_exclusive(&ctx->cq_wait, &iowq.wq,\n\t\t\t\t\t\t\tTASK_INTERRUPTIBLE);\n\t\t}\n\n\t\tret = io_cqring_wait_schedule(ctx, &iowq);\n\t\t__set_current_state(TASK_RUNNING);\n\t\tatomic_set(&ctx->cq_wait_nr, 0);\n\n\t\t \n\t\tio_run_task_work();\n\t\tif (!llist_empty(&ctx->work_llist))\n\t\t\tio_run_local_work(ctx);\n\n\t\t \n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tcheck_cq = READ_ONCE(ctx->check_cq);\n\t\tif (unlikely(check_cq)) {\n\t\t\t \n\t\t\tif (check_cq & BIT(IO_CHECK_CQ_OVERFLOW_BIT))\n\t\t\t\tio_cqring_do_overflow_flush(ctx);\n\t\t\tif (check_cq & BIT(IO_CHECK_CQ_DROPPED_BIT)) {\n\t\t\t\tret = -EBADR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (io_should_wake(&iowq)) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tcond_resched();\n\t} while (1);\n\n\tif (!(ctx->flags & IORING_SETUP_DEFER_TASKRUN))\n\t\tfinish_wait(&ctx->cq_wait, &iowq.wq);\n\trestore_saved_sigmask_unless(ret == -EINTR);\n\n\treturn READ_ONCE(rings->cq.head) == READ_ONCE(rings->cq.tail) ? ret : 0;\n}\n\nvoid io_mem_free(void *ptr)\n{\n\tif (!ptr)\n\t\treturn;\n\n\tfolio_put(virt_to_folio(ptr));\n}\n\nstatic void io_pages_free(struct page ***pages, int npages)\n{\n\tstruct page **page_array;\n\tint i;\n\n\tif (!pages)\n\t\treturn;\n\n\tpage_array = *pages;\n\tif (!page_array)\n\t\treturn;\n\n\tfor (i = 0; i < npages; i++)\n\t\tunpin_user_page(page_array[i]);\n\tkvfree(page_array);\n\t*pages = NULL;\n}\n\nstatic void *__io_uaddr_map(struct page ***pages, unsigned short *npages,\n\t\t\t    unsigned long uaddr, size_t size)\n{\n\tstruct page **page_array;\n\tunsigned int nr_pages;\n\tvoid *page_addr;\n\tint ret, i;\n\n\t*npages = 0;\n\n\tif (uaddr & (PAGE_SIZE - 1) || !size)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tnr_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tif (nr_pages > USHRT_MAX)\n\t\treturn ERR_PTR(-EINVAL);\n\tpage_array = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!page_array)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = pin_user_pages_fast(uaddr, nr_pages, FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t\t\tpage_array);\n\tif (ret != nr_pages) {\nerr:\n\t\tio_pages_free(&page_array, ret > 0 ? ret : 0);\n\t\treturn ret < 0 ? ERR_PTR(ret) : ERR_PTR(-EFAULT);\n\t}\n\n\tpage_addr = page_address(page_array[0]);\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tret = -EINVAL;\n\n\t\t \n\t\tif (PageHighMem(page_array[i]))\n\t\t\tgoto err;\n\n\t\t \n\t\tif (page_address(page_array[i]) != page_addr)\n\t\t\tgoto err;\n\t\tpage_addr += PAGE_SIZE;\n\t}\n\n\t*pages = page_array;\n\t*npages = nr_pages;\n\treturn page_to_virt(page_array[0]);\n}\n\nstatic void *io_rings_map(struct io_ring_ctx *ctx, unsigned long uaddr,\n\t\t\t  size_t size)\n{\n\treturn __io_uaddr_map(&ctx->ring_pages, &ctx->n_ring_pages, uaddr,\n\t\t\t\tsize);\n}\n\nstatic void *io_sqes_map(struct io_ring_ctx *ctx, unsigned long uaddr,\n\t\t\t size_t size)\n{\n\treturn __io_uaddr_map(&ctx->sqe_pages, &ctx->n_sqe_pages, uaddr,\n\t\t\t\tsize);\n}\n\nstatic void io_rings_free(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_NO_MMAP)) {\n\t\tio_mem_free(ctx->rings);\n\t\tio_mem_free(ctx->sq_sqes);\n\t\tctx->rings = NULL;\n\t\tctx->sq_sqes = NULL;\n\t} else {\n\t\tio_pages_free(&ctx->ring_pages, ctx->n_ring_pages);\n\t\tctx->n_ring_pages = 0;\n\t\tio_pages_free(&ctx->sqe_pages, ctx->n_sqe_pages);\n\t\tctx->n_sqe_pages = 0;\n\t}\n}\n\nvoid *io_mem_alloc(size_t size)\n{\n\tgfp_t gfp = GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP;\n\tvoid *ret;\n\n\tret = (void *) __get_free_pages(gfp, get_order(size));\n\tif (ret)\n\t\treturn ret;\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic unsigned long rings_size(struct io_ring_ctx *ctx, unsigned int sq_entries,\n\t\t\t\tunsigned int cq_entries, size_t *sq_offset)\n{\n\tstruct io_rings *rings;\n\tsize_t off, sq_array_size;\n\n\toff = struct_size(rings, cqes, cq_entries);\n\tif (off == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\tif (ctx->flags & IORING_SETUP_CQE32) {\n\t\tif (check_shl_overflow(off, 1, &off))\n\t\t\treturn SIZE_MAX;\n\t}\n\n#ifdef CONFIG_SMP\n\toff = ALIGN(off, SMP_CACHE_BYTES);\n\tif (off == 0)\n\t\treturn SIZE_MAX;\n#endif\n\n\tif (ctx->flags & IORING_SETUP_NO_SQARRAY) {\n\t\tif (sq_offset)\n\t\t\t*sq_offset = SIZE_MAX;\n\t\treturn off;\n\t}\n\n\tif (sq_offset)\n\t\t*sq_offset = off;\n\n\tsq_array_size = array_size(sizeof(u32), sq_entries);\n\tif (sq_array_size == SIZE_MAX)\n\t\treturn SIZE_MAX;\n\n\tif (check_add_overflow(off, sq_array_size, &off))\n\t\treturn SIZE_MAX;\n\n\treturn off;\n}\n\nstatic int io_eventfd_register(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t       unsigned int eventfd_async)\n{\n\tstruct io_ev_fd *ev_fd;\n\t__s32 __user *fds = arg;\n\tint fd;\n\n\tev_fd = rcu_dereference_protected(ctx->io_ev_fd,\n\t\t\t\t\tlockdep_is_held(&ctx->uring_lock));\n\tif (ev_fd)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&fd, fds, sizeof(*fds)))\n\t\treturn -EFAULT;\n\n\tev_fd = kmalloc(sizeof(*ev_fd), GFP_KERNEL);\n\tif (!ev_fd)\n\t\treturn -ENOMEM;\n\n\tev_fd->cq_ev_fd = eventfd_ctx_fdget(fd);\n\tif (IS_ERR(ev_fd->cq_ev_fd)) {\n\t\tint ret = PTR_ERR(ev_fd->cq_ev_fd);\n\t\tkfree(ev_fd);\n\t\treturn ret;\n\t}\n\n\tspin_lock(&ctx->completion_lock);\n\tctx->evfd_last_cq_tail = ctx->cached_cq_tail;\n\tspin_unlock(&ctx->completion_lock);\n\n\tev_fd->eventfd_async = eventfd_async;\n\tctx->has_evfd = true;\n\trcu_assign_pointer(ctx->io_ev_fd, ev_fd);\n\tatomic_set(&ev_fd->refs, 1);\n\tatomic_set(&ev_fd->ops, 0);\n\treturn 0;\n}\n\nstatic int io_eventfd_unregister(struct io_ring_ctx *ctx)\n{\n\tstruct io_ev_fd *ev_fd;\n\n\tev_fd = rcu_dereference_protected(ctx->io_ev_fd,\n\t\t\t\t\tlockdep_is_held(&ctx->uring_lock));\n\tif (ev_fd) {\n\t\tctx->has_evfd = false;\n\t\trcu_assign_pointer(ctx->io_ev_fd, NULL);\n\t\tif (!atomic_fetch_or(BIT(IO_EVENTFD_OP_FREE_BIT), &ev_fd->ops))\n\t\t\tcall_rcu(&ev_fd->rcu, io_eventfd_ops);\n\t\treturn 0;\n\t}\n\n\treturn -ENXIO;\n}\n\nstatic void io_req_caches_free(struct io_ring_ctx *ctx)\n{\n\tstruct io_kiocb *req;\n\tint nr = 0;\n\n\tmutex_lock(&ctx->uring_lock);\n\tio_flush_cached_locked_reqs(ctx, &ctx->submit_state);\n\n\twhile (!io_req_cache_empty(ctx)) {\n\t\treq = io_extract_req(ctx);\n\t\tkmem_cache_free(req_cachep, req);\n\t\tnr++;\n\t}\n\tif (nr)\n\t\tpercpu_ref_put_many(&ctx->refs, nr);\n\tmutex_unlock(&ctx->uring_lock);\n}\n\nstatic void io_rsrc_node_cache_free(struct io_cache_entry *entry)\n{\n\tkfree(container_of(entry, struct io_rsrc_node, cache));\n}\n\nstatic __cold void io_ring_ctx_free(struct io_ring_ctx *ctx)\n{\n\tio_sq_thread_finish(ctx);\n\t \n\tif (WARN_ON_ONCE(!list_empty(&ctx->rsrc_ref_list)))\n\t\treturn;\n\n\tmutex_lock(&ctx->uring_lock);\n\tif (ctx->buf_data)\n\t\t__io_sqe_buffers_unregister(ctx);\n\tif (ctx->file_data)\n\t\t__io_sqe_files_unregister(ctx);\n\tio_cqring_overflow_kill(ctx);\n\tio_eventfd_unregister(ctx);\n\tio_alloc_cache_free(&ctx->apoll_cache, io_apoll_cache_free);\n\tio_alloc_cache_free(&ctx->netmsg_cache, io_netmsg_cache_free);\n\tio_destroy_buffers(ctx);\n\tmutex_unlock(&ctx->uring_lock);\n\tif (ctx->sq_creds)\n\t\tput_cred(ctx->sq_creds);\n\tif (ctx->submitter_task)\n\t\tput_task_struct(ctx->submitter_task);\n\n\t \n\tif (ctx->rsrc_node)\n\t\tio_rsrc_node_destroy(ctx, ctx->rsrc_node);\n\n\tWARN_ON_ONCE(!list_empty(&ctx->rsrc_ref_list));\n\n#if defined(CONFIG_UNIX)\n\tif (ctx->ring_sock) {\n\t\tctx->ring_sock->file = NULL;  \n\t\tsock_release(ctx->ring_sock);\n\t}\n#endif\n\tWARN_ON_ONCE(!list_empty(&ctx->ltimeout_list));\n\n\tio_alloc_cache_free(&ctx->rsrc_node_cache, io_rsrc_node_cache_free);\n\tif (ctx->mm_account) {\n\t\tmmdrop(ctx->mm_account);\n\t\tctx->mm_account = NULL;\n\t}\n\tio_rings_free(ctx);\n\tio_kbuf_mmap_list_free(ctx);\n\n\tpercpu_ref_exit(&ctx->refs);\n\tfree_uid(ctx->user);\n\tio_req_caches_free(ctx);\n\tif (ctx->hash_map)\n\t\tio_wq_put_hash(ctx->hash_map);\n\tkfree(ctx->cancel_table.hbs);\n\tkfree(ctx->cancel_table_locked.hbs);\n\tkfree(ctx->io_bl);\n\txa_destroy(&ctx->io_bl_xa);\n\tkfree(ctx);\n}\n\nstatic __cold void io_activate_pollwq_cb(struct callback_head *cb)\n{\n\tstruct io_ring_ctx *ctx = container_of(cb, struct io_ring_ctx,\n\t\t\t\t\t       poll_wq_task_work);\n\n\tmutex_lock(&ctx->uring_lock);\n\tctx->poll_activated = true;\n\tmutex_unlock(&ctx->uring_lock);\n\n\t \n\twake_up_all(&ctx->poll_wq);\n\tpercpu_ref_put(&ctx->refs);\n}\n\nstatic __cold void io_activate_pollwq(struct io_ring_ctx *ctx)\n{\n\tspin_lock(&ctx->completion_lock);\n\t \n\tif (ctx->poll_activated || ctx->poll_wq_task_work.func)\n\t\tgoto out;\n\tif (WARN_ON_ONCE(!ctx->task_complete))\n\t\tgoto out;\n\tif (!ctx->submitter_task)\n\t\tgoto out;\n\t \n\tinit_task_work(&ctx->poll_wq_task_work, io_activate_pollwq_cb);\n\tpercpu_ref_get(&ctx->refs);\n\tif (task_work_add(ctx->submitter_task, &ctx->poll_wq_task_work, TWA_SIGNAL))\n\t\tpercpu_ref_put(&ctx->refs);\nout:\n\tspin_unlock(&ctx->completion_lock);\n}\n\nstatic __poll_t io_uring_poll(struct file *file, poll_table *wait)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\t__poll_t mask = 0;\n\n\tif (unlikely(!ctx->poll_activated))\n\t\tio_activate_pollwq(ctx);\n\n\tpoll_wait(file, &ctx->poll_wq, wait);\n\t \n\tsmp_rmb();\n\tif (!io_sqring_full(ctx))\n\t\tmask |= EPOLLOUT | EPOLLWRNORM;\n\n\t \n\n\tif (__io_cqring_events_user(ctx) || io_has_work(ctx))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\treturn mask;\n}\n\nstatic int io_unregister_personality(struct io_ring_ctx *ctx, unsigned id)\n{\n\tconst struct cred *creds;\n\n\tcreds = xa_erase(&ctx->personalities, id);\n\tif (creds) {\n\t\tput_cred(creds);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstruct io_tctx_exit {\n\tstruct callback_head\t\ttask_work;\n\tstruct completion\t\tcompletion;\n\tstruct io_ring_ctx\t\t*ctx;\n};\n\nstatic __cold void io_tctx_exit_cb(struct callback_head *cb)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_tctx_exit *work;\n\n\twork = container_of(cb, struct io_tctx_exit, task_work);\n\t \n\tif (tctx && !atomic_read(&tctx->in_cancel))\n\t\tio_uring_del_tctx_node((unsigned long)work->ctx);\n\tcomplete(&work->completion);\n}\n\nstatic __cold bool io_cancel_ctx_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\n\treturn req->ctx == data;\n}\n\nstatic __cold void io_ring_exit_work(struct work_struct *work)\n{\n\tstruct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx, exit_work);\n\tunsigned long timeout = jiffies + HZ * 60 * 5;\n\tunsigned long interval = HZ / 20;\n\tstruct io_tctx_exit exit;\n\tstruct io_tctx_node *node;\n\tint ret;\n\n\t \n\tdo {\n\t\tif (test_bit(IO_CHECK_CQ_OVERFLOW_BIT, &ctx->check_cq)) {\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t\tio_cqring_overflow_kill(ctx);\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t}\n\n\t\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN)\n\t\t\tio_move_task_work_from_local(ctx);\n\n\t\twhile (io_uring_try_cancel_requests(ctx, NULL, true))\n\t\t\tcond_resched();\n\n\t\tif (ctx->sq_data) {\n\t\t\tstruct io_sq_data *sqd = ctx->sq_data;\n\t\t\tstruct task_struct *tsk;\n\n\t\t\tio_sq_thread_park(sqd);\n\t\t\ttsk = sqd->thread;\n\t\t\tif (tsk && tsk->io_uring && tsk->io_uring->io_wq)\n\t\t\t\tio_wq_cancel_cb(tsk->io_uring->io_wq,\n\t\t\t\t\t\tio_cancel_ctx_cb, ctx, true);\n\t\t\tio_sq_thread_unpark(sqd);\n\t\t}\n\n\t\tio_req_caches_free(ctx);\n\n\t\tif (WARN_ON_ONCE(time_after(jiffies, timeout))) {\n\t\t\t \n\t\t\tinterval = HZ * 60;\n\t\t}\n\t\t \n\t} while (!wait_for_completion_interruptible_timeout(&ctx->ref_comp, interval));\n\n\tinit_completion(&exit.completion);\n\tinit_task_work(&exit.task_work, io_tctx_exit_cb);\n\texit.ctx = ctx;\n\n\tmutex_lock(&ctx->uring_lock);\n\twhile (!list_empty(&ctx->tctx_list)) {\n\t\tWARN_ON_ONCE(time_after(jiffies, timeout));\n\n\t\tnode = list_first_entry(&ctx->tctx_list, struct io_tctx_node,\n\t\t\t\t\tctx_node);\n\t\t \n\t\tlist_rotate_left(&ctx->tctx_list);\n\t\tret = task_work_add(node->task, &exit.task_work, TWA_SIGNAL);\n\t\tif (WARN_ON_ONCE(ret))\n\t\t\tcontinue;\n\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\t \n\t\twait_for_completion_interruptible(&exit.completion);\n\t\tmutex_lock(&ctx->uring_lock);\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n\tspin_lock(&ctx->completion_lock);\n\tspin_unlock(&ctx->completion_lock);\n\n\t \n\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN)\n\t\tsynchronize_rcu();\n\n\tio_ring_ctx_free(ctx);\n}\n\nstatic __cold void io_ring_ctx_wait_and_kill(struct io_ring_ctx *ctx)\n{\n\tunsigned long index;\n\tstruct creds *creds;\n\n\tmutex_lock(&ctx->uring_lock);\n\tpercpu_ref_kill(&ctx->refs);\n\txa_for_each(&ctx->personalities, index, creds)\n\t\tio_unregister_personality(ctx, index);\n\tif (ctx->rings)\n\t\tio_poll_remove_all(ctx, NULL, true);\n\tmutex_unlock(&ctx->uring_lock);\n\n\t \n\tif (ctx->rings)\n\t\tio_kill_timeouts(ctx, NULL, true);\n\n\tflush_delayed_work(&ctx->fallback_work);\n\n\tINIT_WORK(&ctx->exit_work, io_ring_exit_work);\n\t \n\tqueue_work(system_unbound_wq, &ctx->exit_work);\n}\n\nstatic int io_uring_release(struct inode *inode, struct file *file)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\n\tfile->private_data = NULL;\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn 0;\n}\n\nstruct io_task_cancel {\n\tstruct task_struct *task;\n\tbool all;\n};\n\nstatic bool io_cancel_task_cb(struct io_wq_work *work, void *data)\n{\n\tstruct io_kiocb *req = container_of(work, struct io_kiocb, work);\n\tstruct io_task_cancel *cancel = data;\n\n\treturn io_match_task_safe(req, cancel->task, cancel->all);\n}\n\nstatic __cold bool io_cancel_defer_files(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct task_struct *task,\n\t\t\t\t\t bool cancel_all)\n{\n\tstruct io_defer_entry *de;\n\tLIST_HEAD(list);\n\n\tspin_lock(&ctx->completion_lock);\n\tlist_for_each_entry_reverse(de, &ctx->defer_list, list) {\n\t\tif (io_match_task_safe(de->req, task, cancel_all)) {\n\t\t\tlist_cut_position(&list, &ctx->defer_list, &de->list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&ctx->completion_lock);\n\tif (list_empty(&list))\n\t\treturn false;\n\n\twhile (!list_empty(&list)) {\n\t\tde = list_first_entry(&list, struct io_defer_entry, list);\n\t\tlist_del_init(&de->list);\n\t\tio_req_task_queue_fail(de->req, -ECANCELED);\n\t\tkfree(de);\n\t}\n\treturn true;\n}\n\nstatic __cold bool io_uring_try_cancel_iowq(struct io_ring_ctx *ctx)\n{\n\tstruct io_tctx_node *node;\n\tenum io_wq_cancel cret;\n\tbool ret = false;\n\n\tmutex_lock(&ctx->uring_lock);\n\tlist_for_each_entry(node, &ctx->tctx_list, ctx_node) {\n\t\tstruct io_uring_task *tctx = node->task->io_uring;\n\n\t\t \n\t\tif (!tctx || !tctx->io_wq)\n\t\t\tcontinue;\n\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_ctx_cb, ctx, true);\n\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t}\n\tmutex_unlock(&ctx->uring_lock);\n\n\treturn ret;\n}\n\nstatic __cold bool io_uring_try_cancel_requests(struct io_ring_ctx *ctx,\n\t\t\t\t\t\tstruct task_struct *task,\n\t\t\t\t\t\tbool cancel_all)\n{\n\tstruct io_task_cancel cancel = { .task = task, .all = cancel_all, };\n\tstruct io_uring_task *tctx = task ? task->io_uring : NULL;\n\tenum io_wq_cancel cret;\n\tbool ret = false;\n\n\t \n\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN) {\n\t\tatomic_set(&ctx->cq_wait_nr, 1);\n\t\tsmp_mb();\n\t}\n\n\t \n\tif (!ctx->rings)\n\t\treturn false;\n\n\tif (!task) {\n\t\tret |= io_uring_try_cancel_iowq(ctx);\n\t} else if (tctx && tctx->io_wq) {\n\t\t \n\t\tcret = io_wq_cancel_cb(tctx->io_wq, io_cancel_task_cb,\n\t\t\t\t       &cancel, true);\n\t\tret |= (cret != IO_WQ_CANCEL_NOTFOUND);\n\t}\n\n\t \n\tif ((!(ctx->flags & IORING_SETUP_SQPOLL) && cancel_all) ||\n\t    (ctx->sq_data && ctx->sq_data->thread == current)) {\n\t\twhile (!wq_list_empty(&ctx->iopoll_list)) {\n\t\t\tio_iopoll_try_reap_events(ctx);\n\t\t\tret = true;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tif ((ctx->flags & IORING_SETUP_DEFER_TASKRUN) &&\n\t    io_allowed_defer_tw_run(ctx))\n\t\tret |= io_run_local_work(ctx) > 0;\n\tret |= io_cancel_defer_files(ctx, task, cancel_all);\n\tmutex_lock(&ctx->uring_lock);\n\tret |= io_poll_remove_all(ctx, task, cancel_all);\n\tmutex_unlock(&ctx->uring_lock);\n\tret |= io_kill_timeouts(ctx, task, cancel_all);\n\tif (task)\n\t\tret |= io_run_task_work() > 0;\n\treturn ret;\n}\n\nstatic s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)\n{\n\tif (tracked)\n\t\treturn atomic_read(&tctx->inflight_tracked);\n\treturn percpu_counter_sum(&tctx->inflight);\n}\n\n \n__cold void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd)\n{\n\tstruct io_uring_task *tctx = current->io_uring;\n\tstruct io_ring_ctx *ctx;\n\tstruct io_tctx_node *node;\n\tunsigned long index;\n\ts64 inflight;\n\tDEFINE_WAIT(wait);\n\n\tWARN_ON_ONCE(sqd && sqd->thread != current);\n\n\tif (!current->io_uring)\n\t\treturn;\n\tif (tctx->io_wq)\n\t\tio_wq_exit_start(tctx->io_wq);\n\n\tatomic_inc(&tctx->in_cancel);\n\tdo {\n\t\tbool loop = false;\n\n\t\tio_uring_drop_tctx_refs(current);\n\t\t \n\t\tinflight = tctx_inflight(tctx, !cancel_all);\n\t\tif (!inflight)\n\t\t\tbreak;\n\n\t\tif (!sqd) {\n\t\t\txa_for_each(&tctx->xa, index, node) {\n\t\t\t\t \n\t\t\t\tif (node->ctx->sq_data)\n\t\t\t\t\tcontinue;\n\t\t\t\tloop |= io_uring_try_cancel_requests(node->ctx,\n\t\t\t\t\t\t\tcurrent, cancel_all);\n\t\t\t}\n\t\t} else {\n\t\t\tlist_for_each_entry(ctx, &sqd->ctx_list, sqd_list)\n\t\t\t\tloop |= io_uring_try_cancel_requests(ctx,\n\t\t\t\t\t\t\t\t     current,\n\t\t\t\t\t\t\t\t     cancel_all);\n\t\t}\n\n\t\tif (loop) {\n\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t}\n\n\t\tprepare_to_wait(&tctx->wait, &wait, TASK_INTERRUPTIBLE);\n\t\tio_run_task_work();\n\t\tio_uring_drop_tctx_refs(current);\n\t\txa_for_each(&tctx->xa, index, node) {\n\t\t\tif (!llist_empty(&node->ctx->work_llist)) {\n\t\t\t\tWARN_ON_ONCE(node->ctx->submitter_task &&\n\t\t\t\t\t     node->ctx->submitter_task != current);\n\t\t\t\tgoto end_wait;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (inflight == tctx_inflight(tctx, !cancel_all))\n\t\t\tschedule();\nend_wait:\n\t\tfinish_wait(&tctx->wait, &wait);\n\t} while (1);\n\n\tio_uring_clean_tctx(tctx);\n\tif (cancel_all) {\n\t\t \n\t\tatomic_dec(&tctx->in_cancel);\n\t\t \n\t\t__io_uring_free(current);\n\t}\n}\n\nvoid __io_uring_cancel(bool cancel_all)\n{\n\tio_uring_cancel_generic(cancel_all, NULL);\n}\n\nstatic void *io_uring_validate_mmap_request(struct file *file,\n\t\t\t\t\t    loff_t pgoff, size_t sz)\n{\n\tstruct io_ring_ctx *ctx = file->private_data;\n\tloff_t offset = pgoff << PAGE_SHIFT;\n\tstruct page *page;\n\tvoid *ptr;\n\n\tswitch (offset & IORING_OFF_MMAP_MASK) {\n\tcase IORING_OFF_SQ_RING:\n\tcase IORING_OFF_CQ_RING:\n\t\t \n\t\tif (ctx->flags & IORING_SETUP_NO_MMAP)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tptr = ctx->rings;\n\t\tbreak;\n\tcase IORING_OFF_SQES:\n\t\t \n\t\tif (ctx->flags & IORING_SETUP_NO_MMAP)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tptr = ctx->sq_sqes;\n\t\tbreak;\n\tcase IORING_OFF_PBUF_RING: {\n\t\tunsigned int bgid;\n\n\t\tbgid = (offset & ~IORING_OFF_MMAP_MASK) >> IORING_OFF_PBUF_SHIFT;\n\t\trcu_read_lock();\n\t\tptr = io_pbuf_get_address(ctx, bgid);\n\t\trcu_read_unlock();\n\t\tif (!ptr)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tbreak;\n\t\t}\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tpage = virt_to_head_page(ptr);\n\tif (sz > page_size(page))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ptr;\n}\n\n#ifdef CONFIG_MMU\n\nstatic __cold int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t sz = vma->vm_end - vma->vm_start;\n\tunsigned long pfn;\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\tpfn = virt_to_phys(ptr) >> PAGE_SHIFT;\n\treturn remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot);\n}\n\nstatic unsigned long io_uring_mmu_get_unmapped_area(struct file *filp,\n\t\t\tunsigned long addr, unsigned long len,\n\t\t\tunsigned long pgoff, unsigned long flags)\n{\n\tvoid *ptr;\n\n\t \n\tif (addr)\n\t\treturn -EINVAL;\n\n\tptr = io_uring_validate_mmap_request(filp, pgoff, len);\n\tif (IS_ERR(ptr))\n\t\treturn -ENOMEM;\n\n\t \n\tfilp = NULL;\n\tflags |= MAP_SHARED;\n\tpgoff = 0;\t \n#ifdef SHM_COLOUR\n\taddr = (uintptr_t) ptr;\n\tpgoff = addr >> PAGE_SHIFT;\n#else\n\taddr = 0UL;\n#endif\n\treturn current->mm->get_unmapped_area(filp, addr, len, pgoff, flags);\n}\n\n#else  \n\nstatic int io_uring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\treturn is_nommu_shared_mapping(vma->vm_flags) ? 0 : -EINVAL;\n}\n\nstatic unsigned int io_uring_nommu_mmap_capabilities(struct file *file)\n{\n\treturn NOMMU_MAP_DIRECT | NOMMU_MAP_READ | NOMMU_MAP_WRITE;\n}\n\nstatic unsigned long io_uring_nommu_get_unmapped_area(struct file *file,\n\tunsigned long addr, unsigned long len,\n\tunsigned long pgoff, unsigned long flags)\n{\n\tvoid *ptr;\n\n\tptr = io_uring_validate_mmap_request(file, pgoff, len);\n\tif (IS_ERR(ptr))\n\t\treturn PTR_ERR(ptr);\n\n\treturn (unsigned long) ptr;\n}\n\n#endif  \n\nstatic int io_validate_ext_arg(unsigned flags, const void __user *argp, size_t argsz)\n{\n\tif (flags & IORING_ENTER_EXT_ARG) {\n\t\tstruct io_uring_getevents_arg arg;\n\n\t\tif (argsz != sizeof(arg))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&arg, argp, sizeof(arg)))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\nstatic int io_get_ext_arg(unsigned flags, const void __user *argp, size_t *argsz,\n\t\t\t  struct __kernel_timespec __user **ts,\n\t\t\t  const sigset_t __user **sig)\n{\n\tstruct io_uring_getevents_arg arg;\n\n\t \n\tif (!(flags & IORING_ENTER_EXT_ARG)) {\n\t\t*sig = (const sigset_t __user *) argp;\n\t\t*ts = NULL;\n\t\treturn 0;\n\t}\n\n\t \n\tif (*argsz != sizeof(arg))\n\t\treturn -EINVAL;\n\tif (copy_from_user(&arg, argp, sizeof(arg)))\n\t\treturn -EFAULT;\n\tif (arg.pad)\n\t\treturn -EINVAL;\n\t*sig = u64_to_user_ptr(arg.sigmask);\n\t*argsz = arg.sigmask_sz;\n\t*ts = u64_to_user_ptr(arg.ts);\n\treturn 0;\n}\n\nSYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,\n\t\tu32, min_complete, u32, flags, const void __user *, argp,\n\t\tsize_t, argsz)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct file *file;\n\tlong ret;\n\n\tif (unlikely(flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |\n\t\t\t       IORING_ENTER_SQ_WAIT | IORING_ENTER_EXT_ARG |\n\t\t\t       IORING_ENTER_REGISTERED_RING)))\n\t\treturn -EINVAL;\n\n\t \n\tif (flags & IORING_ENTER_REGISTERED_RING) {\n\t\tstruct io_uring_task *tctx = current->io_uring;\n\n\t\tif (unlikely(!tctx || fd >= IO_RINGFD_REG_MAX))\n\t\t\treturn -EINVAL;\n\t\tfd = array_index_nospec(fd, IO_RINGFD_REG_MAX);\n\t\tfile = tctx->registered_rings[fd];\n\t\tif (unlikely(!file))\n\t\t\treturn -EBADF;\n\t} else {\n\t\tfile = fget(fd);\n\t\tif (unlikely(!file))\n\t\t\treturn -EBADF;\n\t\tret = -EOPNOTSUPP;\n\t\tif (unlikely(!io_is_uring_fops(file)))\n\t\t\tgoto out;\n\t}\n\n\tctx = file->private_data;\n\tret = -EBADFD;\n\tif (unlikely(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\tgoto out;\n\n\t \n\tret = 0;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tio_cqring_overflow_flush(ctx);\n\n\t\tif (unlikely(ctx->sq_data->thread == NULL)) {\n\t\t\tret = -EOWNERDEAD;\n\t\t\tgoto out;\n\t\t}\n\t\tif (flags & IORING_ENTER_SQ_WAKEUP)\n\t\t\twake_up(&ctx->sq_data->wait);\n\t\tif (flags & IORING_ENTER_SQ_WAIT)\n\t\t\tio_sqpoll_wait_sq(ctx);\n\n\t\tret = to_submit;\n\t} else if (to_submit) {\n\t\tret = io_uring_add_tctx_node(ctx);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tmutex_lock(&ctx->uring_lock);\n\t\tret = io_submit_sqes(ctx, to_submit);\n\t\tif (ret != to_submit) {\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\t\tif (ctx->syscall_iopoll)\n\t\t\t\tgoto iopoll_locked;\n\t\t\t \n\t\t\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN)\n\t\t\t\t(void)io_run_local_work_locked(ctx);\n\t\t}\n\t\tmutex_unlock(&ctx->uring_lock);\n\t}\n\n\tif (flags & IORING_ENTER_GETEVENTS) {\n\t\tint ret2;\n\n\t\tif (ctx->syscall_iopoll) {\n\t\t\t \n\t\t\tmutex_lock(&ctx->uring_lock);\niopoll_locked:\n\t\t\tret2 = io_validate_ext_arg(flags, argp, argsz);\n\t\t\tif (likely(!ret2)) {\n\t\t\t\tmin_complete = min(min_complete,\n\t\t\t\t\t\t   ctx->cq_entries);\n\t\t\t\tret2 = io_iopoll_check(ctx, min_complete);\n\t\t\t}\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t} else {\n\t\t\tconst sigset_t __user *sig;\n\t\t\tstruct __kernel_timespec __user *ts;\n\n\t\t\tret2 = io_get_ext_arg(flags, argp, &argsz, &ts, &sig);\n\t\t\tif (likely(!ret2)) {\n\t\t\t\tmin_complete = min(min_complete,\n\t\t\t\t\t\t   ctx->cq_entries);\n\t\t\t\tret2 = io_cqring_wait(ctx, min_complete, sig,\n\t\t\t\t\t\t      argsz, ts);\n\t\t\t}\n\t\t}\n\n\t\tif (!ret) {\n\t\t\tret = ret2;\n\n\t\t\t \n\t\t\tif (unlikely(ret2 == -EBADR))\n\t\t\t\tclear_bit(IO_CHECK_CQ_DROPPED_BIT,\n\t\t\t\t\t  &ctx->check_cq);\n\t\t}\n\t}\nout:\n\tif (!(flags & IORING_ENTER_REGISTERED_RING))\n\t\tfput(file);\n\treturn ret;\n}\n\nstatic const struct file_operations io_uring_fops = {\n\t.release\t= io_uring_release,\n\t.mmap\t\t= io_uring_mmap,\n#ifndef CONFIG_MMU\n\t.get_unmapped_area = io_uring_nommu_get_unmapped_area,\n\t.mmap_capabilities = io_uring_nommu_mmap_capabilities,\n#else\n\t.get_unmapped_area = io_uring_mmu_get_unmapped_area,\n#endif\n\t.poll\t\t= io_uring_poll,\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= io_uring_show_fdinfo,\n#endif\n};\n\nbool io_is_uring_fops(struct file *file)\n{\n\treturn file->f_op == &io_uring_fops;\n}\n\nstatic __cold int io_allocate_scq_urings(struct io_ring_ctx *ctx,\n\t\t\t\t\t struct io_uring_params *p)\n{\n\tstruct io_rings *rings;\n\tsize_t size, sq_array_offset;\n\tvoid *ptr;\n\n\t \n\tctx->sq_entries = p->sq_entries;\n\tctx->cq_entries = p->cq_entries;\n\n\tsize = rings_size(ctx, p->sq_entries, p->cq_entries, &sq_array_offset);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\tif (!(ctx->flags & IORING_SETUP_NO_MMAP))\n\t\trings = io_mem_alloc(size);\n\telse\n\t\trings = io_rings_map(ctx, p->cq_off.user_addr, size);\n\n\tif (IS_ERR(rings))\n\t\treturn PTR_ERR(rings);\n\n\tctx->rings = rings;\n\tif (!(ctx->flags & IORING_SETUP_NO_SQARRAY))\n\t\tctx->sq_array = (u32 *)((char *)rings + sq_array_offset);\n\trings->sq_ring_mask = p->sq_entries - 1;\n\trings->cq_ring_mask = p->cq_entries - 1;\n\trings->sq_ring_entries = p->sq_entries;\n\trings->cq_ring_entries = p->cq_entries;\n\n\tif (p->flags & IORING_SETUP_SQE128)\n\t\tsize = array_size(2 * sizeof(struct io_uring_sqe), p->sq_entries);\n\telse\n\t\tsize = array_size(sizeof(struct io_uring_sqe), p->sq_entries);\n\tif (size == SIZE_MAX) {\n\t\tio_rings_free(ctx);\n\t\treturn -EOVERFLOW;\n\t}\n\n\tif (!(ctx->flags & IORING_SETUP_NO_MMAP))\n\t\tptr = io_mem_alloc(size);\n\telse\n\t\tptr = io_sqes_map(ctx, p->sq_off.user_addr, size);\n\n\tif (IS_ERR(ptr)) {\n\t\tio_rings_free(ctx);\n\t\treturn PTR_ERR(ptr);\n\t}\n\n\tctx->sq_sqes = ptr;\n\treturn 0;\n}\n\nstatic int io_uring_install_fd(struct file *file)\n{\n\tint fd;\n\n\tfd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);\n\tif (fd < 0)\n\t\treturn fd;\n\tfd_install(fd, file);\n\treturn fd;\n}\n\n \nstatic struct file *io_uring_get_file(struct io_ring_ctx *ctx)\n{\n\tstruct file *file;\n#if defined(CONFIG_UNIX)\n\tint ret;\n\n\tret = sock_create_kern(&init_net, PF_UNIX, SOCK_RAW, IPPROTO_IP,\n\t\t\t\t&ctx->ring_sock);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n#endif\n\n\tfile = anon_inode_getfile_secure(\"[io_uring]\", &io_uring_fops, ctx,\n\t\t\t\t\t O_RDWR | O_CLOEXEC, NULL);\n#if defined(CONFIG_UNIX)\n\tif (IS_ERR(file)) {\n\t\tsock_release(ctx->ring_sock);\n\t\tctx->ring_sock = NULL;\n\t} else {\n\t\tctx->ring_sock->file = file;\n\t}\n#endif\n\treturn file;\n}\n\nstatic __cold int io_uring_create(unsigned entries, struct io_uring_params *p,\n\t\t\t\t  struct io_uring_params __user *params)\n{\n\tstruct io_ring_ctx *ctx;\n\tstruct io_uring_task *tctx;\n\tstruct file *file;\n\tint ret;\n\n\tif (!entries)\n\t\treturn -EINVAL;\n\tif (entries > IORING_MAX_ENTRIES) {\n\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\treturn -EINVAL;\n\t\tentries = IORING_MAX_ENTRIES;\n\t}\n\n\tif ((p->flags & IORING_SETUP_REGISTERED_FD_ONLY)\n\t    && !(p->flags & IORING_SETUP_NO_MMAP))\n\t\treturn -EINVAL;\n\n\t \n\tp->sq_entries = roundup_pow_of_two(entries);\n\tif (p->flags & IORING_SETUP_CQSIZE) {\n\t\t \n\t\tif (!p->cq_entries)\n\t\t\treturn -EINVAL;\n\t\tif (p->cq_entries > IORING_MAX_CQ_ENTRIES) {\n\t\t\tif (!(p->flags & IORING_SETUP_CLAMP))\n\t\t\t\treturn -EINVAL;\n\t\t\tp->cq_entries = IORING_MAX_CQ_ENTRIES;\n\t\t}\n\t\tp->cq_entries = roundup_pow_of_two(p->cq_entries);\n\t\tif (p->cq_entries < p->sq_entries)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tp->cq_entries = 2 * p->sq_entries;\n\t}\n\n\tctx = io_ring_ctx_alloc(p);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tif ((ctx->flags & IORING_SETUP_DEFER_TASKRUN) &&\n\t    !(ctx->flags & IORING_SETUP_IOPOLL) &&\n\t    !(ctx->flags & IORING_SETUP_SQPOLL))\n\t\tctx->task_complete = true;\n\n\tif (ctx->task_complete || (ctx->flags & IORING_SETUP_IOPOLL))\n\t\tctx->lockless_cq = true;\n\n\t \n\tif (!ctx->task_complete)\n\t\tctx->poll_activated = true;\n\n\t \n\tif (ctx->flags & IORING_SETUP_IOPOLL &&\n\t    !(ctx->flags & IORING_SETUP_SQPOLL))\n\t\tctx->syscall_iopoll = 1;\n\n\tctx->compat = in_compat_syscall();\n\tif (!ns_capable_noaudit(&init_user_ns, CAP_IPC_LOCK))\n\t\tctx->user = get_uid(current_user());\n\n\t \n\tret = -EINVAL;\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\t \n\t\tif (ctx->flags & (IORING_SETUP_COOP_TASKRUN |\n\t\t\t\t  IORING_SETUP_TASKRUN_FLAG |\n\t\t\t\t  IORING_SETUP_DEFER_TASKRUN))\n\t\t\tgoto err;\n\t\tctx->notify_method = TWA_SIGNAL_NO_IPI;\n\t} else if (ctx->flags & IORING_SETUP_COOP_TASKRUN) {\n\t\tctx->notify_method = TWA_SIGNAL_NO_IPI;\n\t} else {\n\t\tif (ctx->flags & IORING_SETUP_TASKRUN_FLAG &&\n\t\t    !(ctx->flags & IORING_SETUP_DEFER_TASKRUN))\n\t\t\tgoto err;\n\t\tctx->notify_method = TWA_SIGNAL;\n\t}\n\n\t \n\tif (ctx->flags & IORING_SETUP_DEFER_TASKRUN &&\n\t    !(ctx->flags & IORING_SETUP_SINGLE_ISSUER)) {\n\t\tgoto err;\n\t}\n\n\t \n\tmmgrab(current->mm);\n\tctx->mm_account = current->mm;\n\n\tret = io_allocate_scq_urings(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_sq_offload_create(ctx, p);\n\tif (ret)\n\t\tgoto err;\n\n\tret = io_rsrc_init(ctx);\n\tif (ret)\n\t\tgoto err;\n\n\tp->sq_off.head = offsetof(struct io_rings, sq.head);\n\tp->sq_off.tail = offsetof(struct io_rings, sq.tail);\n\tp->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);\n\tp->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);\n\tp->sq_off.flags = offsetof(struct io_rings, sq_flags);\n\tp->sq_off.dropped = offsetof(struct io_rings, sq_dropped);\n\tif (!(ctx->flags & IORING_SETUP_NO_SQARRAY))\n\t\tp->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;\n\tp->sq_off.resv1 = 0;\n\tif (!(ctx->flags & IORING_SETUP_NO_MMAP))\n\t\tp->sq_off.user_addr = 0;\n\n\tp->cq_off.head = offsetof(struct io_rings, cq.head);\n\tp->cq_off.tail = offsetof(struct io_rings, cq.tail);\n\tp->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);\n\tp->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);\n\tp->cq_off.overflow = offsetof(struct io_rings, cq_overflow);\n\tp->cq_off.cqes = offsetof(struct io_rings, cqes);\n\tp->cq_off.flags = offsetof(struct io_rings, cq_flags);\n\tp->cq_off.resv1 = 0;\n\tif (!(ctx->flags & IORING_SETUP_NO_MMAP))\n\t\tp->cq_off.user_addr = 0;\n\n\tp->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |\n\t\t\tIORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |\n\t\t\tIORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |\n\t\t\tIORING_FEAT_POLL_32BITS | IORING_FEAT_SQPOLL_NONFIXED |\n\t\t\tIORING_FEAT_EXT_ARG | IORING_FEAT_NATIVE_WORKERS |\n\t\t\tIORING_FEAT_RSRC_TAGS | IORING_FEAT_CQE_SKIP |\n\t\t\tIORING_FEAT_LINKED_FILE | IORING_FEAT_REG_REG_RING;\n\n\tif (copy_to_user(params, p, sizeof(*p))) {\n\t\tret = -EFAULT;\n\t\tgoto err;\n\t}\n\n\tif (ctx->flags & IORING_SETUP_SINGLE_ISSUER\n\t    && !(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\tWRITE_ONCE(ctx->submitter_task, get_task_struct(current));\n\n\tfile = io_uring_get_file(ctx);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto err;\n\t}\n\n\tret = __io_uring_add_tctx_node(ctx);\n\tif (ret)\n\t\tgoto err_fput;\n\ttctx = current->io_uring;\n\n\t \n\tif (p->flags & IORING_SETUP_REGISTERED_FD_ONLY)\n\t\tret = io_ring_add_registered_file(tctx, file, 0, IO_RINGFD_REG_MAX);\n\telse\n\t\tret = io_uring_install_fd(file);\n\tif (ret < 0)\n\t\tgoto err_fput;\n\n\ttrace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);\n\treturn ret;\nerr:\n\tio_ring_ctx_wait_and_kill(ctx);\n\treturn ret;\nerr_fput:\n\tfput(file);\n\treturn ret;\n}\n\n \nstatic long io_uring_setup(u32 entries, struct io_uring_params __user *params)\n{\n\tstruct io_uring_params p;\n\tint i;\n\n\tif (copy_from_user(&p, params, sizeof(p)))\n\t\treturn -EFAULT;\n\tfor (i = 0; i < ARRAY_SIZE(p.resv); i++) {\n\t\tif (p.resv[i])\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |\n\t\t\tIORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |\n\t\t\tIORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |\n\t\t\tIORING_SETUP_R_DISABLED | IORING_SETUP_SUBMIT_ALL |\n\t\t\tIORING_SETUP_COOP_TASKRUN | IORING_SETUP_TASKRUN_FLAG |\n\t\t\tIORING_SETUP_SQE128 | IORING_SETUP_CQE32 |\n\t\t\tIORING_SETUP_SINGLE_ISSUER | IORING_SETUP_DEFER_TASKRUN |\n\t\t\tIORING_SETUP_NO_MMAP | IORING_SETUP_REGISTERED_FD_ONLY |\n\t\t\tIORING_SETUP_NO_SQARRAY))\n\t\treturn -EINVAL;\n\n\treturn io_uring_create(entries, &p, params);\n}\n\nstatic inline bool io_uring_allowed(void)\n{\n\tint disabled = READ_ONCE(sysctl_io_uring_disabled);\n\tkgid_t io_uring_group;\n\n\tif (disabled == 2)\n\t\treturn false;\n\n\tif (disabled == 0 || capable(CAP_SYS_ADMIN))\n\t\treturn true;\n\n\tio_uring_group = make_kgid(&init_user_ns, sysctl_io_uring_group);\n\tif (!gid_valid(io_uring_group))\n\t\treturn false;\n\n\treturn in_group_p(io_uring_group);\n}\n\nSYSCALL_DEFINE2(io_uring_setup, u32, entries,\n\t\tstruct io_uring_params __user *, params)\n{\n\tif (!io_uring_allowed())\n\t\treturn -EPERM;\n\n\treturn io_uring_setup(entries, params);\n}\n\nstatic __cold int io_probe(struct io_ring_ctx *ctx, void __user *arg,\n\t\t\t   unsigned nr_args)\n{\n\tstruct io_uring_probe *p;\n\tsize_t size;\n\tint i, ret;\n\n\tsize = struct_size(p, ops, nr_args);\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\tp = kzalloc(size, GFP_KERNEL);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tret = -EFAULT;\n\tif (copy_from_user(p, arg, size))\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (memchr_inv(p, 0, size))\n\t\tgoto out;\n\n\tp->last_op = IORING_OP_LAST - 1;\n\tif (nr_args > IORING_OP_LAST)\n\t\tnr_args = IORING_OP_LAST;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tp->ops[i].op = i;\n\t\tif (!io_issue_defs[i].not_supported)\n\t\t\tp->ops[i].flags = IO_URING_OP_SUPPORTED;\n\t}\n\tp->ops_len = i;\n\n\tret = 0;\n\tif (copy_to_user(arg, p, size))\n\t\tret = -EFAULT;\nout:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int io_register_personality(struct io_ring_ctx *ctx)\n{\n\tconst struct cred *creds;\n\tu32 id;\n\tint ret;\n\n\tcreds = get_current_cred();\n\n\tret = xa_alloc_cyclic(&ctx->personalities, &id, (void *)creds,\n\t\t\tXA_LIMIT(0, USHRT_MAX), &ctx->pers_next, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_cred(creds);\n\t\treturn ret;\n\t}\n\treturn id;\n}\n\nstatic __cold int io_register_restrictions(struct io_ring_ctx *ctx,\n\t\t\t\t\t   void __user *arg, unsigned int nr_args)\n{\n\tstruct io_uring_restriction *res;\n\tsize_t size;\n\tint i, ret;\n\n\t \n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\t \n\tif (ctx->restrictions.registered)\n\t\treturn -EBUSY;\n\n\tif (!arg || nr_args > IORING_MAX_RESTRICTIONS)\n\t\treturn -EINVAL;\n\n\tsize = array_size(nr_args, sizeof(*res));\n\tif (size == SIZE_MAX)\n\t\treturn -EOVERFLOW;\n\n\tres = memdup_user(arg, size);\n\tif (IS_ERR(res))\n\t\treturn PTR_ERR(res);\n\n\tret = 0;\n\n\tfor (i = 0; i < nr_args; i++) {\n\t\tswitch (res[i].opcode) {\n\t\tcase IORING_RESTRICTION_REGISTER_OP:\n\t\t\tif (res[i].register_op >= IORING_REGISTER_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].register_op,\n\t\t\t\t  ctx->restrictions.register_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_OP:\n\t\t\tif (res[i].sqe_op >= IORING_OP_LAST) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t__set_bit(res[i].sqe_op, ctx->restrictions.sqe_op);\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_ALLOWED:\n\t\t\tctx->restrictions.sqe_flags_allowed = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tcase IORING_RESTRICTION_SQE_FLAGS_REQUIRED:\n\t\t\tctx->restrictions.sqe_flags_required = res[i].sqe_flags;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t \n\tif (ret != 0)\n\t\tmemset(&ctx->restrictions, 0, sizeof(ctx->restrictions));\n\telse\n\t\tctx->restrictions.registered = true;\n\n\tkfree(res);\n\treturn ret;\n}\n\nstatic int io_register_enable_rings(struct io_ring_ctx *ctx)\n{\n\tif (!(ctx->flags & IORING_SETUP_R_DISABLED))\n\t\treturn -EBADFD;\n\n\tif (ctx->flags & IORING_SETUP_SINGLE_ISSUER && !ctx->submitter_task) {\n\t\tWRITE_ONCE(ctx->submitter_task, get_task_struct(current));\n\t\t \n\t\tif (wq_has_sleeper(&ctx->poll_wq))\n\t\t\tio_activate_pollwq(ctx);\n\t}\n\n\tif (ctx->restrictions.registered)\n\t\tctx->restricted = 1;\n\n\tctx->flags &= ~IORING_SETUP_R_DISABLED;\n\tif (ctx->sq_data && wq_has_sleeper(&ctx->sq_data->wait))\n\t\twake_up(&ctx->sq_data->wait);\n\treturn 0;\n}\n\nstatic __cold int __io_register_iowq_aff(struct io_ring_ctx *ctx,\n\t\t\t\t\t cpumask_var_t new_mask)\n{\n\tint ret;\n\n\tif (!(ctx->flags & IORING_SETUP_SQPOLL)) {\n\t\tret = io_wq_cpu_affinity(current->io_uring, new_mask);\n\t} else {\n\t\tmutex_unlock(&ctx->uring_lock);\n\t\tret = io_sqpoll_wq_cpu_affinity(ctx, new_mask);\n\t\tmutex_lock(&ctx->uring_lock);\n\t}\n\n\treturn ret;\n}\n\nstatic __cold int io_register_iowq_aff(struct io_ring_ctx *ctx,\n\t\t\t\t       void __user *arg, unsigned len)\n{\n\tcpumask_var_t new_mask;\n\tint ret;\n\n\tif (!alloc_cpumask_var(&new_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tcpumask_clear(new_mask);\n\tif (len > cpumask_size())\n\t\tlen = cpumask_size();\n\n\tif (in_compat_syscall()) {\n\t\tret = compat_get_bitmap(cpumask_bits(new_mask),\n\t\t\t\t\t(const compat_ulong_t __user *)arg,\n\t\t\t\t\tlen * 8  );\n\t} else {\n\t\tret = copy_from_user(new_mask, arg, len);\n\t}\n\n\tif (ret) {\n\t\tfree_cpumask_var(new_mask);\n\t\treturn -EFAULT;\n\t}\n\n\tret = __io_register_iowq_aff(ctx, new_mask);\n\tfree_cpumask_var(new_mask);\n\treturn ret;\n}\n\nstatic __cold int io_unregister_iowq_aff(struct io_ring_ctx *ctx)\n{\n\treturn __io_register_iowq_aff(ctx, NULL);\n}\n\nstatic __cold int io_register_iowq_max_workers(struct io_ring_ctx *ctx,\n\t\t\t\t\t       void __user *arg)\n\t__must_hold(&ctx->uring_lock)\n{\n\tstruct io_tctx_node *node;\n\tstruct io_uring_task *tctx = NULL;\n\tstruct io_sq_data *sqd = NULL;\n\t__u32 new_count[2];\n\tint i, ret;\n\n\tif (copy_from_user(new_count, arg, sizeof(new_count)))\n\t\treturn -EFAULT;\n\tfor (i = 0; i < ARRAY_SIZE(new_count); i++)\n\t\tif (new_count[i] > INT_MAX)\n\t\t\treturn -EINVAL;\n\n\tif (ctx->flags & IORING_SETUP_SQPOLL) {\n\t\tsqd = ctx->sq_data;\n\t\tif (sqd) {\n\t\t\t \n\t\t\trefcount_inc(&sqd->refs);\n\t\t\tmutex_unlock(&ctx->uring_lock);\n\t\t\tmutex_lock(&sqd->lock);\n\t\t\tmutex_lock(&ctx->uring_lock);\n\t\t\tif (sqd->thread)\n\t\t\t\ttctx = sqd->thread->io_uring;\n\t\t}\n\t} else {\n\t\ttctx = current->io_uring;\n\t}\n\n\tBUILD_BUG_ON(sizeof(new_count) != sizeof(ctx->iowq_limits));\n\n\tfor (i = 0; i < ARRAY_SIZE(new_count); i++)\n\t\tif (new_count[i])\n\t\t\tctx->iowq_limits[i] = new_count[i];\n\tctx->iowq_limits_set = true;\n\n\tif (tctx && tctx->io_wq) {\n\t\tret = io_wq_max_workers(tctx->io_wq, new_count);\n\t\tif (ret)\n\t\t\tgoto err;\n\t} else {\n\t\tmemset(new_count, 0, sizeof(new_count));\n\t}\n\n\tif (sqd) {\n\t\tmutex_unlock(&sqd->lock);\n\t\tio_put_sq_data(sqd);\n\t}\n\n\tif (copy_to_user(arg, new_count, sizeof(new_count)))\n\t\treturn -EFAULT;\n\n\t \n\tif (sqd)\n\t\treturn 0;\n\n\t \n\tlist_for_each_entry(node, &ctx->tctx_list, ctx_node) {\n\t\tstruct io_uring_task *tctx = node->task->io_uring;\n\n\t\tif (WARN_ON_ONCE(!tctx->io_wq))\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(new_count); i++)\n\t\t\tnew_count[i] = ctx->iowq_limits[i];\n\t\t \n\t\t(void)io_wq_max_workers(tctx->io_wq, new_count);\n\t}\n\treturn 0;\nerr:\n\tif (sqd) {\n\t\tmutex_unlock(&sqd->lock);\n\t\tio_put_sq_data(sqd);\n\t}\n\treturn ret;\n}\n\nstatic int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,\n\t\t\t       void __user *arg, unsigned nr_args)\n\t__releases(ctx->uring_lock)\n\t__acquires(ctx->uring_lock)\n{\n\tint ret;\n\n\t \n\tif (WARN_ON_ONCE(percpu_ref_is_dying(&ctx->refs)))\n\t\treturn -ENXIO;\n\n\tif (ctx->submitter_task && ctx->submitter_task != current)\n\t\treturn -EEXIST;\n\n\tif (ctx->restricted) {\n\t\topcode = array_index_nospec(opcode, IORING_REGISTER_LAST);\n\t\tif (!test_bit(opcode, ctx->restrictions.register_op))\n\t\t\treturn -EACCES;\n\t}\n\n\tswitch (opcode) {\n\tcase IORING_REGISTER_BUFFERS:\n\t\tret = -EFAULT;\n\t\tif (!arg)\n\t\t\tbreak;\n\t\tret = io_sqe_buffers_register(ctx, arg, nr_args, NULL);\n\t\tbreak;\n\tcase IORING_UNREGISTER_BUFFERS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_buffers_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES:\n\t\tret = -EFAULT;\n\t\tif (!arg)\n\t\t\tbreak;\n\t\tret = io_sqe_files_register(ctx, arg, nr_args, NULL);\n\t\tbreak;\n\tcase IORING_UNREGISTER_FILES:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_sqe_files_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES_UPDATE:\n\t\tret = io_register_files_update(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_EVENTFD:\n\t\tret = -EINVAL;\n\t\tif (nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_eventfd_register(ctx, arg, 0);\n\t\tbreak;\n\tcase IORING_REGISTER_EVENTFD_ASYNC:\n\t\tret = -EINVAL;\n\t\tif (nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_eventfd_register(ctx, arg, 1);\n\t\tbreak;\n\tcase IORING_UNREGISTER_EVENTFD:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_eventfd_unregister(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_PROBE:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args > 256)\n\t\t\tbreak;\n\t\tret = io_probe(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_personality(ctx);\n\t\tbreak;\n\tcase IORING_UNREGISTER_PERSONALITY:\n\t\tret = -EINVAL;\n\t\tif (arg)\n\t\t\tbreak;\n\t\tret = io_unregister_personality(ctx, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_ENABLE_RINGS:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_enable_rings(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_RESTRICTIONS:\n\t\tret = io_register_restrictions(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES2:\n\t\tret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_FILE);\n\t\tbreak;\n\tcase IORING_REGISTER_FILES_UPDATE2:\n\t\tret = io_register_rsrc_update(ctx, arg, nr_args,\n\t\t\t\t\t      IORING_RSRC_FILE);\n\t\tbreak;\n\tcase IORING_REGISTER_BUFFERS2:\n\t\tret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_BUFFER);\n\t\tbreak;\n\tcase IORING_REGISTER_BUFFERS_UPDATE:\n\t\tret = io_register_rsrc_update(ctx, arg, nr_args,\n\t\t\t\t\t      IORING_RSRC_BUFFER);\n\t\tbreak;\n\tcase IORING_REGISTER_IOWQ_AFF:\n\t\tret = -EINVAL;\n\t\tif (!arg || !nr_args)\n\t\t\tbreak;\n\t\tret = io_register_iowq_aff(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_IOWQ_AFF:\n\t\tret = -EINVAL;\n\t\tif (arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_unregister_iowq_aff(ctx);\n\t\tbreak;\n\tcase IORING_REGISTER_IOWQ_MAX_WORKERS:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args != 2)\n\t\t\tbreak;\n\t\tret = io_register_iowq_max_workers(ctx, arg);\n\t\tbreak;\n\tcase IORING_REGISTER_RING_FDS:\n\t\tret = io_ringfd_register(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_UNREGISTER_RING_FDS:\n\t\tret = io_ringfd_unregister(ctx, arg, nr_args);\n\t\tbreak;\n\tcase IORING_REGISTER_PBUF_RING:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_register_pbuf_ring(ctx, arg);\n\t\tbreak;\n\tcase IORING_UNREGISTER_PBUF_RING:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_unregister_pbuf_ring(ctx, arg);\n\t\tbreak;\n\tcase IORING_REGISTER_SYNC_CANCEL:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args != 1)\n\t\t\tbreak;\n\t\tret = io_sync_cancel(ctx, arg);\n\t\tbreak;\n\tcase IORING_REGISTER_FILE_ALLOC_RANGE:\n\t\tret = -EINVAL;\n\t\tif (!arg || nr_args)\n\t\t\tbreak;\n\t\tret = io_register_file_alloc_range(ctx, arg);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nSYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,\n\t\tvoid __user *, arg, unsigned int, nr_args)\n{\n\tstruct io_ring_ctx *ctx;\n\tlong ret = -EBADF;\n\tstruct file *file;\n\tbool use_registered_ring;\n\n\tuse_registered_ring = !!(opcode & IORING_REGISTER_USE_REGISTERED_RING);\n\topcode &= ~IORING_REGISTER_USE_REGISTERED_RING;\n\n\tif (opcode >= IORING_REGISTER_LAST)\n\t\treturn -EINVAL;\n\n\tif (use_registered_ring) {\n\t\t \n\t\tstruct io_uring_task *tctx = current->io_uring;\n\n\t\tif (unlikely(!tctx || fd >= IO_RINGFD_REG_MAX))\n\t\t\treturn -EINVAL;\n\t\tfd = array_index_nospec(fd, IO_RINGFD_REG_MAX);\n\t\tfile = tctx->registered_rings[fd];\n\t\tif (unlikely(!file))\n\t\t\treturn -EBADF;\n\t} else {\n\t\tfile = fget(fd);\n\t\tif (unlikely(!file))\n\t\t\treturn -EBADF;\n\t\tret = -EOPNOTSUPP;\n\t\tif (!io_is_uring_fops(file))\n\t\t\tgoto out_fput;\n\t}\n\n\tctx = file->private_data;\n\n\tmutex_lock(&ctx->uring_lock);\n\tret = __io_uring_register(ctx, opcode, arg, nr_args);\n\tmutex_unlock(&ctx->uring_lock);\n\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs, ret);\nout_fput:\n\tif (!use_registered_ring)\n\t\tfput(file);\n\treturn ret;\n}\n\nstatic int __init io_uring_init(void)\n{\n#define __BUILD_BUG_VERIFY_OFFSET_SIZE(stype, eoffset, esize, ename) do { \\\n\tBUILD_BUG_ON(offsetof(stype, ename) != eoffset); \\\n\tBUILD_BUG_ON(sizeof_field(stype, ename) != esize); \\\n} while (0)\n\n#define BUILD_BUG_SQE_ELEM(eoffset, etype, ename) \\\n\t__BUILD_BUG_VERIFY_OFFSET_SIZE(struct io_uring_sqe, eoffset, sizeof(etype), ename)\n#define BUILD_BUG_SQE_ELEM_SIZE(eoffset, esize, ename) \\\n\t__BUILD_BUG_VERIFY_OFFSET_SIZE(struct io_uring_sqe, eoffset, esize, ename)\n\tBUILD_BUG_ON(sizeof(struct io_uring_sqe) != 64);\n\tBUILD_BUG_SQE_ELEM(0,  __u8,   opcode);\n\tBUILD_BUG_SQE_ELEM(1,  __u8,   flags);\n\tBUILD_BUG_SQE_ELEM(2,  __u16,  ioprio);\n\tBUILD_BUG_SQE_ELEM(4,  __s32,  fd);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  off);\n\tBUILD_BUG_SQE_ELEM(8,  __u64,  addr2);\n\tBUILD_BUG_SQE_ELEM(8,  __u32,  cmd_op);\n\tBUILD_BUG_SQE_ELEM(12, __u32, __pad1);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  addr);\n\tBUILD_BUG_SQE_ELEM(16, __u64,  splice_off_in);\n\tBUILD_BUG_SQE_ELEM(24, __u32,  len);\n\tBUILD_BUG_SQE_ELEM(28,     __kernel_rwf_t, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28,     int, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28,   __u32, rw_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fsync_flags);\n\tBUILD_BUG_SQE_ELEM(28,   __u16,  poll_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  poll32_events);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  sync_range_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  msg_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  timeout_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  accept_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  cancel_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  open_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  statx_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  fadvise_advice);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  splice_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  rename_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  unlink_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  hardlink_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  xattr_flags);\n\tBUILD_BUG_SQE_ELEM(28, __u32,  msg_ring_flags);\n\tBUILD_BUG_SQE_ELEM(32, __u64,  user_data);\n\tBUILD_BUG_SQE_ELEM(40, __u16,  buf_index);\n\tBUILD_BUG_SQE_ELEM(40, __u16,  buf_group);\n\tBUILD_BUG_SQE_ELEM(42, __u16,  personality);\n\tBUILD_BUG_SQE_ELEM(44, __s32,  splice_fd_in);\n\tBUILD_BUG_SQE_ELEM(44, __u32,  file_index);\n\tBUILD_BUG_SQE_ELEM(44, __u16,  addr_len);\n\tBUILD_BUG_SQE_ELEM(46, __u16,  __pad3[0]);\n\tBUILD_BUG_SQE_ELEM(48, __u64,  addr3);\n\tBUILD_BUG_SQE_ELEM_SIZE(48, 0, cmd);\n\tBUILD_BUG_SQE_ELEM(56, __u64,  __pad2);\n\n\tBUILD_BUG_ON(sizeof(struct io_uring_files_update) !=\n\t\t     sizeof(struct io_uring_rsrc_update));\n\tBUILD_BUG_ON(sizeof(struct io_uring_rsrc_update) >\n\t\t     sizeof(struct io_uring_rsrc_update2));\n\n\t \n\tBUILD_BUG_ON(offsetof(struct io_uring_buf_ring, bufs) != 0);\n\tBUILD_BUG_ON(offsetof(struct io_uring_buf, resv) !=\n\t\t     offsetof(struct io_uring_buf_ring, tail));\n\n\t \n\tBUILD_BUG_ON(SQE_VALID_FLAGS >= (1 << 8));\n\tBUILD_BUG_ON(SQE_COMMON_FLAGS >= (1 << 8));\n\tBUILD_BUG_ON((SQE_VALID_FLAGS | SQE_COMMON_FLAGS) != SQE_VALID_FLAGS);\n\n\tBUILD_BUG_ON(__REQ_F_LAST_BIT > 8 * sizeof(int));\n\n\tBUILD_BUG_ON(sizeof(atomic_t) != sizeof(u32));\n\n\tio_uring_optable_init();\n\n\t \n\treq_cachep = kmem_cache_create_usercopy(\"io_kiocb\",\n\t\t\t\tsizeof(struct io_kiocb), 0,\n\t\t\t\tSLAB_HWCACHE_ALIGN | SLAB_PANIC |\n\t\t\t\tSLAB_ACCOUNT | SLAB_TYPESAFE_BY_RCU,\n\t\t\t\toffsetof(struct io_kiocb, cmd.data),\n\t\t\t\tsizeof_field(struct io_kiocb, cmd.data), NULL);\n\n#ifdef CONFIG_SYSCTL\n\tregister_sysctl_init(\"kernel\", kernel_io_uring_disabled_table);\n#endif\n\n\treturn 0;\n};\n__initcall(io_uring_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}