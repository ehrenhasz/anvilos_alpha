{
  "module_name": "topdown.txt",
  "hash_id": "3e1a8ead48a29c07076c8129f037e2226c7f1a1ded6bd396b238816f94364062",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/Documentation/topdown.txt",
  "human_readable_source": "Using TopDown metrics\n---------------------\n\nTopDown metrics break apart performance bottlenecks. Starting at level\n1 it is typical to get metrics on retiring, bad speculation, frontend\nbound, and backend bound. Higher levels provide more detail in to the\nlevel 1 bottlenecks, such as at level 2: core bound, memory bound,\nheavy operations, light operations, branch mispredicts, machine\nclears, fetch latency and fetch bandwidth. For more details see [1][2][3].\n\nperf stat --topdown implements this using available metrics that vary\nper architecture.\n\n% perf stat -a --topdown -I1000\n#           time      %  tma_retiring %  tma_backend_bound %  tma_frontend_bound %  tma_bad_speculation\n     1.001141351                 11.5                 34.9                  46.9                    6.7\n     2.006141972                 13.4                 28.1                  50.4                    8.1\n     3.010162040                 12.9                 28.1                  51.1                    8.0\n     4.014009311                 12.5                 28.6                  51.8                    7.2\n     5.017838554                 11.8                 33.0                  48.0                    7.2\n     5.704818971                 14.0                 27.5                  51.3                    7.3\n...\n\nNew Topdown features in Intel Ice Lake\n======================================\n\nWith Ice Lake CPUs the TopDown metrics are directly available as\nfixed counters and do not require generic counters. This allows\nto collect TopDown always in addition to other events.\n\nUsing TopDown through RDPMC in applications on Intel Ice Lake\n=============================================================\n\nFor more fine grained measurements it can be useful to\naccess the new  directly from user space. This is more complicated,\nbut drastically lowers overhead.\n\nOn Ice Lake, there is a new fixed counter 3: SLOTS, which reports\n\"pipeline SLOTS\" (cycles multiplied by core issue width) and a\nmetric register that reports slots ratios for the different bottleneck\ncategories.\n\nThe metrics counter is CPU model specific and is not available on older\nCPUs.\n\nExample code\n============\n\nLibrary functions to do the functionality described below\nis also available in libjevents [4]\n\nThe application opens a group with fixed counter 3 (SLOTS) and any\nmetric event, and allow user programs to read the performance counters.\n\nFixed counter 3 is mapped to a pseudo event event=0x00, umask=04,\nso the perf_event_attr structure should be initialized with\n{ .config = 0x0400, .type = PERF_TYPE_RAW }\nThe metric events are mapped to the pseudo event event=0x00, umask=0x8X.\nFor example, the perf_event_attr structure can be initialized with\n{ .config = 0x8000, .type = PERF_TYPE_RAW } for Retiring metric event\nThe Fixed counter 3 must be the leader of the group.\n\n#include <linux/perf_event.h>\n#include <sys/mman.h>\n#include <sys/syscall.h>\n#include <unistd.h>\n\n/* Provide own perf_event_open stub because glibc doesn't */\n__attribute__((weak))\nint perf_event_open(struct perf_event_attr *attr, pid_t pid,\n\t\t    int cpu, int group_fd, unsigned long flags)\n{\n\treturn syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags);\n}\n\n/* Open slots counter file descriptor for current task. */\nstruct perf_event_attr slots = {\n\t.type = PERF_TYPE_RAW,\n\t.size = sizeof(struct perf_event_attr),\n\t.config = 0x400,\n\t.exclude_kernel = 1,\n};\n\nint slots_fd = perf_event_open(&slots, 0, -1, -1, 0);\nif (slots_fd < 0)\n\t... error ...\n\n/* Memory mapping the fd permits _rdpmc calls from userspace */\nvoid *slots_p = mmap(0, getpagesize(), PROT_READ, MAP_SHARED, slots_fd, 0);\nif (!slot_p)\n\t.... error ...\n\n/*\n * Open metrics event file descriptor for current task.\n * Set slots event as the leader of the group.\n */\nstruct perf_event_attr metrics = {\n\t.type = PERF_TYPE_RAW,\n\t.size = sizeof(struct perf_event_attr),\n\t.config = 0x8000,\n\t.exclude_kernel = 1,\n};\n\nint metrics_fd = perf_event_open(&metrics, 0, -1, slots_fd, 0);\nif (metrics_fd < 0)\n\t... error ...\n\n/* Memory mapping the fd permits _rdpmc calls from userspace */\nvoid *metrics_p = mmap(0, getpagesize(), PROT_READ, MAP_SHARED, metrics_fd, 0);\nif (!metrics_p)\n\t... error ...\n\nNote: the file descriptors returned by the perf_event_open calls must be memory\nmapped to permit calls to the _rdpmd instruction. Permission may also be granted\nby writing the /sys/devices/cpu/rdpmc sysfs node.\n\nThe RDPMC instruction (or _rdpmc compiler intrinsic) can now be used\nto read slots and the topdown metrics at different points of the program:\n\n#include <stdint.h>\n#include <x86intrin.h>\n\n#define RDPMC_FIXED\t(1 << 30)\t/* return fixed counters */\n#define RDPMC_METRIC\t(1 << 29)\t/* return metric counters */\n\n#define FIXED_COUNTER_SLOTS\t\t3\n#define METRIC_COUNTER_TOPDOWN_L1_L2\t0\n\nstatic inline uint64_t read_slots(void)\n{\n\treturn _rdpmc(RDPMC_FIXED | FIXED_COUNTER_SLOTS);\n}\n\nstatic inline uint64_t read_metrics(void)\n{\n\treturn _rdpmc(RDPMC_METRIC | METRIC_COUNTER_TOPDOWN_L1_L2);\n}\n\nThen the program can be instrumented to read these metrics at different\npoints.\n\nIt's not a good idea to do this with too short code regions,\nas the parallelism and overlap in the CPU program execution will\ncause too much measurement inaccuracy. For example instrumenting\nindividual basic blocks is definitely too fine grained.\n\n_rdpmc calls should not be mixed with reading the metrics and slots counters\nthrough system calls, as the kernel will reset these counters after each system\ncall.\n\nDecoding metrics values\n=======================\n\nThe value reported by read_metrics() contains four 8 bit fields\nthat represent a scaled ratio that represent the Level 1 bottleneck.\nAll four fields add up to 0xff (= 100%)\n\nThe binary ratios in the metric value can be converted to float ratios:\n\n#define GET_METRIC(m, i) (((m) >> (i*8)) & 0xff)\n\n/* L1 Topdown metric events */\n#define TOPDOWN_RETIRING(val)\t((float)GET_METRIC(val, 0) / 0xff)\n#define TOPDOWN_BAD_SPEC(val)\t((float)GET_METRIC(val, 1) / 0xff)\n#define TOPDOWN_FE_BOUND(val)\t((float)GET_METRIC(val, 2) / 0xff)\n#define TOPDOWN_BE_BOUND(val)\t((float)GET_METRIC(val, 3) / 0xff)\n\n/*\n * L2 Topdown metric events.\n * Available on Sapphire Rapids and later platforms.\n */\n#define TOPDOWN_HEAVY_OPS(val)\t\t((float)GET_METRIC(val, 4) / 0xff)\n#define TOPDOWN_BR_MISPREDICT(val)\t((float)GET_METRIC(val, 5) / 0xff)\n#define TOPDOWN_FETCH_LAT(val)\t\t((float)GET_METRIC(val, 6) / 0xff)\n#define TOPDOWN_MEM_BOUND(val)\t\t((float)GET_METRIC(val, 7) / 0xff)\n\nand then converted to percent for printing.\n\nThe ratios in the metric accumulate for the time when the counter\nis enabled. For measuring programs it is often useful to measure\nspecific sections. For this it is needed to deltas on metrics.\n\nThis can be done by scaling the metrics with the slots counter\nread at the same time.\n\nThen it's possible to take deltas of these slots counts\nmeasured at different points, and determine the metrics\nfor that time period.\n\n\tslots_a = read_slots();\n\tmetric_a = read_metrics();\n\n\t... larger code region ...\n\n\tslots_b = read_slots()\n\tmetric_b = read_metrics()\n\n\t# compute scaled metrics for measurement a\n\tretiring_slots_a = GET_METRIC(metric_a, 0) * slots_a\n\tbad_spec_slots_a = GET_METRIC(metric_a, 1) * slots_a\n\tfe_bound_slots_a = GET_METRIC(metric_a, 2) * slots_a\n\tbe_bound_slots_a = GET_METRIC(metric_a, 3) * slots_a\n\n\t# compute delta scaled metrics between b and a\n\tretiring_slots = GET_METRIC(metric_b, 0) * slots_b - retiring_slots_a\n\tbad_spec_slots = GET_METRIC(metric_b, 1) * slots_b - bad_spec_slots_a\n\tfe_bound_slots = GET_METRIC(metric_b, 2) * slots_b - fe_bound_slots_a\n\tbe_bound_slots = GET_METRIC(metric_b, 3) * slots_b - be_bound_slots_a\n\nLater the individual ratios of L1 metric events for the measurement period can\nbe recreated from these counts.\n\n\tslots_delta = slots_b - slots_a\n\tretiring_ratio = (float)retiring_slots / slots_delta\n\tbad_spec_ratio = (float)bad_spec_slots / slots_delta\n\tfe_bound_ratio = (float)fe_bound_slots / slots_delta\n\tbe_bound_ratio = (float)be_bound_slots / slota_delta\n\n\tprintf(\"Retiring %.2f%% Bad Speculation %.2f%% FE Bound %.2f%% BE Bound %.2f%%\\n\",\n\t\tretiring_ratio * 100.,\n\t\tbad_spec_ratio * 100.,\n\t\tfe_bound_ratio * 100.,\n\t\tbe_bound_ratio * 100.);\n\nThe individual ratios of L2 metric events for the measurement period can be\nrecreated from L1 and L2 metric counters. (Available on Sapphire Rapids and\nlater platforms)\n\n\t# compute scaled metrics for measurement a\n\theavy_ops_slots_a = GET_METRIC(metric_a, 4) * slots_a\n\tbr_mispredict_slots_a = GET_METRIC(metric_a, 5) * slots_a\n\tfetch_lat_slots_a = GET_METRIC(metric_a, 6) * slots_a\n\tmem_bound_slots_a = GET_METRIC(metric_a, 7) * slots_a\n\n\t# compute delta scaled metrics between b and a\n\theavy_ops_slots = GET_METRIC(metric_b, 4) * slots_b - heavy_ops_slots_a\n\tbr_mispredict_slots = GET_METRIC(metric_b, 5) * slots_b - br_mispredict_slots_a\n\tfetch_lat_slots = GET_METRIC(metric_b, 6) * slots_b - fetch_lat_slots_a\n\tmem_bound_slots = GET_METRIC(metric_b, 7) * slots_b - mem_bound_slots_a\n\n\tslots_delta = slots_b - slots_a\n\theavy_ops_ratio = (float)heavy_ops_slots / slots_delta\n\tlight_ops_ratio = retiring_ratio - heavy_ops_ratio;\n\n\tbr_mispredict_ratio = (float)br_mispredict_slots / slots_delta\n\tmachine_clears_ratio = bad_spec_ratio - br_mispredict_ratio;\n\n\tfetch_lat_ratio = (float)fetch_lat_slots / slots_delta\n\tfetch_bw_ratio = fe_bound_ratio - fetch_lat_ratio;\n\n\tmem_bound_ratio = (float)mem_bound_slots / slota_delta\n\tcore_bound_ratio = be_bound_ratio - mem_bound_ratio;\n\n\tprintf(\"Heavy Operations %.2f%% Light Operations %.2f%% \"\n\t       \"Branch Mispredict %.2f%% Machine Clears %.2f%% \"\n\t       \"Fetch Latency %.2f%% Fetch Bandwidth %.2f%% \"\n\t       \"Mem Bound %.2f%% Core Bound %.2f%%\\n\",\n\t\theavy_ops_ratio * 100.,\n\t\tlight_ops_ratio * 100.,\n\t\tbr_mispredict_ratio * 100.,\n\t\tmachine_clears_ratio * 100.,\n\t\tfetch_lat_ratio * 100.,\n\t\tfetch_bw_ratio * 100.,\n\t\tmem_bound_ratio * 100.,\n\t\tcore_bound_ratio * 100.);\n\nResetting metrics counters\n==========================\n\nSince the individual metrics are only 8bit they lose precision for\nshort regions over time because the number of cycles covered by each\nfraction bit shrinks. So the counters need to be reset regularly.\n\nWhen using the kernel perf API the kernel resets on every read.\nSo as long as the reading is at reasonable intervals (every few\nseconds) the precision is good.\n\nWhen using perf stat it is recommended to always use the -I option,\nwith no longer interval than a few seconds\n\n\tperf stat -I 1000 --topdown ...\n\nFor user programs using RDPMC directly the counter can\nbe reset explicitly using ioctl:\n\n\tioctl(perf_fd, PERF_EVENT_IOC_RESET, 0);\n\nThis \"opens\" a new measurement period.\n\nA program using RDPMC for TopDown should schedule such a reset\nregularly, as in every few seconds.\n\nLimits on Intel Ice Lake\n========================\n\nFour pseudo TopDown metric events are exposed for the end-users,\ntopdown-retiring, topdown-bad-spec, topdown-fe-bound and topdown-be-bound.\nThey can be used to collect the TopDown value under the following\nrules:\n- All the TopDown metric events must be in a group with the SLOTS event.\n- The SLOTS event must be the leader of the group.\n- The PERF_FORMAT_GROUP flag must be applied for each TopDown metric\n  events\n\nThe SLOTS event and the TopDown metric events can be counting members of\na sampling read group. Since the SLOTS event must be the leader of a TopDown\ngroup, the second event of the group is the sampling event.\nFor example, perf record -e '{slots, $sampling_event, topdown-retiring}:S'\n\nExtension on Intel Sapphire Rapids Server\n=========================================\nThe metrics counter is extended to support TMA method level 2 metrics.\nThe lower half of the register is the TMA level 1 metrics (legacy).\nThe upper half is also divided into four 8-bit fields for the new level 2\nmetrics. Four more TopDown metric events are exposed for the end-users,\ntopdown-heavy-ops, topdown-br-mispredict, topdown-fetch-lat and\ntopdown-mem-bound.\n\nEach of the new level 2 metrics in the upper half is a subset of the\ncorresponding level 1 metric in the lower half. Software can deduce the\nother four level 2 metrics by subtracting corresponding metrics as below.\n\n    Light_Operations = Retiring - Heavy_Operations\n    Machine_Clears = Bad_Speculation - Branch_Mispredicts\n    Fetch_Bandwidth = Frontend_Bound - Fetch_Latency\n    Core_Bound = Backend_Bound - Memory_Bound\n\n\n[1] https://software.intel.com/en-us/top-down-microarchitecture-analysis-method-win\n[2] https://sites.google.com/site/analysismethods/yasin-pubs\n[3] https://perf.wiki.kernel.org/index.php/Top-Down_Analysis\n[4] https://github.com/andikleen/pmu-tools/tree/master/jevents\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}