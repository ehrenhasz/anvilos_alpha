{
  "module_name": "builtin-record.c",
  "hash_id": "ec2faa00a95b4f373ba6cfe4cdfca004247759d2ad8cc614787d37ac518bb28a",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/builtin-record.c",
  "human_readable_source": "\n \n#include \"builtin.h\"\n\n#include \"util/build-id.h\"\n#include <subcmd/parse-options.h>\n#include <internal/xyarray.h>\n#include \"util/parse-events.h\"\n#include \"util/config.h\"\n\n#include \"util/callchain.h\"\n#include \"util/cgroup.h\"\n#include \"util/header.h\"\n#include \"util/event.h\"\n#include \"util/evlist.h\"\n#include \"util/evsel.h\"\n#include \"util/debug.h\"\n#include \"util/mmap.h\"\n#include \"util/mutex.h\"\n#include \"util/target.h\"\n#include \"util/session.h\"\n#include \"util/tool.h\"\n#include \"util/symbol.h\"\n#include \"util/record.h\"\n#include \"util/cpumap.h\"\n#include \"util/thread_map.h\"\n#include \"util/data.h\"\n#include \"util/perf_regs.h\"\n#include \"util/auxtrace.h\"\n#include \"util/tsc.h\"\n#include \"util/parse-branch-options.h\"\n#include \"util/parse-regs-options.h\"\n#include \"util/perf_api_probe.h\"\n#include \"util/trigger.h\"\n#include \"util/perf-hooks.h\"\n#include \"util/cpu-set-sched.h\"\n#include \"util/synthetic-events.h\"\n#include \"util/time-utils.h\"\n#include \"util/units.h\"\n#include \"util/bpf-event.h\"\n#include \"util/util.h\"\n#include \"util/pfm.h\"\n#include \"util/pmu.h\"\n#include \"util/pmus.h\"\n#include \"util/clockid.h\"\n#include \"util/off_cpu.h\"\n#include \"util/bpf-filter.h\"\n#include \"asm/bug.h\"\n#include \"perf.h\"\n#include \"cputopo.h\"\n\n#include <errno.h>\n#include <inttypes.h>\n#include <locale.h>\n#include <poll.h>\n#include <pthread.h>\n#include <unistd.h>\n#ifndef HAVE_GETTID\n#include <syscall.h>\n#endif\n#include <sched.h>\n#include <signal.h>\n#ifdef HAVE_EVENTFD_SUPPORT\n#include <sys/eventfd.h>\n#endif\n#include <sys/mman.h>\n#include <sys/wait.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n#include <linux/err.h>\n#include <linux/string.h>\n#include <linux/time64.h>\n#include <linux/zalloc.h>\n#include <linux/bitmap.h>\n#include <sys/time.h>\n\nstruct switch_output {\n\tbool\t\t enabled;\n\tbool\t\t signal;\n\tunsigned long\t size;\n\tunsigned long\t time;\n\tconst char\t*str;\n\tbool\t\t set;\n\tchar\t\t **filenames;\n\tint\t\t num_files;\n\tint\t\t cur_file;\n};\n\nstruct thread_mask {\n\tstruct mmap_cpu_mask\tmaps;\n\tstruct mmap_cpu_mask\taffinity;\n};\n\nstruct record_thread {\n\tpid_t\t\t\ttid;\n\tstruct thread_mask\t*mask;\n\tstruct {\n\t\tint\t\tmsg[2];\n\t\tint\t\tack[2];\n\t} pipes;\n\tstruct fdarray\t\tpollfd;\n\tint\t\t\tctlfd_pos;\n\tint\t\t\tnr_mmaps;\n\tstruct mmap\t\t**maps;\n\tstruct mmap\t\t**overwrite_maps;\n\tstruct record\t\t*rec;\n\tunsigned long long\tsamples;\n\tunsigned long\t\twaking;\n\tu64\t\t\tbytes_written;\n\tu64\t\t\tbytes_transferred;\n\tu64\t\t\tbytes_compressed;\n};\n\nstatic __thread struct record_thread *thread;\n\nenum thread_msg {\n\tTHREAD_MSG__UNDEFINED = 0,\n\tTHREAD_MSG__READY,\n\tTHREAD_MSG__MAX,\n};\n\nstatic const char *thread_msg_tags[THREAD_MSG__MAX] = {\n\t\"UNDEFINED\", \"READY\"\n};\n\nenum thread_spec {\n\tTHREAD_SPEC__UNDEFINED = 0,\n\tTHREAD_SPEC__CPU,\n\tTHREAD_SPEC__CORE,\n\tTHREAD_SPEC__PACKAGE,\n\tTHREAD_SPEC__NUMA,\n\tTHREAD_SPEC__USER,\n\tTHREAD_SPEC__MAX,\n};\n\nstatic const char *thread_spec_tags[THREAD_SPEC__MAX] = {\n\t\"undefined\", \"cpu\", \"core\", \"package\", \"numa\", \"user\"\n};\n\nstruct pollfd_index_map {\n\tint evlist_pollfd_index;\n\tint thread_pollfd_index;\n};\n\nstruct record {\n\tstruct perf_tool\ttool;\n\tstruct record_opts\topts;\n\tu64\t\t\tbytes_written;\n\tu64\t\t\tthread_bytes_written;\n\tstruct perf_data\tdata;\n\tstruct auxtrace_record\t*itr;\n\tstruct evlist\t*evlist;\n\tstruct perf_session\t*session;\n\tstruct evlist\t\t*sb_evlist;\n\tpthread_t\t\tthread_id;\n\tint\t\t\trealtime_prio;\n\tbool\t\t\tswitch_output_event_set;\n\tbool\t\t\tno_buildid;\n\tbool\t\t\tno_buildid_set;\n\tbool\t\t\tno_buildid_cache;\n\tbool\t\t\tno_buildid_cache_set;\n\tbool\t\t\tbuildid_all;\n\tbool\t\t\tbuildid_mmap;\n\tbool\t\t\ttimestamp_filename;\n\tbool\t\t\ttimestamp_boundary;\n\tbool\t\t\toff_cpu;\n\tstruct switch_output\tswitch_output;\n\tunsigned long long\tsamples;\n\tunsigned long\t\toutput_max_size;\t \n\tstruct perf_debuginfod\tdebuginfod;\n\tint\t\t\tnr_threads;\n\tstruct thread_mask\t*thread_masks;\n\tstruct record_thread\t*thread_data;\n\tstruct pollfd_index_map\t*index_map;\n\tsize_t\t\t\tindex_map_sz;\n\tsize_t\t\t\tindex_map_cnt;\n};\n\nstatic volatile int done;\n\nstatic volatile int auxtrace_record__snapshot_started;\nstatic DEFINE_TRIGGER(auxtrace_snapshot_trigger);\nstatic DEFINE_TRIGGER(switch_output_trigger);\n\nstatic const char *affinity_tags[PERF_AFFINITY_MAX] = {\n\t\"SYS\", \"NODE\", \"CPU\"\n};\n\n#ifndef HAVE_GETTID\nstatic inline pid_t gettid(void)\n{\n\treturn (pid_t)syscall(__NR_gettid);\n}\n#endif\n\nstatic int record__threads_enabled(struct record *rec)\n{\n\treturn rec->opts.threads_spec;\n}\n\nstatic bool switch_output_signal(struct record *rec)\n{\n\treturn rec->switch_output.signal &&\n\t       trigger_is_ready(&switch_output_trigger);\n}\n\nstatic bool switch_output_size(struct record *rec)\n{\n\treturn rec->switch_output.size &&\n\t       trigger_is_ready(&switch_output_trigger) &&\n\t       (rec->bytes_written >= rec->switch_output.size);\n}\n\nstatic bool switch_output_time(struct record *rec)\n{\n\treturn rec->switch_output.time &&\n\t       trigger_is_ready(&switch_output_trigger);\n}\n\nstatic u64 record__bytes_written(struct record *rec)\n{\n\treturn rec->bytes_written + rec->thread_bytes_written;\n}\n\nstatic bool record__output_max_size_exceeded(struct record *rec)\n{\n\treturn rec->output_max_size &&\n\t       (record__bytes_written(rec) >= rec->output_max_size);\n}\n\nstatic int record__write(struct record *rec, struct mmap *map __maybe_unused,\n\t\t\t void *bf, size_t size)\n{\n\tstruct perf_data_file *file = &rec->session->data->file;\n\n\tif (map && map->file)\n\t\tfile = map->file;\n\n\tif (perf_data_file__write(file, bf, size) < 0) {\n\t\tpr_err(\"failed to write perf data, error: %m\\n\");\n\t\treturn -1;\n\t}\n\n\tif (map && map->file) {\n\t\tthread->bytes_written += size;\n\t\trec->thread_bytes_written += size;\n\t} else {\n\t\trec->bytes_written += size;\n\t}\n\n\tif (record__output_max_size_exceeded(rec) && !done) {\n\t\tfprintf(stderr, \"[ perf record: perf size limit reached (%\" PRIu64 \" KB),\"\n\t\t\t\t\" stopping session ]\\n\",\n\t\t\t\trecord__bytes_written(rec) >> 10);\n\t\tdone = 1;\n\t}\n\n\tif (switch_output_size(rec))\n\t\ttrigger_hit(&switch_output_trigger);\n\n\treturn 0;\n}\n\nstatic int record__aio_enabled(struct record *rec);\nstatic int record__comp_enabled(struct record *rec);\nstatic size_t zstd_compress(struct perf_session *session, struct mmap *map,\n\t\t\t    void *dst, size_t dst_size, void *src, size_t src_size);\n\n#ifdef HAVE_AIO_SUPPORT\nstatic int record__aio_write(struct aiocb *cblock, int trace_fd,\n\t\tvoid *buf, size_t size, off_t off)\n{\n\tint rc;\n\n\tcblock->aio_fildes = trace_fd;\n\tcblock->aio_buf    = buf;\n\tcblock->aio_nbytes = size;\n\tcblock->aio_offset = off;\n\tcblock->aio_sigevent.sigev_notify = SIGEV_NONE;\n\n\tdo {\n\t\trc = aio_write(cblock);\n\t\tif (rc == 0) {\n\t\t\tbreak;\n\t\t} else if (errno != EAGAIN) {\n\t\t\tcblock->aio_fildes = -1;\n\t\t\tpr_err(\"failed to queue perf data, error: %m\\n\");\n\t\t\tbreak;\n\t\t}\n\t} while (1);\n\n\treturn rc;\n}\n\nstatic int record__aio_complete(struct mmap *md, struct aiocb *cblock)\n{\n\tvoid *rem_buf;\n\toff_t rem_off;\n\tsize_t rem_size;\n\tint rc, aio_errno;\n\tssize_t aio_ret, written;\n\n\taio_errno = aio_error(cblock);\n\tif (aio_errno == EINPROGRESS)\n\t\treturn 0;\n\n\twritten = aio_ret = aio_return(cblock);\n\tif (aio_ret < 0) {\n\t\tif (aio_errno != EINTR)\n\t\t\tpr_err(\"failed to write perf data, error: %m\\n\");\n\t\twritten = 0;\n\t}\n\n\trem_size = cblock->aio_nbytes - written;\n\n\tif (rem_size == 0) {\n\t\tcblock->aio_fildes = -1;\n\t\t \n\t\tperf_mmap__put(&md->core);\n\t\trc = 1;\n\t} else {\n\t\t \n\t\trem_off = cblock->aio_offset + written;\n\t\trem_buf = (void *)(cblock->aio_buf + written);\n\t\trecord__aio_write(cblock, cblock->aio_fildes,\n\t\t\t\trem_buf, rem_size, rem_off);\n\t\trc = 0;\n\t}\n\n\treturn rc;\n}\n\nstatic int record__aio_sync(struct mmap *md, bool sync_all)\n{\n\tstruct aiocb **aiocb = md->aio.aiocb;\n\tstruct aiocb *cblocks = md->aio.cblocks;\n\tstruct timespec timeout = { 0, 1000 * 1000  * 1 };  \n\tint i, do_suspend;\n\n\tdo {\n\t\tdo_suspend = 0;\n\t\tfor (i = 0; i < md->aio.nr_cblocks; ++i) {\n\t\t\tif (cblocks[i].aio_fildes == -1 || record__aio_complete(md, &cblocks[i])) {\n\t\t\t\tif (sync_all)\n\t\t\t\t\taiocb[i] = NULL;\n\t\t\t\telse\n\t\t\t\t\treturn i;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\taiocb[i] = &cblocks[i];\n\t\t\t\tdo_suspend = 1;\n\t\t\t}\n\t\t}\n\t\tif (!do_suspend)\n\t\t\treturn -1;\n\n\t\twhile (aio_suspend((const struct aiocb **)aiocb, md->aio.nr_cblocks, &timeout)) {\n\t\t\tif (!(errno == EAGAIN || errno == EINTR))\n\t\t\t\tpr_err(\"failed to sync perf data, error: %m\\n\");\n\t\t}\n\t} while (1);\n}\n\nstruct record_aio {\n\tstruct record\t*rec;\n\tvoid\t\t*data;\n\tsize_t\t\tsize;\n};\n\nstatic int record__aio_pushfn(struct mmap *map, void *to, void *buf, size_t size)\n{\n\tstruct record_aio *aio = to;\n\n\t \n\n\tif (record__comp_enabled(aio->rec)) {\n\t\tsize = zstd_compress(aio->rec->session, NULL, aio->data + aio->size,\n\t\t\t\t     mmap__mmap_len(map) - aio->size,\n\t\t\t\t     buf, size);\n\t} else {\n\t\tmemcpy(aio->data + aio->size, buf, size);\n\t}\n\n\tif (!aio->size) {\n\t\t \n\t\tperf_mmap__get(&map->core);\n\t}\n\n\taio->size += size;\n\n\treturn size;\n}\n\nstatic int record__aio_push(struct record *rec, struct mmap *map, off_t *off)\n{\n\tint ret, idx;\n\tint trace_fd = rec->session->data->file.fd;\n\tstruct record_aio aio = { .rec = rec, .size = 0 };\n\n\t \n\n\tidx = record__aio_sync(map, false);\n\taio.data = map->aio.data[idx];\n\tret = perf_mmap__push(map, &aio, record__aio_pushfn);\n\tif (ret != 0)  \n\t\treturn ret;\n\n\trec->samples++;\n\tret = record__aio_write(&(map->aio.cblocks[idx]), trace_fd, aio.data, aio.size, *off);\n\tif (!ret) {\n\t\t*off += aio.size;\n\t\trec->bytes_written += aio.size;\n\t\tif (switch_output_size(rec))\n\t\t\ttrigger_hit(&switch_output_trigger);\n\t} else {\n\t\t \n\t\tperf_mmap__put(&map->core);\n\t}\n\n\treturn ret;\n}\n\nstatic off_t record__aio_get_pos(int trace_fd)\n{\n\treturn lseek(trace_fd, 0, SEEK_CUR);\n}\n\nstatic void record__aio_set_pos(int trace_fd, off_t pos)\n{\n\tlseek(trace_fd, pos, SEEK_SET);\n}\n\nstatic void record__aio_mmap_read_sync(struct record *rec)\n{\n\tint i;\n\tstruct evlist *evlist = rec->evlist;\n\tstruct mmap *maps = evlist->mmap;\n\n\tif (!record__aio_enabled(rec))\n\t\treturn;\n\n\tfor (i = 0; i < evlist->core.nr_mmaps; i++) {\n\t\tstruct mmap *map = &maps[i];\n\n\t\tif (map->core.base)\n\t\t\trecord__aio_sync(map, true);\n\t}\n}\n\nstatic int nr_cblocks_default = 1;\nstatic int nr_cblocks_max = 4;\n\nstatic int record__aio_parse(const struct option *opt,\n\t\t\t     const char *str,\n\t\t\t     int unset)\n{\n\tstruct record_opts *opts = (struct record_opts *)opt->value;\n\n\tif (unset) {\n\t\topts->nr_cblocks = 0;\n\t} else {\n\t\tif (str)\n\t\t\topts->nr_cblocks = strtol(str, NULL, 0);\n\t\tif (!opts->nr_cblocks)\n\t\t\topts->nr_cblocks = nr_cblocks_default;\n\t}\n\n\treturn 0;\n}\n#else  \nstatic int nr_cblocks_max = 0;\n\nstatic int record__aio_push(struct record *rec __maybe_unused, struct mmap *map __maybe_unused,\n\t\t\t    off_t *off __maybe_unused)\n{\n\treturn -1;\n}\n\nstatic off_t record__aio_get_pos(int trace_fd __maybe_unused)\n{\n\treturn -1;\n}\n\nstatic void record__aio_set_pos(int trace_fd __maybe_unused, off_t pos __maybe_unused)\n{\n}\n\nstatic void record__aio_mmap_read_sync(struct record *rec __maybe_unused)\n{\n}\n#endif\n\nstatic int record__aio_enabled(struct record *rec)\n{\n\treturn rec->opts.nr_cblocks > 0;\n}\n\n#define MMAP_FLUSH_DEFAULT 1\nstatic int record__mmap_flush_parse(const struct option *opt,\n\t\t\t\t    const char *str,\n\t\t\t\t    int unset)\n{\n\tint flush_max;\n\tstruct record_opts *opts = (struct record_opts *)opt->value;\n\tstatic struct parse_tag tags[] = {\n\t\t\t{ .tag  = 'B', .mult = 1       },\n\t\t\t{ .tag  = 'K', .mult = 1 << 10 },\n\t\t\t{ .tag  = 'M', .mult = 1 << 20 },\n\t\t\t{ .tag  = 'G', .mult = 1 << 30 },\n\t\t\t{ .tag  = 0 },\n\t};\n\n\tif (unset)\n\t\treturn 0;\n\n\tif (str) {\n\t\topts->mmap_flush = parse_tag_value(str, tags);\n\t\tif (opts->mmap_flush == (int)-1)\n\t\t\topts->mmap_flush = strtol(str, NULL, 0);\n\t}\n\n\tif (!opts->mmap_flush)\n\t\topts->mmap_flush = MMAP_FLUSH_DEFAULT;\n\n\tflush_max = evlist__mmap_size(opts->mmap_pages);\n\tflush_max /= 4;\n\tif (opts->mmap_flush > flush_max)\n\t\topts->mmap_flush = flush_max;\n\n\treturn 0;\n}\n\n#ifdef HAVE_ZSTD_SUPPORT\nstatic unsigned int comp_level_default = 1;\n\nstatic int record__parse_comp_level(const struct option *opt, const char *str, int unset)\n{\n\tstruct record_opts *opts = opt->value;\n\n\tif (unset) {\n\t\topts->comp_level = 0;\n\t} else {\n\t\tif (str)\n\t\t\topts->comp_level = strtol(str, NULL, 0);\n\t\tif (!opts->comp_level)\n\t\t\topts->comp_level = comp_level_default;\n\t}\n\n\treturn 0;\n}\n#endif\nstatic unsigned int comp_level_max = 22;\n\nstatic int record__comp_enabled(struct record *rec)\n{\n\treturn rec->opts.comp_level > 0;\n}\n\nstatic int process_synthesized_event(struct perf_tool *tool,\n\t\t\t\t     union perf_event *event,\n\t\t\t\t     struct perf_sample *sample __maybe_unused,\n\t\t\t\t     struct machine *machine __maybe_unused)\n{\n\tstruct record *rec = container_of(tool, struct record, tool);\n\treturn record__write(rec, NULL, event, event->header.size);\n}\n\nstatic struct mutex synth_lock;\n\nstatic int process_locked_synthesized_event(struct perf_tool *tool,\n\t\t\t\t     union perf_event *event,\n\t\t\t\t     struct perf_sample *sample __maybe_unused,\n\t\t\t\t     struct machine *machine __maybe_unused)\n{\n\tint ret;\n\n\tmutex_lock(&synth_lock);\n\tret = process_synthesized_event(tool, event, sample, machine);\n\tmutex_unlock(&synth_lock);\n\treturn ret;\n}\n\nstatic int record__pushfn(struct mmap *map, void *to, void *bf, size_t size)\n{\n\tstruct record *rec = to;\n\n\tif (record__comp_enabled(rec)) {\n\t\tsize = zstd_compress(rec->session, map, map->data, mmap__mmap_len(map), bf, size);\n\t\tbf   = map->data;\n\t}\n\n\tthread->samples++;\n\treturn record__write(rec, map, bf, size);\n}\n\nstatic volatile sig_atomic_t signr = -1;\nstatic volatile sig_atomic_t child_finished;\n#ifdef HAVE_EVENTFD_SUPPORT\nstatic volatile sig_atomic_t done_fd = -1;\n#endif\n\nstatic void sig_handler(int sig)\n{\n\tif (sig == SIGCHLD)\n\t\tchild_finished = 1;\n\telse\n\t\tsignr = sig;\n\n\tdone = 1;\n#ifdef HAVE_EVENTFD_SUPPORT\n\tif (done_fd >= 0) {\n\t\tu64 tmp = 1;\n\t\tint orig_errno = errno;\n\n\t\t \n\t\tif (write(done_fd, &tmp, sizeof(tmp)) < 0)\n\t\t\tpr_err(\"failed to signal wakeup fd, error: %m\\n\");\n\n\t\terrno = orig_errno;\n\t}\n#endif \n}\n\nstatic void sigsegv_handler(int sig)\n{\n\tperf_hooks__recover();\n\tsighandler_dump_stack(sig);\n}\n\nstatic void record__sig_exit(void)\n{\n\tif (signr == -1)\n\t\treturn;\n\n\tsignal(signr, SIG_DFL);\n\traise(signr);\n}\n\n#ifdef HAVE_AUXTRACE_SUPPORT\n\nstatic int record__process_auxtrace(struct perf_tool *tool,\n\t\t\t\t    struct mmap *map,\n\t\t\t\t    union perf_event *event, void *data1,\n\t\t\t\t    size_t len1, void *data2, size_t len2)\n{\n\tstruct record *rec = container_of(tool, struct record, tool);\n\tstruct perf_data *data = &rec->data;\n\tsize_t padding;\n\tu8 pad[8] = {0};\n\n\tif (!perf_data__is_pipe(data) && perf_data__is_single_file(data)) {\n\t\toff_t file_offset;\n\t\tint fd = perf_data__fd(data);\n\t\tint err;\n\n\t\tfile_offset = lseek(fd, 0, SEEK_CUR);\n\t\tif (file_offset == -1)\n\t\t\treturn -1;\n\t\terr = auxtrace_index__auxtrace_event(&rec->session->auxtrace_index,\n\t\t\t\t\t\t     event, file_offset);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tpadding = (len1 + len2) & 7;\n\tif (padding)\n\t\tpadding = 8 - padding;\n\n\trecord__write(rec, map, event, event->header.size);\n\trecord__write(rec, map, data1, len1);\n\tif (len2)\n\t\trecord__write(rec, map, data2, len2);\n\trecord__write(rec, map, &pad, padding);\n\n\treturn 0;\n}\n\nstatic int record__auxtrace_mmap_read(struct record *rec,\n\t\t\t\t      struct mmap *map)\n{\n\tint ret;\n\n\tret = auxtrace_mmap__read(map, rec->itr, &rec->tool,\n\t\t\t\t  record__process_auxtrace);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ret)\n\t\trec->samples++;\n\n\treturn 0;\n}\n\nstatic int record__auxtrace_mmap_read_snapshot(struct record *rec,\n\t\t\t\t\t       struct mmap *map)\n{\n\tint ret;\n\n\tret = auxtrace_mmap__read_snapshot(map, rec->itr, &rec->tool,\n\t\t\t\t\t   record__process_auxtrace,\n\t\t\t\t\t   rec->opts.auxtrace_snapshot_size);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (ret)\n\t\trec->samples++;\n\n\treturn 0;\n}\n\nstatic int record__auxtrace_read_snapshot_all(struct record *rec)\n{\n\tint i;\n\tint rc = 0;\n\n\tfor (i = 0; i < rec->evlist->core.nr_mmaps; i++) {\n\t\tstruct mmap *map = &rec->evlist->mmap[i];\n\n\t\tif (!map->auxtrace_mmap.base)\n\t\t\tcontinue;\n\n\t\tif (record__auxtrace_mmap_read_snapshot(rec, map) != 0) {\n\t\t\trc = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\treturn rc;\n}\n\nstatic void record__read_auxtrace_snapshot(struct record *rec, bool on_exit)\n{\n\tpr_debug(\"Recording AUX area tracing snapshot\\n\");\n\tif (record__auxtrace_read_snapshot_all(rec) < 0) {\n\t\ttrigger_error(&auxtrace_snapshot_trigger);\n\t} else {\n\t\tif (auxtrace_record__snapshot_finish(rec->itr, on_exit))\n\t\t\ttrigger_error(&auxtrace_snapshot_trigger);\n\t\telse\n\t\t\ttrigger_ready(&auxtrace_snapshot_trigger);\n\t}\n}\n\nstatic int record__auxtrace_snapshot_exit(struct record *rec)\n{\n\tif (trigger_is_error(&auxtrace_snapshot_trigger))\n\t\treturn 0;\n\n\tif (!auxtrace_record__snapshot_started &&\n\t    auxtrace_record__snapshot_start(rec->itr))\n\t\treturn -1;\n\n\trecord__read_auxtrace_snapshot(rec, true);\n\tif (trigger_is_error(&auxtrace_snapshot_trigger))\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int record__auxtrace_init(struct record *rec)\n{\n\tint err;\n\n\tif ((rec->opts.auxtrace_snapshot_opts || rec->opts.auxtrace_sample_opts)\n\t    && record__threads_enabled(rec)) {\n\t\tpr_err(\"AUX area tracing options are not available in parallel streaming mode.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!rec->itr) {\n\t\trec->itr = auxtrace_record__init(rec->evlist, &err);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = auxtrace_parse_snapshot_options(rec->itr, &rec->opts,\n\t\t\t\t\t      rec->opts.auxtrace_snapshot_opts);\n\tif (err)\n\t\treturn err;\n\n\terr = auxtrace_parse_sample_options(rec->itr, rec->evlist, &rec->opts,\n\t\t\t\t\t    rec->opts.auxtrace_sample_opts);\n\tif (err)\n\t\treturn err;\n\n\tauxtrace_regroup_aux_output(rec->evlist);\n\n\treturn auxtrace_parse_filters(rec->evlist);\n}\n\n#else\n\nstatic inline\nint record__auxtrace_mmap_read(struct record *rec __maybe_unused,\n\t\t\t       struct mmap *map __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic inline\nvoid record__read_auxtrace_snapshot(struct record *rec __maybe_unused,\n\t\t\t\t    bool on_exit __maybe_unused)\n{\n}\n\nstatic inline\nint auxtrace_record__snapshot_start(struct auxtrace_record *itr __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic inline\nint record__auxtrace_snapshot_exit(struct record *rec __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic int record__auxtrace_init(struct record *rec __maybe_unused)\n{\n\treturn 0;\n}\n\n#endif\n\nstatic int record__config_text_poke(struct evlist *evlist)\n{\n\tstruct evsel *evsel;\n\n\t \n\tevlist__for_each_entry(evlist, evsel) {\n\t\tif (evsel->core.attr.text_poke)\n\t\t\treturn 0;\n\t}\n\n\tevsel = evlist__add_dummy_on_all_cpus(evlist);\n\tif (!evsel)\n\t\treturn -ENOMEM;\n\n\tevsel->core.attr.text_poke = 1;\n\tevsel->core.attr.ksymbol = 1;\n\tevsel->immediate = true;\n\tevsel__set_sample_bit(evsel, TIME);\n\n\treturn 0;\n}\n\nstatic int record__config_off_cpu(struct record *rec)\n{\n\treturn off_cpu_prepare(rec->evlist, &rec->opts.target, &rec->opts);\n}\n\nstatic bool record__kcore_readable(struct machine *machine)\n{\n\tchar kcore[PATH_MAX];\n\tint fd;\n\n\tscnprintf(kcore, sizeof(kcore), \"%s/proc/kcore\", machine->root_dir);\n\n\tfd = open(kcore, O_RDONLY);\n\tif (fd < 0)\n\t\treturn false;\n\n\tclose(fd);\n\n\treturn true;\n}\n\nstatic int record__kcore_copy(struct machine *machine, struct perf_data *data)\n{\n\tchar from_dir[PATH_MAX];\n\tchar kcore_dir[PATH_MAX];\n\tint ret;\n\n\tsnprintf(from_dir, sizeof(from_dir), \"%s/proc\", machine->root_dir);\n\n\tret = perf_data__make_kcore_dir(data, kcore_dir, sizeof(kcore_dir));\n\tif (ret)\n\t\treturn ret;\n\n\treturn kcore_copy(from_dir, kcore_dir);\n}\n\nstatic void record__thread_data_init_pipes(struct record_thread *thread_data)\n{\n\tthread_data->pipes.msg[0] = -1;\n\tthread_data->pipes.msg[1] = -1;\n\tthread_data->pipes.ack[0] = -1;\n\tthread_data->pipes.ack[1] = -1;\n}\n\nstatic int record__thread_data_open_pipes(struct record_thread *thread_data)\n{\n\tif (pipe(thread_data->pipes.msg))\n\t\treturn -EINVAL;\n\n\tif (pipe(thread_data->pipes.ack)) {\n\t\tclose(thread_data->pipes.msg[0]);\n\t\tthread_data->pipes.msg[0] = -1;\n\t\tclose(thread_data->pipes.msg[1]);\n\t\tthread_data->pipes.msg[1] = -1;\n\t\treturn -EINVAL;\n\t}\n\n\tpr_debug2(\"thread_data[%p]: msg=[%d,%d], ack=[%d,%d]\\n\", thread_data,\n\t\t thread_data->pipes.msg[0], thread_data->pipes.msg[1],\n\t\t thread_data->pipes.ack[0], thread_data->pipes.ack[1]);\n\n\treturn 0;\n}\n\nstatic void record__thread_data_close_pipes(struct record_thread *thread_data)\n{\n\tif (thread_data->pipes.msg[0] != -1) {\n\t\tclose(thread_data->pipes.msg[0]);\n\t\tthread_data->pipes.msg[0] = -1;\n\t}\n\tif (thread_data->pipes.msg[1] != -1) {\n\t\tclose(thread_data->pipes.msg[1]);\n\t\tthread_data->pipes.msg[1] = -1;\n\t}\n\tif (thread_data->pipes.ack[0] != -1) {\n\t\tclose(thread_data->pipes.ack[0]);\n\t\tthread_data->pipes.ack[0] = -1;\n\t}\n\tif (thread_data->pipes.ack[1] != -1) {\n\t\tclose(thread_data->pipes.ack[1]);\n\t\tthread_data->pipes.ack[1] = -1;\n\t}\n}\n\nstatic bool evlist__per_thread(struct evlist *evlist)\n{\n\treturn cpu_map__is_dummy(evlist->core.user_requested_cpus);\n}\n\nstatic int record__thread_data_init_maps(struct record_thread *thread_data, struct evlist *evlist)\n{\n\tint m, tm, nr_mmaps = evlist->core.nr_mmaps;\n\tstruct mmap *mmap = evlist->mmap;\n\tstruct mmap *overwrite_mmap = evlist->overwrite_mmap;\n\tstruct perf_cpu_map *cpus = evlist->core.all_cpus;\n\tbool per_thread = evlist__per_thread(evlist);\n\n\tif (per_thread)\n\t\tthread_data->nr_mmaps = nr_mmaps;\n\telse\n\t\tthread_data->nr_mmaps = bitmap_weight(thread_data->mask->maps.bits,\n\t\t\t\t\t\t      thread_data->mask->maps.nbits);\n\tif (mmap) {\n\t\tthread_data->maps = zalloc(thread_data->nr_mmaps * sizeof(struct mmap *));\n\t\tif (!thread_data->maps)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (overwrite_mmap) {\n\t\tthread_data->overwrite_maps = zalloc(thread_data->nr_mmaps * sizeof(struct mmap *));\n\t\tif (!thread_data->overwrite_maps) {\n\t\t\tzfree(&thread_data->maps);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\tpr_debug2(\"thread_data[%p]: nr_mmaps=%d, maps=%p, ow_maps=%p\\n\", thread_data,\n\t\t thread_data->nr_mmaps, thread_data->maps, thread_data->overwrite_maps);\n\n\tfor (m = 0, tm = 0; m < nr_mmaps && tm < thread_data->nr_mmaps; m++) {\n\t\tif (per_thread ||\n\t\t    test_bit(perf_cpu_map__cpu(cpus, m).cpu, thread_data->mask->maps.bits)) {\n\t\t\tif (thread_data->maps) {\n\t\t\t\tthread_data->maps[tm] = &mmap[m];\n\t\t\t\tpr_debug2(\"thread_data[%p]: cpu%d: maps[%d] -> mmap[%d]\\n\",\n\t\t\t\t\t  thread_data, perf_cpu_map__cpu(cpus, m).cpu, tm, m);\n\t\t\t}\n\t\t\tif (thread_data->overwrite_maps) {\n\t\t\t\tthread_data->overwrite_maps[tm] = &overwrite_mmap[m];\n\t\t\t\tpr_debug2(\"thread_data[%p]: cpu%d: ow_maps[%d] -> ow_mmap[%d]\\n\",\n\t\t\t\t\t  thread_data, perf_cpu_map__cpu(cpus, m).cpu, tm, m);\n\t\t\t}\n\t\t\ttm++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int record__thread_data_init_pollfd(struct record_thread *thread_data, struct evlist *evlist)\n{\n\tint f, tm, pos;\n\tstruct mmap *map, *overwrite_map;\n\n\tfdarray__init(&thread_data->pollfd, 64);\n\n\tfor (tm = 0; tm < thread_data->nr_mmaps; tm++) {\n\t\tmap = thread_data->maps ? thread_data->maps[tm] : NULL;\n\t\toverwrite_map = thread_data->overwrite_maps ?\n\t\t\t\tthread_data->overwrite_maps[tm] : NULL;\n\n\t\tfor (f = 0; f < evlist->core.pollfd.nr; f++) {\n\t\t\tvoid *ptr = evlist->core.pollfd.priv[f].ptr;\n\n\t\t\tif ((map && ptr == map) || (overwrite_map && ptr == overwrite_map)) {\n\t\t\t\tpos = fdarray__dup_entry_from(&thread_data->pollfd, f,\n\t\t\t\t\t\t\t      &evlist->core.pollfd);\n\t\t\t\tif (pos < 0)\n\t\t\t\t\treturn pos;\n\t\t\t\tpr_debug2(\"thread_data[%p]: pollfd[%d] <- event_fd=%d\\n\",\n\t\t\t\t\t thread_data, pos, evlist->core.pollfd.entries[f].fd);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void record__free_thread_data(struct record *rec)\n{\n\tint t;\n\tstruct record_thread *thread_data = rec->thread_data;\n\n\tif (thread_data == NULL)\n\t\treturn;\n\n\tfor (t = 0; t < rec->nr_threads; t++) {\n\t\trecord__thread_data_close_pipes(&thread_data[t]);\n\t\tzfree(&thread_data[t].maps);\n\t\tzfree(&thread_data[t].overwrite_maps);\n\t\tfdarray__exit(&thread_data[t].pollfd);\n\t}\n\n\tzfree(&rec->thread_data);\n}\n\nstatic int record__map_thread_evlist_pollfd_indexes(struct record *rec,\n\t\t\t\t\t\t    int evlist_pollfd_index,\n\t\t\t\t\t\t    int thread_pollfd_index)\n{\n\tsize_t x = rec->index_map_cnt;\n\n\tif (realloc_array_as_needed(rec->index_map, rec->index_map_sz, x, NULL))\n\t\treturn -ENOMEM;\n\trec->index_map[x].evlist_pollfd_index = evlist_pollfd_index;\n\trec->index_map[x].thread_pollfd_index = thread_pollfd_index;\n\trec->index_map_cnt += 1;\n\treturn 0;\n}\n\nstatic int record__update_evlist_pollfd_from_thread(struct record *rec,\n\t\t\t\t\t\t    struct evlist *evlist,\n\t\t\t\t\t\t    struct record_thread *thread_data)\n{\n\tstruct pollfd *e_entries = evlist->core.pollfd.entries;\n\tstruct pollfd *t_entries = thread_data->pollfd.entries;\n\tint err = 0;\n\tsize_t i;\n\n\tfor (i = 0; i < rec->index_map_cnt; i++) {\n\t\tint e_pos = rec->index_map[i].evlist_pollfd_index;\n\t\tint t_pos = rec->index_map[i].thread_pollfd_index;\n\n\t\tif (e_entries[e_pos].fd != t_entries[t_pos].fd ||\n\t\t    e_entries[e_pos].events != t_entries[t_pos].events) {\n\t\t\tpr_err(\"Thread and evlist pollfd index mismatch\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tcontinue;\n\t\t}\n\t\te_entries[e_pos].revents = t_entries[t_pos].revents;\n\t}\n\treturn err;\n}\n\nstatic int record__dup_non_perf_events(struct record *rec,\n\t\t\t\t       struct evlist *evlist,\n\t\t\t\t       struct record_thread *thread_data)\n{\n\tstruct fdarray *fda = &evlist->core.pollfd;\n\tint i, ret;\n\n\tfor (i = 0; i < fda->nr; i++) {\n\t\tif (!(fda->priv[i].flags & fdarray_flag__non_perf_event))\n\t\t\tcontinue;\n\t\tret = fdarray__dup_entry_from(&thread_data->pollfd, i, fda);\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"Failed to duplicate descriptor in main thread pollfd\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tpr_debug2(\"thread_data[%p]: pollfd[%d] <- non_perf_event fd=%d\\n\",\n\t\t\t  thread_data, ret, fda->entries[i].fd);\n\t\tret = record__map_thread_evlist_pollfd_indexes(rec, i, ret);\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"Failed to map thread and evlist pollfd indexes\\n\");\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int record__alloc_thread_data(struct record *rec, struct evlist *evlist)\n{\n\tint t, ret;\n\tstruct record_thread *thread_data;\n\n\trec->thread_data = zalloc(rec->nr_threads * sizeof(*(rec->thread_data)));\n\tif (!rec->thread_data) {\n\t\tpr_err(\"Failed to allocate thread data\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tthread_data = rec->thread_data;\n\n\tfor (t = 0; t < rec->nr_threads; t++)\n\t\trecord__thread_data_init_pipes(&thread_data[t]);\n\n\tfor (t = 0; t < rec->nr_threads; t++) {\n\t\tthread_data[t].rec = rec;\n\t\tthread_data[t].mask = &rec->thread_masks[t];\n\t\tret = record__thread_data_init_maps(&thread_data[t], evlist);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to initialize thread[%d] maps\\n\", t);\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = record__thread_data_init_pollfd(&thread_data[t], evlist);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to initialize thread[%d] pollfd\\n\", t);\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (t) {\n\t\t\tthread_data[t].tid = -1;\n\t\t\tret = record__thread_data_open_pipes(&thread_data[t]);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"Failed to open thread[%d] communication pipes\\n\", t);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\tret = fdarray__add(&thread_data[t].pollfd, thread_data[t].pipes.msg[0],\n\t\t\t\t\t   POLLIN | POLLERR | POLLHUP, fdarray_flag__nonfilterable);\n\t\t\tif (ret < 0) {\n\t\t\t\tpr_err(\"Failed to add descriptor to thread[%d] pollfd\\n\", t);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\tthread_data[t].ctlfd_pos = ret;\n\t\t\tpr_debug2(\"thread_data[%p]: pollfd[%d] <- ctl_fd=%d\\n\",\n\t\t\t\t thread_data, thread_data[t].ctlfd_pos,\n\t\t\t\t thread_data[t].pipes.msg[0]);\n\t\t} else {\n\t\t\tthread_data[t].tid = gettid();\n\n\t\t\tret = record__dup_non_perf_events(rec, evlist, &thread_data[t]);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_free;\n\n\t\t\tthread_data[t].ctlfd_pos = -1;  \n\t\t}\n\t}\n\n\treturn 0;\n\nout_free:\n\trecord__free_thread_data(rec);\n\n\treturn ret;\n}\n\nstatic int record__mmap_evlist(struct record *rec,\n\t\t\t       struct evlist *evlist)\n{\n\tint i, ret;\n\tstruct record_opts *opts = &rec->opts;\n\tbool auxtrace_overwrite = opts->auxtrace_snapshot_mode ||\n\t\t\t\t  opts->auxtrace_sample_mode;\n\tchar msg[512];\n\n\tif (opts->affinity != PERF_AFFINITY_SYS)\n\t\tcpu__setup_cpunode_map();\n\n\tif (evlist__mmap_ex(evlist, opts->mmap_pages,\n\t\t\t\t opts->auxtrace_mmap_pages,\n\t\t\t\t auxtrace_overwrite,\n\t\t\t\t opts->nr_cblocks, opts->affinity,\n\t\t\t\t opts->mmap_flush, opts->comp_level) < 0) {\n\t\tif (errno == EPERM) {\n\t\t\tpr_err(\"Permission error mapping pages.\\n\"\n\t\t\t       \"Consider increasing \"\n\t\t\t       \"/proc/sys/kernel/perf_event_mlock_kb,\\n\"\n\t\t\t       \"or try again with a smaller value of -m/--mmap_pages.\\n\"\n\t\t\t       \"(current value: %u,%u)\\n\",\n\t\t\t       opts->mmap_pages, opts->auxtrace_mmap_pages);\n\t\t\treturn -errno;\n\t\t} else {\n\t\t\tpr_err(\"failed to mmap with %d (%s)\\n\", errno,\n\t\t\t\tstr_error_r(errno, msg, sizeof(msg)));\n\t\t\tif (errno)\n\t\t\t\treturn -errno;\n\t\t\telse\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (evlist__initialize_ctlfd(evlist, opts->ctl_fd, opts->ctl_fd_ack))\n\t\treturn -1;\n\n\tret = record__alloc_thread_data(rec, evlist);\n\tif (ret)\n\t\treturn ret;\n\n\tif (record__threads_enabled(rec)) {\n\t\tret = perf_data__create_dir(&rec->data, evlist->core.nr_mmaps);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to create data directory: %s\\n\", strerror(-ret));\n\t\t\treturn ret;\n\t\t}\n\t\tfor (i = 0; i < evlist->core.nr_mmaps; i++) {\n\t\t\tif (evlist->mmap)\n\t\t\t\tevlist->mmap[i].file = &rec->data.dir.files[i];\n\t\t\tif (evlist->overwrite_mmap)\n\t\t\t\tevlist->overwrite_mmap[i].file = &rec->data.dir.files[i];\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int record__mmap(struct record *rec)\n{\n\treturn record__mmap_evlist(rec, rec->evlist);\n}\n\nstatic int record__open(struct record *rec)\n{\n\tchar msg[BUFSIZ];\n\tstruct evsel *pos;\n\tstruct evlist *evlist = rec->evlist;\n\tstruct perf_session *session = rec->session;\n\tstruct record_opts *opts = &rec->opts;\n\tint rc = 0;\n\n\t \n\tif (opts->target.initial_delay || target__has_cpu(&opts->target) ||\n\t    perf_pmus__num_core_pmus() > 1) {\n\t\tpos = evlist__get_tracking_event(evlist);\n\t\tif (!evsel__is_dummy_event(pos)) {\n\t\t\t \n\t\t\tif (evlist__add_dummy(evlist))\n\t\t\t\treturn -ENOMEM;\n\t\t\tpos = evlist__last(evlist);\n\t\t\tevlist__set_tracking_event(evlist, pos);\n\t\t}\n\n\t\t \n\t\tif (opts->target.initial_delay && !pos->immediate &&\n\t\t    !target__has_cpu(&opts->target))\n\t\t\tpos->core.attr.enable_on_exec = 1;\n\t\telse\n\t\t\tpos->immediate = 1;\n\t}\n\n\tevlist__config(evlist, opts, &callchain_param);\n\n\tevlist__for_each_entry(evlist, pos) {\ntry_again:\n\t\tif (evsel__open(pos, pos->core.cpus, pos->core.threads) < 0) {\n\t\t\tif (evsel__fallback(pos, errno, msg, sizeof(msg))) {\n\t\t\t\tif (verbose > 0)\n\t\t\t\t\tui__warning(\"%s\\n\", msg);\n\t\t\t\tgoto try_again;\n\t\t\t}\n\t\t\tif ((errno == EINVAL || errno == EBADF) &&\n\t\t\t    pos->core.leader != &pos->core &&\n\t\t\t    pos->weak_group) {\n\t\t\t        pos = evlist__reset_weak_group(evlist, pos, true);\n\t\t\t\tgoto try_again;\n\t\t\t}\n\t\t\trc = -errno;\n\t\t\tevsel__open_strerror(pos, &opts->target, errno, msg, sizeof(msg));\n\t\t\tui__error(\"%s\\n\", msg);\n\t\t\tgoto out;\n\t\t}\n\n\t\tpos->supported = true;\n\t}\n\n\tif (symbol_conf.kptr_restrict && !evlist__exclude_kernel(evlist)) {\n\t\tpr_warning(\n\"WARNING: Kernel address maps (/proc/{kallsyms,modules}) are restricted,\\n\"\n\"check /proc/sys/kernel/kptr_restrict and /proc/sys/kernel/perf_event_paranoid.\\n\\n\"\n\"Samples in kernel functions may not be resolved if a suitable vmlinux\\n\"\n\"file is not found in the buildid cache or in the vmlinux path.\\n\\n\"\n\"Samples in kernel modules won't be resolved at all.\\n\\n\"\n\"If some relocation was applied (e.g. kexec) symbols may be misresolved\\n\"\n\"even with a suitable vmlinux or kallsyms file.\\n\\n\");\n\t}\n\n\tif (evlist__apply_filters(evlist, &pos)) {\n\t\tpr_err(\"failed to set filter \\\"%s\\\" on event %s with %d (%s)\\n\",\n\t\t\tpos->filter ?: \"BPF\", evsel__name(pos), errno,\n\t\t\tstr_error_r(errno, msg, sizeof(msg)));\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\trc = record__mmap(rec);\n\tif (rc)\n\t\tgoto out;\n\n\tsession->evlist = evlist;\n\tperf_session__set_id_hdr_size(session);\nout:\n\treturn rc;\n}\n\nstatic void set_timestamp_boundary(struct record *rec, u64 sample_time)\n{\n\tif (rec->evlist->first_sample_time == 0)\n\t\trec->evlist->first_sample_time = sample_time;\n\n\tif (sample_time)\n\t\trec->evlist->last_sample_time = sample_time;\n}\n\nstatic int process_sample_event(struct perf_tool *tool,\n\t\t\t\tunion perf_event *event,\n\t\t\t\tstruct perf_sample *sample,\n\t\t\t\tstruct evsel *evsel,\n\t\t\t\tstruct machine *machine)\n{\n\tstruct record *rec = container_of(tool, struct record, tool);\n\n\tset_timestamp_boundary(rec, sample->time);\n\n\tif (rec->buildid_all)\n\t\treturn 0;\n\n\trec->samples++;\n\treturn build_id__mark_dso_hit(tool, event, sample, evsel, machine);\n}\n\nstatic int process_buildids(struct record *rec)\n{\n\tstruct perf_session *session = rec->session;\n\n\tif (perf_data__size(&rec->data) == 0)\n\t\treturn 0;\n\n\t \n\tsymbol_conf.ignore_vmlinux_buildid = true;\n\n\t \n\tif (rec->buildid_all && !rec->timestamp_boundary)\n\t\trec->tool.sample = NULL;\n\n\treturn perf_session__process_events(session);\n}\n\nstatic void perf_event__synthesize_guest_os(struct machine *machine, void *data)\n{\n\tint err;\n\tstruct perf_tool *tool = data;\n\t \n\terr = perf_event__synthesize_modules(tool, process_synthesized_event,\n\t\t\t\t\t     machine);\n\tif (err < 0)\n\t\tpr_err(\"Couldn't record guest kernel [%d]'s reference\"\n\t\t       \" relocation symbol.\\n\", machine->pid);\n\n\t \n\terr = perf_event__synthesize_kernel_mmap(tool, process_synthesized_event,\n\t\t\t\t\t\t machine);\n\tif (err < 0)\n\t\tpr_err(\"Couldn't record guest kernel [%d]'s reference\"\n\t\t       \" relocation symbol.\\n\", machine->pid);\n}\n\nstatic struct perf_event_header finished_round_event = {\n\t.size = sizeof(struct perf_event_header),\n\t.type = PERF_RECORD_FINISHED_ROUND,\n};\n\nstatic struct perf_event_header finished_init_event = {\n\t.size = sizeof(struct perf_event_header),\n\t.type = PERF_RECORD_FINISHED_INIT,\n};\n\nstatic void record__adjust_affinity(struct record *rec, struct mmap *map)\n{\n\tif (rec->opts.affinity != PERF_AFFINITY_SYS &&\n\t    !bitmap_equal(thread->mask->affinity.bits, map->affinity_mask.bits,\n\t\t\t  thread->mask->affinity.nbits)) {\n\t\tbitmap_zero(thread->mask->affinity.bits, thread->mask->affinity.nbits);\n\t\tbitmap_or(thread->mask->affinity.bits, thread->mask->affinity.bits,\n\t\t\t  map->affinity_mask.bits, thread->mask->affinity.nbits);\n\t\tsched_setaffinity(0, MMAP_CPU_MASK_BYTES(&thread->mask->affinity),\n\t\t\t\t\t(cpu_set_t *)thread->mask->affinity.bits);\n\t\tif (verbose == 2) {\n\t\t\tpr_debug(\"threads[%d]: running on cpu%d: \", thread->tid, sched_getcpu());\n\t\t\tmmap_cpu_mask__scnprintf(&thread->mask->affinity, \"affinity\");\n\t\t}\n\t}\n}\n\nstatic size_t process_comp_header(void *record, size_t increment)\n{\n\tstruct perf_record_compressed *event = record;\n\tsize_t size = sizeof(*event);\n\n\tif (increment) {\n\t\tevent->header.size += increment;\n\t\treturn increment;\n\t}\n\n\tevent->header.type = PERF_RECORD_COMPRESSED;\n\tevent->header.size = size;\n\n\treturn size;\n}\n\nstatic size_t zstd_compress(struct perf_session *session, struct mmap *map,\n\t\t\t    void *dst, size_t dst_size, void *src, size_t src_size)\n{\n\tsize_t compressed;\n\tsize_t max_record_size = PERF_SAMPLE_MAX_SIZE - sizeof(struct perf_record_compressed) - 1;\n\tstruct zstd_data *zstd_data = &session->zstd_data;\n\n\tif (map && map->file)\n\t\tzstd_data = &map->zstd_data;\n\n\tcompressed = zstd_compress_stream_to_records(zstd_data, dst, dst_size, src, src_size,\n\t\t\t\t\t\t     max_record_size, process_comp_header);\n\n\tif (map && map->file) {\n\t\tthread->bytes_transferred += src_size;\n\t\tthread->bytes_compressed  += compressed;\n\t} else {\n\t\tsession->bytes_transferred += src_size;\n\t\tsession->bytes_compressed  += compressed;\n\t}\n\n\treturn compressed;\n}\n\nstatic int record__mmap_read_evlist(struct record *rec, struct evlist *evlist,\n\t\t\t\t    bool overwrite, bool synch)\n{\n\tu64 bytes_written = rec->bytes_written;\n\tint i;\n\tint rc = 0;\n\tint nr_mmaps;\n\tstruct mmap **maps;\n\tint trace_fd = rec->data.file.fd;\n\toff_t off = 0;\n\n\tif (!evlist)\n\t\treturn 0;\n\n\tnr_mmaps = thread->nr_mmaps;\n\tmaps = overwrite ? thread->overwrite_maps : thread->maps;\n\n\tif (!maps)\n\t\treturn 0;\n\n\tif (overwrite && evlist->bkw_mmap_state != BKW_MMAP_DATA_PENDING)\n\t\treturn 0;\n\n\tif (record__aio_enabled(rec))\n\t\toff = record__aio_get_pos(trace_fd);\n\n\tfor (i = 0; i < nr_mmaps; i++) {\n\t\tu64 flush = 0;\n\t\tstruct mmap *map = maps[i];\n\n\t\tif (map->core.base) {\n\t\t\trecord__adjust_affinity(rec, map);\n\t\t\tif (synch) {\n\t\t\t\tflush = map->core.flush;\n\t\t\t\tmap->core.flush = 1;\n\t\t\t}\n\t\t\tif (!record__aio_enabled(rec)) {\n\t\t\t\tif (perf_mmap__push(map, rec, record__pushfn) < 0) {\n\t\t\t\t\tif (synch)\n\t\t\t\t\t\tmap->core.flush = flush;\n\t\t\t\t\trc = -1;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (record__aio_push(rec, map, &off) < 0) {\n\t\t\t\t\trecord__aio_set_pos(trace_fd, off);\n\t\t\t\t\tif (synch)\n\t\t\t\t\t\tmap->core.flush = flush;\n\t\t\t\t\trc = -1;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (synch)\n\t\t\t\tmap->core.flush = flush;\n\t\t}\n\n\t\tif (map->auxtrace_mmap.base && !rec->opts.auxtrace_snapshot_mode &&\n\t\t    !rec->opts.auxtrace_sample_mode &&\n\t\t    record__auxtrace_mmap_read(rec, map) != 0) {\n\t\t\trc = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (record__aio_enabled(rec))\n\t\trecord__aio_set_pos(trace_fd, off);\n\n\t \n\tif (!record__threads_enabled(rec) && bytes_written != rec->bytes_written)\n\t\trc = record__write(rec, NULL, &finished_round_event, sizeof(finished_round_event));\n\n\tif (overwrite)\n\t\tevlist__toggle_bkw_mmap(evlist, BKW_MMAP_EMPTY);\nout:\n\treturn rc;\n}\n\nstatic int record__mmap_read_all(struct record *rec, bool synch)\n{\n\tint err;\n\n\terr = record__mmap_read_evlist(rec, rec->evlist, false, synch);\n\tif (err)\n\t\treturn err;\n\n\treturn record__mmap_read_evlist(rec, rec->evlist, true, synch);\n}\n\nstatic void record__thread_munmap_filtered(struct fdarray *fda, int fd,\n\t\t\t\t\t   void *arg __maybe_unused)\n{\n\tstruct perf_mmap *map = fda->priv[fd].ptr;\n\n\tif (map)\n\t\tperf_mmap__put(map);\n}\n\nstatic void *record__thread(void *arg)\n{\n\tenum thread_msg msg = THREAD_MSG__READY;\n\tbool terminate = false;\n\tstruct fdarray *pollfd;\n\tint err, ctlfd_pos;\n\n\tthread = arg;\n\tthread->tid = gettid();\n\n\terr = write(thread->pipes.ack[1], &msg, sizeof(msg));\n\tif (err == -1)\n\t\tpr_warning(\"threads[%d]: failed to notify on start: %s\\n\",\n\t\t\t   thread->tid, strerror(errno));\n\n\tpr_debug(\"threads[%d]: started on cpu%d\\n\", thread->tid, sched_getcpu());\n\n\tpollfd = &thread->pollfd;\n\tctlfd_pos = thread->ctlfd_pos;\n\n\tfor (;;) {\n\t\tunsigned long long hits = thread->samples;\n\n\t\tif (record__mmap_read_all(thread->rec, false) < 0 || terminate)\n\t\t\tbreak;\n\n\t\tif (hits == thread->samples) {\n\n\t\t\terr = fdarray__poll(pollfd, -1);\n\t\t\t \n\t\t\tif (err > 0 || (err < 0 && errno == EINTR))\n\t\t\t\terr = 0;\n\t\t\tthread->waking++;\n\n\t\t\tif (fdarray__filter(pollfd, POLLERR | POLLHUP,\n\t\t\t\t\t    record__thread_munmap_filtered, NULL) == 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (pollfd->entries[ctlfd_pos].revents & POLLHUP) {\n\t\t\tterminate = true;\n\t\t\tclose(thread->pipes.msg[0]);\n\t\t\tthread->pipes.msg[0] = -1;\n\t\t\tpollfd->entries[ctlfd_pos].fd = -1;\n\t\t\tpollfd->entries[ctlfd_pos].events = 0;\n\t\t}\n\n\t\tpollfd->entries[ctlfd_pos].revents = 0;\n\t}\n\trecord__mmap_read_all(thread->rec, true);\n\n\terr = write(thread->pipes.ack[1], &msg, sizeof(msg));\n\tif (err == -1)\n\t\tpr_warning(\"threads[%d]: failed to notify on termination: %s\\n\",\n\t\t\t   thread->tid, strerror(errno));\n\n\treturn NULL;\n}\n\nstatic void record__init_features(struct record *rec)\n{\n\tstruct perf_session *session = rec->session;\n\tint feat;\n\n\tfor (feat = HEADER_FIRST_FEATURE; feat < HEADER_LAST_FEATURE; feat++)\n\t\tperf_header__set_feat(&session->header, feat);\n\n\tif (rec->no_buildid)\n\t\tperf_header__clear_feat(&session->header, HEADER_BUILD_ID);\n\n#ifdef HAVE_LIBTRACEEVENT\n\tif (!have_tracepoints(&rec->evlist->core.entries))\n\t\tperf_header__clear_feat(&session->header, HEADER_TRACING_DATA);\n#endif\n\n\tif (!rec->opts.branch_stack)\n\t\tperf_header__clear_feat(&session->header, HEADER_BRANCH_STACK);\n\n\tif (!rec->opts.full_auxtrace)\n\t\tperf_header__clear_feat(&session->header, HEADER_AUXTRACE);\n\n\tif (!(rec->opts.use_clockid && rec->opts.clockid_res_ns))\n\t\tperf_header__clear_feat(&session->header, HEADER_CLOCKID);\n\n\tif (!rec->opts.use_clockid)\n\t\tperf_header__clear_feat(&session->header, HEADER_CLOCK_DATA);\n\n\tif (!record__threads_enabled(rec))\n\t\tperf_header__clear_feat(&session->header, HEADER_DIR_FORMAT);\n\n\tif (!record__comp_enabled(rec))\n\t\tperf_header__clear_feat(&session->header, HEADER_COMPRESSED);\n\n\tperf_header__clear_feat(&session->header, HEADER_STAT);\n}\n\nstatic void\nrecord__finish_output(struct record *rec)\n{\n\tint i;\n\tstruct perf_data *data = &rec->data;\n\tint fd = perf_data__fd(data);\n\n\tif (data->is_pipe)\n\t\treturn;\n\n\trec->session->header.data_size += rec->bytes_written;\n\tdata->file.size = lseek(perf_data__fd(data), 0, SEEK_CUR);\n\tif (record__threads_enabled(rec)) {\n\t\tfor (i = 0; i < data->dir.nr; i++)\n\t\t\tdata->dir.files[i].size = lseek(data->dir.files[i].fd, 0, SEEK_CUR);\n\t}\n\n\tif (!rec->no_buildid) {\n\t\tprocess_buildids(rec);\n\n\t\tif (rec->buildid_all)\n\t\t\tdsos__hit_all(rec->session);\n\t}\n\tperf_session__write_header(rec->session, rec->evlist, fd, true);\n\n\treturn;\n}\n\nstatic int record__synthesize_workload(struct record *rec, bool tail)\n{\n\tint err;\n\tstruct perf_thread_map *thread_map;\n\tbool needs_mmap = rec->opts.synth & PERF_SYNTH_MMAP;\n\n\tif (rec->opts.tail_synthesize != tail)\n\t\treturn 0;\n\n\tthread_map = thread_map__new_by_tid(rec->evlist->workload.pid);\n\tif (thread_map == NULL)\n\t\treturn -1;\n\n\terr = perf_event__synthesize_thread_map(&rec->tool, thread_map,\n\t\t\t\t\t\t process_synthesized_event,\n\t\t\t\t\t\t &rec->session->machines.host,\n\t\t\t\t\t\t needs_mmap,\n\t\t\t\t\t\t rec->opts.sample_address);\n\tperf_thread_map__put(thread_map);\n\treturn err;\n}\n\nstatic int write_finished_init(struct record *rec, bool tail)\n{\n\tif (rec->opts.tail_synthesize != tail)\n\t\treturn 0;\n\n\treturn record__write(rec, NULL, &finished_init_event, sizeof(finished_init_event));\n}\n\nstatic int record__synthesize(struct record *rec, bool tail);\n\nstatic int\nrecord__switch_output(struct record *rec, bool at_exit)\n{\n\tstruct perf_data *data = &rec->data;\n\tint fd, err;\n\tchar *new_filename;\n\n\t \n\tchar timestamp[] = \"InvalidTimestamp\";\n\n\trecord__aio_mmap_read_sync(rec);\n\n\twrite_finished_init(rec, true);\n\n\trecord__synthesize(rec, true);\n\tif (target__none(&rec->opts.target))\n\t\trecord__synthesize_workload(rec, true);\n\n\trec->samples = 0;\n\trecord__finish_output(rec);\n\terr = fetch_current_timestamp(timestamp, sizeof(timestamp));\n\tif (err) {\n\t\tpr_err(\"Failed to get current timestamp\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tfd = perf_data__switch(data, timestamp,\n\t\t\t\t    rec->session->header.data_offset,\n\t\t\t\t    at_exit, &new_filename);\n\tif (fd >= 0 && !at_exit) {\n\t\trec->bytes_written = 0;\n\t\trec->session->header.data_size = 0;\n\t}\n\n\tif (!quiet)\n\t\tfprintf(stderr, \"[ perf record: Dump %s.%s ]\\n\",\n\t\t\tdata->path, timestamp);\n\n\tif (rec->switch_output.num_files) {\n\t\tint n = rec->switch_output.cur_file + 1;\n\n\t\tif (n >= rec->switch_output.num_files)\n\t\t\tn = 0;\n\t\trec->switch_output.cur_file = n;\n\t\tif (rec->switch_output.filenames[n]) {\n\t\t\tremove(rec->switch_output.filenames[n]);\n\t\t\tzfree(&rec->switch_output.filenames[n]);\n\t\t}\n\t\trec->switch_output.filenames[n] = new_filename;\n\t} else {\n\t\tfree(new_filename);\n\t}\n\n\t \n\tif (!at_exit) {\n\t\trecord__synthesize(rec, false);\n\n\t\t \n\t\tif (target__none(&rec->opts.target))\n\t\t\trecord__synthesize_workload(rec, false);\n\t\twrite_finished_init(rec, false);\n\t}\n\treturn fd;\n}\n\nstatic void __record__save_lost_samples(struct record *rec, struct evsel *evsel,\n\t\t\t\t\tstruct perf_record_lost_samples *lost,\n\t\t\t\t\tint cpu_idx, int thread_idx, u64 lost_count,\n\t\t\t\t\tu16 misc_flag)\n{\n\tstruct perf_sample_id *sid;\n\tstruct perf_sample sample = {};\n\tint id_hdr_size;\n\n\tlost->lost = lost_count;\n\tif (evsel->core.ids) {\n\t\tsid = xyarray__entry(evsel->core.sample_id, cpu_idx, thread_idx);\n\t\tsample.id = sid->id;\n\t}\n\n\tid_hdr_size = perf_event__synthesize_id_sample((void *)(lost + 1),\n\t\t\t\t\t\t       evsel->core.attr.sample_type, &sample);\n\tlost->header.size = sizeof(*lost) + id_hdr_size;\n\tlost->header.misc = misc_flag;\n\trecord__write(rec, NULL, lost, lost->header.size);\n}\n\nstatic void record__read_lost_samples(struct record *rec)\n{\n\tstruct perf_session *session = rec->session;\n\tstruct perf_record_lost_samples *lost;\n\tstruct evsel *evsel;\n\n\t \n\tif (session->evlist == NULL)\n\t\treturn;\n\n\tlost = zalloc(PERF_SAMPLE_MAX_SIZE);\n\tif (lost == NULL) {\n\t\tpr_debug(\"Memory allocation failed\\n\");\n\t\treturn;\n\t}\n\n\tlost->header.type = PERF_RECORD_LOST_SAMPLES;\n\n\tevlist__for_each_entry(session->evlist, evsel) {\n\t\tstruct xyarray *xy = evsel->core.sample_id;\n\t\tu64 lost_count;\n\n\t\tif (xy == NULL || evsel->core.fd == NULL)\n\t\t\tcontinue;\n\t\tif (xyarray__max_x(evsel->core.fd) != xyarray__max_x(xy) ||\n\t\t    xyarray__max_y(evsel->core.fd) != xyarray__max_y(xy)) {\n\t\t\tpr_debug(\"Unmatched FD vs. sample ID: skip reading LOST count\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (int x = 0; x < xyarray__max_x(xy); x++) {\n\t\t\tfor (int y = 0; y < xyarray__max_y(xy); y++) {\n\t\t\t\tstruct perf_counts_values count;\n\n\t\t\t\tif (perf_evsel__read(&evsel->core, x, y, &count) < 0) {\n\t\t\t\t\tpr_debug(\"read LOST count failed\\n\");\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tif (count.lost) {\n\t\t\t\t\t__record__save_lost_samples(rec, evsel, lost,\n\t\t\t\t\t\t\t\t    x, y, count.lost, 0);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tlost_count = perf_bpf_filter__lost_count(evsel);\n\t\tif (lost_count)\n\t\t\t__record__save_lost_samples(rec, evsel, lost, 0, 0, lost_count,\n\t\t\t\t\t\t    PERF_RECORD_MISC_LOST_SAMPLES_BPF);\n\t}\nout:\n\tfree(lost);\n}\n\nstatic volatile sig_atomic_t workload_exec_errno;\n\n \nstatic void workload_exec_failed_signal(int signo __maybe_unused,\n\t\t\t\t\tsiginfo_t *info,\n\t\t\t\t\tvoid *ucontext __maybe_unused)\n{\n\tworkload_exec_errno = info->si_value.sival_int;\n\tdone = 1;\n\tchild_finished = 1;\n}\n\nstatic void snapshot_sig_handler(int sig);\nstatic void alarm_sig_handler(int sig);\n\nstatic const struct perf_event_mmap_page *evlist__pick_pc(struct evlist *evlist)\n{\n\tif (evlist) {\n\t\tif (evlist->mmap && evlist->mmap[0].core.base)\n\t\t\treturn evlist->mmap[0].core.base;\n\t\tif (evlist->overwrite_mmap && evlist->overwrite_mmap[0].core.base)\n\t\t\treturn evlist->overwrite_mmap[0].core.base;\n\t}\n\treturn NULL;\n}\n\nstatic const struct perf_event_mmap_page *record__pick_pc(struct record *rec)\n{\n\tconst struct perf_event_mmap_page *pc = evlist__pick_pc(rec->evlist);\n\tif (pc)\n\t\treturn pc;\n\treturn NULL;\n}\n\nstatic int record__synthesize(struct record *rec, bool tail)\n{\n\tstruct perf_session *session = rec->session;\n\tstruct machine *machine = &session->machines.host;\n\tstruct perf_data *data = &rec->data;\n\tstruct record_opts *opts = &rec->opts;\n\tstruct perf_tool *tool = &rec->tool;\n\tint err = 0;\n\tevent_op f = process_synthesized_event;\n\n\tif (rec->opts.tail_synthesize != tail)\n\t\treturn 0;\n\n\tif (data->is_pipe) {\n\t\terr = perf_event__synthesize_for_pipe(tool, session, data,\n\t\t\t\t\t\t      process_synthesized_event);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\n\t\trec->bytes_written += err;\n\t}\n\n\terr = perf_event__synth_time_conv(record__pick_pc(rec), tool,\n\t\t\t\t\t  process_synthesized_event, machine);\n\tif (err)\n\t\tgoto out;\n\n\t \n\terr = perf_event__synthesize_id_index(tool,\n\t\t\t\t\t      process_synthesized_event,\n\t\t\t\t\t      session->evlist, machine);\n\tif (err)\n\t\tgoto out;\n\n\tif (rec->opts.full_auxtrace) {\n\t\terr = perf_event__synthesize_auxtrace_info(rec->itr, tool,\n\t\t\t\t\tsession, process_synthesized_event);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (!evlist__exclude_kernel(rec->evlist)) {\n\t\terr = perf_event__synthesize_kernel_mmap(tool, process_synthesized_event,\n\t\t\t\t\t\t\t machine);\n\t\tWARN_ONCE(err < 0, \"Couldn't record kernel reference relocation symbol\\n\"\n\t\t\t\t   \"Symbol resolution may be skewed if relocation was used (e.g. kexec).\\n\"\n\t\t\t\t   \"Check /proc/kallsyms permission or run as root.\\n\");\n\n\t\terr = perf_event__synthesize_modules(tool, process_synthesized_event,\n\t\t\t\t\t\t     machine);\n\t\tWARN_ONCE(err < 0, \"Couldn't record kernel module information.\\n\"\n\t\t\t\t   \"Symbol resolution may be skewed if relocation was used (e.g. kexec).\\n\"\n\t\t\t\t   \"Check /proc/modules permission or run as root.\\n\");\n\t}\n\n\tif (perf_guest) {\n\t\tmachines__process_guests(&session->machines,\n\t\t\t\t\t perf_event__synthesize_guest_os, tool);\n\t}\n\n\terr = perf_event__synthesize_extra_attr(&rec->tool,\n\t\t\t\t\t\trec->evlist,\n\t\t\t\t\t\tprocess_synthesized_event,\n\t\t\t\t\t\tdata->is_pipe);\n\tif (err)\n\t\tgoto out;\n\n\terr = perf_event__synthesize_thread_map2(&rec->tool, rec->evlist->core.threads,\n\t\t\t\t\t\t process_synthesized_event,\n\t\t\t\t\t\tNULL);\n\tif (err < 0) {\n\t\tpr_err(\"Couldn't synthesize thread map.\\n\");\n\t\treturn err;\n\t}\n\n\terr = perf_event__synthesize_cpu_map(&rec->tool, rec->evlist->core.all_cpus,\n\t\t\t\t\t     process_synthesized_event, NULL);\n\tif (err < 0) {\n\t\tpr_err(\"Couldn't synthesize cpu map.\\n\");\n\t\treturn err;\n\t}\n\n\terr = perf_event__synthesize_bpf_events(session, process_synthesized_event,\n\t\t\t\t\t\tmachine, opts);\n\tif (err < 0) {\n\t\tpr_warning(\"Couldn't synthesize bpf events.\\n\");\n\t\terr = 0;\n\t}\n\n\tif (rec->opts.synth & PERF_SYNTH_CGROUP) {\n\t\terr = perf_event__synthesize_cgroups(tool, process_synthesized_event,\n\t\t\t\t\t\t     machine);\n\t\tif (err < 0) {\n\t\t\tpr_warning(\"Couldn't synthesize cgroup events.\\n\");\n\t\t\terr = 0;\n\t\t}\n\t}\n\n\tif (rec->opts.nr_threads_synthesize > 1) {\n\t\tmutex_init(&synth_lock);\n\t\tperf_set_multithreaded();\n\t\tf = process_locked_synthesized_event;\n\t}\n\n\tif (rec->opts.synth & PERF_SYNTH_TASK) {\n\t\tbool needs_mmap = rec->opts.synth & PERF_SYNTH_MMAP;\n\n\t\terr = __machine__synthesize_threads(machine, tool, &opts->target,\n\t\t\t\t\t\t    rec->evlist->core.threads,\n\t\t\t\t\t\t    f, needs_mmap, opts->sample_address,\n\t\t\t\t\t\t    rec->opts.nr_threads_synthesize);\n\t}\n\n\tif (rec->opts.nr_threads_synthesize > 1) {\n\t\tperf_set_singlethreaded();\n\t\tmutex_destroy(&synth_lock);\n\t}\n\nout:\n\treturn err;\n}\n\nstatic int record__process_signal_event(union perf_event *event __maybe_unused, void *data)\n{\n\tstruct record *rec = data;\n\tpthread_kill(rec->thread_id, SIGUSR2);\n\treturn 0;\n}\n\nstatic int record__setup_sb_evlist(struct record *rec)\n{\n\tstruct record_opts *opts = &rec->opts;\n\n\tif (rec->sb_evlist != NULL) {\n\t\t \n\t\tevlist__set_cb(rec->sb_evlist, record__process_signal_event, rec);\n\t\trec->thread_id = pthread_self();\n\t}\n#ifdef HAVE_LIBBPF_SUPPORT\n\tif (!opts->no_bpf_event) {\n\t\tif (rec->sb_evlist == NULL) {\n\t\t\trec->sb_evlist = evlist__new();\n\n\t\t\tif (rec->sb_evlist == NULL) {\n\t\t\t\tpr_err(\"Couldn't create side band evlist.\\n.\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\n\t\tif (evlist__add_bpf_sb_event(rec->sb_evlist, &rec->session->header.env)) {\n\t\t\tpr_err(\"Couldn't ask for PERF_RECORD_BPF_EVENT side band events.\\n.\");\n\t\t\treturn -1;\n\t\t}\n\t}\n#endif\n\tif (evlist__start_sb_thread(rec->sb_evlist, &rec->opts.target)) {\n\t\tpr_debug(\"Couldn't start the BPF side band thread:\\nBPF programs starting from now on won't be annotatable\\n\");\n\t\topts->no_bpf_event = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int record__init_clock(struct record *rec)\n{\n\tstruct perf_session *session = rec->session;\n\tstruct timespec ref_clockid;\n\tstruct timeval ref_tod;\n\tu64 ref;\n\n\tif (!rec->opts.use_clockid)\n\t\treturn 0;\n\n\tif (rec->opts.use_clockid && rec->opts.clockid_res_ns)\n\t\tsession->header.env.clock.clockid_res_ns = rec->opts.clockid_res_ns;\n\n\tsession->header.env.clock.clockid = rec->opts.clockid;\n\n\tif (gettimeofday(&ref_tod, NULL) != 0) {\n\t\tpr_err(\"gettimeofday failed, cannot set reference time.\\n\");\n\t\treturn -1;\n\t}\n\n\tif (clock_gettime(rec->opts.clockid, &ref_clockid)) {\n\t\tpr_err(\"clock_gettime failed, cannot set reference time.\\n\");\n\t\treturn -1;\n\t}\n\n\tref = (u64) ref_tod.tv_sec * NSEC_PER_SEC +\n\t      (u64) ref_tod.tv_usec * NSEC_PER_USEC;\n\n\tsession->header.env.clock.tod_ns = ref;\n\n\tref = (u64) ref_clockid.tv_sec * NSEC_PER_SEC +\n\t      (u64) ref_clockid.tv_nsec;\n\n\tsession->header.env.clock.clockid_ns = ref;\n\treturn 0;\n}\n\nstatic void hit_auxtrace_snapshot_trigger(struct record *rec)\n{\n\tif (trigger_is_ready(&auxtrace_snapshot_trigger)) {\n\t\ttrigger_hit(&auxtrace_snapshot_trigger);\n\t\tauxtrace_record__snapshot_started = 1;\n\t\tif (auxtrace_record__snapshot_start(rec->itr))\n\t\t\ttrigger_error(&auxtrace_snapshot_trigger);\n\t}\n}\n\nstatic void record__uniquify_name(struct record *rec)\n{\n\tstruct evsel *pos;\n\tstruct evlist *evlist = rec->evlist;\n\tchar *new_name;\n\tint ret;\n\n\tif (perf_pmus__num_core_pmus() == 1)\n\t\treturn;\n\n\tevlist__for_each_entry(evlist, pos) {\n\t\tif (!evsel__is_hybrid(pos))\n\t\t\tcontinue;\n\n\t\tif (strchr(pos->name, '/'))\n\t\t\tcontinue;\n\n\t\tret = asprintf(&new_name, \"%s/%s/\",\n\t\t\t       pos->pmu_name, pos->name);\n\t\tif (ret) {\n\t\t\tfree(pos->name);\n\t\t\tpos->name = new_name;\n\t\t}\n\t}\n}\n\nstatic int record__terminate_thread(struct record_thread *thread_data)\n{\n\tint err;\n\tenum thread_msg ack = THREAD_MSG__UNDEFINED;\n\tpid_t tid = thread_data->tid;\n\n\tclose(thread_data->pipes.msg[1]);\n\tthread_data->pipes.msg[1] = -1;\n\terr = read(thread_data->pipes.ack[0], &ack, sizeof(ack));\n\tif (err > 0)\n\t\tpr_debug2(\"threads[%d]: sent %s\\n\", tid, thread_msg_tags[ack]);\n\telse\n\t\tpr_warning(\"threads[%d]: failed to receive termination notification from %d\\n\",\n\t\t\t   thread->tid, tid);\n\n\treturn 0;\n}\n\nstatic int record__start_threads(struct record *rec)\n{\n\tint t, tt, err, ret = 0, nr_threads = rec->nr_threads;\n\tstruct record_thread *thread_data = rec->thread_data;\n\tsigset_t full, mask;\n\tpthread_t handle;\n\tpthread_attr_t attrs;\n\n\tthread = &thread_data[0];\n\n\tif (!record__threads_enabled(rec))\n\t\treturn 0;\n\n\tsigfillset(&full);\n\tif (sigprocmask(SIG_SETMASK, &full, &mask)) {\n\t\tpr_err(\"Failed to block signals on threads start: %s\\n\", strerror(errno));\n\t\treturn -1;\n\t}\n\n\tpthread_attr_init(&attrs);\n\tpthread_attr_setdetachstate(&attrs, PTHREAD_CREATE_DETACHED);\n\n\tfor (t = 1; t < nr_threads; t++) {\n\t\tenum thread_msg msg = THREAD_MSG__UNDEFINED;\n\n#ifdef HAVE_PTHREAD_ATTR_SETAFFINITY_NP\n\t\tpthread_attr_setaffinity_np(&attrs,\n\t\t\t\t\t    MMAP_CPU_MASK_BYTES(&(thread_data[t].mask->affinity)),\n\t\t\t\t\t    (cpu_set_t *)(thread_data[t].mask->affinity.bits));\n#endif\n\t\tif (pthread_create(&handle, &attrs, record__thread, &thread_data[t])) {\n\t\t\tfor (tt = 1; tt < t; tt++)\n\t\t\t\trecord__terminate_thread(&thread_data[t]);\n\t\t\tpr_err(\"Failed to start threads: %s\\n\", strerror(errno));\n\t\t\tret = -1;\n\t\t\tgoto out_err;\n\t\t}\n\n\t\terr = read(thread_data[t].pipes.ack[0], &msg, sizeof(msg));\n\t\tif (err > 0)\n\t\t\tpr_debug2(\"threads[%d]: sent %s\\n\", rec->thread_data[t].tid,\n\t\t\t\t  thread_msg_tags[msg]);\n\t\telse\n\t\t\tpr_warning(\"threads[%d]: failed to receive start notification from %d\\n\",\n\t\t\t\t   thread->tid, rec->thread_data[t].tid);\n\t}\n\n\tsched_setaffinity(0, MMAP_CPU_MASK_BYTES(&thread->mask->affinity),\n\t\t\t(cpu_set_t *)thread->mask->affinity.bits);\n\n\tpr_debug(\"threads[%d]: started on cpu%d\\n\", thread->tid, sched_getcpu());\n\nout_err:\n\tpthread_attr_destroy(&attrs);\n\n\tif (sigprocmask(SIG_SETMASK, &mask, NULL)) {\n\t\tpr_err(\"Failed to unblock signals on threads start: %s\\n\", strerror(errno));\n\t\tret = -1;\n\t}\n\n\treturn ret;\n}\n\nstatic int record__stop_threads(struct record *rec)\n{\n\tint t;\n\tstruct record_thread *thread_data = rec->thread_data;\n\n\tfor (t = 1; t < rec->nr_threads; t++)\n\t\trecord__terminate_thread(&thread_data[t]);\n\n\tfor (t = 0; t < rec->nr_threads; t++) {\n\t\trec->samples += thread_data[t].samples;\n\t\tif (!record__threads_enabled(rec))\n\t\t\tcontinue;\n\t\trec->session->bytes_transferred += thread_data[t].bytes_transferred;\n\t\trec->session->bytes_compressed += thread_data[t].bytes_compressed;\n\t\tpr_debug(\"threads[%d]: samples=%lld, wakes=%ld, \", thread_data[t].tid,\n\t\t\t thread_data[t].samples, thread_data[t].waking);\n\t\tif (thread_data[t].bytes_transferred && thread_data[t].bytes_compressed)\n\t\t\tpr_debug(\"transferred=%\" PRIu64 \", compressed=%\" PRIu64 \"\\n\",\n\t\t\t\t thread_data[t].bytes_transferred, thread_data[t].bytes_compressed);\n\t\telse\n\t\t\tpr_debug(\"written=%\" PRIu64 \"\\n\", thread_data[t].bytes_written);\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned long record__waking(struct record *rec)\n{\n\tint t;\n\tunsigned long waking = 0;\n\tstruct record_thread *thread_data = rec->thread_data;\n\n\tfor (t = 0; t < rec->nr_threads; t++)\n\t\twaking += thread_data[t].waking;\n\n\treturn waking;\n}\n\nstatic int __cmd_record(struct record *rec, int argc, const char **argv)\n{\n\tint err;\n\tint status = 0;\n\tconst bool forks = argc > 0;\n\tstruct perf_tool *tool = &rec->tool;\n\tstruct record_opts *opts = &rec->opts;\n\tstruct perf_data *data = &rec->data;\n\tstruct perf_session *session;\n\tbool disabled = false, draining = false;\n\tint fd;\n\tfloat ratio = 0;\n\tenum evlist_ctl_cmd cmd = EVLIST_CTL_CMD_UNSUPPORTED;\n\n\tatexit(record__sig_exit);\n\tsignal(SIGCHLD, sig_handler);\n\tsignal(SIGINT, sig_handler);\n\tsignal(SIGTERM, sig_handler);\n\tsignal(SIGSEGV, sigsegv_handler);\n\n\tif (rec->opts.record_namespaces)\n\t\ttool->namespace_events = true;\n\n\tif (rec->opts.record_cgroup) {\n#ifdef HAVE_FILE_HANDLE\n\t\ttool->cgroup_events = true;\n#else\n\t\tpr_err(\"cgroup tracking is not supported\\n\");\n\t\treturn -1;\n#endif\n\t}\n\n\tif (rec->opts.auxtrace_snapshot_mode || rec->switch_output.enabled) {\n\t\tsignal(SIGUSR2, snapshot_sig_handler);\n\t\tif (rec->opts.auxtrace_snapshot_mode)\n\t\t\ttrigger_on(&auxtrace_snapshot_trigger);\n\t\tif (rec->switch_output.enabled)\n\t\t\ttrigger_on(&switch_output_trigger);\n\t} else {\n\t\tsignal(SIGUSR2, SIG_IGN);\n\t}\n\n\tsession = perf_session__new(data, tool);\n\tif (IS_ERR(session)) {\n\t\tpr_err(\"Perf session creation failed.\\n\");\n\t\treturn PTR_ERR(session);\n\t}\n\n\tif (record__threads_enabled(rec)) {\n\t\tif (perf_data__is_pipe(&rec->data)) {\n\t\t\tpr_err(\"Parallel trace streaming is not available in pipe mode.\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (rec->opts.full_auxtrace) {\n\t\t\tpr_err(\"Parallel trace streaming is not available in AUX area tracing mode.\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tfd = perf_data__fd(data);\n\trec->session = session;\n\n\tif (zstd_init(&session->zstd_data, rec->opts.comp_level) < 0) {\n\t\tpr_err(\"Compression initialization failed.\\n\");\n\t\treturn -1;\n\t}\n#ifdef HAVE_EVENTFD_SUPPORT\n\tdone_fd = eventfd(0, EFD_NONBLOCK);\n\tif (done_fd < 0) {\n\t\tpr_err(\"Failed to create wakeup eventfd, error: %m\\n\");\n\t\tstatus = -1;\n\t\tgoto out_delete_session;\n\t}\n\terr = evlist__add_wakeup_eventfd(rec->evlist, done_fd);\n\tif (err < 0) {\n\t\tpr_err(\"Failed to add wakeup eventfd to poll list\\n\");\n\t\tstatus = err;\n\t\tgoto out_delete_session;\n\t}\n#endif \n\n\tsession->header.env.comp_type  = PERF_COMP_ZSTD;\n\tsession->header.env.comp_level = rec->opts.comp_level;\n\n\tif (rec->opts.kcore &&\n\t    !record__kcore_readable(&session->machines.host)) {\n\t\tpr_err(\"ERROR: kcore is not readable.\\n\");\n\t\treturn -1;\n\t}\n\n\tif (record__init_clock(rec))\n\t\treturn -1;\n\n\trecord__init_features(rec);\n\n\tif (forks) {\n\t\terr = evlist__prepare_workload(rec->evlist, &opts->target, argv, data->is_pipe,\n\t\t\t\t\t       workload_exec_failed_signal);\n\t\tif (err < 0) {\n\t\t\tpr_err(\"Couldn't run the workload!\\n\");\n\t\t\tstatus = err;\n\t\t\tgoto out_delete_session;\n\t\t}\n\t}\n\n\t \n\tif (data->is_pipe && rec->evlist->core.nr_entries == 1)\n\t\trec->opts.sample_id = true;\n\n\trecord__uniquify_name(rec);\n\n\t \n\tpr_debug3(\"perf record opening and mmapping events\\n\");\n\tif (record__open(rec) != 0) {\n\t\terr = -1;\n\t\tgoto out_free_threads;\n\t}\n\t \n\tpr_debug3(\"perf record done opening and mmapping events\\n\");\n\tsession->header.env.comp_mmap_len = session->evlist->core.mmap_len;\n\n\tif (rec->opts.kcore) {\n\t\terr = record__kcore_copy(&session->machines.host, data);\n\t\tif (err) {\n\t\t\tpr_err(\"ERROR: Failed to copy kcore\\n\");\n\t\t\tgoto out_free_threads;\n\t\t}\n\t}\n\n\t \n\tif (rec->tool.ordered_events && !evlist__sample_id_all(rec->evlist)) {\n\t\tpr_warning(\"WARNING: No sample_id_all support, falling back to unordered processing\\n\");\n\t\trec->tool.ordered_events = false;\n\t}\n\n\tif (evlist__nr_groups(rec->evlist) == 0)\n\t\tperf_header__clear_feat(&session->header, HEADER_GROUP_DESC);\n\n\tif (data->is_pipe) {\n\t\terr = perf_header__write_pipe(fd);\n\t\tif (err < 0)\n\t\t\tgoto out_free_threads;\n\t} else {\n\t\terr = perf_session__write_header(session, rec->evlist, fd, false);\n\t\tif (err < 0)\n\t\t\tgoto out_free_threads;\n\t}\n\n\terr = -1;\n\tif (!rec->no_buildid\n\t    && !perf_header__has_feat(&session->header, HEADER_BUILD_ID)) {\n\t\tpr_err(\"Couldn't generate buildids. \"\n\t\t       \"Use --no-buildid to profile anyway.\\n\");\n\t\tgoto out_free_threads;\n\t}\n\n\terr = record__setup_sb_evlist(rec);\n\tif (err)\n\t\tgoto out_free_threads;\n\n\terr = record__synthesize(rec, false);\n\tif (err < 0)\n\t\tgoto out_free_threads;\n\n\tif (rec->realtime_prio) {\n\t\tstruct sched_param param;\n\n\t\tparam.sched_priority = rec->realtime_prio;\n\t\tif (sched_setscheduler(0, SCHED_FIFO, &param)) {\n\t\t\tpr_err(\"Could not set realtime priority.\\n\");\n\t\t\terr = -1;\n\t\t\tgoto out_free_threads;\n\t\t}\n\t}\n\n\tif (record__start_threads(rec))\n\t\tgoto out_free_threads;\n\n\t \n\tif (!target__none(&opts->target) && !opts->target.initial_delay)\n\t\tevlist__enable(rec->evlist);\n\n\t \n\tif (forks) {\n\t\tstruct machine *machine = &session->machines.host;\n\t\tunion perf_event *event;\n\t\tpid_t tgid;\n\n\t\tevent = malloc(sizeof(event->comm) + machine->id_hdr_size);\n\t\tif (event == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_child;\n\t\t}\n\n\t\t \n\t\ttgid = perf_event__synthesize_comm(tool, event,\n\t\t\t\t\t\t   rec->evlist->workload.pid,\n\t\t\t\t\t\t   process_synthesized_event,\n\t\t\t\t\t\t   machine);\n\t\tfree(event);\n\n\t\tif (tgid == -1)\n\t\t\tgoto out_child;\n\n\t\tevent = malloc(sizeof(event->namespaces) +\n\t\t\t       (NR_NAMESPACES * sizeof(struct perf_ns_link_info)) +\n\t\t\t       machine->id_hdr_size);\n\t\tif (event == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_child;\n\t\t}\n\n\t\t \n\t\tperf_event__synthesize_namespaces(tool, event,\n\t\t\t\t\t\t  rec->evlist->workload.pid,\n\t\t\t\t\t\t  tgid, process_synthesized_event,\n\t\t\t\t\t\t  machine);\n\t\tfree(event);\n\n\t\tevlist__start_workload(rec->evlist);\n\t}\n\n\tif (opts->target.initial_delay) {\n\t\tpr_info(EVLIST_DISABLED_MSG);\n\t\tif (opts->target.initial_delay > 0) {\n\t\t\tusleep(opts->target.initial_delay * USEC_PER_MSEC);\n\t\t\tevlist__enable(rec->evlist);\n\t\t\tpr_info(EVLIST_ENABLED_MSG);\n\t\t}\n\t}\n\n\terr = event_enable_timer__start(rec->evlist->eet);\n\tif (err)\n\t\tgoto out_child;\n\n\t \n\tpr_debug3(\"perf record has started\\n\");\n\tfflush(stderr);\n\n\ttrigger_ready(&auxtrace_snapshot_trigger);\n\ttrigger_ready(&switch_output_trigger);\n\tperf_hooks__invoke_record_start();\n\n\t \n\terr = write_finished_init(rec, false);\n\tif (err < 0)\n\t\tgoto out_child;\n\n\tfor (;;) {\n\t\tunsigned long long hits = thread->samples;\n\n\t\t \n\t\tif (trigger_is_hit(&switch_output_trigger) || done || draining)\n\t\t\tevlist__toggle_bkw_mmap(rec->evlist, BKW_MMAP_DATA_PENDING);\n\n\t\tif (record__mmap_read_all(rec, false) < 0) {\n\t\t\ttrigger_error(&auxtrace_snapshot_trigger);\n\t\t\ttrigger_error(&switch_output_trigger);\n\t\t\terr = -1;\n\t\t\tgoto out_child;\n\t\t}\n\n\t\tif (auxtrace_record__snapshot_started) {\n\t\t\tauxtrace_record__snapshot_started = 0;\n\t\t\tif (!trigger_is_error(&auxtrace_snapshot_trigger))\n\t\t\t\trecord__read_auxtrace_snapshot(rec, false);\n\t\t\tif (trigger_is_error(&auxtrace_snapshot_trigger)) {\n\t\t\t\tpr_err(\"AUX area tracing snapshot failed\\n\");\n\t\t\t\terr = -1;\n\t\t\t\tgoto out_child;\n\t\t\t}\n\t\t}\n\n\t\tif (trigger_is_hit(&switch_output_trigger)) {\n\t\t\t \n\t\t\tif (rec->evlist->bkw_mmap_state == BKW_MMAP_RUNNING)\n\t\t\t\tcontinue;\n\t\t\ttrigger_ready(&switch_output_trigger);\n\n\t\t\t \n\t\t\tevlist__toggle_bkw_mmap(rec->evlist, BKW_MMAP_RUNNING);\n\n\t\t\tif (!quiet)\n\t\t\t\tfprintf(stderr, \"[ perf record: dump data: Woken up %ld times ]\\n\",\n\t\t\t\t\trecord__waking(rec));\n\t\t\tthread->waking = 0;\n\t\t\tfd = record__switch_output(rec, false);\n\t\t\tif (fd < 0) {\n\t\t\t\tpr_err(\"Failed to switch to new file\\n\");\n\t\t\t\ttrigger_error(&switch_output_trigger);\n\t\t\t\terr = fd;\n\t\t\t\tgoto out_child;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (rec->switch_output.time)\n\t\t\t\talarm(rec->switch_output.time);\n\t\t}\n\n\t\tif (hits == thread->samples) {\n\t\t\tif (done || draining)\n\t\t\t\tbreak;\n\t\t\terr = fdarray__poll(&thread->pollfd, -1);\n\t\t\t \n\t\t\tif (err > 0 || (err < 0 && errno == EINTR))\n\t\t\t\terr = 0;\n\t\t\tthread->waking++;\n\n\t\t\tif (fdarray__filter(&thread->pollfd, POLLERR | POLLHUP,\n\t\t\t\t\t    record__thread_munmap_filtered, NULL) == 0)\n\t\t\t\tdraining = true;\n\n\t\t\terr = record__update_evlist_pollfd_from_thread(rec, rec->evlist, thread);\n\t\t\tif (err)\n\t\t\t\tgoto out_child;\n\t\t}\n\n\t\tif (evlist__ctlfd_process(rec->evlist, &cmd) > 0) {\n\t\t\tswitch (cmd) {\n\t\t\tcase EVLIST_CTL_CMD_SNAPSHOT:\n\t\t\t\thit_auxtrace_snapshot_trigger(rec);\n\t\t\t\tevlist__ctlfd_ack(rec->evlist);\n\t\t\t\tbreak;\n\t\t\tcase EVLIST_CTL_CMD_STOP:\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\tcase EVLIST_CTL_CMD_ACK:\n\t\t\tcase EVLIST_CTL_CMD_UNSUPPORTED:\n\t\t\tcase EVLIST_CTL_CMD_ENABLE:\n\t\t\tcase EVLIST_CTL_CMD_DISABLE:\n\t\t\tcase EVLIST_CTL_CMD_EVLIST:\n\t\t\tcase EVLIST_CTL_CMD_PING:\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\terr = event_enable_timer__process(rec->evlist->eet);\n\t\tif (err < 0)\n\t\t\tgoto out_child;\n\t\tif (err) {\n\t\t\terr = 0;\n\t\t\tdone = 1;\n\t\t}\n\n\t\t \n\t\tif (done && !disabled && !target__none(&opts->target)) {\n\t\t\ttrigger_off(&auxtrace_snapshot_trigger);\n\t\t\tevlist__disable(rec->evlist);\n\t\t\tdisabled = true;\n\t\t}\n\t}\n\n\ttrigger_off(&auxtrace_snapshot_trigger);\n\ttrigger_off(&switch_output_trigger);\n\n\tif (opts->auxtrace_snapshot_on_exit)\n\t\trecord__auxtrace_snapshot_exit(rec);\n\n\tif (forks && workload_exec_errno) {\n\t\tchar msg[STRERR_BUFSIZE], strevsels[2048];\n\t\tconst char *emsg = str_error_r(workload_exec_errno, msg, sizeof(msg));\n\n\t\tevlist__scnprintf_evsels(rec->evlist, sizeof(strevsels), strevsels);\n\n\t\tpr_err(\"Failed to collect '%s' for the '%s' workload: %s\\n\",\n\t\t\tstrevsels, argv[0], emsg);\n\t\terr = -1;\n\t\tgoto out_child;\n\t}\n\n\tif (!quiet)\n\t\tfprintf(stderr, \"[ perf record: Woken up %ld times to write data ]\\n\",\n\t\t\trecord__waking(rec));\n\n\twrite_finished_init(rec, true);\n\n\tif (target__none(&rec->opts.target))\n\t\trecord__synthesize_workload(rec, true);\n\nout_child:\n\trecord__stop_threads(rec);\n\trecord__mmap_read_all(rec, true);\nout_free_threads:\n\trecord__free_thread_data(rec);\n\tevlist__finalize_ctlfd(rec->evlist);\n\trecord__aio_mmap_read_sync(rec);\n\n\tif (rec->session->bytes_transferred && rec->session->bytes_compressed) {\n\t\tratio = (float)rec->session->bytes_transferred/(float)rec->session->bytes_compressed;\n\t\tsession->header.env.comp_ratio = ratio + 0.5;\n\t}\n\n\tif (forks) {\n\t\tint exit_status;\n\n\t\tif (!child_finished)\n\t\t\tkill(rec->evlist->workload.pid, SIGTERM);\n\n\t\twait(&exit_status);\n\n\t\tif (err < 0)\n\t\t\tstatus = err;\n\t\telse if (WIFEXITED(exit_status))\n\t\t\tstatus = WEXITSTATUS(exit_status);\n\t\telse if (WIFSIGNALED(exit_status))\n\t\t\tsignr = WTERMSIG(exit_status);\n\t} else\n\t\tstatus = err;\n\n\tif (rec->off_cpu)\n\t\trec->bytes_written += off_cpu_write(rec->session);\n\n\trecord__read_lost_samples(rec);\n\trecord__synthesize(rec, true);\n\t \n\trec->samples = 0;\n\n\tif (!err) {\n\t\tif (!rec->timestamp_filename) {\n\t\t\trecord__finish_output(rec);\n\t\t} else {\n\t\t\tfd = record__switch_output(rec, true);\n\t\t\tif (fd < 0) {\n\t\t\t\tstatus = fd;\n\t\t\t\tgoto out_delete_session;\n\t\t\t}\n\t\t}\n\t}\n\n\tperf_hooks__invoke_record_end();\n\n\tif (!err && !quiet) {\n\t\tchar samples[128];\n\t\tconst char *postfix = rec->timestamp_filename ?\n\t\t\t\t\t\".<timestamp>\" : \"\";\n\n\t\tif (rec->samples && !rec->opts.full_auxtrace)\n\t\t\tscnprintf(samples, sizeof(samples),\n\t\t\t\t  \" (%\" PRIu64 \" samples)\", rec->samples);\n\t\telse\n\t\t\tsamples[0] = '\\0';\n\n\t\tfprintf(stderr,\t\"[ perf record: Captured and wrote %.3f MB %s%s%s\",\n\t\t\tperf_data__size(data) / 1024.0 / 1024.0,\n\t\t\tdata->path, postfix, samples);\n\t\tif (ratio) {\n\t\t\tfprintf(stderr,\t\", compressed (original %.3f MB, ratio is %.3f)\",\n\t\t\t\t\trec->session->bytes_transferred / 1024.0 / 1024.0,\n\t\t\t\t\tratio);\n\t\t}\n\t\tfprintf(stderr, \" ]\\n\");\n\t}\n\nout_delete_session:\n#ifdef HAVE_EVENTFD_SUPPORT\n\tif (done_fd >= 0) {\n\t\tfd = done_fd;\n\t\tdone_fd = -1;\n\n\t\tclose(fd);\n\t}\n#endif\n\tzstd_fini(&session->zstd_data);\n\tperf_session__delete(session);\n\n\tif (!opts->no_bpf_event)\n\t\tevlist__stop_sb_thread(rec->sb_evlist);\n\treturn status;\n}\n\nstatic void callchain_debug(struct callchain_param *callchain)\n{\n\tstatic const char *str[CALLCHAIN_MAX] = { \"NONE\", \"FP\", \"DWARF\", \"LBR\" };\n\n\tpr_debug(\"callchain: type %s\\n\", str[callchain->record_mode]);\n\n\tif (callchain->record_mode == CALLCHAIN_DWARF)\n\t\tpr_debug(\"callchain: stack dump size %d\\n\",\n\t\t\t callchain->dump_size);\n}\n\nint record_opts__parse_callchain(struct record_opts *record,\n\t\t\t\t struct callchain_param *callchain,\n\t\t\t\t const char *arg, bool unset)\n{\n\tint ret;\n\tcallchain->enabled = !unset;\n\n\t \n\tif (unset) {\n\t\tcallchain->record_mode = CALLCHAIN_NONE;\n\t\tpr_debug(\"callchain: disabled\\n\");\n\t\treturn 0;\n\t}\n\n\tret = parse_callchain_record_opt(arg, callchain);\n\tif (!ret) {\n\t\t \n\t\tif (callchain->record_mode == CALLCHAIN_DWARF)\n\t\t\trecord->sample_address = true;\n\t\tcallchain_debug(callchain);\n\t}\n\n\treturn ret;\n}\n\nint record_parse_callchain_opt(const struct option *opt,\n\t\t\t       const char *arg,\n\t\t\t       int unset)\n{\n\treturn record_opts__parse_callchain(opt->value, &callchain_param, arg, unset);\n}\n\nint record_callchain_opt(const struct option *opt,\n\t\t\t const char *arg __maybe_unused,\n\t\t\t int unset __maybe_unused)\n{\n\tstruct callchain_param *callchain = opt->value;\n\n\tcallchain->enabled = true;\n\n\tif (callchain->record_mode == CALLCHAIN_NONE)\n\t\tcallchain->record_mode = CALLCHAIN_FP;\n\n\tcallchain_debug(callchain);\n\treturn 0;\n}\n\nstatic int perf_record_config(const char *var, const char *value, void *cb)\n{\n\tstruct record *rec = cb;\n\n\tif (!strcmp(var, \"record.build-id\")) {\n\t\tif (!strcmp(value, \"cache\"))\n\t\t\trec->no_buildid_cache = false;\n\t\telse if (!strcmp(value, \"no-cache\"))\n\t\t\trec->no_buildid_cache = true;\n\t\telse if (!strcmp(value, \"skip\"))\n\t\t\trec->no_buildid = true;\n\t\telse if (!strcmp(value, \"mmap\"))\n\t\t\trec->buildid_mmap = true;\n\t\telse\n\t\t\treturn -1;\n\t\treturn 0;\n\t}\n\tif (!strcmp(var, \"record.call-graph\")) {\n\t\tvar = \"call-graph.record-mode\";\n\t\treturn perf_default_config(var, value, cb);\n\t}\n#ifdef HAVE_AIO_SUPPORT\n\tif (!strcmp(var, \"record.aio\")) {\n\t\trec->opts.nr_cblocks = strtol(value, NULL, 0);\n\t\tif (!rec->opts.nr_cblocks)\n\t\t\trec->opts.nr_cblocks = nr_cblocks_default;\n\t}\n#endif\n\tif (!strcmp(var, \"record.debuginfod\")) {\n\t\trec->debuginfod.urls = strdup(value);\n\t\tif (!rec->debuginfod.urls)\n\t\t\treturn -ENOMEM;\n\t\trec->debuginfod.set = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int record__parse_event_enable_time(const struct option *opt, const char *str, int unset)\n{\n\tstruct record *rec = (struct record *)opt->value;\n\n\treturn evlist__parse_event_enable_time(rec->evlist, &rec->opts, str, unset);\n}\n\nstatic int record__parse_affinity(const struct option *opt, const char *str, int unset)\n{\n\tstruct record_opts *opts = (struct record_opts *)opt->value;\n\n\tif (unset || !str)\n\t\treturn 0;\n\n\tif (!strcasecmp(str, \"node\"))\n\t\topts->affinity = PERF_AFFINITY_NODE;\n\telse if (!strcasecmp(str, \"cpu\"))\n\t\topts->affinity = PERF_AFFINITY_CPU;\n\n\treturn 0;\n}\n\nstatic int record__mmap_cpu_mask_alloc(struct mmap_cpu_mask *mask, int nr_bits)\n{\n\tmask->nbits = nr_bits;\n\tmask->bits = bitmap_zalloc(mask->nbits);\n\tif (!mask->bits)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void record__mmap_cpu_mask_free(struct mmap_cpu_mask *mask)\n{\n\tbitmap_free(mask->bits);\n\tmask->nbits = 0;\n}\n\nstatic int record__thread_mask_alloc(struct thread_mask *mask, int nr_bits)\n{\n\tint ret;\n\n\tret = record__mmap_cpu_mask_alloc(&mask->maps, nr_bits);\n\tif (ret) {\n\t\tmask->affinity.bits = NULL;\n\t\treturn ret;\n\t}\n\n\tret = record__mmap_cpu_mask_alloc(&mask->affinity, nr_bits);\n\tif (ret) {\n\t\trecord__mmap_cpu_mask_free(&mask->maps);\n\t\tmask->maps.bits = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic void record__thread_mask_free(struct thread_mask *mask)\n{\n\trecord__mmap_cpu_mask_free(&mask->maps);\n\trecord__mmap_cpu_mask_free(&mask->affinity);\n}\n\nstatic int record__parse_threads(const struct option *opt, const char *str, int unset)\n{\n\tint s;\n\tstruct record_opts *opts = opt->value;\n\n\tif (unset || !str || !strlen(str)) {\n\t\topts->threads_spec = THREAD_SPEC__CPU;\n\t} else {\n\t\tfor (s = 1; s < THREAD_SPEC__MAX; s++) {\n\t\t\tif (s == THREAD_SPEC__USER) {\n\t\t\t\topts->threads_user_spec = strdup(str);\n\t\t\t\tif (!opts->threads_user_spec)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\topts->threads_spec = THREAD_SPEC__USER;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!strncasecmp(str, thread_spec_tags[s], strlen(thread_spec_tags[s]))) {\n\t\t\t\topts->threads_spec = s;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (opts->threads_spec == THREAD_SPEC__USER)\n\t\tpr_debug(\"threads_spec: %s\\n\", opts->threads_user_spec);\n\telse\n\t\tpr_debug(\"threads_spec: %s\\n\", thread_spec_tags[opts->threads_spec]);\n\n\treturn 0;\n}\n\nstatic int parse_output_max_size(const struct option *opt,\n\t\t\t\t const char *str, int unset)\n{\n\tunsigned long *s = (unsigned long *)opt->value;\n\tstatic struct parse_tag tags_size[] = {\n\t\t{ .tag  = 'B', .mult = 1       },\n\t\t{ .tag  = 'K', .mult = 1 << 10 },\n\t\t{ .tag  = 'M', .mult = 1 << 20 },\n\t\t{ .tag  = 'G', .mult = 1 << 30 },\n\t\t{ .tag  = 0 },\n\t};\n\tunsigned long val;\n\n\tif (unset) {\n\t\t*s = 0;\n\t\treturn 0;\n\t}\n\n\tval = parse_tag_value(str, tags_size);\n\tif (val != (unsigned long) -1) {\n\t\t*s = val;\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n\nstatic int record__parse_mmap_pages(const struct option *opt,\n\t\t\t\t    const char *str,\n\t\t\t\t    int unset __maybe_unused)\n{\n\tstruct record_opts *opts = opt->value;\n\tchar *s, *p;\n\tunsigned int mmap_pages;\n\tint ret;\n\n\tif (!str)\n\t\treturn -EINVAL;\n\n\ts = strdup(str);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\tp = strchr(s, ',');\n\tif (p)\n\t\t*p = '\\0';\n\n\tif (*s) {\n\t\tret = __evlist__parse_mmap_pages(&mmap_pages, s);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\topts->mmap_pages = mmap_pages;\n\t}\n\n\tif (!p) {\n\t\tret = 0;\n\t\tgoto out_free;\n\t}\n\n\tret = __evlist__parse_mmap_pages(&mmap_pages, p + 1);\n\tif (ret)\n\t\tgoto out_free;\n\n\topts->auxtrace_mmap_pages = mmap_pages;\n\nout_free:\n\tfree(s);\n\treturn ret;\n}\n\nvoid __weak arch__add_leaf_frame_record_opts(struct record_opts *opts __maybe_unused)\n{\n}\n\nstatic int parse_control_option(const struct option *opt,\n\t\t\t\tconst char *str,\n\t\t\t\tint unset __maybe_unused)\n{\n\tstruct record_opts *opts = opt->value;\n\n\treturn evlist__parse_control(str, &opts->ctl_fd, &opts->ctl_fd_ack, &opts->ctl_fd_close);\n}\n\nstatic void switch_output_size_warn(struct record *rec)\n{\n\tu64 wakeup_size = evlist__mmap_size(rec->opts.mmap_pages);\n\tstruct switch_output *s = &rec->switch_output;\n\n\twakeup_size /= 2;\n\n\tif (s->size < wakeup_size) {\n\t\tchar buf[100];\n\n\t\tunit_number__scnprintf(buf, sizeof(buf), wakeup_size);\n\t\tpr_warning(\"WARNING: switch-output data size lower than \"\n\t\t\t   \"wakeup kernel buffer size (%s) \"\n\t\t\t   \"expect bigger perf.data sizes\\n\", buf);\n\t}\n}\n\nstatic int switch_output_setup(struct record *rec)\n{\n\tstruct switch_output *s = &rec->switch_output;\n\tstatic struct parse_tag tags_size[] = {\n\t\t{ .tag  = 'B', .mult = 1       },\n\t\t{ .tag  = 'K', .mult = 1 << 10 },\n\t\t{ .tag  = 'M', .mult = 1 << 20 },\n\t\t{ .tag  = 'G', .mult = 1 << 30 },\n\t\t{ .tag  = 0 },\n\t};\n\tstatic struct parse_tag tags_time[] = {\n\t\t{ .tag  = 's', .mult = 1        },\n\t\t{ .tag  = 'm', .mult = 60       },\n\t\t{ .tag  = 'h', .mult = 60*60    },\n\t\t{ .tag  = 'd', .mult = 60*60*24 },\n\t\t{ .tag  = 0 },\n\t};\n\tunsigned long val;\n\n\t \n\tif (rec->switch_output_event_set) {\n\t\tif (record__threads_enabled(rec)) {\n\t\t\tpr_warning(\"WARNING: --switch-output-event option is not available in parallel streaming mode.\\n\");\n\t\t\treturn 0;\n\t\t}\n\t\tgoto do_signal;\n\t}\n\n\tif (!s->set)\n\t\treturn 0;\n\n\tif (record__threads_enabled(rec)) {\n\t\tpr_warning(\"WARNING: --switch-output option is not available in parallel streaming mode.\\n\");\n\t\treturn 0;\n\t}\n\n\tif (!strcmp(s->str, \"signal\")) {\ndo_signal:\n\t\ts->signal = true;\n\t\tpr_debug(\"switch-output with SIGUSR2 signal\\n\");\n\t\tgoto enabled;\n\t}\n\n\tval = parse_tag_value(s->str, tags_size);\n\tif (val != (unsigned long) -1) {\n\t\ts->size = val;\n\t\tpr_debug(\"switch-output with %s size threshold\\n\", s->str);\n\t\tgoto enabled;\n\t}\n\n\tval = parse_tag_value(s->str, tags_time);\n\tif (val != (unsigned long) -1) {\n\t\ts->time = val;\n\t\tpr_debug(\"switch-output with %s time threshold (%lu seconds)\\n\",\n\t\t\t s->str, s->time);\n\t\tgoto enabled;\n\t}\n\n\treturn -1;\n\nenabled:\n\trec->timestamp_filename = true;\n\ts->enabled              = true;\n\n\tif (s->size && !rec->opts.no_buffering)\n\t\tswitch_output_size_warn(rec);\n\n\treturn 0;\n}\n\nstatic const char * const __record_usage[] = {\n\t\"perf record [<options>] [<command>]\",\n\t\"perf record [<options>] -- <command> [<options>]\",\n\tNULL\n};\nconst char * const *record_usage = __record_usage;\n\nstatic int build_id__process_mmap(struct perf_tool *tool, union perf_event *event,\n\t\t\t\t  struct perf_sample *sample, struct machine *machine)\n{\n\t \n\tif (!(event->header.misc & PERF_RECORD_MISC_USER))\n\t\treturn 0;\n\treturn perf_event__process_mmap(tool, event, sample, machine);\n}\n\nstatic int build_id__process_mmap2(struct perf_tool *tool, union perf_event *event,\n\t\t\t\t   struct perf_sample *sample, struct machine *machine)\n{\n\t \n\tif (!(event->header.misc & PERF_RECORD_MISC_USER))\n\t\treturn 0;\n\n\treturn perf_event__process_mmap2(tool, event, sample, machine);\n}\n\nstatic int process_timestamp_boundary(struct perf_tool *tool,\n\t\t\t\t      union perf_event *event __maybe_unused,\n\t\t\t\t      struct perf_sample *sample,\n\t\t\t\t      struct machine *machine __maybe_unused)\n{\n\tstruct record *rec = container_of(tool, struct record, tool);\n\n\tset_timestamp_boundary(rec, sample->time);\n\treturn 0;\n}\n\nstatic int parse_record_synth_option(const struct option *opt,\n\t\t\t\t     const char *str,\n\t\t\t\t     int unset __maybe_unused)\n{\n\tstruct record_opts *opts = opt->value;\n\tchar *p = strdup(str);\n\n\tif (p == NULL)\n\t\treturn -1;\n\n\topts->synth = parse_synth_opt(p);\n\tfree(p);\n\n\tif (opts->synth < 0) {\n\t\tpr_err(\"Invalid synth option: %s\\n\", str);\n\t\treturn -1;\n\t}\n\treturn 0;\n}\n\n \nstatic struct record record = {\n\t.opts = {\n\t\t.sample_time\t     = true,\n\t\t.mmap_pages\t     = UINT_MAX,\n\t\t.user_freq\t     = UINT_MAX,\n\t\t.user_interval\t     = ULLONG_MAX,\n\t\t.freq\t\t     = 4000,\n\t\t.target\t\t     = {\n\t\t\t.uses_mmap   = true,\n\t\t\t.default_per_cpu = true,\n\t\t},\n\t\t.mmap_flush          = MMAP_FLUSH_DEFAULT,\n\t\t.nr_threads_synthesize = 1,\n\t\t.ctl_fd              = -1,\n\t\t.ctl_fd_ack          = -1,\n\t\t.synth               = PERF_SYNTH_ALL,\n\t},\n\t.tool = {\n\t\t.sample\t\t= process_sample_event,\n\t\t.fork\t\t= perf_event__process_fork,\n\t\t.exit\t\t= perf_event__process_exit,\n\t\t.comm\t\t= perf_event__process_comm,\n\t\t.namespaces\t= perf_event__process_namespaces,\n\t\t.mmap\t\t= build_id__process_mmap,\n\t\t.mmap2\t\t= build_id__process_mmap2,\n\t\t.itrace_start\t= process_timestamp_boundary,\n\t\t.aux\t\t= process_timestamp_boundary,\n\t\t.ordered_events\t= true,\n\t},\n};\n\nconst char record_callchain_help[] = CALLCHAIN_RECORD_HELP\n\t\"\\n\\t\\t\\t\\tDefault: fp\";\n\nstatic bool dry_run;\n\nstatic struct parse_events_option_args parse_events_option_args = {\n\t.evlistp = &record.evlist,\n};\n\nstatic struct parse_events_option_args switch_output_parse_events_option_args = {\n\t.evlistp = &record.sb_evlist,\n};\n\n \nstatic struct option __record_options[] = {\n\tOPT_CALLBACK('e', \"event\", &parse_events_option_args, \"event\",\n\t\t     \"event selector. use 'perf list' to list available events\",\n\t\t     parse_events_option),\n\tOPT_CALLBACK(0, \"filter\", &record.evlist, \"filter\",\n\t\t     \"event filter\", parse_filter),\n\tOPT_CALLBACK_NOOPT(0, \"exclude-perf\", &record.evlist,\n\t\t\t   NULL, \"don't record events from perf itself\",\n\t\t\t   exclude_perf),\n\tOPT_STRING('p', \"pid\", &record.opts.target.pid, \"pid\",\n\t\t    \"record events on existing process id\"),\n\tOPT_STRING('t', \"tid\", &record.opts.target.tid, \"tid\",\n\t\t    \"record events on existing thread id\"),\n\tOPT_INTEGER('r', \"realtime\", &record.realtime_prio,\n\t\t    \"collect data with this RT SCHED_FIFO priority\"),\n\tOPT_BOOLEAN(0, \"no-buffering\", &record.opts.no_buffering,\n\t\t    \"collect data without buffering\"),\n\tOPT_BOOLEAN('R', \"raw-samples\", &record.opts.raw_samples,\n\t\t    \"collect raw sample records from all opened counters\"),\n\tOPT_BOOLEAN('a', \"all-cpus\", &record.opts.target.system_wide,\n\t\t\t    \"system-wide collection from all CPUs\"),\n\tOPT_STRING('C', \"cpu\", &record.opts.target.cpu_list, \"cpu\",\n\t\t    \"list of cpus to monitor\"),\n\tOPT_U64('c', \"count\", &record.opts.user_interval, \"event period to sample\"),\n\tOPT_STRING('o', \"output\", &record.data.path, \"file\",\n\t\t    \"output file name\"),\n\tOPT_BOOLEAN_SET('i', \"no-inherit\", &record.opts.no_inherit,\n\t\t\t&record.opts.no_inherit_set,\n\t\t\t\"child tasks do not inherit counters\"),\n\tOPT_BOOLEAN(0, \"tail-synthesize\", &record.opts.tail_synthesize,\n\t\t    \"synthesize non-sample events at the end of output\"),\n\tOPT_BOOLEAN(0, \"overwrite\", &record.opts.overwrite, \"use overwrite mode\"),\n\tOPT_BOOLEAN(0, \"no-bpf-event\", &record.opts.no_bpf_event, \"do not record bpf events\"),\n\tOPT_BOOLEAN(0, \"strict-freq\", &record.opts.strict_freq,\n\t\t    \"Fail if the specified frequency can't be used\"),\n\tOPT_CALLBACK('F', \"freq\", &record.opts, \"freq or 'max'\",\n\t\t     \"profile at this frequency\",\n\t\t      record__parse_freq),\n\tOPT_CALLBACK('m', \"mmap-pages\", &record.opts, \"pages[,pages]\",\n\t\t     \"number of mmap data pages and AUX area tracing mmap pages\",\n\t\t     record__parse_mmap_pages),\n\tOPT_CALLBACK(0, \"mmap-flush\", &record.opts, \"number\",\n\t\t     \"Minimal number of bytes that is extracted from mmap data pages (default: 1)\",\n\t\t     record__mmap_flush_parse),\n\tOPT_CALLBACK_NOOPT('g', NULL, &callchain_param,\n\t\t\t   NULL, \"enables call-graph recording\" ,\n\t\t\t   &record_callchain_opt),\n\tOPT_CALLBACK(0, \"call-graph\", &record.opts,\n\t\t     \"record_mode[,record_size]\", record_callchain_help,\n\t\t     &record_parse_callchain_opt),\n\tOPT_INCR('v', \"verbose\", &verbose,\n\t\t    \"be more verbose (show counter open errors, etc)\"),\n\tOPT_BOOLEAN('q', \"quiet\", &quiet, \"don't print any warnings or messages\"),\n\tOPT_BOOLEAN('s', \"stat\", &record.opts.inherit_stat,\n\t\t    \"per thread counts\"),\n\tOPT_BOOLEAN('d', \"data\", &record.opts.sample_address, \"Record the sample addresses\"),\n\tOPT_BOOLEAN(0, \"phys-data\", &record.opts.sample_phys_addr,\n\t\t    \"Record the sample physical addresses\"),\n\tOPT_BOOLEAN(0, \"data-page-size\", &record.opts.sample_data_page_size,\n\t\t    \"Record the sampled data address data page size\"),\n\tOPT_BOOLEAN(0, \"code-page-size\", &record.opts.sample_code_page_size,\n\t\t    \"Record the sampled code address (ip) page size\"),\n\tOPT_BOOLEAN(0, \"sample-cpu\", &record.opts.sample_cpu, \"Record the sample cpu\"),\n\tOPT_BOOLEAN(0, \"sample-identifier\", &record.opts.sample_identifier,\n\t\t    \"Record the sample identifier\"),\n\tOPT_BOOLEAN_SET('T', \"timestamp\", &record.opts.sample_time,\n\t\t\t&record.opts.sample_time_set,\n\t\t\t\"Record the sample timestamps\"),\n\tOPT_BOOLEAN_SET('P', \"period\", &record.opts.period, &record.opts.period_set,\n\t\t\t\"Record the sample period\"),\n\tOPT_BOOLEAN('n', \"no-samples\", &record.opts.no_samples,\n\t\t    \"don't sample\"),\n\tOPT_BOOLEAN_SET('N', \"no-buildid-cache\", &record.no_buildid_cache,\n\t\t\t&record.no_buildid_cache_set,\n\t\t\t\"do not update the buildid cache\"),\n\tOPT_BOOLEAN_SET('B', \"no-buildid\", &record.no_buildid,\n\t\t\t&record.no_buildid_set,\n\t\t\t\"do not collect buildids in perf.data\"),\n\tOPT_CALLBACK('G', \"cgroup\", &record.evlist, \"name\",\n\t\t     \"monitor event in cgroup name only\",\n\t\t     parse_cgroups),\n\tOPT_CALLBACK('D', \"delay\", &record, \"ms\",\n\t\t     \"ms to wait before starting measurement after program start (-1: start with events disabled), \"\n\t\t     \"or ranges of time to enable events e.g. '-D 10-20,30-40'\",\n\t\t     record__parse_event_enable_time),\n\tOPT_BOOLEAN(0, \"kcore\", &record.opts.kcore, \"copy /proc/kcore\"),\n\tOPT_STRING('u', \"uid\", &record.opts.target.uid_str, \"user\",\n\t\t   \"user to profile\"),\n\n\tOPT_CALLBACK_NOOPT('b', \"branch-any\", &record.opts.branch_stack,\n\t\t     \"branch any\", \"sample any taken branches\",\n\t\t     parse_branch_stack),\n\n\tOPT_CALLBACK('j', \"branch-filter\", &record.opts.branch_stack,\n\t\t     \"branch filter mask\", \"branch stack filter modes\",\n\t\t     parse_branch_stack),\n\tOPT_BOOLEAN('W', \"weight\", &record.opts.sample_weight,\n\t\t    \"sample by weight (on special events only)\"),\n\tOPT_BOOLEAN(0, \"transaction\", &record.opts.sample_transaction,\n\t\t    \"sample transaction flags (special events only)\"),\n\tOPT_BOOLEAN(0, \"per-thread\", &record.opts.target.per_thread,\n\t\t    \"use per-thread mmaps\"),\n\tOPT_CALLBACK_OPTARG('I', \"intr-regs\", &record.opts.sample_intr_regs, NULL, \"any register\",\n\t\t    \"sample selected machine registers on interrupt,\"\n\t\t    \" use '-I?' to list register names\", parse_intr_regs),\n\tOPT_CALLBACK_OPTARG(0, \"user-regs\", &record.opts.sample_user_regs, NULL, \"any register\",\n\t\t    \"sample selected machine registers on interrupt,\"\n\t\t    \" use '--user-regs=?' to list register names\", parse_user_regs),\n\tOPT_BOOLEAN(0, \"running-time\", &record.opts.running_time,\n\t\t    \"Record running/enabled time of read (:S) events\"),\n\tOPT_CALLBACK('k', \"clockid\", &record.opts,\n\t\"clockid\", \"clockid to use for events, see clock_gettime()\",\n\tparse_clockid),\n\tOPT_STRING_OPTARG('S', \"snapshot\", &record.opts.auxtrace_snapshot_opts,\n\t\t\t  \"opts\", \"AUX area tracing Snapshot Mode\", \"\"),\n\tOPT_STRING_OPTARG(0, \"aux-sample\", &record.opts.auxtrace_sample_opts,\n\t\t\t  \"opts\", \"sample AUX area\", \"\"),\n\tOPT_UINTEGER(0, \"proc-map-timeout\", &proc_map_timeout,\n\t\t\t\"per thread proc mmap processing timeout in ms\"),\n\tOPT_BOOLEAN(0, \"namespaces\", &record.opts.record_namespaces,\n\t\t    \"Record namespaces events\"),\n\tOPT_BOOLEAN(0, \"all-cgroups\", &record.opts.record_cgroup,\n\t\t    \"Record cgroup events\"),\n\tOPT_BOOLEAN_SET(0, \"switch-events\", &record.opts.record_switch_events,\n\t\t\t&record.opts.record_switch_events_set,\n\t\t\t\"Record context switch events\"),\n\tOPT_BOOLEAN_FLAG(0, \"all-kernel\", &record.opts.all_kernel,\n\t\t\t \"Configure all used events to run in kernel space.\",\n\t\t\t PARSE_OPT_EXCLUSIVE),\n\tOPT_BOOLEAN_FLAG(0, \"all-user\", &record.opts.all_user,\n\t\t\t \"Configure all used events to run in user space.\",\n\t\t\t PARSE_OPT_EXCLUSIVE),\n\tOPT_BOOLEAN(0, \"kernel-callchains\", &record.opts.kernel_callchains,\n\t\t    \"collect kernel callchains\"),\n\tOPT_BOOLEAN(0, \"user-callchains\", &record.opts.user_callchains,\n\t\t    \"collect user callchains\"),\n\tOPT_STRING(0, \"vmlinux\", &symbol_conf.vmlinux_name,\n\t\t   \"file\", \"vmlinux pathname\"),\n\tOPT_BOOLEAN(0, \"buildid-all\", &record.buildid_all,\n\t\t    \"Record build-id of all DSOs regardless of hits\"),\n\tOPT_BOOLEAN(0, \"buildid-mmap\", &record.buildid_mmap,\n\t\t    \"Record build-id in map events\"),\n\tOPT_BOOLEAN(0, \"timestamp-filename\", &record.timestamp_filename,\n\t\t    \"append timestamp to output filename\"),\n\tOPT_BOOLEAN(0, \"timestamp-boundary\", &record.timestamp_boundary,\n\t\t    \"Record timestamp boundary (time of first/last samples)\"),\n\tOPT_STRING_OPTARG_SET(0, \"switch-output\", &record.switch_output.str,\n\t\t\t  &record.switch_output.set, \"signal or size[BKMG] or time[smhd]\",\n\t\t\t  \"Switch output when receiving SIGUSR2 (signal) or cross a size or time threshold\",\n\t\t\t  \"signal\"),\n\tOPT_CALLBACK_SET(0, \"switch-output-event\", &switch_output_parse_events_option_args,\n\t\t\t &record.switch_output_event_set, \"switch output event\",\n\t\t\t \"switch output event selector. use 'perf list' to list available events\",\n\t\t\t parse_events_option_new_evlist),\n\tOPT_INTEGER(0, \"switch-max-files\", &record.switch_output.num_files,\n\t\t   \"Limit number of switch output generated files\"),\n\tOPT_BOOLEAN(0, \"dry-run\", &dry_run,\n\t\t    \"Parse options then exit\"),\n#ifdef HAVE_AIO_SUPPORT\n\tOPT_CALLBACK_OPTARG(0, \"aio\", &record.opts,\n\t\t     &nr_cblocks_default, \"n\", \"Use <n> control blocks in asynchronous trace writing mode (default: 1, max: 4)\",\n\t\t     record__aio_parse),\n#endif\n\tOPT_CALLBACK(0, \"affinity\", &record.opts, \"node|cpu\",\n\t\t     \"Set affinity mask of trace reading thread to NUMA node cpu mask or cpu of processed mmap buffer\",\n\t\t     record__parse_affinity),\n#ifdef HAVE_ZSTD_SUPPORT\n\tOPT_CALLBACK_OPTARG('z', \"compression-level\", &record.opts, &comp_level_default, \"n\",\n\t\t\t    \"Compress records using specified level (default: 1 - fastest compression, 22 - greatest compression)\",\n\t\t\t    record__parse_comp_level),\n#endif\n\tOPT_CALLBACK(0, \"max-size\", &record.output_max_size,\n\t\t     \"size\", \"Limit the maximum size of the output file\", parse_output_max_size),\n\tOPT_UINTEGER(0, \"num-thread-synthesize\",\n\t\t     &record.opts.nr_threads_synthesize,\n\t\t     \"number of threads to run for event synthesis\"),\n#ifdef HAVE_LIBPFM\n\tOPT_CALLBACK(0, \"pfm-events\", &record.evlist, \"event\",\n\t\t\"libpfm4 event selector. use 'perf list' to list available events\",\n\t\tparse_libpfm_events_option),\n#endif\n\tOPT_CALLBACK(0, \"control\", &record.opts, \"fd:ctl-fd[,ack-fd] or fifo:ctl-fifo[,ack-fifo]\",\n\t\t     \"Listen on ctl-fd descriptor for command to control measurement ('enable': enable events, 'disable': disable events,\\n\"\n\t\t     \"\\t\\t\\t  'snapshot': AUX area tracing snapshot).\\n\"\n\t\t     \"\\t\\t\\t  Optionally send control command completion ('ack\\\\n') to ack-fd descriptor.\\n\"\n\t\t     \"\\t\\t\\t  Alternatively, ctl-fifo / ack-fifo will be opened and used as ctl-fd / ack-fd.\",\n\t\t      parse_control_option),\n\tOPT_CALLBACK(0, \"synth\", &record.opts, \"no|all|task|mmap|cgroup\",\n\t\t     \"Fine-tune event synthesis: default=all\", parse_record_synth_option),\n\tOPT_STRING_OPTARG_SET(0, \"debuginfod\", &record.debuginfod.urls,\n\t\t\t  &record.debuginfod.set, \"debuginfod urls\",\n\t\t\t  \"Enable debuginfod data retrieval from DEBUGINFOD_URLS or specified urls\",\n\t\t\t  \"system\"),\n\tOPT_CALLBACK_OPTARG(0, \"threads\", &record.opts, NULL, \"spec\",\n\t\t\t    \"write collected trace data into several data files using parallel threads\",\n\t\t\t    record__parse_threads),\n\tOPT_BOOLEAN(0, \"off-cpu\", &record.off_cpu, \"Enable off-cpu analysis\"),\n\tOPT_END()\n};\n\nstruct option *record_options = __record_options;\n\nstatic int record__mmap_cpu_mask_init(struct mmap_cpu_mask *mask, struct perf_cpu_map *cpus)\n{\n\tstruct perf_cpu cpu;\n\tint idx;\n\n\tif (cpu_map__is_dummy(cpus))\n\t\treturn 0;\n\n\tperf_cpu_map__for_each_cpu(cpu, idx, cpus) {\n\t\tif (cpu.cpu == -1)\n\t\t\tcontinue;\n\t\t \n\t\tif ((unsigned long)cpu.cpu > mask->nbits)\n\t\t\treturn -ENODEV;\n\t\t__set_bit(cpu.cpu, mask->bits);\n\t}\n\n\treturn 0;\n}\n\nstatic int record__mmap_cpu_mask_init_spec(struct mmap_cpu_mask *mask, const char *mask_spec)\n{\n\tstruct perf_cpu_map *cpus;\n\n\tcpus = perf_cpu_map__new(mask_spec);\n\tif (!cpus)\n\t\treturn -ENOMEM;\n\n\tbitmap_zero(mask->bits, mask->nbits);\n\tif (record__mmap_cpu_mask_init(mask, cpus))\n\t\treturn -ENODEV;\n\n\tperf_cpu_map__put(cpus);\n\n\treturn 0;\n}\n\nstatic void record__free_thread_masks(struct record *rec, int nr_threads)\n{\n\tint t;\n\n\tif (rec->thread_masks)\n\t\tfor (t = 0; t < nr_threads; t++)\n\t\t\trecord__thread_mask_free(&rec->thread_masks[t]);\n\n\tzfree(&rec->thread_masks);\n}\n\nstatic int record__alloc_thread_masks(struct record *rec, int nr_threads, int nr_bits)\n{\n\tint t, ret;\n\n\trec->thread_masks = zalloc(nr_threads * sizeof(*(rec->thread_masks)));\n\tif (!rec->thread_masks) {\n\t\tpr_err(\"Failed to allocate thread masks\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (t = 0; t < nr_threads; t++) {\n\t\tret = record__thread_mask_alloc(&rec->thread_masks[t], nr_bits);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to allocate thread masks[%d]\\n\", t);\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\treturn 0;\n\nout_free:\n\trecord__free_thread_masks(rec, nr_threads);\n\n\treturn ret;\n}\n\nstatic int record__init_thread_cpu_masks(struct record *rec, struct perf_cpu_map *cpus)\n{\n\tint t, ret, nr_cpus = perf_cpu_map__nr(cpus);\n\n\tret = record__alloc_thread_masks(rec, nr_cpus, cpu__max_cpu().cpu);\n\tif (ret)\n\t\treturn ret;\n\n\trec->nr_threads = nr_cpus;\n\tpr_debug(\"nr_threads: %d\\n\", rec->nr_threads);\n\n\tfor (t = 0; t < rec->nr_threads; t++) {\n\t\t__set_bit(perf_cpu_map__cpu(cpus, t).cpu, rec->thread_masks[t].maps.bits);\n\t\t__set_bit(perf_cpu_map__cpu(cpus, t).cpu, rec->thread_masks[t].affinity.bits);\n\t\tif (verbose > 0) {\n\t\t\tpr_debug(\"thread_masks[%d]: \", t);\n\t\t\tmmap_cpu_mask__scnprintf(&rec->thread_masks[t].maps, \"maps\");\n\t\t\tpr_debug(\"thread_masks[%d]: \", t);\n\t\t\tmmap_cpu_mask__scnprintf(&rec->thread_masks[t].affinity, \"affinity\");\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int record__init_thread_masks_spec(struct record *rec, struct perf_cpu_map *cpus,\n\t\t\t\t\t  const char **maps_spec, const char **affinity_spec,\n\t\t\t\t\t  u32 nr_spec)\n{\n\tu32 s;\n\tint ret = 0, t = 0;\n\tstruct mmap_cpu_mask cpus_mask;\n\tstruct thread_mask thread_mask, full_mask, *thread_masks;\n\n\tret = record__mmap_cpu_mask_alloc(&cpus_mask, cpu__max_cpu().cpu);\n\tif (ret) {\n\t\tpr_err(\"Failed to allocate CPUs mask\\n\");\n\t\treturn ret;\n\t}\n\n\tret = record__mmap_cpu_mask_init(&cpus_mask, cpus);\n\tif (ret) {\n\t\tpr_err(\"Failed to init cpu mask\\n\");\n\t\tgoto out_free_cpu_mask;\n\t}\n\n\tret = record__thread_mask_alloc(&full_mask, cpu__max_cpu().cpu);\n\tif (ret) {\n\t\tpr_err(\"Failed to allocate full mask\\n\");\n\t\tgoto out_free_cpu_mask;\n\t}\n\n\tret = record__thread_mask_alloc(&thread_mask, cpu__max_cpu().cpu);\n\tif (ret) {\n\t\tpr_err(\"Failed to allocate thread mask\\n\");\n\t\tgoto out_free_full_and_cpu_masks;\n\t}\n\n\tfor (s = 0; s < nr_spec; s++) {\n\t\tret = record__mmap_cpu_mask_init_spec(&thread_mask.maps, maps_spec[s]);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to initialize maps thread mask\\n\");\n\t\t\tgoto out_free;\n\t\t}\n\t\tret = record__mmap_cpu_mask_init_spec(&thread_mask.affinity, affinity_spec[s]);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to initialize affinity thread mask\\n\");\n\t\t\tgoto out_free;\n\t\t}\n\n\t\t \n\t\tif (!bitmap_and(thread_mask.maps.bits, thread_mask.maps.bits,\n\t\t\t\tcpus_mask.bits, thread_mask.maps.nbits)) {\n\t\t\tpr_err(\"Empty maps mask: %s\\n\", maps_spec[s]);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (!bitmap_and(thread_mask.affinity.bits, thread_mask.affinity.bits,\n\t\t\t\tcpus_mask.bits, thread_mask.affinity.nbits)) {\n\t\t\tpr_err(\"Empty affinity mask: %s\\n\", affinity_spec[s]);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\t \n\t\tif (bitmap_intersects(thread_mask.maps.bits, full_mask.maps.bits,\n\t\t\t\t      thread_mask.maps.nbits)) {\n\t\t\tpr_err(\"Intersecting maps mask: %s\\n\", maps_spec[s]);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif (bitmap_intersects(thread_mask.affinity.bits, full_mask.affinity.bits,\n\t\t\t\t      thread_mask.affinity.nbits)) {\n\t\t\tpr_err(\"Intersecting affinity mask: %s\\n\", affinity_spec[s]);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tbitmap_or(full_mask.maps.bits, full_mask.maps.bits,\n\t\t\t  thread_mask.maps.bits, full_mask.maps.nbits);\n\t\tbitmap_or(full_mask.affinity.bits, full_mask.affinity.bits,\n\t\t\t  thread_mask.affinity.bits, full_mask.maps.nbits);\n\n\t\tthread_masks = realloc(rec->thread_masks, (t + 1) * sizeof(struct thread_mask));\n\t\tif (!thread_masks) {\n\t\t\tpr_err(\"Failed to reallocate thread masks\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t\trec->thread_masks = thread_masks;\n\t\trec->thread_masks[t] = thread_mask;\n\t\tif (verbose > 0) {\n\t\t\tpr_debug(\"thread_masks[%d]: \", t);\n\t\t\tmmap_cpu_mask__scnprintf(&rec->thread_masks[t].maps, \"maps\");\n\t\t\tpr_debug(\"thread_masks[%d]: \", t);\n\t\t\tmmap_cpu_mask__scnprintf(&rec->thread_masks[t].affinity, \"affinity\");\n\t\t}\n\t\tt++;\n\t\tret = record__thread_mask_alloc(&thread_mask, cpu__max_cpu().cpu);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to allocate thread mask\\n\");\n\t\t\tgoto out_free_full_and_cpu_masks;\n\t\t}\n\t}\n\trec->nr_threads = t;\n\tpr_debug(\"nr_threads: %d\\n\", rec->nr_threads);\n\tif (!rec->nr_threads)\n\t\tret = -EINVAL;\n\nout_free:\n\trecord__thread_mask_free(&thread_mask);\nout_free_full_and_cpu_masks:\n\trecord__thread_mask_free(&full_mask);\nout_free_cpu_mask:\n\trecord__mmap_cpu_mask_free(&cpus_mask);\n\n\treturn ret;\n}\n\nstatic int record__init_thread_core_masks(struct record *rec, struct perf_cpu_map *cpus)\n{\n\tint ret;\n\tstruct cpu_topology *topo;\n\n\ttopo = cpu_topology__new();\n\tif (!topo) {\n\t\tpr_err(\"Failed to allocate CPU topology\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tret = record__init_thread_masks_spec(rec, cpus, topo->core_cpus_list,\n\t\t\t\t\t     topo->core_cpus_list, topo->core_cpus_lists);\n\tcpu_topology__delete(topo);\n\n\treturn ret;\n}\n\nstatic int record__init_thread_package_masks(struct record *rec, struct perf_cpu_map *cpus)\n{\n\tint ret;\n\tstruct cpu_topology *topo;\n\n\ttopo = cpu_topology__new();\n\tif (!topo) {\n\t\tpr_err(\"Failed to allocate CPU topology\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tret = record__init_thread_masks_spec(rec, cpus, topo->package_cpus_list,\n\t\t\t\t\t     topo->package_cpus_list, topo->package_cpus_lists);\n\tcpu_topology__delete(topo);\n\n\treturn ret;\n}\n\nstatic int record__init_thread_numa_masks(struct record *rec, struct perf_cpu_map *cpus)\n{\n\tu32 s;\n\tint ret;\n\tconst char **spec;\n\tstruct numa_topology *topo;\n\n\ttopo = numa_topology__new();\n\tif (!topo) {\n\t\tpr_err(\"Failed to allocate NUMA topology\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tspec = zalloc(topo->nr * sizeof(char *));\n\tif (!spec) {\n\t\tpr_err(\"Failed to allocate NUMA spec\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto out_delete_topo;\n\t}\n\tfor (s = 0; s < topo->nr; s++)\n\t\tspec[s] = topo->nodes[s].cpus;\n\n\tret = record__init_thread_masks_spec(rec, cpus, spec, spec, topo->nr);\n\n\tzfree(&spec);\n\nout_delete_topo:\n\tnuma_topology__delete(topo);\n\n\treturn ret;\n}\n\nstatic int record__init_thread_user_masks(struct record *rec, struct perf_cpu_map *cpus)\n{\n\tint t, ret;\n\tu32 s, nr_spec = 0;\n\tchar **maps_spec = NULL, **affinity_spec = NULL, **tmp_spec;\n\tchar *user_spec, *spec, *spec_ptr, *mask, *mask_ptr, *dup_mask = NULL;\n\n\tfor (t = 0, user_spec = (char *)rec->opts.threads_user_spec; ; t++, user_spec = NULL) {\n\t\tspec = strtok_r(user_spec, \":\", &spec_ptr);\n\t\tif (spec == NULL)\n\t\t\tbreak;\n\t\tpr_debug2(\"threads_spec[%d]: %s\\n\", t, spec);\n\t\tmask = strtok_r(spec, \"/\", &mask_ptr);\n\t\tif (mask == NULL)\n\t\t\tbreak;\n\t\tpr_debug2(\"  maps mask: %s\\n\", mask);\n\t\ttmp_spec = realloc(maps_spec, (nr_spec + 1) * sizeof(char *));\n\t\tif (!tmp_spec) {\n\t\t\tpr_err(\"Failed to reallocate maps spec\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t\tmaps_spec = tmp_spec;\n\t\tmaps_spec[nr_spec] = dup_mask = strdup(mask);\n\t\tif (!maps_spec[nr_spec]) {\n\t\t\tpr_err(\"Failed to allocate maps spec[%d]\\n\", nr_spec);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t\tmask = strtok_r(NULL, \"/\", &mask_ptr);\n\t\tif (mask == NULL) {\n\t\t\tpr_err(\"Invalid thread maps or affinity specs\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t\tpr_debug2(\"  affinity mask: %s\\n\", mask);\n\t\ttmp_spec = realloc(affinity_spec, (nr_spec + 1) * sizeof(char *));\n\t\tif (!tmp_spec) {\n\t\t\tpr_err(\"Failed to reallocate affinity spec\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t\taffinity_spec = tmp_spec;\n\t\taffinity_spec[nr_spec] = strdup(mask);\n\t\tif (!affinity_spec[nr_spec]) {\n\t\t\tpr_err(\"Failed to allocate affinity spec[%d]\\n\", nr_spec);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\t\tdup_mask = NULL;\n\t\tnr_spec++;\n\t}\n\n\tret = record__init_thread_masks_spec(rec, cpus, (const char **)maps_spec,\n\t\t\t\t\t     (const char **)affinity_spec, nr_spec);\n\nout_free:\n\tfree(dup_mask);\n\tfor (s = 0; s < nr_spec; s++) {\n\t\tif (maps_spec)\n\t\t\tfree(maps_spec[s]);\n\t\tif (affinity_spec)\n\t\t\tfree(affinity_spec[s]);\n\t}\n\tfree(affinity_spec);\n\tfree(maps_spec);\n\n\treturn ret;\n}\n\nstatic int record__init_thread_default_masks(struct record *rec, struct perf_cpu_map *cpus)\n{\n\tint ret;\n\n\tret = record__alloc_thread_masks(rec, 1, cpu__max_cpu().cpu);\n\tif (ret)\n\t\treturn ret;\n\n\tif (record__mmap_cpu_mask_init(&rec->thread_masks->maps, cpus))\n\t\treturn -ENODEV;\n\n\trec->nr_threads = 1;\n\n\treturn 0;\n}\n\nstatic int record__init_thread_masks(struct record *rec)\n{\n\tint ret = 0;\n\tstruct perf_cpu_map *cpus = rec->evlist->core.all_cpus;\n\n\tif (!record__threads_enabled(rec))\n\t\treturn record__init_thread_default_masks(rec, cpus);\n\n\tif (evlist__per_thread(rec->evlist)) {\n\t\tpr_err(\"--per-thread option is mutually exclusive to parallel streaming mode.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (rec->opts.threads_spec) {\n\tcase THREAD_SPEC__CPU:\n\t\tret = record__init_thread_cpu_masks(rec, cpus);\n\t\tbreak;\n\tcase THREAD_SPEC__CORE:\n\t\tret = record__init_thread_core_masks(rec, cpus);\n\t\tbreak;\n\tcase THREAD_SPEC__PACKAGE:\n\t\tret = record__init_thread_package_masks(rec, cpus);\n\t\tbreak;\n\tcase THREAD_SPEC__NUMA:\n\t\tret = record__init_thread_numa_masks(rec, cpus);\n\t\tbreak;\n\tcase THREAD_SPEC__USER:\n\t\tret = record__init_thread_user_masks(rec, cpus);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint cmd_record(int argc, const char **argv)\n{\n\tint err;\n\tstruct record *rec = &record;\n\tchar errbuf[BUFSIZ];\n\n\tsetlocale(LC_ALL, \"\");\n\n#ifndef HAVE_BPF_SKEL\n# define set_nobuild(s, l, m, c) set_option_nobuild(record_options, s, l, m, c)\n\tset_nobuild('\\0', \"off-cpu\", \"no BUILD_BPF_SKEL=1\", true);\n# undef set_nobuild\n#endif\n\n\trec->opts.affinity = PERF_AFFINITY_SYS;\n\n\trec->evlist = evlist__new();\n\tif (rec->evlist == NULL)\n\t\treturn -ENOMEM;\n\n\terr = perf_config(perf_record_config, rec);\n\tif (err)\n\t\treturn err;\n\n\targc = parse_options(argc, argv, record_options, record_usage,\n\t\t\t    PARSE_OPT_STOP_AT_NON_OPTION);\n\tif (quiet)\n\t\tperf_quiet_option();\n\n\terr = symbol__validate_sym_arguments();\n\tif (err)\n\t\treturn err;\n\n\tperf_debuginfod_setup(&record.debuginfod);\n\n\t \n\tif (!argc && target__none(&rec->opts.target))\n\t\trec->opts.target.system_wide = true;\n\n\tif (nr_cgroups && !rec->opts.target.system_wide) {\n\t\tusage_with_options_msg(record_usage, record_options,\n\t\t\t\"cgroup monitoring only available in system-wide mode\");\n\n\t}\n\n\tif (rec->buildid_mmap) {\n\t\tif (!perf_can_record_build_id()) {\n\t\t\tpr_err(\"Failed: no support to record build id in mmap events, update your kernel.\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_opts;\n\t\t}\n\t\tpr_debug(\"Enabling build id in mmap2 events.\\n\");\n\t\t \n\t\tsymbol_conf.buildid_mmap2 = true;\n\t\t \n\t\trec->opts.build_id = true;\n\t\t \n\t\trec->no_buildid = true;\n\t}\n\n\tif (rec->opts.record_cgroup && !perf_can_record_cgroup()) {\n\t\tpr_err(\"Kernel has no cgroup sampling support.\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out_opts;\n\t}\n\n\tif (rec->opts.kcore)\n\t\trec->opts.text_poke = true;\n\n\tif (rec->opts.kcore || record__threads_enabled(rec))\n\t\trec->data.is_dir = true;\n\n\tif (record__threads_enabled(rec)) {\n\t\tif (rec->opts.affinity != PERF_AFFINITY_SYS) {\n\t\t\tpr_err(\"--affinity option is mutually exclusive to parallel streaming mode.\\n\");\n\t\t\tgoto out_opts;\n\t\t}\n\t\tif (record__aio_enabled(rec)) {\n\t\t\tpr_err(\"Asynchronous streaming mode (--aio) is mutually exclusive to parallel streaming mode.\\n\");\n\t\t\tgoto out_opts;\n\t\t}\n\t}\n\n\tif (rec->opts.comp_level != 0) {\n\t\tpr_debug(\"Compression enabled, disabling build id collection at the end of the session.\\n\");\n\t\trec->no_buildid = true;\n\t}\n\n\tif (rec->opts.record_switch_events &&\n\t    !perf_can_record_switch_events()) {\n\t\tui__error(\"kernel does not support recording context switch events\\n\");\n\t\tparse_options_usage(record_usage, record_options, \"switch-events\", 0);\n\t\terr = -EINVAL;\n\t\tgoto out_opts;\n\t}\n\n\tif (switch_output_setup(rec)) {\n\t\tparse_options_usage(record_usage, record_options, \"switch-output\", 0);\n\t\terr = -EINVAL;\n\t\tgoto out_opts;\n\t}\n\n\tif (rec->switch_output.time) {\n\t\tsignal(SIGALRM, alarm_sig_handler);\n\t\talarm(rec->switch_output.time);\n\t}\n\n\tif (rec->switch_output.num_files) {\n\t\trec->switch_output.filenames = calloc(sizeof(char *),\n\t\t\t\t\t\t      rec->switch_output.num_files);\n\t\tif (!rec->switch_output.filenames) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_opts;\n\t\t}\n\t}\n\n\tif (rec->timestamp_filename && record__threads_enabled(rec)) {\n\t\trec->timestamp_filename = false;\n\t\tpr_warning(\"WARNING: --timestamp-filename option is not available in parallel streaming mode.\\n\");\n\t}\n\n\t \n\tsymbol_conf.allow_aliases = true;\n\n\tsymbol__init(NULL);\n\n\terr = record__auxtrace_init(rec);\n\tif (err)\n\t\tgoto out;\n\n\tif (dry_run)\n\t\tgoto out;\n\n\terr = -ENOMEM;\n\n\tif (rec->no_buildid_cache || rec->no_buildid) {\n\t\tdisable_buildid_cache();\n\t} else if (rec->switch_output.enabled) {\n\t\t \n\t\tbool disable = true;\n\n\t\tif (rec->no_buildid_set && !rec->no_buildid)\n\t\t\tdisable = false;\n\t\tif (rec->no_buildid_cache_set && !rec->no_buildid_cache)\n\t\t\tdisable = false;\n\t\tif (disable) {\n\t\t\trec->no_buildid = true;\n\t\t\trec->no_buildid_cache = true;\n\t\t\tdisable_buildid_cache();\n\t\t}\n\t}\n\n\tif (record.opts.overwrite)\n\t\trecord.opts.tail_synthesize = true;\n\n\tif (rec->evlist->core.nr_entries == 0) {\n\t\tbool can_profile_kernel = perf_event_paranoid_check(1);\n\n\t\terr = parse_event(rec->evlist, can_profile_kernel ? \"cycles:P\" : \"cycles:Pu\");\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (rec->opts.target.tid && !rec->opts.no_inherit_set)\n\t\trec->opts.no_inherit = true;\n\n\terr = target__validate(&rec->opts.target);\n\tif (err) {\n\t\ttarget__strerror(&rec->opts.target, err, errbuf, BUFSIZ);\n\t\tui__warning(\"%s\\n\", errbuf);\n\t}\n\n\terr = target__parse_uid(&rec->opts.target);\n\tif (err) {\n\t\tint saved_errno = errno;\n\n\t\ttarget__strerror(&rec->opts.target, err, errbuf, BUFSIZ);\n\t\tui__error(\"%s\", errbuf);\n\n\t\terr = -saved_errno;\n\t\tgoto out;\n\t}\n\n\t \n\trec->opts.ignore_missing_thread = rec->opts.target.uid != UINT_MAX || rec->opts.target.pid;\n\n\tevlist__warn_user_requested_cpus(rec->evlist, rec->opts.target.cpu_list);\n\n\tif (callchain_param.enabled && callchain_param.record_mode == CALLCHAIN_FP)\n\t\tarch__add_leaf_frame_record_opts(&rec->opts);\n\n\terr = -ENOMEM;\n\tif (evlist__create_maps(rec->evlist, &rec->opts.target) < 0) {\n\t\tif (rec->opts.target.pid != NULL) {\n\t\t\tpr_err(\"Couldn't create thread/CPU maps: %s\\n\",\n\t\t\t\terrno == ENOENT ? \"No such process\" : str_error_r(errno, errbuf, sizeof(errbuf)));\n\t\t\tgoto out;\n\t\t}\n\t\telse\n\t\t\tusage_with_options(record_usage, record_options);\n\t}\n\n\terr = auxtrace_record__options(rec->itr, rec->evlist, &rec->opts);\n\tif (err)\n\t\tgoto out;\n\n\t \n\tif (rec->opts.full_auxtrace)\n\t\trec->buildid_all = true;\n\n\tif (rec->opts.text_poke) {\n\t\terr = record__config_text_poke(rec->evlist);\n\t\tif (err) {\n\t\t\tpr_err(\"record__config_text_poke failed, error %d\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (rec->off_cpu) {\n\t\terr = record__config_off_cpu(rec);\n\t\tif (err) {\n\t\t\tpr_err(\"record__config_off_cpu failed, error %d\\n\", err);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (record_opts__config(&rec->opts)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = record__init_thread_masks(rec);\n\tif (err) {\n\t\tpr_err(\"Failed to initialize parallel data streaming masks\\n\");\n\t\tgoto out;\n\t}\n\n\tif (rec->opts.nr_cblocks > nr_cblocks_max)\n\t\trec->opts.nr_cblocks = nr_cblocks_max;\n\tpr_debug(\"nr_cblocks: %d\\n\", rec->opts.nr_cblocks);\n\n\tpr_debug(\"affinity: %s\\n\", affinity_tags[rec->opts.affinity]);\n\tpr_debug(\"mmap flush: %d\\n\", rec->opts.mmap_flush);\n\n\tif (rec->opts.comp_level > comp_level_max)\n\t\trec->opts.comp_level = comp_level_max;\n\tpr_debug(\"comp level: %d\\n\", rec->opts.comp_level);\n\n\terr = __cmd_record(&record, argc, argv);\nout:\n\tevlist__delete(rec->evlist);\n\tsymbol__exit();\n\tauxtrace_record__free(rec->itr);\nout_opts:\n\trecord__free_thread_masks(rec, rec->nr_threads);\n\trec->nr_threads = 0;\n\tevlist__close_control(rec->opts.ctl_fd, rec->opts.ctl_fd_ack, &rec->opts.ctl_fd_close);\n\treturn err;\n}\n\nstatic void snapshot_sig_handler(int sig __maybe_unused)\n{\n\tstruct record *rec = &record;\n\n\thit_auxtrace_snapshot_trigger(rec);\n\n\tif (switch_output_signal(rec))\n\t\ttrigger_hit(&switch_output_trigger);\n}\n\nstatic void alarm_sig_handler(int sig __maybe_unused)\n{\n\tstruct record *rec = &record;\n\n\tif (switch_output_time(rec))\n\t\ttrigger_hit(&switch_output_trigger);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}