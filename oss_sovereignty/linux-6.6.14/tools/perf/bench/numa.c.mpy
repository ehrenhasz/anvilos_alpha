{
  "module_name": "numa.c",
  "hash_id": "873a4cdfe3e105ccd3d2ec7be0302874b8e1a676e8b7a72ee6f10e07b81ed981",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/bench/numa.c",
  "human_readable_source": "\n \n\n#include <inttypes.h>\n\n#include <subcmd/parse-options.h>\n#include \"../util/cloexec.h\"\n\n#include \"bench.h\"\n\n#include <errno.h>\n#include <sched.h>\n#include <stdio.h>\n#include <assert.h>\n#include <debug.h>\n#include <malloc.h>\n#include <signal.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <sys/mman.h>\n#include <sys/time.h>\n#include <sys/resource.h>\n#include <sys/wait.h>\n#include <sys/prctl.h>\n#include <sys/types.h>\n#include <linux/kernel.h>\n#include <linux/time64.h>\n#include <linux/numa.h>\n#include <linux/zalloc.h>\n\n#include \"../util/header.h\"\n#include \"../util/mutex.h\"\n#include <numa.h>\n#include <numaif.h>\n\n#ifndef RUSAGE_THREAD\n# define RUSAGE_THREAD 1\n#endif\n\n \n#define tprintf(x...) do { if (g && g->p.show_details >= 0) printf(x); } while (0)\n\n \n#undef dprintf\n#define dprintf(x...) do { if (g && g->p.show_details >= 1) printf(x); } while (0)\n\nstruct thread_data {\n\tint\t\t\tcurr_cpu;\n\tcpu_set_t\t\t*bind_cpumask;\n\tint\t\t\tbind_node;\n\tu8\t\t\t*process_data;\n\tint\t\t\tprocess_nr;\n\tint\t\t\tthread_nr;\n\tint\t\t\ttask_nr;\n\tunsigned int\t\tloops_done;\n\tu64\t\t\tval;\n\tu64\t\t\truntime_ns;\n\tu64\t\t\tsystem_time_ns;\n\tu64\t\t\tuser_time_ns;\n\tdouble\t\t\tspeed_gbs;\n\tstruct mutex\t\t*process_lock;\n};\n\n \n\nstruct params {\n\t \n\tbool\t\t\tserialize_startup;\n\n\t \n\tint\t\t\tnr_proc;\n\tint\t\t\tnr_threads;\n\n\t \n\tconst char\t\t*mb_global_str;\n\tconst char\t\t*mb_proc_str;\n\tconst char\t\t*mb_proc_locked_str;\n\tconst char\t\t*mb_thread_str;\n\n\tdouble\t\t\tmb_global;\n\tdouble\t\t\tmb_proc;\n\tdouble\t\t\tmb_proc_locked;\n\tdouble\t\t\tmb_thread;\n\n\t \n\tbool\t\t\tdata_reads;\n\tbool\t\t\tdata_writes;\n\tbool\t\t\tdata_backwards;\n\tbool\t\t\tdata_zero_memset;\n\tbool\t\t\tdata_rand_walk;\n\tu32\t\t\tnr_loops;\n\tu32\t\t\tnr_secs;\n\tu32\t\t\tsleep_usecs;\n\n\t \n\tbool\t\t\tinit_zero;\n\tbool\t\t\tinit_random;\n\tbool\t\t\tinit_cpu0;\n\n\t \n\tint\t\t\tshow_details;\n\tint\t\t\trun_all;\n\tint\t\t\tthp;\n\n\tlong\t\t\tbytes_global;\n\tlong\t\t\tbytes_process;\n\tlong\t\t\tbytes_process_locked;\n\tlong\t\t\tbytes_thread;\n\n\tint\t\t\tnr_tasks;\n\n\tbool\t\t\tshow_convergence;\n\tbool\t\t\tmeasure_convergence;\n\n\tint\t\t\tperturb_secs;\n\tint\t\t\tnr_cpus;\n\tint\t\t\tnr_nodes;\n\n\t \n\tchar\t\t\t*cpu_list_str;\n\tchar\t\t\t*node_list_str;\n};\n\n\n \n\nstruct global_info {\n\tu8\t\t\t*data;\n\n\tstruct mutex\t\tstartup_mutex;\n\tstruct cond\t\tstartup_cond;\n\tint\t\t\tnr_tasks_started;\n\n\tstruct mutex\t\tstart_work_mutex;\n\tstruct cond\t\tstart_work_cond;\n\tint\t\t\tnr_tasks_working;\n\tbool\t\t\tstart_work;\n\n\tstruct mutex\t\tstop_work_mutex;\n\tu64\t\t\tbytes_done;\n\n\tstruct thread_data\t*threads;\n\n\t \n\tbool\t\t\tall_converged;\n\tbool\t\t\tstop_work;\n\n\tint\t\t\tprint_once;\n\n\tstruct params\t\tp;\n};\n\nstatic struct global_info\t*g = NULL;\n\nstatic int parse_cpus_opt(const struct option *opt, const char *arg, int unset);\nstatic int parse_nodes_opt(const struct option *opt, const char *arg, int unset);\n\nstruct params p0;\n\nstatic const struct option options[] = {\n\tOPT_INTEGER('p', \"nr_proc\"\t, &p0.nr_proc,\t\t\"number of processes\"),\n\tOPT_INTEGER('t', \"nr_threads\"\t, &p0.nr_threads,\t\"number of threads per process\"),\n\n\tOPT_STRING('G', \"mb_global\"\t, &p0.mb_global_str,\t\"MB\", \"global  memory (MBs)\"),\n\tOPT_STRING('P', \"mb_proc\"\t, &p0.mb_proc_str,\t\"MB\", \"process memory (MBs)\"),\n\tOPT_STRING('L', \"mb_proc_locked\", &p0.mb_proc_locked_str,\"MB\", \"process serialized/locked memory access (MBs), <= process_memory\"),\n\tOPT_STRING('T', \"mb_thread\"\t, &p0.mb_thread_str,\t\"MB\", \"thread  memory (MBs)\"),\n\n\tOPT_UINTEGER('l', \"nr_loops\"\t, &p0.nr_loops,\t\t\"max number of loops to run (default: unlimited)\"),\n\tOPT_UINTEGER('s', \"nr_secs\"\t, &p0.nr_secs,\t\t\"max number of seconds to run (default: 5 secs)\"),\n\tOPT_UINTEGER('u', \"usleep\"\t, &p0.sleep_usecs,\t\"usecs to sleep per loop iteration\"),\n\n\tOPT_BOOLEAN('R', \"data_reads\"\t, &p0.data_reads,\t\"access the data via reads (can be mixed with -W)\"),\n\tOPT_BOOLEAN('W', \"data_writes\"\t, &p0.data_writes,\t\"access the data via writes (can be mixed with -R)\"),\n\tOPT_BOOLEAN('B', \"data_backwards\", &p0.data_backwards,\t\"access the data backwards as well\"),\n\tOPT_BOOLEAN('Z', \"data_zero_memset\", &p0.data_zero_memset,\"access the data via glibc bzero only\"),\n\tOPT_BOOLEAN('r', \"data_rand_walk\", &p0.data_rand_walk,\t\"access the data with random (32bit LFSR) walk\"),\n\n\n\tOPT_BOOLEAN('z', \"init_zero\"\t, &p0.init_zero,\t\"bzero the initial allocations\"),\n\tOPT_BOOLEAN('I', \"init_random\"\t, &p0.init_random,\t\"randomize the contents of the initial allocations\"),\n\tOPT_BOOLEAN('0', \"init_cpu0\"\t, &p0.init_cpu0,\t\"do the initial allocations on CPU#0\"),\n\tOPT_INTEGER('x', \"perturb_secs\", &p0.perturb_secs,\t\"perturb thread 0/0 every X secs, to test convergence stability\"),\n\n\tOPT_INCR   ('d', \"show_details\"\t, &p0.show_details,\t\"Show details\"),\n\tOPT_INCR   ('a', \"all\"\t\t, &p0.run_all,\t\t\"Run all tests in the suite\"),\n\tOPT_INTEGER('H', \"thp\"\t\t, &p0.thp,\t\t\"MADV_NOHUGEPAGE < 0 < MADV_HUGEPAGE\"),\n\tOPT_BOOLEAN('c', \"show_convergence\", &p0.show_convergence, \"show convergence details, \"\n\t\t    \"convergence is reached when each process (all its threads) is running on a single NUMA node.\"),\n\tOPT_BOOLEAN('m', \"measure_convergence\",\t&p0.measure_convergence, \"measure convergence latency\"),\n\tOPT_BOOLEAN('q', \"quiet\"\t, &quiet,\n\t\t    \"quiet mode (do not show any warnings or messages)\"),\n\tOPT_BOOLEAN('S', \"serialize-startup\", &p0.serialize_startup,\"serialize thread startup\"),\n\n\t \n        OPT_CALLBACK('C', \"cpus\", NULL, \"cpu[,cpu2,...cpuN]\",\n\t\t\t\"bind the first N tasks to these specific cpus (the rest is unbound)\",\n\t\t\tparse_cpus_opt),\n        OPT_CALLBACK('M', \"memnodes\", NULL, \"node[,node2,...nodeN]\",\n\t\t\t\"bind the first N tasks to these specific memory nodes (the rest is unbound)\",\n\t\t\tparse_nodes_opt),\n\tOPT_END()\n};\n\nstatic const char * const bench_numa_usage[] = {\n\t\"perf bench numa <options>\",\n\tNULL\n};\n\nstatic const char * const numa_usage[] = {\n\t\"perf bench numa mem [<options>]\",\n\tNULL\n};\n\n \nstatic int nr_numa_nodes(void)\n{\n\tint i, nr_nodes = 0;\n\n\tfor (i = 0; i < g->p.nr_nodes; i++) {\n\t\tif (numa_bitmask_isbitset(numa_nodes_ptr, i))\n\t\t\tnr_nodes++;\n\t}\n\n\treturn nr_nodes;\n}\n\n \nstatic int is_node_present(int node)\n{\n\treturn numa_bitmask_isbitset(numa_nodes_ptr, node);\n}\n\n \nstatic bool node_has_cpus(int node)\n{\n\tstruct bitmask *cpumask = numa_allocate_cpumask();\n\tbool ret = false;  \n\tint cpu;\n\n\tBUG_ON(!cpumask);\n\tif (!numa_node_to_cpus(node, cpumask)) {\n\t\tfor (cpu = 0; cpu < (int)cpumask->size; cpu++) {\n\t\t\tif (numa_bitmask_isbitset(cpumask, cpu)) {\n\t\t\t\tret = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tnuma_free_cpumask(cpumask);\n\n\treturn ret;\n}\n\nstatic cpu_set_t *bind_to_cpu(int target_cpu)\n{\n\tint nrcpus = numa_num_possible_cpus();\n\tcpu_set_t *orig_mask, *mask;\n\tsize_t size;\n\n\torig_mask = CPU_ALLOC(nrcpus);\n\tBUG_ON(!orig_mask);\n\tsize = CPU_ALLOC_SIZE(nrcpus);\n\tCPU_ZERO_S(size, orig_mask);\n\n\tif (sched_getaffinity(0, size, orig_mask))\n\t\tgoto err_out;\n\n\tmask = CPU_ALLOC(nrcpus);\n\tif (!mask)\n\t\tgoto err_out;\n\n\tCPU_ZERO_S(size, mask);\n\n\tif (target_cpu == -1) {\n\t\tint cpu;\n\n\t\tfor (cpu = 0; cpu < g->p.nr_cpus; cpu++)\n\t\t\tCPU_SET_S(cpu, size, mask);\n\t} else {\n\t\tif (target_cpu < 0 || target_cpu >= g->p.nr_cpus)\n\t\t\tgoto err;\n\n\t\tCPU_SET_S(target_cpu, size, mask);\n\t}\n\n\tif (sched_setaffinity(0, size, mask))\n\t\tgoto err;\n\n\treturn orig_mask;\n\nerr:\n\tCPU_FREE(mask);\nerr_out:\n\tCPU_FREE(orig_mask);\n\n\t \n\tBUG_ON(-1);\n\treturn NULL;\n}\n\nstatic cpu_set_t *bind_to_node(int target_node)\n{\n\tint nrcpus = numa_num_possible_cpus();\n\tsize_t size;\n\tcpu_set_t *orig_mask, *mask;\n\tint cpu;\n\n\torig_mask = CPU_ALLOC(nrcpus);\n\tBUG_ON(!orig_mask);\n\tsize = CPU_ALLOC_SIZE(nrcpus);\n\tCPU_ZERO_S(size, orig_mask);\n\n\tif (sched_getaffinity(0, size, orig_mask))\n\t\tgoto err_out;\n\n\tmask = CPU_ALLOC(nrcpus);\n\tif (!mask)\n\t\tgoto err_out;\n\n\tCPU_ZERO_S(size, mask);\n\n\tif (target_node == NUMA_NO_NODE) {\n\t\tfor (cpu = 0; cpu < g->p.nr_cpus; cpu++)\n\t\t\tCPU_SET_S(cpu, size, mask);\n\t} else {\n\t\tstruct bitmask *cpumask = numa_allocate_cpumask();\n\n\t\tif (!cpumask)\n\t\t\tgoto err;\n\n\t\tif (!numa_node_to_cpus(target_node, cpumask)) {\n\t\t\tfor (cpu = 0; cpu < (int)cpumask->size; cpu++) {\n\t\t\t\tif (numa_bitmask_isbitset(cpumask, cpu))\n\t\t\t\t\tCPU_SET_S(cpu, size, mask);\n\t\t\t}\n\t\t}\n\t\tnuma_free_cpumask(cpumask);\n\t}\n\n\tif (sched_setaffinity(0, size, mask))\n\t\tgoto err;\n\n\treturn orig_mask;\n\nerr:\n\tCPU_FREE(mask);\nerr_out:\n\tCPU_FREE(orig_mask);\n\n\t \n\tBUG_ON(-1);\n\treturn NULL;\n}\n\nstatic void bind_to_cpumask(cpu_set_t *mask)\n{\n\tint ret;\n\tsize_t size = CPU_ALLOC_SIZE(numa_num_possible_cpus());\n\n\tret = sched_setaffinity(0, size, mask);\n\tif (ret) {\n\t\tCPU_FREE(mask);\n\t\tBUG_ON(ret);\n\t}\n}\n\nstatic void mempol_restore(void)\n{\n\tint ret;\n\n\tret = set_mempolicy(MPOL_DEFAULT, NULL, g->p.nr_nodes-1);\n\n\tBUG_ON(ret);\n}\n\nstatic void bind_to_memnode(int node)\n{\n\tstruct bitmask *node_mask;\n\tint ret;\n\n\tif (node == NUMA_NO_NODE)\n\t\treturn;\n\n\tnode_mask = numa_allocate_nodemask();\n\tBUG_ON(!node_mask);\n\n\tnuma_bitmask_clearall(node_mask);\n\tnuma_bitmask_setbit(node_mask, node);\n\n\tret = set_mempolicy(MPOL_BIND, node_mask->maskp, node_mask->size + 1);\n\tdprintf(\"binding to node %d, mask: %016lx => %d\\n\", node, *node_mask->maskp, ret);\n\n\tnuma_bitmask_free(node_mask);\n\tBUG_ON(ret);\n}\n\n#define HPSIZE (2*1024*1024)\n\n#define set_taskname(fmt...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tchar name[20];\t\t\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tsnprintf(name, 20, fmt);\t\t\t\\\n\tprctl(PR_SET_NAME, name);\t\t\t\\\n} while (0)\n\nstatic u8 *alloc_data(ssize_t bytes0, int map_flags,\n\t\t      int init_zero, int init_cpu0, int thp, int init_random)\n{\n\tcpu_set_t *orig_mask = NULL;\n\tssize_t bytes;\n\tu8 *buf;\n\tint ret;\n\n\tif (!bytes0)\n\t\treturn NULL;\n\n\t \n\tif (init_cpu0) {\n\t\tint node = numa_node_of_cpu(0);\n\n\t\torig_mask = bind_to_node(node);\n\t\tbind_to_memnode(node);\n\t}\n\n\tbytes = bytes0 + HPSIZE;\n\n\tbuf = (void *)mmap(0, bytes, PROT_READ|PROT_WRITE, MAP_ANON|map_flags, -1, 0);\n\tBUG_ON(buf == (void *)-1);\n\n\tif (map_flags == MAP_PRIVATE) {\n\t\tif (thp > 0) {\n\t\t\tret = madvise(buf, bytes, MADV_HUGEPAGE);\n\t\t\tif (ret && !g->print_once) {\n\t\t\t\tg->print_once = 1;\n\t\t\t\tprintf(\"WARNING: Could not enable THP - do: 'echo madvise > /sys/kernel/mm/transparent_hugepage/enabled'\\n\");\n\t\t\t}\n\t\t}\n\t\tif (thp < 0) {\n\t\t\tret = madvise(buf, bytes, MADV_NOHUGEPAGE);\n\t\t\tif (ret && !g->print_once) {\n\t\t\t\tg->print_once = 1;\n\t\t\t\tprintf(\"WARNING: Could not disable THP: run a CONFIG_TRANSPARENT_HUGEPAGE kernel?\\n\");\n\t\t\t}\n\t\t}\n\t}\n\n\tif (init_zero) {\n\t\tbzero(buf, bytes);\n\t} else {\n\t\t \n\t\tif (init_random) {\n\t\t\tu64 *wbuf = (void *)buf;\n\t\t\tlong off = rand();\n\t\t\tlong i;\n\n\t\t\tfor (i = 0; i < bytes/8; i++)\n\t\t\t\twbuf[i] = i + off;\n\t\t}\n\t}\n\n\t \n\tbuf = (void *)(((unsigned long)buf + HPSIZE-1) & ~(HPSIZE-1));\n\n\t \n\tif (init_cpu0) {\n\t\tbind_to_cpumask(orig_mask);\n\t\tCPU_FREE(orig_mask);\n\t\tmempol_restore();\n\t}\n\n\treturn buf;\n}\n\nstatic void free_data(void *data, ssize_t bytes)\n{\n\tint ret;\n\n\tif (!data)\n\t\treturn;\n\n\tret = munmap(data, bytes);\n\tBUG_ON(ret);\n}\n\n \nstatic void * zalloc_shared_data(ssize_t bytes)\n{\n\treturn alloc_data(bytes, MAP_SHARED, 1, g->p.init_cpu0,  g->p.thp, g->p.init_random);\n}\n\n \nstatic void * setup_shared_data(ssize_t bytes)\n{\n\treturn alloc_data(bytes, MAP_SHARED, 0, g->p.init_cpu0,  g->p.thp, g->p.init_random);\n}\n\n \nstatic void * setup_private_data(ssize_t bytes)\n{\n\treturn alloc_data(bytes, MAP_PRIVATE, 0, g->p.init_cpu0,  g->p.thp, g->p.init_random);\n}\n\nstatic int parse_cpu_list(const char *arg)\n{\n\tp0.cpu_list_str = strdup(arg);\n\n\tdprintf(\"got CPU list: {%s}\\n\", p0.cpu_list_str);\n\n\treturn 0;\n}\n\nstatic int parse_setup_cpu_list(void)\n{\n\tstruct thread_data *td;\n\tchar *str0, *str;\n\tint t;\n\n\tif (!g->p.cpu_list_str)\n\t\treturn 0;\n\n\tdprintf(\"g->p.nr_tasks: %d\\n\", g->p.nr_tasks);\n\n\tstr0 = str = strdup(g->p.cpu_list_str);\n\tt = 0;\n\n\tBUG_ON(!str);\n\n\ttprintf(\"# binding tasks to CPUs:\\n\");\n\ttprintf(\"#  \");\n\n\twhile (true) {\n\t\tint bind_cpu, bind_cpu_0, bind_cpu_1;\n\t\tchar *tok, *tok_end, *tok_step, *tok_len, *tok_mul;\n\t\tint bind_len;\n\t\tint step;\n\t\tint mul;\n\n\t\ttok = strsep(&str, \",\");\n\t\tif (!tok)\n\t\t\tbreak;\n\n\t\ttok_end = strstr(tok, \"-\");\n\n\t\tdprintf(\"\\ntoken: {%s}, end: {%s}\\n\", tok, tok_end);\n\t\tif (!tok_end) {\n\t\t\t \n\t\t\tbind_cpu_0 = bind_cpu_1 = atol(tok);\n\t\t} else {\n\t\t\t \n\t\t\tbind_cpu_0 = atol(tok);\n\t\t\tbind_cpu_1 = atol(tok_end + 1);\n\t\t}\n\n\t\tstep = 1;\n\t\ttok_step = strstr(tok, \"#\");\n\t\tif (tok_step) {\n\t\t\tstep = atol(tok_step + 1);\n\t\t\tBUG_ON(step <= 0 || step >= g->p.nr_cpus);\n\t\t}\n\n\t\t \n\t\tbind_len = 1;\n\t\ttok_len = strstr(tok, \"_\");\n\t\tif (tok_len) {\n\t\t\tbind_len = atol(tok_len + 1);\n\t\t\tBUG_ON(bind_len <= 0 || bind_len > g->p.nr_cpus);\n\t\t}\n\n\t\t \n\t\tmul = 1;\n\t\ttok_mul = strstr(tok, \"x\");\n\t\tif (tok_mul) {\n\t\t\tmul = atol(tok_mul + 1);\n\t\t\tBUG_ON(mul <= 0);\n\t\t}\n\n\t\tdprintf(\"CPUs: %d_%d-%d#%dx%d\\n\", bind_cpu_0, bind_len, bind_cpu_1, step, mul);\n\n\t\tif (bind_cpu_0 >= g->p.nr_cpus || bind_cpu_1 >= g->p.nr_cpus) {\n\t\t\tprintf(\"\\nTest not applicable, system has only %d CPUs.\\n\", g->p.nr_cpus);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (is_cpu_online(bind_cpu_0) != 1 || is_cpu_online(bind_cpu_1) != 1) {\n\t\t\tprintf(\"\\nTest not applicable, bind_cpu_0 or bind_cpu_1 is offline\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tBUG_ON(bind_cpu_0 < 0 || bind_cpu_1 < 0);\n\t\tBUG_ON(bind_cpu_0 > bind_cpu_1);\n\n\t\tfor (bind_cpu = bind_cpu_0; bind_cpu <= bind_cpu_1; bind_cpu += step) {\n\t\t\tsize_t size = CPU_ALLOC_SIZE(g->p.nr_cpus);\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < mul; i++) {\n\t\t\t\tint cpu;\n\n\t\t\t\tif (t >= g->p.nr_tasks) {\n\t\t\t\t\tprintf(\"\\n# NOTE: ignoring bind CPUs starting at CPU#%d\\n #\", bind_cpu);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\ttd = g->threads + t;\n\n\t\t\t\tif (t)\n\t\t\t\t\ttprintf(\",\");\n\t\t\t\tif (bind_len > 1) {\n\t\t\t\t\ttprintf(\"%2d/%d\", bind_cpu, bind_len);\n\t\t\t\t} else {\n\t\t\t\t\ttprintf(\"%2d\", bind_cpu);\n\t\t\t\t}\n\n\t\t\t\ttd->bind_cpumask = CPU_ALLOC(g->p.nr_cpus);\n\t\t\t\tBUG_ON(!td->bind_cpumask);\n\t\t\t\tCPU_ZERO_S(size, td->bind_cpumask);\n\t\t\t\tfor (cpu = bind_cpu; cpu < bind_cpu+bind_len; cpu++) {\n\t\t\t\t\tif (cpu < 0 || cpu >= g->p.nr_cpus) {\n\t\t\t\t\t\tCPU_FREE(td->bind_cpumask);\n\t\t\t\t\t\tBUG_ON(-1);\n\t\t\t\t\t}\n\t\t\t\t\tCPU_SET_S(cpu, size, td->bind_cpumask);\n\t\t\t\t}\n\t\t\t\tt++;\n\t\t\t}\n\t\t}\n\t}\nout:\n\n\ttprintf(\"\\n\");\n\n\tif (t < g->p.nr_tasks)\n\t\tprintf(\"# NOTE: %d tasks bound, %d tasks unbound\\n\", t, g->p.nr_tasks - t);\n\n\tfree(str0);\n\treturn 0;\n}\n\nstatic int parse_cpus_opt(const struct option *opt __maybe_unused,\n\t\t\t  const char *arg, int unset __maybe_unused)\n{\n\tif (!arg)\n\t\treturn -1;\n\n\treturn parse_cpu_list(arg);\n}\n\nstatic int parse_node_list(const char *arg)\n{\n\tp0.node_list_str = strdup(arg);\n\n\tdprintf(\"got NODE list: {%s}\\n\", p0.node_list_str);\n\n\treturn 0;\n}\n\nstatic int parse_setup_node_list(void)\n{\n\tstruct thread_data *td;\n\tchar *str0, *str;\n\tint t;\n\n\tif (!g->p.node_list_str)\n\t\treturn 0;\n\n\tdprintf(\"g->p.nr_tasks: %d\\n\", g->p.nr_tasks);\n\n\tstr0 = str = strdup(g->p.node_list_str);\n\tt = 0;\n\n\tBUG_ON(!str);\n\n\ttprintf(\"# binding tasks to NODEs:\\n\");\n\ttprintf(\"# \");\n\n\twhile (true) {\n\t\tint bind_node, bind_node_0, bind_node_1;\n\t\tchar *tok, *tok_end, *tok_step, *tok_mul;\n\t\tint step;\n\t\tint mul;\n\n\t\ttok = strsep(&str, \",\");\n\t\tif (!tok)\n\t\t\tbreak;\n\n\t\ttok_end = strstr(tok, \"-\");\n\n\t\tdprintf(\"\\ntoken: {%s}, end: {%s}\\n\", tok, tok_end);\n\t\tif (!tok_end) {\n\t\t\t \n\t\t\tbind_node_0 = bind_node_1 = atol(tok);\n\t\t} else {\n\t\t\t \n\t\t\tbind_node_0 = atol(tok);\n\t\t\tbind_node_1 = atol(tok_end + 1);\n\t\t}\n\n\t\tstep = 1;\n\t\ttok_step = strstr(tok, \"#\");\n\t\tif (tok_step) {\n\t\t\tstep = atol(tok_step + 1);\n\t\t\tBUG_ON(step <= 0 || step >= g->p.nr_nodes);\n\t\t}\n\n\t\t \n\t\tmul = 1;\n\t\ttok_mul = strstr(tok, \"x\");\n\t\tif (tok_mul) {\n\t\t\tmul = atol(tok_mul + 1);\n\t\t\tBUG_ON(mul <= 0);\n\t\t}\n\n\t\tdprintf(\"NODEs: %d-%d #%d\\n\", bind_node_0, bind_node_1, step);\n\n\t\tif (bind_node_0 >= g->p.nr_nodes || bind_node_1 >= g->p.nr_nodes) {\n\t\t\tprintf(\"\\nTest not applicable, system has only %d nodes.\\n\", g->p.nr_nodes);\n\t\t\treturn -1;\n\t\t}\n\n\t\tBUG_ON(bind_node_0 < 0 || bind_node_1 < 0);\n\t\tBUG_ON(bind_node_0 > bind_node_1);\n\n\t\tfor (bind_node = bind_node_0; bind_node <= bind_node_1; bind_node += step) {\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < mul; i++) {\n\t\t\t\tif (t >= g->p.nr_tasks || !node_has_cpus(bind_node)) {\n\t\t\t\t\tprintf(\"\\n# NOTE: ignoring bind NODEs starting at NODE#%d\\n\", bind_node);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\ttd = g->threads + t;\n\n\t\t\t\tif (!t)\n\t\t\t\t\ttprintf(\" %2d\", bind_node);\n\t\t\t\telse\n\t\t\t\t\ttprintf(\",%2d\", bind_node);\n\n\t\t\t\ttd->bind_node = bind_node;\n\t\t\t\tt++;\n\t\t\t}\n\t\t}\n\t}\nout:\n\n\ttprintf(\"\\n\");\n\n\tif (t < g->p.nr_tasks)\n\t\tprintf(\"# NOTE: %d tasks mem-bound, %d tasks unbound\\n\", t, g->p.nr_tasks - t);\n\n\tfree(str0);\n\treturn 0;\n}\n\nstatic int parse_nodes_opt(const struct option *opt __maybe_unused,\n\t\t\t  const char *arg, int unset __maybe_unused)\n{\n\tif (!arg)\n\t\treturn -1;\n\n\treturn parse_node_list(arg);\n}\n\nstatic inline uint32_t lfsr_32(uint32_t lfsr)\n{\n\tconst uint32_t taps = BIT(1) | BIT(5) | BIT(6) | BIT(31);\n\treturn (lfsr>>1) ^ ((0x0u - (lfsr & 0x1u)) & taps);\n}\n\n \nstatic inline u64 access_data(u64 *data, u64 val)\n{\n\tif (g->p.data_reads)\n\t\tval += *data;\n\tif (g->p.data_writes)\n\t\t*data = val + 1;\n\treturn val;\n}\n\n \nstatic u64 do_work(u8 *__data, long bytes, int nr, int nr_max, int loop, u64 val)\n{\n\tlong words = bytes/sizeof(u64);\n\tu64 *data = (void *)__data;\n\tlong chunk_0, chunk_1;\n\tu64 *d0, *d, *d1;\n\tlong off;\n\tlong i;\n\n\tBUG_ON(!data && words);\n\tBUG_ON(data && !words);\n\n\tif (!data)\n\t\treturn val;\n\n\t \n\tif (g->p.data_zero_memset && !g->p.data_rand_walk) {\n\t\tbzero(data, bytes);\n\t\treturn val;\n\t}\n\n\t \n\tchunk_0 = words/nr_max;\n\tchunk_1 = words/g->p.nr_loops;\n\toff = nr*chunk_0 + loop*chunk_1;\n\n\twhile (off >= words)\n\t\toff -= words;\n\n\tif (g->p.data_rand_walk) {\n\t\tu32 lfsr = nr + loop + val;\n\t\tlong j;\n\n\t\tfor (i = 0; i < words/1024; i++) {\n\t\t\tlong start, end;\n\n\t\t\tlfsr = lfsr_32(lfsr);\n\n\t\t\tstart = lfsr % words;\n\t\t\tend = min(start + 1024, words-1);\n\n\t\t\tif (g->p.data_zero_memset) {\n\t\t\t\tbzero(data + start, (end-start) * sizeof(u64));\n\t\t\t} else {\n\t\t\t\tfor (j = start; j < end; j++)\n\t\t\t\t\tval = access_data(data + j, val);\n\t\t\t}\n\t\t}\n\t} else if (!g->p.data_backwards || (nr + loop) & 1) {\n\t\t \n\n\t\td0 = data + off;\n\t\td  = data + off + 1;\n\t\td1 = data + words;\n\n\t\tfor (;;) {\n\t\t\tif (unlikely(d >= d1))\n\t\t\t\td = data;\n\t\t\tif (unlikely(d == d0))\n\t\t\t\tbreak;\n\n\t\t\tval = access_data(d, val);\n\n\t\t\td++;\n\t\t}\n\t} else {\n\t\t \n\n\t\td0 = data + off;\n\t\td  = data + off - 1;\n\t\td1 = data + words;\n\n\t\tfor (;;) {\n\t\t\tif (unlikely(d < data))\n\t\t\t\td = data + words-1;\n\t\t\tif (unlikely(d == d0))\n\t\t\t\tbreak;\n\n\t\t\tval = access_data(d, val);\n\n\t\t\td--;\n\t\t}\n\t}\n\n\treturn val;\n}\n\nstatic void update_curr_cpu(int task_nr, unsigned long bytes_worked)\n{\n\tunsigned int cpu;\n\n\tcpu = sched_getcpu();\n\n\tg->threads[task_nr].curr_cpu = cpu;\n\tprctl(0, bytes_worked);\n}\n\n \nstatic int count_process_nodes(int process_nr)\n{\n\tchar *node_present;\n\tint nodes;\n\tint n, t;\n\n\tnode_present = (char *)malloc(g->p.nr_nodes * sizeof(char));\n\tBUG_ON(!node_present);\n\tfor (nodes = 0; nodes < g->p.nr_nodes; nodes++)\n\t\tnode_present[nodes] = 0;\n\n\tfor (t = 0; t < g->p.nr_threads; t++) {\n\t\tstruct thread_data *td;\n\t\tint task_nr;\n\t\tint node;\n\n\t\ttask_nr = process_nr*g->p.nr_threads + t;\n\t\ttd = g->threads + task_nr;\n\n\t\tnode = numa_node_of_cpu(td->curr_cpu);\n\t\tif (node < 0)   {\n\t\t\tfree(node_present);\n\t\t\treturn 0;\n\t\t}\n\n\t\tnode_present[node] = 1;\n\t}\n\n\tnodes = 0;\n\n\tfor (n = 0; n < g->p.nr_nodes; n++)\n\t\tnodes += node_present[n];\n\n\tfree(node_present);\n\treturn nodes;\n}\n\n \nstatic int count_node_processes(int node)\n{\n\tint processes = 0;\n\tint t, p;\n\n\tfor (p = 0; p < g->p.nr_proc; p++) {\n\t\tfor (t = 0; t < g->p.nr_threads; t++) {\n\t\t\tstruct thread_data *td;\n\t\t\tint task_nr;\n\t\t\tint n;\n\n\t\t\ttask_nr = p*g->p.nr_threads + t;\n\t\t\ttd = g->threads + task_nr;\n\n\t\t\tn = numa_node_of_cpu(td->curr_cpu);\n\t\t\tif (n == node) {\n\t\t\t\tprocesses++;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn processes;\n}\n\nstatic void calc_convergence_compression(int *strong)\n{\n\tunsigned int nodes_min, nodes_max;\n\tint p;\n\n\tnodes_min = -1;\n\tnodes_max =  0;\n\n\tfor (p = 0; p < g->p.nr_proc; p++) {\n\t\tunsigned int nodes = count_process_nodes(p);\n\n\t\tif (!nodes) {\n\t\t\t*strong = 0;\n\t\t\treturn;\n\t\t}\n\n\t\tnodes_min = min(nodes, nodes_min);\n\t\tnodes_max = max(nodes, nodes_max);\n\t}\n\n\t \n\tif (nodes_min == 1 && nodes_max == 1) {\n\t\t*strong = 1;\n\t} else {\n\t\t*strong = 0;\n\t\ttprintf(\" {%d-%d}\", nodes_min, nodes_max);\n\t}\n}\n\nstatic void calc_convergence(double runtime_ns_max, double *convergence)\n{\n\tunsigned int loops_done_min, loops_done_max;\n\tint process_groups;\n\tint *nodes;\n\tint distance;\n\tint nr_min;\n\tint nr_max;\n\tint strong;\n\tint sum;\n\tint nr;\n\tint node;\n\tint cpu;\n\tint t;\n\n\tif (!g->p.show_convergence && !g->p.measure_convergence)\n\t\treturn;\n\n\tnodes = (int *)malloc(g->p.nr_nodes * sizeof(int));\n\tBUG_ON(!nodes);\n\tfor (node = 0; node < g->p.nr_nodes; node++)\n\t\tnodes[node] = 0;\n\n\tloops_done_min = -1;\n\tloops_done_max = 0;\n\n\tfor (t = 0; t < g->p.nr_tasks; t++) {\n\t\tstruct thread_data *td = g->threads + t;\n\t\tunsigned int loops_done;\n\n\t\tcpu = td->curr_cpu;\n\n\t\t \n\t\tif (cpu < 0)\n\t\t\tcontinue;\n\n\t\tnode = numa_node_of_cpu(cpu);\n\n\t\tnodes[node]++;\n\n\t\tloops_done = td->loops_done;\n\t\tloops_done_min = min(loops_done, loops_done_min);\n\t\tloops_done_max = max(loops_done, loops_done_max);\n\t}\n\n\tnr_max = 0;\n\tnr_min = g->p.nr_tasks;\n\tsum = 0;\n\n\tfor (node = 0; node < g->p.nr_nodes; node++) {\n\t\tif (!is_node_present(node))\n\t\t\tcontinue;\n\t\tnr = nodes[node];\n\t\tnr_min = min(nr, nr_min);\n\t\tnr_max = max(nr, nr_max);\n\t\tsum += nr;\n\t}\n\tBUG_ON(nr_min > nr_max);\n\n\tBUG_ON(sum > g->p.nr_tasks);\n\n\tif (0 && (sum < g->p.nr_tasks)) {\n\t\tfree(nodes);\n\t\treturn;\n\t}\n\n\t \n\tprocess_groups = 0;\n\n\tfor (node = 0; node < g->p.nr_nodes; node++) {\n\t\tint processes;\n\n\t\tif (!is_node_present(node))\n\t\t\tcontinue;\n\t\tprocesses = count_node_processes(node);\n\t\tnr = nodes[node];\n\t\ttprintf(\" %2d/%-2d\", nr, processes);\n\n\t\tprocess_groups += processes;\n\t}\n\n\tdistance = nr_max - nr_min;\n\n\ttprintf(\" [%2d/%-2d]\", distance, process_groups);\n\n\ttprintf(\" l:%3d-%-3d (%3d)\",\n\t\tloops_done_min, loops_done_max, loops_done_max-loops_done_min);\n\n\tif (loops_done_min && loops_done_max) {\n\t\tdouble skew = 1.0 - (double)loops_done_min/loops_done_max;\n\n\t\ttprintf(\" [%4.1f%%]\", skew * 100.0);\n\t}\n\n\tcalc_convergence_compression(&strong);\n\n\tif (strong && process_groups == g->p.nr_proc) {\n\t\tif (!*convergence) {\n\t\t\t*convergence = runtime_ns_max;\n\t\t\ttprintf(\" (%6.1fs converged)\\n\", *convergence / NSEC_PER_SEC);\n\t\t\tif (g->p.measure_convergence) {\n\t\t\t\tg->all_converged = true;\n\t\t\t\tg->stop_work = true;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (*convergence) {\n\t\t\ttprintf(\" (%6.1fs de-converged)\", runtime_ns_max / NSEC_PER_SEC);\n\t\t\t*convergence = 0;\n\t\t}\n\t\ttprintf(\"\\n\");\n\t}\n\n\tfree(nodes);\n}\n\nstatic void show_summary(double runtime_ns_max, int l, double *convergence)\n{\n\ttprintf(\"\\r #  %5.1f%%  [%.1f mins]\",\n\t\t(double)(l+1)/g->p.nr_loops*100.0, runtime_ns_max / NSEC_PER_SEC / 60.0);\n\n\tcalc_convergence(runtime_ns_max, convergence);\n\n\tif (g->p.show_details >= 0)\n\t\tfflush(stdout);\n}\n\nstatic void *worker_thread(void *__tdata)\n{\n\tstruct thread_data *td = __tdata;\n\tstruct timeval start0, start, stop, diff;\n\tint process_nr = td->process_nr;\n\tint thread_nr = td->thread_nr;\n\tunsigned long last_perturbance;\n\tint task_nr = td->task_nr;\n\tint details = g->p.show_details;\n\tint first_task, last_task;\n\tdouble convergence = 0;\n\tu64 val = td->val;\n\tdouble runtime_ns_max;\n\tu8 *global_data;\n\tu8 *process_data;\n\tu8 *thread_data;\n\tu64 bytes_done, secs;\n\tlong work_done;\n\tu32 l;\n\tstruct rusage rusage;\n\n\tbind_to_cpumask(td->bind_cpumask);\n\tbind_to_memnode(td->bind_node);\n\n\tset_taskname(\"thread %d/%d\", process_nr, thread_nr);\n\n\tglobal_data = g->data;\n\tprocess_data = td->process_data;\n\tthread_data = setup_private_data(g->p.bytes_thread);\n\n\tbytes_done = 0;\n\n\tlast_task = 0;\n\tif (process_nr == g->p.nr_proc-1 && thread_nr == g->p.nr_threads-1)\n\t\tlast_task = 1;\n\n\tfirst_task = 0;\n\tif (process_nr == 0 && thread_nr == 0)\n\t\tfirst_task = 1;\n\n\tif (details >= 2) {\n\t\tprintf(\"#  thread %2d / %2d global mem: %p, process mem: %p, thread mem: %p\\n\",\n\t\t\tprocess_nr, thread_nr, global_data, process_data, thread_data);\n\t}\n\n\tif (g->p.serialize_startup) {\n\t\tmutex_lock(&g->startup_mutex);\n\t\tg->nr_tasks_started++;\n\t\t \n\t\tif (g->nr_tasks_started == g->p.nr_tasks)\n\t\t\tcond_signal(&g->startup_cond);\n\n\t\tmutex_unlock(&g->startup_mutex);\n\n\t\t \n\t\tmutex_lock(&g->start_work_mutex);\n\t\tg->start_work = false;\n\t\tg->nr_tasks_working++;\n\t\twhile (!g->start_work)\n\t\t\tcond_wait(&g->start_work_cond, &g->start_work_mutex);\n\n\t\tmutex_unlock(&g->start_work_mutex);\n\t}\n\n\tgettimeofday(&start0, NULL);\n\n\tstart = stop = start0;\n\tlast_perturbance = start.tv_sec;\n\n\tfor (l = 0; l < g->p.nr_loops; l++) {\n\t\tstart = stop;\n\n\t\tif (g->stop_work)\n\t\t\tbreak;\n\n\t\tval += do_work(global_data,  g->p.bytes_global,  process_nr, g->p.nr_proc,\tl, val);\n\t\tval += do_work(process_data, g->p.bytes_process, thread_nr,  g->p.nr_threads,\tl, val);\n\t\tval += do_work(thread_data,  g->p.bytes_thread,  0,          1,\t\tl, val);\n\n\t\tif (g->p.sleep_usecs) {\n\t\t\tmutex_lock(td->process_lock);\n\t\t\tusleep(g->p.sleep_usecs);\n\t\t\tmutex_unlock(td->process_lock);\n\t\t}\n\t\t \n\t\tif (g->p.bytes_process_locked) {\n\t\t\tmutex_lock(td->process_lock);\n\t\t\tval += do_work(process_data, g->p.bytes_process_locked, thread_nr,  g->p.nr_threads,\tl, val);\n\t\t\tmutex_unlock(td->process_lock);\n\t\t}\n\n\t\twork_done = g->p.bytes_global + g->p.bytes_process +\n\t\t\t    g->p.bytes_process_locked + g->p.bytes_thread;\n\n\t\tupdate_curr_cpu(task_nr, work_done);\n\t\tbytes_done += work_done;\n\n\t\tif (details < 0 && !g->p.perturb_secs && !g->p.measure_convergence && !g->p.nr_secs)\n\t\t\tcontinue;\n\n\t\ttd->loops_done = l;\n\n\t\tgettimeofday(&stop, NULL);\n\n\t\t \n\t\tif (g->p.nr_secs) {\n\t\t\ttimersub(&stop, &start0, &diff);\n\t\t\tif ((u32)diff.tv_sec >= g->p.nr_secs) {\n\t\t\t\tg->stop_work = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (start.tv_sec == stop.tv_sec)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (first_task && g->p.perturb_secs && (int)(stop.tv_sec - last_perturbance) >= g->p.perturb_secs) {\n\t\t\tcpu_set_t *orig_mask;\n\t\t\tint target_cpu;\n\t\t\tint this_cpu;\n\n\t\t\tlast_perturbance = stop.tv_sec;\n\n\t\t\t \n\t\t\tthis_cpu = g->threads[task_nr].curr_cpu;\n\t\t\tif (this_cpu < g->p.nr_cpus/2)\n\t\t\t\ttarget_cpu = g->p.nr_cpus-1;\n\t\t\telse\n\t\t\t\ttarget_cpu = 0;\n\n\t\t\torig_mask = bind_to_cpu(target_cpu);\n\n\t\t\t \n\t\t\tif (details >= 1)\n\t\t\t\tprintf(\" (injecting perturbalance, moved to CPU#%d)\\n\", target_cpu);\n\n\t\t\tbind_to_cpumask(orig_mask);\n\t\t\tCPU_FREE(orig_mask);\n\t\t}\n\n\t\tif (details >= 3) {\n\t\t\ttimersub(&stop, &start, &diff);\n\t\t\truntime_ns_max = diff.tv_sec * NSEC_PER_SEC;\n\t\t\truntime_ns_max += diff.tv_usec * NSEC_PER_USEC;\n\n\t\t\tif (details >= 0) {\n\t\t\t\tprintf(\" #%2d / %2d: %14.2lf nsecs/op [val: %016\"PRIx64\"]\\n\",\n\t\t\t\t\tprocess_nr, thread_nr, runtime_ns_max / bytes_done, val);\n\t\t\t}\n\t\t\tfflush(stdout);\n\t\t}\n\t\tif (!last_task)\n\t\t\tcontinue;\n\n\t\ttimersub(&stop, &start0, &diff);\n\t\truntime_ns_max = diff.tv_sec * NSEC_PER_SEC;\n\t\truntime_ns_max += diff.tv_usec * NSEC_PER_USEC;\n\n\t\tshow_summary(runtime_ns_max, l, &convergence);\n\t}\n\n\tgettimeofday(&stop, NULL);\n\ttimersub(&stop, &start0, &diff);\n\ttd->runtime_ns = diff.tv_sec * NSEC_PER_SEC;\n\ttd->runtime_ns += diff.tv_usec * NSEC_PER_USEC;\n\tsecs = td->runtime_ns / NSEC_PER_SEC;\n\ttd->speed_gbs = secs ? bytes_done / secs / 1e9 : 0;\n\n\tgetrusage(RUSAGE_THREAD, &rusage);\n\ttd->system_time_ns = rusage.ru_stime.tv_sec * NSEC_PER_SEC;\n\ttd->system_time_ns += rusage.ru_stime.tv_usec * NSEC_PER_USEC;\n\ttd->user_time_ns = rusage.ru_utime.tv_sec * NSEC_PER_SEC;\n\ttd->user_time_ns += rusage.ru_utime.tv_usec * NSEC_PER_USEC;\n\n\tfree_data(thread_data, g->p.bytes_thread);\n\n\tmutex_lock(&g->stop_work_mutex);\n\tg->bytes_done += bytes_done;\n\tmutex_unlock(&g->stop_work_mutex);\n\n\treturn NULL;\n}\n\n \nstatic void worker_process(int process_nr)\n{\n\tstruct mutex process_lock;\n\tstruct thread_data *td;\n\tpthread_t *pthreads;\n\tu8 *process_data;\n\tint task_nr;\n\tint ret;\n\tint t;\n\n\tmutex_init(&process_lock);\n\tset_taskname(\"process %d\", process_nr);\n\n\t \n\ttask_nr = process_nr*g->p.nr_threads;\n\ttd = g->threads + task_nr;\n\n\tbind_to_memnode(td->bind_node);\n\tbind_to_cpumask(td->bind_cpumask);\n\n\tpthreads = zalloc(g->p.nr_threads * sizeof(pthread_t));\n\tprocess_data = setup_private_data(g->p.bytes_process);\n\n\tif (g->p.show_details >= 3) {\n\t\tprintf(\" # process %2d global mem: %p, process mem: %p\\n\",\n\t\t\tprocess_nr, g->data, process_data);\n\t}\n\n\tfor (t = 0; t < g->p.nr_threads; t++) {\n\t\ttask_nr = process_nr*g->p.nr_threads + t;\n\t\ttd = g->threads + task_nr;\n\n\t\ttd->process_data = process_data;\n\t\ttd->process_nr   = process_nr;\n\t\ttd->thread_nr    = t;\n\t\ttd->task_nr\t = task_nr;\n\t\ttd->val          = rand();\n\t\ttd->curr_cpu\t = -1;\n\t\ttd->process_lock = &process_lock;\n\n\t\tret = pthread_create(pthreads + t, NULL, worker_thread, td);\n\t\tBUG_ON(ret);\n\t}\n\n\tfor (t = 0; t < g->p.nr_threads; t++) {\n                ret = pthread_join(pthreads[t], NULL);\n\t\tBUG_ON(ret);\n\t}\n\n\tfree_data(process_data, g->p.bytes_process);\n\tfree(pthreads);\n}\n\nstatic void print_summary(void)\n{\n\tif (g->p.show_details < 0)\n\t\treturn;\n\n\tprintf(\"\\n ###\\n\");\n\tprintf(\" # %d %s will execute (on %d nodes, %d CPUs):\\n\",\n\t\tg->p.nr_tasks, g->p.nr_tasks == 1 ? \"task\" : \"tasks\", nr_numa_nodes(), g->p.nr_cpus);\n\tprintf(\" #      %5dx %5ldMB global  shared mem operations\\n\",\n\t\t\tg->p.nr_loops, g->p.bytes_global/1024/1024);\n\tprintf(\" #      %5dx %5ldMB process shared mem operations\\n\",\n\t\t\tg->p.nr_loops, g->p.bytes_process/1024/1024);\n\tprintf(\" #      %5dx %5ldMB thread  local  mem operations\\n\",\n\t\t\tg->p.nr_loops, g->p.bytes_thread/1024/1024);\n\n\tprintf(\" ###\\n\");\n\n\tprintf(\"\\n ###\\n\"); fflush(stdout);\n}\n\nstatic void init_thread_data(void)\n{\n\tssize_t size = sizeof(*g->threads)*g->p.nr_tasks;\n\tint t;\n\n\tg->threads = zalloc_shared_data(size);\n\n\tfor (t = 0; t < g->p.nr_tasks; t++) {\n\t\tstruct thread_data *td = g->threads + t;\n\t\tsize_t cpuset_size = CPU_ALLOC_SIZE(g->p.nr_cpus);\n\t\tint cpu;\n\n\t\t \n\t\ttd->bind_node = NUMA_NO_NODE;\n\n\t\t \n\t\ttd->bind_cpumask = CPU_ALLOC(g->p.nr_cpus);\n\t\tBUG_ON(!td->bind_cpumask);\n\t\tCPU_ZERO_S(cpuset_size, td->bind_cpumask);\n\t\tfor (cpu = 0; cpu < g->p.nr_cpus; cpu++)\n\t\t\tCPU_SET_S(cpu, cpuset_size, td->bind_cpumask);\n\t}\n}\n\nstatic void deinit_thread_data(void)\n{\n\tssize_t size = sizeof(*g->threads)*g->p.nr_tasks;\n\tint t;\n\n\t \n\tfor (t = 0; t < g->p.nr_tasks; t++) {\n\t\tstruct thread_data *td = g->threads + t;\n\t\tCPU_FREE(td->bind_cpumask);\n\t}\n\n\tfree_data(g->threads, size);\n}\n\nstatic int init(void)\n{\n\tg = (void *)alloc_data(sizeof(*g), MAP_SHARED, 1, 0, 0  , 0);\n\n\t \n\tg->p = p0;\n\n\tg->p.nr_cpus = numa_num_configured_cpus();\n\n\tg->p.nr_nodes = numa_max_node() + 1;\n\n\t \n\tBUG_ON(g->p.nr_nodes < 0);\n\n\tif (quiet && !g->p.show_details)\n\t\tg->p.show_details = -1;\n\n\t \n\tif (!g->p.mb_global_str && !g->p.mb_proc_str && !g->p.mb_thread_str)\n\t\treturn -1;\n\n\tif (g->p.mb_global_str) {\n\t\tg->p.mb_global = atof(g->p.mb_global_str);\n\t\tBUG_ON(g->p.mb_global < 0);\n\t}\n\n\tif (g->p.mb_proc_str) {\n\t\tg->p.mb_proc = atof(g->p.mb_proc_str);\n\t\tBUG_ON(g->p.mb_proc < 0);\n\t}\n\n\tif (g->p.mb_proc_locked_str) {\n\t\tg->p.mb_proc_locked = atof(g->p.mb_proc_locked_str);\n\t\tBUG_ON(g->p.mb_proc_locked < 0);\n\t\tBUG_ON(g->p.mb_proc_locked > g->p.mb_proc);\n\t}\n\n\tif (g->p.mb_thread_str) {\n\t\tg->p.mb_thread = atof(g->p.mb_thread_str);\n\t\tBUG_ON(g->p.mb_thread < 0);\n\t}\n\n\tBUG_ON(g->p.nr_threads <= 0);\n\tBUG_ON(g->p.nr_proc <= 0);\n\n\tg->p.nr_tasks = g->p.nr_proc*g->p.nr_threads;\n\n\tg->p.bytes_global\t\t= g->p.mb_global\t*1024L*1024L;\n\tg->p.bytes_process\t\t= g->p.mb_proc\t\t*1024L*1024L;\n\tg->p.bytes_process_locked\t= g->p.mb_proc_locked\t*1024L*1024L;\n\tg->p.bytes_thread\t\t= g->p.mb_thread\t*1024L*1024L;\n\n\tg->data = setup_shared_data(g->p.bytes_global);\n\n\t \n\tmutex_init_pshared(&g->start_work_mutex);\n\tcond_init_pshared(&g->start_work_cond);\n\tmutex_init_pshared(&g->startup_mutex);\n\tcond_init_pshared(&g->startup_cond);\n\tmutex_init_pshared(&g->stop_work_mutex);\n\n\tinit_thread_data();\n\n\ttprintf(\"#\\n\");\n\tif (parse_setup_cpu_list() || parse_setup_node_list())\n\t\treturn -1;\n\ttprintf(\"#\\n\");\n\n\tprint_summary();\n\n\treturn 0;\n}\n\nstatic void deinit(void)\n{\n\tfree_data(g->data, g->p.bytes_global);\n\tg->data = NULL;\n\n\tdeinit_thread_data();\n\n\tfree_data(g, sizeof(*g));\n\tg = NULL;\n}\n\n \nstatic void print_res(const char *name, double val,\n\t\t      const char *txt_unit, const char *txt_short, const char *txt_long)\n{\n\tif (!name)\n\t\tname = \"main,\";\n\n\tif (!quiet)\n\t\tprintf(\" %-30s %15.3f, %-15s %s\\n\", name, val, txt_unit, txt_short);\n\telse\n\t\tprintf(\" %14.3f %s\\n\", val, txt_long);\n}\n\nstatic int __bench_numa(const char *name)\n{\n\tstruct timeval start, stop, diff;\n\tu64 runtime_ns_min, runtime_ns_sum;\n\tpid_t *pids, pid, wpid;\n\tdouble delta_runtime;\n\tdouble runtime_avg;\n\tdouble runtime_sec_max;\n\tdouble runtime_sec_min;\n\tint wait_stat;\n\tdouble bytes;\n\tint i, t, p;\n\n\tif (init())\n\t\treturn -1;\n\n\tpids = zalloc(g->p.nr_proc * sizeof(*pids));\n\tpid = -1;\n\n\tif (g->p.serialize_startup) {\n\t\ttprintf(\" #\\n\");\n\t\ttprintf(\" # Startup synchronization: ...\"); fflush(stdout);\n\t}\n\n\tgettimeofday(&start, NULL);\n\n\tfor (i = 0; i < g->p.nr_proc; i++) {\n\t\tpid = fork();\n\t\tdprintf(\" # process %2d: PID %d\\n\", i, pid);\n\n\t\tBUG_ON(pid < 0);\n\t\tif (!pid) {\n\t\t\t \n\t\t\tworker_process(i);\n\n\t\t\texit(0);\n\t\t}\n\t\tpids[i] = pid;\n\n\t}\n\n\tif (g->p.serialize_startup) {\n\t\tbool threads_ready = false;\n\t\tdouble startup_sec;\n\n\t\t \n\t\tmutex_lock(&g->startup_mutex);\n\t\twhile (g->nr_tasks_started != g->p.nr_tasks)\n\t\t\tcond_wait(&g->startup_cond, &g->startup_mutex);\n\n\t\tmutex_unlock(&g->startup_mutex);\n\n\t\t \n\t\twhile (!threads_ready) {\n\t\t\tmutex_lock(&g->start_work_mutex);\n\t\t\tthreads_ready = (g->nr_tasks_working == g->p.nr_tasks);\n\t\t\tmutex_unlock(&g->start_work_mutex);\n\t\t\tif (!threads_ready)\n\t\t\t\tusleep(1);\n\t\t}\n\n\t\tgettimeofday(&stop, NULL);\n\n\t\ttimersub(&stop, &start, &diff);\n\n\t\tstartup_sec = diff.tv_sec * NSEC_PER_SEC;\n\t\tstartup_sec += diff.tv_usec * NSEC_PER_USEC;\n\t\tstartup_sec /= NSEC_PER_SEC;\n\n\t\ttprintf(\" threads initialized in %.6f seconds.\\n\", startup_sec);\n\t\ttprintf(\" #\\n\");\n\n\t\tstart = stop;\n\t\t \n\t\tmutex_lock(&g->start_work_mutex);\n\t\tg->start_work = true;\n\t\tmutex_unlock(&g->start_work_mutex);\n\t\tcond_broadcast(&g->start_work_cond);\n\t} else {\n\t\tgettimeofday(&start, NULL);\n\t}\n\n\t \n\n\n\tfor (i = 0; i < g->p.nr_proc; i++) {\n\t\twpid = waitpid(pids[i], &wait_stat, 0);\n\t\tBUG_ON(wpid < 0);\n\t\tBUG_ON(!WIFEXITED(wait_stat));\n\n\t}\n\n\truntime_ns_sum = 0;\n\truntime_ns_min = -1LL;\n\n\tfor (t = 0; t < g->p.nr_tasks; t++) {\n\t\tu64 thread_runtime_ns = g->threads[t].runtime_ns;\n\n\t\truntime_ns_sum += thread_runtime_ns;\n\t\truntime_ns_min = min(thread_runtime_ns, runtime_ns_min);\n\t}\n\n\tgettimeofday(&stop, NULL);\n\ttimersub(&stop, &start, &diff);\n\n\tBUG_ON(bench_format != BENCH_FORMAT_DEFAULT);\n\n\ttprintf(\"\\n ###\\n\");\n\ttprintf(\"\\n\");\n\n\truntime_sec_max = diff.tv_sec * NSEC_PER_SEC;\n\truntime_sec_max += diff.tv_usec * NSEC_PER_USEC;\n\truntime_sec_max /= NSEC_PER_SEC;\n\n\truntime_sec_min = runtime_ns_min / NSEC_PER_SEC;\n\n\tbytes = g->bytes_done;\n\truntime_avg = (double)runtime_ns_sum / g->p.nr_tasks / NSEC_PER_SEC;\n\n\tif (g->p.measure_convergence) {\n\t\tprint_res(name, runtime_sec_max,\n\t\t\t\"secs,\", \"NUMA-convergence-latency\", \"secs latency to NUMA-converge\");\n\t}\n\n\tprint_res(name, runtime_sec_max,\n\t\t\"secs,\", \"runtime-max/thread\",\t\"secs slowest (max) thread-runtime\");\n\n\tprint_res(name, runtime_sec_min,\n\t\t\"secs,\", \"runtime-min/thread\",\t\"secs fastest (min) thread-runtime\");\n\n\tprint_res(name, runtime_avg,\n\t\t\"secs,\", \"runtime-avg/thread\",\t\"secs average thread-runtime\");\n\n\tdelta_runtime = (runtime_sec_max - runtime_sec_min)/2.0;\n\tprint_res(name, delta_runtime / runtime_sec_max * 100.0,\n\t\t\"%,\", \"spread-runtime/thread\",\t\"% difference between max/avg runtime\");\n\n\tprint_res(name, bytes / g->p.nr_tasks / 1e9,\n\t\t\"GB,\", \"data/thread\",\t\t\"GB data processed, per thread\");\n\n\tprint_res(name, bytes / 1e9,\n\t\t\"GB,\", \"data-total\",\t\t\"GB data processed, total\");\n\n\tprint_res(name, runtime_sec_max * NSEC_PER_SEC / (bytes / g->p.nr_tasks),\n\t\t\"nsecs,\", \"runtime/byte/thread\",\"nsecs/byte/thread runtime\");\n\n\tprint_res(name, bytes / g->p.nr_tasks / 1e9 / runtime_sec_max,\n\t\t\"GB/sec,\", \"thread-speed\",\t\"GB/sec/thread speed\");\n\n\tprint_res(name, bytes / runtime_sec_max / 1e9,\n\t\t\"GB/sec,\", \"total-speed\",\t\"GB/sec total speed\");\n\n\tif (g->p.show_details >= 2) {\n\t\tchar tname[14 + 2 * 11 + 1];\n\t\tstruct thread_data *td;\n\t\tfor (p = 0; p < g->p.nr_proc; p++) {\n\t\t\tfor (t = 0; t < g->p.nr_threads; t++) {\n\t\t\t\tmemset(tname, 0, sizeof(tname));\n\t\t\t\ttd = g->threads + p*g->p.nr_threads + t;\n\t\t\t\tsnprintf(tname, sizeof(tname), \"process%d:thread%d\", p, t);\n\t\t\t\tprint_res(tname, td->speed_gbs,\n\t\t\t\t\t\"GB/sec\",\t\"thread-speed\", \"GB/sec/thread speed\");\n\t\t\t\tprint_res(tname, td->system_time_ns / NSEC_PER_SEC,\n\t\t\t\t\t\"secs\",\t\"thread-system-time\", \"system CPU time/thread\");\n\t\t\t\tprint_res(tname, td->user_time_ns / NSEC_PER_SEC,\n\t\t\t\t\t\"secs\",\t\"thread-user-time\", \"user CPU time/thread\");\n\t\t\t}\n\t\t}\n\t}\n\n\tfree(pids);\n\n\tdeinit();\n\n\treturn 0;\n}\n\n#define MAX_ARGS 50\n\nstatic int command_size(const char **argv)\n{\n\tint size = 0;\n\n\twhile (*argv) {\n\t\tsize++;\n\t\targv++;\n\t}\n\n\tBUG_ON(size >= MAX_ARGS);\n\n\treturn size;\n}\n\nstatic void init_params(struct params *p, const char *name, int argc, const char **argv)\n{\n\tint i;\n\n\tprintf(\"\\n # Running %s \\\"perf bench numa\", name);\n\n\tfor (i = 0; i < argc; i++)\n\t\tprintf(\" %s\", argv[i]);\n\n\tprintf(\"\\\"\\n\");\n\n\tmemset(p, 0, sizeof(*p));\n\n\t \n\n\tp->serialize_startup\t\t= 1;\n\tp->data_reads\t\t\t= true;\n\tp->data_writes\t\t\t= true;\n\tp->data_backwards\t\t= true;\n\tp->data_rand_walk\t\t= true;\n\tp->nr_loops\t\t\t= -1;\n\tp->init_random\t\t\t= true;\n\tp->mb_global_str\t\t= \"1\";\n\tp->nr_proc\t\t\t= 1;\n\tp->nr_threads\t\t\t= 1;\n\tp->nr_secs\t\t\t= 5;\n\tp->run_all\t\t\t= argc == 1;\n}\n\nstatic int run_bench_numa(const char *name, const char **argv)\n{\n\tint argc = command_size(argv);\n\n\tinit_params(&p0, name, argc, argv);\n\targc = parse_options(argc, argv, options, bench_numa_usage, 0);\n\tif (argc)\n\t\tgoto err;\n\n\tif (__bench_numa(name))\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\treturn -1;\n}\n\n#define OPT_BW_RAM\t\t\"-s\",  \"20\", \"-zZq\",    \"--thp\", \" 1\", \"--no-data_rand_walk\"\n#define OPT_BW_RAM_NOTHP\tOPT_BW_RAM,\t\t\"--thp\", \"-1\"\n\n#define OPT_CONV\t\t\"-s\", \"100\", \"-zZ0qcm\", \"--thp\", \" 1\"\n#define OPT_CONV_NOTHP\t\tOPT_CONV,\t\t\"--thp\", \"-1\"\n\n#define OPT_BW\t\t\t\"-s\",  \"20\", \"-zZ0q\",   \"--thp\", \" 1\"\n#define OPT_BW_NOTHP\t\tOPT_BW,\t\t\t\"--thp\", \"-1\"\n\n \nstatic const char *tests[][MAX_ARGS] = {\n    \n   { \"RAM-bw-local,\",     \"mem\",  \"-p\",  \"1\",  \"-t\",  \"1\", \"-P\", \"1024\",\n\t\t\t  \"-C\" ,   \"0\", \"-M\",   \"0\", OPT_BW_RAM },\n   { \"RAM-bw-local-NOTHP,\",\n\t\t\t  \"mem\",  \"-p\",  \"1\",  \"-t\",  \"1\", \"-P\", \"1024\",\n\t\t\t  \"-C\" ,   \"0\", \"-M\",   \"0\", OPT_BW_RAM_NOTHP },\n   { \"RAM-bw-remote,\",    \"mem\",  \"-p\",  \"1\",  \"-t\",  \"1\", \"-P\", \"1024\",\n\t\t\t  \"-C\" ,   \"0\", \"-M\",   \"1\", OPT_BW_RAM },\n\n    \n   { \"RAM-bw-local-2x,\",  \"mem\",  \"-p\",  \"2\",  \"-t\",  \"1\", \"-P\", \"1024\",\n\t\t\t   \"-C\", \"0,2\", \"-M\", \"0x2\", OPT_BW_RAM },\n   { \"RAM-bw-remote-2x,\", \"mem\",  \"-p\",  \"2\",  \"-t\",  \"1\", \"-P\", \"1024\",\n\t\t \t   \"-C\", \"0,2\", \"-M\", \"1x2\", OPT_BW_RAM },\n\n    \n   { \"RAM-bw-cross,\",     \"mem\",  \"-p\",  \"2\",  \"-t\",  \"1\", \"-P\", \"1024\",\n\t\t \t   \"-C\", \"0,8\", \"-M\", \"1,0\", OPT_BW_RAM },\n\n    \n   { \" 1x3-convergence,\", \"mem\",  \"-p\",  \"1\", \"-t\",  \"3\", \"-P\",  \"512\", OPT_CONV },\n   { \" 1x4-convergence,\", \"mem\",  \"-p\",  \"1\", \"-t\",  \"4\", \"-P\",  \"512\", OPT_CONV },\n   { \" 1x6-convergence,\", \"mem\",  \"-p\",  \"1\", \"-t\",  \"6\", \"-P\", \"1020\", OPT_CONV },\n   { \" 2x3-convergence,\", \"mem\",  \"-p\",  \"2\", \"-t\",  \"3\", \"-P\", \"1020\", OPT_CONV },\n   { \" 3x3-convergence,\", \"mem\",  \"-p\",  \"3\", \"-t\",  \"3\", \"-P\", \"1020\", OPT_CONV },\n   { \" 4x4-convergence,\", \"mem\",  \"-p\",  \"4\", \"-t\",  \"4\", \"-P\",  \"512\", OPT_CONV },\n   { \" 4x4-convergence-NOTHP,\",\n\t\t\t  \"mem\",  \"-p\",  \"4\", \"-t\",  \"4\", \"-P\",  \"512\", OPT_CONV_NOTHP },\n   { \" 4x6-convergence,\", \"mem\",  \"-p\",  \"4\", \"-t\",  \"6\", \"-P\", \"1020\", OPT_CONV },\n   { \" 4x8-convergence,\", \"mem\",  \"-p\",  \"4\", \"-t\",  \"8\", \"-P\",  \"512\", OPT_CONV },\n   { \" 8x4-convergence,\", \"mem\",  \"-p\",  \"8\", \"-t\",  \"4\", \"-P\",  \"512\", OPT_CONV },\n   { \" 8x4-convergence-NOTHP,\",\n\t\t\t  \"mem\",  \"-p\",  \"8\", \"-t\",  \"4\", \"-P\",  \"512\", OPT_CONV_NOTHP },\n   { \" 3x1-convergence,\", \"mem\",  \"-p\",  \"3\", \"-t\",  \"1\", \"-P\",  \"512\", OPT_CONV },\n   { \" 4x1-convergence,\", \"mem\",  \"-p\",  \"4\", \"-t\",  \"1\", \"-P\",  \"512\", OPT_CONV },\n   { \" 8x1-convergence,\", \"mem\",  \"-p\",  \"8\", \"-t\",  \"1\", \"-P\",  \"512\", OPT_CONV },\n   { \"16x1-convergence,\", \"mem\",  \"-p\", \"16\", \"-t\",  \"1\", \"-P\",  \"256\", OPT_CONV },\n   { \"32x1-convergence,\", \"mem\",  \"-p\", \"32\", \"-t\",  \"1\", \"-P\",  \"128\", OPT_CONV },\n\n    \n   { \" 2x1-bw-process,\",  \"mem\",  \"-p\",  \"2\", \"-t\",  \"1\", \"-P\", \"1024\", OPT_BW },\n   { \" 3x1-bw-process,\",  \"mem\",  \"-p\",  \"3\", \"-t\",  \"1\", \"-P\", \"1024\", OPT_BW },\n   { \" 4x1-bw-process,\",  \"mem\",  \"-p\",  \"4\", \"-t\",  \"1\", \"-P\", \"1024\", OPT_BW },\n   { \" 8x1-bw-process,\",  \"mem\",  \"-p\",  \"8\", \"-t\",  \"1\", \"-P\", \" 512\", OPT_BW },\n   { \" 8x1-bw-process-NOTHP,\",\n\t\t\t  \"mem\",  \"-p\",  \"8\", \"-t\",  \"1\", \"-P\", \" 512\", OPT_BW_NOTHP },\n   { \"16x1-bw-process,\",  \"mem\",  \"-p\", \"16\", \"-t\",  \"1\", \"-P\",  \"256\", OPT_BW },\n\n   { \" 1x4-bw-thread,\",   \"mem\",  \"-p\",  \"1\", \"-t\",  \"4\", \"-T\",  \"256\", OPT_BW },\n   { \" 1x8-bw-thread,\",   \"mem\",  \"-p\",  \"1\", \"-t\",  \"8\", \"-T\",  \"256\", OPT_BW },\n   { \"1x16-bw-thread,\",   \"mem\",  \"-p\",  \"1\", \"-t\", \"16\", \"-T\",  \"128\", OPT_BW },\n   { \"1x32-bw-thread,\",   \"mem\",  \"-p\",  \"1\", \"-t\", \"32\", \"-T\",   \"64\", OPT_BW },\n\n   { \" 2x3-bw-process,\",  \"mem\",  \"-p\",  \"2\", \"-t\",  \"3\", \"-P\",  \"512\", OPT_BW },\n   { \" 4x4-bw-process,\",  \"mem\",  \"-p\",  \"4\", \"-t\",  \"4\", \"-P\",  \"512\", OPT_BW },\n   { \" 4x6-bw-process,\",  \"mem\",  \"-p\",  \"4\", \"-t\",  \"6\", \"-P\",  \"512\", OPT_BW },\n   { \" 4x8-bw-process,\",  \"mem\",  \"-p\",  \"4\", \"-t\",  \"8\", \"-P\",  \"512\", OPT_BW },\n   { \" 4x8-bw-process-NOTHP,\",\n\t\t\t  \"mem\",  \"-p\",  \"4\", \"-t\",  \"8\", \"-P\",  \"512\", OPT_BW_NOTHP },\n   { \" 3x3-bw-process,\",  \"mem\",  \"-p\",  \"3\", \"-t\",  \"3\", \"-P\",  \"512\", OPT_BW },\n   { \" 5x5-bw-process,\",  \"mem\",  \"-p\",  \"5\", \"-t\",  \"5\", \"-P\",  \"512\", OPT_BW },\n\n   { \"2x16-bw-process,\",  \"mem\",  \"-p\",  \"2\", \"-t\", \"16\", \"-P\",  \"512\", OPT_BW },\n   { \"1x32-bw-process,\",  \"mem\",  \"-p\",  \"1\", \"-t\", \"32\", \"-P\", \"2048\", OPT_BW },\n\n   { \"numa02-bw,\",        \"mem\",  \"-p\",  \"1\", \"-t\", \"32\", \"-T\",   \"32\", OPT_BW },\n   { \"numa02-bw-NOTHP,\",  \"mem\",  \"-p\",  \"1\", \"-t\", \"32\", \"-T\",   \"32\", OPT_BW_NOTHP },\n   { \"numa01-bw-thread,\", \"mem\",  \"-p\",  \"2\", \"-t\", \"16\", \"-T\",  \"192\", OPT_BW },\n   { \"numa01-bw-thread-NOTHP,\",\n\t\t\t  \"mem\",  \"-p\",  \"2\", \"-t\", \"16\", \"-T\",  \"192\", OPT_BW_NOTHP },\n};\n\nstatic int bench_all(void)\n{\n\tint nr = ARRAY_SIZE(tests);\n\tint ret;\n\tint i;\n\n\tret = system(\"echo ' #'; echo ' # Running test on: '$(uname -a); echo ' #'\");\n\tBUG_ON(ret < 0);\n\n\tfor (i = 0; i < nr; i++) {\n\t\trun_bench_numa(tests[i][0], tests[i] + 1);\n\t}\n\n\tprintf(\"\\n\");\n\n\treturn 0;\n}\n\nint bench_numa(int argc, const char **argv)\n{\n\tinit_params(&p0, \"main,\", argc, argv);\n\targc = parse_options(argc, argv, options, bench_numa_usage, 0);\n\tif (argc)\n\t\tgoto err;\n\n\tif (p0.run_all)\n\t\treturn bench_all();\n\n\tif (__bench_numa(NULL))\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tusage_with_options(numa_usage, options);\n\treturn -1;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}