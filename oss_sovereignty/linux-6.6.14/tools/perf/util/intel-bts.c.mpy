{
  "module_name": "intel-bts.c",
  "hash_id": "90c4c52e257c47aa6434a9e8a1d1a8b01e83c1f520a85246a3a8d433844785bc",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/intel-bts.c",
  "human_readable_source": "\n \n\n#include <endian.h>\n#include <errno.h>\n#include <byteswap.h>\n#include <inttypes.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/bitops.h>\n#include <linux/log2.h>\n#include <linux/zalloc.h>\n\n#include \"color.h\"\n#include \"evsel.h\"\n#include \"evlist.h\"\n#include \"machine.h\"\n#include \"symbol.h\"\n#include \"session.h\"\n#include \"tool.h\"\n#include \"thread.h\"\n#include \"thread-stack.h\"\n#include \"debug.h\"\n#include \"tsc.h\"\n#include \"auxtrace.h\"\n#include \"intel-pt-decoder/intel-pt-insn-decoder.h\"\n#include \"intel-bts.h\"\n#include \"util/synthetic-events.h\"\n\n#define MAX_TIMESTAMP (~0ULL)\n\n#define INTEL_BTS_ERR_NOINSN  5\n#define INTEL_BTS_ERR_LOST    9\n\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n#define le64_to_cpu bswap_64\n#else\n#define le64_to_cpu\n#endif\n\nstruct intel_bts {\n\tstruct auxtrace\t\t\tauxtrace;\n\tstruct auxtrace_queues\t\tqueues;\n\tstruct auxtrace_heap\t\theap;\n\tu32\t\t\t\tauxtrace_type;\n\tstruct perf_session\t\t*session;\n\tstruct machine\t\t\t*machine;\n\tbool\t\t\t\tsampling_mode;\n\tbool\t\t\t\tsnapshot_mode;\n\tbool\t\t\t\tdata_queued;\n\tu32\t\t\t\tpmu_type;\n\tstruct perf_tsc_conversion\ttc;\n\tbool\t\t\t\tcap_user_time_zero;\n\tstruct itrace_synth_opts\tsynth_opts;\n\tbool\t\t\t\tsample_branches;\n\tu32\t\t\t\tbranches_filter;\n\tu64\t\t\t\tbranches_sample_type;\n\tu64\t\t\t\tbranches_id;\n\tsize_t\t\t\t\tbranches_event_size;\n\tunsigned long\t\t\tnum_events;\n};\n\nstruct intel_bts_queue {\n\tstruct intel_bts\t*bts;\n\tunsigned int\t\tqueue_nr;\n\tstruct auxtrace_buffer\t*buffer;\n\tbool\t\t\ton_heap;\n\tbool\t\t\tdone;\n\tpid_t\t\t\tpid;\n\tpid_t\t\t\ttid;\n\tint\t\t\tcpu;\n\tu64\t\t\ttime;\n\tstruct intel_pt_insn\tintel_pt_insn;\n\tu32\t\t\tsample_flags;\n};\n\nstruct branch {\n\tu64 from;\n\tu64 to;\n\tu64 misc;\n};\n\nstatic void intel_bts_dump(struct intel_bts *bts __maybe_unused,\n\t\t\t   unsigned char *buf, size_t len)\n{\n\tstruct branch *branch;\n\tsize_t i, pos = 0, br_sz = sizeof(struct branch), sz;\n\tconst char *color = PERF_COLOR_BLUE;\n\n\tcolor_fprintf(stdout, color,\n\t\t      \". ... Intel BTS data: size %zu bytes\\n\",\n\t\t      len);\n\n\twhile (len) {\n\t\tif (len >= br_sz)\n\t\t\tsz = br_sz;\n\t\telse\n\t\t\tsz = len;\n\t\tprintf(\".\");\n\t\tcolor_fprintf(stdout, color, \"  %08x: \", pos);\n\t\tfor (i = 0; i < sz; i++)\n\t\t\tcolor_fprintf(stdout, color, \" %02x\", buf[i]);\n\t\tfor (; i < br_sz; i++)\n\t\t\tcolor_fprintf(stdout, color, \"   \");\n\t\tif (len >= br_sz) {\n\t\t\tbranch = (struct branch *)buf;\n\t\t\tcolor_fprintf(stdout, color, \" %\"PRIx64\" -> %\"PRIx64\" %s\\n\",\n\t\t\t\t      le64_to_cpu(branch->from),\n\t\t\t\t      le64_to_cpu(branch->to),\n\t\t\t\t      le64_to_cpu(branch->misc) & 0x10 ?\n\t\t\t\t\t\t\t\"pred\" : \"miss\");\n\t\t} else {\n\t\t\tcolor_fprintf(stdout, color, \" Bad record!\\n\");\n\t\t}\n\t\tpos += sz;\n\t\tbuf += sz;\n\t\tlen -= sz;\n\t}\n}\n\nstatic void intel_bts_dump_event(struct intel_bts *bts, unsigned char *buf,\n\t\t\t\t size_t len)\n{\n\tprintf(\".\\n\");\n\tintel_bts_dump(bts, buf, len);\n}\n\nstatic int intel_bts_lost(struct intel_bts *bts, struct perf_sample *sample)\n{\n\tunion perf_event event;\n\tint err;\n\n\tauxtrace_synth_error(&event.auxtrace_error, PERF_AUXTRACE_ERROR_ITRACE,\n\t\t\t     INTEL_BTS_ERR_LOST, sample->cpu, sample->pid,\n\t\t\t     sample->tid, 0, \"Lost trace data\", sample->time);\n\n\terr = perf_session__deliver_synth_event(bts->session, &event, NULL);\n\tif (err)\n\t\tpr_err(\"Intel BTS: failed to deliver error event, error %d\\n\",\n\t\t       err);\n\n\treturn err;\n}\n\nstatic struct intel_bts_queue *intel_bts_alloc_queue(struct intel_bts *bts,\n\t\t\t\t\t\t     unsigned int queue_nr)\n{\n\tstruct intel_bts_queue *btsq;\n\n\tbtsq = zalloc(sizeof(struct intel_bts_queue));\n\tif (!btsq)\n\t\treturn NULL;\n\n\tbtsq->bts = bts;\n\tbtsq->queue_nr = queue_nr;\n\tbtsq->pid = -1;\n\tbtsq->tid = -1;\n\tbtsq->cpu = -1;\n\n\treturn btsq;\n}\n\nstatic int intel_bts_setup_queue(struct intel_bts *bts,\n\t\t\t\t struct auxtrace_queue *queue,\n\t\t\t\t unsigned int queue_nr)\n{\n\tstruct intel_bts_queue *btsq = queue->priv;\n\n\tif (list_empty(&queue->head))\n\t\treturn 0;\n\n\tif (!btsq) {\n\t\tbtsq = intel_bts_alloc_queue(bts, queue_nr);\n\t\tif (!btsq)\n\t\t\treturn -ENOMEM;\n\t\tqueue->priv = btsq;\n\n\t\tif (queue->cpu != -1)\n\t\t\tbtsq->cpu = queue->cpu;\n\t\tbtsq->tid = queue->tid;\n\t}\n\n\tif (bts->sampling_mode)\n\t\treturn 0;\n\n\tif (!btsq->on_heap && !btsq->buffer) {\n\t\tint ret;\n\n\t\tbtsq->buffer = auxtrace_buffer__next(queue, NULL);\n\t\tif (!btsq->buffer)\n\t\t\treturn 0;\n\n\t\tret = auxtrace_heap__add(&bts->heap, queue_nr,\n\t\t\t\t\t btsq->buffer->reference);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tbtsq->on_heap = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_bts_setup_queues(struct intel_bts *bts)\n{\n\tunsigned int i;\n\tint ret;\n\n\tfor (i = 0; i < bts->queues.nr_queues; i++) {\n\t\tret = intel_bts_setup_queue(bts, &bts->queues.queue_array[i],\n\t\t\t\t\t    i);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic inline int intel_bts_update_queues(struct intel_bts *bts)\n{\n\tif (bts->queues.new_data) {\n\t\tbts->queues.new_data = false;\n\t\treturn intel_bts_setup_queues(bts);\n\t}\n\treturn 0;\n}\n\nstatic unsigned char *intel_bts_find_overlap(unsigned char *buf_a, size_t len_a,\n\t\t\t\t\t     unsigned char *buf_b, size_t len_b)\n{\n\tsize_t offs, len;\n\n\tif (len_a > len_b)\n\t\toffs = len_a - len_b;\n\telse\n\t\toffs = 0;\n\n\tfor (; offs < len_a; offs += sizeof(struct branch)) {\n\t\tlen = len_a - offs;\n\t\tif (!memcmp(buf_a + offs, buf_b, len))\n\t\t\treturn buf_b + len;\n\t}\n\n\treturn buf_b;\n}\n\nstatic int intel_bts_do_fix_overlap(struct auxtrace_queue *queue,\n\t\t\t\t    struct auxtrace_buffer *b)\n{\n\tstruct auxtrace_buffer *a;\n\tvoid *start;\n\n\tif (b->list.prev == &queue->head)\n\t\treturn 0;\n\ta = list_entry(b->list.prev, struct auxtrace_buffer, list);\n\tstart = intel_bts_find_overlap(a->data, a->size, b->data, b->size);\n\tif (!start)\n\t\treturn -EINVAL;\n\tb->use_size = b->data + b->size - start;\n\tb->use_data = start;\n\treturn 0;\n}\n\nstatic inline u8 intel_bts_cpumode(struct intel_bts *bts, uint64_t ip)\n{\n\treturn machine__kernel_ip(bts->machine, ip) ?\n\t       PERF_RECORD_MISC_KERNEL :\n\t       PERF_RECORD_MISC_USER;\n}\n\nstatic int intel_bts_synth_branch_sample(struct intel_bts_queue *btsq,\n\t\t\t\t\t struct branch *branch)\n{\n\tint ret;\n\tstruct intel_bts *bts = btsq->bts;\n\tunion perf_event event;\n\tstruct perf_sample sample = { .ip = 0, };\n\n\tif (bts->synth_opts.initial_skip &&\n\t    bts->num_events++ <= bts->synth_opts.initial_skip)\n\t\treturn 0;\n\n\tsample.ip = le64_to_cpu(branch->from);\n\tsample.cpumode = intel_bts_cpumode(bts, sample.ip);\n\tsample.pid = btsq->pid;\n\tsample.tid = btsq->tid;\n\tsample.addr = le64_to_cpu(branch->to);\n\tsample.id = btsq->bts->branches_id;\n\tsample.stream_id = btsq->bts->branches_id;\n\tsample.period = 1;\n\tsample.cpu = btsq->cpu;\n\tsample.flags = btsq->sample_flags;\n\tsample.insn_len = btsq->intel_pt_insn.length;\n\tmemcpy(sample.insn, btsq->intel_pt_insn.buf, INTEL_PT_INSN_BUF_SZ);\n\n\tevent.sample.header.type = PERF_RECORD_SAMPLE;\n\tevent.sample.header.misc = sample.cpumode;\n\tevent.sample.header.size = sizeof(struct perf_event_header);\n\n\tif (bts->synth_opts.inject) {\n\t\tevent.sample.header.size = bts->branches_event_size;\n\t\tret = perf_event__synthesize_sample(&event,\n\t\t\t\t\t\t    bts->branches_sample_type,\n\t\t\t\t\t\t    0, &sample);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = perf_session__deliver_synth_event(bts->session, &event, &sample);\n\tif (ret)\n\t\tpr_err(\"Intel BTS: failed to deliver branch event, error %d\\n\",\n\t\t       ret);\n\n\treturn ret;\n}\n\nstatic int intel_bts_get_next_insn(struct intel_bts_queue *btsq, u64 ip)\n{\n\tstruct machine *machine = btsq->bts->machine;\n\tstruct thread *thread;\n\tunsigned char buf[INTEL_PT_INSN_BUF_SZ];\n\tssize_t len;\n\tbool x86_64;\n\tint err = -1;\n\n\tthread = machine__find_thread(machine, -1, btsq->tid);\n\tif (!thread)\n\t\treturn -1;\n\n\tlen = thread__memcpy(thread, machine, buf, ip, INTEL_PT_INSN_BUF_SZ, &x86_64);\n\tif (len <= 0)\n\t\tgoto out_put;\n\n\tif (intel_pt_get_insn(buf, len, x86_64, &btsq->intel_pt_insn))\n\t\tgoto out_put;\n\n\terr = 0;\nout_put:\n\tthread__put(thread);\n\treturn err;\n}\n\nstatic int intel_bts_synth_error(struct intel_bts *bts, int cpu, pid_t pid,\n\t\t\t\t pid_t tid, u64 ip)\n{\n\tunion perf_event event;\n\tint err;\n\n\tauxtrace_synth_error(&event.auxtrace_error, PERF_AUXTRACE_ERROR_ITRACE,\n\t\t\t     INTEL_BTS_ERR_NOINSN, cpu, pid, tid, ip,\n\t\t\t     \"Failed to get instruction\", 0);\n\n\terr = perf_session__deliver_synth_event(bts->session, &event, NULL);\n\tif (err)\n\t\tpr_err(\"Intel BTS: failed to deliver error event, error %d\\n\",\n\t\t       err);\n\n\treturn err;\n}\n\nstatic int intel_bts_get_branch_type(struct intel_bts_queue *btsq,\n\t\t\t\t     struct branch *branch)\n{\n\tint err;\n\n\tif (!branch->from) {\n\t\tif (branch->to)\n\t\t\tbtsq->sample_flags = PERF_IP_FLAG_BRANCH |\n\t\t\t\t\t     PERF_IP_FLAG_TRACE_BEGIN;\n\t\telse\n\t\t\tbtsq->sample_flags = 0;\n\t\tbtsq->intel_pt_insn.length = 0;\n\t} else if (!branch->to) {\n\t\tbtsq->sample_flags = PERF_IP_FLAG_BRANCH |\n\t\t\t\t     PERF_IP_FLAG_TRACE_END;\n\t\tbtsq->intel_pt_insn.length = 0;\n\t} else {\n\t\terr = intel_bts_get_next_insn(btsq, branch->from);\n\t\tif (err) {\n\t\t\tbtsq->sample_flags = 0;\n\t\t\tbtsq->intel_pt_insn.length = 0;\n\t\t\tif (!btsq->bts->synth_opts.errors)\n\t\t\t\treturn 0;\n\t\t\terr = intel_bts_synth_error(btsq->bts, btsq->cpu,\n\t\t\t\t\t\t    btsq->pid, btsq->tid,\n\t\t\t\t\t\t    branch->from);\n\t\t\treturn err;\n\t\t}\n\t\tbtsq->sample_flags = intel_pt_insn_type(btsq->intel_pt_insn.op);\n\t\t \n\t\tif (!machine__kernel_ip(btsq->bts->machine, branch->from) &&\n\t\t    machine__kernel_ip(btsq->bts->machine, branch->to) &&\n\t\t    btsq->sample_flags != (PERF_IP_FLAG_BRANCH |\n\t\t\t\t\t   PERF_IP_FLAG_CALL |\n\t\t\t\t\t   PERF_IP_FLAG_SYSCALLRET))\n\t\t\tbtsq->sample_flags = PERF_IP_FLAG_BRANCH |\n\t\t\t\t\t     PERF_IP_FLAG_CALL |\n\t\t\t\t\t     PERF_IP_FLAG_ASYNC |\n\t\t\t\t\t     PERF_IP_FLAG_INTERRUPT;\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_bts_process_buffer(struct intel_bts_queue *btsq,\n\t\t\t\t    struct auxtrace_buffer *buffer,\n\t\t\t\t    struct thread *thread)\n{\n\tstruct branch *branch;\n\tsize_t sz, bsz = sizeof(struct branch);\n\tu32 filter = btsq->bts->branches_filter;\n\tint err = 0;\n\n\tif (buffer->use_data) {\n\t\tsz = buffer->use_size;\n\t\tbranch = buffer->use_data;\n\t} else {\n\t\tsz = buffer->size;\n\t\tbranch = buffer->data;\n\t}\n\n\tif (!btsq->bts->sample_branches)\n\t\treturn 0;\n\n\tfor (; sz > bsz; branch += 1, sz -= bsz) {\n\t\tif (!branch->from && !branch->to)\n\t\t\tcontinue;\n\t\tintel_bts_get_branch_type(btsq, branch);\n\t\tif (btsq->bts->synth_opts.thread_stack)\n\t\t\tthread_stack__event(thread, btsq->cpu, btsq->sample_flags,\n\t\t\t\t\t    le64_to_cpu(branch->from),\n\t\t\t\t\t    le64_to_cpu(branch->to),\n\t\t\t\t\t    btsq->intel_pt_insn.length,\n\t\t\t\t\t    buffer->buffer_nr + 1, true, 0, 0);\n\t\tif (filter && !(filter & btsq->sample_flags))\n\t\t\tcontinue;\n\t\terr = intel_bts_synth_branch_sample(btsq, branch);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\treturn err;\n}\n\nstatic int intel_bts_process_queue(struct intel_bts_queue *btsq, u64 *timestamp)\n{\n\tstruct auxtrace_buffer *buffer = btsq->buffer, *old_buffer = buffer;\n\tstruct auxtrace_queue *queue;\n\tstruct thread *thread;\n\tint err;\n\n\tif (btsq->done)\n\t\treturn 1;\n\n\tif (btsq->pid == -1) {\n\t\tthread = machine__find_thread(btsq->bts->machine, -1,\n\t\t\t\t\t      btsq->tid);\n\t\tif (thread)\n\t\t\tbtsq->pid = thread__pid(thread);\n\t} else {\n\t\tthread = machine__findnew_thread(btsq->bts->machine, btsq->pid,\n\t\t\t\t\t\t btsq->tid);\n\t}\n\n\tqueue = &btsq->bts->queues.queue_array[btsq->queue_nr];\n\n\tif (!buffer)\n\t\tbuffer = auxtrace_buffer__next(queue, NULL);\n\n\tif (!buffer) {\n\t\tif (!btsq->bts->sampling_mode)\n\t\t\tbtsq->done = 1;\n\t\terr = 1;\n\t\tgoto out_put;\n\t}\n\n\t \n\tif (buffer->consecutive) {\n\t\terr = -EINVAL;\n\t\tgoto out_put;\n\t}\n\n\tif (!buffer->data) {\n\t\tint fd = perf_data__fd(btsq->bts->session->data);\n\n\t\tbuffer->data = auxtrace_buffer__get_data(buffer, fd);\n\t\tif (!buffer->data) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_put;\n\t\t}\n\t}\n\n\tif (btsq->bts->snapshot_mode && !buffer->consecutive &&\n\t    intel_bts_do_fix_overlap(queue, buffer)) {\n\t\terr = -ENOMEM;\n\t\tgoto out_put;\n\t}\n\n\tif (!btsq->bts->synth_opts.callchain &&\n\t    !btsq->bts->synth_opts.thread_stack && thread &&\n\t    (!old_buffer || btsq->bts->sampling_mode ||\n\t     (btsq->bts->snapshot_mode && !buffer->consecutive)))\n\t\tthread_stack__set_trace_nr(thread, btsq->cpu, buffer->buffer_nr + 1);\n\n\terr = intel_bts_process_buffer(btsq, buffer, thread);\n\n\tauxtrace_buffer__drop_data(buffer);\n\n\tbtsq->buffer = auxtrace_buffer__next(queue, buffer);\n\tif (btsq->buffer) {\n\t\tif (timestamp)\n\t\t\t*timestamp = btsq->buffer->reference;\n\t} else {\n\t\tif (!btsq->bts->sampling_mode)\n\t\t\tbtsq->done = 1;\n\t}\nout_put:\n\tthread__put(thread);\n\treturn err;\n}\n\nstatic int intel_bts_flush_queue(struct intel_bts_queue *btsq)\n{\n\tu64 ts = 0;\n\tint ret;\n\n\twhile (1) {\n\t\tret = intel_bts_process_queue(btsq, &ts);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int intel_bts_process_tid_exit(struct intel_bts *bts, pid_t tid)\n{\n\tstruct auxtrace_queues *queues = &bts->queues;\n\tunsigned int i;\n\n\tfor (i = 0; i < queues->nr_queues; i++) {\n\t\tstruct auxtrace_queue *queue = &bts->queues.queue_array[i];\n\t\tstruct intel_bts_queue *btsq = queue->priv;\n\n\t\tif (btsq && btsq->tid == tid)\n\t\t\treturn intel_bts_flush_queue(btsq);\n\t}\n\treturn 0;\n}\n\nstatic int intel_bts_process_queues(struct intel_bts *bts, u64 timestamp)\n{\n\twhile (1) {\n\t\tunsigned int queue_nr;\n\t\tstruct auxtrace_queue *queue;\n\t\tstruct intel_bts_queue *btsq;\n\t\tu64 ts = 0;\n\t\tint ret;\n\n\t\tif (!bts->heap.heap_cnt)\n\t\t\treturn 0;\n\n\t\tif (bts->heap.heap_array[0].ordinal > timestamp)\n\t\t\treturn 0;\n\n\t\tqueue_nr = bts->heap.heap_array[0].queue_nr;\n\t\tqueue = &bts->queues.queue_array[queue_nr];\n\t\tbtsq = queue->priv;\n\n\t\tauxtrace_heap__pop(&bts->heap);\n\n\t\tret = intel_bts_process_queue(btsq, &ts);\n\t\tif (ret < 0) {\n\t\t\tauxtrace_heap__add(&bts->heap, queue_nr, ts);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (!ret) {\n\t\t\tret = auxtrace_heap__add(&bts->heap, queue_nr, ts);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t} else {\n\t\t\tbtsq->on_heap = false;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_bts_process_event(struct perf_session *session,\n\t\t\t\t   union perf_event *event,\n\t\t\t\t   struct perf_sample *sample,\n\t\t\t\t   struct perf_tool *tool)\n{\n\tstruct intel_bts *bts = container_of(session->auxtrace, struct intel_bts,\n\t\t\t\t\t     auxtrace);\n\tu64 timestamp;\n\tint err;\n\n\tif (dump_trace)\n\t\treturn 0;\n\n\tif (!tool->ordered_events) {\n\t\tpr_err(\"Intel BTS requires ordered events\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (sample->time && sample->time != (u64)-1)\n\t\ttimestamp = perf_time_to_tsc(sample->time, &bts->tc);\n\telse\n\t\ttimestamp = 0;\n\n\terr = intel_bts_update_queues(bts);\n\tif (err)\n\t\treturn err;\n\n\terr = intel_bts_process_queues(bts, timestamp);\n\tif (err)\n\t\treturn err;\n\tif (event->header.type == PERF_RECORD_EXIT) {\n\t\terr = intel_bts_process_tid_exit(bts, event->fork.tid);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (event->header.type == PERF_RECORD_AUX &&\n\t    (event->aux.flags & PERF_AUX_FLAG_TRUNCATED) &&\n\t    bts->synth_opts.errors)\n\t\terr = intel_bts_lost(bts, sample);\n\n\treturn err;\n}\n\nstatic int intel_bts_process_auxtrace_event(struct perf_session *session,\n\t\t\t\t\t    union perf_event *event,\n\t\t\t\t\t    struct perf_tool *tool __maybe_unused)\n{\n\tstruct intel_bts *bts = container_of(session->auxtrace, struct intel_bts,\n\t\t\t\t\t     auxtrace);\n\n\tif (bts->sampling_mode)\n\t\treturn 0;\n\n\tif (!bts->data_queued) {\n\t\tstruct auxtrace_buffer *buffer;\n\t\toff_t data_offset;\n\t\tint fd = perf_data__fd(session->data);\n\t\tint err;\n\n\t\tif (perf_data__is_pipe(session->data)) {\n\t\t\tdata_offset = 0;\n\t\t} else {\n\t\t\tdata_offset = lseek(fd, 0, SEEK_CUR);\n\t\t\tif (data_offset == -1)\n\t\t\t\treturn -errno;\n\t\t}\n\n\t\terr = auxtrace_queues__add_event(&bts->queues, session, event,\n\t\t\t\t\t\t data_offset, &buffer);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\tif (dump_trace) {\n\t\t\tif (auxtrace_buffer__get_data(buffer, fd)) {\n\t\t\t\tintel_bts_dump_event(bts, buffer->data,\n\t\t\t\t\t\t     buffer->size);\n\t\t\t\tauxtrace_buffer__put_data(buffer);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_bts_flush(struct perf_session *session,\n\t\t\t   struct perf_tool *tool __maybe_unused)\n{\n\tstruct intel_bts *bts = container_of(session->auxtrace, struct intel_bts,\n\t\t\t\t\t     auxtrace);\n\tint ret;\n\n\tif (dump_trace || bts->sampling_mode)\n\t\treturn 0;\n\n\tif (!tool->ordered_events)\n\t\treturn -EINVAL;\n\n\tret = intel_bts_update_queues(bts);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn intel_bts_process_queues(bts, MAX_TIMESTAMP);\n}\n\nstatic void intel_bts_free_queue(void *priv)\n{\n\tstruct intel_bts_queue *btsq = priv;\n\n\tif (!btsq)\n\t\treturn;\n\tfree(btsq);\n}\n\nstatic void intel_bts_free_events(struct perf_session *session)\n{\n\tstruct intel_bts *bts = container_of(session->auxtrace, struct intel_bts,\n\t\t\t\t\t     auxtrace);\n\tstruct auxtrace_queues *queues = &bts->queues;\n\tunsigned int i;\n\n\tfor (i = 0; i < queues->nr_queues; i++) {\n\t\tintel_bts_free_queue(queues->queue_array[i].priv);\n\t\tqueues->queue_array[i].priv = NULL;\n\t}\n\tauxtrace_queues__free(queues);\n}\n\nstatic void intel_bts_free(struct perf_session *session)\n{\n\tstruct intel_bts *bts = container_of(session->auxtrace, struct intel_bts,\n\t\t\t\t\t     auxtrace);\n\n\tauxtrace_heap__free(&bts->heap);\n\tintel_bts_free_events(session);\n\tsession->auxtrace = NULL;\n\tfree(bts);\n}\n\nstatic bool intel_bts_evsel_is_auxtrace(struct perf_session *session,\n\t\t\t\t\tstruct evsel *evsel)\n{\n\tstruct intel_bts *bts = container_of(session->auxtrace, struct intel_bts,\n\t\t\t\t\t     auxtrace);\n\n\treturn evsel->core.attr.type == bts->pmu_type;\n}\n\nstruct intel_bts_synth {\n\tstruct perf_tool dummy_tool;\n\tstruct perf_session *session;\n};\n\nstatic int intel_bts_event_synth(struct perf_tool *tool,\n\t\t\t\t union perf_event *event,\n\t\t\t\t struct perf_sample *sample __maybe_unused,\n\t\t\t\t struct machine *machine __maybe_unused)\n{\n\tstruct intel_bts_synth *intel_bts_synth =\n\t\t\tcontainer_of(tool, struct intel_bts_synth, dummy_tool);\n\n\treturn perf_session__deliver_synth_event(intel_bts_synth->session,\n\t\t\t\t\t\t event, NULL);\n}\n\nstatic int intel_bts_synth_event(struct perf_session *session,\n\t\t\t\t struct perf_event_attr *attr, u64 id)\n{\n\tstruct intel_bts_synth intel_bts_synth;\n\n\tmemset(&intel_bts_synth, 0, sizeof(struct intel_bts_synth));\n\tintel_bts_synth.session = session;\n\n\treturn perf_event__synthesize_attr(&intel_bts_synth.dummy_tool, attr, 1,\n\t\t\t\t\t   &id, intel_bts_event_synth);\n}\n\nstatic int intel_bts_synth_events(struct intel_bts *bts,\n\t\t\t\t  struct perf_session *session)\n{\n\tstruct evlist *evlist = session->evlist;\n\tstruct evsel *evsel;\n\tstruct perf_event_attr attr;\n\tbool found = false;\n\tu64 id;\n\tint err;\n\n\tevlist__for_each_entry(evlist, evsel) {\n\t\tif (evsel->core.attr.type == bts->pmu_type && evsel->core.ids) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found) {\n\t\tpr_debug(\"There are no selected events with Intel BTS data\\n\");\n\t\treturn 0;\n\t}\n\n\tmemset(&attr, 0, sizeof(struct perf_event_attr));\n\tattr.size = sizeof(struct perf_event_attr);\n\tattr.type = PERF_TYPE_HARDWARE;\n\tattr.sample_type = evsel->core.attr.sample_type & PERF_SAMPLE_MASK;\n\tattr.sample_type |= PERF_SAMPLE_IP | PERF_SAMPLE_TID |\n\t\t\t    PERF_SAMPLE_PERIOD;\n\tattr.sample_type &= ~(u64)PERF_SAMPLE_TIME;\n\tattr.sample_type &= ~(u64)PERF_SAMPLE_CPU;\n\tattr.exclude_user = evsel->core.attr.exclude_user;\n\tattr.exclude_kernel = evsel->core.attr.exclude_kernel;\n\tattr.exclude_hv = evsel->core.attr.exclude_hv;\n\tattr.exclude_host = evsel->core.attr.exclude_host;\n\tattr.exclude_guest = evsel->core.attr.exclude_guest;\n\tattr.sample_id_all = evsel->core.attr.sample_id_all;\n\tattr.read_format = evsel->core.attr.read_format;\n\n\tid = evsel->core.id[0] + 1000000000;\n\tif (!id)\n\t\tid = 1;\n\n\tif (bts->synth_opts.branches) {\n\t\tattr.config = PERF_COUNT_HW_BRANCH_INSTRUCTIONS;\n\t\tattr.sample_period = 1;\n\t\tattr.sample_type |= PERF_SAMPLE_ADDR;\n\t\tpr_debug(\"Synthesizing 'branches' event with id %\" PRIu64 \" sample type %#\" PRIx64 \"\\n\",\n\t\t\t id, (u64)attr.sample_type);\n\t\terr = intel_bts_synth_event(session, &attr, id);\n\t\tif (err) {\n\t\t\tpr_err(\"%s: failed to synthesize 'branches' event type\\n\",\n\t\t\t       __func__);\n\t\t\treturn err;\n\t\t}\n\t\tbts->sample_branches = true;\n\t\tbts->branches_sample_type = attr.sample_type;\n\t\tbts->branches_id = id;\n\t\t \n\t\tbts->branches_event_size = sizeof(struct perf_record_sample) +\n\t\t\t\t\t   __evsel__sample_size(attr.sample_type);\n\t}\n\n\treturn 0;\n}\n\nstatic const char * const intel_bts_info_fmts[] = {\n\t[INTEL_BTS_PMU_TYPE]\t\t= \"  PMU Type           %\"PRId64\"\\n\",\n\t[INTEL_BTS_TIME_SHIFT]\t\t= \"  Time Shift         %\"PRIu64\"\\n\",\n\t[INTEL_BTS_TIME_MULT]\t\t= \"  Time Muliplier     %\"PRIu64\"\\n\",\n\t[INTEL_BTS_TIME_ZERO]\t\t= \"  Time Zero          %\"PRIu64\"\\n\",\n\t[INTEL_BTS_CAP_USER_TIME_ZERO]\t= \"  Cap Time Zero      %\"PRId64\"\\n\",\n\t[INTEL_BTS_SNAPSHOT_MODE]\t= \"  Snapshot mode      %\"PRId64\"\\n\",\n};\n\nstatic void intel_bts_print_info(__u64 *arr, int start, int finish)\n{\n\tint i;\n\n\tif (!dump_trace)\n\t\treturn;\n\n\tfor (i = start; i <= finish; i++)\n\t\tfprintf(stdout, intel_bts_info_fmts[i], arr[i]);\n}\n\nint intel_bts_process_auxtrace_info(union perf_event *event,\n\t\t\t\t    struct perf_session *session)\n{\n\tstruct perf_record_auxtrace_info *auxtrace_info = &event->auxtrace_info;\n\tsize_t min_sz = sizeof(u64) * INTEL_BTS_SNAPSHOT_MODE;\n\tstruct intel_bts *bts;\n\tint err;\n\n\tif (auxtrace_info->header.size < sizeof(struct perf_record_auxtrace_info) +\n\t\t\t\t\tmin_sz)\n\t\treturn -EINVAL;\n\n\tbts = zalloc(sizeof(struct intel_bts));\n\tif (!bts)\n\t\treturn -ENOMEM;\n\n\terr = auxtrace_queues__init(&bts->queues);\n\tif (err)\n\t\tgoto err_free;\n\n\tbts->session = session;\n\tbts->machine = &session->machines.host;  \n\tbts->auxtrace_type = auxtrace_info->type;\n\tbts->pmu_type = auxtrace_info->priv[INTEL_BTS_PMU_TYPE];\n\tbts->tc.time_shift = auxtrace_info->priv[INTEL_BTS_TIME_SHIFT];\n\tbts->tc.time_mult = auxtrace_info->priv[INTEL_BTS_TIME_MULT];\n\tbts->tc.time_zero = auxtrace_info->priv[INTEL_BTS_TIME_ZERO];\n\tbts->cap_user_time_zero =\n\t\t\tauxtrace_info->priv[INTEL_BTS_CAP_USER_TIME_ZERO];\n\tbts->snapshot_mode = auxtrace_info->priv[INTEL_BTS_SNAPSHOT_MODE];\n\n\tbts->sampling_mode = false;\n\n\tbts->auxtrace.process_event = intel_bts_process_event;\n\tbts->auxtrace.process_auxtrace_event = intel_bts_process_auxtrace_event;\n\tbts->auxtrace.flush_events = intel_bts_flush;\n\tbts->auxtrace.free_events = intel_bts_free_events;\n\tbts->auxtrace.free = intel_bts_free;\n\tbts->auxtrace.evsel_is_auxtrace = intel_bts_evsel_is_auxtrace;\n\tsession->auxtrace = &bts->auxtrace;\n\n\tintel_bts_print_info(&auxtrace_info->priv[0], INTEL_BTS_PMU_TYPE,\n\t\t\t     INTEL_BTS_SNAPSHOT_MODE);\n\n\tif (dump_trace)\n\t\treturn 0;\n\n\tif (session->itrace_synth_opts->set) {\n\t\tbts->synth_opts = *session->itrace_synth_opts;\n\t} else {\n\t\titrace_synth_opts__set_default(&bts->synth_opts,\n\t\t\t\tsession->itrace_synth_opts->default_no_sample);\n\t\tbts->synth_opts.thread_stack =\n\t\t\t\tsession->itrace_synth_opts->thread_stack;\n\t}\n\n\tif (bts->synth_opts.calls)\n\t\tbts->branches_filter |= PERF_IP_FLAG_CALL | PERF_IP_FLAG_ASYNC |\n\t\t\t\t\tPERF_IP_FLAG_TRACE_END;\n\tif (bts->synth_opts.returns)\n\t\tbts->branches_filter |= PERF_IP_FLAG_RETURN |\n\t\t\t\t\tPERF_IP_FLAG_TRACE_BEGIN;\n\n\terr = intel_bts_synth_events(bts, session);\n\tif (err)\n\t\tgoto err_free_queues;\n\n\terr = auxtrace_queues__process_index(&bts->queues, session);\n\tif (err)\n\t\tgoto err_free_queues;\n\n\tif (bts->queues.populated)\n\t\tbts->data_queued = true;\n\n\treturn 0;\n\nerr_free_queues:\n\tauxtrace_queues__free(&bts->queues);\n\tsession->auxtrace = NULL;\nerr_free:\n\tfree(bts);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}