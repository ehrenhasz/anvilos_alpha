{
  "module_name": "mmap.c",
  "hash_id": "762daf7b968632c81e717f413d797fedb25028479c1e1a66361ad5c1f626a3d7",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/mmap.c",
  "human_readable_source": "\n \n\n#include <sys/mman.h>\n#include <inttypes.h>\n#include <asm/bug.h>\n#include <linux/zalloc.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h> \n#include <perf/mmap.h>\n#ifdef HAVE_LIBNUMA_SUPPORT\n#include <numaif.h>\n#endif\n#include \"cpumap.h\"\n#include \"debug.h\"\n#include \"event.h\"\n#include \"mmap.h\"\n#include \"../perf.h\"\n#include <internal/lib.h>  \n#include <linux/bitmap.h>\n\n#define MASK_SIZE 1023\nvoid mmap_cpu_mask__scnprintf(struct mmap_cpu_mask *mask, const char *tag)\n{\n\tchar buf[MASK_SIZE + 1];\n\tsize_t len;\n\n\tlen = bitmap_scnprintf(mask->bits, mask->nbits, buf, MASK_SIZE);\n\tbuf[len] = '\\0';\n\tpr_debug(\"%p: %s mask[%zd]: %s\\n\", mask, tag, mask->nbits, buf);\n}\n\nsize_t mmap__mmap_len(struct mmap *map)\n{\n\treturn perf_mmap__mmap_len(&map->core);\n}\n\nint __weak auxtrace_mmap__mmap(struct auxtrace_mmap *mm __maybe_unused,\n\t\t\t       struct auxtrace_mmap_params *mp __maybe_unused,\n\t\t\t       void *userpg __maybe_unused,\n\t\t\t       int fd __maybe_unused)\n{\n\treturn 0;\n}\n\nvoid __weak auxtrace_mmap__munmap(struct auxtrace_mmap *mm __maybe_unused)\n{\n}\n\nvoid __weak auxtrace_mmap_params__init(struct auxtrace_mmap_params *mp __maybe_unused,\n\t\t\t\t       off_t auxtrace_offset __maybe_unused,\n\t\t\t\t       unsigned int auxtrace_pages __maybe_unused,\n\t\t\t\t       bool auxtrace_overwrite __maybe_unused)\n{\n}\n\nvoid __weak auxtrace_mmap_params__set_idx(struct auxtrace_mmap_params *mp __maybe_unused,\n\t\t\t\t\t  struct evlist *evlist __maybe_unused,\n\t\t\t\t\t  struct evsel *evsel __maybe_unused,\n\t\t\t\t\t  int idx __maybe_unused)\n{\n}\n\n#ifdef HAVE_AIO_SUPPORT\nstatic int perf_mmap__aio_enabled(struct mmap *map)\n{\n\treturn map->aio.nr_cblocks > 0;\n}\n\n#ifdef HAVE_LIBNUMA_SUPPORT\nstatic int perf_mmap__aio_alloc(struct mmap *map, int idx)\n{\n\tmap->aio.data[idx] = mmap(NULL, mmap__mmap_len(map), PROT_READ|PROT_WRITE,\n\t\t\t\t  MAP_PRIVATE|MAP_ANONYMOUS, 0, 0);\n\tif (map->aio.data[idx] == MAP_FAILED) {\n\t\tmap->aio.data[idx] = NULL;\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic void perf_mmap__aio_free(struct mmap *map, int idx)\n{\n\tif (map->aio.data[idx]) {\n\t\tmunmap(map->aio.data[idx], mmap__mmap_len(map));\n\t\tmap->aio.data[idx] = NULL;\n\t}\n}\n\nstatic int perf_mmap__aio_bind(struct mmap *map, int idx, struct perf_cpu cpu, int affinity)\n{\n\tvoid *data;\n\tsize_t mmap_len;\n\tunsigned long *node_mask;\n\tunsigned long node_index;\n\tint err = 0;\n\n\tif (affinity != PERF_AFFINITY_SYS && cpu__max_node() > 1) {\n\t\tdata = map->aio.data[idx];\n\t\tmmap_len = mmap__mmap_len(map);\n\t\tnode_index = cpu__get_node(cpu);\n\t\tnode_mask = bitmap_zalloc(node_index + 1);\n\t\tif (!node_mask) {\n\t\t\tpr_err(\"Failed to allocate node mask for mbind: error %m\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\t__set_bit(node_index, node_mask);\n\t\tif (mbind(data, mmap_len, MPOL_BIND, node_mask, node_index + 1 + 1, 0)) {\n\t\t\tpr_err(\"Failed to bind [%p-%p] AIO buffer to node %lu: error %m\\n\",\n\t\t\t\tdata, data + mmap_len, node_index);\n\t\t\terr = -1;\n\t\t}\n\t\tbitmap_free(node_mask);\n\t}\n\n\treturn err;\n}\n#else  \nstatic int perf_mmap__aio_alloc(struct mmap *map, int idx)\n{\n\tmap->aio.data[idx] = malloc(mmap__mmap_len(map));\n\tif (map->aio.data[idx] == NULL)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic void perf_mmap__aio_free(struct mmap *map, int idx)\n{\n\tzfree(&(map->aio.data[idx]));\n}\n\nstatic int perf_mmap__aio_bind(struct mmap *map __maybe_unused, int idx __maybe_unused,\n\t\tstruct perf_cpu cpu __maybe_unused, int affinity __maybe_unused)\n{\n\treturn 0;\n}\n#endif\n\nstatic int perf_mmap__aio_mmap(struct mmap *map, struct mmap_params *mp)\n{\n\tint delta_max, i, prio, ret;\n\n\tmap->aio.nr_cblocks = mp->nr_cblocks;\n\tif (map->aio.nr_cblocks) {\n\t\tmap->aio.aiocb = calloc(map->aio.nr_cblocks, sizeof(struct aiocb *));\n\t\tif (!map->aio.aiocb) {\n\t\t\tpr_debug2(\"failed to allocate aiocb for data buffer, error %m\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tmap->aio.cblocks = calloc(map->aio.nr_cblocks, sizeof(struct aiocb));\n\t\tif (!map->aio.cblocks) {\n\t\t\tpr_debug2(\"failed to allocate cblocks for data buffer, error %m\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tmap->aio.data = calloc(map->aio.nr_cblocks, sizeof(void *));\n\t\tif (!map->aio.data) {\n\t\t\tpr_debug2(\"failed to allocate data buffer, error %m\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tdelta_max = sysconf(_SC_AIO_PRIO_DELTA_MAX);\n\t\tfor (i = 0; i < map->aio.nr_cblocks; ++i) {\n\t\t\tret = perf_mmap__aio_alloc(map, i);\n\t\t\tif (ret == -1) {\n\t\t\t\tpr_debug2(\"failed to allocate data buffer area, error %m\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tret = perf_mmap__aio_bind(map, i, map->core.cpu, mp->affinity);\n\t\t\tif (ret == -1)\n\t\t\t\treturn -1;\n\t\t\t \n\t\t\tmap->aio.cblocks[i].aio_fildes = -1;\n\t\t\t \n\t\t\tprio = delta_max - i;\n\t\t\tmap->aio.cblocks[i].aio_reqprio = prio >= 0 ? prio : 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void perf_mmap__aio_munmap(struct mmap *map)\n{\n\tint i;\n\n\tfor (i = 0; i < map->aio.nr_cblocks; ++i)\n\t\tperf_mmap__aio_free(map, i);\n\tif (map->aio.data)\n\t\tzfree(&map->aio.data);\n\tzfree(&map->aio.cblocks);\n\tzfree(&map->aio.aiocb);\n}\n#else  \nstatic int perf_mmap__aio_enabled(struct mmap *map __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic int perf_mmap__aio_mmap(struct mmap *map __maybe_unused,\n\t\t\t       struct mmap_params *mp __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic void perf_mmap__aio_munmap(struct mmap *map __maybe_unused)\n{\n}\n#endif\n\nvoid mmap__munmap(struct mmap *map)\n{\n\tbitmap_free(map->affinity_mask.bits);\n\n#ifndef PYTHON_PERF\n\tzstd_fini(&map->zstd_data);\n#endif\n\n\tperf_mmap__aio_munmap(map);\n\tif (map->data != NULL) {\n\t\tmunmap(map->data, mmap__mmap_len(map));\n\t\tmap->data = NULL;\n\t}\n\tauxtrace_mmap__munmap(&map->auxtrace_mmap);\n}\n\nstatic void build_node_mask(int node, struct mmap_cpu_mask *mask)\n{\n\tint idx, nr_cpus;\n\tstruct perf_cpu cpu;\n\tconst struct perf_cpu_map *cpu_map = NULL;\n\n\tcpu_map = cpu_map__online();\n\tif (!cpu_map)\n\t\treturn;\n\n\tnr_cpus = perf_cpu_map__nr(cpu_map);\n\tfor (idx = 0; idx < nr_cpus; idx++) {\n\t\tcpu = perf_cpu_map__cpu(cpu_map, idx);  \n\t\tif (cpu__get_node(cpu) == node)\n\t\t\t__set_bit(cpu.cpu, mask->bits);\n\t}\n}\n\nstatic int perf_mmap__setup_affinity_mask(struct mmap *map, struct mmap_params *mp)\n{\n\tmap->affinity_mask.nbits = cpu__max_cpu().cpu;\n\tmap->affinity_mask.bits = bitmap_zalloc(map->affinity_mask.nbits);\n\tif (!map->affinity_mask.bits)\n\t\treturn -1;\n\n\tif (mp->affinity == PERF_AFFINITY_NODE && cpu__max_node() > 1)\n\t\tbuild_node_mask(cpu__get_node(map->core.cpu), &map->affinity_mask);\n\telse if (mp->affinity == PERF_AFFINITY_CPU)\n\t\t__set_bit(map->core.cpu.cpu, map->affinity_mask.bits);\n\n\treturn 0;\n}\n\nint mmap__mmap(struct mmap *map, struct mmap_params *mp, int fd, struct perf_cpu cpu)\n{\n\tif (perf_mmap__mmap(&map->core, &mp->core, fd, cpu)) {\n\t\tpr_debug2(\"failed to mmap perf event ring buffer, error %d\\n\",\n\t\t\t  errno);\n\t\treturn -1;\n\t}\n\n\tif (mp->affinity != PERF_AFFINITY_SYS &&\n\t\tperf_mmap__setup_affinity_mask(map, mp)) {\n\t\tpr_debug2(\"failed to alloc mmap affinity mask, error %d\\n\",\n\t\t\t  errno);\n\t\treturn -1;\n\t}\n\n\tif (verbose == 2)\n\t\tmmap_cpu_mask__scnprintf(&map->affinity_mask, \"mmap\");\n\n\tmap->core.flush = mp->flush;\n\n\tmap->comp_level = mp->comp_level;\n#ifndef PYTHON_PERF\n\tif (zstd_init(&map->zstd_data, map->comp_level)) {\n\t\tpr_debug2(\"failed to init mmap compressor, error %d\\n\", errno);\n\t\treturn -1;\n\t}\n#endif\n\n\tif (map->comp_level && !perf_mmap__aio_enabled(map)) {\n\t\tmap->data = mmap(NULL, mmap__mmap_len(map), PROT_READ|PROT_WRITE,\n\t\t\t\t MAP_PRIVATE|MAP_ANONYMOUS, 0, 0);\n\t\tif (map->data == MAP_FAILED) {\n\t\t\tpr_debug2(\"failed to mmap data buffer, error %d\\n\",\n\t\t\t\t\terrno);\n\t\t\tmap->data = NULL;\n\t\t\treturn -1;\n\t\t}\n\t}\n\n\tif (auxtrace_mmap__mmap(&map->auxtrace_mmap,\n\t\t\t\t&mp->auxtrace_mp, map->core.base, fd))\n\t\treturn -1;\n\n\treturn perf_mmap__aio_mmap(map, mp);\n}\n\nint perf_mmap__push(struct mmap *md, void *to,\n\t\t    int push(struct mmap *map, void *to, void *buf, size_t size))\n{\n\tu64 head = perf_mmap__read_head(&md->core);\n\tunsigned char *data = md->core.base + page_size;\n\tunsigned long size;\n\tvoid *buf;\n\tint rc = 0;\n\n\trc = perf_mmap__read_init(&md->core);\n\tif (rc < 0)\n\t\treturn (rc == -EAGAIN) ? 1 : -1;\n\n\tsize = md->core.end - md->core.start;\n\n\tif ((md->core.start & md->core.mask) + size != (md->core.end & md->core.mask)) {\n\t\tbuf = &data[md->core.start & md->core.mask];\n\t\tsize = md->core.mask + 1 - (md->core.start & md->core.mask);\n\t\tmd->core.start += size;\n\n\t\tif (push(md, to, buf, size) < 0) {\n\t\t\trc = -1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tbuf = &data[md->core.start & md->core.mask];\n\tsize = md->core.end - md->core.start;\n\tmd->core.start += size;\n\n\tif (push(md, to, buf, size) < 0) {\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\tmd->core.prev = head;\n\tperf_mmap__consume(&md->core);\nout:\n\treturn rc;\n}\n\nint mmap_cpu_mask__duplicate(struct mmap_cpu_mask *original, struct mmap_cpu_mask *clone)\n{\n\tclone->nbits = original->nbits;\n\tclone->bits  = bitmap_zalloc(original->nbits);\n\tif (!clone->bits)\n\t\treturn -ENOMEM;\n\n\tmemcpy(clone->bits, original->bits, MMAP_CPU_MASK_BYTES(original));\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}