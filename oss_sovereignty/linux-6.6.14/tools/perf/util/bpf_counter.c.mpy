{
  "module_name": "bpf_counter.c",
  "hash_id": "369ffe18b22dad641b1ea6e87e322e9772633cbe0df9506ddfd9b2d5c52c16fe",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/bpf_counter.c",
  "human_readable_source": "\n\n \n\n#include <assert.h>\n#include <limits.h>\n#include <unistd.h>\n#include <sys/file.h>\n#include <sys/time.h>\n#include <linux/err.h>\n#include <linux/zalloc.h>\n#include <api/fs/fs.h>\n#include <perf/bpf_perf.h>\n\n#include \"bpf_counter.h\"\n#include \"bpf-utils.h\"\n#include \"counts.h\"\n#include \"debug.h\"\n#include \"evsel.h\"\n#include \"evlist.h\"\n#include \"target.h\"\n#include \"cgroup.h\"\n#include \"cpumap.h\"\n#include \"thread_map.h\"\n\n#include \"bpf_skel/bpf_prog_profiler.skel.h\"\n#include \"bpf_skel/bperf_u.h\"\n#include \"bpf_skel/bperf_leader.skel.h\"\n#include \"bpf_skel/bperf_follower.skel.h\"\n\n#define ATTR_MAP_SIZE 16\n\nstatic inline void *u64_to_ptr(__u64 ptr)\n{\n\treturn (void *)(unsigned long)ptr;\n}\n\nstatic struct bpf_counter *bpf_counter_alloc(void)\n{\n\tstruct bpf_counter *counter;\n\n\tcounter = zalloc(sizeof(*counter));\n\tif (counter)\n\t\tINIT_LIST_HEAD(&counter->list);\n\treturn counter;\n}\n\nstatic int bpf_program_profiler__destroy(struct evsel *evsel)\n{\n\tstruct bpf_counter *counter, *tmp;\n\n\tlist_for_each_entry_safe(counter, tmp,\n\t\t\t\t &evsel->bpf_counter_list, list) {\n\t\tlist_del_init(&counter->list);\n\t\tbpf_prog_profiler_bpf__destroy(counter->skel);\n\t\tfree(counter);\n\t}\n\tassert(list_empty(&evsel->bpf_counter_list));\n\n\treturn 0;\n}\n\nstatic char *bpf_target_prog_name(int tgt_fd)\n{\n\tstruct bpf_func_info *func_info;\n\tstruct perf_bpil *info_linear;\n\tconst struct btf_type *t;\n\tstruct btf *btf = NULL;\n\tchar *name = NULL;\n\n\tinfo_linear = get_bpf_prog_info_linear(tgt_fd, 1UL << PERF_BPIL_FUNC_INFO);\n\tif (IS_ERR_OR_NULL(info_linear)) {\n\t\tpr_debug(\"failed to get info_linear for prog FD %d\\n\", tgt_fd);\n\t\treturn NULL;\n\t}\n\n\tif (info_linear->info.btf_id == 0) {\n\t\tpr_debug(\"prog FD %d doesn't have valid btf\\n\", tgt_fd);\n\t\tgoto out;\n\t}\n\n\tbtf = btf__load_from_kernel_by_id(info_linear->info.btf_id);\n\tif (libbpf_get_error(btf)) {\n\t\tpr_debug(\"failed to load btf for prog FD %d\\n\", tgt_fd);\n\t\tgoto out;\n\t}\n\n\tfunc_info = u64_to_ptr(info_linear->info.func_info);\n\tt = btf__type_by_id(btf, func_info[0].type_id);\n\tif (!t) {\n\t\tpr_debug(\"btf %d doesn't have type %d\\n\",\n\t\t\t info_linear->info.btf_id, func_info[0].type_id);\n\t\tgoto out;\n\t}\n\tname = strdup(btf__name_by_offset(btf, t->name_off));\nout:\n\tbtf__free(btf);\n\tfree(info_linear);\n\treturn name;\n}\n\nstatic int bpf_program_profiler_load_one(struct evsel *evsel, u32 prog_id)\n{\n\tstruct bpf_prog_profiler_bpf *skel;\n\tstruct bpf_counter *counter;\n\tstruct bpf_program *prog;\n\tchar *prog_name;\n\tint prog_fd;\n\tint err;\n\n\tprog_fd = bpf_prog_get_fd_by_id(prog_id);\n\tif (prog_fd < 0) {\n\t\tpr_err(\"Failed to open fd for bpf prog %u\\n\", prog_id);\n\t\treturn -1;\n\t}\n\tcounter = bpf_counter_alloc();\n\tif (!counter) {\n\t\tclose(prog_fd);\n\t\treturn -1;\n\t}\n\n\tskel = bpf_prog_profiler_bpf__open();\n\tif (!skel) {\n\t\tpr_err(\"Failed to open bpf skeleton\\n\");\n\t\tgoto err_out;\n\t}\n\n\tskel->rodata->num_cpu = evsel__nr_cpus(evsel);\n\n\tbpf_map__set_max_entries(skel->maps.events, evsel__nr_cpus(evsel));\n\tbpf_map__set_max_entries(skel->maps.fentry_readings, 1);\n\tbpf_map__set_max_entries(skel->maps.accum_readings, 1);\n\n\tprog_name = bpf_target_prog_name(prog_fd);\n\tif (!prog_name) {\n\t\tpr_err(\"Failed to get program name for bpf prog %u. Does it have BTF?\\n\", prog_id);\n\t\tgoto err_out;\n\t}\n\n\tbpf_object__for_each_program(prog, skel->obj) {\n\t\terr = bpf_program__set_attach_target(prog, prog_fd, prog_name);\n\t\tif (err) {\n\t\t\tpr_err(\"bpf_program__set_attach_target failed.\\n\"\n\t\t\t       \"Does bpf prog %u have BTF?\\n\", prog_id);\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\tset_max_rlimit();\n\terr = bpf_prog_profiler_bpf__load(skel);\n\tif (err) {\n\t\tpr_err(\"bpf_prog_profiler_bpf__load failed\\n\");\n\t\tgoto err_out;\n\t}\n\n\tassert(skel != NULL);\n\tcounter->skel = skel;\n\tlist_add(&counter->list, &evsel->bpf_counter_list);\n\tclose(prog_fd);\n\treturn 0;\nerr_out:\n\tbpf_prog_profiler_bpf__destroy(skel);\n\tfree(counter);\n\tclose(prog_fd);\n\treturn -1;\n}\n\nstatic int bpf_program_profiler__load(struct evsel *evsel, struct target *target)\n{\n\tchar *bpf_str, *bpf_str_, *tok, *saveptr = NULL, *p;\n\tu32 prog_id;\n\tint ret;\n\n\tbpf_str_ = bpf_str = strdup(target->bpf_str);\n\tif (!bpf_str)\n\t\treturn -1;\n\n\twhile ((tok = strtok_r(bpf_str, \",\", &saveptr)) != NULL) {\n\t\tprog_id = strtoul(tok, &p, 10);\n\t\tif (prog_id == 0 || prog_id == UINT_MAX ||\n\t\t    (*p != '\\0' && *p != ',')) {\n\t\t\tpr_err(\"Failed to parse bpf prog ids %s\\n\",\n\t\t\t       target->bpf_str);\n\t\t\treturn -1;\n\t\t}\n\n\t\tret = bpf_program_profiler_load_one(evsel, prog_id);\n\t\tif (ret) {\n\t\t\tbpf_program_profiler__destroy(evsel);\n\t\t\tfree(bpf_str_);\n\t\t\treturn -1;\n\t\t}\n\t\tbpf_str = NULL;\n\t}\n\tfree(bpf_str_);\n\treturn 0;\n}\n\nstatic int bpf_program_profiler__enable(struct evsel *evsel)\n{\n\tstruct bpf_counter *counter;\n\tint ret;\n\n\tlist_for_each_entry(counter, &evsel->bpf_counter_list, list) {\n\t\tassert(counter->skel != NULL);\n\t\tret = bpf_prog_profiler_bpf__attach(counter->skel);\n\t\tif (ret) {\n\t\t\tbpf_program_profiler__destroy(evsel);\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int bpf_program_profiler__disable(struct evsel *evsel)\n{\n\tstruct bpf_counter *counter;\n\n\tlist_for_each_entry(counter, &evsel->bpf_counter_list, list) {\n\t\tassert(counter->skel != NULL);\n\t\tbpf_prog_profiler_bpf__detach(counter->skel);\n\t}\n\treturn 0;\n}\n\nstatic int bpf_program_profiler__read(struct evsel *evsel)\n{\n\t\n\t\n\t\n\tint num_cpu_bpf = libbpf_num_possible_cpus();\n\tstruct bpf_perf_event_value values[num_cpu_bpf];\n\tstruct bpf_counter *counter;\n\tstruct perf_counts_values *counts;\n\tint reading_map_fd;\n\t__u32 key = 0;\n\tint err, idx, bpf_cpu;\n\n\tif (list_empty(&evsel->bpf_counter_list))\n\t\treturn -EAGAIN;\n\n\tperf_cpu_map__for_each_idx(idx, evsel__cpus(evsel)) {\n\t\tcounts = perf_counts(evsel->counts, idx, 0);\n\t\tcounts->val = 0;\n\t\tcounts->ena = 0;\n\t\tcounts->run = 0;\n\t}\n\tlist_for_each_entry(counter, &evsel->bpf_counter_list, list) {\n\t\tstruct bpf_prog_profiler_bpf *skel = counter->skel;\n\n\t\tassert(skel != NULL);\n\t\treading_map_fd = bpf_map__fd(skel->maps.accum_readings);\n\n\t\terr = bpf_map_lookup_elem(reading_map_fd, &key, values);\n\t\tif (err) {\n\t\t\tpr_err(\"failed to read value\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\tfor (bpf_cpu = 0; bpf_cpu < num_cpu_bpf; bpf_cpu++) {\n\t\t\tidx = perf_cpu_map__idx(evsel__cpus(evsel),\n\t\t\t\t\t\t(struct perf_cpu){.cpu = bpf_cpu});\n\t\t\tif (idx == -1)\n\t\t\t\tcontinue;\n\t\t\tcounts = perf_counts(evsel->counts, idx, 0);\n\t\t\tcounts->val += values[bpf_cpu].counter;\n\t\t\tcounts->ena += values[bpf_cpu].enabled;\n\t\t\tcounts->run += values[bpf_cpu].running;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int bpf_program_profiler__install_pe(struct evsel *evsel, int cpu_map_idx,\n\t\t\t\t\t    int fd)\n{\n\tstruct bpf_prog_profiler_bpf *skel;\n\tstruct bpf_counter *counter;\n\tint ret;\n\n\tlist_for_each_entry(counter, &evsel->bpf_counter_list, list) {\n\t\tskel = counter->skel;\n\t\tassert(skel != NULL);\n\n\t\tret = bpf_map_update_elem(bpf_map__fd(skel->maps.events),\n\t\t\t\t\t  &cpu_map_idx, &fd, BPF_ANY);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstruct bpf_counter_ops bpf_program_profiler_ops = {\n\t.load       = bpf_program_profiler__load,\n\t.enable\t    = bpf_program_profiler__enable,\n\t.disable    = bpf_program_profiler__disable,\n\t.read       = bpf_program_profiler__read,\n\t.destroy    = bpf_program_profiler__destroy,\n\t.install_pe = bpf_program_profiler__install_pe,\n};\n\nstatic bool bperf_attr_map_compatible(int attr_map_fd)\n{\n\tstruct bpf_map_info map_info = {0};\n\t__u32 map_info_len = sizeof(map_info);\n\tint err;\n\n\terr = bpf_obj_get_info_by_fd(attr_map_fd, &map_info, &map_info_len);\n\n\tif (err)\n\t\treturn false;\n\treturn (map_info.key_size == sizeof(struct perf_event_attr)) &&\n\t\t(map_info.value_size == sizeof(struct perf_event_attr_map_entry));\n}\n\nstatic int bperf_lock_attr_map(struct target *target)\n{\n\tchar path[PATH_MAX];\n\tint map_fd, err;\n\n\tif (target->attr_map) {\n\t\tscnprintf(path, PATH_MAX, \"%s\", target->attr_map);\n\t} else {\n\t\tscnprintf(path, PATH_MAX, \"%s/fs/bpf/%s\", sysfs__mountpoint(),\n\t\t\t  BPF_PERF_DEFAULT_ATTR_MAP_PATH);\n\t}\n\n\tif (access(path, F_OK)) {\n\t\tmap_fd = bpf_map_create(BPF_MAP_TYPE_HASH, NULL,\n\t\t\t\t\tsizeof(struct perf_event_attr),\n\t\t\t\t\tsizeof(struct perf_event_attr_map_entry),\n\t\t\t\t\tATTR_MAP_SIZE, NULL);\n\t\tif (map_fd < 0)\n\t\t\treturn -1;\n\n\t\terr = bpf_obj_pin(map_fd, path);\n\t\tif (err) {\n\t\t\t \n\t\t\tclose(map_fd);\n\t\t\tmap_fd = bpf_obj_get(path);\n\t\t\tif (map_fd < 0)\n\t\t\t\treturn -1;\n\t\t}\n\t} else {\n\t\tmap_fd = bpf_obj_get(path);\n\t\tif (map_fd < 0)\n\t\t\treturn -1;\n\t}\n\n\tif (!bperf_attr_map_compatible(map_fd)) {\n\t\tclose(map_fd);\n\t\treturn -1;\n\n\t}\n\terr = flock(map_fd, LOCK_EX);\n\tif (err) {\n\t\tclose(map_fd);\n\t\treturn -1;\n\t}\n\treturn map_fd;\n}\n\nstatic int bperf_check_target(struct evsel *evsel,\n\t\t\t      struct target *target,\n\t\t\t      enum bperf_filter_type *filter_type,\n\t\t\t      __u32 *filter_entry_cnt)\n{\n\tif (evsel->core.leader->nr_members > 1) {\n\t\tpr_err(\"bpf managed perf events do not yet support groups.\\n\");\n\t\treturn -1;\n\t}\n\n\t \n\tif (target->system_wide) {\n\t\t*filter_type = BPERF_FILTER_GLOBAL;\n\t\t*filter_entry_cnt = 1;\n\t} else if (target->cpu_list) {\n\t\t*filter_type = BPERF_FILTER_CPU;\n\t\t*filter_entry_cnt = perf_cpu_map__nr(evsel__cpus(evsel));\n\t} else if (target->tid) {\n\t\t*filter_type = BPERF_FILTER_PID;\n\t\t*filter_entry_cnt = perf_thread_map__nr(evsel->core.threads);\n\t} else if (target->pid || evsel->evlist->workload.pid != -1) {\n\t\t*filter_type = BPERF_FILTER_TGID;\n\t\t*filter_entry_cnt = perf_thread_map__nr(evsel->core.threads);\n\t} else {\n\t\tpr_err(\"bpf managed perf events do not yet support these targets.\\n\");\n\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic\tstruct perf_cpu_map *all_cpu_map;\n\nstatic int bperf_reload_leader_program(struct evsel *evsel, int attr_map_fd,\n\t\t\t\t       struct perf_event_attr_map_entry *entry)\n{\n\tstruct bperf_leader_bpf *skel = bperf_leader_bpf__open();\n\tint link_fd, diff_map_fd, err;\n\tstruct bpf_link *link = NULL;\n\n\tif (!skel) {\n\t\tpr_err(\"Failed to open leader skeleton\\n\");\n\t\treturn -1;\n\t}\n\n\tbpf_map__set_max_entries(skel->maps.events, libbpf_num_possible_cpus());\n\terr = bperf_leader_bpf__load(skel);\n\tif (err) {\n\t\tpr_err(\"Failed to load leader skeleton\\n\");\n\t\tgoto out;\n\t}\n\n\tlink = bpf_program__attach(skel->progs.on_switch);\n\tif (IS_ERR(link)) {\n\t\tpr_err(\"Failed to attach leader program\\n\");\n\t\terr = PTR_ERR(link);\n\t\tgoto out;\n\t}\n\n\tlink_fd = bpf_link__fd(link);\n\tdiff_map_fd = bpf_map__fd(skel->maps.diff_readings);\n\tentry->link_id = bpf_link_get_id(link_fd);\n\tentry->diff_map_id = bpf_map_get_id(diff_map_fd);\n\terr = bpf_map_update_elem(attr_map_fd, &evsel->core.attr, entry, BPF_ANY);\n\tassert(err == 0);\n\n\tevsel->bperf_leader_link_fd = bpf_link_get_fd_by_id(entry->link_id);\n\tassert(evsel->bperf_leader_link_fd >= 0);\n\n\t \n\tevsel->leader_skel = skel;\n\tevsel__open_per_cpu(evsel, all_cpu_map, -1);\n\nout:\n\tbperf_leader_bpf__destroy(skel);\n\tbpf_link__destroy(link);\n\treturn err;\n}\n\nstatic int bperf__load(struct evsel *evsel, struct target *target)\n{\n\tstruct perf_event_attr_map_entry entry = {0xffffffff, 0xffffffff};\n\tint attr_map_fd, diff_map_fd = -1, err;\n\tenum bperf_filter_type filter_type;\n\t__u32 filter_entry_cnt, i;\n\n\tif (bperf_check_target(evsel, target, &filter_type, &filter_entry_cnt))\n\t\treturn -1;\n\n\tif (!all_cpu_map) {\n\t\tall_cpu_map = perf_cpu_map__new(NULL);\n\t\tif (!all_cpu_map)\n\t\t\treturn -1;\n\t}\n\n\tevsel->bperf_leader_prog_fd = -1;\n\tevsel->bperf_leader_link_fd = -1;\n\n\t \n\tattr_map_fd = bperf_lock_attr_map(target);\n\tif (attr_map_fd < 0) {\n\t\tpr_err(\"Failed to lock perf_event_attr map\\n\");\n\t\treturn -1;\n\t}\n\n\terr = bpf_map_lookup_elem(attr_map_fd, &evsel->core.attr, &entry);\n\tif (err) {\n\t\terr = bpf_map_update_elem(attr_map_fd, &evsel->core.attr, &entry, BPF_ANY);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tevsel->bperf_leader_link_fd = bpf_link_get_fd_by_id(entry.link_id);\n\tif (evsel->bperf_leader_link_fd < 0 &&\n\t    bperf_reload_leader_program(evsel, attr_map_fd, &entry)) {\n\t\terr = -1;\n\t\tgoto out;\n\t}\n\t \n\tevsel->bperf_leader_prog_fd = bpf_prog_get_fd_by_id(\n\t\tbpf_link_get_prog_id(evsel->bperf_leader_link_fd));\n\tassert(evsel->bperf_leader_prog_fd >= 0);\n\n\tdiff_map_fd = bpf_map_get_fd_by_id(entry.diff_map_id);\n\tassert(diff_map_fd >= 0);\n\n\t \n\terr = bperf_trigger_reading(evsel->bperf_leader_prog_fd, 0);\n\tif (err) {\n\t\tpr_err(\"The kernel does not support test_run for raw_tp BPF programs.\\n\"\n\t\t       \"Therefore, --use-bpf might show inaccurate readings\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tevsel->follower_skel = bperf_follower_bpf__open();\n\tif (!evsel->follower_skel) {\n\t\terr = -1;\n\t\tpr_err(\"Failed to open follower skeleton\\n\");\n\t\tgoto out;\n\t}\n\n\t \n\tbpf_program__set_attach_target(evsel->follower_skel->progs.fexit_XXX,\n\t\t\t\t       evsel->bperf_leader_prog_fd, \"on_switch\");\n\n\t \n\tbpf_map__reuse_fd(evsel->follower_skel->maps.diff_readings, diff_map_fd);\n\n\t \n\tbpf_map__set_max_entries(evsel->follower_skel->maps.accum_readings,\n\t\t\t\t filter_entry_cnt);\n\t \n\tbpf_map__set_max_entries(evsel->follower_skel->maps.filter,\n\t\t\t\t filter_entry_cnt);\n\terr = bperf_follower_bpf__load(evsel->follower_skel);\n\tif (err) {\n\t\tpr_err(\"Failed to load follower skeleton\\n\");\n\t\tbperf_follower_bpf__destroy(evsel->follower_skel);\n\t\tevsel->follower_skel = NULL;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < filter_entry_cnt; i++) {\n\t\tint filter_map_fd;\n\t\t__u32 key;\n\n\t\tif (filter_type == BPERF_FILTER_PID ||\n\t\t    filter_type == BPERF_FILTER_TGID)\n\t\t\tkey = perf_thread_map__pid(evsel->core.threads, i);\n\t\telse if (filter_type == BPERF_FILTER_CPU)\n\t\t\tkey = perf_cpu_map__cpu(evsel->core.cpus, i).cpu;\n\t\telse\n\t\t\tbreak;\n\n\t\tfilter_map_fd = bpf_map__fd(evsel->follower_skel->maps.filter);\n\t\tbpf_map_update_elem(filter_map_fd, &key, &i, BPF_ANY);\n\t}\n\n\tevsel->follower_skel->bss->type = filter_type;\n\n\terr = bperf_follower_bpf__attach(evsel->follower_skel);\n\nout:\n\tif (err && evsel->bperf_leader_link_fd >= 0)\n\t\tclose(evsel->bperf_leader_link_fd);\n\tif (err && evsel->bperf_leader_prog_fd >= 0)\n\t\tclose(evsel->bperf_leader_prog_fd);\n\tif (diff_map_fd >= 0)\n\t\tclose(diff_map_fd);\n\n\tflock(attr_map_fd, LOCK_UN);\n\tclose(attr_map_fd);\n\n\treturn err;\n}\n\nstatic int bperf__install_pe(struct evsel *evsel, int cpu_map_idx, int fd)\n{\n\tstruct bperf_leader_bpf *skel = evsel->leader_skel;\n\n\treturn bpf_map_update_elem(bpf_map__fd(skel->maps.events),\n\t\t\t\t   &cpu_map_idx, &fd, BPF_ANY);\n}\n\n \nstatic int bperf_sync_counters(struct evsel *evsel)\n{\n\tint num_cpu, i, cpu;\n\n\tnum_cpu = perf_cpu_map__nr(all_cpu_map);\n\tfor (i = 0; i < num_cpu; i++) {\n\t\tcpu = perf_cpu_map__cpu(all_cpu_map, i).cpu;\n\t\tbperf_trigger_reading(evsel->bperf_leader_prog_fd, cpu);\n\t}\n\treturn 0;\n}\n\nstatic int bperf__enable(struct evsel *evsel)\n{\n\tevsel->follower_skel->bss->enabled = 1;\n\treturn 0;\n}\n\nstatic int bperf__disable(struct evsel *evsel)\n{\n\tevsel->follower_skel->bss->enabled = 0;\n\treturn 0;\n}\n\nstatic int bperf__read(struct evsel *evsel)\n{\n\tstruct bperf_follower_bpf *skel = evsel->follower_skel;\n\t__u32 num_cpu_bpf = cpu__max_cpu().cpu;\n\tstruct bpf_perf_event_value values[num_cpu_bpf];\n\tstruct perf_counts_values *counts;\n\tint reading_map_fd, err = 0;\n\t__u32 i;\n\tint j;\n\n\tbperf_sync_counters(evsel);\n\treading_map_fd = bpf_map__fd(skel->maps.accum_readings);\n\n\tfor (i = 0; i < bpf_map__max_entries(skel->maps.accum_readings); i++) {\n\t\tstruct perf_cpu entry;\n\t\t__u32 cpu;\n\n\t\terr = bpf_map_lookup_elem(reading_map_fd, &i, values);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tswitch (evsel->follower_skel->bss->type) {\n\t\tcase BPERF_FILTER_GLOBAL:\n\t\t\tassert(i == 0);\n\n\t\t\tperf_cpu_map__for_each_cpu(entry, j, evsel__cpus(evsel)) {\n\t\t\t\tcounts = perf_counts(evsel->counts, j, 0);\n\t\t\t\tcounts->val = values[entry.cpu].counter;\n\t\t\t\tcounts->ena = values[entry.cpu].enabled;\n\t\t\t\tcounts->run = values[entry.cpu].running;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BPERF_FILTER_CPU:\n\t\t\tcpu = perf_cpu_map__cpu(evsel__cpus(evsel), i).cpu;\n\t\t\tassert(cpu >= 0);\n\t\t\tcounts = perf_counts(evsel->counts, i, 0);\n\t\t\tcounts->val = values[cpu].counter;\n\t\t\tcounts->ena = values[cpu].enabled;\n\t\t\tcounts->run = values[cpu].running;\n\t\t\tbreak;\n\t\tcase BPERF_FILTER_PID:\n\t\tcase BPERF_FILTER_TGID:\n\t\t\tcounts = perf_counts(evsel->counts, 0, i);\n\t\t\tcounts->val = 0;\n\t\t\tcounts->ena = 0;\n\t\t\tcounts->run = 0;\n\n\t\t\tfor (cpu = 0; cpu < num_cpu_bpf; cpu++) {\n\t\t\t\tcounts->val += values[cpu].counter;\n\t\t\t\tcounts->ena += values[cpu].enabled;\n\t\t\t\tcounts->run += values[cpu].running;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n\nstatic int bperf__destroy(struct evsel *evsel)\n{\n\tbperf_follower_bpf__destroy(evsel->follower_skel);\n\tclose(evsel->bperf_leader_prog_fd);\n\tclose(evsel->bperf_leader_link_fd);\n\treturn 0;\n}\n\n \n\nstruct bpf_counter_ops bperf_ops = {\n\t.load       = bperf__load,\n\t.enable     = bperf__enable,\n\t.disable    = bperf__disable,\n\t.read       = bperf__read,\n\t.install_pe = bperf__install_pe,\n\t.destroy    = bperf__destroy,\n};\n\nextern struct bpf_counter_ops bperf_cgrp_ops;\n\nstatic inline bool bpf_counter_skip(struct evsel *evsel)\n{\n\treturn evsel->bpf_counter_ops == NULL;\n}\n\nint bpf_counter__install_pe(struct evsel *evsel, int cpu_map_idx, int fd)\n{\n\tif (bpf_counter_skip(evsel))\n\t\treturn 0;\n\treturn evsel->bpf_counter_ops->install_pe(evsel, cpu_map_idx, fd);\n}\n\nint bpf_counter__load(struct evsel *evsel, struct target *target)\n{\n\tif (target->bpf_str)\n\t\tevsel->bpf_counter_ops = &bpf_program_profiler_ops;\n\telse if (cgrp_event_expanded && target->use_bpf)\n\t\tevsel->bpf_counter_ops = &bperf_cgrp_ops;\n\telse if (target->use_bpf || evsel->bpf_counter ||\n\t\t evsel__match_bpf_counter_events(evsel->name))\n\t\tevsel->bpf_counter_ops = &bperf_ops;\n\n\tif (evsel->bpf_counter_ops)\n\t\treturn evsel->bpf_counter_ops->load(evsel, target);\n\treturn 0;\n}\n\nint bpf_counter__enable(struct evsel *evsel)\n{\n\tif (bpf_counter_skip(evsel))\n\t\treturn 0;\n\treturn evsel->bpf_counter_ops->enable(evsel);\n}\n\nint bpf_counter__disable(struct evsel *evsel)\n{\n\tif (bpf_counter_skip(evsel))\n\t\treturn 0;\n\treturn evsel->bpf_counter_ops->disable(evsel);\n}\n\nint bpf_counter__read(struct evsel *evsel)\n{\n\tif (bpf_counter_skip(evsel))\n\t\treturn -EAGAIN;\n\treturn evsel->bpf_counter_ops->read(evsel);\n}\n\nvoid bpf_counter__destroy(struct evsel *evsel)\n{\n\tif (bpf_counter_skip(evsel))\n\t\treturn;\n\tevsel->bpf_counter_ops->destroy(evsel);\n\tevsel->bpf_counter_ops = NULL;\n\tevsel->bpf_skel = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}