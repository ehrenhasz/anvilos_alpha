{
  "module_name": "env.c",
  "hash_id": "2e6ff23bf57005f976fc83257121f2514c7a9675a7e8bac498c1865499c1f647",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/env.c",
  "human_readable_source": "\n#include \"cpumap.h\"\n#include \"debug.h\"\n#include \"env.h\"\n#include \"util/header.h\"\n#include <linux/ctype.h>\n#include <linux/zalloc.h>\n#include \"cgroup.h\"\n#include <errno.h>\n#include <sys/utsname.h>\n#include <stdlib.h>\n#include <string.h>\n#include \"pmus.h\"\n#include \"strbuf.h\"\n\nstruct perf_env perf_env;\n\n#ifdef HAVE_LIBBPF_SUPPORT\n#include \"bpf-event.h\"\n#include \"bpf-utils.h\"\n#include <bpf/libbpf.h>\n\nvoid perf_env__insert_bpf_prog_info(struct perf_env *env,\n\t\t\t\t    struct bpf_prog_info_node *info_node)\n{\n\tdown_write(&env->bpf_progs.lock);\n\t__perf_env__insert_bpf_prog_info(env, info_node);\n\tup_write(&env->bpf_progs.lock);\n}\n\nvoid __perf_env__insert_bpf_prog_info(struct perf_env *env, struct bpf_prog_info_node *info_node)\n{\n\t__u32 prog_id = info_node->info_linear->info.id;\n\tstruct bpf_prog_info_node *node;\n\tstruct rb_node *parent = NULL;\n\tstruct rb_node **p;\n\n\tp = &env->bpf_progs.infos.rb_node;\n\n\twhile (*p != NULL) {\n\t\tparent = *p;\n\t\tnode = rb_entry(parent, struct bpf_prog_info_node, rb_node);\n\t\tif (prog_id < node->info_linear->info.id) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (prog_id > node->info_linear->info.id) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tpr_debug(\"duplicated bpf prog info %u\\n\", prog_id);\n\t\t\treturn;\n\t\t}\n\t}\n\n\trb_link_node(&info_node->rb_node, parent, p);\n\trb_insert_color(&info_node->rb_node, &env->bpf_progs.infos);\n\tenv->bpf_progs.infos_cnt++;\n}\n\nstruct bpf_prog_info_node *perf_env__find_bpf_prog_info(struct perf_env *env,\n\t\t\t\t\t\t\t__u32 prog_id)\n{\n\tstruct bpf_prog_info_node *node = NULL;\n\tstruct rb_node *n;\n\n\tdown_read(&env->bpf_progs.lock);\n\tn = env->bpf_progs.infos.rb_node;\n\n\twhile (n) {\n\t\tnode = rb_entry(n, struct bpf_prog_info_node, rb_node);\n\t\tif (prog_id < node->info_linear->info.id)\n\t\t\tn = n->rb_left;\n\t\telse if (prog_id > node->info_linear->info.id)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\tgoto out;\n\t}\n\tnode = NULL;\n\nout:\n\tup_read(&env->bpf_progs.lock);\n\treturn node;\n}\n\nbool perf_env__insert_btf(struct perf_env *env, struct btf_node *btf_node)\n{\n\tbool ret;\n\n\tdown_write(&env->bpf_progs.lock);\n\tret = __perf_env__insert_btf(env, btf_node);\n\tup_write(&env->bpf_progs.lock);\n\treturn ret;\n}\n\nbool __perf_env__insert_btf(struct perf_env *env, struct btf_node *btf_node)\n{\n\tstruct rb_node *parent = NULL;\n\t__u32 btf_id = btf_node->id;\n\tstruct btf_node *node;\n\tstruct rb_node **p;\n\n\tp = &env->bpf_progs.btfs.rb_node;\n\n\twhile (*p != NULL) {\n\t\tparent = *p;\n\t\tnode = rb_entry(parent, struct btf_node, rb_node);\n\t\tif (btf_id < node->id) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (btf_id > node->id) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\tpr_debug(\"duplicated btf %u\\n\", btf_id);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\trb_link_node(&btf_node->rb_node, parent, p);\n\trb_insert_color(&btf_node->rb_node, &env->bpf_progs.btfs);\n\tenv->bpf_progs.btfs_cnt++;\n\treturn true;\n}\n\nstruct btf_node *perf_env__find_btf(struct perf_env *env, __u32 btf_id)\n{\n\tstruct btf_node *res;\n\n\tdown_read(&env->bpf_progs.lock);\n\tres = __perf_env__find_btf(env, btf_id);\n\tup_read(&env->bpf_progs.lock);\n\treturn res;\n}\n\nstruct btf_node *__perf_env__find_btf(struct perf_env *env, __u32 btf_id)\n{\n\tstruct btf_node *node = NULL;\n\tstruct rb_node *n;\n\n\tn = env->bpf_progs.btfs.rb_node;\n\n\twhile (n) {\n\t\tnode = rb_entry(n, struct btf_node, rb_node);\n\t\tif (btf_id < node->id)\n\t\t\tn = n->rb_left;\n\t\telse if (btf_id > node->id)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\treturn node;\n\t}\n\treturn NULL;\n}\n\n \nstatic void perf_env__purge_bpf(struct perf_env *env)\n{\n\tstruct rb_root *root;\n\tstruct rb_node *next;\n\n\tdown_write(&env->bpf_progs.lock);\n\n\troot = &env->bpf_progs.infos;\n\tnext = rb_first(root);\n\n\twhile (next) {\n\t\tstruct bpf_prog_info_node *node;\n\n\t\tnode = rb_entry(next, struct bpf_prog_info_node, rb_node);\n\t\tnext = rb_next(&node->rb_node);\n\t\trb_erase(&node->rb_node, root);\n\t\tzfree(&node->info_linear);\n\t\tfree(node);\n\t}\n\n\tenv->bpf_progs.infos_cnt = 0;\n\n\troot = &env->bpf_progs.btfs;\n\tnext = rb_first(root);\n\n\twhile (next) {\n\t\tstruct btf_node *node;\n\n\t\tnode = rb_entry(next, struct btf_node, rb_node);\n\t\tnext = rb_next(&node->rb_node);\n\t\trb_erase(&node->rb_node, root);\n\t\tfree(node);\n\t}\n\n\tenv->bpf_progs.btfs_cnt = 0;\n\n\tup_write(&env->bpf_progs.lock);\n}\n#else \nstatic void perf_env__purge_bpf(struct perf_env *env __maybe_unused)\n{\n}\n#endif \n\nvoid perf_env__exit(struct perf_env *env)\n{\n\tint i, j;\n\n\tperf_env__purge_bpf(env);\n\tperf_env__purge_cgroups(env);\n\tzfree(&env->hostname);\n\tzfree(&env->os_release);\n\tzfree(&env->version);\n\tzfree(&env->arch);\n\tzfree(&env->cpu_desc);\n\tzfree(&env->cpuid);\n\tzfree(&env->cmdline);\n\tzfree(&env->cmdline_argv);\n\tzfree(&env->sibling_dies);\n\tzfree(&env->sibling_cores);\n\tzfree(&env->sibling_threads);\n\tzfree(&env->pmu_mappings);\n\tzfree(&env->cpu);\n\tfor (i = 0; i < env->nr_cpu_pmu_caps; i++)\n\t\tzfree(&env->cpu_pmu_caps[i]);\n\tzfree(&env->cpu_pmu_caps);\n\tzfree(&env->numa_map);\n\n\tfor (i = 0; i < env->nr_numa_nodes; i++)\n\t\tperf_cpu_map__put(env->numa_nodes[i].map);\n\tzfree(&env->numa_nodes);\n\n\tfor (i = 0; i < env->caches_cnt; i++)\n\t\tcpu_cache_level__free(&env->caches[i]);\n\tzfree(&env->caches);\n\n\tfor (i = 0; i < env->nr_memory_nodes; i++)\n\t\tzfree(&env->memory_nodes[i].set);\n\tzfree(&env->memory_nodes);\n\n\tfor (i = 0; i < env->nr_hybrid_nodes; i++) {\n\t\tzfree(&env->hybrid_nodes[i].pmu_name);\n\t\tzfree(&env->hybrid_nodes[i].cpus);\n\t}\n\tzfree(&env->hybrid_nodes);\n\n\tfor (i = 0; i < env->nr_pmus_with_caps; i++) {\n\t\tfor (j = 0; j < env->pmu_caps[i].nr_caps; j++)\n\t\t\tzfree(&env->pmu_caps[i].caps[j]);\n\t\tzfree(&env->pmu_caps[i].caps);\n\t\tzfree(&env->pmu_caps[i].pmu_name);\n\t}\n\tzfree(&env->pmu_caps);\n}\n\nvoid perf_env__init(struct perf_env *env)\n{\n#ifdef HAVE_LIBBPF_SUPPORT\n\tenv->bpf_progs.infos = RB_ROOT;\n\tenv->bpf_progs.btfs = RB_ROOT;\n\tinit_rwsem(&env->bpf_progs.lock);\n#endif\n\tenv->kernel_is_64_bit = -1;\n}\n\nstatic void perf_env__init_kernel_mode(struct perf_env *env)\n{\n\tconst char *arch = perf_env__raw_arch(env);\n\n\tif (!strncmp(arch, \"x86_64\", 6) || !strncmp(arch, \"aarch64\", 7) ||\n\t    !strncmp(arch, \"arm64\", 5) || !strncmp(arch, \"mips64\", 6) ||\n\t    !strncmp(arch, \"parisc64\", 8) || !strncmp(arch, \"riscv64\", 7) ||\n\t    !strncmp(arch, \"s390x\", 5) || !strncmp(arch, \"sparc64\", 7))\n\t\tenv->kernel_is_64_bit = 1;\n\telse\n\t\tenv->kernel_is_64_bit = 0;\n}\n\nint perf_env__kernel_is_64_bit(struct perf_env *env)\n{\n\tif (env->kernel_is_64_bit == -1)\n\t\tperf_env__init_kernel_mode(env);\n\n\treturn env->kernel_is_64_bit;\n}\n\nint perf_env__set_cmdline(struct perf_env *env, int argc, const char *argv[])\n{\n\tint i;\n\n\t \n\tenv->cmdline_argv = calloc(argc, sizeof(char *));\n\tif (env->cmdline_argv == NULL)\n\t\tgoto out_enomem;\n\n\t \n\tfor (i = 0; i < argc ; i++) {\n\t\tenv->cmdline_argv[i] = argv[i];\n\t\tif (env->cmdline_argv[i] == NULL)\n\t\t\tgoto out_free;\n\t}\n\n\tenv->nr_cmdline = argc;\n\n\treturn 0;\nout_free:\n\tzfree(&env->cmdline_argv);\nout_enomem:\n\treturn -ENOMEM;\n}\n\nint perf_env__read_cpu_topology_map(struct perf_env *env)\n{\n\tint idx, nr_cpus;\n\n\tif (env->cpu != NULL)\n\t\treturn 0;\n\n\tif (env->nr_cpus_avail == 0)\n\t\tenv->nr_cpus_avail = cpu__max_present_cpu().cpu;\n\n\tnr_cpus = env->nr_cpus_avail;\n\tif (nr_cpus == -1)\n\t\treturn -EINVAL;\n\n\tenv->cpu = calloc(nr_cpus, sizeof(env->cpu[0]));\n\tif (env->cpu == NULL)\n\t\treturn -ENOMEM;\n\n\tfor (idx = 0; idx < nr_cpus; ++idx) {\n\t\tstruct perf_cpu cpu = { .cpu = idx };\n\n\t\tenv->cpu[idx].core_id\t= cpu__get_core_id(cpu);\n\t\tenv->cpu[idx].socket_id\t= cpu__get_socket_id(cpu);\n\t\tenv->cpu[idx].die_id\t= cpu__get_die_id(cpu);\n\t}\n\n\tenv->nr_cpus_avail = nr_cpus;\n\treturn 0;\n}\n\nint perf_env__read_pmu_mappings(struct perf_env *env)\n{\n\tstruct perf_pmu *pmu = NULL;\n\tu32 pmu_num = 0;\n\tstruct strbuf sb;\n\n\twhile ((pmu = perf_pmus__scan(pmu)))\n\t\tpmu_num++;\n\n\tif (!pmu_num) {\n\t\tpr_debug(\"pmu mappings not available\\n\");\n\t\treturn -ENOENT;\n\t}\n\tenv->nr_pmu_mappings = pmu_num;\n\n\tif (strbuf_init(&sb, 128 * pmu_num) < 0)\n\t\treturn -ENOMEM;\n\n\twhile ((pmu = perf_pmus__scan(pmu))) {\n\t\tif (strbuf_addf(&sb, \"%u:%s\", pmu->type, pmu->name) < 0)\n\t\t\tgoto error;\n\t\t \n\t\tif (strbuf_add(&sb, \"\", 1) < 0)\n\t\t\tgoto error;\n\t}\n\n\tenv->pmu_mappings = strbuf_detach(&sb, NULL);\n\n\treturn 0;\n\nerror:\n\tstrbuf_release(&sb);\n\treturn -1;\n}\n\nint perf_env__read_cpuid(struct perf_env *env)\n{\n\tchar cpuid[128];\n\tint err = get_cpuid(cpuid, sizeof(cpuid));\n\n\tif (err)\n\t\treturn err;\n\n\tfree(env->cpuid);\n\tenv->cpuid = strdup(cpuid);\n\tif (env->cpuid == NULL)\n\t\treturn ENOMEM;\n\treturn 0;\n}\n\nstatic int perf_env__read_arch(struct perf_env *env)\n{\n\tstruct utsname uts;\n\n\tif (env->arch)\n\t\treturn 0;\n\n\tif (!uname(&uts))\n\t\tenv->arch = strdup(uts.machine);\n\n\treturn env->arch ? 0 : -ENOMEM;\n}\n\nstatic int perf_env__read_nr_cpus_avail(struct perf_env *env)\n{\n\tif (env->nr_cpus_avail == 0)\n\t\tenv->nr_cpus_avail = cpu__max_present_cpu().cpu;\n\n\treturn env->nr_cpus_avail ? 0 : -ENOENT;\n}\n\nconst char *perf_env__raw_arch(struct perf_env *env)\n{\n\treturn env && !perf_env__read_arch(env) ? env->arch : \"unknown\";\n}\n\nint perf_env__nr_cpus_avail(struct perf_env *env)\n{\n\treturn env && !perf_env__read_nr_cpus_avail(env) ? env->nr_cpus_avail : 0;\n}\n\nvoid cpu_cache_level__free(struct cpu_cache_level *cache)\n{\n\tzfree(&cache->type);\n\tzfree(&cache->map);\n\tzfree(&cache->size);\n}\n\n \nstatic const char *normalize_arch(char *arch)\n{\n\tif (!strcmp(arch, \"x86_64\"))\n\t\treturn \"x86\";\n\tif (arch[0] == 'i' && arch[2] == '8' && arch[3] == '6')\n\t\treturn \"x86\";\n\tif (!strcmp(arch, \"sun4u\") || !strncmp(arch, \"sparc\", 5))\n\t\treturn \"sparc\";\n\tif (!strncmp(arch, \"aarch64\", 7) || !strncmp(arch, \"arm64\", 5))\n\t\treturn \"arm64\";\n\tif (!strncmp(arch, \"arm\", 3) || !strcmp(arch, \"sa110\"))\n\t\treturn \"arm\";\n\tif (!strncmp(arch, \"s390\", 4))\n\t\treturn \"s390\";\n\tif (!strncmp(arch, \"parisc\", 6))\n\t\treturn \"parisc\";\n\tif (!strncmp(arch, \"powerpc\", 7) || !strncmp(arch, \"ppc\", 3))\n\t\treturn \"powerpc\";\n\tif (!strncmp(arch, \"mips\", 4))\n\t\treturn \"mips\";\n\tif (!strncmp(arch, \"sh\", 2) && isdigit(arch[2]))\n\t\treturn \"sh\";\n\tif (!strncmp(arch, \"loongarch\", 9))\n\t\treturn \"loongarch\";\n\n\treturn arch;\n}\n\nconst char *perf_env__arch(struct perf_env *env)\n{\n\tchar *arch_name;\n\n\tif (!env || !env->arch) {  \n\t\tstatic struct utsname uts = { .machine[0] = '\\0', };\n\t\tif (uts.machine[0] == '\\0' && uname(&uts) < 0)\n\t\t\treturn NULL;\n\t\tarch_name = uts.machine;\n\t} else\n\t\tarch_name = env->arch;\n\n\treturn normalize_arch(arch_name);\n}\n\nconst char *perf_env__cpuid(struct perf_env *env)\n{\n\tint status;\n\n\tif (!env || !env->cpuid) {  \n\t\tstatus = perf_env__read_cpuid(env);\n\t\tif (status)\n\t\t\treturn NULL;\n\t}\n\n\treturn env->cpuid;\n}\n\nint perf_env__nr_pmu_mappings(struct perf_env *env)\n{\n\tint status;\n\n\tif (!env || !env->nr_pmu_mappings) {  \n\t\tstatus = perf_env__read_pmu_mappings(env);\n\t\tif (status)\n\t\t\treturn 0;\n\t}\n\n\treturn env->nr_pmu_mappings;\n}\n\nconst char *perf_env__pmu_mappings(struct perf_env *env)\n{\n\tint status;\n\n\tif (!env || !env->pmu_mappings) {  \n\t\tstatus = perf_env__read_pmu_mappings(env);\n\t\tif (status)\n\t\t\treturn NULL;\n\t}\n\n\treturn env->pmu_mappings;\n}\n\nint perf_env__numa_node(struct perf_env *env, struct perf_cpu cpu)\n{\n\tif (!env->nr_numa_map) {\n\t\tstruct numa_node *nn;\n\t\tint i, nr = 0;\n\n\t\tfor (i = 0; i < env->nr_numa_nodes; i++) {\n\t\t\tnn = &env->numa_nodes[i];\n\t\t\tnr = max(nr, perf_cpu_map__max(nn->map).cpu);\n\t\t}\n\n\t\tnr++;\n\n\t\t \n\t\tenv->numa_map = malloc(nr * sizeof(int));\n\t\tif (!env->numa_map)\n\t\t\treturn -1;\n\n\t\tfor (i = 0; i < nr; i++)\n\t\t\tenv->numa_map[i] = -1;\n\n\t\tenv->nr_numa_map = nr;\n\n\t\tfor (i = 0; i < env->nr_numa_nodes; i++) {\n\t\t\tstruct perf_cpu tmp;\n\t\t\tint j;\n\n\t\t\tnn = &env->numa_nodes[i];\n\t\t\tperf_cpu_map__for_each_cpu(tmp, j, nn->map)\n\t\t\t\tenv->numa_map[tmp.cpu] = i;\n\t\t}\n\t}\n\n\treturn cpu.cpu >= 0 && cpu.cpu < env->nr_numa_map ? env->numa_map[cpu.cpu] : -1;\n}\n\nchar *perf_env__find_pmu_cap(struct perf_env *env, const char *pmu_name,\n\t\t\t     const char *cap)\n{\n\tchar *cap_eq;\n\tint cap_size;\n\tchar **ptr;\n\tint i, j;\n\n\tif (!pmu_name || !cap)\n\t\treturn NULL;\n\n\tcap_size = strlen(cap);\n\tcap_eq = zalloc(cap_size + 2);\n\tif (!cap_eq)\n\t\treturn NULL;\n\n\tmemcpy(cap_eq, cap, cap_size);\n\tcap_eq[cap_size] = '=';\n\n\tif (!strcmp(pmu_name, \"cpu\")) {\n\t\tfor (i = 0; i < env->nr_cpu_pmu_caps; i++) {\n\t\t\tif (!strncmp(env->cpu_pmu_caps[i], cap_eq, cap_size + 1)) {\n\t\t\t\tfree(cap_eq);\n\t\t\t\treturn &env->cpu_pmu_caps[i][cap_size + 1];\n\t\t\t}\n\t\t}\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < env->nr_pmus_with_caps; i++) {\n\t\tif (strcmp(env->pmu_caps[i].pmu_name, pmu_name))\n\t\t\tcontinue;\n\n\t\tptr = env->pmu_caps[i].caps;\n\n\t\tfor (j = 0; j < env->pmu_caps[i].nr_caps; j++) {\n\t\t\tif (!strncmp(ptr[j], cap_eq, cap_size + 1)) {\n\t\t\t\tfree(cap_eq);\n\t\t\t\treturn &ptr[j][cap_size + 1];\n\t\t\t}\n\t\t}\n\t}\n\nout:\n\tfree(cap_eq);\n\treturn NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}