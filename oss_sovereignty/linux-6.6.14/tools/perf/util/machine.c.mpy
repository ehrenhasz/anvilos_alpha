{
  "module_name": "machine.c",
  "hash_id": "8e2e523eae377d65da5ae7779dde27b6fadd80ab51d8fbb8ce3c4189fb8c757b",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/machine.c",
  "human_readable_source": "\n#include <dirent.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <regex.h>\n#include <stdlib.h>\n#include \"callchain.h\"\n#include \"debug.h\"\n#include \"dso.h\"\n#include \"env.h\"\n#include \"event.h\"\n#include \"evsel.h\"\n#include \"hist.h\"\n#include \"machine.h\"\n#include \"map.h\"\n#include \"map_symbol.h\"\n#include \"branch.h\"\n#include \"mem-events.h\"\n#include \"path.h\"\n#include \"srcline.h\"\n#include \"symbol.h\"\n#include \"sort.h\"\n#include \"strlist.h\"\n#include \"target.h\"\n#include \"thread.h\"\n#include \"util.h\"\n#include \"vdso.h\"\n#include <stdbool.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n#include \"unwind.h\"\n#include \"linux/hash.h\"\n#include \"asm/bug.h\"\n#include \"bpf-event.h\"\n#include <internal/lib.h> \n#include \"cgroup.h\"\n#include \"arm64-frame-pointer-unwind-support.h\"\n\n#include <linux/ctype.h>\n#include <symbol/kallsyms.h>\n#include <linux/mman.h>\n#include <linux/string.h>\n#include <linux/zalloc.h>\n\nstatic void __machine__remove_thread(struct machine *machine, struct thread_rb_node *nd,\n\t\t\t\t     struct thread *th, bool lock);\n\nstatic struct dso *machine__kernel_dso(struct machine *machine)\n{\n\treturn map__dso(machine->vmlinux_map);\n}\n\nstatic void dsos__init(struct dsos *dsos)\n{\n\tINIT_LIST_HEAD(&dsos->head);\n\tdsos->root = RB_ROOT;\n\tinit_rwsem(&dsos->lock);\n}\n\nstatic void machine__threads_init(struct machine *machine)\n{\n\tint i;\n\n\tfor (i = 0; i < THREADS__TABLE_SIZE; i++) {\n\t\tstruct threads *threads = &machine->threads[i];\n\t\tthreads->entries = RB_ROOT_CACHED;\n\t\tinit_rwsem(&threads->lock);\n\t\tthreads->nr = 0;\n\t\tINIT_LIST_HEAD(&threads->dead);\n\t\tthreads->last_match = NULL;\n\t}\n}\n\nstatic int thread_rb_node__cmp_tid(const void *key, const struct rb_node *nd)\n{\n\tint to_find = (int) *((pid_t *)key);\n\n\treturn to_find - (int)thread__tid(rb_entry(nd, struct thread_rb_node, rb_node)->thread);\n}\n\nstatic struct thread_rb_node *thread_rb_node__find(const struct thread *th,\n\t\t\t\t\t\t   struct rb_root *tree)\n{\n\tpid_t to_find = thread__tid(th);\n\tstruct rb_node *nd = rb_find(&to_find, tree, thread_rb_node__cmp_tid);\n\n\treturn rb_entry(nd, struct thread_rb_node, rb_node);\n}\n\nstatic int machine__set_mmap_name(struct machine *machine)\n{\n\tif (machine__is_host(machine))\n\t\tmachine->mmap_name = strdup(\"[kernel.kallsyms]\");\n\telse if (machine__is_default_guest(machine))\n\t\tmachine->mmap_name = strdup(\"[guest.kernel.kallsyms]\");\n\telse if (asprintf(&machine->mmap_name, \"[guest.kernel.kallsyms.%d]\",\n\t\t\t  machine->pid) < 0)\n\t\tmachine->mmap_name = NULL;\n\n\treturn machine->mmap_name ? 0 : -ENOMEM;\n}\n\nstatic void thread__set_guest_comm(struct thread *thread, pid_t pid)\n{\n\tchar comm[64];\n\n\tsnprintf(comm, sizeof(comm), \"[guest/%d]\", pid);\n\tthread__set_comm(thread, comm, 0);\n}\n\nint machine__init(struct machine *machine, const char *root_dir, pid_t pid)\n{\n\tint err = -ENOMEM;\n\n\tmemset(machine, 0, sizeof(*machine));\n\tmachine->kmaps = maps__new(machine);\n\tif (machine->kmaps == NULL)\n\t\treturn -ENOMEM;\n\n\tRB_CLEAR_NODE(&machine->rb_node);\n\tdsos__init(&machine->dsos);\n\n\tmachine__threads_init(machine);\n\n\tmachine->vdso_info = NULL;\n\tmachine->env = NULL;\n\n\tmachine->pid = pid;\n\n\tmachine->id_hdr_size = 0;\n\tmachine->kptr_restrict_warned = false;\n\tmachine->comm_exec = false;\n\tmachine->kernel_start = 0;\n\tmachine->vmlinux_map = NULL;\n\n\tmachine->root_dir = strdup(root_dir);\n\tif (machine->root_dir == NULL)\n\t\tgoto out;\n\n\tif (machine__set_mmap_name(machine))\n\t\tgoto out;\n\n\tif (pid != HOST_KERNEL_ID) {\n\t\tstruct thread *thread = machine__findnew_thread(machine, -1,\n\t\t\t\t\t\t\t\tpid);\n\n\t\tif (thread == NULL)\n\t\t\tgoto out;\n\n\t\tthread__set_guest_comm(thread, pid);\n\t\tthread__put(thread);\n\t}\n\n\tmachine->current_tid = NULL;\n\terr = 0;\n\nout:\n\tif (err) {\n\t\tzfree(&machine->kmaps);\n\t\tzfree(&machine->root_dir);\n\t\tzfree(&machine->mmap_name);\n\t}\n\treturn 0;\n}\n\nstruct machine *machine__new_host(void)\n{\n\tstruct machine *machine = malloc(sizeof(*machine));\n\n\tif (machine != NULL) {\n\t\tmachine__init(machine, \"\", HOST_KERNEL_ID);\n\n\t\tif (machine__create_kernel_maps(machine) < 0)\n\t\t\tgoto out_delete;\n\t}\n\n\treturn machine;\nout_delete:\n\tfree(machine);\n\treturn NULL;\n}\n\nstruct machine *machine__new_kallsyms(void)\n{\n\tstruct machine *machine = machine__new_host();\n\t \n\tif (machine && machine__load_kallsyms(machine, \"/proc/kallsyms\") <= 0) {\n\t\tmachine__delete(machine);\n\t\tmachine = NULL;\n\t}\n\n\treturn machine;\n}\n\nstatic void dsos__purge(struct dsos *dsos)\n{\n\tstruct dso *pos, *n;\n\n\tdown_write(&dsos->lock);\n\n\tlist_for_each_entry_safe(pos, n, &dsos->head, node) {\n\t\tRB_CLEAR_NODE(&pos->rb_node);\n\t\tpos->root = NULL;\n\t\tlist_del_init(&pos->node);\n\t\tdso__put(pos);\n\t}\n\n\tup_write(&dsos->lock);\n}\n\nstatic void dsos__exit(struct dsos *dsos)\n{\n\tdsos__purge(dsos);\n\texit_rwsem(&dsos->lock);\n}\n\nvoid machine__delete_threads(struct machine *machine)\n{\n\tstruct rb_node *nd;\n\tint i;\n\n\tfor (i = 0; i < THREADS__TABLE_SIZE; i++) {\n\t\tstruct threads *threads = &machine->threads[i];\n\t\tdown_write(&threads->lock);\n\t\tnd = rb_first_cached(&threads->entries);\n\t\twhile (nd) {\n\t\t\tstruct thread_rb_node *trb = rb_entry(nd, struct thread_rb_node, rb_node);\n\n\t\t\tnd = rb_next(nd);\n\t\t\t__machine__remove_thread(machine, trb, trb->thread, false);\n\t\t}\n\t\tup_write(&threads->lock);\n\t}\n}\n\nvoid machine__exit(struct machine *machine)\n{\n\tint i;\n\n\tif (machine == NULL)\n\t\treturn;\n\n\tmachine__destroy_kernel_maps(machine);\n\tmaps__zput(machine->kmaps);\n\tdsos__exit(&machine->dsos);\n\tmachine__exit_vdso(machine);\n\tzfree(&machine->root_dir);\n\tzfree(&machine->mmap_name);\n\tzfree(&machine->current_tid);\n\tzfree(&machine->kallsyms_filename);\n\n\tmachine__delete_threads(machine);\n\tfor (i = 0; i < THREADS__TABLE_SIZE; i++) {\n\t\tstruct threads *threads = &machine->threads[i];\n\n\t\texit_rwsem(&threads->lock);\n\t}\n}\n\nvoid machine__delete(struct machine *machine)\n{\n\tif (machine) {\n\t\tmachine__exit(machine);\n\t\tfree(machine);\n\t}\n}\n\nvoid machines__init(struct machines *machines)\n{\n\tmachine__init(&machines->host, \"\", HOST_KERNEL_ID);\n\tmachines->guests = RB_ROOT_CACHED;\n}\n\nvoid machines__exit(struct machines *machines)\n{\n\tmachine__exit(&machines->host);\n\t \n}\n\nstruct machine *machines__add(struct machines *machines, pid_t pid,\n\t\t\t      const char *root_dir)\n{\n\tstruct rb_node **p = &machines->guests.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct machine *pos, *machine = malloc(sizeof(*machine));\n\tbool leftmost = true;\n\n\tif (machine == NULL)\n\t\treturn NULL;\n\n\tif (machine__init(machine, root_dir, pid) != 0) {\n\t\tfree(machine);\n\t\treturn NULL;\n\t}\n\n\twhile (*p != NULL) {\n\t\tparent = *p;\n\t\tpos = rb_entry(parent, struct machine, rb_node);\n\t\tif (pid < pos->pid)\n\t\t\tp = &(*p)->rb_left;\n\t\telse {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&machine->rb_node, parent, p);\n\trb_insert_color_cached(&machine->rb_node, &machines->guests, leftmost);\n\n\tmachine->machines = machines;\n\n\treturn machine;\n}\n\nvoid machines__set_comm_exec(struct machines *machines, bool comm_exec)\n{\n\tstruct rb_node *nd;\n\n\tmachines->host.comm_exec = comm_exec;\n\n\tfor (nd = rb_first_cached(&machines->guests); nd; nd = rb_next(nd)) {\n\t\tstruct machine *machine = rb_entry(nd, struct machine, rb_node);\n\n\t\tmachine->comm_exec = comm_exec;\n\t}\n}\n\nstruct machine *machines__find(struct machines *machines, pid_t pid)\n{\n\tstruct rb_node **p = &machines->guests.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct machine *machine;\n\tstruct machine *default_machine = NULL;\n\n\tif (pid == HOST_KERNEL_ID)\n\t\treturn &machines->host;\n\n\twhile (*p != NULL) {\n\t\tparent = *p;\n\t\tmachine = rb_entry(parent, struct machine, rb_node);\n\t\tif (pid < machine->pid)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (pid > machine->pid)\n\t\t\tp = &(*p)->rb_right;\n\t\telse\n\t\t\treturn machine;\n\t\tif (!machine->pid)\n\t\t\tdefault_machine = machine;\n\t}\n\n\treturn default_machine;\n}\n\nstruct machine *machines__findnew(struct machines *machines, pid_t pid)\n{\n\tchar path[PATH_MAX];\n\tconst char *root_dir = \"\";\n\tstruct machine *machine = machines__find(machines, pid);\n\n\tif (machine && (machine->pid == pid))\n\t\tgoto out;\n\n\tif ((pid != HOST_KERNEL_ID) &&\n\t    (pid != DEFAULT_GUEST_KERNEL_ID) &&\n\t    (symbol_conf.guestmount)) {\n\t\tsprintf(path, \"%s/%d\", symbol_conf.guestmount, pid);\n\t\tif (access(path, R_OK)) {\n\t\t\tstatic struct strlist *seen;\n\n\t\t\tif (!seen)\n\t\t\t\tseen = strlist__new(NULL, NULL);\n\n\t\t\tif (!strlist__has_entry(seen, path)) {\n\t\t\t\tpr_err(\"Can't access file %s\\n\", path);\n\t\t\t\tstrlist__add(seen, path);\n\t\t\t}\n\t\t\tmachine = NULL;\n\t\t\tgoto out;\n\t\t}\n\t\troot_dir = path;\n\t}\n\n\tmachine = machines__add(machines, pid, root_dir);\nout:\n\treturn machine;\n}\n\nstruct machine *machines__find_guest(struct machines *machines, pid_t pid)\n{\n\tstruct machine *machine = machines__find(machines, pid);\n\n\tif (!machine)\n\t\tmachine = machines__findnew(machines, DEFAULT_GUEST_KERNEL_ID);\n\treturn machine;\n}\n\n \nstatic struct thread *findnew_guest_code(struct machine *machine,\n\t\t\t\t\t struct machine *host_machine,\n\t\t\t\t\t pid_t pid)\n{\n\tstruct thread *host_thread;\n\tstruct thread *thread;\n\tint err;\n\n\tif (!machine)\n\t\treturn NULL;\n\n\tthread = machine__findnew_thread(machine, -1, pid);\n\tif (!thread)\n\t\treturn NULL;\n\n\t \n\tif (maps__nr_maps(thread__maps(thread)))\n\t\treturn thread;\n\n\thost_thread = machine__find_thread(host_machine, -1, pid);\n\tif (!host_thread)\n\t\tgoto out_err;\n\n\tthread__set_guest_comm(thread, pid);\n\n\t \n\terr = maps__clone(thread, thread__maps(host_thread));\n\tthread__put(host_thread);\n\tif (err)\n\t\tgoto out_err;\n\n\treturn thread;\n\nout_err:\n\tthread__zput(thread);\n\treturn NULL;\n}\n\nstruct thread *machines__findnew_guest_code(struct machines *machines, pid_t pid)\n{\n\tstruct machine *host_machine = machines__find(machines, HOST_KERNEL_ID);\n\tstruct machine *machine = machines__findnew(machines, pid);\n\n\treturn findnew_guest_code(machine, host_machine, pid);\n}\n\nstruct thread *machine__findnew_guest_code(struct machine *machine, pid_t pid)\n{\n\tstruct machines *machines = machine->machines;\n\tstruct machine *host_machine;\n\n\tif (!machines)\n\t\treturn NULL;\n\n\thost_machine = machines__find(machines, HOST_KERNEL_ID);\n\n\treturn findnew_guest_code(machine, host_machine, pid);\n}\n\nvoid machines__process_guests(struct machines *machines,\n\t\t\t      machine__process_t process, void *data)\n{\n\tstruct rb_node *nd;\n\n\tfor (nd = rb_first_cached(&machines->guests); nd; nd = rb_next(nd)) {\n\t\tstruct machine *pos = rb_entry(nd, struct machine, rb_node);\n\t\tprocess(pos, data);\n\t}\n}\n\nvoid machines__set_id_hdr_size(struct machines *machines, u16 id_hdr_size)\n{\n\tstruct rb_node *node;\n\tstruct machine *machine;\n\n\tmachines->host.id_hdr_size = id_hdr_size;\n\n\tfor (node = rb_first_cached(&machines->guests); node;\n\t     node = rb_next(node)) {\n\t\tmachine = rb_entry(node, struct machine, rb_node);\n\t\tmachine->id_hdr_size = id_hdr_size;\n\t}\n\n\treturn;\n}\n\nstatic void machine__update_thread_pid(struct machine *machine,\n\t\t\t\t       struct thread *th, pid_t pid)\n{\n\tstruct thread *leader;\n\n\tif (pid == thread__pid(th) || pid == -1 || thread__pid(th) != -1)\n\t\treturn;\n\n\tthread__set_pid(th, pid);\n\n\tif (thread__pid(th) == thread__tid(th))\n\t\treturn;\n\n\tleader = __machine__findnew_thread(machine, thread__pid(th), thread__pid(th));\n\tif (!leader)\n\t\tgoto out_err;\n\n\tif (!thread__maps(leader))\n\t\tthread__set_maps(leader, maps__new(machine));\n\n\tif (!thread__maps(leader))\n\t\tgoto out_err;\n\n\tif (thread__maps(th) == thread__maps(leader))\n\t\tgoto out_put;\n\n\tif (thread__maps(th)) {\n\t\t \n\t\tif (!maps__empty(thread__maps(th)))\n\t\t\tpr_err(\"Discarding thread maps for %d:%d\\n\",\n\t\t\t\tthread__pid(th), thread__tid(th));\n\t\tmaps__put(thread__maps(th));\n\t}\n\n\tthread__set_maps(th, maps__get(thread__maps(leader)));\nout_put:\n\tthread__put(leader);\n\treturn;\nout_err:\n\tpr_err(\"Failed to join map groups for %d:%d\\n\", thread__pid(th), thread__tid(th));\n\tgoto out_put;\n}\n\n \nstatic struct thread*\n__threads__get_last_match(struct threads *threads, struct machine *machine,\n\t\t\t  int pid, int tid)\n{\n\tstruct thread *th;\n\n\tth = threads->last_match;\n\tif (th != NULL) {\n\t\tif (thread__tid(th) == tid) {\n\t\t\tmachine__update_thread_pid(machine, th, pid);\n\t\t\treturn thread__get(th);\n\t\t}\n\t\tthread__put(threads->last_match);\n\t\tthreads->last_match = NULL;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct thread*\nthreads__get_last_match(struct threads *threads, struct machine *machine,\n\t\t\tint pid, int tid)\n{\n\tstruct thread *th = NULL;\n\n\tif (perf_singlethreaded)\n\t\tth = __threads__get_last_match(threads, machine, pid, tid);\n\n\treturn th;\n}\n\nstatic void\n__threads__set_last_match(struct threads *threads, struct thread *th)\n{\n\tthread__put(threads->last_match);\n\tthreads->last_match = thread__get(th);\n}\n\nstatic void\nthreads__set_last_match(struct threads *threads, struct thread *th)\n{\n\tif (perf_singlethreaded)\n\t\t__threads__set_last_match(threads, th);\n}\n\n \nstatic struct thread *____machine__findnew_thread(struct machine *machine,\n\t\t\t\t\t\t  struct threads *threads,\n\t\t\t\t\t\t  pid_t pid, pid_t tid,\n\t\t\t\t\t\t  bool create)\n{\n\tstruct rb_node **p = &threads->entries.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct thread *th;\n\tstruct thread_rb_node *nd;\n\tbool leftmost = true;\n\n\tth = threads__get_last_match(threads, machine, pid, tid);\n\tif (th)\n\t\treturn th;\n\n\twhile (*p != NULL) {\n\t\tparent = *p;\n\t\tth = rb_entry(parent, struct thread_rb_node, rb_node)->thread;\n\n\t\tif (thread__tid(th) == tid) {\n\t\t\tthreads__set_last_match(threads, th);\n\t\t\tmachine__update_thread_pid(machine, th, pid);\n\t\t\treturn thread__get(th);\n\t\t}\n\n\t\tif (tid < thread__tid(th))\n\t\t\tp = &(*p)->rb_left;\n\t\telse {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\tif (!create)\n\t\treturn NULL;\n\n\tth = thread__new(pid, tid);\n\tif (th == NULL)\n\t\treturn NULL;\n\n\tnd = malloc(sizeof(*nd));\n\tif (nd == NULL) {\n\t\tthread__put(th);\n\t\treturn NULL;\n\t}\n\tnd->thread = th;\n\n\trb_link_node(&nd->rb_node, parent, p);\n\trb_insert_color_cached(&nd->rb_node, &threads->entries, leftmost);\n\t \n\tif (thread__init_maps(th, machine)) {\n\t\tpr_err(\"Thread init failed thread %d\\n\", pid);\n\t\trb_erase_cached(&nd->rb_node, &threads->entries);\n\t\tRB_CLEAR_NODE(&nd->rb_node);\n\t\tfree(nd);\n\t\tthread__put(th);\n\t\treturn NULL;\n\t}\n\t \n\tthreads__set_last_match(threads, th);\n\t++threads->nr;\n\n\treturn thread__get(th);\n}\n\nstruct thread *__machine__findnew_thread(struct machine *machine, pid_t pid, pid_t tid)\n{\n\treturn ____machine__findnew_thread(machine, machine__threads(machine, tid), pid, tid, true);\n}\n\nstruct thread *machine__findnew_thread(struct machine *machine, pid_t pid,\n\t\t\t\t       pid_t tid)\n{\n\tstruct threads *threads = machine__threads(machine, tid);\n\tstruct thread *th;\n\n\tdown_write(&threads->lock);\n\tth = __machine__findnew_thread(machine, pid, tid);\n\tup_write(&threads->lock);\n\treturn th;\n}\n\nstruct thread *machine__find_thread(struct machine *machine, pid_t pid,\n\t\t\t\t    pid_t tid)\n{\n\tstruct threads *threads = machine__threads(machine, tid);\n\tstruct thread *th;\n\n\tdown_read(&threads->lock);\n\tth =  ____machine__findnew_thread(machine, threads, pid, tid, false);\n\tup_read(&threads->lock);\n\treturn th;\n}\n\n \nstruct thread *machine__idle_thread(struct machine *machine)\n{\n\tstruct thread *thread = machine__findnew_thread(machine, 0, 0);\n\n\tif (!thread || thread__set_comm(thread, \"swapper\", 0) ||\n\t    thread__set_namespaces(thread, 0, NULL))\n\t\tpr_err(\"problem inserting idle task for machine pid %d\\n\", machine->pid);\n\n\treturn thread;\n}\n\nstruct comm *machine__thread_exec_comm(struct machine *machine,\n\t\t\t\t       struct thread *thread)\n{\n\tif (machine->comm_exec)\n\t\treturn thread__exec_comm(thread);\n\telse\n\t\treturn thread__comm(thread);\n}\n\nint machine__process_comm_event(struct machine *machine, union perf_event *event,\n\t\t\t\tstruct perf_sample *sample)\n{\n\tstruct thread *thread = machine__findnew_thread(machine,\n\t\t\t\t\t\t\tevent->comm.pid,\n\t\t\t\t\t\t\tevent->comm.tid);\n\tbool exec = event->header.misc & PERF_RECORD_MISC_COMM_EXEC;\n\tint err = 0;\n\n\tif (exec)\n\t\tmachine->comm_exec = true;\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_comm(event, stdout);\n\n\tif (thread == NULL ||\n\t    __thread__set_comm(thread, event->comm.comm, sample->time, exec)) {\n\t\tdump_printf(\"problem processing PERF_RECORD_COMM, skipping event.\\n\");\n\t\terr = -1;\n\t}\n\n\tthread__put(thread);\n\n\treturn err;\n}\n\nint machine__process_namespaces_event(struct machine *machine __maybe_unused,\n\t\t\t\t      union perf_event *event,\n\t\t\t\t      struct perf_sample *sample __maybe_unused)\n{\n\tstruct thread *thread = machine__findnew_thread(machine,\n\t\t\t\t\t\t\tevent->namespaces.pid,\n\t\t\t\t\t\t\tevent->namespaces.tid);\n\tint err = 0;\n\n\tWARN_ONCE(event->namespaces.nr_namespaces > NR_NAMESPACES,\n\t\t  \"\\nWARNING: kernel seems to support more namespaces than perf\"\n\t\t  \" tool.\\nTry updating the perf tool..\\n\\n\");\n\n\tWARN_ONCE(event->namespaces.nr_namespaces < NR_NAMESPACES,\n\t\t  \"\\nWARNING: perf tool seems to support more namespaces than\"\n\t\t  \" the kernel.\\nTry updating the kernel..\\n\\n\");\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_namespaces(event, stdout);\n\n\tif (thread == NULL ||\n\t    thread__set_namespaces(thread, sample->time, &event->namespaces)) {\n\t\tdump_printf(\"problem processing PERF_RECORD_NAMESPACES, skipping event.\\n\");\n\t\terr = -1;\n\t}\n\n\tthread__put(thread);\n\n\treturn err;\n}\n\nint machine__process_cgroup_event(struct machine *machine,\n\t\t\t\t  union perf_event *event,\n\t\t\t\t  struct perf_sample *sample __maybe_unused)\n{\n\tstruct cgroup *cgrp;\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_cgroup(event, stdout);\n\n\tcgrp = cgroup__findnew(machine->env, event->cgroup.id, event->cgroup.path);\n\tif (cgrp == NULL)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nint machine__process_lost_event(struct machine *machine __maybe_unused,\n\t\t\t\tunion perf_event *event, struct perf_sample *sample __maybe_unused)\n{\n\tdump_printf(\": id:%\" PRI_lu64 \": lost:%\" PRI_lu64 \"\\n\",\n\t\t    event->lost.id, event->lost.lost);\n\treturn 0;\n}\n\nint machine__process_lost_samples_event(struct machine *machine __maybe_unused,\n\t\t\t\t\tunion perf_event *event, struct perf_sample *sample)\n{\n\tdump_printf(\": id:%\" PRIu64 \": lost samples :%\" PRI_lu64 \"\\n\",\n\t\t    sample->id, event->lost_samples.lost);\n\treturn 0;\n}\n\nstatic struct dso *machine__findnew_module_dso(struct machine *machine,\n\t\t\t\t\t       struct kmod_path *m,\n\t\t\t\t\t       const char *filename)\n{\n\tstruct dso *dso;\n\n\tdown_write(&machine->dsos.lock);\n\n\tdso = __dsos__find(&machine->dsos, m->name, true);\n\tif (!dso) {\n\t\tdso = __dsos__addnew(&machine->dsos, m->name);\n\t\tif (dso == NULL)\n\t\t\tgoto out_unlock;\n\n\t\tdso__set_module_info(dso, m, machine);\n\t\tdso__set_long_name(dso, strdup(filename), true);\n\t\tdso->kernel = DSO_SPACE__KERNEL;\n\t}\n\n\tdso__get(dso);\nout_unlock:\n\tup_write(&machine->dsos.lock);\n\treturn dso;\n}\n\nint machine__process_aux_event(struct machine *machine __maybe_unused,\n\t\t\t       union perf_event *event)\n{\n\tif (dump_trace)\n\t\tperf_event__fprintf_aux(event, stdout);\n\treturn 0;\n}\n\nint machine__process_itrace_start_event(struct machine *machine __maybe_unused,\n\t\t\t\t\tunion perf_event *event)\n{\n\tif (dump_trace)\n\t\tperf_event__fprintf_itrace_start(event, stdout);\n\treturn 0;\n}\n\nint machine__process_aux_output_hw_id_event(struct machine *machine __maybe_unused,\n\t\t\t\t\t    union perf_event *event)\n{\n\tif (dump_trace)\n\t\tperf_event__fprintf_aux_output_hw_id(event, stdout);\n\treturn 0;\n}\n\nint machine__process_switch_event(struct machine *machine __maybe_unused,\n\t\t\t\t  union perf_event *event)\n{\n\tif (dump_trace)\n\t\tperf_event__fprintf_switch(event, stdout);\n\treturn 0;\n}\n\nstatic int machine__process_ksymbol_register(struct machine *machine,\n\t\t\t\t\t     union perf_event *event,\n\t\t\t\t\t     struct perf_sample *sample __maybe_unused)\n{\n\tstruct symbol *sym;\n\tstruct dso *dso;\n\tstruct map *map = maps__find(machine__kernel_maps(machine), event->ksymbol.addr);\n\tbool put_map = false;\n\tint err = 0;\n\n\tif (!map) {\n\t\tdso = dso__new(event->ksymbol.name);\n\n\t\tif (!dso) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tdso->kernel = DSO_SPACE__KERNEL;\n\t\tmap = map__new2(0, dso);\n\t\tdso__put(dso);\n\t\tif (!map) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tput_map = true;\n\t\tif (event->ksymbol.ksym_type == PERF_RECORD_KSYMBOL_TYPE_OOL) {\n\t\t\tdso->binary_type = DSO_BINARY_TYPE__OOL;\n\t\t\tdso->data.file_size = event->ksymbol.len;\n\t\t\tdso__set_loaded(dso);\n\t\t}\n\n\t\tmap__set_start(map, event->ksymbol.addr);\n\t\tmap__set_end(map, map__start(map) + event->ksymbol.len);\n\t\terr = maps__insert(machine__kernel_maps(machine), map);\n\t\tif (err) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tdso__set_loaded(dso);\n\n\t\tif (is_bpf_image(event->ksymbol.name)) {\n\t\t\tdso->binary_type = DSO_BINARY_TYPE__BPF_IMAGE;\n\t\t\tdso__set_long_name(dso, \"\", false);\n\t\t}\n\t} else {\n\t\tdso = map__dso(map);\n\t}\n\n\tsym = symbol__new(map__map_ip(map, map__start(map)),\n\t\t\t  event->ksymbol.len,\n\t\t\t  0, 0, event->ksymbol.name);\n\tif (!sym) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tdso__insert_symbol(dso, sym);\nout:\n\tif (put_map)\n\t\tmap__put(map);\n\treturn err;\n}\n\nstatic int machine__process_ksymbol_unregister(struct machine *machine,\n\t\t\t\t\t       union perf_event *event,\n\t\t\t\t\t       struct perf_sample *sample __maybe_unused)\n{\n\tstruct symbol *sym;\n\tstruct map *map;\n\n\tmap = maps__find(machine__kernel_maps(machine), event->ksymbol.addr);\n\tif (!map)\n\t\treturn 0;\n\n\tif (RC_CHK_ACCESS(map) != RC_CHK_ACCESS(machine->vmlinux_map))\n\t\tmaps__remove(machine__kernel_maps(machine), map);\n\telse {\n\t\tstruct dso *dso = map__dso(map);\n\n\t\tsym = dso__find_symbol(dso, map__map_ip(map, map__start(map)));\n\t\tif (sym)\n\t\t\tdso__delete_symbol(dso, sym);\n\t}\n\n\treturn 0;\n}\n\nint machine__process_ksymbol(struct machine *machine __maybe_unused,\n\t\t\t     union perf_event *event,\n\t\t\t     struct perf_sample *sample)\n{\n\tif (dump_trace)\n\t\tperf_event__fprintf_ksymbol(event, stdout);\n\n\tif (event->ksymbol.flags & PERF_RECORD_KSYMBOL_FLAGS_UNREGISTER)\n\t\treturn machine__process_ksymbol_unregister(machine, event,\n\t\t\t\t\t\t\t   sample);\n\treturn machine__process_ksymbol_register(machine, event, sample);\n}\n\nint machine__process_text_poke(struct machine *machine, union perf_event *event,\n\t\t\t       struct perf_sample *sample __maybe_unused)\n{\n\tstruct map *map = maps__find(machine__kernel_maps(machine), event->text_poke.addr);\n\tu8 cpumode = event->header.misc & PERF_RECORD_MISC_CPUMODE_MASK;\n\tstruct dso *dso = map ? map__dso(map) : NULL;\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_text_poke(event, machine, stdout);\n\n\tif (!event->text_poke.new_len)\n\t\treturn 0;\n\n\tif (cpumode != PERF_RECORD_MISC_KERNEL) {\n\t\tpr_debug(\"%s: unsupported cpumode - ignoring\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tif (dso) {\n\t\tu8 *new_bytes = event->text_poke.bytes + event->text_poke.old_len;\n\t\tint ret;\n\n\t\t \n\t\tmap__load(map);\n\t\tret = dso__data_write_cache_addr(dso, map, machine,\n\t\t\t\t\t\t event->text_poke.addr,\n\t\t\t\t\t\t new_bytes,\n\t\t\t\t\t\t event->text_poke.new_len);\n\t\tif (ret != event->text_poke.new_len)\n\t\t\tpr_debug(\"Failed to write kernel text poke at %#\" PRI_lx64 \"\\n\",\n\t\t\t\t event->text_poke.addr);\n\t} else {\n\t\tpr_debug(\"Failed to find kernel text poke address map for %#\" PRI_lx64 \"\\n\",\n\t\t\t event->text_poke.addr);\n\t}\n\n\treturn 0;\n}\n\nstatic struct map *machine__addnew_module_map(struct machine *machine, u64 start,\n\t\t\t\t\t      const char *filename)\n{\n\tstruct map *map = NULL;\n\tstruct kmod_path m;\n\tstruct dso *dso;\n\tint err;\n\n\tif (kmod_path__parse_name(&m, filename))\n\t\treturn NULL;\n\n\tdso = machine__findnew_module_dso(machine, &m, filename);\n\tif (dso == NULL)\n\t\tgoto out;\n\n\tmap = map__new2(start, dso);\n\tif (map == NULL)\n\t\tgoto out;\n\n\terr = maps__insert(machine__kernel_maps(machine), map);\n\t \n\tif (err) {\n\t\tmap__put(map);\n\t\tmap = NULL;\n\t}\nout:\n\t \n\tdso__put(dso);\n\tzfree(&m.name);\n\treturn map;\n}\n\nsize_t machines__fprintf_dsos(struct machines *machines, FILE *fp)\n{\n\tstruct rb_node *nd;\n\tsize_t ret = __dsos__fprintf(&machines->host.dsos.head, fp);\n\n\tfor (nd = rb_first_cached(&machines->guests); nd; nd = rb_next(nd)) {\n\t\tstruct machine *pos = rb_entry(nd, struct machine, rb_node);\n\t\tret += __dsos__fprintf(&pos->dsos.head, fp);\n\t}\n\n\treturn ret;\n}\n\nsize_t machine__fprintf_dsos_buildid(struct machine *m, FILE *fp,\n\t\t\t\t     bool (skip)(struct dso *dso, int parm), int parm)\n{\n\treturn __dsos__fprintf_buildid(&m->dsos.head, fp, skip, parm);\n}\n\nsize_t machines__fprintf_dsos_buildid(struct machines *machines, FILE *fp,\n\t\t\t\t     bool (skip)(struct dso *dso, int parm), int parm)\n{\n\tstruct rb_node *nd;\n\tsize_t ret = machine__fprintf_dsos_buildid(&machines->host, fp, skip, parm);\n\n\tfor (nd = rb_first_cached(&machines->guests); nd; nd = rb_next(nd)) {\n\t\tstruct machine *pos = rb_entry(nd, struct machine, rb_node);\n\t\tret += machine__fprintf_dsos_buildid(pos, fp, skip, parm);\n\t}\n\treturn ret;\n}\n\nsize_t machine__fprintf_vmlinux_path(struct machine *machine, FILE *fp)\n{\n\tint i;\n\tsize_t printed = 0;\n\tstruct dso *kdso = machine__kernel_dso(machine);\n\n\tif (kdso->has_build_id) {\n\t\tchar filename[PATH_MAX];\n\t\tif (dso__build_id_filename(kdso, filename, sizeof(filename),\n\t\t\t\t\t   false))\n\t\t\tprinted += fprintf(fp, \"[0] %s\\n\", filename);\n\t}\n\n\tfor (i = 0; i < vmlinux_path__nr_entries; ++i)\n\t\tprinted += fprintf(fp, \"[%d] %s\\n\",\n\t\t\t\t   i + kdso->has_build_id, vmlinux_path[i]);\n\n\treturn printed;\n}\n\nsize_t machine__fprintf(struct machine *machine, FILE *fp)\n{\n\tstruct rb_node *nd;\n\tsize_t ret;\n\tint i;\n\n\tfor (i = 0; i < THREADS__TABLE_SIZE; i++) {\n\t\tstruct threads *threads = &machine->threads[i];\n\n\t\tdown_read(&threads->lock);\n\n\t\tret = fprintf(fp, \"Threads: %u\\n\", threads->nr);\n\n\t\tfor (nd = rb_first_cached(&threads->entries); nd;\n\t\t     nd = rb_next(nd)) {\n\t\t\tstruct thread *pos = rb_entry(nd, struct thread_rb_node, rb_node)->thread;\n\n\t\t\tret += thread__fprintf(pos, fp);\n\t\t}\n\n\t\tup_read(&threads->lock);\n\t}\n\treturn ret;\n}\n\nstatic struct dso *machine__get_kernel(struct machine *machine)\n{\n\tconst char *vmlinux_name = machine->mmap_name;\n\tstruct dso *kernel;\n\n\tif (machine__is_host(machine)) {\n\t\tif (symbol_conf.vmlinux_name)\n\t\t\tvmlinux_name = symbol_conf.vmlinux_name;\n\n\t\tkernel = machine__findnew_kernel(machine, vmlinux_name,\n\t\t\t\t\t\t \"[kernel]\", DSO_SPACE__KERNEL);\n\t} else {\n\t\tif (symbol_conf.default_guest_vmlinux_name)\n\t\t\tvmlinux_name = symbol_conf.default_guest_vmlinux_name;\n\n\t\tkernel = machine__findnew_kernel(machine, vmlinux_name,\n\t\t\t\t\t\t \"[guest.kernel]\",\n\t\t\t\t\t\t DSO_SPACE__KERNEL_GUEST);\n\t}\n\n\tif (kernel != NULL && (!kernel->has_build_id))\n\t\tdso__read_running_kernel_build_id(kernel, machine);\n\n\treturn kernel;\n}\n\nvoid machine__get_kallsyms_filename(struct machine *machine, char *buf,\n\t\t\t\t    size_t bufsz)\n{\n\tif (machine__is_default_guest(machine))\n\t\tscnprintf(buf, bufsz, \"%s\", symbol_conf.default_guest_kallsyms);\n\telse\n\t\tscnprintf(buf, bufsz, \"%s/proc/kallsyms\", machine->root_dir);\n}\n\nconst char *ref_reloc_sym_names[] = {\"_text\", \"_stext\", NULL};\n\n \nstatic int machine__get_running_kernel_start(struct machine *machine,\n\t\t\t\t\t     const char **symbol_name,\n\t\t\t\t\t     u64 *start, u64 *end)\n{\n\tchar filename[PATH_MAX];\n\tint i, err = -1;\n\tconst char *name;\n\tu64 addr = 0;\n\n\tmachine__get_kallsyms_filename(machine, filename, PATH_MAX);\n\n\tif (symbol__restricted_filename(filename, \"/proc/kallsyms\"))\n\t\treturn 0;\n\n\tfor (i = 0; (name = ref_reloc_sym_names[i]) != NULL; i++) {\n\t\terr = kallsyms__get_function_start(filename, name, &addr);\n\t\tif (!err)\n\t\t\tbreak;\n\t}\n\n\tif (err)\n\t\treturn -1;\n\n\tif (symbol_name)\n\t\t*symbol_name = name;\n\n\t*start = addr;\n\n\terr = kallsyms__get_symbol_start(filename, \"_edata\", &addr);\n\tif (err)\n\t\terr = kallsyms__get_function_start(filename, \"_etext\", &addr);\n\tif (!err)\n\t\t*end = addr;\n\n\treturn 0;\n}\n\nint machine__create_extra_kernel_map(struct machine *machine,\n\t\t\t\t     struct dso *kernel,\n\t\t\t\t     struct extra_kernel_map *xm)\n{\n\tstruct kmap *kmap;\n\tstruct map *map;\n\tint err;\n\n\tmap = map__new2(xm->start, kernel);\n\tif (!map)\n\t\treturn -ENOMEM;\n\n\tmap__set_end(map, xm->end);\n\tmap__set_pgoff(map, xm->pgoff);\n\n\tkmap = map__kmap(map);\n\n\tstrlcpy(kmap->name, xm->name, KMAP_NAME_LEN);\n\n\terr = maps__insert(machine__kernel_maps(machine), map);\n\n\tif (!err) {\n\t\tpr_debug2(\"Added extra kernel map %s %\" PRIx64 \"-%\" PRIx64 \"\\n\",\n\t\t\tkmap->name, map__start(map), map__end(map));\n\t}\n\n\tmap__put(map);\n\n\treturn err;\n}\n\nstatic u64 find_entry_trampoline(struct dso *dso)\n{\n\t \n\tconst char *syms[] = {\n\t\t\"_entry_trampoline\",\n\t\t\"__entry_trampoline_start\",\n\t\t\"entry_SYSCALL_64_trampoline\",\n\t};\n\tstruct symbol *sym = dso__first_symbol(dso);\n\tunsigned int i;\n\n\tfor (; sym; sym = dso__next_symbol(sym)) {\n\t\tif (sym->binding != STB_GLOBAL)\n\t\t\tcontinue;\n\t\tfor (i = 0; i < ARRAY_SIZE(syms); i++) {\n\t\t\tif (!strcmp(sym->name, syms[i]))\n\t\t\t\treturn sym->start;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \n#define X86_64_CPU_ENTRY_AREA_PER_CPU\t0xfffffe0000000000ULL\n#define X86_64_CPU_ENTRY_AREA_SIZE\t0x2c000\n#define X86_64_ENTRY_TRAMPOLINE\t\t0x6000\n\n \nint machine__map_x86_64_entry_trampolines(struct machine *machine,\n\t\t\t\t\t  struct dso *kernel)\n{\n\tstruct maps *kmaps = machine__kernel_maps(machine);\n\tint nr_cpus_avail, cpu;\n\tbool found = false;\n\tstruct map_rb_node *rb_node;\n\tu64 pgoff;\n\n\t \n\tmaps__for_each_entry(kmaps, rb_node) {\n\t\tstruct map *dest_map, *map = rb_node->map;\n\t\tstruct kmap *kmap = __map__kmap(map);\n\n\t\tif (!kmap || !is_entry_trampoline(kmap->name))\n\t\t\tcontinue;\n\n\t\tdest_map = maps__find(kmaps, map__pgoff(map));\n\t\tif (dest_map != map)\n\t\t\tmap__set_pgoff(map, map__map_ip(dest_map, map__pgoff(map)));\n\t\tfound = true;\n\t}\n\tif (found || machine->trampolines_mapped)\n\t\treturn 0;\n\n\tpgoff = find_entry_trampoline(kernel);\n\tif (!pgoff)\n\t\treturn 0;\n\n\tnr_cpus_avail = machine__nr_cpus_avail(machine);\n\n\t \n\tfor (cpu = 0; cpu < nr_cpus_avail; cpu++) {\n\t\tu64 va = X86_64_CPU_ENTRY_AREA_PER_CPU +\n\t\t\t cpu * X86_64_CPU_ENTRY_AREA_SIZE +\n\t\t\t X86_64_ENTRY_TRAMPOLINE;\n\t\tstruct extra_kernel_map xm = {\n\t\t\t.start = va,\n\t\t\t.end   = va + page_size,\n\t\t\t.pgoff = pgoff,\n\t\t};\n\n\t\tstrlcpy(xm.name, ENTRY_TRAMPOLINE_NAME, KMAP_NAME_LEN);\n\n\t\tif (machine__create_extra_kernel_map(machine, kernel, &xm) < 0)\n\t\t\treturn -1;\n\t}\n\n\tmachine->trampolines_mapped = nr_cpus_avail;\n\n\treturn 0;\n}\n\nint __weak machine__create_extra_kernel_maps(struct machine *machine __maybe_unused,\n\t\t\t\t\t     struct dso *kernel __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic int\n__machine__create_kernel_maps(struct machine *machine, struct dso *kernel)\n{\n\t \n\tmachine__destroy_kernel_maps(machine);\n\n\tmap__put(machine->vmlinux_map);\n\tmachine->vmlinux_map = map__new2(0, kernel);\n\tif (machine->vmlinux_map == NULL)\n\t\treturn -ENOMEM;\n\n\tmap__set_map_ip(machine->vmlinux_map, identity__map_ip);\n\tmap__set_unmap_ip(machine->vmlinux_map, identity__map_ip);\n\treturn maps__insert(machine__kernel_maps(machine), machine->vmlinux_map);\n}\n\nvoid machine__destroy_kernel_maps(struct machine *machine)\n{\n\tstruct kmap *kmap;\n\tstruct map *map = machine__kernel_map(machine);\n\n\tif (map == NULL)\n\t\treturn;\n\n\tkmap = map__kmap(map);\n\tmaps__remove(machine__kernel_maps(machine), map);\n\tif (kmap && kmap->ref_reloc_sym) {\n\t\tzfree((char **)&kmap->ref_reloc_sym->name);\n\t\tzfree(&kmap->ref_reloc_sym);\n\t}\n\n\tmap__zput(machine->vmlinux_map);\n}\n\nint machines__create_guest_kernel_maps(struct machines *machines)\n{\n\tint ret = 0;\n\tstruct dirent **namelist = NULL;\n\tint i, items = 0;\n\tchar path[PATH_MAX];\n\tpid_t pid;\n\tchar *endp;\n\n\tif (symbol_conf.default_guest_vmlinux_name ||\n\t    symbol_conf.default_guest_modules ||\n\t    symbol_conf.default_guest_kallsyms) {\n\t\tmachines__create_kernel_maps(machines, DEFAULT_GUEST_KERNEL_ID);\n\t}\n\n\tif (symbol_conf.guestmount) {\n\t\titems = scandir(symbol_conf.guestmount, &namelist, NULL, NULL);\n\t\tif (items <= 0)\n\t\t\treturn -ENOENT;\n\t\tfor (i = 0; i < items; i++) {\n\t\t\tif (!isdigit(namelist[i]->d_name[0])) {\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tpid = (pid_t)strtol(namelist[i]->d_name, &endp, 10);\n\t\t\tif ((*endp != '\\0') ||\n\t\t\t    (endp == namelist[i]->d_name) ||\n\t\t\t    (errno == ERANGE)) {\n\t\t\t\tpr_debug(\"invalid directory (%s). Skipping.\\n\",\n\t\t\t\t\t namelist[i]->d_name);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tsprintf(path, \"%s/%s/proc/kallsyms\",\n\t\t\t\tsymbol_conf.guestmount,\n\t\t\t\tnamelist[i]->d_name);\n\t\t\tret = access(path, R_OK);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Can't access file %s\\n\", path);\n\t\t\t\tgoto failure;\n\t\t\t}\n\t\t\tmachines__create_kernel_maps(machines, pid);\n\t\t}\nfailure:\n\t\tfree(namelist);\n\t}\n\n\treturn ret;\n}\n\nvoid machines__destroy_kernel_maps(struct machines *machines)\n{\n\tstruct rb_node *next = rb_first_cached(&machines->guests);\n\n\tmachine__destroy_kernel_maps(&machines->host);\n\n\twhile (next) {\n\t\tstruct machine *pos = rb_entry(next, struct machine, rb_node);\n\n\t\tnext = rb_next(&pos->rb_node);\n\t\trb_erase_cached(&pos->rb_node, &machines->guests);\n\t\tmachine__delete(pos);\n\t}\n}\n\nint machines__create_kernel_maps(struct machines *machines, pid_t pid)\n{\n\tstruct machine *machine = machines__findnew(machines, pid);\n\n\tif (machine == NULL)\n\t\treturn -1;\n\n\treturn machine__create_kernel_maps(machine);\n}\n\nint machine__load_kallsyms(struct machine *machine, const char *filename)\n{\n\tstruct map *map = machine__kernel_map(machine);\n\tstruct dso *dso = map__dso(map);\n\tint ret = __dso__load_kallsyms(dso, filename, map, true);\n\n\tif (ret > 0) {\n\t\tdso__set_loaded(dso);\n\t\t \n\t\tmaps__fixup_end(machine__kernel_maps(machine));\n\t}\n\n\treturn ret;\n}\n\nint machine__load_vmlinux_path(struct machine *machine)\n{\n\tstruct map *map = machine__kernel_map(machine);\n\tstruct dso *dso = map__dso(map);\n\tint ret = dso__load_vmlinux_path(dso, map);\n\n\tif (ret > 0)\n\t\tdso__set_loaded(dso);\n\n\treturn ret;\n}\n\nstatic char *get_kernel_version(const char *root_dir)\n{\n\tchar version[PATH_MAX];\n\tFILE *file;\n\tchar *name, *tmp;\n\tconst char *prefix = \"Linux version \";\n\n\tsprintf(version, \"%s/proc/version\", root_dir);\n\tfile = fopen(version, \"r\");\n\tif (!file)\n\t\treturn NULL;\n\n\ttmp = fgets(version, sizeof(version), file);\n\tfclose(file);\n\tif (!tmp)\n\t\treturn NULL;\n\n\tname = strstr(version, prefix);\n\tif (!name)\n\t\treturn NULL;\n\tname += strlen(prefix);\n\ttmp = strchr(name, ' ');\n\tif (tmp)\n\t\t*tmp = '\\0';\n\n\treturn strdup(name);\n}\n\nstatic bool is_kmod_dso(struct dso *dso)\n{\n\treturn dso->symtab_type == DSO_BINARY_TYPE__SYSTEM_PATH_KMODULE ||\n\t       dso->symtab_type == DSO_BINARY_TYPE__GUEST_KMODULE;\n}\n\nstatic int maps__set_module_path(struct maps *maps, const char *path, struct kmod_path *m)\n{\n\tchar *long_name;\n\tstruct dso *dso;\n\tstruct map *map = maps__find_by_name(maps, m->name);\n\n\tif (map == NULL)\n\t\treturn 0;\n\n\tlong_name = strdup(path);\n\tif (long_name == NULL)\n\t\treturn -ENOMEM;\n\n\tdso = map__dso(map);\n\tdso__set_long_name(dso, long_name, true);\n\tdso__kernel_module_get_build_id(dso, \"\");\n\n\t \n\tif (m->comp && is_kmod_dso(dso)) {\n\t\tdso->symtab_type++;\n\t\tdso->comp = m->comp;\n\t}\n\n\treturn 0;\n}\n\nstatic int maps__set_modules_path_dir(struct maps *maps, const char *dir_name, int depth)\n{\n\tstruct dirent *dent;\n\tDIR *dir = opendir(dir_name);\n\tint ret = 0;\n\n\tif (!dir) {\n\t\tpr_debug(\"%s: cannot open %s dir\\n\", __func__, dir_name);\n\t\treturn -1;\n\t}\n\n\twhile ((dent = readdir(dir)) != NULL) {\n\t\tchar path[PATH_MAX];\n\t\tstruct stat st;\n\n\t\t \n\t\tpath__join(path, sizeof(path), dir_name, dent->d_name);\n\t\tif (stat(path, &st))\n\t\t\tcontinue;\n\n\t\tif (S_ISDIR(st.st_mode)) {\n\t\t\tif (!strcmp(dent->d_name, \".\") ||\n\t\t\t    !strcmp(dent->d_name, \"..\"))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (depth == 0) {\n\t\t\t\tif (!strcmp(dent->d_name, \"source\") ||\n\t\t\t\t    !strcmp(dent->d_name, \"build\"))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tret = maps__set_modules_path_dir(maps, path, depth + 1);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tstruct kmod_path m;\n\n\t\t\tret = kmod_path__parse_name(&m, dent->d_name);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\n\t\t\tif (m.kmod)\n\t\t\t\tret = maps__set_module_path(maps, path, &m);\n\n\t\t\tzfree(&m.name);\n\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tclosedir(dir);\n\treturn ret;\n}\n\nstatic int machine__set_modules_path(struct machine *machine)\n{\n\tchar *version;\n\tchar modules_path[PATH_MAX];\n\n\tversion = get_kernel_version(machine->root_dir);\n\tif (!version)\n\t\treturn -1;\n\n\tsnprintf(modules_path, sizeof(modules_path), \"%s/lib/modules/%s\",\n\t\t machine->root_dir, version);\n\tfree(version);\n\n\treturn maps__set_modules_path_dir(machine__kernel_maps(machine), modules_path, 0);\n}\nint __weak arch__fix_module_text_start(u64 *start __maybe_unused,\n\t\t\t\tu64 *size __maybe_unused,\n\t\t\t\tconst char *name __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic int machine__create_module(void *arg, const char *name, u64 start,\n\t\t\t\t  u64 size)\n{\n\tstruct machine *machine = arg;\n\tstruct map *map;\n\n\tif (arch__fix_module_text_start(&start, &size, name) < 0)\n\t\treturn -1;\n\n\tmap = machine__addnew_module_map(machine, start, name);\n\tif (map == NULL)\n\t\treturn -1;\n\tmap__set_end(map, start + size);\n\n\tdso__kernel_module_get_build_id(map__dso(map), machine->root_dir);\n\tmap__put(map);\n\treturn 0;\n}\n\nstatic int machine__create_modules(struct machine *machine)\n{\n\tconst char *modules;\n\tchar path[PATH_MAX];\n\n\tif (machine__is_default_guest(machine)) {\n\t\tmodules = symbol_conf.default_guest_modules;\n\t} else {\n\t\tsnprintf(path, PATH_MAX, \"%s/proc/modules\", machine->root_dir);\n\t\tmodules = path;\n\t}\n\n\tif (symbol__restricted_filename(modules, \"/proc/modules\"))\n\t\treturn -1;\n\n\tif (modules__parse(modules, machine, machine__create_module))\n\t\treturn -1;\n\n\tif (!machine__set_modules_path(machine))\n\t\treturn 0;\n\n\tpr_debug(\"Problems setting modules path maps, continuing anyway...\\n\");\n\n\treturn 0;\n}\n\nstatic void machine__set_kernel_mmap(struct machine *machine,\n\t\t\t\t     u64 start, u64 end)\n{\n\tmap__set_start(machine->vmlinux_map, start);\n\tmap__set_end(machine->vmlinux_map, end);\n\t \n\tif (start == 0 && end == 0)\n\t\tmap__set_end(machine->vmlinux_map, ~0ULL);\n}\n\nstatic int machine__update_kernel_mmap(struct machine *machine,\n\t\t\t\t     u64 start, u64 end)\n{\n\tstruct map *orig, *updated;\n\tint err;\n\n\torig = machine->vmlinux_map;\n\tupdated = map__get(orig);\n\n\tmachine->vmlinux_map = updated;\n\tmachine__set_kernel_mmap(machine, start, end);\n\tmaps__remove(machine__kernel_maps(machine), orig);\n\terr = maps__insert(machine__kernel_maps(machine), updated);\n\tmap__put(orig);\n\n\treturn err;\n}\n\nint machine__create_kernel_maps(struct machine *machine)\n{\n\tstruct dso *kernel = machine__get_kernel(machine);\n\tconst char *name = NULL;\n\tu64 start = 0, end = ~0ULL;\n\tint ret;\n\n\tif (kernel == NULL)\n\t\treturn -1;\n\n\tret = __machine__create_kernel_maps(machine, kernel);\n\tif (ret < 0)\n\t\tgoto out_put;\n\n\tif (symbol_conf.use_modules && machine__create_modules(machine) < 0) {\n\t\tif (machine__is_host(machine))\n\t\t\tpr_debug(\"Problems creating module maps, \"\n\t\t\t\t \"continuing anyway...\\n\");\n\t\telse\n\t\t\tpr_debug(\"Problems creating module maps for guest %d, \"\n\t\t\t\t \"continuing anyway...\\n\", machine->pid);\n\t}\n\n\tif (!machine__get_running_kernel_start(machine, &name, &start, &end)) {\n\t\tif (name &&\n\t\t    map__set_kallsyms_ref_reloc_sym(machine->vmlinux_map, name, start)) {\n\t\t\tmachine__destroy_kernel_maps(machine);\n\t\t\tret = -1;\n\t\t\tgoto out_put;\n\t\t}\n\n\t\t \n\t\tret = machine__update_kernel_mmap(machine, start, end);\n\t\tif (ret < 0)\n\t\t\tgoto out_put;\n\t}\n\n\tif (machine__create_extra_kernel_maps(machine, kernel))\n\t\tpr_debug(\"Problems creating extra kernel maps, continuing anyway...\\n\");\n\n\tif (end == ~0ULL) {\n\t\t \n\t\tstruct map_rb_node *rb_node = maps__find_node(machine__kernel_maps(machine),\n\t\t\t\t\t\t\tmachine__kernel_map(machine));\n\t\tstruct map_rb_node *next = map_rb_node__next(rb_node);\n\n\t\tif (next)\n\t\t\tmachine__set_kernel_mmap(machine, start, map__start(next->map));\n\t}\n\nout_put:\n\tdso__put(kernel);\n\treturn ret;\n}\n\nstatic bool machine__uses_kcore(struct machine *machine)\n{\n\tstruct dso *dso;\n\n\tlist_for_each_entry(dso, &machine->dsos.head, node) {\n\t\tif (dso__is_kcore(dso))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool perf_event__is_extra_kernel_mmap(struct machine *machine,\n\t\t\t\t\t     struct extra_kernel_map *xm)\n{\n\treturn machine__is(machine, \"x86_64\") &&\n\t       is_entry_trampoline(xm->name);\n}\n\nstatic int machine__process_extra_kernel_map(struct machine *machine,\n\t\t\t\t\t     struct extra_kernel_map *xm)\n{\n\tstruct dso *kernel = machine__kernel_dso(machine);\n\n\tif (kernel == NULL)\n\t\treturn -1;\n\n\treturn machine__create_extra_kernel_map(machine, kernel, xm);\n}\n\nstatic int machine__process_kernel_mmap_event(struct machine *machine,\n\t\t\t\t\t      struct extra_kernel_map *xm,\n\t\t\t\t\t      struct build_id *bid)\n{\n\tenum dso_space_type dso_space;\n\tbool is_kernel_mmap;\n\tconst char *mmap_name = machine->mmap_name;\n\n\t \n\tif (machine__uses_kcore(machine))\n\t\treturn 0;\n\n\tif (machine__is_host(machine))\n\t\tdso_space = DSO_SPACE__KERNEL;\n\telse\n\t\tdso_space = DSO_SPACE__KERNEL_GUEST;\n\n\tis_kernel_mmap = memcmp(xm->name, mmap_name, strlen(mmap_name) - 1) == 0;\n\tif (!is_kernel_mmap && !machine__is_host(machine)) {\n\t\t \n\t\tmmap_name = \"[kernel.kallsyms]\";\n\t\tis_kernel_mmap = memcmp(xm->name, mmap_name, strlen(mmap_name) - 1) == 0;\n\t}\n\tif (xm->name[0] == '/' ||\n\t    (!is_kernel_mmap && xm->name[0] == '[')) {\n\t\tstruct map *map = machine__addnew_module_map(machine, xm->start, xm->name);\n\n\t\tif (map == NULL)\n\t\t\tgoto out_problem;\n\n\t\tmap__set_end(map, map__start(map) + xm->end - xm->start);\n\n\t\tif (build_id__is_defined(bid))\n\t\t\tdso__set_build_id(map__dso(map), bid);\n\n\t\tmap__put(map);\n\t} else if (is_kernel_mmap) {\n\t\tconst char *symbol_name = xm->name + strlen(mmap_name);\n\t\t \n\t\tstruct dso *kernel = NULL;\n\t\tstruct dso *dso;\n\n\t\tdown_read(&machine->dsos.lock);\n\n\t\tlist_for_each_entry(dso, &machine->dsos.head, node) {\n\n\t\t\t \n\n\t\t\tif (!dso->kernel ||\n\t\t\t    is_kernel_module(dso->long_name,\n\t\t\t\t\t     PERF_RECORD_MISC_CPUMODE_UNKNOWN))\n\t\t\t\tcontinue;\n\n\n\t\t\tkernel = dso__get(dso);\n\t\t\tbreak;\n\t\t}\n\n\t\tup_read(&machine->dsos.lock);\n\n\t\tif (kernel == NULL)\n\t\t\tkernel = machine__findnew_dso(machine, machine->mmap_name);\n\t\tif (kernel == NULL)\n\t\t\tgoto out_problem;\n\n\t\tkernel->kernel = dso_space;\n\t\tif (__machine__create_kernel_maps(machine, kernel) < 0) {\n\t\t\tdso__put(kernel);\n\t\t\tgoto out_problem;\n\t\t}\n\n\t\tif (strstr(kernel->long_name, \"vmlinux\"))\n\t\t\tdso__set_short_name(kernel, \"[kernel.vmlinux]\", false);\n\n\t\tif (machine__update_kernel_mmap(machine, xm->start, xm->end) < 0) {\n\t\t\tdso__put(kernel);\n\t\t\tgoto out_problem;\n\t\t}\n\n\t\tif (build_id__is_defined(bid))\n\t\t\tdso__set_build_id(kernel, bid);\n\n\t\t \n\t\tif (xm->pgoff != 0) {\n\t\t\tmap__set_kallsyms_ref_reloc_sym(machine->vmlinux_map,\n\t\t\t\t\t\t\tsymbol_name,\n\t\t\t\t\t\t\txm->pgoff);\n\t\t}\n\n\t\tif (machine__is_default_guest(machine)) {\n\t\t\t \n\t\t\tdso__load(kernel, machine__kernel_map(machine));\n\t\t}\n\t\tdso__put(kernel);\n\t} else if (perf_event__is_extra_kernel_mmap(machine, xm)) {\n\t\treturn machine__process_extra_kernel_map(machine, xm);\n\t}\n\treturn 0;\nout_problem:\n\treturn -1;\n}\n\nint machine__process_mmap2_event(struct machine *machine,\n\t\t\t\t union perf_event *event,\n\t\t\t\t struct perf_sample *sample)\n{\n\tstruct thread *thread;\n\tstruct map *map;\n\tstruct dso_id dso_id = {\n\t\t.maj = event->mmap2.maj,\n\t\t.min = event->mmap2.min,\n\t\t.ino = event->mmap2.ino,\n\t\t.ino_generation = event->mmap2.ino_generation,\n\t};\n\tstruct build_id __bid, *bid = NULL;\n\tint ret = 0;\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_mmap2(event, stdout);\n\n\tif (event->header.misc & PERF_RECORD_MISC_MMAP_BUILD_ID) {\n\t\tbid = &__bid;\n\t\tbuild_id__init(bid, event->mmap2.build_id, event->mmap2.build_id_size);\n\t}\n\n\tif (sample->cpumode == PERF_RECORD_MISC_GUEST_KERNEL ||\n\t    sample->cpumode == PERF_RECORD_MISC_KERNEL) {\n\t\tstruct extra_kernel_map xm = {\n\t\t\t.start = event->mmap2.start,\n\t\t\t.end   = event->mmap2.start + event->mmap2.len,\n\t\t\t.pgoff = event->mmap2.pgoff,\n\t\t};\n\n\t\tstrlcpy(xm.name, event->mmap2.filename, KMAP_NAME_LEN);\n\t\tret = machine__process_kernel_mmap_event(machine, &xm, bid);\n\t\tif (ret < 0)\n\t\t\tgoto out_problem;\n\t\treturn 0;\n\t}\n\n\tthread = machine__findnew_thread(machine, event->mmap2.pid,\n\t\t\t\t\tevent->mmap2.tid);\n\tif (thread == NULL)\n\t\tgoto out_problem;\n\n\tmap = map__new(machine, event->mmap2.start,\n\t\t\tevent->mmap2.len, event->mmap2.pgoff,\n\t\t\t&dso_id, event->mmap2.prot,\n\t\t\tevent->mmap2.flags, bid,\n\t\t\tevent->mmap2.filename, thread);\n\n\tif (map == NULL)\n\t\tgoto out_problem_map;\n\n\tret = thread__insert_map(thread, map);\n\tif (ret)\n\t\tgoto out_problem_insert;\n\n\tthread__put(thread);\n\tmap__put(map);\n\treturn 0;\n\nout_problem_insert:\n\tmap__put(map);\nout_problem_map:\n\tthread__put(thread);\nout_problem:\n\tdump_printf(\"problem processing PERF_RECORD_MMAP2, skipping event.\\n\");\n\treturn 0;\n}\n\nint machine__process_mmap_event(struct machine *machine, union perf_event *event,\n\t\t\t\tstruct perf_sample *sample)\n{\n\tstruct thread *thread;\n\tstruct map *map;\n\tu32 prot = 0;\n\tint ret = 0;\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_mmap(event, stdout);\n\n\tif (sample->cpumode == PERF_RECORD_MISC_GUEST_KERNEL ||\n\t    sample->cpumode == PERF_RECORD_MISC_KERNEL) {\n\t\tstruct extra_kernel_map xm = {\n\t\t\t.start = event->mmap.start,\n\t\t\t.end   = event->mmap.start + event->mmap.len,\n\t\t\t.pgoff = event->mmap.pgoff,\n\t\t};\n\n\t\tstrlcpy(xm.name, event->mmap.filename, KMAP_NAME_LEN);\n\t\tret = machine__process_kernel_mmap_event(machine, &xm, NULL);\n\t\tif (ret < 0)\n\t\t\tgoto out_problem;\n\t\treturn 0;\n\t}\n\n\tthread = machine__findnew_thread(machine, event->mmap.pid,\n\t\t\t\t\t event->mmap.tid);\n\tif (thread == NULL)\n\t\tgoto out_problem;\n\n\tif (!(event->header.misc & PERF_RECORD_MISC_MMAP_DATA))\n\t\tprot = PROT_EXEC;\n\n\tmap = map__new(machine, event->mmap.start,\n\t\t\tevent->mmap.len, event->mmap.pgoff,\n\t\t\tNULL, prot, 0, NULL, event->mmap.filename, thread);\n\n\tif (map == NULL)\n\t\tgoto out_problem_map;\n\n\tret = thread__insert_map(thread, map);\n\tif (ret)\n\t\tgoto out_problem_insert;\n\n\tthread__put(thread);\n\tmap__put(map);\n\treturn 0;\n\nout_problem_insert:\n\tmap__put(map);\nout_problem_map:\n\tthread__put(thread);\nout_problem:\n\tdump_printf(\"problem processing PERF_RECORD_MMAP, skipping event.\\n\");\n\treturn 0;\n}\n\nstatic void __machine__remove_thread(struct machine *machine, struct thread_rb_node *nd,\n\t\t\t\t     struct thread *th, bool lock)\n{\n\tstruct threads *threads = machine__threads(machine, thread__tid(th));\n\n\tif (!nd)\n\t\tnd = thread_rb_node__find(th, &threads->entries.rb_root);\n\n\tif (threads->last_match && RC_CHK_ACCESS(threads->last_match) == RC_CHK_ACCESS(th))\n\t\tthreads__set_last_match(threads, NULL);\n\n\tif (lock)\n\t\tdown_write(&threads->lock);\n\n\tBUG_ON(refcount_read(thread__refcnt(th)) == 0);\n\n\tthread__put(nd->thread);\n\trb_erase_cached(&nd->rb_node, &threads->entries);\n\tRB_CLEAR_NODE(&nd->rb_node);\n\t--threads->nr;\n\n\tfree(nd);\n\n\tif (lock)\n\t\tup_write(&threads->lock);\n}\n\nvoid machine__remove_thread(struct machine *machine, struct thread *th)\n{\n\treturn __machine__remove_thread(machine, NULL, th, true);\n}\n\nint machine__process_fork_event(struct machine *machine, union perf_event *event,\n\t\t\t\tstruct perf_sample *sample)\n{\n\tstruct thread *thread = machine__find_thread(machine,\n\t\t\t\t\t\t     event->fork.pid,\n\t\t\t\t\t\t     event->fork.tid);\n\tstruct thread *parent = machine__findnew_thread(machine,\n\t\t\t\t\t\t\tevent->fork.ppid,\n\t\t\t\t\t\t\tevent->fork.ptid);\n\tbool do_maps_clone = true;\n\tint err = 0;\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_task(event, stdout);\n\n\t \n\tif (thread__pid(parent) != (pid_t)event->fork.ppid) {\n\t\tdump_printf(\"removing erroneous parent thread %d/%d\\n\",\n\t\t\t    thread__pid(parent), thread__tid(parent));\n\t\tmachine__remove_thread(machine, parent);\n\t\tthread__put(parent);\n\t\tparent = machine__findnew_thread(machine, event->fork.ppid,\n\t\t\t\t\t\t event->fork.ptid);\n\t}\n\n\t \n\tif (thread != NULL) {\n\t\tmachine__remove_thread(machine, thread);\n\t\tthread__put(thread);\n\t}\n\n\tthread = machine__findnew_thread(machine, event->fork.pid,\n\t\t\t\t\t event->fork.tid);\n\t \n\tif (event->fork.header.misc & PERF_RECORD_MISC_FORK_EXEC)\n\t\tdo_maps_clone = false;\n\n\tif (thread == NULL || parent == NULL ||\n\t    thread__fork(thread, parent, sample->time, do_maps_clone) < 0) {\n\t\tdump_printf(\"problem processing PERF_RECORD_FORK, skipping event.\\n\");\n\t\terr = -1;\n\t}\n\tthread__put(thread);\n\tthread__put(parent);\n\n\treturn err;\n}\n\nint machine__process_exit_event(struct machine *machine, union perf_event *event,\n\t\t\t\tstruct perf_sample *sample __maybe_unused)\n{\n\tstruct thread *thread = machine__find_thread(machine,\n\t\t\t\t\t\t     event->fork.pid,\n\t\t\t\t\t\t     event->fork.tid);\n\n\tif (dump_trace)\n\t\tperf_event__fprintf_task(event, stdout);\n\n\tif (thread != NULL)\n\t\tthread__put(thread);\n\n\treturn 0;\n}\n\nint machine__process_event(struct machine *machine, union perf_event *event,\n\t\t\t   struct perf_sample *sample)\n{\n\tint ret;\n\n\tswitch (event->header.type) {\n\tcase PERF_RECORD_COMM:\n\t\tret = machine__process_comm_event(machine, event, sample); break;\n\tcase PERF_RECORD_MMAP:\n\t\tret = machine__process_mmap_event(machine, event, sample); break;\n\tcase PERF_RECORD_NAMESPACES:\n\t\tret = machine__process_namespaces_event(machine, event, sample); break;\n\tcase PERF_RECORD_CGROUP:\n\t\tret = machine__process_cgroup_event(machine, event, sample); break;\n\tcase PERF_RECORD_MMAP2:\n\t\tret = machine__process_mmap2_event(machine, event, sample); break;\n\tcase PERF_RECORD_FORK:\n\t\tret = machine__process_fork_event(machine, event, sample); break;\n\tcase PERF_RECORD_EXIT:\n\t\tret = machine__process_exit_event(machine, event, sample); break;\n\tcase PERF_RECORD_LOST:\n\t\tret = machine__process_lost_event(machine, event, sample); break;\n\tcase PERF_RECORD_AUX:\n\t\tret = machine__process_aux_event(machine, event); break;\n\tcase PERF_RECORD_ITRACE_START:\n\t\tret = machine__process_itrace_start_event(machine, event); break;\n\tcase PERF_RECORD_LOST_SAMPLES:\n\t\tret = machine__process_lost_samples_event(machine, event, sample); break;\n\tcase PERF_RECORD_SWITCH:\n\tcase PERF_RECORD_SWITCH_CPU_WIDE:\n\t\tret = machine__process_switch_event(machine, event); break;\n\tcase PERF_RECORD_KSYMBOL:\n\t\tret = machine__process_ksymbol(machine, event, sample); break;\n\tcase PERF_RECORD_BPF_EVENT:\n\t\tret = machine__process_bpf(machine, event, sample); break;\n\tcase PERF_RECORD_TEXT_POKE:\n\t\tret = machine__process_text_poke(machine, event, sample); break;\n\tcase PERF_RECORD_AUX_OUTPUT_HW_ID:\n\t\tret = machine__process_aux_output_hw_id_event(machine, event); break;\n\tdefault:\n\t\tret = -1;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic bool symbol__match_regex(struct symbol *sym, regex_t *regex)\n{\n\tif (!regexec(regex, sym->name, 0, NULL, 0))\n\t\treturn true;\n\treturn false;\n}\n\nstatic void ip__resolve_ams(struct thread *thread,\n\t\t\t    struct addr_map_symbol *ams,\n\t\t\t    u64 ip)\n{\n\tstruct addr_location al;\n\n\taddr_location__init(&al);\n\t \n\tthread__find_cpumode_addr_location(thread, ip, &al);\n\n\tams->addr = ip;\n\tams->al_addr = al.addr;\n\tams->al_level = al.level;\n\tams->ms.maps = maps__get(al.maps);\n\tams->ms.sym = al.sym;\n\tams->ms.map = map__get(al.map);\n\tams->phys_addr = 0;\n\tams->data_page_size = 0;\n\taddr_location__exit(&al);\n}\n\nstatic void ip__resolve_data(struct thread *thread,\n\t\t\t     u8 m, struct addr_map_symbol *ams,\n\t\t\t     u64 addr, u64 phys_addr, u64 daddr_page_size)\n{\n\tstruct addr_location al;\n\n\taddr_location__init(&al);\n\n\tthread__find_symbol(thread, m, addr, &al);\n\n\tams->addr = addr;\n\tams->al_addr = al.addr;\n\tams->al_level = al.level;\n\tams->ms.maps = maps__get(al.maps);\n\tams->ms.sym = al.sym;\n\tams->ms.map = map__get(al.map);\n\tams->phys_addr = phys_addr;\n\tams->data_page_size = daddr_page_size;\n\taddr_location__exit(&al);\n}\n\nstruct mem_info *sample__resolve_mem(struct perf_sample *sample,\n\t\t\t\t     struct addr_location *al)\n{\n\tstruct mem_info *mi = mem_info__new();\n\n\tif (!mi)\n\t\treturn NULL;\n\n\tip__resolve_ams(al->thread, &mi->iaddr, sample->ip);\n\tip__resolve_data(al->thread, al->cpumode, &mi->daddr,\n\t\t\t sample->addr, sample->phys_addr,\n\t\t\t sample->data_page_size);\n\tmi->data_src.val = sample->data_src;\n\n\treturn mi;\n}\n\nstatic char *callchain_srcline(struct map_symbol *ms, u64 ip)\n{\n\tstruct map *map = ms->map;\n\tchar *srcline = NULL;\n\tstruct dso *dso;\n\n\tif (!map || callchain_param.key == CCKEY_FUNCTION)\n\t\treturn srcline;\n\n\tdso = map__dso(map);\n\tsrcline = srcline__tree_find(&dso->srclines, ip);\n\tif (!srcline) {\n\t\tbool show_sym = false;\n\t\tbool show_addr = callchain_param.key == CCKEY_ADDRESS;\n\n\t\tsrcline = get_srcline(dso, map__rip_2objdump(map, ip),\n\t\t\t\t      ms->sym, show_sym, show_addr, ip);\n\t\tsrcline__tree_insert(&dso->srclines, ip, srcline);\n\t}\n\n\treturn srcline;\n}\n\nstruct iterations {\n\tint nr_loop_iter;\n\tu64 cycles;\n};\n\nstatic int add_callchain_ip(struct thread *thread,\n\t\t\t    struct callchain_cursor *cursor,\n\t\t\t    struct symbol **parent,\n\t\t\t    struct addr_location *root_al,\n\t\t\t    u8 *cpumode,\n\t\t\t    u64 ip,\n\t\t\t    bool branch,\n\t\t\t    struct branch_flags *flags,\n\t\t\t    struct iterations *iter,\n\t\t\t    u64 branch_from)\n{\n\tstruct map_symbol ms = {};\n\tstruct addr_location al;\n\tint nr_loop_iter = 0, err = 0;\n\tu64 iter_cycles = 0;\n\tconst char *srcline = NULL;\n\n\taddr_location__init(&al);\n\tal.filtered = 0;\n\tal.sym = NULL;\n\tal.srcline = NULL;\n\tif (!cpumode) {\n\t\tthread__find_cpumode_addr_location(thread, ip, &al);\n\t} else {\n\t\tif (ip >= PERF_CONTEXT_MAX) {\n\t\t\tswitch (ip) {\n\t\t\tcase PERF_CONTEXT_HV:\n\t\t\t\t*cpumode = PERF_RECORD_MISC_HYPERVISOR;\n\t\t\t\tbreak;\n\t\t\tcase PERF_CONTEXT_KERNEL:\n\t\t\t\t*cpumode = PERF_RECORD_MISC_KERNEL;\n\t\t\t\tbreak;\n\t\t\tcase PERF_CONTEXT_USER:\n\t\t\t\t*cpumode = PERF_RECORD_MISC_USER;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tpr_debug(\"invalid callchain context: \"\n\t\t\t\t\t \"%\"PRId64\"\\n\", (s64) ip);\n\t\t\t\t \n\t\t\t\tcallchain_cursor_reset(cursor);\n\t\t\t\terr = 1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\tthread__find_symbol(thread, *cpumode, ip, &al);\n\t}\n\n\tif (al.sym != NULL) {\n\t\tif (perf_hpp_list.parent && !*parent &&\n\t\t    symbol__match_regex(al.sym, &parent_regex))\n\t\t\t*parent = al.sym;\n\t\telse if (have_ignore_callees && root_al &&\n\t\t  symbol__match_regex(al.sym, &ignore_callees_regex)) {\n\t\t\t \n\t\t\taddr_location__copy(root_al, &al);\n\t\t\tcallchain_cursor_reset(cursor);\n\t\t}\n\t}\n\n\tif (symbol_conf.hide_unresolved && al.sym == NULL)\n\t\tgoto out;\n\n\tif (iter) {\n\t\tnr_loop_iter = iter->nr_loop_iter;\n\t\titer_cycles = iter->cycles;\n\t}\n\n\tms.maps = maps__get(al.maps);\n\tms.map = map__get(al.map);\n\tms.sym = al.sym;\n\tsrcline = callchain_srcline(&ms, al.addr);\n\terr = callchain_cursor_append(cursor, ip, &ms,\n\t\t\t\t      branch, flags, nr_loop_iter,\n\t\t\t\t      iter_cycles, branch_from, srcline);\nout:\n\taddr_location__exit(&al);\n\tmaps__put(ms.maps);\n\tmap__put(ms.map);\n\treturn err;\n}\n\nstruct branch_info *sample__resolve_bstack(struct perf_sample *sample,\n\t\t\t\t\t   struct addr_location *al)\n{\n\tunsigned int i;\n\tconst struct branch_stack *bs = sample->branch_stack;\n\tstruct branch_entry *entries = perf_sample__branch_entries(sample);\n\tstruct branch_info *bi = calloc(bs->nr, sizeof(struct branch_info));\n\n\tif (!bi)\n\t\treturn NULL;\n\n\tfor (i = 0; i < bs->nr; i++) {\n\t\tip__resolve_ams(al->thread, &bi[i].to, entries[i].to);\n\t\tip__resolve_ams(al->thread, &bi[i].from, entries[i].from);\n\t\tbi[i].flags = entries[i].flags;\n\t}\n\treturn bi;\n}\n\nstatic void save_iterations(struct iterations *iter,\n\t\t\t    struct branch_entry *be, int nr)\n{\n\tint i;\n\n\titer->nr_loop_iter++;\n\titer->cycles = 0;\n\n\tfor (i = 0; i < nr; i++)\n\t\titer->cycles += be[i].flags.cycles;\n}\n\n#define CHASHSZ 127\n#define CHASHBITS 7\n#define NO_ENTRY 0xff\n\n#define PERF_MAX_BRANCH_DEPTH 127\n\n \nstatic int remove_loops(struct branch_entry *l, int nr,\n\t\t\tstruct iterations *iter)\n{\n\tint i, j, off;\n\tunsigned char chash[CHASHSZ];\n\n\tmemset(chash, NO_ENTRY, sizeof(chash));\n\n\tBUG_ON(PERF_MAX_BRANCH_DEPTH > 255);\n\n\tfor (i = 0; i < nr; i++) {\n\t\tint h = hash_64(l[i].from, CHASHBITS) % CHASHSZ;\n\n\t\t \n\t\tif (chash[h] == NO_ENTRY) {\n\t\t\tchash[h] = i;\n\t\t} else if (l[chash[h]].from == l[i].from) {\n\t\t\tbool is_loop = true;\n\t\t\t \n\t\t\toff = 0;\n\t\t\tfor (j = chash[h]; j < i && i + off < nr; j++, off++)\n\t\t\t\tif (l[j].from != l[i + off].from) {\n\t\t\t\t\tis_loop = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tif (is_loop) {\n\t\t\t\tj = nr - (i + off);\n\t\t\t\tif (j > 0) {\n\t\t\t\t\tsave_iterations(iter + i + off,\n\t\t\t\t\t\tl + i, off);\n\n\t\t\t\t\tmemmove(iter + i, iter + i + off,\n\t\t\t\t\t\tj * sizeof(*iter));\n\n\t\t\t\t\tmemmove(l + i, l + i + off,\n\t\t\t\t\t\tj * sizeof(*l));\n\t\t\t\t}\n\n\t\t\t\tnr -= off;\n\t\t\t}\n\t\t}\n\t}\n\treturn nr;\n}\n\nstatic int lbr_callchain_add_kernel_ip(struct thread *thread,\n\t\t\t\t       struct callchain_cursor *cursor,\n\t\t\t\t       struct perf_sample *sample,\n\t\t\t\t       struct symbol **parent,\n\t\t\t\t       struct addr_location *root_al,\n\t\t\t\t       u64 branch_from,\n\t\t\t\t       bool callee, int end)\n{\n\tstruct ip_callchain *chain = sample->callchain;\n\tu8 cpumode = PERF_RECORD_MISC_USER;\n\tint err, i;\n\n\tif (callee) {\n\t\tfor (i = 0; i < end + 1; i++) {\n\t\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t\t       root_al, &cpumode, chain->ips[i],\n\t\t\t\t\t       false, NULL, NULL, branch_from);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tfor (i = end; i >= 0; i--) {\n\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t       root_al, &cpumode, chain->ips[i],\n\t\t\t\t       false, NULL, NULL, branch_from);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void save_lbr_cursor_node(struct thread *thread,\n\t\t\t\t struct callchain_cursor *cursor,\n\t\t\t\t int idx)\n{\n\tstruct lbr_stitch *lbr_stitch = thread__lbr_stitch(thread);\n\n\tif (!lbr_stitch)\n\t\treturn;\n\n\tif (cursor->pos == cursor->nr) {\n\t\tlbr_stitch->prev_lbr_cursor[idx].valid = false;\n\t\treturn;\n\t}\n\n\tif (!cursor->curr)\n\t\tcursor->curr = cursor->first;\n\telse\n\t\tcursor->curr = cursor->curr->next;\n\tmemcpy(&lbr_stitch->prev_lbr_cursor[idx], cursor->curr,\n\t       sizeof(struct callchain_cursor_node));\n\n\tlbr_stitch->prev_lbr_cursor[idx].valid = true;\n\tcursor->pos++;\n}\n\nstatic int lbr_callchain_add_lbr_ip(struct thread *thread,\n\t\t\t\t    struct callchain_cursor *cursor,\n\t\t\t\t    struct perf_sample *sample,\n\t\t\t\t    struct symbol **parent,\n\t\t\t\t    struct addr_location *root_al,\n\t\t\t\t    u64 *branch_from,\n\t\t\t\t    bool callee)\n{\n\tstruct branch_stack *lbr_stack = sample->branch_stack;\n\tstruct branch_entry *entries = perf_sample__branch_entries(sample);\n\tu8 cpumode = PERF_RECORD_MISC_USER;\n\tint lbr_nr = lbr_stack->nr;\n\tstruct branch_flags *flags;\n\tint err, i;\n\tu64 ip;\n\n\t \n\tif (thread__lbr_stitch(thread)) {\n\t\tcursor->curr = NULL;\n\t\tcursor->pos = cursor->nr;\n\t\tif (cursor->nr) {\n\t\t\tcursor->curr = cursor->first;\n\t\t\tfor (i = 0; i < (int)(cursor->nr - 1); i++)\n\t\t\t\tcursor->curr = cursor->curr->next;\n\t\t}\n\t}\n\n\tif (callee) {\n\t\t \n\t\tip = entries[0].to;\n\t\tflags = &entries[0].flags;\n\t\t*branch_from = entries[0].from;\n\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t       root_al, &cpumode, ip,\n\t\t\t\t       true, flags, NULL,\n\t\t\t\t       *branch_from);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\tif (thread__lbr_stitch(thread) && (cursor->pos != cursor->nr)) {\n\t\t\tif (!cursor->curr)\n\t\t\t\tcursor->curr = cursor->first;\n\t\t\telse\n\t\t\t\tcursor->curr = cursor->curr->next;\n\t\t\tcursor->pos++;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < lbr_nr; i++) {\n\t\t\tip = entries[i].from;\n\t\t\tflags = &entries[i].flags;\n\t\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t\t       root_al, &cpumode, ip,\n\t\t\t\t\t       true, flags, NULL,\n\t\t\t\t\t       *branch_from);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tsave_lbr_cursor_node(thread, cursor, i);\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tfor (i = lbr_nr - 1; i >= 0; i--) {\n\t\tip = entries[i].from;\n\t\tflags = &entries[i].flags;\n\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t       root_al, &cpumode, ip,\n\t\t\t\t       true, flags, NULL,\n\t\t\t\t       *branch_from);\n\t\tif (err)\n\t\t\treturn err;\n\t\tsave_lbr_cursor_node(thread, cursor, i);\n\t}\n\n\tif (lbr_nr > 0) {\n\t\t \n\t\tip = entries[0].to;\n\t\tflags = &entries[0].flags;\n\t\t*branch_from = entries[0].from;\n\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\troot_al, &cpumode, ip,\n\t\t\t\ttrue, flags, NULL,\n\t\t\t\t*branch_from);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int lbr_callchain_add_stitched_lbr_ip(struct thread *thread,\n\t\t\t\t\t     struct callchain_cursor *cursor)\n{\n\tstruct lbr_stitch *lbr_stitch = thread__lbr_stitch(thread);\n\tstruct callchain_cursor_node *cnode;\n\tstruct stitch_list *stitch_node;\n\tint err;\n\n\tlist_for_each_entry(stitch_node, &lbr_stitch->lists, node) {\n\t\tcnode = &stitch_node->cursor;\n\n\t\terr = callchain_cursor_append(cursor, cnode->ip,\n\t\t\t\t\t      &cnode->ms,\n\t\t\t\t\t      cnode->branch,\n\t\t\t\t\t      &cnode->branch_flags,\n\t\t\t\t\t      cnode->nr_loop_iter,\n\t\t\t\t\t      cnode->iter_cycles,\n\t\t\t\t\t      cnode->branch_from,\n\t\t\t\t\t      cnode->srcline);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic struct stitch_list *get_stitch_node(struct thread *thread)\n{\n\tstruct lbr_stitch *lbr_stitch = thread__lbr_stitch(thread);\n\tstruct stitch_list *stitch_node;\n\n\tif (!list_empty(&lbr_stitch->free_lists)) {\n\t\tstitch_node = list_first_entry(&lbr_stitch->free_lists,\n\t\t\t\t\t       struct stitch_list, node);\n\t\tlist_del(&stitch_node->node);\n\n\t\treturn stitch_node;\n\t}\n\n\treturn malloc(sizeof(struct stitch_list));\n}\n\nstatic bool has_stitched_lbr(struct thread *thread,\n\t\t\t     struct perf_sample *cur,\n\t\t\t     struct perf_sample *prev,\n\t\t\t     unsigned int max_lbr,\n\t\t\t     bool callee)\n{\n\tstruct branch_stack *cur_stack = cur->branch_stack;\n\tstruct branch_entry *cur_entries = perf_sample__branch_entries(cur);\n\tstruct branch_stack *prev_stack = prev->branch_stack;\n\tstruct branch_entry *prev_entries = perf_sample__branch_entries(prev);\n\tstruct lbr_stitch *lbr_stitch = thread__lbr_stitch(thread);\n\tint i, j, nr_identical_branches = 0;\n\tstruct stitch_list *stitch_node;\n\tu64 cur_base, distance;\n\n\tif (!cur_stack || !prev_stack)\n\t\treturn false;\n\n\t \n\tcur_base = max_lbr - cur_stack->nr + cur_stack->hw_idx + 1;\n\n\tdistance = (prev_stack->hw_idx > cur_base) ? (prev_stack->hw_idx - cur_base) :\n\t\t\t\t\t\t     (max_lbr + prev_stack->hw_idx - cur_base);\n\t \n\tif (distance + 1 > prev_stack->nr)\n\t\treturn false;\n\n\t \n\tfor (i = distance, j = cur_stack->nr - 1; (i >= 0) && (j >= 0); i--, j--) {\n\t\tif ((prev_entries[i].from != cur_entries[j].from) ||\n\t\t    (prev_entries[i].to != cur_entries[j].to) ||\n\t\t    (prev_entries[i].flags.value != cur_entries[j].flags.value))\n\t\t\tbreak;\n\t\tnr_identical_branches++;\n\t}\n\n\tif (!nr_identical_branches)\n\t\treturn false;\n\n\t \n\tfor (i = prev_stack->nr - 1; i > (int)distance; i--) {\n\n\t\tif (!lbr_stitch->prev_lbr_cursor[i].valid)\n\t\t\tcontinue;\n\n\t\tstitch_node = get_stitch_node(thread);\n\t\tif (!stitch_node)\n\t\t\treturn false;\n\n\t\tmemcpy(&stitch_node->cursor, &lbr_stitch->prev_lbr_cursor[i],\n\t\t       sizeof(struct callchain_cursor_node));\n\n\t\tif (callee)\n\t\t\tlist_add(&stitch_node->node, &lbr_stitch->lists);\n\t\telse\n\t\t\tlist_add_tail(&stitch_node->node, &lbr_stitch->lists);\n\t}\n\n\treturn true;\n}\n\nstatic bool alloc_lbr_stitch(struct thread *thread, unsigned int max_lbr)\n{\n\tif (thread__lbr_stitch(thread))\n\t\treturn true;\n\n\tthread__set_lbr_stitch(thread, zalloc(sizeof(struct lbr_stitch)));\n\tif (!thread__lbr_stitch(thread))\n\t\tgoto err;\n\n\tthread__lbr_stitch(thread)->prev_lbr_cursor =\n\t\tcalloc(max_lbr + 1, sizeof(struct callchain_cursor_node));\n\tif (!thread__lbr_stitch(thread)->prev_lbr_cursor)\n\t\tgoto free_lbr_stitch;\n\n\tINIT_LIST_HEAD(&thread__lbr_stitch(thread)->lists);\n\tINIT_LIST_HEAD(&thread__lbr_stitch(thread)->free_lists);\n\n\treturn true;\n\nfree_lbr_stitch:\n\tfree(thread__lbr_stitch(thread));\n\tthread__set_lbr_stitch(thread, NULL);\nerr:\n\tpr_warning(\"Failed to allocate space for stitched LBRs. Disable LBR stitch\\n\");\n\tthread__set_lbr_stitch_enable(thread, false);\n\treturn false;\n}\n\n \nstatic int resolve_lbr_callchain_sample(struct thread *thread,\n\t\t\t\t\tstruct callchain_cursor *cursor,\n\t\t\t\t\tstruct perf_sample *sample,\n\t\t\t\t\tstruct symbol **parent,\n\t\t\t\t\tstruct addr_location *root_al,\n\t\t\t\t\tint max_stack,\n\t\t\t\t\tunsigned int max_lbr)\n{\n\tbool callee = (callchain_param.order == ORDER_CALLEE);\n\tstruct ip_callchain *chain = sample->callchain;\n\tint chain_nr = min(max_stack, (int)chain->nr), i;\n\tstruct lbr_stitch *lbr_stitch;\n\tbool stitched_lbr = false;\n\tu64 branch_from = 0;\n\tint err;\n\n\tfor (i = 0; i < chain_nr; i++) {\n\t\tif (chain->ips[i] == PERF_CONTEXT_USER)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (i == chain_nr)\n\t\treturn 0;\n\n\tif (thread__lbr_stitch_enable(thread) && !sample->no_hw_idx &&\n\t    (max_lbr > 0) && alloc_lbr_stitch(thread, max_lbr)) {\n\t\tlbr_stitch = thread__lbr_stitch(thread);\n\n\t\tstitched_lbr = has_stitched_lbr(thread, sample,\n\t\t\t\t\t\t&lbr_stitch->prev_sample,\n\t\t\t\t\t\tmax_lbr, callee);\n\n\t\tif (!stitched_lbr && !list_empty(&lbr_stitch->lists)) {\n\t\t\tlist_replace_init(&lbr_stitch->lists,\n\t\t\t\t\t  &lbr_stitch->free_lists);\n\t\t}\n\t\tmemcpy(&lbr_stitch->prev_sample, sample, sizeof(*sample));\n\t}\n\n\tif (callee) {\n\t\t \n\t\terr = lbr_callchain_add_kernel_ip(thread, cursor, sample,\n\t\t\t\t\t\t  parent, root_al, branch_from,\n\t\t\t\t\t\t  true, i);\n\t\tif (err)\n\t\t\tgoto error;\n\n\t\terr = lbr_callchain_add_lbr_ip(thread, cursor, sample, parent,\n\t\t\t\t\t       root_al, &branch_from, true);\n\t\tif (err)\n\t\t\tgoto error;\n\n\t\tif (stitched_lbr) {\n\t\t\terr = lbr_callchain_add_stitched_lbr_ip(thread, cursor);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t}\n\n\t} else {\n\t\tif (stitched_lbr) {\n\t\t\terr = lbr_callchain_add_stitched_lbr_ip(thread, cursor);\n\t\t\tif (err)\n\t\t\t\tgoto error;\n\t\t}\n\t\terr = lbr_callchain_add_lbr_ip(thread, cursor, sample, parent,\n\t\t\t\t\t       root_al, &branch_from, false);\n\t\tif (err)\n\t\t\tgoto error;\n\n\t\t \n\t\terr = lbr_callchain_add_kernel_ip(thread, cursor, sample,\n\t\t\t\t\t\t  parent, root_al, branch_from,\n\t\t\t\t\t\t  false, i);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\treturn 1;\n\nerror:\n\treturn (err < 0) ? err : 0;\n}\n\nstatic int find_prev_cpumode(struct ip_callchain *chain, struct thread *thread,\n\t\t\t     struct callchain_cursor *cursor,\n\t\t\t     struct symbol **parent,\n\t\t\t     struct addr_location *root_al,\n\t\t\t     u8 *cpumode, int ent)\n{\n\tint err = 0;\n\n\twhile (--ent >= 0) {\n\t\tu64 ip = chain->ips[ent];\n\n\t\tif (ip >= PERF_CONTEXT_MAX) {\n\t\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t\t       root_al, cpumode, ip,\n\t\t\t\t\t       false, NULL, NULL, 0);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn err;\n}\n\nstatic u64 get_leaf_frame_caller(struct perf_sample *sample,\n\t\tstruct thread *thread, int usr_idx)\n{\n\tif (machine__normalized_is(maps__machine(thread__maps(thread)), \"arm64\"))\n\t\treturn get_leaf_frame_caller_aarch64(sample, thread, usr_idx);\n\telse\n\t\treturn 0;\n}\n\nstatic int thread__resolve_callchain_sample(struct thread *thread,\n\t\t\t\t\t    struct callchain_cursor *cursor,\n\t\t\t\t\t    struct evsel *evsel,\n\t\t\t\t\t    struct perf_sample *sample,\n\t\t\t\t\t    struct symbol **parent,\n\t\t\t\t\t    struct addr_location *root_al,\n\t\t\t\t\t    int max_stack)\n{\n\tstruct branch_stack *branch = sample->branch_stack;\n\tstruct branch_entry *entries = perf_sample__branch_entries(sample);\n\tstruct ip_callchain *chain = sample->callchain;\n\tint chain_nr = 0;\n\tu8 cpumode = PERF_RECORD_MISC_USER;\n\tint i, j, err, nr_entries, usr_idx;\n\tint skip_idx = -1;\n\tint first_call = 0;\n\tu64 leaf_frame_caller;\n\n\tif (chain)\n\t\tchain_nr = chain->nr;\n\n\tif (evsel__has_branch_callstack(evsel)) {\n\t\tstruct perf_env *env = evsel__env(evsel);\n\n\t\terr = resolve_lbr_callchain_sample(thread, cursor, sample, parent,\n\t\t\t\t\t\t   root_al, max_stack,\n\t\t\t\t\t\t   !env ? 0 : env->max_branches);\n\t\tif (err)\n\t\t\treturn (err < 0) ? err : 0;\n\t}\n\n\t \n\tskip_idx = arch_skip_callchain_idx(thread, chain);\n\n\t \n\n\tif (branch && callchain_param.branch_callstack) {\n\t\tint nr = min(max_stack, (int)branch->nr);\n\t\tstruct branch_entry be[nr];\n\t\tstruct iterations iter[nr];\n\n\t\tif (branch->nr > PERF_MAX_BRANCH_DEPTH) {\n\t\t\tpr_warning(\"corrupted branch chain. skipping...\\n\");\n\t\t\tgoto check_calls;\n\t\t}\n\n\t\tfor (i = 0; i < nr; i++) {\n\t\t\tif (callchain_param.order == ORDER_CALLEE) {\n\t\t\t\tbe[i] = entries[i];\n\n\t\t\t\tif (chain == NULL)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t \n\t\t\t\tif (i == skip_idx ||\n\t\t\t\t    chain->ips[first_call] >= PERF_CONTEXT_MAX)\n\t\t\t\t\tfirst_call++;\n\t\t\t\telse if (be[i].from < chain->ips[first_call] &&\n\t\t\t\t    be[i].from >= chain->ips[first_call] - 8)\n\t\t\t\t\tfirst_call++;\n\t\t\t} else\n\t\t\t\tbe[i] = entries[branch->nr - i - 1];\n\t\t}\n\n\t\tmemset(iter, 0, sizeof(struct iterations) * nr);\n\t\tnr = remove_loops(be, nr, iter);\n\n\t\tfor (i = 0; i < nr; i++) {\n\t\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t\t       root_al,\n\t\t\t\t\t       NULL, be[i].to,\n\t\t\t\t\t       true, &be[i].flags,\n\t\t\t\t\t       NULL, be[i].from);\n\n\t\t\tif (!err)\n\t\t\t\terr = add_callchain_ip(thread, cursor, parent, root_al,\n\t\t\t\t\t\t       NULL, be[i].from,\n\t\t\t\t\t\t       true, &be[i].flags,\n\t\t\t\t\t\t       &iter[i], 0);\n\t\t\tif (err == -EINVAL)\n\t\t\t\tbreak;\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (chain_nr == 0)\n\t\t\treturn 0;\n\n\t\tchain_nr -= nr;\n\t}\n\ncheck_calls:\n\tif (chain && callchain_param.order != ORDER_CALLEE) {\n\t\terr = find_prev_cpumode(chain, thread, cursor, parent, root_al,\n\t\t\t\t\t&cpumode, chain->nr - first_call);\n\t\tif (err)\n\t\t\treturn (err < 0) ? err : 0;\n\t}\n\tfor (i = first_call, nr_entries = 0;\n\t     i < chain_nr && nr_entries < max_stack; i++) {\n\t\tu64 ip;\n\n\t\tif (callchain_param.order == ORDER_CALLEE)\n\t\t\tj = i;\n\t\telse\n\t\t\tj = chain->nr - i - 1;\n\n#ifdef HAVE_SKIP_CALLCHAIN_IDX\n\t\tif (j == skip_idx)\n\t\t\tcontinue;\n#endif\n\t\tip = chain->ips[j];\n\t\tif (ip < PERF_CONTEXT_MAX)\n                       ++nr_entries;\n\t\telse if (callchain_param.order != ORDER_CALLEE) {\n\t\t\terr = find_prev_cpumode(chain, thread, cursor, parent,\n\t\t\t\t\t\troot_al, &cpumode, j);\n\t\t\tif (err)\n\t\t\t\treturn (err < 0) ? err : 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\n\t\tusr_idx = callchain_param.order == ORDER_CALLEE ? j-2 : j-1;\n\n\t\tif (usr_idx >= 0 && chain->ips[usr_idx] == PERF_CONTEXT_USER) {\n\n\t\t\tleaf_frame_caller = get_leaf_frame_caller(sample, thread, usr_idx);\n\n\t\t\t \n\n\t\t\tif (leaf_frame_caller && leaf_frame_caller != ip) {\n\n\t\t\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t\t       root_al, &cpumode, leaf_frame_caller,\n\t\t\t\t\t       false, NULL, NULL, 0);\n\t\t\t\tif (err)\n\t\t\t\t\treturn (err < 0) ? err : 0;\n\t\t\t}\n\t\t}\n\n\t\terr = add_callchain_ip(thread, cursor, parent,\n\t\t\t\t       root_al, &cpumode, ip,\n\t\t\t\t       false, NULL, NULL, 0);\n\n\t\tif (err)\n\t\t\treturn (err < 0) ? err : 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int append_inlines(struct callchain_cursor *cursor, struct map_symbol *ms, u64 ip)\n{\n\tstruct symbol *sym = ms->sym;\n\tstruct map *map = ms->map;\n\tstruct inline_node *inline_node;\n\tstruct inline_list *ilist;\n\tstruct dso *dso;\n\tu64 addr;\n\tint ret = 1;\n\tstruct map_symbol ilist_ms;\n\n\tif (!symbol_conf.inline_name || !map || !sym)\n\t\treturn ret;\n\n\taddr = map__dso_map_ip(map, ip);\n\taddr = map__rip_2objdump(map, addr);\n\tdso = map__dso(map);\n\n\tinline_node = inlines__tree_find(&dso->inlined_nodes, addr);\n\tif (!inline_node) {\n\t\tinline_node = dso__parse_addr_inlines(dso, addr, sym);\n\t\tif (!inline_node)\n\t\t\treturn ret;\n\t\tinlines__tree_insert(&dso->inlined_nodes, inline_node);\n\t}\n\n\tilist_ms = (struct map_symbol) {\n\t\t.maps = maps__get(ms->maps),\n\t\t.map = map__get(map),\n\t};\n\tlist_for_each_entry(ilist, &inline_node->val, list) {\n\t\tilist_ms.sym = ilist->symbol;\n\t\tret = callchain_cursor_append(cursor, ip, &ilist_ms, false,\n\t\t\t\t\t      NULL, 0, 0, 0, ilist->srcline);\n\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\tmap__put(ilist_ms.map);\n\tmaps__put(ilist_ms.maps);\n\n\treturn ret;\n}\n\nstatic int unwind_entry(struct unwind_entry *entry, void *arg)\n{\n\tstruct callchain_cursor *cursor = arg;\n\tconst char *srcline = NULL;\n\tu64 addr = entry->ip;\n\n\tif (symbol_conf.hide_unresolved && entry->ms.sym == NULL)\n\t\treturn 0;\n\n\tif (append_inlines(cursor, &entry->ms, entry->ip) == 0)\n\t\treturn 0;\n\n\t \n\tif (entry->ms.map)\n\t\taddr = map__dso_map_ip(entry->ms.map, entry->ip);\n\n\tsrcline = callchain_srcline(&entry->ms, addr);\n\treturn callchain_cursor_append(cursor, entry->ip, &entry->ms,\n\t\t\t\t       false, NULL, 0, 0, 0, srcline);\n}\n\nstatic int thread__resolve_callchain_unwind(struct thread *thread,\n\t\t\t\t\t    struct callchain_cursor *cursor,\n\t\t\t\t\t    struct evsel *evsel,\n\t\t\t\t\t    struct perf_sample *sample,\n\t\t\t\t\t    int max_stack)\n{\n\t \n\tif (!((evsel->core.attr.sample_type & PERF_SAMPLE_REGS_USER) &&\n\t      (evsel->core.attr.sample_type & PERF_SAMPLE_STACK_USER)))\n\t\treturn 0;\n\n\t \n\tif ((!sample->user_regs.regs) ||\n\t    (!sample->user_stack.size))\n\t\treturn 0;\n\n\treturn unwind__get_entries(unwind_entry, cursor,\n\t\t\t\t   thread, sample, max_stack, false);\n}\n\nint thread__resolve_callchain(struct thread *thread,\n\t\t\t      struct callchain_cursor *cursor,\n\t\t\t      struct evsel *evsel,\n\t\t\t      struct perf_sample *sample,\n\t\t\t      struct symbol **parent,\n\t\t\t      struct addr_location *root_al,\n\t\t\t      int max_stack)\n{\n\tint ret = 0;\n\n\tif (cursor == NULL)\n\t\treturn -ENOMEM;\n\n\tcallchain_cursor_reset(cursor);\n\n\tif (callchain_param.order == ORDER_CALLEE) {\n\t\tret = thread__resolve_callchain_sample(thread, cursor,\n\t\t\t\t\t\t       evsel, sample,\n\t\t\t\t\t\t       parent, root_al,\n\t\t\t\t\t\t       max_stack);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = thread__resolve_callchain_unwind(thread, cursor,\n\t\t\t\t\t\t       evsel, sample,\n\t\t\t\t\t\t       max_stack);\n\t} else {\n\t\tret = thread__resolve_callchain_unwind(thread, cursor,\n\t\t\t\t\t\t       evsel, sample,\n\t\t\t\t\t\t       max_stack);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = thread__resolve_callchain_sample(thread, cursor,\n\t\t\t\t\t\t       evsel, sample,\n\t\t\t\t\t\t       parent, root_al,\n\t\t\t\t\t\t       max_stack);\n\t}\n\n\treturn ret;\n}\n\nint machine__for_each_thread(struct machine *machine,\n\t\t\t     int (*fn)(struct thread *thread, void *p),\n\t\t\t     void *priv)\n{\n\tstruct threads *threads;\n\tstruct rb_node *nd;\n\tint rc = 0;\n\tint i;\n\n\tfor (i = 0; i < THREADS__TABLE_SIZE; i++) {\n\t\tthreads = &machine->threads[i];\n\t\tfor (nd = rb_first_cached(&threads->entries); nd;\n\t\t     nd = rb_next(nd)) {\n\t\t\tstruct thread_rb_node *trb = rb_entry(nd, struct thread_rb_node, rb_node);\n\n\t\t\trc = fn(trb->thread, priv);\n\t\t\tif (rc != 0)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\treturn rc;\n}\n\nint machines__for_each_thread(struct machines *machines,\n\t\t\t      int (*fn)(struct thread *thread, void *p),\n\t\t\t      void *priv)\n{\n\tstruct rb_node *nd;\n\tint rc = 0;\n\n\trc = machine__for_each_thread(&machines->host, fn, priv);\n\tif (rc != 0)\n\t\treturn rc;\n\n\tfor (nd = rb_first_cached(&machines->guests); nd; nd = rb_next(nd)) {\n\t\tstruct machine *machine = rb_entry(nd, struct machine, rb_node);\n\n\t\trc = machine__for_each_thread(machine, fn, priv);\n\t\tif (rc != 0)\n\t\t\treturn rc;\n\t}\n\treturn rc;\n}\n\npid_t machine__get_current_tid(struct machine *machine, int cpu)\n{\n\tif (cpu < 0 || (size_t)cpu >= machine->current_tid_sz)\n\t\treturn -1;\n\n\treturn machine->current_tid[cpu];\n}\n\nint machine__set_current_tid(struct machine *machine, int cpu, pid_t pid,\n\t\t\t     pid_t tid)\n{\n\tstruct thread *thread;\n\tconst pid_t init_val = -1;\n\n\tif (cpu < 0)\n\t\treturn -EINVAL;\n\n\tif (realloc_array_as_needed(machine->current_tid,\n\t\t\t\t    machine->current_tid_sz,\n\t\t\t\t    (unsigned int)cpu,\n\t\t\t\t    &init_val))\n\t\treturn -ENOMEM;\n\n\tmachine->current_tid[cpu] = tid;\n\n\tthread = machine__findnew_thread(machine, pid, tid);\n\tif (!thread)\n\t\treturn -ENOMEM;\n\n\tthread__set_cpu(thread, cpu);\n\tthread__put(thread);\n\n\treturn 0;\n}\n\n \nbool machine__is(struct machine *machine, const char *arch)\n{\n\treturn machine && !strcmp(perf_env__raw_arch(machine->env), arch);\n}\n\nbool machine__normalized_is(struct machine *machine, const char *arch)\n{\n\treturn machine && !strcmp(perf_env__arch(machine->env), arch);\n}\n\nint machine__nr_cpus_avail(struct machine *machine)\n{\n\treturn machine ? perf_env__nr_cpus_avail(machine->env) : 0;\n}\n\nint machine__get_kernel_start(struct machine *machine)\n{\n\tstruct map *map = machine__kernel_map(machine);\n\tint err = 0;\n\n\t \n\tmachine->kernel_start = 1ULL << 63;\n\tif (map) {\n\t\terr = map__load(map);\n\t\t \n\t\tif (!err && !machine__is(machine, \"x86_64\"))\n\t\t\tmachine->kernel_start = map__start(map);\n\t}\n\treturn err;\n}\n\nu8 machine__addr_cpumode(struct machine *machine, u8 cpumode, u64 addr)\n{\n\tu8 addr_cpumode = cpumode;\n\tbool kernel_ip;\n\n\tif (!machine->single_address_space)\n\t\tgoto out;\n\n\tkernel_ip = machine__kernel_ip(machine, addr);\n\tswitch (cpumode) {\n\tcase PERF_RECORD_MISC_KERNEL:\n\tcase PERF_RECORD_MISC_USER:\n\t\taddr_cpumode = kernel_ip ? PERF_RECORD_MISC_KERNEL :\n\t\t\t\t\t   PERF_RECORD_MISC_USER;\n\t\tbreak;\n\tcase PERF_RECORD_MISC_GUEST_KERNEL:\n\tcase PERF_RECORD_MISC_GUEST_USER:\n\t\taddr_cpumode = kernel_ip ? PERF_RECORD_MISC_GUEST_KERNEL :\n\t\t\t\t\t   PERF_RECORD_MISC_GUEST_USER;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\nout:\n\treturn addr_cpumode;\n}\n\nstruct dso *machine__findnew_dso_id(struct machine *machine, const char *filename, struct dso_id *id)\n{\n\treturn dsos__findnew_id(&machine->dsos, filename, id);\n}\n\nstruct dso *machine__findnew_dso(struct machine *machine, const char *filename)\n{\n\treturn machine__findnew_dso_id(machine, filename, NULL);\n}\n\nchar *machine__resolve_kernel_addr(void *vmachine, unsigned long long *addrp, char **modp)\n{\n\tstruct machine *machine = vmachine;\n\tstruct map *map;\n\tstruct symbol *sym = machine__find_kernel_symbol(machine, *addrp, &map);\n\n\tif (sym == NULL)\n\t\treturn NULL;\n\n\t*modp = __map__is_kmodule(map) ? (char *)map__dso(map)->short_name : NULL;\n\t*addrp = map__unmap_ip(map, sym->start);\n\treturn sym->name;\n}\n\nint machine__for_each_dso(struct machine *machine, machine__dso_t fn, void *priv)\n{\n\tstruct dso *pos;\n\tint err = 0;\n\n\tlist_for_each_entry(pos, &machine->dsos.head, node) {\n\t\tif (fn(pos, machine, priv))\n\t\t\terr = -1;\n\t}\n\treturn err;\n}\n\nint machine__for_each_kernel_map(struct machine *machine, machine__map_t fn, void *priv)\n{\n\tstruct maps *maps = machine__kernel_maps(machine);\n\tstruct map_rb_node *pos;\n\tint err = 0;\n\n\tmaps__for_each_entry(maps, pos) {\n\t\terr = fn(pos->map, priv);\n\t\tif (err != 0) {\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn err;\n}\n\nbool machine__is_lock_function(struct machine *machine, u64 addr)\n{\n\tif (!machine->sched.text_start) {\n\t\tstruct map *kmap;\n\t\tstruct symbol *sym = machine__find_kernel_symbol_by_name(machine, \"__sched_text_start\", &kmap);\n\n\t\tif (!sym) {\n\t\t\t \n\t\t\tmachine->sched.text_start = 1;\n\t\t\treturn false;\n\t\t}\n\n\t\tmachine->sched.text_start = map__unmap_ip(kmap, sym->start);\n\n\t\t \n\t\tsym = machine__find_kernel_symbol_by_name(machine, \"__sched_text_end\", &kmap);\n\t\tmachine->sched.text_end = map__unmap_ip(kmap, sym->start);\n\n\t\tsym = machine__find_kernel_symbol_by_name(machine, \"__lock_text_start\", &kmap);\n\t\tmachine->lock.text_start = map__unmap_ip(kmap, sym->start);\n\n\t\tsym = machine__find_kernel_symbol_by_name(machine, \"__lock_text_end\", &kmap);\n\t\tmachine->lock.text_end = map__unmap_ip(kmap, sym->start);\n\t}\n\n\t \n\tif (machine->sched.text_start == 1)\n\t\treturn false;\n\n\t \n\tif (machine->sched.text_start <= addr && addr < machine->sched.text_end)\n\t\treturn true;\n\n\t \n\tif (machine->lock.text_start <= addr && addr < machine->lock.text_end)\n\t\treturn true;\n\n\treturn false;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}