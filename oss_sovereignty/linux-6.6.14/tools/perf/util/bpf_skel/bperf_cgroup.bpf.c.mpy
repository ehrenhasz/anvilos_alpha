{
  "module_name": "bperf_cgroup.bpf.c",
  "hash_id": "6f719e67d54b743ef1610bfa73aa4ced41c276cb54561166b56a8353221d4a4b",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/bpf_skel/bperf_cgroup.bpf.c",
  "human_readable_source": "\n\n\n#include \"vmlinux.h\"\n#include <bpf/bpf_helpers.h>\n#include <bpf/bpf_tracing.h>\n#include <bpf/bpf_core_read.h>\n\n#define MAX_LEVELS  10  \n#define MAX_EVENTS  32  \n\n\n\n\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY);\n\t__uint(key_size, sizeof(__u32));\n\t__uint(value_size, sizeof(int));\n\t__uint(max_entries, 1);\n} events SEC(\".maps\");\n\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__uint(key_size, sizeof(__u64));\n\t__uint(value_size, sizeof(__u32));\n\t__uint(max_entries, 1);\n} cgrp_idx SEC(\".maps\");\n\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);\n\t__uint(key_size, sizeof(__u32));\n\t__uint(value_size, sizeof(struct bpf_perf_event_value));\n} prev_readings SEC(\".maps\");\n\n\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);\n\t__uint(key_size, sizeof(__u32));\n\t__uint(value_size, sizeof(struct bpf_perf_event_value));\n} cgrp_readings SEC(\".maps\");\n\n \nstruct cgroup___new {\n\tint level;\n\tstruct cgroup *ancestors[];\n} __attribute__((preserve_access_index));\n\n \nstruct cgroup___old {\n\tint level;\n\tu64 ancestor_ids[];\n} __attribute__((preserve_access_index));\n\nconst volatile __u32 num_events = 1;\nconst volatile __u32 num_cpus = 1;\n\nint enabled = 0;\nint use_cgroup_v2 = 0;\nint perf_subsys_id = -1;\n\nstatic inline __u64 get_cgroup_v1_ancestor_id(struct cgroup *cgrp, int level)\n{\n\t \n\tstruct cgroup___new *cgrp_new = (void *)cgrp;\n\n\tif (bpf_core_field_exists(cgrp_new->ancestors)) {\n\t\treturn BPF_CORE_READ(cgrp_new, ancestors[level], kn, id);\n\t} else {\n\t\t \n\t\tstruct cgroup___old *cgrp_old = (void *)cgrp;\n\n\t\treturn BPF_CORE_READ(cgrp_old, ancestor_ids[level]);\n\t}\n}\n\nstatic inline int get_cgroup_v1_idx(__u32 *cgrps, int size)\n{\n\tstruct task_struct *p = (void *)bpf_get_current_task();\n\tstruct cgroup *cgrp;\n\tregister int i = 0;\n\t__u32 *elem;\n\tint level;\n\tint cnt;\n\n\tif (perf_subsys_id == -1) {\n#if __has_builtin(__builtin_preserve_enum_value)\n\t\tperf_subsys_id = bpf_core_enum_value(enum cgroup_subsys_id,\n\t\t\t\t\t\t     perf_event_cgrp_id);\n#else\n\t\tperf_subsys_id = perf_event_cgrp_id;\n#endif\n\t}\n\tcgrp = BPF_CORE_READ(p, cgroups, subsys[perf_subsys_id], cgroup);\n\tlevel = BPF_CORE_READ(cgrp, level);\n\n\tfor (cnt = 0; i < MAX_LEVELS; i++) {\n\t\t__u64 cgrp_id;\n\n\t\tif (i > level)\n\t\t\tbreak;\n\n\t\t\n\t\tcgrp_id = get_cgroup_v1_ancestor_id(cgrp, i);\n\t\telem = bpf_map_lookup_elem(&cgrp_idx, &cgrp_id);\n\t\tif (!elem)\n\t\t\tcontinue;\n\n\t\tcgrps[cnt++] = *elem;\n\t\tif (cnt == size)\n\t\t\tbreak;\n\t}\n\n\treturn cnt;\n}\n\nstatic inline int get_cgroup_v2_idx(__u32 *cgrps, int size)\n{\n\tregister int i = 0;\n\t__u32 *elem;\n\tint cnt;\n\n\tfor (cnt = 0; i < MAX_LEVELS; i++) {\n\t\t__u64 cgrp_id = bpf_get_current_ancestor_cgroup_id(i);\n\n\t\tif (cgrp_id == 0)\n\t\t\tbreak;\n\n\t\t\n\t\telem = bpf_map_lookup_elem(&cgrp_idx, &cgrp_id);\n\t\tif (!elem)\n\t\t\tcontinue;\n\n\t\tcgrps[cnt++] = *elem;\n\t\tif (cnt == size)\n\t\t\tbreak;\n\t}\n\n\treturn cnt;\n}\n\nstatic int bperf_cgroup_count(void)\n{\n\tregister __u32 idx = 0;  \n\tregister int c = 0;\n\tstruct bpf_perf_event_value val, delta, *prev_val, *cgrp_val;\n\t__u32 cpu = bpf_get_smp_processor_id();\n\t__u32 cgrp_idx[MAX_LEVELS];\n\tint cgrp_cnt;\n\t__u32 key, cgrp;\n\tlong err;\n\n\tif (use_cgroup_v2)\n\t\tcgrp_cnt = get_cgroup_v2_idx(cgrp_idx, MAX_LEVELS);\n\telse\n\t\tcgrp_cnt = get_cgroup_v1_idx(cgrp_idx, MAX_LEVELS);\n\n\tfor ( ; idx < MAX_EVENTS; idx++) {\n\t\tif (idx == num_events)\n\t\t\tbreak;\n\n\t\t\n\t\tkey = idx;\n\t\t\n\t\tprev_val = bpf_map_lookup_elem(&prev_readings, &key);\n\t\tif (!prev_val) {\n\t\t\tval.counter = val.enabled = val.running = 0;\n\t\t\tbpf_map_update_elem(&prev_readings, &key, &val, BPF_ANY);\n\n\t\t\tprev_val = bpf_map_lookup_elem(&prev_readings, &key);\n\t\t\tif (!prev_val)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t\n\t\tkey = idx * num_cpus + cpu;\n\t\terr = bpf_perf_event_read_value(&events, key, &val, sizeof(val));\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tif (enabled) {\n\t\t\tdelta.counter = val.counter - prev_val->counter;\n\t\t\tdelta.enabled = val.enabled - prev_val->enabled;\n\t\t\tdelta.running = val.running - prev_val->running;\n\n\t\t\tfor (c = 0; c < MAX_LEVELS; c++) {\n\t\t\t\tif (c == cgrp_cnt)\n\t\t\t\t\tbreak;\n\n\t\t\t\tcgrp = cgrp_idx[c];\n\n\t\t\t\t\n\t\t\t\tkey = cgrp * num_events + idx;\n\t\t\t\tcgrp_val = bpf_map_lookup_elem(&cgrp_readings, &key);\n\t\t\t\tif (cgrp_val) {\n\t\t\t\t\tcgrp_val->counter += delta.counter;\n\t\t\t\t\tcgrp_val->enabled += delta.enabled;\n\t\t\t\t\tcgrp_val->running += delta.running;\n\t\t\t\t} else {\n\t\t\t\t\tbpf_map_update_elem(&cgrp_readings, &key,\n\t\t\t\t\t\t\t    &delta, BPF_ANY);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t*prev_val = val;\n\t}\n\treturn 0;\n}\n\n\nSEC(\"perf_event\")\nint BPF_PROG(on_cgrp_switch)\n{\n\treturn bperf_cgroup_count();\n}\n\nSEC(\"raw_tp/sched_switch\")\nint BPF_PROG(trigger_read)\n{\n\treturn bperf_cgroup_count();\n}\n\nchar LICENSE[] SEC(\"license\") = \"Dual BSD/GPL\";\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}