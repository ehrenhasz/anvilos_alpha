{
  "module_name": "kwork_trace.bpf.c",
  "hash_id": "c3c411e92ee2aad7857891df43552d73e83d787c56e994f7f27f56f09219cc31",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/bpf_skel/kwork_trace.bpf.c",
  "human_readable_source": "\n\n\n#include \"vmlinux.h\"\n#include <bpf/bpf_helpers.h>\n#include <bpf/bpf_tracing.h>\n\n#define KWORK_COUNT 100\n#define MAX_KWORKNAME 128\n\n \nenum kwork_class_type {\n\tKWORK_CLASS_IRQ,\n\tKWORK_CLASS_SOFTIRQ,\n\tKWORK_CLASS_WORKQUEUE,\n\tKWORK_CLASS_MAX,\n};\n\nstruct work_key {\n\t__u32 type;\n\t__u32 cpu;\n\t__u64 id;\n};\n\nstruct report_data {\n\t__u64 nr;\n\t__u64 total_time;\n\t__u64 max_time;\n\t__u64 max_time_start;\n\t__u64 max_time_end;\n};\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__uint(key_size, sizeof(struct work_key));\n\t__uint(value_size, MAX_KWORKNAME);\n\t__uint(max_entries, KWORK_COUNT);\n} perf_kwork_names SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__uint(key_size, sizeof(struct work_key));\n\t__uint(value_size, sizeof(__u64));\n\t__uint(max_entries, KWORK_COUNT);\n} perf_kwork_time SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__uint(key_size, sizeof(struct work_key));\n\t__uint(value_size, sizeof(struct report_data));\n\t__uint(max_entries, KWORK_COUNT);\n} perf_kwork_report SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__uint(key_size, sizeof(__u32));\n\t__uint(value_size, sizeof(__u8));\n\t__uint(max_entries, 1);\n} perf_kwork_cpu_filter SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_ARRAY);\n\t__uint(key_size, sizeof(__u32));\n\t__uint(value_size, MAX_KWORKNAME);\n\t__uint(max_entries, 1);\n} perf_kwork_name_filter SEC(\".maps\");\n\nint enabled = 0;\nint has_cpu_filter = 0;\nint has_name_filter = 0;\n\nstatic __always_inline int local_strncmp(const char *s1,\n\t\t\t\t\t unsigned int sz, const char *s2)\n{\n\tint ret = 0;\n\tunsigned int i;\n\n\tfor (i = 0; i < sz; i++) {\n\t\tret = (unsigned char)s1[i] - (unsigned char)s2[i];\n\t\tif (ret || !s1[i] || !s2[i])\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic __always_inline int trace_event_match(struct work_key *key, char *name)\n{\n\t__u8 *cpu_val;\n\tchar *name_val;\n\t__u32 zero = 0;\n\t__u32 cpu = bpf_get_smp_processor_id();\n\n\tif (!enabled)\n\t\treturn 0;\n\n\tif (has_cpu_filter) {\n\t\tcpu_val = bpf_map_lookup_elem(&perf_kwork_cpu_filter, &cpu);\n\t\tif (!cpu_val)\n\t\t\treturn 0;\n\t}\n\n\tif (has_name_filter && (name != NULL)) {\n\t\tname_val = bpf_map_lookup_elem(&perf_kwork_name_filter, &zero);\n\t\tif (name_val &&\n\t\t    (local_strncmp(name_val, MAX_KWORKNAME, name) != 0)) {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\nstatic __always_inline void do_update_time(void *map, struct work_key *key,\n\t\t\t\t\t   __u64 time_start, __u64 time_end)\n{\n\tstruct report_data zero, *data;\n\t__s64 delta = time_end - time_start;\n\n\tif (delta < 0)\n\t\treturn;\n\n\tdata = bpf_map_lookup_elem(map, key);\n\tif (!data) {\n\t\t__builtin_memset(&zero, 0, sizeof(zero));\n\t\tbpf_map_update_elem(map, key, &zero, BPF_NOEXIST);\n\t\tdata = bpf_map_lookup_elem(map, key);\n\t\tif (!data)\n\t\t\treturn;\n\t}\n\n\tif ((delta > data->max_time) ||\n\t    (data->max_time == 0)) {\n\t\tdata->max_time       = delta;\n\t\tdata->max_time_start = time_start;\n\t\tdata->max_time_end   = time_end;\n\t}\n\n\tdata->total_time += delta;\n\tdata->nr++;\n}\n\nstatic __always_inline void do_update_timestart(void *map, struct work_key *key)\n{\n\t__u64 ts = bpf_ktime_get_ns();\n\n\tbpf_map_update_elem(map, key, &ts, BPF_ANY);\n}\n\nstatic __always_inline void do_update_timeend(void *report_map, void *time_map,\n\t\t\t\t\t      struct work_key *key)\n{\n\t__u64 *time = bpf_map_lookup_elem(time_map, key);\n\n\tif (time) {\n\t\tbpf_map_delete_elem(time_map, key);\n\t\tdo_update_time(report_map, key, *time, bpf_ktime_get_ns());\n\t}\n}\n\nstatic __always_inline void do_update_name(void *map,\n\t\t\t\t\t   struct work_key *key, char *name)\n{\n\tif (!bpf_map_lookup_elem(map, key))\n\t\tbpf_map_update_elem(map, key, name, BPF_ANY);\n}\n\nstatic __always_inline int update_timestart(void *map, struct work_key *key)\n{\n\tif (!trace_event_match(key, NULL))\n\t\treturn 0;\n\n\tdo_update_timestart(map, key);\n\treturn 0;\n}\n\nstatic __always_inline int update_timestart_and_name(void *time_map,\n\t\t\t\t\t\t     void *names_map,\n\t\t\t\t\t\t     struct work_key *key,\n\t\t\t\t\t\t     char *name)\n{\n\tif (!trace_event_match(key, name))\n\t\treturn 0;\n\n\tdo_update_timestart(time_map, key);\n\tdo_update_name(names_map, key, name);\n\n\treturn 0;\n}\n\nstatic __always_inline int update_timeend(void *report_map,\n\t\t\t\t\t  void *time_map, struct work_key *key)\n{\n\tif (!trace_event_match(key, NULL))\n\t\treturn 0;\n\n\tdo_update_timeend(report_map, time_map, key);\n\n\treturn 0;\n}\n\nstatic __always_inline int update_timeend_and_name(void *report_map,\n\t\t\t\t\t\t   void *time_map,\n\t\t\t\t\t\t   void *names_map,\n\t\t\t\t\t\t   struct work_key *key,\n\t\t\t\t\t\t   char *name)\n{\n\tif (!trace_event_match(key, name))\n\t\treturn 0;\n\n\tdo_update_timeend(report_map, time_map, key);\n\tdo_update_name(names_map, key, name);\n\n\treturn 0;\n}\n\nSEC(\"tracepoint/irq/irq_handler_entry\")\nint report_irq_handler_entry(struct trace_event_raw_irq_handler_entry *ctx)\n{\n\tchar name[MAX_KWORKNAME];\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_IRQ,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->irq,\n\t};\n\tvoid *name_addr = (void *)ctx + (ctx->__data_loc_name & 0xffff);\n\n\tbpf_probe_read_kernel_str(name, sizeof(name), name_addr);\n\n\treturn update_timestart_and_name(&perf_kwork_time,\n\t\t\t\t\t &perf_kwork_names, &key, name);\n}\n\nSEC(\"tracepoint/irq/irq_handler_exit\")\nint report_irq_handler_exit(struct trace_event_raw_irq_handler_exit *ctx)\n{\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_IRQ,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->irq,\n\t};\n\n\treturn update_timeend(&perf_kwork_report, &perf_kwork_time, &key);\n}\n\nstatic char softirq_name_list[NR_SOFTIRQS][MAX_KWORKNAME] = {\n\t{ \"HI\"       },\n\t{ \"TIMER\"    },\n\t{ \"NET_TX\"   },\n\t{ \"NET_RX\"   },\n\t{ \"BLOCK\"    },\n\t{ \"IRQ_POLL\" },\n\t{ \"TASKLET\"  },\n\t{ \"SCHED\"    },\n\t{ \"HRTIMER\"  },\n\t{ \"RCU\"      },\n};\n\nSEC(\"tracepoint/irq/softirq_entry\")\nint report_softirq_entry(struct trace_event_raw_softirq *ctx)\n{\n\tunsigned int vec = ctx->vec;\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_SOFTIRQ,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)vec,\n\t};\n\n\tif (vec < NR_SOFTIRQS) {\n\t\treturn update_timestart_and_name(&perf_kwork_time,\n\t\t\t\t\t\t &perf_kwork_names, &key,\n\t\t\t\t\t\t softirq_name_list[vec]);\n\t}\n\n\treturn 0;\n}\n\nSEC(\"tracepoint/irq/softirq_exit\")\nint report_softirq_exit(struct trace_event_raw_softirq *ctx)\n{\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_SOFTIRQ,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->vec,\n\t};\n\n\treturn update_timeend(&perf_kwork_report, &perf_kwork_time, &key);\n}\n\nSEC(\"tracepoint/irq/softirq_raise\")\nint latency_softirq_raise(struct trace_event_raw_softirq *ctx)\n{\n\tunsigned int vec = ctx->vec;\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_SOFTIRQ,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)vec,\n\t};\n\n\tif (vec < NR_SOFTIRQS) {\n\t\treturn update_timestart_and_name(&perf_kwork_time,\n\t\t\t\t\t\t &perf_kwork_names, &key,\n\t\t\t\t\t\t softirq_name_list[vec]);\n\t}\n\n\treturn 0;\n}\n\nSEC(\"tracepoint/irq/softirq_entry\")\nint latency_softirq_entry(struct trace_event_raw_softirq *ctx)\n{\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_SOFTIRQ,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->vec,\n\t};\n\n\treturn update_timeend(&perf_kwork_report, &perf_kwork_time, &key);\n}\n\nSEC(\"tracepoint/workqueue/workqueue_execute_start\")\nint report_workqueue_execute_start(struct trace_event_raw_workqueue_execute_start *ctx)\n{\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_WORKQUEUE,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->work,\n\t};\n\n\treturn update_timestart(&perf_kwork_time, &key);\n}\n\nSEC(\"tracepoint/workqueue/workqueue_execute_end\")\nint report_workqueue_execute_end(struct trace_event_raw_workqueue_execute_end *ctx)\n{\n\tchar name[MAX_KWORKNAME];\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_WORKQUEUE,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->work,\n\t};\n\tunsigned long long func_addr = (unsigned long long)ctx->function;\n\n\t__builtin_memset(name, 0, sizeof(name));\n\tbpf_snprintf(name, sizeof(name), \"%ps\", &func_addr, sizeof(func_addr));\n\n\treturn update_timeend_and_name(&perf_kwork_report, &perf_kwork_time,\n\t\t\t\t       &perf_kwork_names, &key, name);\n}\n\nSEC(\"tracepoint/workqueue/workqueue_activate_work\")\nint latency_workqueue_activate_work(struct trace_event_raw_workqueue_activate_work *ctx)\n{\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_WORKQUEUE,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->work,\n\t};\n\n\treturn update_timestart(&perf_kwork_time, &key);\n}\n\nSEC(\"tracepoint/workqueue/workqueue_execute_start\")\nint latency_workqueue_execute_start(struct trace_event_raw_workqueue_execute_start *ctx)\n{\n\tchar name[MAX_KWORKNAME];\n\tstruct work_key key = {\n\t\t.type = KWORK_CLASS_WORKQUEUE,\n\t\t.cpu  = bpf_get_smp_processor_id(),\n\t\t.id   = (__u64)ctx->work,\n\t};\n\tunsigned long long func_addr = (unsigned long long)ctx->function;\n\n\t__builtin_memset(name, 0, sizeof(name));\n\tbpf_snprintf(name, sizeof(name), \"%ps\", &func_addr, sizeof(func_addr));\n\n\treturn update_timeend_and_name(&perf_kwork_report, &perf_kwork_time,\n\t\t\t\t       &perf_kwork_names, &key, name);\n}\n\nchar LICENSE[] SEC(\"license\") = \"Dual BSD/GPL\";\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}