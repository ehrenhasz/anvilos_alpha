{
  "module_name": "evsel.c",
  "hash_id": "e464f11610f8a3854b1431779a8ce45651ebfe8af6f56a4689092d7ac639bdda",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/evsel.c",
  "human_readable_source": "\n \n\n#include <byteswap.h>\n#include <errno.h>\n#include <inttypes.h>\n#include <linux/bitops.h>\n#include <api/fs/fs.h>\n#include <api/fs/tracing_path.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/perf_event.h>\n#include <linux/compiler.h>\n#include <linux/err.h>\n#include <linux/zalloc.h>\n#include <sys/ioctl.h>\n#include <sys/resource.h>\n#include <sys/types.h>\n#include <dirent.h>\n#include <stdlib.h>\n#include <perf/evsel.h>\n#include \"asm/bug.h\"\n#include \"bpf_counter.h\"\n#include \"callchain.h\"\n#include \"cgroup.h\"\n#include \"counts.h\"\n#include \"event.h\"\n#include \"evsel.h\"\n#include \"util/env.h\"\n#include \"util/evsel_config.h\"\n#include \"util/evsel_fprintf.h\"\n#include \"evlist.h\"\n#include <perf/cpumap.h>\n#include \"thread_map.h\"\n#include \"target.h\"\n#include \"perf_regs.h\"\n#include \"record.h\"\n#include \"debug.h\"\n#include \"trace-event.h\"\n#include \"stat.h\"\n#include \"string2.h\"\n#include \"memswap.h\"\n#include \"util.h\"\n#include \"util/hashmap.h\"\n#include \"off_cpu.h\"\n#include \"pmu.h\"\n#include \"pmus.h\"\n#include \"../perf-sys.h\"\n#include \"util/parse-branch-options.h\"\n#include \"util/bpf-filter.h\"\n#include <internal/xyarray.h>\n#include <internal/lib.h>\n#include <internal/threadmap.h>\n\n#include <linux/ctype.h>\n\n#ifdef HAVE_LIBTRACEEVENT\n#include <traceevent/event-parse.h>\n#endif\n\nstruct perf_missing_features perf_missing_features;\n\nstatic clockid_t clockid;\n\nstatic const char *const perf_tool_event__tool_names[PERF_TOOL_MAX] = {\n\tNULL,\n\t\"duration_time\",\n\t\"user_time\",\n\t\"system_time\",\n};\n\nconst char *perf_tool_event__to_str(enum perf_tool_event ev)\n{\n\tif (ev > PERF_TOOL_NONE && ev < PERF_TOOL_MAX)\n\t\treturn perf_tool_event__tool_names[ev];\n\n\treturn NULL;\n}\n\nenum perf_tool_event perf_tool_event__from_str(const char *str)\n{\n\tint i;\n\n\tperf_tool_event__for_each_event(i) {\n\t\tif (!strcmp(str, perf_tool_event__tool_names[i]))\n\t\t\treturn i;\n\t}\n\treturn PERF_TOOL_NONE;\n}\n\n\nstatic int evsel__no_extra_init(struct evsel *evsel __maybe_unused)\n{\n\treturn 0;\n}\n\nvoid __weak test_attr__ready(void) { }\n\nstatic void evsel__no_extra_fini(struct evsel *evsel __maybe_unused)\n{\n}\n\nstatic struct {\n\tsize_t\tsize;\n\tint\t(*init)(struct evsel *evsel);\n\tvoid\t(*fini)(struct evsel *evsel);\n} perf_evsel__object = {\n\t.size = sizeof(struct evsel),\n\t.init = evsel__no_extra_init,\n\t.fini = evsel__no_extra_fini,\n};\n\nint evsel__object_config(size_t object_size, int (*init)(struct evsel *evsel),\n\t\t\t void (*fini)(struct evsel *evsel))\n{\n\n\tif (object_size == 0)\n\t\tgoto set_methods;\n\n\tif (perf_evsel__object.size > object_size)\n\t\treturn -EINVAL;\n\n\tperf_evsel__object.size = object_size;\n\nset_methods:\n\tif (init != NULL)\n\t\tperf_evsel__object.init = init;\n\n\tif (fini != NULL)\n\t\tperf_evsel__object.fini = fini;\n\n\treturn 0;\n}\n\n#define FD(e, x, y) (*(int *)xyarray__entry(e->core.fd, x, y))\n\nint __evsel__sample_size(u64 sample_type)\n{\n\tu64 mask = sample_type & PERF_SAMPLE_MASK;\n\tint size = 0;\n\tint i;\n\n\tfor (i = 0; i < 64; i++) {\n\t\tif (mask & (1ULL << i))\n\t\t\tsize++;\n\t}\n\n\tsize *= sizeof(u64);\n\n\treturn size;\n}\n\n \nstatic int __perf_evsel__calc_id_pos(u64 sample_type)\n{\n\tint idx = 0;\n\n\tif (sample_type & PERF_SAMPLE_IDENTIFIER)\n\t\treturn 0;\n\n\tif (!(sample_type & PERF_SAMPLE_ID))\n\t\treturn -1;\n\n\tif (sample_type & PERF_SAMPLE_IP)\n\t\tidx += 1;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tidx += 1;\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tidx += 1;\n\n\tif (sample_type & PERF_SAMPLE_ADDR)\n\t\tidx += 1;\n\n\treturn idx;\n}\n\n \nstatic int __perf_evsel__calc_is_pos(u64 sample_type)\n{\n\tint idx = 1;\n\n\tif (sample_type & PERF_SAMPLE_IDENTIFIER)\n\t\treturn 1;\n\n\tif (!(sample_type & PERF_SAMPLE_ID))\n\t\treturn -1;\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tidx += 1;\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tidx += 1;\n\n\treturn idx;\n}\n\nvoid evsel__calc_id_pos(struct evsel *evsel)\n{\n\tevsel->id_pos = __perf_evsel__calc_id_pos(evsel->core.attr.sample_type);\n\tevsel->is_pos = __perf_evsel__calc_is_pos(evsel->core.attr.sample_type);\n}\n\nvoid __evsel__set_sample_bit(struct evsel *evsel,\n\t\t\t\t  enum perf_event_sample_format bit)\n{\n\tif (!(evsel->core.attr.sample_type & bit)) {\n\t\tevsel->core.attr.sample_type |= bit;\n\t\tevsel->sample_size += sizeof(u64);\n\t\tevsel__calc_id_pos(evsel);\n\t}\n}\n\nvoid __evsel__reset_sample_bit(struct evsel *evsel,\n\t\t\t\t    enum perf_event_sample_format bit)\n{\n\tif (evsel->core.attr.sample_type & bit) {\n\t\tevsel->core.attr.sample_type &= ~bit;\n\t\tevsel->sample_size -= sizeof(u64);\n\t\tevsel__calc_id_pos(evsel);\n\t}\n}\n\nvoid evsel__set_sample_id(struct evsel *evsel,\n\t\t\t       bool can_sample_identifier)\n{\n\tif (can_sample_identifier) {\n\t\tevsel__reset_sample_bit(evsel, ID);\n\t\tevsel__set_sample_bit(evsel, IDENTIFIER);\n\t} else {\n\t\tevsel__set_sample_bit(evsel, ID);\n\t}\n\tevsel->core.attr.read_format |= PERF_FORMAT_ID;\n}\n\n \nbool evsel__is_function_event(struct evsel *evsel)\n{\n#define FUNCTION_EVENT \"ftrace:function\"\n\n\treturn evsel->name &&\n\t       !strncmp(FUNCTION_EVENT, evsel->name, sizeof(FUNCTION_EVENT));\n\n#undef FUNCTION_EVENT\n}\n\nvoid evsel__init(struct evsel *evsel,\n\t\t struct perf_event_attr *attr, int idx)\n{\n\tperf_evsel__init(&evsel->core, attr, idx);\n\tevsel->tracking\t   = !idx;\n\tevsel->unit\t   = strdup(\"\");\n\tevsel->scale\t   = 1.0;\n\tevsel->max_events  = ULONG_MAX;\n\tevsel->evlist\t   = NULL;\n\tevsel->bpf_obj\t   = NULL;\n\tevsel->bpf_fd\t   = -1;\n\tINIT_LIST_HEAD(&evsel->config_terms);\n\tINIT_LIST_HEAD(&evsel->bpf_counter_list);\n\tINIT_LIST_HEAD(&evsel->bpf_filters);\n\tperf_evsel__object.init(evsel);\n\tevsel->sample_size = __evsel__sample_size(attr->sample_type);\n\tevsel__calc_id_pos(evsel);\n\tevsel->cmdline_group_boundary = false;\n\tevsel->metric_events = NULL;\n\tevsel->per_pkg_mask  = NULL;\n\tevsel->collect_stat  = false;\n\tevsel->pmu_name      = NULL;\n\tevsel->group_pmu_name = NULL;\n\tevsel->skippable     = false;\n}\n\nstruct evsel *evsel__new_idx(struct perf_event_attr *attr, int idx)\n{\n\tstruct evsel *evsel = zalloc(perf_evsel__object.size);\n\n\tif (!evsel)\n\t\treturn NULL;\n\tevsel__init(evsel, attr, idx);\n\n\tif (evsel__is_bpf_output(evsel) && !attr->sample_type) {\n\t\tevsel->core.attr.sample_type = (PERF_SAMPLE_RAW | PERF_SAMPLE_TIME |\n\t\t\t\t\t    PERF_SAMPLE_CPU | PERF_SAMPLE_PERIOD),\n\t\tevsel->core.attr.sample_period = 1;\n\t}\n\n\tif (evsel__is_clock(evsel)) {\n\t\tfree((char *)evsel->unit);\n\t\tevsel->unit = strdup(\"msec\");\n\t\tevsel->scale = 1e-6;\n\t}\n\n\treturn evsel;\n}\n\nint copy_config_terms(struct list_head *dst, struct list_head *src)\n{\n\tstruct evsel_config_term *pos, *tmp;\n\n\tlist_for_each_entry(pos, src, list) {\n\t\ttmp = malloc(sizeof(*tmp));\n\t\tif (tmp == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\t*tmp = *pos;\n\t\tif (tmp->free_str) {\n\t\t\ttmp->val.str = strdup(pos->val.str);\n\t\t\tif (tmp->val.str == NULL) {\n\t\t\t\tfree(tmp);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\t\tlist_add_tail(&tmp->list, dst);\n\t}\n\treturn 0;\n}\n\nstatic int evsel__copy_config_terms(struct evsel *dst, struct evsel *src)\n{\n\treturn copy_config_terms(&dst->config_terms, &src->config_terms);\n}\n\n \nstruct evsel *evsel__clone(struct evsel *orig)\n{\n\tstruct evsel *evsel;\n\n\tBUG_ON(orig->core.fd);\n\tBUG_ON(orig->counts);\n\tBUG_ON(orig->priv);\n\tBUG_ON(orig->per_pkg_mask);\n\n\t \n\tif (orig->bpf_obj)\n\t\treturn NULL;\n\n\tevsel = evsel__new(&orig->core.attr);\n\tif (evsel == NULL)\n\t\treturn NULL;\n\n\tevsel->core.cpus = perf_cpu_map__get(orig->core.cpus);\n\tevsel->core.own_cpus = perf_cpu_map__get(orig->core.own_cpus);\n\tevsel->core.threads = perf_thread_map__get(orig->core.threads);\n\tevsel->core.nr_members = orig->core.nr_members;\n\tevsel->core.system_wide = orig->core.system_wide;\n\tevsel->core.requires_cpu = orig->core.requires_cpu;\n\tevsel->core.is_pmu_core = orig->core.is_pmu_core;\n\n\tif (orig->name) {\n\t\tevsel->name = strdup(orig->name);\n\t\tif (evsel->name == NULL)\n\t\t\tgoto out_err;\n\t}\n\tif (orig->group_name) {\n\t\tevsel->group_name = strdup(orig->group_name);\n\t\tif (evsel->group_name == NULL)\n\t\t\tgoto out_err;\n\t}\n\tif (orig->pmu_name) {\n\t\tevsel->pmu_name = strdup(orig->pmu_name);\n\t\tif (evsel->pmu_name == NULL)\n\t\t\tgoto out_err;\n\t}\n\tif (orig->group_pmu_name) {\n\t\tevsel->group_pmu_name = strdup(orig->group_pmu_name);\n\t\tif (evsel->group_pmu_name == NULL)\n\t\t\tgoto out_err;\n\t}\n\tif (orig->filter) {\n\t\tevsel->filter = strdup(orig->filter);\n\t\tif (evsel->filter == NULL)\n\t\t\tgoto out_err;\n\t}\n\tif (orig->metric_id) {\n\t\tevsel->metric_id = strdup(orig->metric_id);\n\t\tif (evsel->metric_id == NULL)\n\t\t\tgoto out_err;\n\t}\n\tevsel->cgrp = cgroup__get(orig->cgrp);\n#ifdef HAVE_LIBTRACEEVENT\n\tevsel->tp_format = orig->tp_format;\n#endif\n\tevsel->handler = orig->handler;\n\tevsel->core.leader = orig->core.leader;\n\n\tevsel->max_events = orig->max_events;\n\tevsel->tool_event = orig->tool_event;\n\tfree((char *)evsel->unit);\n\tevsel->unit = strdup(orig->unit);\n\tif (evsel->unit == NULL)\n\t\tgoto out_err;\n\n\tevsel->scale = orig->scale;\n\tevsel->snapshot = orig->snapshot;\n\tevsel->per_pkg = orig->per_pkg;\n\tevsel->percore = orig->percore;\n\tevsel->precise_max = orig->precise_max;\n\tevsel->is_libpfm_event = orig->is_libpfm_event;\n\n\tevsel->exclude_GH = orig->exclude_GH;\n\tevsel->sample_read = orig->sample_read;\n\tevsel->auto_merge_stats = orig->auto_merge_stats;\n\tevsel->collect_stat = orig->collect_stat;\n\tevsel->weak_group = orig->weak_group;\n\tevsel->use_config_name = orig->use_config_name;\n\tevsel->pmu = orig->pmu;\n\n\tif (evsel__copy_config_terms(evsel, orig) < 0)\n\t\tgoto out_err;\n\n\treturn evsel;\n\nout_err:\n\tevsel__delete(evsel);\n\treturn NULL;\n}\n\n \n#ifdef HAVE_LIBTRACEEVENT\nstruct evsel *evsel__newtp_idx(const char *sys, const char *name, int idx)\n{\n\tstruct evsel *evsel = zalloc(perf_evsel__object.size);\n\tint err = -ENOMEM;\n\n\tif (evsel == NULL) {\n\t\tgoto out_err;\n\t} else {\n\t\tstruct perf_event_attr attr = {\n\t\t\t.type\t       = PERF_TYPE_TRACEPOINT,\n\t\t\t.sample_type   = (PERF_SAMPLE_RAW | PERF_SAMPLE_TIME |\n\t\t\t\t\t  PERF_SAMPLE_CPU | PERF_SAMPLE_PERIOD),\n\t\t};\n\n\t\tif (asprintf(&evsel->name, \"%s:%s\", sys, name) < 0)\n\t\t\tgoto out_free;\n\n\t\tevsel->tp_format = trace_event__tp_format(sys, name);\n\t\tif (IS_ERR(evsel->tp_format)) {\n\t\t\terr = PTR_ERR(evsel->tp_format);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tevent_attr_init(&attr);\n\t\tattr.config = evsel->tp_format->id;\n\t\tattr.sample_period = 1;\n\t\tevsel__init(evsel, &attr, idx);\n\t}\n\n\treturn evsel;\n\nout_free:\n\tzfree(&evsel->name);\n\tfree(evsel);\nout_err:\n\treturn ERR_PTR(err);\n}\n#endif\n\nconst char *const evsel__hw_names[PERF_COUNT_HW_MAX] = {\n\t\"cycles\",\n\t\"instructions\",\n\t\"cache-references\",\n\t\"cache-misses\",\n\t\"branches\",\n\t\"branch-misses\",\n\t\"bus-cycles\",\n\t\"stalled-cycles-frontend\",\n\t\"stalled-cycles-backend\",\n\t\"ref-cycles\",\n};\n\nchar *evsel__bpf_counter_events;\n\nbool evsel__match_bpf_counter_events(const char *name)\n{\n\tint name_len;\n\tbool match;\n\tchar *ptr;\n\n\tif (!evsel__bpf_counter_events)\n\t\treturn false;\n\n\tptr = strstr(evsel__bpf_counter_events, name);\n\tname_len = strlen(name);\n\n\t \n\tmatch = (ptr != NULL) &&\n\t\t((ptr == evsel__bpf_counter_events) || (*(ptr - 1) == ',')) &&\n\t\t((*(ptr + name_len) == ',') || (*(ptr + name_len) == '\\0'));\n\n\treturn match;\n}\n\nstatic const char *__evsel__hw_name(u64 config)\n{\n\tif (config < PERF_COUNT_HW_MAX && evsel__hw_names[config])\n\t\treturn evsel__hw_names[config];\n\n\treturn \"unknown-hardware\";\n}\n\nstatic int evsel__add_modifiers(struct evsel *evsel, char *bf, size_t size)\n{\n\tint colon = 0, r = 0;\n\tstruct perf_event_attr *attr = &evsel->core.attr;\n\tbool exclude_guest_default = false;\n\n#define MOD_PRINT(context, mod)\tdo {\t\t\t\t\t\\\n\t\tif (!attr->exclude_##context) {\t\t\t\t\\\n\t\t\tif (!colon) colon = ++r;\t\t\t\\\n\t\t\tr += scnprintf(bf + r, size - r, \"%c\", mod);\t\\\n\t\t} } while(0)\n\n\tif (attr->exclude_kernel || attr->exclude_user || attr->exclude_hv) {\n\t\tMOD_PRINT(kernel, 'k');\n\t\tMOD_PRINT(user, 'u');\n\t\tMOD_PRINT(hv, 'h');\n\t\texclude_guest_default = true;\n\t}\n\n\tif (attr->precise_ip) {\n\t\tif (!colon)\n\t\t\tcolon = ++r;\n\t\tr += scnprintf(bf + r, size - r, \"%.*s\", attr->precise_ip, \"ppp\");\n\t\texclude_guest_default = true;\n\t}\n\n\tif (attr->exclude_host || attr->exclude_guest == exclude_guest_default) {\n\t\tMOD_PRINT(host, 'H');\n\t\tMOD_PRINT(guest, 'G');\n\t}\n#undef MOD_PRINT\n\tif (colon)\n\t\tbf[colon - 1] = ':';\n\treturn r;\n}\n\nint __weak arch_evsel__hw_name(struct evsel *evsel, char *bf, size_t size)\n{\n\treturn scnprintf(bf, size, \"%s\", __evsel__hw_name(evsel->core.attr.config));\n}\n\nstatic int evsel__hw_name(struct evsel *evsel, char *bf, size_t size)\n{\n\tint r = arch_evsel__hw_name(evsel, bf, size);\n\treturn r + evsel__add_modifiers(evsel, bf + r, size - r);\n}\n\nconst char *const evsel__sw_names[PERF_COUNT_SW_MAX] = {\n\t\"cpu-clock\",\n\t\"task-clock\",\n\t\"page-faults\",\n\t\"context-switches\",\n\t\"cpu-migrations\",\n\t\"minor-faults\",\n\t\"major-faults\",\n\t\"alignment-faults\",\n\t\"emulation-faults\",\n\t\"dummy\",\n};\n\nstatic const char *__evsel__sw_name(u64 config)\n{\n\tif (config < PERF_COUNT_SW_MAX && evsel__sw_names[config])\n\t\treturn evsel__sw_names[config];\n\treturn \"unknown-software\";\n}\n\nstatic int evsel__sw_name(struct evsel *evsel, char *bf, size_t size)\n{\n\tint r = scnprintf(bf, size, \"%s\", __evsel__sw_name(evsel->core.attr.config));\n\treturn r + evsel__add_modifiers(evsel, bf + r, size - r);\n}\n\nstatic int evsel__tool_name(enum perf_tool_event ev, char *bf, size_t size)\n{\n\treturn scnprintf(bf, size, \"%s\", perf_tool_event__to_str(ev));\n}\n\nstatic int __evsel__bp_name(char *bf, size_t size, u64 addr, u64 type)\n{\n\tint r;\n\n\tr = scnprintf(bf, size, \"mem:0x%\" PRIx64 \":\", addr);\n\n\tif (type & HW_BREAKPOINT_R)\n\t\tr += scnprintf(bf + r, size - r, \"r\");\n\n\tif (type & HW_BREAKPOINT_W)\n\t\tr += scnprintf(bf + r, size - r, \"w\");\n\n\tif (type & HW_BREAKPOINT_X)\n\t\tr += scnprintf(bf + r, size - r, \"x\");\n\n\treturn r;\n}\n\nstatic int evsel__bp_name(struct evsel *evsel, char *bf, size_t size)\n{\n\tstruct perf_event_attr *attr = &evsel->core.attr;\n\tint r = __evsel__bp_name(bf, size, attr->bp_addr, attr->bp_type);\n\treturn r + evsel__add_modifiers(evsel, bf + r, size - r);\n}\n\nconst char *const evsel__hw_cache[PERF_COUNT_HW_CACHE_MAX][EVSEL__MAX_ALIASES] = {\n { \"L1-dcache\",\t\"l1-d\",\t\t\"l1d\",\t\t\"L1-data\",\t\t},\n { \"L1-icache\",\t\"l1-i\",\t\t\"l1i\",\t\t\"L1-instruction\",\t},\n { \"LLC\",\t\"L2\",\t\t\t\t\t\t\t},\n { \"dTLB\",\t\"d-tlb\",\t\"Data-TLB\",\t\t\t\t},\n { \"iTLB\",\t\"i-tlb\",\t\"Instruction-TLB\",\t\t\t},\n { \"branch\",\t\"branches\",\t\"bpu\",\t\t\"btb\",\t\t\"bpc\",\t},\n { \"node\",\t\t\t\t\t\t\t\t},\n};\n\nconst char *const evsel__hw_cache_op[PERF_COUNT_HW_CACHE_OP_MAX][EVSEL__MAX_ALIASES] = {\n { \"load\",\t\"loads\",\t\"read\",\t\t\t\t\t},\n { \"store\",\t\"stores\",\t\"write\",\t\t\t\t},\n { \"prefetch\",\t\"prefetches\",\t\"speculative-read\", \"speculative-load\",\t},\n};\n\nconst char *const evsel__hw_cache_result[PERF_COUNT_HW_CACHE_RESULT_MAX][EVSEL__MAX_ALIASES] = {\n { \"refs\",\t\"Reference\",\t\"ops\",\t\t\"access\",\t\t},\n { \"misses\",\t\"miss\",\t\t\t\t\t\t\t},\n};\n\n#define C(x)\t\tPERF_COUNT_HW_CACHE_##x\n#define CACHE_READ\t(1 << C(OP_READ))\n#define CACHE_WRITE\t(1 << C(OP_WRITE))\n#define CACHE_PREFETCH\t(1 << C(OP_PREFETCH))\n#define COP(x)\t\t(1 << x)\n\n \nstatic const unsigned long evsel__hw_cache_stat[C(MAX)] = {\n [C(L1D)]\t= (CACHE_READ | CACHE_WRITE | CACHE_PREFETCH),\n [C(L1I)]\t= (CACHE_READ | CACHE_PREFETCH),\n [C(LL)]\t= (CACHE_READ | CACHE_WRITE | CACHE_PREFETCH),\n [C(DTLB)]\t= (CACHE_READ | CACHE_WRITE | CACHE_PREFETCH),\n [C(ITLB)]\t= (CACHE_READ),\n [C(BPU)]\t= (CACHE_READ),\n [C(NODE)]\t= (CACHE_READ | CACHE_WRITE | CACHE_PREFETCH),\n};\n\nbool evsel__is_cache_op_valid(u8 type, u8 op)\n{\n\tif (evsel__hw_cache_stat[type] & COP(op))\n\t\treturn true;\t \n\telse\n\t\treturn false;\t \n}\n\nint __evsel__hw_cache_type_op_res_name(u8 type, u8 op, u8 result, char *bf, size_t size)\n{\n\tif (result) {\n\t\treturn scnprintf(bf, size, \"%s-%s-%s\", evsel__hw_cache[type][0],\n\t\t\t\t evsel__hw_cache_op[op][0],\n\t\t\t\t evsel__hw_cache_result[result][0]);\n\t}\n\n\treturn scnprintf(bf, size, \"%s-%s\", evsel__hw_cache[type][0],\n\t\t\t evsel__hw_cache_op[op][1]);\n}\n\nstatic int __evsel__hw_cache_name(u64 config, char *bf, size_t size)\n{\n\tu8 op, result, type = (config >>  0) & 0xff;\n\tconst char *err = \"unknown-ext-hardware-cache-type\";\n\n\tif (type >= PERF_COUNT_HW_CACHE_MAX)\n\t\tgoto out_err;\n\n\top = (config >>  8) & 0xff;\n\terr = \"unknown-ext-hardware-cache-op\";\n\tif (op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\tgoto out_err;\n\n\tresult = (config >> 16) & 0xff;\n\terr = \"unknown-ext-hardware-cache-result\";\n\tif (result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\tgoto out_err;\n\n\terr = \"invalid-cache\";\n\tif (!evsel__is_cache_op_valid(type, op))\n\t\tgoto out_err;\n\n\treturn __evsel__hw_cache_type_op_res_name(type, op, result, bf, size);\nout_err:\n\treturn scnprintf(bf, size, \"%s\", err);\n}\n\nstatic int evsel__hw_cache_name(struct evsel *evsel, char *bf, size_t size)\n{\n\tint ret = __evsel__hw_cache_name(evsel->core.attr.config, bf, size);\n\treturn ret + evsel__add_modifiers(evsel, bf + ret, size - ret);\n}\n\nstatic int evsel__raw_name(struct evsel *evsel, char *bf, size_t size)\n{\n\tint ret = scnprintf(bf, size, \"raw 0x%\" PRIx64, evsel->core.attr.config);\n\treturn ret + evsel__add_modifiers(evsel, bf + ret, size - ret);\n}\n\nconst char *evsel__name(struct evsel *evsel)\n{\n\tchar bf[128];\n\n\tif (!evsel)\n\t\tgoto out_unknown;\n\n\tif (evsel->name)\n\t\treturn evsel->name;\n\n\tswitch (evsel->core.attr.type) {\n\tcase PERF_TYPE_RAW:\n\t\tevsel__raw_name(evsel, bf, sizeof(bf));\n\t\tbreak;\n\n\tcase PERF_TYPE_HARDWARE:\n\t\tevsel__hw_name(evsel, bf, sizeof(bf));\n\t\tbreak;\n\n\tcase PERF_TYPE_HW_CACHE:\n\t\tevsel__hw_cache_name(evsel, bf, sizeof(bf));\n\t\tbreak;\n\n\tcase PERF_TYPE_SOFTWARE:\n\t\tif (evsel__is_tool(evsel))\n\t\t\tevsel__tool_name(evsel->tool_event, bf, sizeof(bf));\n\t\telse\n\t\t\tevsel__sw_name(evsel, bf, sizeof(bf));\n\t\tbreak;\n\n\tcase PERF_TYPE_TRACEPOINT:\n\t\tscnprintf(bf, sizeof(bf), \"%s\", \"unknown tracepoint\");\n\t\tbreak;\n\n\tcase PERF_TYPE_BREAKPOINT:\n\t\tevsel__bp_name(evsel, bf, sizeof(bf));\n\t\tbreak;\n\n\tdefault:\n\t\tscnprintf(bf, sizeof(bf), \"unknown attr type: %d\",\n\t\t\t  evsel->core.attr.type);\n\t\tbreak;\n\t}\n\n\tevsel->name = strdup(bf);\n\n\tif (evsel->name)\n\t\treturn evsel->name;\nout_unknown:\n\treturn \"unknown\";\n}\n\nbool evsel__name_is(struct evsel *evsel, const char *name)\n{\n\treturn !strcmp(evsel__name(evsel), name);\n}\n\nconst char *evsel__metric_id(const struct evsel *evsel)\n{\n\tif (evsel->metric_id)\n\t\treturn evsel->metric_id;\n\n\tif (evsel__is_tool(evsel))\n\t\treturn perf_tool_event__to_str(evsel->tool_event);\n\n\treturn \"unknown\";\n}\n\nconst char *evsel__group_name(struct evsel *evsel)\n{\n\treturn evsel->group_name ?: \"anon group\";\n}\n\n \nint evsel__group_desc(struct evsel *evsel, char *buf, size_t size)\n{\n\tint ret = 0;\n\tstruct evsel *pos;\n\tconst char *group_name = evsel__group_name(evsel);\n\n\tif (!evsel->forced_leader)\n\t\tret = scnprintf(buf, size, \"%s { \", group_name);\n\n\tret += scnprintf(buf + ret, size - ret, \"%s\", evsel__name(evsel));\n\n\tfor_each_group_member(pos, evsel)\n\t\tret += scnprintf(buf + ret, size - ret, \", %s\", evsel__name(pos));\n\n\tif (!evsel->forced_leader)\n\t\tret += scnprintf(buf + ret, size - ret, \" }\");\n\n\treturn ret;\n}\n\nstatic void __evsel__config_callchain(struct evsel *evsel, struct record_opts *opts,\n\t\t\t\t      struct callchain_param *param)\n{\n\tbool function = evsel__is_function_event(evsel);\n\tstruct perf_event_attr *attr = &evsel->core.attr;\n\tconst char *arch = perf_env__arch(evsel__env(evsel));\n\n\tevsel__set_sample_bit(evsel, CALLCHAIN);\n\n\tattr->sample_max_stack = param->max_stack;\n\n\tif (opts->kernel_callchains)\n\t\tattr->exclude_callchain_user = 1;\n\tif (opts->user_callchains)\n\t\tattr->exclude_callchain_kernel = 1;\n\tif (param->record_mode == CALLCHAIN_LBR) {\n\t\tif (!opts->branch_stack) {\n\t\t\tif (attr->exclude_user) {\n\t\t\t\tpr_warning(\"LBR callstack option is only available \"\n\t\t\t\t\t   \"to get user callchain information. \"\n\t\t\t\t\t   \"Falling back to framepointers.\\n\");\n\t\t\t} else {\n\t\t\t\tevsel__set_sample_bit(evsel, BRANCH_STACK);\n\t\t\t\tattr->branch_sample_type = PERF_SAMPLE_BRANCH_USER |\n\t\t\t\t\t\t\tPERF_SAMPLE_BRANCH_CALL_STACK |\n\t\t\t\t\t\t\tPERF_SAMPLE_BRANCH_NO_CYCLES |\n\t\t\t\t\t\t\tPERF_SAMPLE_BRANCH_NO_FLAGS |\n\t\t\t\t\t\t\tPERF_SAMPLE_BRANCH_HW_INDEX;\n\t\t\t}\n\t\t} else\n\t\t\t pr_warning(\"Cannot use LBR callstack with branch stack. \"\n\t\t\t\t    \"Falling back to framepointers.\\n\");\n\t}\n\n\tif (param->record_mode == CALLCHAIN_DWARF) {\n\t\tif (!function) {\n\t\t\tevsel__set_sample_bit(evsel, REGS_USER);\n\t\t\tevsel__set_sample_bit(evsel, STACK_USER);\n\t\t\tif (opts->sample_user_regs &&\n\t\t\t    DWARF_MINIMAL_REGS(arch) != arch__user_reg_mask()) {\n\t\t\t\tattr->sample_regs_user |= DWARF_MINIMAL_REGS(arch);\n\t\t\t\tpr_warning(\"WARNING: The use of --call-graph=dwarf may require all the user registers, \"\n\t\t\t\t\t   \"specifying a subset with --user-regs may render DWARF unwinding unreliable, \"\n\t\t\t\t\t   \"so the minimal registers set (IP, SP) is explicitly forced.\\n\");\n\t\t\t} else {\n\t\t\t\tattr->sample_regs_user |= arch__user_reg_mask();\n\t\t\t}\n\t\t\tattr->sample_stack_user = param->dump_size;\n\t\t\tattr->exclude_callchain_user = 1;\n\t\t} else {\n\t\t\tpr_info(\"Cannot use DWARF unwind for function trace event,\"\n\t\t\t\t\" falling back to framepointers.\\n\");\n\t\t}\n\t}\n\n\tif (function) {\n\t\tpr_info(\"Disabling user space callchains for function trace event.\\n\");\n\t\tattr->exclude_callchain_user = 1;\n\t}\n}\n\nvoid evsel__config_callchain(struct evsel *evsel, struct record_opts *opts,\n\t\t\t     struct callchain_param *param)\n{\n\tif (param->enabled)\n\t\treturn __evsel__config_callchain(evsel, opts, param);\n}\n\nstatic void evsel__reset_callgraph(struct evsel *evsel, struct callchain_param *param)\n{\n\tstruct perf_event_attr *attr = &evsel->core.attr;\n\n\tevsel__reset_sample_bit(evsel, CALLCHAIN);\n\tif (param->record_mode == CALLCHAIN_LBR) {\n\t\tevsel__reset_sample_bit(evsel, BRANCH_STACK);\n\t\tattr->branch_sample_type &= ~(PERF_SAMPLE_BRANCH_USER |\n\t\t\t\t\t      PERF_SAMPLE_BRANCH_CALL_STACK |\n\t\t\t\t\t      PERF_SAMPLE_BRANCH_HW_INDEX);\n\t}\n\tif (param->record_mode == CALLCHAIN_DWARF) {\n\t\tevsel__reset_sample_bit(evsel, REGS_USER);\n\t\tevsel__reset_sample_bit(evsel, STACK_USER);\n\t}\n}\n\nstatic void evsel__apply_config_terms(struct evsel *evsel,\n\t\t\t\t      struct record_opts *opts, bool track)\n{\n\tstruct evsel_config_term *term;\n\tstruct list_head *config_terms = &evsel->config_terms;\n\tstruct perf_event_attr *attr = &evsel->core.attr;\n\t \n\tstruct callchain_param param = {\n\t\t.record_mode = callchain_param.record_mode,\n\t};\n\tu32 dump_size = 0;\n\tint max_stack = 0;\n\tconst char *callgraph_buf = NULL;\n\n\tlist_for_each_entry(term, config_terms, list) {\n\t\tswitch (term->type) {\n\t\tcase EVSEL__CONFIG_TERM_PERIOD:\n\t\t\tif (!(term->weak && opts->user_interval != ULLONG_MAX)) {\n\t\t\t\tattr->sample_period = term->val.period;\n\t\t\t\tattr->freq = 0;\n\t\t\t\tevsel__reset_sample_bit(evsel, PERIOD);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_FREQ:\n\t\t\tif (!(term->weak && opts->user_freq != UINT_MAX)) {\n\t\t\t\tattr->sample_freq = term->val.freq;\n\t\t\t\tattr->freq = 1;\n\t\t\t\tevsel__set_sample_bit(evsel, PERIOD);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_TIME:\n\t\t\tif (term->val.time)\n\t\t\t\tevsel__set_sample_bit(evsel, TIME);\n\t\t\telse\n\t\t\t\tevsel__reset_sample_bit(evsel, TIME);\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_CALLGRAPH:\n\t\t\tcallgraph_buf = term->val.str;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_BRANCH:\n\t\t\tif (term->val.str && strcmp(term->val.str, \"no\")) {\n\t\t\t\tevsel__set_sample_bit(evsel, BRANCH_STACK);\n\t\t\t\tparse_branch_str(term->val.str,\n\t\t\t\t\t\t &attr->branch_sample_type);\n\t\t\t} else\n\t\t\t\tevsel__reset_sample_bit(evsel, BRANCH_STACK);\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_STACK_USER:\n\t\t\tdump_size = term->val.stack_user;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_MAX_STACK:\n\t\t\tmax_stack = term->val.max_stack;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_MAX_EVENTS:\n\t\t\tevsel->max_events = term->val.max_events;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_INHERIT:\n\t\t\t \n\t\t\tattr->inherit = term->val.inherit ? 1 : 0;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_OVERWRITE:\n\t\t\tattr->write_backward = term->val.overwrite ? 1 : 0;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_DRV_CFG:\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_PERCORE:\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_AUX_OUTPUT:\n\t\t\tattr->aux_output = term->val.aux_output ? 1 : 0;\n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_AUX_SAMPLE_SIZE:\n\t\t\t \n\t\t\tbreak;\n\t\tcase EVSEL__CONFIG_TERM_CFG_CHG:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif ((callgraph_buf != NULL) || (dump_size > 0) || max_stack) {\n\t\tbool sample_address = false;\n\n\t\tif (max_stack) {\n\t\t\tparam.max_stack = max_stack;\n\t\t\tif (callgraph_buf == NULL)\n\t\t\t\tcallgraph_buf = \"fp\";\n\t\t}\n\n\t\t \n\t\tif (callgraph_buf != NULL) {\n\t\t\tif (!strcmp(callgraph_buf, \"no\")) {\n\t\t\t\tparam.enabled = false;\n\t\t\t\tparam.record_mode = CALLCHAIN_NONE;\n\t\t\t} else {\n\t\t\t\tparam.enabled = true;\n\t\t\t\tif (parse_callchain_record(callgraph_buf, &param)) {\n\t\t\t\t\tpr_err(\"per-event callgraph setting for %s failed. \"\n\t\t\t\t\t       \"Apply callgraph global setting for it\\n\",\n\t\t\t\t\t       evsel->name);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tif (param.record_mode == CALLCHAIN_DWARF)\n\t\t\t\t\tsample_address = true;\n\t\t\t}\n\t\t}\n\t\tif (dump_size > 0) {\n\t\t\tdump_size = round_up(dump_size, sizeof(u64));\n\t\t\tparam.dump_size = dump_size;\n\t\t}\n\n\t\t \n\t\tif (callchain_param.enabled)\n\t\t\tevsel__reset_callgraph(evsel, &callchain_param);\n\n\t\t \n\t\tif (param.enabled) {\n\t\t\tif (sample_address) {\n\t\t\t\tevsel__set_sample_bit(evsel, ADDR);\n\t\t\t\tevsel__set_sample_bit(evsel, DATA_SRC);\n\t\t\t\tevsel->core.attr.mmap_data = track;\n\t\t\t}\n\t\t\tevsel__config_callchain(evsel, opts, &param);\n\t\t}\n\t}\n}\n\nstruct evsel_config_term *__evsel__get_config_term(struct evsel *evsel, enum evsel_term_type type)\n{\n\tstruct evsel_config_term *term, *found_term = NULL;\n\n\tlist_for_each_entry(term, &evsel->config_terms, list) {\n\t\tif (term->type == type)\n\t\t\tfound_term = term;\n\t}\n\n\treturn found_term;\n}\n\nvoid __weak arch_evsel__set_sample_weight(struct evsel *evsel)\n{\n\tevsel__set_sample_bit(evsel, WEIGHT);\n}\n\nvoid __weak arch__post_evsel_config(struct evsel *evsel __maybe_unused,\n\t\t\t\t    struct perf_event_attr *attr __maybe_unused)\n{\n}\n\nstatic void evsel__set_default_freq_period(struct record_opts *opts,\n\t\t\t\t\t   struct perf_event_attr *attr)\n{\n\tif (opts->freq) {\n\t\tattr->freq = 1;\n\t\tattr->sample_freq = opts->freq;\n\t} else {\n\t\tattr->sample_period = opts->default_interval;\n\t}\n}\n\nstatic bool evsel__is_offcpu_event(struct evsel *evsel)\n{\n\treturn evsel__is_bpf_output(evsel) && evsel__name_is(evsel, OFFCPU_EVENT);\n}\n\n \nvoid evsel__config(struct evsel *evsel, struct record_opts *opts,\n\t\t   struct callchain_param *callchain)\n{\n\tstruct evsel *leader = evsel__leader(evsel);\n\tstruct perf_event_attr *attr = &evsel->core.attr;\n\tint track = evsel->tracking;\n\tbool per_cpu = opts->target.default_per_cpu && !opts->target.per_thread;\n\n\tattr->sample_id_all = perf_missing_features.sample_id_all ? 0 : 1;\n\tattr->inherit\t    = !opts->no_inherit;\n\tattr->write_backward = opts->overwrite ? 1 : 0;\n\tattr->read_format   = PERF_FORMAT_LOST;\n\n\tevsel__set_sample_bit(evsel, IP);\n\tevsel__set_sample_bit(evsel, TID);\n\n\tif (evsel->sample_read) {\n\t\tevsel__set_sample_bit(evsel, READ);\n\n\t\t \n\t\tevsel__set_sample_id(evsel, false);\n\n\t\t \n\t\tif (leader->core.nr_members > 1) {\n\t\t\tattr->read_format |= PERF_FORMAT_GROUP;\n\t\t\tattr->inherit = 0;\n\t\t}\n\t}\n\n\t \n\tif ((evsel->is_libpfm_event && !attr->sample_period) ||\n\t    (!evsel->is_libpfm_event && (!attr->sample_period ||\n\t\t\t\t\t opts->user_freq != UINT_MAX ||\n\t\t\t\t\t opts->user_interval != ULLONG_MAX)))\n\t\tevsel__set_default_freq_period(opts, attr);\n\n\t \n\tif (attr->freq)\n\t\tevsel__set_sample_bit(evsel, PERIOD);\n\n\tif (opts->no_samples)\n\t\tattr->sample_freq = 0;\n\n\tif (opts->inherit_stat) {\n\t\tevsel->core.attr.read_format |=\n\t\t\tPERF_FORMAT_TOTAL_TIME_ENABLED |\n\t\t\tPERF_FORMAT_TOTAL_TIME_RUNNING |\n\t\t\tPERF_FORMAT_ID;\n\t\tattr->inherit_stat = 1;\n\t}\n\n\tif (opts->sample_address) {\n\t\tevsel__set_sample_bit(evsel, ADDR);\n\t\tattr->mmap_data = track;\n\t}\n\n\t \n\tif (evsel__is_function_event(evsel))\n\t\tevsel->core.attr.exclude_callchain_user = 1;\n\n\tif (callchain && callchain->enabled && !evsel->no_aux_samples)\n\t\tevsel__config_callchain(evsel, opts, callchain);\n\n\tif (opts->sample_intr_regs && !evsel->no_aux_samples &&\n\t    !evsel__is_dummy_event(evsel)) {\n\t\tattr->sample_regs_intr = opts->sample_intr_regs;\n\t\tevsel__set_sample_bit(evsel, REGS_INTR);\n\t}\n\n\tif (opts->sample_user_regs && !evsel->no_aux_samples &&\n\t    !evsel__is_dummy_event(evsel)) {\n\t\tattr->sample_regs_user |= opts->sample_user_regs;\n\t\tevsel__set_sample_bit(evsel, REGS_USER);\n\t}\n\n\tif (target__has_cpu(&opts->target) || opts->sample_cpu)\n\t\tevsel__set_sample_bit(evsel, CPU);\n\n\t \n\tif (opts->sample_time &&\n\t    (!perf_missing_features.sample_id_all &&\n\t    (!opts->no_inherit || target__has_cpu(&opts->target) || per_cpu ||\n\t     opts->sample_time_set)))\n\t\tevsel__set_sample_bit(evsel, TIME);\n\n\tif (opts->raw_samples && !evsel->no_aux_samples) {\n\t\tevsel__set_sample_bit(evsel, TIME);\n\t\tevsel__set_sample_bit(evsel, RAW);\n\t\tevsel__set_sample_bit(evsel, CPU);\n\t}\n\n\tif (opts->sample_address)\n\t\tevsel__set_sample_bit(evsel, DATA_SRC);\n\n\tif (opts->sample_phys_addr)\n\t\tevsel__set_sample_bit(evsel, PHYS_ADDR);\n\n\tif (opts->no_buffering) {\n\t\tattr->watermark = 0;\n\t\tattr->wakeup_events = 1;\n\t}\n\tif (opts->branch_stack && !evsel->no_aux_samples) {\n\t\tevsel__set_sample_bit(evsel, BRANCH_STACK);\n\t\tattr->branch_sample_type = opts->branch_stack;\n\t}\n\n\tif (opts->sample_weight)\n\t\tarch_evsel__set_sample_weight(evsel);\n\n\tattr->task     = track;\n\tattr->mmap     = track;\n\tattr->mmap2    = track && !perf_missing_features.mmap2;\n\tattr->comm     = track;\n\tattr->build_id = track && opts->build_id;\n\n\t \n\tif (!opts->text_poke)\n\t\tattr->ksymbol = track && !perf_missing_features.ksymbol;\n\tattr->bpf_event = track && !opts->no_bpf_event && !perf_missing_features.bpf;\n\n\tif (opts->record_namespaces)\n\t\tattr->namespaces  = track;\n\n\tif (opts->record_cgroup) {\n\t\tattr->cgroup = track && !perf_missing_features.cgroup;\n\t\tevsel__set_sample_bit(evsel, CGROUP);\n\t}\n\n\tif (opts->sample_data_page_size)\n\t\tevsel__set_sample_bit(evsel, DATA_PAGE_SIZE);\n\n\tif (opts->sample_code_page_size)\n\t\tevsel__set_sample_bit(evsel, CODE_PAGE_SIZE);\n\n\tif (opts->record_switch_events)\n\t\tattr->context_switch = track;\n\n\tif (opts->sample_transaction)\n\t\tevsel__set_sample_bit(evsel, TRANSACTION);\n\n\tif (opts->running_time) {\n\t\tevsel->core.attr.read_format |=\n\t\t\tPERF_FORMAT_TOTAL_TIME_ENABLED |\n\t\t\tPERF_FORMAT_TOTAL_TIME_RUNNING;\n\t}\n\n\t \n\tif (evsel__is_group_leader(evsel))\n\t\tattr->disabled = 1;\n\n\t \n\tif (target__none(&opts->target) && evsel__is_group_leader(evsel) &&\n\t    !opts->target.initial_delay)\n\t\tattr->enable_on_exec = 1;\n\n\tif (evsel->immediate) {\n\t\tattr->disabled = 0;\n\t\tattr->enable_on_exec = 0;\n\t}\n\n\tclockid = opts->clockid;\n\tif (opts->use_clockid) {\n\t\tattr->use_clockid = 1;\n\t\tattr->clockid = opts->clockid;\n\t}\n\n\tif (evsel->precise_max)\n\t\tattr->precise_ip = 3;\n\n\tif (opts->all_user) {\n\t\tattr->exclude_kernel = 1;\n\t\tattr->exclude_user   = 0;\n\t}\n\n\tif (opts->all_kernel) {\n\t\tattr->exclude_kernel = 0;\n\t\tattr->exclude_user   = 1;\n\t}\n\n\tif (evsel->core.own_cpus || evsel->unit)\n\t\tevsel->core.attr.read_format |= PERF_FORMAT_ID;\n\n\t \n\tevsel__apply_config_terms(evsel, opts, track);\n\n\tevsel->ignore_missing_thread = opts->ignore_missing_thread;\n\n\t \n\tif (opts->period_set) {\n\t\tif (opts->period)\n\t\t\tevsel__set_sample_bit(evsel, PERIOD);\n\t\telse\n\t\t\tevsel__reset_sample_bit(evsel, PERIOD);\n\t}\n\n\t \n\tif (evsel__is_dummy_event(evsel))\n\t\tevsel__reset_sample_bit(evsel, BRANCH_STACK);\n\n\tif (evsel__is_offcpu_event(evsel))\n\t\tevsel->core.attr.sample_type &= OFFCPU_SAMPLE_TYPES;\n\n\tarch__post_evsel_config(evsel, attr);\n}\n\nint evsel__set_filter(struct evsel *evsel, const char *filter)\n{\n\tchar *new_filter = strdup(filter);\n\n\tif (new_filter != NULL) {\n\t\tfree(evsel->filter);\n\t\tevsel->filter = new_filter;\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n\nstatic int evsel__append_filter(struct evsel *evsel, const char *fmt, const char *filter)\n{\n\tchar *new_filter;\n\n\tif (evsel->filter == NULL)\n\t\treturn evsel__set_filter(evsel, filter);\n\n\tif (asprintf(&new_filter, fmt, evsel->filter, filter) > 0) {\n\t\tfree(evsel->filter);\n\t\tevsel->filter = new_filter;\n\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n\nint evsel__append_tp_filter(struct evsel *evsel, const char *filter)\n{\n\treturn evsel__append_filter(evsel, \"(%s) && (%s)\", filter);\n}\n\nint evsel__append_addr_filter(struct evsel *evsel, const char *filter)\n{\n\treturn evsel__append_filter(evsel, \"%s,%s\", filter);\n}\n\n \nint evsel__enable_cpu(struct evsel *evsel, int cpu_map_idx)\n{\n\treturn perf_evsel__enable_cpu(&evsel->core, cpu_map_idx);\n}\n\nint evsel__enable(struct evsel *evsel)\n{\n\tint err = perf_evsel__enable(&evsel->core);\n\n\tif (!err)\n\t\tevsel->disabled = false;\n\treturn err;\n}\n\n \nint evsel__disable_cpu(struct evsel *evsel, int cpu_map_idx)\n{\n\treturn perf_evsel__disable_cpu(&evsel->core, cpu_map_idx);\n}\n\nint evsel__disable(struct evsel *evsel)\n{\n\tint err = perf_evsel__disable(&evsel->core);\n\t \n\tif (!err)\n\t\tevsel->disabled = true;\n\n\treturn err;\n}\n\nvoid free_config_terms(struct list_head *config_terms)\n{\n\tstruct evsel_config_term *term, *h;\n\n\tlist_for_each_entry_safe(term, h, config_terms, list) {\n\t\tlist_del_init(&term->list);\n\t\tif (term->free_str)\n\t\t\tzfree(&term->val.str);\n\t\tfree(term);\n\t}\n}\n\nstatic void evsel__free_config_terms(struct evsel *evsel)\n{\n\tfree_config_terms(&evsel->config_terms);\n}\n\nvoid evsel__exit(struct evsel *evsel)\n{\n\tassert(list_empty(&evsel->core.node));\n\tassert(evsel->evlist == NULL);\n\tbpf_counter__destroy(evsel);\n\tperf_bpf_filter__destroy(evsel);\n\tevsel__free_counts(evsel);\n\tperf_evsel__free_fd(&evsel->core);\n\tperf_evsel__free_id(&evsel->core);\n\tevsel__free_config_terms(evsel);\n\tcgroup__put(evsel->cgrp);\n\tperf_cpu_map__put(evsel->core.cpus);\n\tperf_cpu_map__put(evsel->core.own_cpus);\n\tperf_thread_map__put(evsel->core.threads);\n\tzfree(&evsel->group_name);\n\tzfree(&evsel->name);\n\tzfree(&evsel->filter);\n\tzfree(&evsel->pmu_name);\n\tzfree(&evsel->group_pmu_name);\n\tzfree(&evsel->unit);\n\tzfree(&evsel->metric_id);\n\tevsel__zero_per_pkg(evsel);\n\thashmap__free(evsel->per_pkg_mask);\n\tevsel->per_pkg_mask = NULL;\n\tzfree(&evsel->metric_events);\n\tperf_evsel__object.fini(evsel);\n}\n\nvoid evsel__delete(struct evsel *evsel)\n{\n\tif (!evsel)\n\t\treturn;\n\n\tevsel__exit(evsel);\n\tfree(evsel);\n}\n\nvoid evsel__compute_deltas(struct evsel *evsel, int cpu_map_idx, int thread,\n\t\t\t   struct perf_counts_values *count)\n{\n\tstruct perf_counts_values tmp;\n\n\tif (!evsel->prev_raw_counts)\n\t\treturn;\n\n\ttmp = *perf_counts(evsel->prev_raw_counts, cpu_map_idx, thread);\n\t*perf_counts(evsel->prev_raw_counts, cpu_map_idx, thread) = *count;\n\n\tcount->val = count->val - tmp.val;\n\tcount->ena = count->ena - tmp.ena;\n\tcount->run = count->run - tmp.run;\n}\n\nstatic int evsel__read_one(struct evsel *evsel, int cpu_map_idx, int thread)\n{\n\tstruct perf_counts_values *count = perf_counts(evsel->counts, cpu_map_idx, thread);\n\n\treturn perf_evsel__read(&evsel->core, cpu_map_idx, thread, count);\n}\n\nstatic void evsel__set_count(struct evsel *counter, int cpu_map_idx, int thread,\n\t\t\t     u64 val, u64 ena, u64 run, u64 lost)\n{\n\tstruct perf_counts_values *count;\n\n\tcount = perf_counts(counter->counts, cpu_map_idx, thread);\n\n\tcount->val    = val;\n\tcount->ena    = ena;\n\tcount->run    = run;\n\tcount->lost   = lost;\n\n\tperf_counts__set_loaded(counter->counts, cpu_map_idx, thread, true);\n}\n\nstatic int evsel__process_group_data(struct evsel *leader, int cpu_map_idx, int thread, u64 *data)\n{\n\tu64 read_format = leader->core.attr.read_format;\n\tstruct sample_read_value *v;\n\tu64 nr, ena = 0, run = 0, lost = 0;\n\n\tnr = *data++;\n\n\tif (nr != (u64) leader->core.nr_members)\n\t\treturn -EINVAL;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\tena = *data++;\n\n\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\trun = *data++;\n\n\tv = (void *)data;\n\tsample_read_group__for_each(v, nr, read_format) {\n\t\tstruct evsel *counter;\n\n\t\tcounter = evlist__id2evsel(leader->evlist, v->id);\n\t\tif (!counter)\n\t\t\treturn -EINVAL;\n\n\t\tif (read_format & PERF_FORMAT_LOST)\n\t\t\tlost = v->lost;\n\n\t\tevsel__set_count(counter, cpu_map_idx, thread, v->value, ena, run, lost);\n\t}\n\n\treturn 0;\n}\n\nstatic int evsel__read_group(struct evsel *leader, int cpu_map_idx, int thread)\n{\n\tstruct perf_stat_evsel *ps = leader->stats;\n\tu64 read_format = leader->core.attr.read_format;\n\tint size = perf_evsel__read_size(&leader->core);\n\tu64 *data = ps->group_data;\n\n\tif (!(read_format & PERF_FORMAT_ID))\n\t\treturn -EINVAL;\n\n\tif (!evsel__is_group_leader(leader))\n\t\treturn -EINVAL;\n\n\tif (!data) {\n\t\tdata = zalloc(size);\n\t\tif (!data)\n\t\t\treturn -ENOMEM;\n\n\t\tps->group_data = data;\n\t}\n\n\tif (FD(leader, cpu_map_idx, thread) < 0)\n\t\treturn -EINVAL;\n\n\tif (readn(FD(leader, cpu_map_idx, thread), data, size) <= 0)\n\t\treturn -errno;\n\n\treturn evsel__process_group_data(leader, cpu_map_idx, thread, data);\n}\n\nint evsel__read_counter(struct evsel *evsel, int cpu_map_idx, int thread)\n{\n\tu64 read_format = evsel->core.attr.read_format;\n\n\tif (read_format & PERF_FORMAT_GROUP)\n\t\treturn evsel__read_group(evsel, cpu_map_idx, thread);\n\n\treturn evsel__read_one(evsel, cpu_map_idx, thread);\n}\n\nint __evsel__read_on_cpu(struct evsel *evsel, int cpu_map_idx, int thread, bool scale)\n{\n\tstruct perf_counts_values count;\n\tsize_t nv = scale ? 3 : 1;\n\n\tif (FD(evsel, cpu_map_idx, thread) < 0)\n\t\treturn -EINVAL;\n\n\tif (evsel->counts == NULL && evsel__alloc_counts(evsel) < 0)\n\t\treturn -ENOMEM;\n\n\tif (readn(FD(evsel, cpu_map_idx, thread), &count, nv * sizeof(u64)) <= 0)\n\t\treturn -errno;\n\n\tevsel__compute_deltas(evsel, cpu_map_idx, thread, &count);\n\tperf_counts_values__scale(&count, scale, NULL);\n\t*perf_counts(evsel->counts, cpu_map_idx, thread) = count;\n\treturn 0;\n}\n\nstatic int evsel__match_other_cpu(struct evsel *evsel, struct evsel *other,\n\t\t\t\t  int cpu_map_idx)\n{\n\tstruct perf_cpu cpu;\n\n\tcpu = perf_cpu_map__cpu(evsel->core.cpus, cpu_map_idx);\n\treturn perf_cpu_map__idx(other->core.cpus, cpu);\n}\n\nstatic int evsel__hybrid_group_cpu_map_idx(struct evsel *evsel, int cpu_map_idx)\n{\n\tstruct evsel *leader = evsel__leader(evsel);\n\n\tif ((evsel__is_hybrid(evsel) && !evsel__is_hybrid(leader)) ||\n\t    (!evsel__is_hybrid(evsel) && evsel__is_hybrid(leader))) {\n\t\treturn evsel__match_other_cpu(evsel, leader, cpu_map_idx);\n\t}\n\n\treturn cpu_map_idx;\n}\n\nstatic int get_group_fd(struct evsel *evsel, int cpu_map_idx, int thread)\n{\n\tstruct evsel *leader = evsel__leader(evsel);\n\tint fd;\n\n\tif (evsel__is_group_leader(evsel))\n\t\treturn -1;\n\n\t \n\tBUG_ON(!leader->core.fd);\n\n\tcpu_map_idx = evsel__hybrid_group_cpu_map_idx(evsel, cpu_map_idx);\n\tif (cpu_map_idx == -1)\n\t\treturn -1;\n\n\tfd = FD(leader, cpu_map_idx, thread);\n\tBUG_ON(fd == -1 && !leader->skippable);\n\n\t \n\treturn fd == -1 ? -2 : fd;\n}\n\nstatic void evsel__remove_fd(struct evsel *pos, int nr_cpus, int nr_threads, int thread_idx)\n{\n\tfor (int cpu = 0; cpu < nr_cpus; cpu++)\n\t\tfor (int thread = thread_idx; thread < nr_threads - 1; thread++)\n\t\t\tFD(pos, cpu, thread) = FD(pos, cpu, thread + 1);\n}\n\nstatic int update_fds(struct evsel *evsel,\n\t\t      int nr_cpus, int cpu_map_idx,\n\t\t      int nr_threads, int thread_idx)\n{\n\tstruct evsel *pos;\n\n\tif (cpu_map_idx >= nr_cpus || thread_idx >= nr_threads)\n\t\treturn -EINVAL;\n\n\tevlist__for_each_entry(evsel->evlist, pos) {\n\t\tnr_cpus = pos != evsel ? nr_cpus : cpu_map_idx;\n\n\t\tevsel__remove_fd(pos, nr_cpus, nr_threads, thread_idx);\n\n\t\t \n\t\tif (pos == evsel)\n\t\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic bool evsel__ignore_missing_thread(struct evsel *evsel,\n\t\t\t\t\t int nr_cpus, int cpu_map_idx,\n\t\t\t\t\t struct perf_thread_map *threads,\n\t\t\t\t\t int thread, int err)\n{\n\tpid_t ignore_pid = perf_thread_map__pid(threads, thread);\n\n\tif (!evsel->ignore_missing_thread)\n\t\treturn false;\n\n\t \n\tif (evsel->core.system_wide)\n\t\treturn false;\n\n\t \n\tif (err != -ESRCH)\n\t\treturn false;\n\n\t \n\tif (threads->nr == 1)\n\t\treturn false;\n\n\t \n\tif (update_fds(evsel, nr_cpus, cpu_map_idx, threads->nr, thread))\n\t\treturn false;\n\n\tif (thread_map__remove(threads, thread))\n\t\treturn false;\n\n\tpr_warning(\"WARNING: Ignored open failure for pid %d\\n\",\n\t\t   ignore_pid);\n\treturn true;\n}\n\nstatic int __open_attr__fprintf(FILE *fp, const char *name, const char *val,\n\t\t\t\tvoid *priv __maybe_unused)\n{\n\treturn fprintf(fp, \"  %-32s %s\\n\", name, val);\n}\n\nstatic void display_attr(struct perf_event_attr *attr)\n{\n\tif (verbose >= 2 || debug_peo_args) {\n\t\tfprintf(stderr, \"%.60s\\n\", graph_dotted_line);\n\t\tfprintf(stderr, \"perf_event_attr:\\n\");\n\t\tperf_event_attr__fprintf(stderr, attr, __open_attr__fprintf, NULL);\n\t\tfprintf(stderr, \"%.60s\\n\", graph_dotted_line);\n\t}\n}\n\nbool evsel__precise_ip_fallback(struct evsel *evsel)\n{\n\t \n\tif (!evsel->precise_max)\n\t\treturn false;\n\n\t \n\tif (!evsel->core.attr.precise_ip) {\n\t\tevsel->core.attr.precise_ip = evsel->precise_ip_original;\n\t\treturn false;\n\t}\n\n\tif (!evsel->precise_ip_original)\n\t\tevsel->precise_ip_original = evsel->core.attr.precise_ip;\n\n\tevsel->core.attr.precise_ip--;\n\tpr_debug2_peo(\"decreasing precise_ip by one (%d)\\n\", evsel->core.attr.precise_ip);\n\tdisplay_attr(&evsel->core.attr);\n\treturn true;\n}\n\nstatic struct perf_cpu_map *empty_cpu_map;\nstatic struct perf_thread_map *empty_thread_map;\n\nstatic int __evsel__prepare_open(struct evsel *evsel, struct perf_cpu_map *cpus,\n\t\tstruct perf_thread_map *threads)\n{\n\tint nthreads = perf_thread_map__nr(threads);\n\n\tif ((perf_missing_features.write_backward && evsel->core.attr.write_backward) ||\n\t    (perf_missing_features.aux_output     && evsel->core.attr.aux_output))\n\t\treturn -EINVAL;\n\n\tif (cpus == NULL) {\n\t\tif (empty_cpu_map == NULL) {\n\t\t\tempty_cpu_map = perf_cpu_map__dummy_new();\n\t\t\tif (empty_cpu_map == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tcpus = empty_cpu_map;\n\t}\n\n\tif (threads == NULL) {\n\t\tif (empty_thread_map == NULL) {\n\t\t\tempty_thread_map = thread_map__new_by_tid(-1);\n\t\t\tif (empty_thread_map == NULL)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tthreads = empty_thread_map;\n\t}\n\n\tif (evsel->core.fd == NULL &&\n\t    perf_evsel__alloc_fd(&evsel->core, perf_cpu_map__nr(cpus), nthreads) < 0)\n\t\treturn -ENOMEM;\n\n\tevsel->open_flags = PERF_FLAG_FD_CLOEXEC;\n\tif (evsel->cgrp)\n\t\tevsel->open_flags |= PERF_FLAG_PID_CGROUP;\n\n\treturn 0;\n}\n\nstatic void evsel__disable_missing_features(struct evsel *evsel)\n{\n\tif (perf_missing_features.read_lost)\n\t\tevsel->core.attr.read_format &= ~PERF_FORMAT_LOST;\n\tif (perf_missing_features.weight_struct) {\n\t\tevsel__set_sample_bit(evsel, WEIGHT);\n\t\tevsel__reset_sample_bit(evsel, WEIGHT_STRUCT);\n\t}\n\tif (perf_missing_features.clockid_wrong)\n\t\tevsel->core.attr.clockid = CLOCK_MONOTONIC;  \n\tif (perf_missing_features.clockid) {\n\t\tevsel->core.attr.use_clockid = 0;\n\t\tevsel->core.attr.clockid = 0;\n\t}\n\tif (perf_missing_features.cloexec)\n\t\tevsel->open_flags &= ~(unsigned long)PERF_FLAG_FD_CLOEXEC;\n\tif (perf_missing_features.mmap2)\n\t\tevsel->core.attr.mmap2 = 0;\n\tif (evsel->pmu && evsel->pmu->missing_features.exclude_guest)\n\t\tevsel->core.attr.exclude_guest = evsel->core.attr.exclude_host = 0;\n\tif (perf_missing_features.lbr_flags)\n\t\tevsel->core.attr.branch_sample_type &= ~(PERF_SAMPLE_BRANCH_NO_FLAGS |\n\t\t\t\t     PERF_SAMPLE_BRANCH_NO_CYCLES);\n\tif (perf_missing_features.group_read && evsel->core.attr.inherit)\n\t\tevsel->core.attr.read_format &= ~(PERF_FORMAT_GROUP|PERF_FORMAT_ID);\n\tif (perf_missing_features.ksymbol)\n\t\tevsel->core.attr.ksymbol = 0;\n\tif (perf_missing_features.bpf)\n\t\tevsel->core.attr.bpf_event = 0;\n\tif (perf_missing_features.branch_hw_idx)\n\t\tevsel->core.attr.branch_sample_type &= ~PERF_SAMPLE_BRANCH_HW_INDEX;\n\tif (perf_missing_features.sample_id_all)\n\t\tevsel->core.attr.sample_id_all = 0;\n}\n\nint evsel__prepare_open(struct evsel *evsel, struct perf_cpu_map *cpus,\n\t\t\tstruct perf_thread_map *threads)\n{\n\tint err;\n\n\terr = __evsel__prepare_open(evsel, cpus, threads);\n\tif (err)\n\t\treturn err;\n\n\tevsel__disable_missing_features(evsel);\n\n\treturn err;\n}\n\nbool evsel__detect_missing_features(struct evsel *evsel)\n{\n\t \n\tif (!perf_missing_features.read_lost &&\n\t    (evsel->core.attr.read_format & PERF_FORMAT_LOST)) {\n\t\tperf_missing_features.read_lost = true;\n\t\tpr_debug2(\"switching off PERF_FORMAT_LOST support\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.weight_struct &&\n\t    (evsel->core.attr.sample_type & PERF_SAMPLE_WEIGHT_STRUCT)) {\n\t\tperf_missing_features.weight_struct = true;\n\t\tpr_debug2(\"switching off weight struct support\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.code_page_size &&\n\t    (evsel->core.attr.sample_type & PERF_SAMPLE_CODE_PAGE_SIZE)) {\n\t\tperf_missing_features.code_page_size = true;\n\t\tpr_debug2_peo(\"Kernel has no PERF_SAMPLE_CODE_PAGE_SIZE support, bailing out\\n\");\n\t\treturn false;\n\t} else if (!perf_missing_features.data_page_size &&\n\t    (evsel->core.attr.sample_type & PERF_SAMPLE_DATA_PAGE_SIZE)) {\n\t\tperf_missing_features.data_page_size = true;\n\t\tpr_debug2_peo(\"Kernel has no PERF_SAMPLE_DATA_PAGE_SIZE support, bailing out\\n\");\n\t\treturn false;\n\t} else if (!perf_missing_features.cgroup && evsel->core.attr.cgroup) {\n\t\tperf_missing_features.cgroup = true;\n\t\tpr_debug2_peo(\"Kernel has no cgroup sampling support, bailing out\\n\");\n\t\treturn false;\n\t} else if (!perf_missing_features.branch_hw_idx &&\n\t    (evsel->core.attr.branch_sample_type & PERF_SAMPLE_BRANCH_HW_INDEX)) {\n\t\tperf_missing_features.branch_hw_idx = true;\n\t\tpr_debug2(\"switching off branch HW index support\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.aux_output && evsel->core.attr.aux_output) {\n\t\tperf_missing_features.aux_output = true;\n\t\tpr_debug2_peo(\"Kernel has no attr.aux_output support, bailing out\\n\");\n\t\treturn false;\n\t} else if (!perf_missing_features.bpf && evsel->core.attr.bpf_event) {\n\t\tperf_missing_features.bpf = true;\n\t\tpr_debug2_peo(\"switching off bpf_event\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.ksymbol && evsel->core.attr.ksymbol) {\n\t\tperf_missing_features.ksymbol = true;\n\t\tpr_debug2_peo(\"switching off ksymbol\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.write_backward && evsel->core.attr.write_backward) {\n\t\tperf_missing_features.write_backward = true;\n\t\tpr_debug2_peo(\"switching off write_backward\\n\");\n\t\treturn false;\n\t} else if (!perf_missing_features.clockid_wrong && evsel->core.attr.use_clockid) {\n\t\tperf_missing_features.clockid_wrong = true;\n\t\tpr_debug2_peo(\"switching off clockid\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.clockid && evsel->core.attr.use_clockid) {\n\t\tperf_missing_features.clockid = true;\n\t\tpr_debug2_peo(\"switching off use_clockid\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.cloexec && (evsel->open_flags & PERF_FLAG_FD_CLOEXEC)) {\n\t\tperf_missing_features.cloexec = true;\n\t\tpr_debug2_peo(\"switching off cloexec flag\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.mmap2 && evsel->core.attr.mmap2) {\n\t\tperf_missing_features.mmap2 = true;\n\t\tpr_debug2_peo(\"switching off mmap2\\n\");\n\t\treturn true;\n\t} else if (evsel->core.attr.exclude_guest || evsel->core.attr.exclude_host) {\n\t\tif (evsel->pmu == NULL)\n\t\t\tevsel->pmu = evsel__find_pmu(evsel);\n\n\t\tif (evsel->pmu)\n\t\t\tevsel->pmu->missing_features.exclude_guest = true;\n\t\telse {\n\t\t\t \n\t\t\tevsel->core.attr.exclude_host = false;\n\t\t\tevsel->core.attr.exclude_guest = false;\n\t\t}\n\n\t\tif (evsel->exclude_GH) {\n\t\t\tpr_debug2_peo(\"PMU has no exclude_host/guest support, bailing out\\n\");\n\t\t\treturn false;\n\t\t}\n\t\tif (!perf_missing_features.exclude_guest) {\n\t\t\tperf_missing_features.exclude_guest = true;\n\t\t\tpr_debug2_peo(\"switching off exclude_guest, exclude_host\\n\");\n\t\t}\n\t\treturn true;\n\t} else if (!perf_missing_features.sample_id_all) {\n\t\tperf_missing_features.sample_id_all = true;\n\t\tpr_debug2_peo(\"switching off sample_id_all\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.lbr_flags &&\n\t\t\t(evsel->core.attr.branch_sample_type &\n\t\t\t (PERF_SAMPLE_BRANCH_NO_CYCLES |\n\t\t\t  PERF_SAMPLE_BRANCH_NO_FLAGS))) {\n\t\tperf_missing_features.lbr_flags = true;\n\t\tpr_debug2_peo(\"switching off branch sample type no (cycles/flags)\\n\");\n\t\treturn true;\n\t} else if (!perf_missing_features.group_read &&\n\t\t    evsel->core.attr.inherit &&\n\t\t   (evsel->core.attr.read_format & PERF_FORMAT_GROUP) &&\n\t\t   evsel__is_group_leader(evsel)) {\n\t\tperf_missing_features.group_read = true;\n\t\tpr_debug2_peo(\"switching off group read\\n\");\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n\nbool evsel__increase_rlimit(enum rlimit_action *set_rlimit)\n{\n\tint old_errno;\n\tstruct rlimit l;\n\n\tif (*set_rlimit < INCREASED_MAX) {\n\t\told_errno = errno;\n\n\t\tif (getrlimit(RLIMIT_NOFILE, &l) == 0) {\n\t\t\tif (*set_rlimit == NO_CHANGE) {\n\t\t\t\tl.rlim_cur = l.rlim_max;\n\t\t\t} else {\n\t\t\t\tl.rlim_cur = l.rlim_max + 1000;\n\t\t\t\tl.rlim_max = l.rlim_cur;\n\t\t\t}\n\t\t\tif (setrlimit(RLIMIT_NOFILE, &l) == 0) {\n\t\t\t\t(*set_rlimit) += 1;\n\t\t\t\terrno = old_errno;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\terrno = old_errno;\n\t}\n\n\treturn false;\n}\n\nstatic int evsel__open_cpu(struct evsel *evsel, struct perf_cpu_map *cpus,\n\t\tstruct perf_thread_map *threads,\n\t\tint start_cpu_map_idx, int end_cpu_map_idx)\n{\n\tint idx, thread, nthreads;\n\tint pid = -1, err, old_errno;\n\tenum rlimit_action set_rlimit = NO_CHANGE;\n\n\terr = __evsel__prepare_open(evsel, cpus, threads);\n\tif (err)\n\t\treturn err;\n\n\tif (cpus == NULL)\n\t\tcpus = empty_cpu_map;\n\n\tif (threads == NULL)\n\t\tthreads = empty_thread_map;\n\n\tnthreads = perf_thread_map__nr(threads);\n\n\tif (evsel->cgrp)\n\t\tpid = evsel->cgrp->fd;\n\nfallback_missing_features:\n\tevsel__disable_missing_features(evsel);\n\n\tpr_debug3(\"Opening: %s\\n\", evsel__name(evsel));\n\tdisplay_attr(&evsel->core.attr);\n\n\tfor (idx = start_cpu_map_idx; idx < end_cpu_map_idx; idx++) {\n\n\t\tfor (thread = 0; thread < nthreads; thread++) {\n\t\t\tint fd, group_fd;\nretry_open:\n\t\t\tif (thread >= nthreads)\n\t\t\t\tbreak;\n\n\t\t\tif (!evsel->cgrp && !evsel->core.system_wide)\n\t\t\t\tpid = perf_thread_map__pid(threads, thread);\n\n\t\t\tgroup_fd = get_group_fd(evsel, idx, thread);\n\n\t\t\tif (group_fd == -2) {\n\t\t\t\tpr_debug(\"broken group leader for %s\\n\", evsel->name);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_close;\n\t\t\t}\n\n\t\t\ttest_attr__ready();\n\n\t\t\t \n\t\t\tpr_debug2_peo(\"sys_perf_event_open: pid %d  cpu %d  group_fd %d  flags %#lx\",\n\t\t\t\tpid, perf_cpu_map__cpu(cpus, idx).cpu, group_fd, evsel->open_flags);\n\n\t\t\tfd = sys_perf_event_open(&evsel->core.attr, pid,\n\t\t\t\t\t\tperf_cpu_map__cpu(cpus, idx).cpu,\n\t\t\t\t\t\tgroup_fd, evsel->open_flags);\n\n\t\t\tFD(evsel, idx, thread) = fd;\n\n\t\t\tif (fd < 0) {\n\t\t\t\terr = -errno;\n\n\t\t\t\tpr_debug2_peo(\"\\nsys_perf_event_open failed, error %d\\n\",\n\t\t\t\t\t  err);\n\t\t\t\tgoto try_fallback;\n\t\t\t}\n\n\t\t\tbpf_counter__install_pe(evsel, idx, fd);\n\n\t\t\tif (unlikely(test_attr__enabled)) {\n\t\t\t\ttest_attr__open(&evsel->core.attr, pid,\n\t\t\t\t\t\tperf_cpu_map__cpu(cpus, idx),\n\t\t\t\t\t\tfd, group_fd, evsel->open_flags);\n\t\t\t}\n\n\t\t\t \n\t\t\tpr_debug2_peo(\" = %d\\n\", fd);\n\n\t\t\tif (evsel->bpf_fd >= 0) {\n\t\t\t\tint evt_fd = fd;\n\t\t\t\tint bpf_fd = evsel->bpf_fd;\n\n\t\t\t\terr = ioctl(evt_fd,\n\t\t\t\t\t    PERF_EVENT_IOC_SET_BPF,\n\t\t\t\t\t    bpf_fd);\n\t\t\t\tif (err && errno != EEXIST) {\n\t\t\t\t\tpr_err(\"failed to attach bpf fd %d: %s\\n\",\n\t\t\t\t\t       bpf_fd, strerror(errno));\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out_close;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tset_rlimit = NO_CHANGE;\n\n\t\t\t \n\t\t\tif (perf_missing_features.clockid ||\n\t\t\t    perf_missing_features.clockid_wrong) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_close;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n\ntry_fallback:\n\tif (evsel__precise_ip_fallback(evsel))\n\t\tgoto retry_open;\n\n\tif (evsel__ignore_missing_thread(evsel, perf_cpu_map__nr(cpus),\n\t\t\t\t\t idx, threads, thread, err)) {\n\t\t \n\t\tnthreads--;\n\n\t\t \n\t\terr = 0;\n\t\tgoto retry_open;\n\t}\n\t \n\tif (err == -EMFILE && evsel__increase_rlimit(&set_rlimit))\n\t\tgoto retry_open;\n\n\tif (err != -EINVAL || idx > 0 || thread > 0)\n\t\tgoto out_close;\n\n\tif (evsel__detect_missing_features(evsel))\n\t\tgoto fallback_missing_features;\nout_close:\n\tif (err)\n\t\tthreads->err_thread = thread;\n\n\told_errno = errno;\n\tdo {\n\t\twhile (--thread >= 0) {\n\t\t\tif (FD(evsel, idx, thread) >= 0)\n\t\t\t\tclose(FD(evsel, idx, thread));\n\t\t\tFD(evsel, idx, thread) = -1;\n\t\t}\n\t\tthread = nthreads;\n\t} while (--idx >= 0);\n\terrno = old_errno;\n\treturn err;\n}\n\nint evsel__open(struct evsel *evsel, struct perf_cpu_map *cpus,\n\t\tstruct perf_thread_map *threads)\n{\n\treturn evsel__open_cpu(evsel, cpus, threads, 0, perf_cpu_map__nr(cpus));\n}\n\nvoid evsel__close(struct evsel *evsel)\n{\n\tperf_evsel__close(&evsel->core);\n\tperf_evsel__free_id(&evsel->core);\n}\n\nint evsel__open_per_cpu(struct evsel *evsel, struct perf_cpu_map *cpus, int cpu_map_idx)\n{\n\tif (cpu_map_idx == -1)\n\t\treturn evsel__open_cpu(evsel, cpus, NULL, 0, perf_cpu_map__nr(cpus));\n\n\treturn evsel__open_cpu(evsel, cpus, NULL, cpu_map_idx, cpu_map_idx + 1);\n}\n\nint evsel__open_per_thread(struct evsel *evsel, struct perf_thread_map *threads)\n{\n\treturn evsel__open(evsel, NULL, threads);\n}\n\nstatic int perf_evsel__parse_id_sample(const struct evsel *evsel,\n\t\t\t\t       const union perf_event *event,\n\t\t\t\t       struct perf_sample *sample)\n{\n\tu64 type = evsel->core.attr.sample_type;\n\tconst __u64 *array = event->sample.array;\n\tbool swapped = evsel->needs_swap;\n\tunion u64_swap u;\n\n\tarray += ((event->header.size -\n\t\t   sizeof(event->header)) / sizeof(u64)) - 1;\n\n\tif (type & PERF_SAMPLE_IDENTIFIER) {\n\t\tsample->id = *array;\n\t\tarray--;\n\t}\n\n\tif (type & PERF_SAMPLE_CPU) {\n\t\tu.val64 = *array;\n\t\tif (swapped) {\n\t\t\t \n\t\t\tu.val64 = bswap_64(u.val64);\n\t\t\tu.val32[0] = bswap_32(u.val32[0]);\n\t\t}\n\n\t\tsample->cpu = u.val32[0];\n\t\tarray--;\n\t}\n\n\tif (type & PERF_SAMPLE_STREAM_ID) {\n\t\tsample->stream_id = *array;\n\t\tarray--;\n\t}\n\n\tif (type & PERF_SAMPLE_ID) {\n\t\tsample->id = *array;\n\t\tarray--;\n\t}\n\n\tif (type & PERF_SAMPLE_TIME) {\n\t\tsample->time = *array;\n\t\tarray--;\n\t}\n\n\tif (type & PERF_SAMPLE_TID) {\n\t\tu.val64 = *array;\n\t\tif (swapped) {\n\t\t\t \n\t\t\tu.val64 = bswap_64(u.val64);\n\t\t\tu.val32[0] = bswap_32(u.val32[0]);\n\t\t\tu.val32[1] = bswap_32(u.val32[1]);\n\t\t}\n\n\t\tsample->pid = u.val32[0];\n\t\tsample->tid = u.val32[1];\n\t\tarray--;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool overflow(const void *endp, u16 max_size, const void *offset,\n\t\t\t    u64 size)\n{\n\treturn size > max_size || offset + size > endp;\n}\n\n#define OVERFLOW_CHECK(offset, size, max_size)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (overflow(endp, (max_size), (offset), (size)))\t\\\n\t\t\treturn -EFAULT;\t\t\t\t\t\\\n\t} while (0)\n\n#define OVERFLOW_CHECK_u64(offset) \\\n\tOVERFLOW_CHECK(offset, sizeof(u64), sizeof(u64))\n\nstatic int\nperf_event__check_size(union perf_event *event, unsigned int sample_size)\n{\n\t \n\tif (sample_size + sizeof(event->header) > event->header.size)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nvoid __weak arch_perf_parse_sample_weight(struct perf_sample *data,\n\t\t\t\t\t  const __u64 *array,\n\t\t\t\t\t  u64 type __maybe_unused)\n{\n\tdata->weight = *array;\n}\n\nu64 evsel__bitfield_swap_branch_flags(u64 value)\n{\n\tu64 new_val = 0;\n\n\t \n\tif (host_is_bigendian()) {\n\t\tnew_val = bitfield_swap(value, 0, 1);\n\t\tnew_val |= bitfield_swap(value, 1, 1);\n\t\tnew_val |= bitfield_swap(value, 2, 1);\n\t\tnew_val |= bitfield_swap(value, 3, 1);\n\t\tnew_val |= bitfield_swap(value, 4, 16);\n\t\tnew_val |= bitfield_swap(value, 20, 4);\n\t\tnew_val |= bitfield_swap(value, 24, 2);\n\t\tnew_val |= bitfield_swap(value, 26, 4);\n\t\tnew_val |= bitfield_swap(value, 30, 3);\n\t\tnew_val |= bitfield_swap(value, 33, 31);\n\t} else {\n\t\tnew_val = bitfield_swap(value, 63, 1);\n\t\tnew_val |= bitfield_swap(value, 62, 1);\n\t\tnew_val |= bitfield_swap(value, 61, 1);\n\t\tnew_val |= bitfield_swap(value, 60, 1);\n\t\tnew_val |= bitfield_swap(value, 44, 16);\n\t\tnew_val |= bitfield_swap(value, 40, 4);\n\t\tnew_val |= bitfield_swap(value, 38, 2);\n\t\tnew_val |= bitfield_swap(value, 34, 4);\n\t\tnew_val |= bitfield_swap(value, 31, 3);\n\t\tnew_val |= bitfield_swap(value, 0, 31);\n\t}\n\n\treturn new_val;\n}\n\nint evsel__parse_sample(struct evsel *evsel, union perf_event *event,\n\t\t\tstruct perf_sample *data)\n{\n\tu64 type = evsel->core.attr.sample_type;\n\tbool swapped = evsel->needs_swap;\n\tconst __u64 *array;\n\tu16 max_size = event->header.size;\n\tconst void *endp = (void *)event + max_size;\n\tu64 sz;\n\n\t \n\tunion u64_swap u;\n\n\tmemset(data, 0, sizeof(*data));\n\tdata->cpu = data->pid = data->tid = -1;\n\tdata->stream_id = data->id = data->time = -1ULL;\n\tdata->period = evsel->core.attr.sample_period;\n\tdata->cpumode = event->header.misc & PERF_RECORD_MISC_CPUMODE_MASK;\n\tdata->misc    = event->header.misc;\n\tdata->id = -1ULL;\n\tdata->data_src = PERF_MEM_DATA_SRC_NONE;\n\tdata->vcpu = -1;\n\n\tif (event->header.type != PERF_RECORD_SAMPLE) {\n\t\tif (!evsel->core.attr.sample_id_all)\n\t\t\treturn 0;\n\t\treturn perf_evsel__parse_id_sample(evsel, event, data);\n\t}\n\n\tarray = event->sample.array;\n\n\tif (perf_event__check_size(event, evsel->sample_size))\n\t\treturn -EFAULT;\n\n\tif (type & PERF_SAMPLE_IDENTIFIER) {\n\t\tdata->id = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_IP) {\n\t\tdata->ip = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_TID) {\n\t\tu.val64 = *array;\n\t\tif (swapped) {\n\t\t\t \n\t\t\tu.val64 = bswap_64(u.val64);\n\t\t\tu.val32[0] = bswap_32(u.val32[0]);\n\t\t\tu.val32[1] = bswap_32(u.val32[1]);\n\t\t}\n\n\t\tdata->pid = u.val32[0];\n\t\tdata->tid = u.val32[1];\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_TIME) {\n\t\tdata->time = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_ADDR) {\n\t\tdata->addr = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_ID) {\n\t\tdata->id = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_STREAM_ID) {\n\t\tdata->stream_id = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_CPU) {\n\n\t\tu.val64 = *array;\n\t\tif (swapped) {\n\t\t\t \n\t\t\tu.val64 = bswap_64(u.val64);\n\t\t\tu.val32[0] = bswap_32(u.val32[0]);\n\t\t}\n\n\t\tdata->cpu = u.val32[0];\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_PERIOD) {\n\t\tdata->period = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_READ) {\n\t\tu64 read_format = evsel->core.attr.read_format;\n\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tif (read_format & PERF_FORMAT_GROUP)\n\t\t\tdata->read.group.nr = *array;\n\t\telse\n\t\t\tdata->read.one.value = *array;\n\n\t\tarray++;\n\n\t\tif (read_format & PERF_FORMAT_TOTAL_TIME_ENABLED) {\n\t\t\tOVERFLOW_CHECK_u64(array);\n\t\t\tdata->read.time_enabled = *array;\n\t\t\tarray++;\n\t\t}\n\n\t\tif (read_format & PERF_FORMAT_TOTAL_TIME_RUNNING) {\n\t\t\tOVERFLOW_CHECK_u64(array);\n\t\t\tdata->read.time_running = *array;\n\t\t\tarray++;\n\t\t}\n\n\t\t \n\t\tif (read_format & PERF_FORMAT_GROUP) {\n\t\t\tconst u64 max_group_nr = UINT64_MAX /\n\t\t\t\t\tsizeof(struct sample_read_value);\n\n\t\t\tif (data->read.group.nr > max_group_nr)\n\t\t\t\treturn -EFAULT;\n\n\t\t\tsz = data->read.group.nr * sample_read_value_size(read_format);\n\t\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\t\tdata->read.group.values =\n\t\t\t\t\t(struct sample_read_value *)array;\n\t\t\tarray = (void *)array + sz;\n\t\t} else {\n\t\t\tOVERFLOW_CHECK_u64(array);\n\t\t\tdata->read.one.id = *array;\n\t\t\tarray++;\n\n\t\t\tif (read_format & PERF_FORMAT_LOST) {\n\t\t\t\tOVERFLOW_CHECK_u64(array);\n\t\t\t\tdata->read.one.lost = *array;\n\t\t\t\tarray++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (type & PERF_SAMPLE_CALLCHAIN) {\n\t\tconst u64 max_callchain_nr = UINT64_MAX / sizeof(u64);\n\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tdata->callchain = (struct ip_callchain *)array++;\n\t\tif (data->callchain->nr > max_callchain_nr)\n\t\t\treturn -EFAULT;\n\t\tsz = data->callchain->nr * sizeof(u64);\n\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\tarray = (void *)array + sz;\n\t}\n\n\tif (type & PERF_SAMPLE_RAW) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tu.val64 = *array;\n\n\t\t \n\t\tif (swapped) {\n\t\t\tu.val64 = bswap_64(u.val64);\n\t\t\tu.val32[0] = bswap_32(u.val32[0]);\n\t\t\tu.val32[1] = bswap_32(u.val32[1]);\n\t\t}\n\t\tdata->raw_size = u.val32[0];\n\n\t\t \n\t\tif (swapped)\n\t\t\tmem_bswap_64((void *) array, data->raw_size);\n\n\t\tarray = (void *)array + sizeof(u32);\n\n\t\tOVERFLOW_CHECK(array, data->raw_size, max_size);\n\t\tdata->raw_data = (void *)array;\n\t\tarray = (void *)array + data->raw_size;\n\t}\n\n\tif (type & PERF_SAMPLE_BRANCH_STACK) {\n\t\tconst u64 max_branch_nr = UINT64_MAX /\n\t\t\t\t\t  sizeof(struct branch_entry);\n\t\tstruct branch_entry *e;\n\t\tunsigned int i;\n\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tdata->branch_stack = (struct branch_stack *)array++;\n\n\t\tif (data->branch_stack->nr > max_branch_nr)\n\t\t\treturn -EFAULT;\n\n\t\tsz = data->branch_stack->nr * sizeof(struct branch_entry);\n\t\tif (evsel__has_branch_hw_idx(evsel)) {\n\t\t\tsz += sizeof(u64);\n\t\t\te = &data->branch_stack->entries[0];\n\t\t} else {\n\t\t\tdata->no_hw_idx = true;\n\t\t\t \n\t\t\te = (struct branch_entry *)&data->branch_stack->hw_idx;\n\t\t}\n\n\t\tif (swapped) {\n\t\t\t \n\t\t\tfor (i = 0; i < data->branch_stack->nr; i++, e++)\n\t\t\t\te->flags.value = evsel__bitfield_swap_branch_flags(e->flags.value);\n\t\t}\n\n\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\tarray = (void *)array + sz;\n\t}\n\n\tif (type & PERF_SAMPLE_REGS_USER) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tdata->user_regs.abi = *array;\n\t\tarray++;\n\n\t\tif (data->user_regs.abi) {\n\t\t\tu64 mask = evsel->core.attr.sample_regs_user;\n\n\t\t\tsz = hweight64(mask) * sizeof(u64);\n\t\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\t\tdata->user_regs.mask = mask;\n\t\t\tdata->user_regs.regs = (u64 *)array;\n\t\t\tarray = (void *)array + sz;\n\t\t}\n\t}\n\n\tif (type & PERF_SAMPLE_STACK_USER) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tsz = *array++;\n\n\t\tdata->user_stack.offset = ((char *)(array - 1)\n\t\t\t\t\t  - (char *) event);\n\n\t\tif (!sz) {\n\t\t\tdata->user_stack.size = 0;\n\t\t} else {\n\t\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\t\tdata->user_stack.data = (char *)array;\n\t\t\tarray = (void *)array + sz;\n\t\t\tOVERFLOW_CHECK_u64(array);\n\t\t\tdata->user_stack.size = *array++;\n\t\t\tif (WARN_ONCE(data->user_stack.size > sz,\n\t\t\t\t      \"user stack dump failure\\n\"))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (type & PERF_SAMPLE_WEIGHT_TYPE) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tarch_perf_parse_sample_weight(data, array, type);\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_DATA_SRC) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tdata->data_src = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_TRANSACTION) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tdata->transaction = *array;\n\t\tarray++;\n\t}\n\n\tdata->intr_regs.abi = PERF_SAMPLE_REGS_ABI_NONE;\n\tif (type & PERF_SAMPLE_REGS_INTR) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tdata->intr_regs.abi = *array;\n\t\tarray++;\n\n\t\tif (data->intr_regs.abi != PERF_SAMPLE_REGS_ABI_NONE) {\n\t\t\tu64 mask = evsel->core.attr.sample_regs_intr;\n\n\t\t\tsz = hweight64(mask) * sizeof(u64);\n\t\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\t\tdata->intr_regs.mask = mask;\n\t\t\tdata->intr_regs.regs = (u64 *)array;\n\t\t\tarray = (void *)array + sz;\n\t\t}\n\t}\n\n\tdata->phys_addr = 0;\n\tif (type & PERF_SAMPLE_PHYS_ADDR) {\n\t\tdata->phys_addr = *array;\n\t\tarray++;\n\t}\n\n\tdata->cgroup = 0;\n\tif (type & PERF_SAMPLE_CGROUP) {\n\t\tdata->cgroup = *array;\n\t\tarray++;\n\t}\n\n\tdata->data_page_size = 0;\n\tif (type & PERF_SAMPLE_DATA_PAGE_SIZE) {\n\t\tdata->data_page_size = *array;\n\t\tarray++;\n\t}\n\n\tdata->code_page_size = 0;\n\tif (type & PERF_SAMPLE_CODE_PAGE_SIZE) {\n\t\tdata->code_page_size = *array;\n\t\tarray++;\n\t}\n\n\tif (type & PERF_SAMPLE_AUX) {\n\t\tOVERFLOW_CHECK_u64(array);\n\t\tsz = *array++;\n\n\t\tOVERFLOW_CHECK(array, sz, max_size);\n\t\t \n\t\tif (swapped)\n\t\t\tmem_bswap_64((char *)array, sz);\n\t\tdata->aux_sample.size = sz;\n\t\tdata->aux_sample.data = (char *)array;\n\t\tarray = (void *)array + sz;\n\t}\n\n\treturn 0;\n}\n\nint evsel__parse_sample_timestamp(struct evsel *evsel, union perf_event *event,\n\t\t\t\t  u64 *timestamp)\n{\n\tu64 type = evsel->core.attr.sample_type;\n\tconst __u64 *array;\n\n\tif (!(type & PERF_SAMPLE_TIME))\n\t\treturn -1;\n\n\tif (event->header.type != PERF_RECORD_SAMPLE) {\n\t\tstruct perf_sample data = {\n\t\t\t.time = -1ULL,\n\t\t};\n\n\t\tif (!evsel->core.attr.sample_id_all)\n\t\t\treturn -1;\n\t\tif (perf_evsel__parse_id_sample(evsel, event, &data))\n\t\t\treturn -1;\n\n\t\t*timestamp = data.time;\n\t\treturn 0;\n\t}\n\n\tarray = event->sample.array;\n\n\tif (perf_event__check_size(event, evsel->sample_size))\n\t\treturn -EFAULT;\n\n\tif (type & PERF_SAMPLE_IDENTIFIER)\n\t\tarray++;\n\n\tif (type & PERF_SAMPLE_IP)\n\t\tarray++;\n\n\tif (type & PERF_SAMPLE_TID)\n\t\tarray++;\n\n\tif (type & PERF_SAMPLE_TIME)\n\t\t*timestamp = *array;\n\n\treturn 0;\n}\n\nu16 evsel__id_hdr_size(struct evsel *evsel)\n{\n\tu64 sample_type = evsel->core.attr.sample_type;\n\tu16 size = 0;\n\n\tif (sample_type & PERF_SAMPLE_TID)\n\t\tsize += sizeof(u64);\n\n\tif (sample_type & PERF_SAMPLE_TIME)\n\t\tsize += sizeof(u64);\n\n\tif (sample_type & PERF_SAMPLE_ID)\n\t\tsize += sizeof(u64);\n\n\tif (sample_type & PERF_SAMPLE_STREAM_ID)\n\t\tsize += sizeof(u64);\n\n\tif (sample_type & PERF_SAMPLE_CPU)\n\t\tsize += sizeof(u64);\n\n\tif (sample_type & PERF_SAMPLE_IDENTIFIER)\n\t\tsize += sizeof(u64);\n\n\treturn size;\n}\n\n#ifdef HAVE_LIBTRACEEVENT\nstruct tep_format_field *evsel__field(struct evsel *evsel, const char *name)\n{\n\treturn tep_find_field(evsel->tp_format, name);\n}\n\nvoid *evsel__rawptr(struct evsel *evsel, struct perf_sample *sample, const char *name)\n{\n\tstruct tep_format_field *field = evsel__field(evsel, name);\n\tint offset;\n\n\tif (!field)\n\t\treturn NULL;\n\n\toffset = field->offset;\n\n\tif (field->flags & TEP_FIELD_IS_DYNAMIC) {\n\t\toffset = *(int *)(sample->raw_data + field->offset);\n\t\toffset &= 0xffff;\n\t\tif (tep_field_is_relative(field->flags))\n\t\t\toffset += field->offset + field->size;\n\t}\n\n\treturn sample->raw_data + offset;\n}\n\nu64 format_field__intval(struct tep_format_field *field, struct perf_sample *sample,\n\t\t\t bool needs_swap)\n{\n\tu64 value;\n\tvoid *ptr = sample->raw_data + field->offset;\n\n\tswitch (field->size) {\n\tcase 1:\n\t\treturn *(u8 *)ptr;\n\tcase 2:\n\t\tvalue = *(u16 *)ptr;\n\t\tbreak;\n\tcase 4:\n\t\tvalue = *(u32 *)ptr;\n\t\tbreak;\n\tcase 8:\n\t\tmemcpy(&value, ptr, sizeof(u64));\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (!needs_swap)\n\t\treturn value;\n\n\tswitch (field->size) {\n\tcase 2:\n\t\treturn bswap_16(value);\n\tcase 4:\n\t\treturn bswap_32(value);\n\tcase 8:\n\t\treturn bswap_64(value);\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\nu64 evsel__intval(struct evsel *evsel, struct perf_sample *sample, const char *name)\n{\n\tstruct tep_format_field *field = evsel__field(evsel, name);\n\n\treturn field ? format_field__intval(field, sample, evsel->needs_swap) : 0;\n}\n#endif\n\nbool evsel__fallback(struct evsel *evsel, int err, char *msg, size_t msgsize)\n{\n\tint paranoid;\n\n\tif ((err == ENOENT || err == ENXIO || err == ENODEV) &&\n\t    evsel->core.attr.type   == PERF_TYPE_HARDWARE &&\n\t    evsel->core.attr.config == PERF_COUNT_HW_CPU_CYCLES) {\n\t\t \n\t\tscnprintf(msg, msgsize, \"%s\",\n\"The cycles event is not supported, trying to fall back to cpu-clock-ticks\");\n\n\t\tevsel->core.attr.type   = PERF_TYPE_SOFTWARE;\n\t\tevsel->core.attr.config = PERF_COUNT_SW_CPU_CLOCK;\n\n\t\tzfree(&evsel->name);\n\t\treturn true;\n\t} else if (err == EACCES && !evsel->core.attr.exclude_kernel &&\n\t\t   (paranoid = perf_event_paranoid()) > 1) {\n\t\tconst char *name = evsel__name(evsel);\n\t\tchar *new_name;\n\t\tconst char *sep = \":\";\n\n\t\t \n\t\tif (evsel->core.attr.exclude_user)\n\t\t\treturn false;\n\n\t\t \n\t\tif (strchr(name, '/') ||\n\t\t    (strchr(name, ':') && !evsel->is_libpfm_event))\n\t\t\tsep = \"\";\n\n\t\tif (asprintf(&new_name, \"%s%su\", name, sep) < 0)\n\t\t\treturn false;\n\n\t\tfree(evsel->name);\n\t\tevsel->name = new_name;\n\t\tscnprintf(msg, msgsize, \"kernel.perf_event_paranoid=%d, trying \"\n\t\t\t  \"to fall back to excluding kernel and hypervisor \"\n\t\t\t  \" samples\", paranoid);\n\t\tevsel->core.attr.exclude_kernel = 1;\n\t\tevsel->core.attr.exclude_hv     = 1;\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool find_process(const char *name)\n{\n\tsize_t len = strlen(name);\n\tDIR *dir;\n\tstruct dirent *d;\n\tint ret = -1;\n\n\tdir = opendir(procfs__mountpoint());\n\tif (!dir)\n\t\treturn false;\n\n\t \n\twhile (ret && (d = readdir(dir)) != NULL) {\n\t\tchar path[PATH_MAX];\n\t\tchar *data;\n\t\tsize_t size;\n\n\t\tif ((d->d_type != DT_DIR) ||\n\t\t     !strcmp(\".\", d->d_name) ||\n\t\t     !strcmp(\"..\", d->d_name))\n\t\t\tcontinue;\n\n\t\tscnprintf(path, sizeof(path), \"%s/%s/comm\",\n\t\t\t  procfs__mountpoint(), d->d_name);\n\n\t\tif (filename__read_str(path, &data, &size))\n\t\t\tcontinue;\n\n\t\tret = strncmp(name, data, len);\n\t\tfree(data);\n\t}\n\n\tclosedir(dir);\n\treturn ret ? false : true;\n}\n\nint __weak arch_evsel__open_strerror(struct evsel *evsel __maybe_unused,\n\t\t\t\t     char *msg __maybe_unused,\n\t\t\t\t     size_t size __maybe_unused)\n{\n\treturn 0;\n}\n\nint evsel__open_strerror(struct evsel *evsel, struct target *target,\n\t\t\t int err, char *msg, size_t size)\n{\n\tchar sbuf[STRERR_BUFSIZE];\n\tint printed = 0, enforced = 0;\n\tint ret;\n\n\tswitch (err) {\n\tcase EPERM:\n\tcase EACCES:\n\t\tprinted += scnprintf(msg + printed, size - printed,\n\t\t\t\"Access to performance monitoring and observability operations is limited.\\n\");\n\n\t\tif (!sysfs__read_int(\"fs/selinux/enforce\", &enforced)) {\n\t\t\tif (enforced) {\n\t\t\t\tprinted += scnprintf(msg + printed, size - printed,\n\t\t\t\t\t\"Enforced MAC policy settings (SELinux) can limit access to performance\\n\"\n\t\t\t\t\t\"monitoring and observability operations. Inspect system audit records for\\n\"\n\t\t\t\t\t\"more perf_event access control information and adjusting the policy.\\n\");\n\t\t\t}\n\t\t}\n\n\t\tif (err == EPERM)\n\t\t\tprinted += scnprintf(msg, size,\n\t\t\t\t\"No permission to enable %s event.\\n\\n\", evsel__name(evsel));\n\n\t\treturn scnprintf(msg + printed, size - printed,\n\t\t \"Consider adjusting /proc/sys/kernel/perf_event_paranoid setting to open\\n\"\n\t\t \"access to performance monitoring and observability operations for processes\\n\"\n\t\t \"without CAP_PERFMON, CAP_SYS_PTRACE or CAP_SYS_ADMIN Linux capability.\\n\"\n\t\t \"More information can be found at 'Perf events and tool security' document:\\n\"\n\t\t \"https://www.kernel.org/doc/html/latest/admin-guide/perf-security.html\\n\"\n\t\t \"perf_event_paranoid setting is %d:\\n\"\n\t\t \"  -1: Allow use of (almost) all events by all users\\n\"\n\t\t \"      Ignore mlock limit after perf_event_mlock_kb without CAP_IPC_LOCK\\n\"\n\t\t \">= 0: Disallow raw and ftrace function tracepoint access\\n\"\n\t\t \">= 1: Disallow CPU event access\\n\"\n\t\t \">= 2: Disallow kernel profiling\\n\"\n\t\t \"To make the adjusted perf_event_paranoid setting permanent preserve it\\n\"\n\t\t \"in /etc/sysctl.conf (e.g. kernel.perf_event_paranoid = <setting>)\",\n\t\t perf_event_paranoid());\n\tcase ENOENT:\n\t\treturn scnprintf(msg, size, \"The %s event is not supported.\", evsel__name(evsel));\n\tcase EMFILE:\n\t\treturn scnprintf(msg, size, \"%s\",\n\t\t\t \"Too many events are opened.\\n\"\n\t\t\t \"Probably the maximum number of open file descriptors has been reached.\\n\"\n\t\t\t \"Hint: Try again after reducing the number of events.\\n\"\n\t\t\t \"Hint: Try increasing the limit with 'ulimit -n <limit>'\");\n\tcase ENOMEM:\n\t\tif (evsel__has_callchain(evsel) &&\n\t\t    access(\"/proc/sys/kernel/perf_event_max_stack\", F_OK) == 0)\n\t\t\treturn scnprintf(msg, size,\n\t\t\t\t\t \"Not enough memory to setup event with callchain.\\n\"\n\t\t\t\t\t \"Hint: Try tweaking /proc/sys/kernel/perf_event_max_stack\\n\"\n\t\t\t\t\t \"Hint: Current value: %d\", sysctl__max_stack());\n\t\tbreak;\n\tcase ENODEV:\n\t\tif (target->cpu_list)\n\t\t\treturn scnprintf(msg, size, \"%s\",\n\t \"No such device - did you specify an out-of-range profile CPU?\");\n\t\tbreak;\n\tcase EOPNOTSUPP:\n\t\tif (evsel->core.attr.sample_type & PERF_SAMPLE_BRANCH_STACK)\n\t\t\treturn scnprintf(msg, size,\n\t\"%s: PMU Hardware or event type doesn't support branch stack sampling.\",\n\t\t\t\t\t evsel__name(evsel));\n\t\tif (evsel->core.attr.aux_output)\n\t\t\treturn scnprintf(msg, size,\n\t\"%s: PMU Hardware doesn't support 'aux_output' feature\",\n\t\t\t\t\t evsel__name(evsel));\n\t\tif (evsel->core.attr.sample_period != 0)\n\t\t\treturn scnprintf(msg, size,\n\t\"%s: PMU Hardware doesn't support sampling/overflow-interrupts. Try 'perf stat'\",\n\t\t\t\t\t evsel__name(evsel));\n\t\tif (evsel->core.attr.precise_ip)\n\t\t\treturn scnprintf(msg, size, \"%s\",\n\t\"\\'precise\\' request may not be supported. Try removing 'p' modifier.\");\n#if defined(__i386__) || defined(__x86_64__)\n\t\tif (evsel->core.attr.type == PERF_TYPE_HARDWARE)\n\t\t\treturn scnprintf(msg, size, \"%s\",\n\t\"No hardware sampling interrupt available.\\n\");\n#endif\n\t\tbreak;\n\tcase EBUSY:\n\t\tif (find_process(\"oprofiled\"))\n\t\t\treturn scnprintf(msg, size,\n\t\"The PMU counters are busy/taken by another profiler.\\n\"\n\t\"We found oprofile daemon running, please stop it and try again.\");\n\t\tbreak;\n\tcase EINVAL:\n\t\tif (evsel->core.attr.sample_type & PERF_SAMPLE_CODE_PAGE_SIZE && perf_missing_features.code_page_size)\n\t\t\treturn scnprintf(msg, size, \"Asking for the code page size isn't supported by this kernel.\");\n\t\tif (evsel->core.attr.sample_type & PERF_SAMPLE_DATA_PAGE_SIZE && perf_missing_features.data_page_size)\n\t\t\treturn scnprintf(msg, size, \"Asking for the data page size isn't supported by this kernel.\");\n\t\tif (evsel->core.attr.write_backward && perf_missing_features.write_backward)\n\t\t\treturn scnprintf(msg, size, \"Reading from overwrite event is not supported by this kernel.\");\n\t\tif (perf_missing_features.clockid)\n\t\t\treturn scnprintf(msg, size, \"clockid feature not supported.\");\n\t\tif (perf_missing_features.clockid_wrong)\n\t\t\treturn scnprintf(msg, size, \"wrong clockid (%d).\", clockid);\n\t\tif (perf_missing_features.aux_output)\n\t\t\treturn scnprintf(msg, size, \"The 'aux_output' feature is not supported, update the kernel.\");\n\t\tif (!target__has_cpu(target))\n\t\t\treturn scnprintf(msg, size,\n\t\"Invalid event (%s) in per-thread mode, enable system wide with '-a'.\",\n\t\t\t\t\tevsel__name(evsel));\n\n\t\tbreak;\n\tcase ENODATA:\n\t\treturn scnprintf(msg, size, \"Cannot collect data source with the load latency event alone. \"\n\t\t\t\t \"Please add an auxiliary event in front of the load latency event.\");\n\tdefault:\n\t\tbreak;\n\t}\n\n\tret = arch_evsel__open_strerror(evsel, msg, size);\n\tif (ret)\n\t\treturn ret;\n\n\treturn scnprintf(msg, size,\n\t\"The sys_perf_event_open() syscall returned with %d (%s) for event (%s).\\n\"\n\t\"/bin/dmesg | grep -i perf may provide additional information.\\n\",\n\t\t\t err, str_error_r(err, sbuf, sizeof(sbuf)), evsel__name(evsel));\n}\n\nstruct perf_env *evsel__env(struct evsel *evsel)\n{\n\tif (evsel && evsel->evlist && evsel->evlist->env)\n\t\treturn evsel->evlist->env;\n\treturn &perf_env;\n}\n\nstatic int store_evsel_ids(struct evsel *evsel, struct evlist *evlist)\n{\n\tint cpu_map_idx, thread;\n\n\tfor (cpu_map_idx = 0; cpu_map_idx < xyarray__max_x(evsel->core.fd); cpu_map_idx++) {\n\t\tfor (thread = 0; thread < xyarray__max_y(evsel->core.fd);\n\t\t     thread++) {\n\t\t\tint fd = FD(evsel, cpu_map_idx, thread);\n\n\t\t\tif (perf_evlist__id_add_fd(&evlist->core, &evsel->core,\n\t\t\t\t\t\t   cpu_map_idx, thread, fd) < 0)\n\t\t\t\treturn -1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint evsel__store_ids(struct evsel *evsel, struct evlist *evlist)\n{\n\tstruct perf_cpu_map *cpus = evsel->core.cpus;\n\tstruct perf_thread_map *threads = evsel->core.threads;\n\n\tif (perf_evsel__alloc_id(&evsel->core, perf_cpu_map__nr(cpus), threads->nr))\n\t\treturn -ENOMEM;\n\n\treturn store_evsel_ids(evsel, evlist);\n}\n\nvoid evsel__zero_per_pkg(struct evsel *evsel)\n{\n\tstruct hashmap_entry *cur;\n\tsize_t bkt;\n\n\tif (evsel->per_pkg_mask) {\n\t\thashmap__for_each_entry(evsel->per_pkg_mask, cur, bkt)\n\t\t\tzfree(&cur->pkey);\n\n\t\thashmap__clear(evsel->per_pkg_mask);\n\t}\n}\n\n \nbool evsel__is_hybrid(const struct evsel *evsel)\n{\n\tif (perf_pmus__num_core_pmus() == 1)\n\t\treturn false;\n\n\treturn evsel->core.is_pmu_core;\n}\n\nstruct evsel *evsel__leader(const struct evsel *evsel)\n{\n\treturn container_of(evsel->core.leader, struct evsel, core);\n}\n\nbool evsel__has_leader(struct evsel *evsel, struct evsel *leader)\n{\n\treturn evsel->core.leader == &leader->core;\n}\n\nbool evsel__is_leader(struct evsel *evsel)\n{\n\treturn evsel__has_leader(evsel, evsel);\n}\n\nvoid evsel__set_leader(struct evsel *evsel, struct evsel *leader)\n{\n\tevsel->core.leader = &leader->core;\n}\n\nint evsel__source_count(const struct evsel *evsel)\n{\n\tstruct evsel *pos;\n\tint count = 0;\n\n\tevlist__for_each_entry(evsel->evlist, pos) {\n\t\tif (pos->metric_leader == evsel)\n\t\t\tcount++;\n\t}\n\treturn count;\n}\n\nbool __weak arch_evsel__must_be_in_group(const struct evsel *evsel __maybe_unused)\n{\n\treturn false;\n}\n\n \nvoid evsel__remove_from_group(struct evsel *evsel, struct evsel *leader)\n{\n\tif (!arch_evsel__must_be_in_group(evsel) && evsel != leader) {\n\t\tevsel__set_leader(evsel, evsel);\n\t\tevsel->core.nr_members = 0;\n\t\tleader->core.nr_members--;\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}