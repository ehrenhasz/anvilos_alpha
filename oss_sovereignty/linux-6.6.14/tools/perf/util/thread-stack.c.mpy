{
  "module_name": "thread-stack.c",
  "hash_id": "93343ae487c5a8fa8d2b8098f4d155d15a95ef2f657ca16a82e82a1854ddd994",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/thread-stack.c",
  "human_readable_source": "\n \n\n#include <linux/rbtree.h>\n#include <linux/list.h>\n#include <linux/log2.h>\n#include <linux/zalloc.h>\n#include <errno.h>\n#include <stdlib.h>\n#include <string.h>\n#include \"thread.h\"\n#include \"event.h\"\n#include \"machine.h\"\n#include \"env.h\"\n#include \"debug.h\"\n#include \"symbol.h\"\n#include \"comm.h\"\n#include \"call-path.h\"\n#include \"thread-stack.h\"\n\n#define STACK_GROWTH 2048\n\n \nenum retpoline_state_t {\n\tRETPOLINE_NONE,\n\tX86_RETPOLINE_POSSIBLE,\n\tX86_RETPOLINE_DETECTED,\n};\n\n \nstruct thread_stack_entry {\n\tu64 ret_addr;\n\tu64 timestamp;\n\tu64 ref;\n\tu64 branch_count;\n\tu64 insn_count;\n\tu64 cyc_count;\n\tu64 db_id;\n\tstruct call_path *cp;\n\tbool no_call;\n\tbool trace_end;\n\tbool non_call;\n};\n\n \nstruct thread_stack {\n\tstruct thread_stack_entry *stack;\n\tsize_t cnt;\n\tsize_t sz;\n\tu64 trace_nr;\n\tu64 branch_count;\n\tu64 insn_count;\n\tu64 cyc_count;\n\tu64 kernel_start;\n\tu64 last_time;\n\tstruct call_return_processor *crp;\n\tstruct comm *comm;\n\tunsigned int arr_sz;\n\tenum retpoline_state_t rstate;\n\tstruct branch_stack *br_stack_rb;\n\tunsigned int br_stack_sz;\n\tunsigned int br_stack_pos;\n\tbool mispred_all;\n};\n\n \nstatic inline bool thread_stack__per_cpu(struct thread *thread)\n{\n\treturn !(thread__tid(thread) || thread__pid(thread));\n}\n\nstatic int thread_stack__grow(struct thread_stack *ts)\n{\n\tstruct thread_stack_entry *new_stack;\n\tsize_t sz, new_sz;\n\n\tnew_sz = ts->sz + STACK_GROWTH;\n\tsz = new_sz * sizeof(struct thread_stack_entry);\n\n\tnew_stack = realloc(ts->stack, sz);\n\tif (!new_stack)\n\t\treturn -ENOMEM;\n\n\tts->stack = new_stack;\n\tts->sz = new_sz;\n\n\treturn 0;\n}\n\nstatic int thread_stack__init(struct thread_stack *ts, struct thread *thread,\n\t\t\t      struct call_return_processor *crp,\n\t\t\t      bool callstack, unsigned int br_stack_sz)\n{\n\tint err;\n\n\tif (callstack) {\n\t\terr = thread_stack__grow(ts);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (br_stack_sz) {\n\t\tsize_t sz = sizeof(struct branch_stack);\n\n\t\tsz += br_stack_sz * sizeof(struct branch_entry);\n\t\tts->br_stack_rb = zalloc(sz);\n\t\tif (!ts->br_stack_rb)\n\t\t\treturn -ENOMEM;\n\t\tts->br_stack_sz = br_stack_sz;\n\t}\n\n\tif (thread__maps(thread) && maps__machine(thread__maps(thread))) {\n\t\tstruct machine *machine = maps__machine(thread__maps(thread));\n\t\tconst char *arch = perf_env__arch(machine->env);\n\n\t\tts->kernel_start = machine__kernel_start(machine);\n\t\tif (!strcmp(arch, \"x86\"))\n\t\t\tts->rstate = X86_RETPOLINE_POSSIBLE;\n\t} else {\n\t\tts->kernel_start = 1ULL << 63;\n\t}\n\tts->crp = crp;\n\n\treturn 0;\n}\n\nstatic struct thread_stack *thread_stack__new(struct thread *thread, int cpu,\n\t\t\t\t\t      struct call_return_processor *crp,\n\t\t\t\t\t      bool callstack,\n\t\t\t\t\t      unsigned int br_stack_sz)\n{\n\tstruct thread_stack *ts = thread__ts(thread), *new_ts;\n\tunsigned int old_sz = ts ? ts->arr_sz : 0;\n\tunsigned int new_sz = 1;\n\n\tif (thread_stack__per_cpu(thread) && cpu > 0)\n\t\tnew_sz = roundup_pow_of_two(cpu + 1);\n\n\tif (!ts || new_sz > old_sz) {\n\t\tnew_ts = calloc(new_sz, sizeof(*ts));\n\t\tif (!new_ts)\n\t\t\treturn NULL;\n\t\tif (ts)\n\t\t\tmemcpy(new_ts, ts, old_sz * sizeof(*ts));\n\t\tnew_ts->arr_sz = new_sz;\n\t\tfree(thread__ts(thread));\n\t\tthread__set_ts(thread, new_ts);\n\t\tts = new_ts;\n\t}\n\n\tif (thread_stack__per_cpu(thread) && cpu > 0 &&\n\t    (unsigned int)cpu < ts->arr_sz)\n\t\tts += cpu;\n\n\tif (!ts->stack &&\n\t    thread_stack__init(ts, thread, crp, callstack, br_stack_sz))\n\t\treturn NULL;\n\n\treturn ts;\n}\n\nstatic struct thread_stack *thread__cpu_stack(struct thread *thread, int cpu)\n{\n\tstruct thread_stack *ts = thread__ts(thread);\n\n\tif (cpu < 0)\n\t\tcpu = 0;\n\n\tif (!ts || (unsigned int)cpu >= ts->arr_sz)\n\t\treturn NULL;\n\n\tts += cpu;\n\n\tif (!ts->stack)\n\t\treturn NULL;\n\n\treturn ts;\n}\n\nstatic inline struct thread_stack *thread__stack(struct thread *thread,\n\t\t\t\t\t\t    int cpu)\n{\n\tif (!thread)\n\t\treturn NULL;\n\n\tif (thread_stack__per_cpu(thread))\n\t\treturn thread__cpu_stack(thread, cpu);\n\n\treturn thread__ts(thread);\n}\n\nstatic int thread_stack__push(struct thread_stack *ts, u64 ret_addr,\n\t\t\t      bool trace_end)\n{\n\tint err = 0;\n\n\tif (ts->cnt == ts->sz) {\n\t\terr = thread_stack__grow(ts);\n\t\tif (err) {\n\t\t\tpr_warning(\"Out of memory: discarding thread stack\\n\");\n\t\t\tts->cnt = 0;\n\t\t}\n\t}\n\n\tts->stack[ts->cnt].trace_end = trace_end;\n\tts->stack[ts->cnt++].ret_addr = ret_addr;\n\n\treturn err;\n}\n\nstatic void thread_stack__pop(struct thread_stack *ts, u64 ret_addr)\n{\n\tsize_t i;\n\n\t \n\tfor (i = ts->cnt; i; ) {\n\t\tif (ts->stack[--i].ret_addr == ret_addr) {\n\t\t\tts->cnt = i;\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void thread_stack__pop_trace_end(struct thread_stack *ts)\n{\n\tsize_t i;\n\n\tfor (i = ts->cnt; i; ) {\n\t\tif (ts->stack[--i].trace_end)\n\t\t\tts->cnt = i;\n\t\telse\n\t\t\treturn;\n\t}\n}\n\nstatic bool thread_stack__in_kernel(struct thread_stack *ts)\n{\n\tif (!ts->cnt)\n\t\treturn false;\n\n\treturn ts->stack[ts->cnt - 1].cp->in_kernel;\n}\n\nstatic int thread_stack__call_return(struct thread *thread,\n\t\t\t\t     struct thread_stack *ts, size_t idx,\n\t\t\t\t     u64 timestamp, u64 ref, bool no_return)\n{\n\tstruct call_return_processor *crp = ts->crp;\n\tstruct thread_stack_entry *tse;\n\tstruct call_return cr = {\n\t\t.thread = thread,\n\t\t.comm = ts->comm,\n\t\t.db_id = 0,\n\t};\n\tu64 *parent_db_id;\n\n\ttse = &ts->stack[idx];\n\tcr.cp = tse->cp;\n\tcr.call_time = tse->timestamp;\n\tcr.return_time = timestamp;\n\tcr.branch_count = ts->branch_count - tse->branch_count;\n\tcr.insn_count = ts->insn_count - tse->insn_count;\n\tcr.cyc_count = ts->cyc_count - tse->cyc_count;\n\tcr.db_id = tse->db_id;\n\tcr.call_ref = tse->ref;\n\tcr.return_ref = ref;\n\tif (tse->no_call)\n\t\tcr.flags |= CALL_RETURN_NO_CALL;\n\tif (no_return)\n\t\tcr.flags |= CALL_RETURN_NO_RETURN;\n\tif (tse->non_call)\n\t\tcr.flags |= CALL_RETURN_NON_CALL;\n\n\t \n\tparent_db_id = idx ? &(tse - 1)->db_id : NULL;\n\n\treturn crp->process(&cr, parent_db_id, crp->data);\n}\n\nstatic int __thread_stack__flush(struct thread *thread, struct thread_stack *ts)\n{\n\tstruct call_return_processor *crp = ts->crp;\n\tint err;\n\n\tif (!crp) {\n\t\tts->cnt = 0;\n\t\tts->br_stack_pos = 0;\n\t\tif (ts->br_stack_rb)\n\t\t\tts->br_stack_rb->nr = 0;\n\t\treturn 0;\n\t}\n\n\twhile (ts->cnt) {\n\t\terr = thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\tts->last_time, 0, true);\n\t\tif (err) {\n\t\t\tpr_err(\"Error flushing thread stack!\\n\");\n\t\t\tts->cnt = 0;\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint thread_stack__flush(struct thread *thread)\n{\n\tstruct thread_stack *ts = thread__ts(thread);\n\tunsigned int pos;\n\tint err = 0;\n\n\tif (ts) {\n\t\tfor (pos = 0; pos < ts->arr_sz; pos++) {\n\t\t\tint ret = __thread_stack__flush(thread, ts + pos);\n\n\t\t\tif (ret)\n\t\t\t\terr = ret;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic void thread_stack__update_br_stack(struct thread_stack *ts, u32 flags,\n\t\t\t\t\t  u64 from_ip, u64 to_ip)\n{\n\tstruct branch_stack *bs = ts->br_stack_rb;\n\tstruct branch_entry *be;\n\n\tif (!ts->br_stack_pos)\n\t\tts->br_stack_pos = ts->br_stack_sz;\n\n\tts->br_stack_pos -= 1;\n\n\tbe              = &bs->entries[ts->br_stack_pos];\n\tbe->from        = from_ip;\n\tbe->to          = to_ip;\n\tbe->flags.value = 0;\n\tbe->flags.abort = !!(flags & PERF_IP_FLAG_TX_ABORT);\n\tbe->flags.in_tx = !!(flags & PERF_IP_FLAG_IN_TX);\n\t \n\tbe->flags.mispred = ts->mispred_all;\n\n\tif (bs->nr < ts->br_stack_sz)\n\t\tbs->nr += 1;\n}\n\nint thread_stack__event(struct thread *thread, int cpu, u32 flags, u64 from_ip,\n\t\t\tu64 to_ip, u16 insn_len, u64 trace_nr, bool callstack,\n\t\t\tunsigned int br_stack_sz, bool mispred_all)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\n\tif (!thread)\n\t\treturn -EINVAL;\n\n\tif (!ts) {\n\t\tts = thread_stack__new(thread, cpu, NULL, callstack, br_stack_sz);\n\t\tif (!ts) {\n\t\t\tpr_warning(\"Out of memory: no thread stack\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tts->trace_nr = trace_nr;\n\t\tts->mispred_all = mispred_all;\n\t}\n\n\t \n\tif (trace_nr != ts->trace_nr) {\n\t\tif (ts->trace_nr)\n\t\t\t__thread_stack__flush(thread, ts);\n\t\tts->trace_nr = trace_nr;\n\t}\n\n\tif (br_stack_sz)\n\t\tthread_stack__update_br_stack(ts, flags, from_ip, to_ip);\n\n\t \n\tif (ts->crp || !callstack)\n\t\treturn 0;\n\n\tif (flags & PERF_IP_FLAG_CALL) {\n\t\tu64 ret_addr;\n\n\t\tif (!to_ip)\n\t\t\treturn 0;\n\t\tret_addr = from_ip + insn_len;\n\t\tif (ret_addr == to_ip)\n\t\t\treturn 0;  \n\t\treturn thread_stack__push(ts, ret_addr,\n\t\t\t\t\t  flags & PERF_IP_FLAG_TRACE_END);\n\t} else if (flags & PERF_IP_FLAG_TRACE_BEGIN) {\n\t\t \n\t\tthread_stack__pop(ts, to_ip);\n\t\tthread_stack__pop_trace_end(ts);\n\t} else if ((flags & PERF_IP_FLAG_RETURN) && from_ip) {\n\t\tthread_stack__pop(ts, to_ip);\n\t}\n\n\treturn 0;\n}\n\nvoid thread_stack__set_trace_nr(struct thread *thread, int cpu, u64 trace_nr)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\n\tif (!ts)\n\t\treturn;\n\n\tif (trace_nr != ts->trace_nr) {\n\t\tif (ts->trace_nr)\n\t\t\t__thread_stack__flush(thread, ts);\n\t\tts->trace_nr = trace_nr;\n\t}\n}\n\nstatic void __thread_stack__free(struct thread *thread, struct thread_stack *ts)\n{\n\t__thread_stack__flush(thread, ts);\n\tzfree(&ts->stack);\n\tzfree(&ts->br_stack_rb);\n}\n\nstatic void thread_stack__reset(struct thread *thread, struct thread_stack *ts)\n{\n\tunsigned int arr_sz = ts->arr_sz;\n\n\t__thread_stack__free(thread, ts);\n\tmemset(ts, 0, sizeof(*ts));\n\tts->arr_sz = arr_sz;\n}\n\nvoid thread_stack__free(struct thread *thread)\n{\n\tstruct thread_stack *ts = thread__ts(thread);\n\tunsigned int pos;\n\n\tif (ts) {\n\t\tfor (pos = 0; pos < ts->arr_sz; pos++)\n\t\t\t__thread_stack__free(thread, ts + pos);\n\t\tfree(thread__ts(thread));\n\t\tthread__set_ts(thread, NULL);\n\t}\n}\n\nstatic inline u64 callchain_context(u64 ip, u64 kernel_start)\n{\n\treturn ip < kernel_start ? PERF_CONTEXT_USER : PERF_CONTEXT_KERNEL;\n}\n\nvoid thread_stack__sample(struct thread *thread, int cpu,\n\t\t\t  struct ip_callchain *chain,\n\t\t\t  size_t sz, u64 ip, u64 kernel_start)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\tu64 context = callchain_context(ip, kernel_start);\n\tu64 last_context;\n\tsize_t i, j;\n\n\tif (sz < 2) {\n\t\tchain->nr = 0;\n\t\treturn;\n\t}\n\n\tchain->ips[0] = context;\n\tchain->ips[1] = ip;\n\n\tif (!ts) {\n\t\tchain->nr = 2;\n\t\treturn;\n\t}\n\n\tlast_context = context;\n\n\tfor (i = 2, j = 1; i < sz && j <= ts->cnt; i++, j++) {\n\t\tip = ts->stack[ts->cnt - j].ret_addr;\n\t\tcontext = callchain_context(ip, kernel_start);\n\t\tif (context != last_context) {\n\t\t\tif (i >= sz - 1)\n\t\t\t\tbreak;\n\t\t\tchain->ips[i++] = context;\n\t\t\tlast_context = context;\n\t\t}\n\t\tchain->ips[i] = ip;\n\t}\n\n\tchain->nr = i;\n}\n\n \nvoid thread_stack__sample_late(struct thread *thread, int cpu,\n\t\t\t       struct ip_callchain *chain, size_t sz,\n\t\t\t       u64 sample_ip, u64 kernel_start)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\tu64 sample_context = callchain_context(sample_ip, kernel_start);\n\tu64 last_context, context, ip;\n\tsize_t nr = 0, j;\n\n\tif (sz < 2) {\n\t\tchain->nr = 0;\n\t\treturn;\n\t}\n\n\tif (!ts)\n\t\tgoto out;\n\n\t \n\tfor (j = 1; j <= ts->cnt; j++) {\n\t\tip = ts->stack[ts->cnt - j].ret_addr;\n\t\tcontext = callchain_context(ip, kernel_start);\n\t\tif (context == PERF_CONTEXT_USER ||\n\t\t    (context == sample_context && ip == sample_ip))\n\t\t\tbreak;\n\t}\n\n\tlast_context = sample_ip;  \n\n\tfor (; nr < sz && j <= ts->cnt; nr++, j++) {\n\t\tip = ts->stack[ts->cnt - j].ret_addr;\n\t\tcontext = callchain_context(ip, kernel_start);\n\t\tif (context != last_context) {\n\t\t\tif (nr >= sz - 1)\n\t\t\t\tbreak;\n\t\t\tchain->ips[nr++] = context;\n\t\t\tlast_context = context;\n\t\t}\n\t\tchain->ips[nr] = ip;\n\t}\nout:\n\tif (nr) {\n\t\tchain->nr = nr;\n\t} else {\n\t\tchain->ips[0] = sample_context;\n\t\tchain->ips[1] = sample_ip;\n\t\tchain->nr = 2;\n\t}\n}\n\nvoid thread_stack__br_sample(struct thread *thread, int cpu,\n\t\t\t     struct branch_stack *dst, unsigned int sz)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\tconst size_t bsz = sizeof(struct branch_entry);\n\tstruct branch_stack *src;\n\tstruct branch_entry *be;\n\tunsigned int nr;\n\n\tdst->nr = 0;\n\n\tif (!ts)\n\t\treturn;\n\n\tsrc = ts->br_stack_rb;\n\tif (!src->nr)\n\t\treturn;\n\n\tdst->nr = min((unsigned int)src->nr, sz);\n\n\tbe = &dst->entries[0];\n\tnr = min(ts->br_stack_sz - ts->br_stack_pos, (unsigned int)dst->nr);\n\tmemcpy(be, &src->entries[ts->br_stack_pos], bsz * nr);\n\n\tif (src->nr >= ts->br_stack_sz) {\n\t\tsz -= nr;\n\t\tbe = &dst->entries[nr];\n\t\tnr = min(ts->br_stack_pos, sz);\n\t\tmemcpy(be, &src->entries[0], bsz * ts->br_stack_pos);\n\t}\n}\n\n \nstatic bool us_start(struct branch_entry *be, u64 kernel_start, bool *start)\n{\n\tif (!*start)\n\t\t*start = be->to && be->to < kernel_start;\n\n\treturn *start;\n}\n\n \nstatic bool ks_start(struct branch_entry *be, u64 sample_ip, u64 kernel_start,\n\t\t     bool *start, struct branch_entry *nb)\n{\n\tif (!*start) {\n\t\t*start = (nb && sample_ip >= be->to && sample_ip <= nb->from) ||\n\t\t\t be->from < kernel_start ||\n\t\t\t (be->to && be->to < kernel_start);\n\t}\n\n\treturn *start;\n}\n\n \nvoid thread_stack__br_sample_late(struct thread *thread, int cpu,\n\t\t\t\t  struct branch_stack *dst, unsigned int sz,\n\t\t\t\t  u64 ip, u64 kernel_start)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\tstruct branch_entry *d, *s, *spos, *ssz;\n\tstruct branch_stack *src;\n\tunsigned int nr = 0;\n\tbool start = false;\n\n\tdst->nr = 0;\n\n\tif (!ts)\n\t\treturn;\n\n\tsrc = ts->br_stack_rb;\n\tif (!src->nr)\n\t\treturn;\n\n\tspos = &src->entries[ts->br_stack_pos];\n\tssz  = &src->entries[ts->br_stack_sz];\n\n\td = &dst->entries[0];\n\ts = spos;\n\n\tif (ip < kernel_start) {\n\t\t \n\t\tfor (s = spos; s < ssz && nr < sz; s++) {\n\t\t\tif (us_start(s, kernel_start, &start)) {\n\t\t\t\t*d++ = *s;\n\t\t\t\tnr += 1;\n\t\t\t}\n\t\t}\n\n\t\tif (src->nr >= ts->br_stack_sz) {\n\t\t\tfor (s = &src->entries[0]; s < spos && nr < sz; s++) {\n\t\t\t\tif (us_start(s, kernel_start, &start)) {\n\t\t\t\t\t*d++ = *s;\n\t\t\t\t\tnr += 1;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstruct branch_entry *nb = NULL;\n\n\t\t \n\t\tfor (s = spos; s < ssz && nr < sz; s++) {\n\t\t\tif (ks_start(s, ip, kernel_start, &start, nb)) {\n\t\t\t\t*d++ = *s;\n\t\t\t\tnr += 1;\n\t\t\t}\n\t\t\tnb = s;\n\t\t}\n\n\t\tif (src->nr >= ts->br_stack_sz) {\n\t\t\tfor (s = &src->entries[0]; s < spos && nr < sz; s++) {\n\t\t\t\tif (ks_start(s, ip, kernel_start, &start, nb)) {\n\t\t\t\t\t*d++ = *s;\n\t\t\t\t\tnr += 1;\n\t\t\t\t}\n\t\t\t\tnb = s;\n\t\t\t}\n\t\t}\n\t}\n\n\tdst->nr = nr;\n}\n\nstruct call_return_processor *\ncall_return_processor__new(int (*process)(struct call_return *cr, u64 *parent_db_id, void *data),\n\t\t\t   void *data)\n{\n\tstruct call_return_processor *crp;\n\n\tcrp = zalloc(sizeof(struct call_return_processor));\n\tif (!crp)\n\t\treturn NULL;\n\tcrp->cpr = call_path_root__new();\n\tif (!crp->cpr)\n\t\tgoto out_free;\n\tcrp->process = process;\n\tcrp->data = data;\n\treturn crp;\n\nout_free:\n\tfree(crp);\n\treturn NULL;\n}\n\nvoid call_return_processor__free(struct call_return_processor *crp)\n{\n\tif (crp) {\n\t\tcall_path_root__free(crp->cpr);\n\t\tfree(crp);\n\t}\n}\n\nstatic int thread_stack__push_cp(struct thread_stack *ts, u64 ret_addr,\n\t\t\t\t u64 timestamp, u64 ref, struct call_path *cp,\n\t\t\t\t bool no_call, bool trace_end)\n{\n\tstruct thread_stack_entry *tse;\n\tint err;\n\n\tif (!cp)\n\t\treturn -ENOMEM;\n\n\tif (ts->cnt == ts->sz) {\n\t\terr = thread_stack__grow(ts);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\ttse = &ts->stack[ts->cnt++];\n\ttse->ret_addr = ret_addr;\n\ttse->timestamp = timestamp;\n\ttse->ref = ref;\n\ttse->branch_count = ts->branch_count;\n\ttse->insn_count = ts->insn_count;\n\ttse->cyc_count = ts->cyc_count;\n\ttse->cp = cp;\n\ttse->no_call = no_call;\n\ttse->trace_end = trace_end;\n\ttse->non_call = false;\n\ttse->db_id = 0;\n\n\treturn 0;\n}\n\nstatic int thread_stack__pop_cp(struct thread *thread, struct thread_stack *ts,\n\t\t\t\tu64 ret_addr, u64 timestamp, u64 ref,\n\t\t\t\tstruct symbol *sym)\n{\n\tint err;\n\n\tif (!ts->cnt)\n\t\treturn 1;\n\n\tif (ts->cnt == 1) {\n\t\tstruct thread_stack_entry *tse = &ts->stack[0];\n\n\t\tif (tse->cp->sym == sym)\n\t\t\treturn thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\t\t timestamp, ref, false);\n\t}\n\n\tif (ts->stack[ts->cnt - 1].ret_addr == ret_addr &&\n\t    !ts->stack[ts->cnt - 1].non_call) {\n\t\treturn thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\t timestamp, ref, false);\n\t} else {\n\t\tsize_t i = ts->cnt - 1;\n\n\t\twhile (i--) {\n\t\t\tif (ts->stack[i].ret_addr != ret_addr ||\n\t\t\t    ts->stack[i].non_call)\n\t\t\t\tcontinue;\n\t\t\ti += 1;\n\t\t\twhile (ts->cnt > i) {\n\t\t\t\terr = thread_stack__call_return(thread, ts,\n\t\t\t\t\t\t\t\t--ts->cnt,\n\t\t\t\t\t\t\t\ttimestamp, ref,\n\t\t\t\t\t\t\t\ttrue);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t\treturn thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\t\t timestamp, ref, false);\n\t\t}\n\t}\n\n\treturn 1;\n}\n\nstatic int thread_stack__bottom(struct thread_stack *ts,\n\t\t\t\tstruct perf_sample *sample,\n\t\t\t\tstruct addr_location *from_al,\n\t\t\t\tstruct addr_location *to_al, u64 ref)\n{\n\tstruct call_path_root *cpr = ts->crp->cpr;\n\tstruct call_path *cp;\n\tstruct symbol *sym;\n\tu64 ip;\n\n\tif (sample->ip) {\n\t\tip = sample->ip;\n\t\tsym = from_al->sym;\n\t} else if (sample->addr) {\n\t\tip = sample->addr;\n\t\tsym = to_al->sym;\n\t} else {\n\t\treturn 0;\n\t}\n\n\tcp = call_path__findnew(cpr, &cpr->call_path, sym, ip,\n\t\t\t\tts->kernel_start);\n\n\treturn thread_stack__push_cp(ts, ip, sample->time, ref, cp,\n\t\t\t\t     true, false);\n}\n\nstatic int thread_stack__pop_ks(struct thread *thread, struct thread_stack *ts,\n\t\t\t\tstruct perf_sample *sample, u64 ref)\n{\n\tu64 tm = sample->time;\n\tint err;\n\n\t \n\twhile (thread_stack__in_kernel(ts)) {\n\t\terr = thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\ttm, ref, true);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int thread_stack__no_call_return(struct thread *thread,\n\t\t\t\t\tstruct thread_stack *ts,\n\t\t\t\t\tstruct perf_sample *sample,\n\t\t\t\t\tstruct addr_location *from_al,\n\t\t\t\t\tstruct addr_location *to_al, u64 ref)\n{\n\tstruct call_path_root *cpr = ts->crp->cpr;\n\tstruct call_path *root = &cpr->call_path;\n\tstruct symbol *fsym = from_al->sym;\n\tstruct symbol *tsym = to_al->sym;\n\tstruct call_path *cp, *parent;\n\tu64 ks = ts->kernel_start;\n\tu64 addr = sample->addr;\n\tu64 tm = sample->time;\n\tu64 ip = sample->ip;\n\tint err;\n\n\tif (ip >= ks && addr < ks) {\n\t\t \n\t\terr = thread_stack__pop_ks(thread, ts, sample, ref);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\tif (!ts->cnt) {\n\t\t\tcp = call_path__findnew(cpr, root, tsym, addr, ks);\n\t\t\treturn thread_stack__push_cp(ts, 0, tm, ref, cp, true,\n\t\t\t\t\t\t     false);\n\t\t}\n\t} else if (thread_stack__in_kernel(ts) && ip < ks) {\n\t\t \n\t\terr = thread_stack__pop_ks(thread, ts, sample, ref);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (ts->cnt)\n\t\tparent = ts->stack[ts->cnt - 1].cp;\n\telse\n\t\tparent = root;\n\n\tif (parent->sym == from_al->sym) {\n\t\t \n\t\tif (ts->cnt == 1) {\n\t\t\terr = thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\t\ttm, ref, false);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (!ts->cnt) {\n\t\t\tcp = call_path__findnew(cpr, root, tsym, addr, ks);\n\n\t\t\treturn thread_stack__push_cp(ts, addr, tm, ref, cp,\n\t\t\t\t\t\t     true, false);\n\t\t}\n\n\t\t \n\t\tcp = call_path__findnew(cpr, parent, tsym, addr, ks);\n\n\t\terr = thread_stack__push_cp(ts, 0, tm, ref, cp, true, false);\n\t\tif (!err)\n\t\t\tts->stack[ts->cnt - 1].non_call = true;\n\n\t\treturn err;\n\t}\n\n\t \n\n\tcp = call_path__findnew(cpr, parent, tsym, addr, ks);\n\n\terr = thread_stack__push_cp(ts, addr, tm, ref, cp, true, false);\n\tif (err)\n\t\treturn err;\n\n\tcp = call_path__findnew(cpr, cp, fsym, ip, ks);\n\n\terr = thread_stack__push_cp(ts, ip, tm, ref, cp, true, false);\n\tif (err)\n\t\treturn err;\n\n\treturn thread_stack__call_return(thread, ts, --ts->cnt, tm, ref, false);\n}\n\nstatic int thread_stack__trace_begin(struct thread *thread,\n\t\t\t\t     struct thread_stack *ts, u64 timestamp,\n\t\t\t\t     u64 ref)\n{\n\tstruct thread_stack_entry *tse;\n\tint err;\n\n\tif (!ts->cnt)\n\t\treturn 0;\n\n\t \n\ttse = &ts->stack[ts->cnt - 1];\n\tif (tse->trace_end) {\n\t\terr = thread_stack__call_return(thread, ts, --ts->cnt,\n\t\t\t\t\t\ttimestamp, ref, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int thread_stack__trace_end(struct thread_stack *ts,\n\t\t\t\t   struct perf_sample *sample, u64 ref)\n{\n\tstruct call_path_root *cpr = ts->crp->cpr;\n\tstruct call_path *cp;\n\tu64 ret_addr;\n\n\t \n\tif (!ts->cnt || (ts->cnt == 1 && ts->stack[0].ref == ref))\n\t\treturn 0;\n\n\tcp = call_path__findnew(cpr, ts->stack[ts->cnt - 1].cp, NULL, 0,\n\t\t\t\tts->kernel_start);\n\n\tret_addr = sample->ip + sample->insn_len;\n\n\treturn thread_stack__push_cp(ts, ret_addr, sample->time, ref, cp,\n\t\t\t\t     false, true);\n}\n\nstatic bool is_x86_retpoline(const char *name)\n{\n\treturn strstr(name, \"__x86_indirect_thunk_\") == name;\n}\n\n \nstatic int thread_stack__x86_retpoline(struct thread_stack *ts,\n\t\t\t\t       struct perf_sample *sample,\n\t\t\t\t       struct addr_location *to_al)\n{\n\tstruct thread_stack_entry *tse = &ts->stack[ts->cnt - 1];\n\tstruct call_path_root *cpr = ts->crp->cpr;\n\tstruct symbol *sym = tse->cp->sym;\n\tstruct symbol *tsym = to_al->sym;\n\tstruct call_path *cp;\n\n\tif (sym && is_x86_retpoline(sym->name)) {\n\t\t \n\t\tts->cnt -= 1;\n\t\tsym = ts->stack[ts->cnt - 2].cp->sym;\n\t\tif (sym && sym == tsym && to_al->addr != tsym->start) {\n\t\t\t \n\t\t\tts->cnt -= 1;\n\t\t\treturn 0;\n\t\t}\n\t} else if (sym && sym == tsym) {\n\t\t \n\t\tts->cnt -= 1;\n\t\treturn 0;\n\t}\n\n\tcp = call_path__findnew(cpr, ts->stack[ts->cnt - 2].cp, tsym,\n\t\t\t\tsample->addr, ts->kernel_start);\n\tif (!cp)\n\t\treturn -ENOMEM;\n\n\t \n\tts->stack[ts->cnt - 1].cp = cp;\n\n\treturn 0;\n}\n\nint thread_stack__process(struct thread *thread, struct comm *comm,\n\t\t\t  struct perf_sample *sample,\n\t\t\t  struct addr_location *from_al,\n\t\t\t  struct addr_location *to_al, u64 ref,\n\t\t\t  struct call_return_processor *crp)\n{\n\tstruct thread_stack *ts = thread__stack(thread, sample->cpu);\n\tenum retpoline_state_t rstate;\n\tint err = 0;\n\n\tif (ts && !ts->crp) {\n\t\t \n\t\tthread_stack__reset(thread, ts);\n\t\tts = NULL;\n\t}\n\n\tif (!ts) {\n\t\tts = thread_stack__new(thread, sample->cpu, crp, true, 0);\n\t\tif (!ts)\n\t\t\treturn -ENOMEM;\n\t\tts->comm = comm;\n\t}\n\n\trstate = ts->rstate;\n\tif (rstate == X86_RETPOLINE_DETECTED)\n\t\tts->rstate = X86_RETPOLINE_POSSIBLE;\n\n\t \n\tif (ts->comm != comm && thread__pid(thread) == thread__tid(thread)) {\n\t\terr = __thread_stack__flush(thread, ts);\n\t\tif (err)\n\t\t\treturn err;\n\t\tts->comm = comm;\n\t}\n\n\t \n\tif (!ts->cnt) {\n\t\terr = thread_stack__bottom(ts, sample, from_al, to_al, ref);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tts->branch_count += 1;\n\tts->insn_count += sample->insn_cnt;\n\tts->cyc_count += sample->cyc_cnt;\n\tts->last_time = sample->time;\n\n\tif (sample->flags & PERF_IP_FLAG_CALL) {\n\t\tbool trace_end = sample->flags & PERF_IP_FLAG_TRACE_END;\n\t\tstruct call_path_root *cpr = ts->crp->cpr;\n\t\tstruct call_path *cp;\n\t\tu64 ret_addr;\n\n\t\tif (!sample->ip || !sample->addr)\n\t\t\treturn 0;\n\n\t\tret_addr = sample->ip + sample->insn_len;\n\t\tif (ret_addr == sample->addr)\n\t\t\treturn 0;  \n\n\t\tcp = call_path__findnew(cpr, ts->stack[ts->cnt - 1].cp,\n\t\t\t\t\tto_al->sym, sample->addr,\n\t\t\t\t\tts->kernel_start);\n\t\terr = thread_stack__push_cp(ts, ret_addr, sample->time, ref,\n\t\t\t\t\t    cp, false, trace_end);\n\n\t\t \n\t\tif (!err && rstate == X86_RETPOLINE_POSSIBLE && to_al->sym &&\n\t\t    from_al->sym == to_al->sym &&\n\t\t    to_al->addr != to_al->sym->start)\n\t\t\tts->rstate = X86_RETPOLINE_DETECTED;\n\n\t} else if (sample->flags & PERF_IP_FLAG_RETURN) {\n\t\tif (!sample->addr) {\n\t\t\tu32 return_from_kernel = PERF_IP_FLAG_SYSCALLRET |\n\t\t\t\t\t\t PERF_IP_FLAG_INTERRUPT;\n\n\t\t\tif (!(sample->flags & return_from_kernel))\n\t\t\t\treturn 0;\n\n\t\t\t \n\t\t\treturn thread_stack__pop_ks(thread, ts, sample, ref);\n\t\t}\n\n\t\tif (!sample->ip)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (rstate == X86_RETPOLINE_DETECTED && ts->cnt > 2 &&\n\t\t    ts->stack[ts->cnt - 1].ret_addr != sample->addr)\n\t\t\treturn thread_stack__x86_retpoline(ts, sample, to_al);\n\n\t\terr = thread_stack__pop_cp(thread, ts, sample->addr,\n\t\t\t\t\t   sample->time, ref, from_al->sym);\n\t\tif (err) {\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t\terr = thread_stack__no_call_return(thread, ts, sample,\n\t\t\t\t\t\t\t   from_al, to_al, ref);\n\t\t}\n\t} else if (sample->flags & PERF_IP_FLAG_TRACE_BEGIN) {\n\t\terr = thread_stack__trace_begin(thread, ts, sample->time, ref);\n\t} else if (sample->flags & PERF_IP_FLAG_TRACE_END) {\n\t\terr = thread_stack__trace_end(ts, sample, ref);\n\t} else if (sample->flags & PERF_IP_FLAG_BRANCH &&\n\t\t   from_al->sym != to_al->sym && to_al->sym &&\n\t\t   to_al->addr == to_al->sym->start) {\n\t\tstruct call_path_root *cpr = ts->crp->cpr;\n\t\tstruct call_path *cp;\n\n\t\t \n\t\tcp = call_path__findnew(cpr, ts->stack[ts->cnt - 1].cp,\n\t\t\t\t\tto_al->sym, sample->addr,\n\t\t\t\t\tts->kernel_start);\n\t\terr = thread_stack__push_cp(ts, 0, sample->time, ref, cp, false,\n\t\t\t\t\t    false);\n\t\tif (!err)\n\t\t\tts->stack[ts->cnt - 1].non_call = true;\n\t}\n\n\treturn err;\n}\n\nsize_t thread_stack__depth(struct thread *thread, int cpu)\n{\n\tstruct thread_stack *ts = thread__stack(thread, cpu);\n\n\tif (!ts)\n\t\treturn 0;\n\treturn ts->cnt;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}