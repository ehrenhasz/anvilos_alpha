{
  "module_name": "cpumap.c",
  "hash_id": "266b3e34ffb95cb1c10f0d213ba27441695b83e618643d044dab9bdf3b432eb6",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/util/cpumap.c",
  "human_readable_source": "\n#include <api/fs/fs.h>\n#include \"cpumap.h\"\n#include \"debug.h\"\n#include \"event.h\"\n#include <assert.h>\n#include <dirent.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <linux/bitmap.h>\n#include \"asm/bug.h\"\n\n#include <linux/ctype.h>\n#include <linux/zalloc.h>\n#include <internal/cpumap.h>\n\nstatic struct perf_cpu max_cpu_num;\nstatic struct perf_cpu max_present_cpu_num;\nstatic int max_node_num;\n \nstatic int *cpunode_map;\n\nbool perf_record_cpu_map_data__test_bit(int i,\n\t\t\t\t\tconst struct perf_record_cpu_map_data *data)\n{\n\tint bit_word32 = i / 32;\n\t__u32 bit_mask32 = 1U << (i & 31);\n\tint bit_word64 = i / 64;\n\t__u64 bit_mask64 = ((__u64)1) << (i & 63);\n\n\treturn (data->mask32_data.long_size == 4)\n\t\t? (bit_word32 < data->mask32_data.nr) &&\n\t\t(data->mask32_data.mask[bit_word32] & bit_mask32) != 0\n\t\t: (bit_word64 < data->mask64_data.nr) &&\n\t\t(data->mask64_data.mask[bit_word64] & bit_mask64) != 0;\n}\n\n \nstatic void perf_record_cpu_map_data__read_one_mask(const struct perf_record_cpu_map_data *data,\n\t\t\t\t\t\t    int i, unsigned long *bitmap)\n{\n#if __SIZEOF_LONG__ == 8\n\tif (data->mask32_data.long_size == 4)\n\t\tbitmap[0] = data->mask32_data.mask[i];\n\telse\n\t\tbitmap[0] = data->mask64_data.mask[i];\n#else\n\tif (data->mask32_data.long_size == 4) {\n\t\tbitmap[0] = data->mask32_data.mask[i];\n\t\tbitmap[1] = 0;\n\t} else {\n#if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n\t\tbitmap[0] = (unsigned long)(data->mask64_data.mask[i] >> 32);\n\t\tbitmap[1] = (unsigned long)data->mask64_data.mask[i];\n#else\n\t\tbitmap[0] = (unsigned long)data->mask64_data.mask[i];\n\t\tbitmap[1] = (unsigned long)(data->mask64_data.mask[i] >> 32);\n#endif\n\t}\n#endif\n}\nstatic struct perf_cpu_map *cpu_map__from_entries(const struct perf_record_cpu_map_data *data)\n{\n\tstruct perf_cpu_map *map;\n\n\tmap = perf_cpu_map__empty_new(data->cpus_data.nr);\n\tif (map) {\n\t\tunsigned i;\n\n\t\tfor (i = 0; i < data->cpus_data.nr; i++) {\n\t\t\t \n\t\t\tif (data->cpus_data.cpu[i] == (u16) -1)\n\t\t\t\tRC_CHK_ACCESS(map)->map[i].cpu = -1;\n\t\t\telse\n\t\t\t\tRC_CHK_ACCESS(map)->map[i].cpu = (int) data->cpus_data.cpu[i];\n\t\t}\n\t}\n\n\treturn map;\n}\n\nstatic struct perf_cpu_map *cpu_map__from_mask(const struct perf_record_cpu_map_data *data)\n{\n\tDECLARE_BITMAP(local_copy, 64);\n\tint weight = 0, mask_nr = data->mask32_data.nr;\n\tstruct perf_cpu_map *map;\n\n\tfor (int i = 0; i < mask_nr; i++) {\n\t\tperf_record_cpu_map_data__read_one_mask(data, i, local_copy);\n\t\tweight += bitmap_weight(local_copy, 64);\n\t}\n\n\tmap = perf_cpu_map__empty_new(weight);\n\tif (!map)\n\t\treturn NULL;\n\n\tfor (int i = 0, j = 0; i < mask_nr; i++) {\n\t\tint cpus_per_i = (i * data->mask32_data.long_size  * BITS_PER_BYTE);\n\t\tint cpu;\n\n\t\tperf_record_cpu_map_data__read_one_mask(data, i, local_copy);\n\t\tfor_each_set_bit(cpu, local_copy, 64)\n\t\t\tRC_CHK_ACCESS(map)->map[j++].cpu = cpu + cpus_per_i;\n\t}\n\treturn map;\n\n}\n\nstatic struct perf_cpu_map *cpu_map__from_range(const struct perf_record_cpu_map_data *data)\n{\n\tstruct perf_cpu_map *map;\n\tunsigned int i = 0;\n\n\tmap = perf_cpu_map__empty_new(data->range_cpu_data.end_cpu -\n\t\t\t\tdata->range_cpu_data.start_cpu + 1 + data->range_cpu_data.any_cpu);\n\tif (!map)\n\t\treturn NULL;\n\n\tif (data->range_cpu_data.any_cpu)\n\t\tRC_CHK_ACCESS(map)->map[i++].cpu = -1;\n\n\tfor (int cpu = data->range_cpu_data.start_cpu; cpu <= data->range_cpu_data.end_cpu;\n\t     i++, cpu++)\n\t\tRC_CHK_ACCESS(map)->map[i].cpu = cpu;\n\n\treturn map;\n}\n\nstruct perf_cpu_map *cpu_map__new_data(const struct perf_record_cpu_map_data *data)\n{\n\tswitch (data->type) {\n\tcase PERF_CPU_MAP__CPUS:\n\t\treturn cpu_map__from_entries(data);\n\tcase PERF_CPU_MAP__MASK:\n\t\treturn cpu_map__from_mask(data);\n\tcase PERF_CPU_MAP__RANGE_CPUS:\n\t\treturn cpu_map__from_range(data);\n\tdefault:\n\t\tpr_err(\"cpu_map__new_data unknown type %d\\n\", data->type);\n\t\treturn NULL;\n\t}\n}\n\nsize_t cpu_map__fprintf(struct perf_cpu_map *map, FILE *fp)\n{\n#define BUFSIZE 1024\n\tchar buf[BUFSIZE];\n\n\tcpu_map__snprint(map, buf, sizeof(buf));\n\treturn fprintf(fp, \"%s\\n\", buf);\n#undef BUFSIZE\n}\n\nstruct perf_cpu_map *perf_cpu_map__empty_new(int nr)\n{\n\tstruct perf_cpu_map *cpus = perf_cpu_map__alloc(nr);\n\n\tif (cpus != NULL) {\n\t\tfor (int i = 0; i < nr; i++)\n\t\t\tRC_CHK_ACCESS(cpus)->map[i].cpu = -1;\n\t}\n\n\treturn cpus;\n}\n\nstruct cpu_aggr_map *cpu_aggr_map__empty_new(int nr)\n{\n\tstruct cpu_aggr_map *cpus = malloc(sizeof(*cpus) + sizeof(struct aggr_cpu_id) * nr);\n\n\tif (cpus != NULL) {\n\t\tint i;\n\n\t\tcpus->nr = nr;\n\t\tfor (i = 0; i < nr; i++)\n\t\t\tcpus->map[i] = aggr_cpu_id__empty();\n\n\t\trefcount_set(&cpus->refcnt, 1);\n\t}\n\n\treturn cpus;\n}\n\nstatic int cpu__get_topology_int(int cpu, const char *name, int *value)\n{\n\tchar path[PATH_MAX];\n\n\tsnprintf(path, PATH_MAX,\n\t\t\"devices/system/cpu/cpu%d/topology/%s\", cpu, name);\n\n\treturn sysfs__read_int(path, value);\n}\n\nint cpu__get_socket_id(struct perf_cpu cpu)\n{\n\tint value, ret = cpu__get_topology_int(cpu.cpu, \"physical_package_id\", &value);\n\treturn ret ?: value;\n}\n\nstruct aggr_cpu_id aggr_cpu_id__socket(struct perf_cpu cpu, void *data __maybe_unused)\n{\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tid.socket = cpu__get_socket_id(cpu);\n\treturn id;\n}\n\nstatic int aggr_cpu_id__cmp(const void *a_pointer, const void *b_pointer)\n{\n\tstruct aggr_cpu_id *a = (struct aggr_cpu_id *)a_pointer;\n\tstruct aggr_cpu_id *b = (struct aggr_cpu_id *)b_pointer;\n\n\tif (a->node != b->node)\n\t\treturn a->node - b->node;\n\telse if (a->socket != b->socket)\n\t\treturn a->socket - b->socket;\n\telse if (a->die != b->die)\n\t\treturn a->die - b->die;\n\telse if (a->cache_lvl != b->cache_lvl)\n\t\treturn a->cache_lvl - b->cache_lvl;\n\telse if (a->cache != b->cache)\n\t\treturn a->cache - b->cache;\n\telse if (a->core != b->core)\n\t\treturn a->core - b->core;\n\telse\n\t\treturn a->thread_idx - b->thread_idx;\n}\n\nstruct cpu_aggr_map *cpu_aggr_map__new(const struct perf_cpu_map *cpus,\n\t\t\t\t       aggr_cpu_id_get_t get_id,\n\t\t\t\t       void *data, bool needs_sort)\n{\n\tint idx;\n\tstruct perf_cpu cpu;\n\tstruct cpu_aggr_map *c = cpu_aggr_map__empty_new(perf_cpu_map__nr(cpus));\n\n\tif (!c)\n\t\treturn NULL;\n\n\t \n\tc->nr = 0;\n\n\tperf_cpu_map__for_each_cpu(cpu, idx, cpus) {\n\t\tbool duplicate = false;\n\t\tstruct aggr_cpu_id cpu_id = get_id(cpu, data);\n\n\t\tfor (int j = 0; j < c->nr; j++) {\n\t\t\tif (aggr_cpu_id__equal(&cpu_id, &c->map[j])) {\n\t\t\t\tduplicate = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!duplicate) {\n\t\t\tc->map[c->nr] = cpu_id;\n\t\t\tc->nr++;\n\t\t}\n\t}\n\t \n\tif (c->nr != perf_cpu_map__nr(cpus)) {\n\t\tstruct cpu_aggr_map *trimmed_c =\n\t\t\trealloc(c,\n\t\t\t\tsizeof(struct cpu_aggr_map) + sizeof(struct aggr_cpu_id) * c->nr);\n\n\t\tif (trimmed_c)\n\t\t\tc = trimmed_c;\n\t}\n\n\t \n\tif (needs_sort)\n\t\tqsort(c->map, c->nr, sizeof(struct aggr_cpu_id), aggr_cpu_id__cmp);\n\n\treturn c;\n\n}\n\nint cpu__get_die_id(struct perf_cpu cpu)\n{\n\tint value, ret = cpu__get_topology_int(cpu.cpu, \"die_id\", &value);\n\n\treturn ret ?: value;\n}\n\nstruct aggr_cpu_id aggr_cpu_id__die(struct perf_cpu cpu, void *data)\n{\n\tstruct aggr_cpu_id id;\n\tint die;\n\n\tdie = cpu__get_die_id(cpu);\n\t \n\tif (die == -1)\n\t\tdie = 0;\n\n\t \n\tid = aggr_cpu_id__socket(cpu, data);\n\tif (aggr_cpu_id__is_empty(&id))\n\t\treturn id;\n\n\tid.die = die;\n\treturn id;\n}\n\nint cpu__get_core_id(struct perf_cpu cpu)\n{\n\tint value, ret = cpu__get_topology_int(cpu.cpu, \"core_id\", &value);\n\treturn ret ?: value;\n}\n\nstruct aggr_cpu_id aggr_cpu_id__core(struct perf_cpu cpu, void *data)\n{\n\tstruct aggr_cpu_id id;\n\tint core = cpu__get_core_id(cpu);\n\n\t \n\tid = aggr_cpu_id__die(cpu, data);\n\tif (aggr_cpu_id__is_empty(&id))\n\t\treturn id;\n\n\t \n\tid.core = core;\n\treturn id;\n\n}\n\nstruct aggr_cpu_id aggr_cpu_id__cpu(struct perf_cpu cpu, void *data)\n{\n\tstruct aggr_cpu_id id;\n\n\t \n\tid = aggr_cpu_id__core(cpu, data);\n\tif (aggr_cpu_id__is_empty(&id))\n\t\treturn id;\n\n\tid.cpu = cpu;\n\treturn id;\n\n}\n\nstruct aggr_cpu_id aggr_cpu_id__node(struct perf_cpu cpu, void *data __maybe_unused)\n{\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tid.node = cpu__get_node(cpu);\n\treturn id;\n}\n\nstruct aggr_cpu_id aggr_cpu_id__global(struct perf_cpu cpu, void *data __maybe_unused)\n{\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\t \n\tcpu.cpu = 0;\n\tid.cpu = cpu;\n\treturn id;\n}\n\n \nstatic int get_max_num(char *path, int *max)\n{\n\tsize_t num;\n\tchar *buf;\n\tint err = 0;\n\n\tif (filename__read_str(path, &buf, &num))\n\t\treturn -1;\n\n\tbuf[num] = '\\0';\n\n\t \n\twhile (--num) {\n\t\tif ((buf[num] == ',') || (buf[num] == '-')) {\n\t\t\tnum++;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (sscanf(&buf[num], \"%d\", max) < 1) {\n\t\terr = -1;\n\t\tgoto out;\n\t}\n\n\t \n\t(*max)++;\n\nout:\n\tfree(buf);\n\treturn err;\n}\n\n \nstatic void set_max_cpu_num(void)\n{\n\tconst char *mnt;\n\tchar path[PATH_MAX];\n\tint ret = -1;\n\n\t \n\tmax_cpu_num.cpu = 4096;\n\tmax_present_cpu_num.cpu = 4096;\n\n\tmnt = sysfs__mountpoint();\n\tif (!mnt)\n\t\tgoto out;\n\n\t \n\tret = snprintf(path, PATH_MAX, \"%s/devices/system/cpu/possible\", mnt);\n\tif (ret >= PATH_MAX) {\n\t\tpr_err(\"sysfs path crossed PATH_MAX(%d) size\\n\", PATH_MAX);\n\t\tgoto out;\n\t}\n\n\tret = get_max_num(path, &max_cpu_num.cpu);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = snprintf(path, PATH_MAX, \"%s/devices/system/cpu/present\", mnt);\n\tif (ret >= PATH_MAX) {\n\t\tpr_err(\"sysfs path crossed PATH_MAX(%d) size\\n\", PATH_MAX);\n\t\tgoto out;\n\t}\n\n\tret = get_max_num(path, &max_present_cpu_num.cpu);\n\nout:\n\tif (ret)\n\t\tpr_err(\"Failed to read max cpus, using default of %d\\n\", max_cpu_num.cpu);\n}\n\n \nstatic void set_max_node_num(void)\n{\n\tconst char *mnt;\n\tchar path[PATH_MAX];\n\tint ret = -1;\n\n\t \n\tmax_node_num = 8;\n\n\tmnt = sysfs__mountpoint();\n\tif (!mnt)\n\t\tgoto out;\n\n\t \n\tret = snprintf(path, PATH_MAX, \"%s/devices/system/node/possible\", mnt);\n\tif (ret >= PATH_MAX) {\n\t\tpr_err(\"sysfs path crossed PATH_MAX(%d) size\\n\", PATH_MAX);\n\t\tgoto out;\n\t}\n\n\tret = get_max_num(path, &max_node_num);\n\nout:\n\tif (ret)\n\t\tpr_err(\"Failed to read max nodes, using default of %d\\n\", max_node_num);\n}\n\nint cpu__max_node(void)\n{\n\tif (unlikely(!max_node_num))\n\t\tset_max_node_num();\n\n\treturn max_node_num;\n}\n\nstruct perf_cpu cpu__max_cpu(void)\n{\n\tif (unlikely(!max_cpu_num.cpu))\n\t\tset_max_cpu_num();\n\n\treturn max_cpu_num;\n}\n\nstruct perf_cpu cpu__max_present_cpu(void)\n{\n\tif (unlikely(!max_present_cpu_num.cpu))\n\t\tset_max_cpu_num();\n\n\treturn max_present_cpu_num;\n}\n\n\nint cpu__get_node(struct perf_cpu cpu)\n{\n\tif (unlikely(cpunode_map == NULL)) {\n\t\tpr_debug(\"cpu_map not initialized\\n\");\n\t\treturn -1;\n\t}\n\n\treturn cpunode_map[cpu.cpu];\n}\n\nstatic int init_cpunode_map(void)\n{\n\tint i;\n\n\tset_max_cpu_num();\n\tset_max_node_num();\n\n\tcpunode_map = calloc(max_cpu_num.cpu, sizeof(int));\n\tif (!cpunode_map) {\n\t\tpr_err(\"%s: calloc failed\\n\", __func__);\n\t\treturn -1;\n\t}\n\n\tfor (i = 0; i < max_cpu_num.cpu; i++)\n\t\tcpunode_map[i] = -1;\n\n\treturn 0;\n}\n\nint cpu__setup_cpunode_map(void)\n{\n\tstruct dirent *dent1, *dent2;\n\tDIR *dir1, *dir2;\n\tunsigned int cpu, mem;\n\tchar buf[PATH_MAX];\n\tchar path[PATH_MAX];\n\tconst char *mnt;\n\tint n;\n\n\t \n\tif (init_cpunode_map())\n\t\treturn -1;\n\n\tmnt = sysfs__mountpoint();\n\tif (!mnt)\n\t\treturn 0;\n\n\tn = snprintf(path, PATH_MAX, \"%s/devices/system/node\", mnt);\n\tif (n >= PATH_MAX) {\n\t\tpr_err(\"sysfs path crossed PATH_MAX(%d) size\\n\", PATH_MAX);\n\t\treturn -1;\n\t}\n\n\tdir1 = opendir(path);\n\tif (!dir1)\n\t\treturn 0;\n\n\t \n\twhile ((dent1 = readdir(dir1)) != NULL) {\n\t\tif (dent1->d_type != DT_DIR || sscanf(dent1->d_name, \"node%u\", &mem) < 1)\n\t\t\tcontinue;\n\n\t\tn = snprintf(buf, PATH_MAX, \"%s/%s\", path, dent1->d_name);\n\t\tif (n >= PATH_MAX) {\n\t\t\tpr_err(\"sysfs path crossed PATH_MAX(%d) size\\n\", PATH_MAX);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdir2 = opendir(buf);\n\t\tif (!dir2)\n\t\t\tcontinue;\n\t\twhile ((dent2 = readdir(dir2)) != NULL) {\n\t\t\tif (dent2->d_type != DT_LNK || sscanf(dent2->d_name, \"cpu%u\", &cpu) < 1)\n\t\t\t\tcontinue;\n\t\t\tcpunode_map[cpu] = mem;\n\t\t}\n\t\tclosedir(dir2);\n\t}\n\tclosedir(dir1);\n\treturn 0;\n}\n\nsize_t cpu_map__snprint(struct perf_cpu_map *map, char *buf, size_t size)\n{\n\tint i, start = -1;\n\tbool first = true;\n\tsize_t ret = 0;\n\n#define COMMA first ? \"\" : \",\"\n\n\tfor (i = 0; i < perf_cpu_map__nr(map) + 1; i++) {\n\t\tstruct perf_cpu cpu = { .cpu = INT_MAX };\n\t\tbool last = i == perf_cpu_map__nr(map);\n\n\t\tif (!last)\n\t\t\tcpu = perf_cpu_map__cpu(map, i);\n\n\t\tif (start == -1) {\n\t\t\tstart = i;\n\t\t\tif (last) {\n\t\t\t\tret += snprintf(buf + ret, size - ret,\n\t\t\t\t\t\t\"%s%d\", COMMA,\n\t\t\t\t\t\tperf_cpu_map__cpu(map, i).cpu);\n\t\t\t}\n\t\t} else if (((i - start) != (cpu.cpu - perf_cpu_map__cpu(map, start).cpu)) || last) {\n\t\t\tint end = i - 1;\n\n\t\t\tif (start == end) {\n\t\t\t\tret += snprintf(buf + ret, size - ret,\n\t\t\t\t\t\t\"%s%d\", COMMA,\n\t\t\t\t\t\tperf_cpu_map__cpu(map, start).cpu);\n\t\t\t} else {\n\t\t\t\tret += snprintf(buf + ret, size - ret,\n\t\t\t\t\t\t\"%s%d-%d\", COMMA,\n\t\t\t\t\t\tperf_cpu_map__cpu(map, start).cpu, perf_cpu_map__cpu(map, end).cpu);\n\t\t\t}\n\t\t\tfirst = false;\n\t\t\tstart = i;\n\t\t}\n\t}\n\n#undef COMMA\n\n\tpr_debug2(\"cpumask list: %s\\n\", buf);\n\treturn ret;\n}\n\nstatic char hex_char(unsigned char val)\n{\n\tif (val < 10)\n\t\treturn val + '0';\n\tif (val < 16)\n\t\treturn val - 10 + 'a';\n\treturn '?';\n}\n\nsize_t cpu_map__snprint_mask(struct perf_cpu_map *map, char *buf, size_t size)\n{\n\tint i, cpu;\n\tchar *ptr = buf;\n\tunsigned char *bitmap;\n\tstruct perf_cpu last_cpu = perf_cpu_map__cpu(map, perf_cpu_map__nr(map) - 1);\n\n\tif (buf == NULL)\n\t\treturn 0;\n\n\tbitmap = zalloc(last_cpu.cpu / 8 + 1);\n\tif (bitmap == NULL) {\n\t\tbuf[0] = '\\0';\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < perf_cpu_map__nr(map); i++) {\n\t\tcpu = perf_cpu_map__cpu(map, i).cpu;\n\t\tbitmap[cpu / 8] |= 1 << (cpu % 8);\n\t}\n\n\tfor (cpu = last_cpu.cpu / 4 * 4; cpu >= 0; cpu -= 4) {\n\t\tunsigned char bits = bitmap[cpu / 8];\n\n\t\tif (cpu % 8)\n\t\t\tbits >>= 4;\n\t\telse\n\t\t\tbits &= 0xf;\n\n\t\t*ptr++ = hex_char(bits);\n\t\tif ((cpu % 32) == 0 && cpu > 0)\n\t\t\t*ptr++ = ',';\n\t}\n\t*ptr = '\\0';\n\tfree(bitmap);\n\n\tbuf[size - 1] = '\\0';\n\treturn ptr - buf;\n}\n\nstruct perf_cpu_map *cpu_map__online(void)  \n{\n\tstatic struct perf_cpu_map *online;\n\n\tif (!online)\n\t\tonline = perf_cpu_map__new(NULL);  \n\n\treturn online;\n}\n\nbool aggr_cpu_id__equal(const struct aggr_cpu_id *a, const struct aggr_cpu_id *b)\n{\n\treturn a->thread_idx == b->thread_idx &&\n\t\ta->node == b->node &&\n\t\ta->socket == b->socket &&\n\t\ta->die == b->die &&\n\t\ta->cache_lvl == b->cache_lvl &&\n\t\ta->cache == b->cache &&\n\t\ta->core == b->core &&\n\t\ta->cpu.cpu == b->cpu.cpu;\n}\n\nbool aggr_cpu_id__is_empty(const struct aggr_cpu_id *a)\n{\n\treturn a->thread_idx == -1 &&\n\t\ta->node == -1 &&\n\t\ta->socket == -1 &&\n\t\ta->die == -1 &&\n\t\ta->cache_lvl == -1 &&\n\t\ta->cache == -1 &&\n\t\ta->core == -1 &&\n\t\ta->cpu.cpu == -1;\n}\n\nstruct aggr_cpu_id aggr_cpu_id__empty(void)\n{\n\tstruct aggr_cpu_id ret = {\n\t\t.thread_idx = -1,\n\t\t.node = -1,\n\t\t.socket = -1,\n\t\t.die = -1,\n\t\t.cache_lvl = -1,\n\t\t.cache = -1,\n\t\t.core = -1,\n\t\t.cpu = (struct perf_cpu){ .cpu = -1 },\n\t};\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}