{
  "module_name": "builtin-sched.c",
  "hash_id": "d76a8f004fa19f7706b8d72b22cb4e5d51a67f940bb9ebb680b37d18248a7dc9",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/builtin-sched.c",
  "human_readable_source": "\n#include \"builtin.h\"\n#include \"perf-sys.h\"\n\n#include \"util/cpumap.h\"\n#include \"util/evlist.h\"\n#include \"util/evsel.h\"\n#include \"util/evsel_fprintf.h\"\n#include \"util/mutex.h\"\n#include \"util/symbol.h\"\n#include \"util/thread.h\"\n#include \"util/header.h\"\n#include \"util/session.h\"\n#include \"util/tool.h\"\n#include \"util/cloexec.h\"\n#include \"util/thread_map.h\"\n#include \"util/color.h\"\n#include \"util/stat.h\"\n#include \"util/string2.h\"\n#include \"util/callchain.h\"\n#include \"util/time-utils.h\"\n\n#include <subcmd/pager.h>\n#include <subcmd/parse-options.h>\n#include \"util/trace-event.h\"\n\n#include \"util/debug.h\"\n#include \"util/event.h\"\n#include \"util/util.h\"\n\n#include <linux/kernel.h>\n#include <linux/log2.h>\n#include <linux/zalloc.h>\n#include <sys/prctl.h>\n#include <sys/resource.h>\n#include <inttypes.h>\n\n#include <errno.h>\n#include <semaphore.h>\n#include <pthread.h>\n#include <math.h>\n#include <api/fs/fs.h>\n#include <perf/cpumap.h>\n#include <linux/time64.h>\n#include <linux/err.h>\n\n#include <linux/ctype.h>\n\n#define PR_SET_NAME\t\t15                \n#define MAX_CPUS\t\t4096\n#define COMM_LEN\t\t20\n#define SYM_LEN\t\t\t129\n#define MAX_PID\t\t\t1024000\n\nstatic const char *cpu_list;\nstatic DECLARE_BITMAP(cpu_bitmap, MAX_NR_CPUS);\n\nstruct sched_atom;\n\nstruct task_desc {\n\tunsigned long\t\tnr;\n\tunsigned long\t\tpid;\n\tchar\t\t\tcomm[COMM_LEN];\n\n\tunsigned long\t\tnr_events;\n\tunsigned long\t\tcurr_event;\n\tstruct sched_atom\t**atoms;\n\n\tpthread_t\t\tthread;\n\tsem_t\t\t\tsleep_sem;\n\n\tsem_t\t\t\tready_for_work;\n\tsem_t\t\t\twork_done_sem;\n\n\tu64\t\t\tcpu_usage;\n};\n\nenum sched_event_type {\n\tSCHED_EVENT_RUN,\n\tSCHED_EVENT_SLEEP,\n\tSCHED_EVENT_WAKEUP,\n\tSCHED_EVENT_MIGRATION,\n};\n\nstruct sched_atom {\n\tenum sched_event_type\ttype;\n\tint\t\t\tspecific_wait;\n\tu64\t\t\ttimestamp;\n\tu64\t\t\tduration;\n\tunsigned long\t\tnr;\n\tsem_t\t\t\t*wait_sem;\n\tstruct task_desc\t*wakee;\n};\n\n#define TASK_STATE_TO_CHAR_STR \"RSDTtZXxKWP\"\n\n \n#define TASK_RUNNING\t\t0\n#define TASK_INTERRUPTIBLE\t1\n#define TASK_UNINTERRUPTIBLE\t2\n#define __TASK_STOPPED\t\t4\n#define __TASK_TRACED\t\t8\n \n#define EXIT_DEAD\t\t16\n#define EXIT_ZOMBIE\t\t32\n#define EXIT_TRACE\t\t(EXIT_ZOMBIE | EXIT_DEAD)\n \n#define TASK_DEAD\t\t64\n#define TASK_WAKEKILL\t\t128\n#define TASK_WAKING\t\t256\n#define TASK_PARKED\t\t512\n\nenum thread_state {\n\tTHREAD_SLEEPING = 0,\n\tTHREAD_WAIT_CPU,\n\tTHREAD_SCHED_IN,\n\tTHREAD_IGNORE\n};\n\nstruct work_atom {\n\tstruct list_head\tlist;\n\tenum thread_state\tstate;\n\tu64\t\t\tsched_out_time;\n\tu64\t\t\twake_up_time;\n\tu64\t\t\tsched_in_time;\n\tu64\t\t\truntime;\n};\n\nstruct work_atoms {\n\tstruct list_head\twork_list;\n\tstruct thread\t\t*thread;\n\tstruct rb_node\t\tnode;\n\tu64\t\t\tmax_lat;\n\tu64\t\t\tmax_lat_start;\n\tu64\t\t\tmax_lat_end;\n\tu64\t\t\ttotal_lat;\n\tu64\t\t\tnb_atoms;\n\tu64\t\t\ttotal_runtime;\n\tint\t\t\tnum_merged;\n};\n\ntypedef int (*sort_fn_t)(struct work_atoms *, struct work_atoms *);\n\nstruct perf_sched;\n\nstruct trace_sched_handler {\n\tint (*switch_event)(struct perf_sched *sched, struct evsel *evsel,\n\t\t\t    struct perf_sample *sample, struct machine *machine);\n\n\tint (*runtime_event)(struct perf_sched *sched, struct evsel *evsel,\n\t\t\t     struct perf_sample *sample, struct machine *machine);\n\n\tint (*wakeup_event)(struct perf_sched *sched, struct evsel *evsel,\n\t\t\t    struct perf_sample *sample, struct machine *machine);\n\n\t \n\tint (*fork_event)(struct perf_sched *sched, union perf_event *event,\n\t\t\t  struct machine *machine);\n\n\tint (*migrate_task_event)(struct perf_sched *sched,\n\t\t\t\t  struct evsel *evsel,\n\t\t\t\t  struct perf_sample *sample,\n\t\t\t\t  struct machine *machine);\n};\n\n#define COLOR_PIDS PERF_COLOR_BLUE\n#define COLOR_CPUS PERF_COLOR_BG_RED\n\nstruct perf_sched_map {\n\tDECLARE_BITMAP(comp_cpus_mask, MAX_CPUS);\n\tstruct perf_cpu\t\t*comp_cpus;\n\tbool\t\t\t comp;\n\tstruct perf_thread_map *color_pids;\n\tconst char\t\t*color_pids_str;\n\tstruct perf_cpu_map\t*color_cpus;\n\tconst char\t\t*color_cpus_str;\n\tstruct perf_cpu_map\t*cpus;\n\tconst char\t\t*cpus_str;\n};\n\nstruct perf_sched {\n\tstruct perf_tool tool;\n\tconst char\t *sort_order;\n\tunsigned long\t nr_tasks;\n\tstruct task_desc **pid_to_task;\n\tstruct task_desc **tasks;\n\tconst struct trace_sched_handler *tp_handler;\n\tstruct mutex\t start_work_mutex;\n\tstruct mutex\t work_done_wait_mutex;\n\tint\t\t profile_cpu;\n \n\tstruct perf_cpu\t max_cpu;\n\tu32\t\t *curr_pid;\n\tstruct thread\t **curr_thread;\n\tchar\t\t next_shortname1;\n\tchar\t\t next_shortname2;\n\tunsigned int\t replay_repeat;\n\tunsigned long\t nr_run_events;\n\tunsigned long\t nr_sleep_events;\n\tunsigned long\t nr_wakeup_events;\n\tunsigned long\t nr_sleep_corrections;\n\tunsigned long\t nr_run_events_optimized;\n\tunsigned long\t targetless_wakeups;\n\tunsigned long\t multitarget_wakeups;\n\tunsigned long\t nr_runs;\n\tunsigned long\t nr_timestamps;\n\tunsigned long\t nr_unordered_timestamps;\n\tunsigned long\t nr_context_switch_bugs;\n\tunsigned long\t nr_events;\n\tunsigned long\t nr_lost_chunks;\n\tunsigned long\t nr_lost_events;\n\tu64\t\t run_measurement_overhead;\n\tu64\t\t sleep_measurement_overhead;\n\tu64\t\t start_time;\n\tu64\t\t cpu_usage;\n\tu64\t\t runavg_cpu_usage;\n\tu64\t\t parent_cpu_usage;\n\tu64\t\t runavg_parent_cpu_usage;\n\tu64\t\t sum_runtime;\n\tu64\t\t sum_fluct;\n\tu64\t\t run_avg;\n\tu64\t\t all_runtime;\n\tu64\t\t all_count;\n\tu64\t\t *cpu_last_switched;\n\tstruct rb_root_cached atom_root, sorted_atom_root, merged_atom_root;\n\tstruct list_head sort_list, cmp_pid;\n\tbool force;\n\tbool skip_merge;\n\tstruct perf_sched_map map;\n\n\t \n\tbool\t\tsummary;\n\tbool\t\tsummary_only;\n\tbool\t\tidle_hist;\n\tbool\t\tshow_callchain;\n\tunsigned int\tmax_stack;\n\tbool\t\tshow_cpu_visual;\n\tbool\t\tshow_wakeups;\n\tbool\t\tshow_next;\n\tbool\t\tshow_migrations;\n\tbool\t\tshow_state;\n\tu64\t\tskipped_samples;\n\tconst char\t*time_str;\n\tstruct perf_time_interval ptime;\n\tstruct perf_time_interval hist_time;\n\tvolatile bool   thread_funcs_exit;\n};\n\n \nstruct thread_runtime {\n\tu64 last_time;       \n\tu64 dt_run;          \n\tu64 dt_sleep;        \n\tu64 dt_iowait;       \n\tu64 dt_preempt;      \n\tu64 dt_delay;        \n\tu64 ready_to_run;    \n\n\tstruct stats run_stats;\n\tu64 total_run_time;\n\tu64 total_sleep_time;\n\tu64 total_iowait_time;\n\tu64 total_preempt_time;\n\tu64 total_delay_time;\n\n\tint last_state;\n\n\tchar shortname[3];\n\tbool comm_changed;\n\n\tu64 migrations;\n};\n\n \nstruct evsel_runtime {\n\tu64 *last_time;  \n\tu32 ncpu;        \n};\n\n \nstruct idle_thread_runtime {\n\tstruct thread_runtime\ttr;\n\tstruct thread\t\t*last_thread;\n\tstruct rb_root_cached\tsorted_root;\n\tstruct callchain_root\tcallchain;\n\tstruct callchain_cursor\tcursor;\n};\n\n \nstatic struct thread **idle_threads;\nstatic int idle_max_cpu;\nstatic char idle_comm[] = \"<idle>\";\n\nstatic u64 get_nsecs(void)\n{\n\tstruct timespec ts;\n\n\tclock_gettime(CLOCK_MONOTONIC, &ts);\n\n\treturn ts.tv_sec * NSEC_PER_SEC + ts.tv_nsec;\n}\n\nstatic void burn_nsecs(struct perf_sched *sched, u64 nsecs)\n{\n\tu64 T0 = get_nsecs(), T1;\n\n\tdo {\n\t\tT1 = get_nsecs();\n\t} while (T1 + sched->run_measurement_overhead < T0 + nsecs);\n}\n\nstatic void sleep_nsecs(u64 nsecs)\n{\n\tstruct timespec ts;\n\n\tts.tv_nsec = nsecs % 999999999;\n\tts.tv_sec = nsecs / 999999999;\n\n\tnanosleep(&ts, NULL);\n}\n\nstatic void calibrate_run_measurement_overhead(struct perf_sched *sched)\n{\n\tu64 T0, T1, delta, min_delta = NSEC_PER_SEC;\n\tint i;\n\n\tfor (i = 0; i < 10; i++) {\n\t\tT0 = get_nsecs();\n\t\tburn_nsecs(sched, 0);\n\t\tT1 = get_nsecs();\n\t\tdelta = T1-T0;\n\t\tmin_delta = min(min_delta, delta);\n\t}\n\tsched->run_measurement_overhead = min_delta;\n\n\tprintf(\"run measurement overhead: %\" PRIu64 \" nsecs\\n\", min_delta);\n}\n\nstatic void calibrate_sleep_measurement_overhead(struct perf_sched *sched)\n{\n\tu64 T0, T1, delta, min_delta = NSEC_PER_SEC;\n\tint i;\n\n\tfor (i = 0; i < 10; i++) {\n\t\tT0 = get_nsecs();\n\t\tsleep_nsecs(10000);\n\t\tT1 = get_nsecs();\n\t\tdelta = T1-T0;\n\t\tmin_delta = min(min_delta, delta);\n\t}\n\tmin_delta -= 10000;\n\tsched->sleep_measurement_overhead = min_delta;\n\n\tprintf(\"sleep measurement overhead: %\" PRIu64 \" nsecs\\n\", min_delta);\n}\n\nstatic struct sched_atom *\nget_new_event(struct task_desc *task, u64 timestamp)\n{\n\tstruct sched_atom *event = zalloc(sizeof(*event));\n\tunsigned long idx = task->nr_events;\n\tsize_t size;\n\n\tevent->timestamp = timestamp;\n\tevent->nr = idx;\n\n\ttask->nr_events++;\n\tsize = sizeof(struct sched_atom *) * task->nr_events;\n\ttask->atoms = realloc(task->atoms, size);\n\tBUG_ON(!task->atoms);\n\n\ttask->atoms[idx] = event;\n\n\treturn event;\n}\n\nstatic struct sched_atom *last_event(struct task_desc *task)\n{\n\tif (!task->nr_events)\n\t\treturn NULL;\n\n\treturn task->atoms[task->nr_events - 1];\n}\n\nstatic void add_sched_event_run(struct perf_sched *sched, struct task_desc *task,\n\t\t\t\tu64 timestamp, u64 duration)\n{\n\tstruct sched_atom *event, *curr_event = last_event(task);\n\n\t \n\tif (curr_event && curr_event->type == SCHED_EVENT_RUN) {\n\t\tsched->nr_run_events_optimized++;\n\t\tcurr_event->duration += duration;\n\t\treturn;\n\t}\n\n\tevent = get_new_event(task, timestamp);\n\n\tevent->type = SCHED_EVENT_RUN;\n\tevent->duration = duration;\n\n\tsched->nr_run_events++;\n}\n\nstatic void add_sched_event_wakeup(struct perf_sched *sched, struct task_desc *task,\n\t\t\t\t   u64 timestamp, struct task_desc *wakee)\n{\n\tstruct sched_atom *event, *wakee_event;\n\n\tevent = get_new_event(task, timestamp);\n\tevent->type = SCHED_EVENT_WAKEUP;\n\tevent->wakee = wakee;\n\n\twakee_event = last_event(wakee);\n\tif (!wakee_event || wakee_event->type != SCHED_EVENT_SLEEP) {\n\t\tsched->targetless_wakeups++;\n\t\treturn;\n\t}\n\tif (wakee_event->wait_sem) {\n\t\tsched->multitarget_wakeups++;\n\t\treturn;\n\t}\n\n\twakee_event->wait_sem = zalloc(sizeof(*wakee_event->wait_sem));\n\tsem_init(wakee_event->wait_sem, 0, 0);\n\twakee_event->specific_wait = 1;\n\tevent->wait_sem = wakee_event->wait_sem;\n\n\tsched->nr_wakeup_events++;\n}\n\nstatic void add_sched_event_sleep(struct perf_sched *sched, struct task_desc *task,\n\t\t\t\t  u64 timestamp, u64 task_state __maybe_unused)\n{\n\tstruct sched_atom *event = get_new_event(task, timestamp);\n\n\tevent->type = SCHED_EVENT_SLEEP;\n\n\tsched->nr_sleep_events++;\n}\n\nstatic struct task_desc *register_pid(struct perf_sched *sched,\n\t\t\t\t      unsigned long pid, const char *comm)\n{\n\tstruct task_desc *task;\n\tstatic int pid_max;\n\n\tif (sched->pid_to_task == NULL) {\n\t\tif (sysctl__read_int(\"kernel/pid_max\", &pid_max) < 0)\n\t\t\tpid_max = MAX_PID;\n\t\tBUG_ON((sched->pid_to_task = calloc(pid_max, sizeof(struct task_desc *))) == NULL);\n\t}\n\tif (pid >= (unsigned long)pid_max) {\n\t\tBUG_ON((sched->pid_to_task = realloc(sched->pid_to_task, (pid + 1) *\n\t\t\tsizeof(struct task_desc *))) == NULL);\n\t\twhile (pid >= (unsigned long)pid_max)\n\t\t\tsched->pid_to_task[pid_max++] = NULL;\n\t}\n\n\ttask = sched->pid_to_task[pid];\n\n\tif (task)\n\t\treturn task;\n\n\ttask = zalloc(sizeof(*task));\n\ttask->pid = pid;\n\ttask->nr = sched->nr_tasks;\n\tstrcpy(task->comm, comm);\n\t \n\tadd_sched_event_sleep(sched, task, 0, 0);\n\n\tsched->pid_to_task[pid] = task;\n\tsched->nr_tasks++;\n\tsched->tasks = realloc(sched->tasks, sched->nr_tasks * sizeof(struct task_desc *));\n\tBUG_ON(!sched->tasks);\n\tsched->tasks[task->nr] = task;\n\n\tif (verbose > 0)\n\t\tprintf(\"registered task #%ld, PID %ld (%s)\\n\", sched->nr_tasks, pid, comm);\n\n\treturn task;\n}\n\n\nstatic void print_task_traces(struct perf_sched *sched)\n{\n\tstruct task_desc *task;\n\tunsigned long i;\n\n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\ttask = sched->tasks[i];\n\t\tprintf(\"task %6ld (%20s:%10ld), nr_events: %ld\\n\",\n\t\t\ttask->nr, task->comm, task->pid, task->nr_events);\n\t}\n}\n\nstatic void add_cross_task_wakeups(struct perf_sched *sched)\n{\n\tstruct task_desc *task1, *task2;\n\tunsigned long i, j;\n\n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\ttask1 = sched->tasks[i];\n\t\tj = i + 1;\n\t\tif (j == sched->nr_tasks)\n\t\t\tj = 0;\n\t\ttask2 = sched->tasks[j];\n\t\tadd_sched_event_wakeup(sched, task1, 0, task2);\n\t}\n}\n\nstatic void perf_sched__process_event(struct perf_sched *sched,\n\t\t\t\t      struct sched_atom *atom)\n{\n\tint ret = 0;\n\n\tswitch (atom->type) {\n\t\tcase SCHED_EVENT_RUN:\n\t\t\tburn_nsecs(sched, atom->duration);\n\t\t\tbreak;\n\t\tcase SCHED_EVENT_SLEEP:\n\t\t\tif (atom->wait_sem)\n\t\t\t\tret = sem_wait(atom->wait_sem);\n\t\t\tBUG_ON(ret);\n\t\t\tbreak;\n\t\tcase SCHED_EVENT_WAKEUP:\n\t\t\tif (atom->wait_sem)\n\t\t\t\tret = sem_post(atom->wait_sem);\n\t\t\tBUG_ON(ret);\n\t\t\tbreak;\n\t\tcase SCHED_EVENT_MIGRATION:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG_ON(1);\n\t}\n}\n\nstatic u64 get_cpu_usage_nsec_parent(void)\n{\n\tstruct rusage ru;\n\tu64 sum;\n\tint err;\n\n\terr = getrusage(RUSAGE_SELF, &ru);\n\tBUG_ON(err);\n\n\tsum =  ru.ru_utime.tv_sec * NSEC_PER_SEC + ru.ru_utime.tv_usec * NSEC_PER_USEC;\n\tsum += ru.ru_stime.tv_sec * NSEC_PER_SEC + ru.ru_stime.tv_usec * NSEC_PER_USEC;\n\n\treturn sum;\n}\n\nstatic int self_open_counters(struct perf_sched *sched, unsigned long cur_task)\n{\n\tstruct perf_event_attr attr;\n\tchar sbuf[STRERR_BUFSIZE], info[STRERR_BUFSIZE];\n\tint fd;\n\tstruct rlimit limit;\n\tbool need_privilege = false;\n\n\tmemset(&attr, 0, sizeof(attr));\n\n\tattr.type = PERF_TYPE_SOFTWARE;\n\tattr.config = PERF_COUNT_SW_TASK_CLOCK;\n\nforce_again:\n\tfd = sys_perf_event_open(&attr, 0, -1, -1,\n\t\t\t\t perf_event_open_cloexec_flag());\n\n\tif (fd < 0) {\n\t\tif (errno == EMFILE) {\n\t\t\tif (sched->force) {\n\t\t\t\tBUG_ON(getrlimit(RLIMIT_NOFILE, &limit) == -1);\n\t\t\t\tlimit.rlim_cur += sched->nr_tasks - cur_task;\n\t\t\t\tif (limit.rlim_cur > limit.rlim_max) {\n\t\t\t\t\tlimit.rlim_max = limit.rlim_cur;\n\t\t\t\t\tneed_privilege = true;\n\t\t\t\t}\n\t\t\t\tif (setrlimit(RLIMIT_NOFILE, &limit) == -1) {\n\t\t\t\t\tif (need_privilege && errno == EPERM)\n\t\t\t\t\t\tstrcpy(info, \"Need privilege\\n\");\n\t\t\t\t} else\n\t\t\t\t\tgoto force_again;\n\t\t\t} else\n\t\t\t\tstrcpy(info, \"Have a try with -f option\\n\");\n\t\t}\n\t\tpr_err(\"Error: sys_perf_event_open() syscall returned \"\n\t\t       \"with %d (%s)\\n%s\", fd,\n\t\t       str_error_r(errno, sbuf, sizeof(sbuf)), info);\n\t\texit(EXIT_FAILURE);\n\t}\n\treturn fd;\n}\n\nstatic u64 get_cpu_usage_nsec_self(int fd)\n{\n\tu64 runtime;\n\tint ret;\n\n\tret = read(fd, &runtime, sizeof(runtime));\n\tBUG_ON(ret != sizeof(runtime));\n\n\treturn runtime;\n}\n\nstruct sched_thread_parms {\n\tstruct task_desc  *task;\n\tstruct perf_sched *sched;\n\tint fd;\n};\n\nstatic void *thread_func(void *ctx)\n{\n\tstruct sched_thread_parms *parms = ctx;\n\tstruct task_desc *this_task = parms->task;\n\tstruct perf_sched *sched = parms->sched;\n\tu64 cpu_usage_0, cpu_usage_1;\n\tunsigned long i, ret;\n\tchar comm2[22];\n\tint fd = parms->fd;\n\n\tzfree(&parms);\n\n\tsprintf(comm2, \":%s\", this_task->comm);\n\tprctl(PR_SET_NAME, comm2);\n\tif (fd < 0)\n\t\treturn NULL;\n\n\twhile (!sched->thread_funcs_exit) {\n\t\tret = sem_post(&this_task->ready_for_work);\n\t\tBUG_ON(ret);\n\t\tmutex_lock(&sched->start_work_mutex);\n\t\tmutex_unlock(&sched->start_work_mutex);\n\n\t\tcpu_usage_0 = get_cpu_usage_nsec_self(fd);\n\n\t\tfor (i = 0; i < this_task->nr_events; i++) {\n\t\t\tthis_task->curr_event = i;\n\t\t\tperf_sched__process_event(sched, this_task->atoms[i]);\n\t\t}\n\n\t\tcpu_usage_1 = get_cpu_usage_nsec_self(fd);\n\t\tthis_task->cpu_usage = cpu_usage_1 - cpu_usage_0;\n\t\tret = sem_post(&this_task->work_done_sem);\n\t\tBUG_ON(ret);\n\n\t\tmutex_lock(&sched->work_done_wait_mutex);\n\t\tmutex_unlock(&sched->work_done_wait_mutex);\n\t}\n\treturn NULL;\n}\n\nstatic void create_tasks(struct perf_sched *sched)\n\tEXCLUSIVE_LOCK_FUNCTION(sched->start_work_mutex)\n\tEXCLUSIVE_LOCK_FUNCTION(sched->work_done_wait_mutex)\n{\n\tstruct task_desc *task;\n\tpthread_attr_t attr;\n\tunsigned long i;\n\tint err;\n\n\terr = pthread_attr_init(&attr);\n\tBUG_ON(err);\n\terr = pthread_attr_setstacksize(&attr,\n\t\t\t(size_t) max(16 * 1024, (int)PTHREAD_STACK_MIN));\n\tBUG_ON(err);\n\tmutex_lock(&sched->start_work_mutex);\n\tmutex_lock(&sched->work_done_wait_mutex);\n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\tstruct sched_thread_parms *parms = malloc(sizeof(*parms));\n\t\tBUG_ON(parms == NULL);\n\t\tparms->task = task = sched->tasks[i];\n\t\tparms->sched = sched;\n\t\tparms->fd = self_open_counters(sched, i);\n\t\tsem_init(&task->sleep_sem, 0, 0);\n\t\tsem_init(&task->ready_for_work, 0, 0);\n\t\tsem_init(&task->work_done_sem, 0, 0);\n\t\ttask->curr_event = 0;\n\t\terr = pthread_create(&task->thread, &attr, thread_func, parms);\n\t\tBUG_ON(err);\n\t}\n}\n\nstatic void destroy_tasks(struct perf_sched *sched)\n\tUNLOCK_FUNCTION(sched->start_work_mutex)\n\tUNLOCK_FUNCTION(sched->work_done_wait_mutex)\n{\n\tstruct task_desc *task;\n\tunsigned long i;\n\tint err;\n\n\tmutex_unlock(&sched->start_work_mutex);\n\tmutex_unlock(&sched->work_done_wait_mutex);\n\t \n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\ttask = sched->tasks[i];\n\t\terr = pthread_join(task->thread, NULL);\n\t\tBUG_ON(err);\n\t\tsem_destroy(&task->sleep_sem);\n\t\tsem_destroy(&task->ready_for_work);\n\t\tsem_destroy(&task->work_done_sem);\n\t}\n}\n\nstatic void wait_for_tasks(struct perf_sched *sched)\n\tEXCLUSIVE_LOCKS_REQUIRED(sched->work_done_wait_mutex)\n\tEXCLUSIVE_LOCKS_REQUIRED(sched->start_work_mutex)\n{\n\tu64 cpu_usage_0, cpu_usage_1;\n\tstruct task_desc *task;\n\tunsigned long i, ret;\n\n\tsched->start_time = get_nsecs();\n\tsched->cpu_usage = 0;\n\tmutex_unlock(&sched->work_done_wait_mutex);\n\n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\ttask = sched->tasks[i];\n\t\tret = sem_wait(&task->ready_for_work);\n\t\tBUG_ON(ret);\n\t\tsem_init(&task->ready_for_work, 0, 0);\n\t}\n\tmutex_lock(&sched->work_done_wait_mutex);\n\n\tcpu_usage_0 = get_cpu_usage_nsec_parent();\n\n\tmutex_unlock(&sched->start_work_mutex);\n\n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\ttask = sched->tasks[i];\n\t\tret = sem_wait(&task->work_done_sem);\n\t\tBUG_ON(ret);\n\t\tsem_init(&task->work_done_sem, 0, 0);\n\t\tsched->cpu_usage += task->cpu_usage;\n\t\ttask->cpu_usage = 0;\n\t}\n\n\tcpu_usage_1 = get_cpu_usage_nsec_parent();\n\tif (!sched->runavg_cpu_usage)\n\t\tsched->runavg_cpu_usage = sched->cpu_usage;\n\tsched->runavg_cpu_usage = (sched->runavg_cpu_usage * (sched->replay_repeat - 1) + sched->cpu_usage) / sched->replay_repeat;\n\n\tsched->parent_cpu_usage = cpu_usage_1 - cpu_usage_0;\n\tif (!sched->runavg_parent_cpu_usage)\n\t\tsched->runavg_parent_cpu_usage = sched->parent_cpu_usage;\n\tsched->runavg_parent_cpu_usage = (sched->runavg_parent_cpu_usage * (sched->replay_repeat - 1) +\n\t\t\t\t\t sched->parent_cpu_usage)/sched->replay_repeat;\n\n\tmutex_lock(&sched->start_work_mutex);\n\n\tfor (i = 0; i < sched->nr_tasks; i++) {\n\t\ttask = sched->tasks[i];\n\t\tsem_init(&task->sleep_sem, 0, 0);\n\t\ttask->curr_event = 0;\n\t}\n}\n\nstatic void run_one_test(struct perf_sched *sched)\n\tEXCLUSIVE_LOCKS_REQUIRED(sched->work_done_wait_mutex)\n\tEXCLUSIVE_LOCKS_REQUIRED(sched->start_work_mutex)\n{\n\tu64 T0, T1, delta, avg_delta, fluct;\n\n\tT0 = get_nsecs();\n\twait_for_tasks(sched);\n\tT1 = get_nsecs();\n\n\tdelta = T1 - T0;\n\tsched->sum_runtime += delta;\n\tsched->nr_runs++;\n\n\tavg_delta = sched->sum_runtime / sched->nr_runs;\n\tif (delta < avg_delta)\n\t\tfluct = avg_delta - delta;\n\telse\n\t\tfluct = delta - avg_delta;\n\tsched->sum_fluct += fluct;\n\tif (!sched->run_avg)\n\t\tsched->run_avg = delta;\n\tsched->run_avg = (sched->run_avg * (sched->replay_repeat - 1) + delta) / sched->replay_repeat;\n\n\tprintf(\"#%-3ld: %0.3f, \", sched->nr_runs, (double)delta / NSEC_PER_MSEC);\n\n\tprintf(\"ravg: %0.2f, \", (double)sched->run_avg / NSEC_PER_MSEC);\n\n\tprintf(\"cpu: %0.2f / %0.2f\",\n\t\t(double)sched->cpu_usage / NSEC_PER_MSEC, (double)sched->runavg_cpu_usage / NSEC_PER_MSEC);\n\n#if 0\n\t \n\tprintf(\" [%0.2f / %0.2f]\",\n\t\t(double)sched->parent_cpu_usage / NSEC_PER_MSEC,\n\t\t(double)sched->runavg_parent_cpu_usage / NSEC_PER_MSEC);\n#endif\n\n\tprintf(\"\\n\");\n\n\tif (sched->nr_sleep_corrections)\n\t\tprintf(\" (%ld sleep corrections)\\n\", sched->nr_sleep_corrections);\n\tsched->nr_sleep_corrections = 0;\n}\n\nstatic void test_calibrations(struct perf_sched *sched)\n{\n\tu64 T0, T1;\n\n\tT0 = get_nsecs();\n\tburn_nsecs(sched, NSEC_PER_MSEC);\n\tT1 = get_nsecs();\n\n\tprintf(\"the run test took %\" PRIu64 \" nsecs\\n\", T1 - T0);\n\n\tT0 = get_nsecs();\n\tsleep_nsecs(NSEC_PER_MSEC);\n\tT1 = get_nsecs();\n\n\tprintf(\"the sleep test took %\" PRIu64 \" nsecs\\n\", T1 - T0);\n}\n\nstatic int\nreplay_wakeup_event(struct perf_sched *sched,\n\t\t    struct evsel *evsel, struct perf_sample *sample,\n\t\t    struct machine *machine __maybe_unused)\n{\n\tconst char *comm = evsel__strval(evsel, sample, \"comm\");\n\tconst u32 pid\t = evsel__intval(evsel, sample, \"pid\");\n\tstruct task_desc *waker, *wakee;\n\n\tif (verbose > 0) {\n\t\tprintf(\"sched_wakeup event %p\\n\", evsel);\n\n\t\tprintf(\" ... pid %d woke up %s/%d\\n\", sample->tid, comm, pid);\n\t}\n\n\twaker = register_pid(sched, sample->tid, \"<unknown>\");\n\twakee = register_pid(sched, pid, comm);\n\n\tadd_sched_event_wakeup(sched, waker, sample->time, wakee);\n\treturn 0;\n}\n\nstatic int replay_switch_event(struct perf_sched *sched,\n\t\t\t       struct evsel *evsel,\n\t\t\t       struct perf_sample *sample,\n\t\t\t       struct machine *machine __maybe_unused)\n{\n\tconst char *prev_comm  = evsel__strval(evsel, sample, \"prev_comm\"),\n\t\t   *next_comm  = evsel__strval(evsel, sample, \"next_comm\");\n\tconst u32 prev_pid = evsel__intval(evsel, sample, \"prev_pid\"),\n\t\t  next_pid = evsel__intval(evsel, sample, \"next_pid\");\n\tconst u64 prev_state = evsel__intval(evsel, sample, \"prev_state\");\n\tstruct task_desc *prev, __maybe_unused *next;\n\tu64 timestamp0, timestamp = sample->time;\n\tint cpu = sample->cpu;\n\ts64 delta;\n\n\tif (verbose > 0)\n\t\tprintf(\"sched_switch event %p\\n\", evsel);\n\n\tif (cpu >= MAX_CPUS || cpu < 0)\n\t\treturn 0;\n\n\ttimestamp0 = sched->cpu_last_switched[cpu];\n\tif (timestamp0)\n\t\tdelta = timestamp - timestamp0;\n\telse\n\t\tdelta = 0;\n\n\tif (delta < 0) {\n\t\tpr_err(\"hm, delta: %\" PRIu64 \" < 0 ?\\n\", delta);\n\t\treturn -1;\n\t}\n\n\tpr_debug(\" ... switch from %s/%d to %s/%d [ran %\" PRIu64 \" nsecs]\\n\",\n\t\t prev_comm, prev_pid, next_comm, next_pid, delta);\n\n\tprev = register_pid(sched, prev_pid, prev_comm);\n\tnext = register_pid(sched, next_pid, next_comm);\n\n\tsched->cpu_last_switched[cpu] = timestamp;\n\n\tadd_sched_event_run(sched, prev, timestamp, delta);\n\tadd_sched_event_sleep(sched, prev, timestamp, prev_state);\n\n\treturn 0;\n}\n\nstatic int replay_fork_event(struct perf_sched *sched,\n\t\t\t     union perf_event *event,\n\t\t\t     struct machine *machine)\n{\n\tstruct thread *child, *parent;\n\n\tchild = machine__findnew_thread(machine, event->fork.pid,\n\t\t\t\t\tevent->fork.tid);\n\tparent = machine__findnew_thread(machine, event->fork.ppid,\n\t\t\t\t\t event->fork.ptid);\n\n\tif (child == NULL || parent == NULL) {\n\t\tpr_debug(\"thread does not exist on fork event: child %p, parent %p\\n\",\n\t\t\t\t child, parent);\n\t\tgoto out_put;\n\t}\n\n\tif (verbose > 0) {\n\t\tprintf(\"fork event\\n\");\n\t\tprintf(\"... parent: %s/%d\\n\", thread__comm_str(parent), thread__tid(parent));\n\t\tprintf(\"...  child: %s/%d\\n\", thread__comm_str(child), thread__tid(child));\n\t}\n\n\tregister_pid(sched, thread__tid(parent), thread__comm_str(parent));\n\tregister_pid(sched, thread__tid(child), thread__comm_str(child));\nout_put:\n\tthread__put(child);\n\tthread__put(parent);\n\treturn 0;\n}\n\nstruct sort_dimension {\n\tconst char\t\t*name;\n\tsort_fn_t\t\tcmp;\n\tstruct list_head\tlist;\n};\n\n \nstatic struct thread_runtime *thread__init_runtime(struct thread *thread)\n{\n\tstruct thread_runtime *r;\n\n\tr = zalloc(sizeof(struct thread_runtime));\n\tif (!r)\n\t\treturn NULL;\n\n\tinit_stats(&r->run_stats);\n\tthread__set_priv(thread, r);\n\n\treturn r;\n}\n\nstatic struct thread_runtime *thread__get_runtime(struct thread *thread)\n{\n\tstruct thread_runtime *tr;\n\n\ttr = thread__priv(thread);\n\tif (tr == NULL) {\n\t\ttr = thread__init_runtime(thread);\n\t\tif (tr == NULL)\n\t\t\tpr_debug(\"Failed to malloc memory for runtime data.\\n\");\n\t}\n\n\treturn tr;\n}\n\nstatic int\nthread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *r)\n{\n\tstruct sort_dimension *sort;\n\tint ret = 0;\n\n\tBUG_ON(list_empty(list));\n\n\tlist_for_each_entry(sort, list, list) {\n\t\tret = sort->cmp(l, r);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\nstatic struct work_atoms *\nthread_atoms_search(struct rb_root_cached *root, struct thread *thread,\n\t\t\t struct list_head *sort_list)\n{\n\tstruct rb_node *node = root->rb_root.rb_node;\n\tstruct work_atoms key = { .thread = thread };\n\n\twhile (node) {\n\t\tstruct work_atoms *atoms;\n\t\tint cmp;\n\n\t\tatoms = container_of(node, struct work_atoms, node);\n\n\t\tcmp = thread_lat_cmp(sort_list, &key, atoms);\n\t\tif (cmp > 0)\n\t\t\tnode = node->rb_left;\n\t\telse if (cmp < 0)\n\t\t\tnode = node->rb_right;\n\t\telse {\n\t\t\tBUG_ON(thread != atoms->thread);\n\t\t\treturn atoms;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic void\n__thread_latency_insert(struct rb_root_cached *root, struct work_atoms *data,\n\t\t\t struct list_head *sort_list)\n{\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tbool leftmost = true;\n\n\twhile (*new) {\n\t\tstruct work_atoms *this;\n\t\tint cmp;\n\n\t\tthis = container_of(*new, struct work_atoms, node);\n\t\tparent = *new;\n\n\t\tcmp = thread_lat_cmp(sort_list, data, this);\n\n\t\tif (cmp > 0)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&data->node, parent, new);\n\trb_insert_color_cached(&data->node, root, leftmost);\n}\n\nstatic int thread_atoms_insert(struct perf_sched *sched, struct thread *thread)\n{\n\tstruct work_atoms *atoms = zalloc(sizeof(*atoms));\n\tif (!atoms) {\n\t\tpr_err(\"No memory at %s\\n\", __func__);\n\t\treturn -1;\n\t}\n\n\tatoms->thread = thread__get(thread);\n\tINIT_LIST_HEAD(&atoms->work_list);\n\t__thread_latency_insert(&sched->atom_root, atoms, &sched->cmp_pid);\n\treturn 0;\n}\n\nstatic char sched_out_state(u64 prev_state)\n{\n\tconst char *str = TASK_STATE_TO_CHAR_STR;\n\n\treturn str[prev_state];\n}\n\nstatic int\nadd_sched_out_event(struct work_atoms *atoms,\n\t\t    char run_state,\n\t\t    u64 timestamp)\n{\n\tstruct work_atom *atom = zalloc(sizeof(*atom));\n\tif (!atom) {\n\t\tpr_err(\"Non memory at %s\", __func__);\n\t\treturn -1;\n\t}\n\n\tatom->sched_out_time = timestamp;\n\n\tif (run_state == 'R') {\n\t\tatom->state = THREAD_WAIT_CPU;\n\t\tatom->wake_up_time = atom->sched_out_time;\n\t}\n\n\tlist_add_tail(&atom->list, &atoms->work_list);\n\treturn 0;\n}\n\nstatic void\nadd_runtime_event(struct work_atoms *atoms, u64 delta,\n\t\t  u64 timestamp __maybe_unused)\n{\n\tstruct work_atom *atom;\n\n\tBUG_ON(list_empty(&atoms->work_list));\n\n\tatom = list_entry(atoms->work_list.prev, struct work_atom, list);\n\n\tatom->runtime += delta;\n\tatoms->total_runtime += delta;\n}\n\nstatic void\nadd_sched_in_event(struct work_atoms *atoms, u64 timestamp)\n{\n\tstruct work_atom *atom;\n\tu64 delta;\n\n\tif (list_empty(&atoms->work_list))\n\t\treturn;\n\n\tatom = list_entry(atoms->work_list.prev, struct work_atom, list);\n\n\tif (atom->state != THREAD_WAIT_CPU)\n\t\treturn;\n\n\tif (timestamp < atom->wake_up_time) {\n\t\tatom->state = THREAD_IGNORE;\n\t\treturn;\n\t}\n\n\tatom->state = THREAD_SCHED_IN;\n\tatom->sched_in_time = timestamp;\n\n\tdelta = atom->sched_in_time - atom->wake_up_time;\n\tatoms->total_lat += delta;\n\tif (delta > atoms->max_lat) {\n\t\tatoms->max_lat = delta;\n\t\tatoms->max_lat_start = atom->wake_up_time;\n\t\tatoms->max_lat_end = timestamp;\n\t}\n\tatoms->nb_atoms++;\n}\n\nstatic int latency_switch_event(struct perf_sched *sched,\n\t\t\t\tstruct evsel *evsel,\n\t\t\t\tstruct perf_sample *sample,\n\t\t\t\tstruct machine *machine)\n{\n\tconst u32 prev_pid = evsel__intval(evsel, sample, \"prev_pid\"),\n\t\t  next_pid = evsel__intval(evsel, sample, \"next_pid\");\n\tconst u64 prev_state = evsel__intval(evsel, sample, \"prev_state\");\n\tstruct work_atoms *out_events, *in_events;\n\tstruct thread *sched_out, *sched_in;\n\tu64 timestamp0, timestamp = sample->time;\n\tint cpu = sample->cpu, err = -1;\n\ts64 delta;\n\n\tBUG_ON(cpu >= MAX_CPUS || cpu < 0);\n\n\ttimestamp0 = sched->cpu_last_switched[cpu];\n\tsched->cpu_last_switched[cpu] = timestamp;\n\tif (timestamp0)\n\t\tdelta = timestamp - timestamp0;\n\telse\n\t\tdelta = 0;\n\n\tif (delta < 0) {\n\t\tpr_err(\"hm, delta: %\" PRIu64 \" < 0 ?\\n\", delta);\n\t\treturn -1;\n\t}\n\n\tsched_out = machine__findnew_thread(machine, -1, prev_pid);\n\tsched_in = machine__findnew_thread(machine, -1, next_pid);\n\tif (sched_out == NULL || sched_in == NULL)\n\t\tgoto out_put;\n\n\tout_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);\n\tif (!out_events) {\n\t\tif (thread_atoms_insert(sched, sched_out))\n\t\t\tgoto out_put;\n\t\tout_events = thread_atoms_search(&sched->atom_root, sched_out, &sched->cmp_pid);\n\t\tif (!out_events) {\n\t\t\tpr_err(\"out-event: Internal tree error\");\n\t\t\tgoto out_put;\n\t\t}\n\t}\n\tif (add_sched_out_event(out_events, sched_out_state(prev_state), timestamp))\n\t\treturn -1;\n\n\tin_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);\n\tif (!in_events) {\n\t\tif (thread_atoms_insert(sched, sched_in))\n\t\t\tgoto out_put;\n\t\tin_events = thread_atoms_search(&sched->atom_root, sched_in, &sched->cmp_pid);\n\t\tif (!in_events) {\n\t\t\tpr_err(\"in-event: Internal tree error\");\n\t\t\tgoto out_put;\n\t\t}\n\t\t \n\t\tif (add_sched_out_event(in_events, 'R', timestamp))\n\t\t\tgoto out_put;\n\t}\n\tadd_sched_in_event(in_events, timestamp);\n\terr = 0;\nout_put:\n\tthread__put(sched_out);\n\tthread__put(sched_in);\n\treturn err;\n}\n\nstatic int latency_runtime_event(struct perf_sched *sched,\n\t\t\t\t struct evsel *evsel,\n\t\t\t\t struct perf_sample *sample,\n\t\t\t\t struct machine *machine)\n{\n\tconst u32 pid\t   = evsel__intval(evsel, sample, \"pid\");\n\tconst u64 runtime  = evsel__intval(evsel, sample, \"runtime\");\n\tstruct thread *thread = machine__findnew_thread(machine, -1, pid);\n\tstruct work_atoms *atoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);\n\tu64 timestamp = sample->time;\n\tint cpu = sample->cpu, err = -1;\n\n\tif (thread == NULL)\n\t\treturn -1;\n\n\tBUG_ON(cpu >= MAX_CPUS || cpu < 0);\n\tif (!atoms) {\n\t\tif (thread_atoms_insert(sched, thread))\n\t\t\tgoto out_put;\n\t\tatoms = thread_atoms_search(&sched->atom_root, thread, &sched->cmp_pid);\n\t\tif (!atoms) {\n\t\t\tpr_err(\"in-event: Internal tree error\");\n\t\t\tgoto out_put;\n\t\t}\n\t\tif (add_sched_out_event(atoms, 'R', timestamp))\n\t\t\tgoto out_put;\n\t}\n\n\tadd_runtime_event(atoms, runtime, timestamp);\n\terr = 0;\nout_put:\n\tthread__put(thread);\n\treturn err;\n}\n\nstatic int latency_wakeup_event(struct perf_sched *sched,\n\t\t\t\tstruct evsel *evsel,\n\t\t\t\tstruct perf_sample *sample,\n\t\t\t\tstruct machine *machine)\n{\n\tconst u32 pid\t  = evsel__intval(evsel, sample, \"pid\");\n\tstruct work_atoms *atoms;\n\tstruct work_atom *atom;\n\tstruct thread *wakee;\n\tu64 timestamp = sample->time;\n\tint err = -1;\n\n\twakee = machine__findnew_thread(machine, -1, pid);\n\tif (wakee == NULL)\n\t\treturn -1;\n\tatoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);\n\tif (!atoms) {\n\t\tif (thread_atoms_insert(sched, wakee))\n\t\t\tgoto out_put;\n\t\tatoms = thread_atoms_search(&sched->atom_root, wakee, &sched->cmp_pid);\n\t\tif (!atoms) {\n\t\t\tpr_err(\"wakeup-event: Internal tree error\");\n\t\t\tgoto out_put;\n\t\t}\n\t\tif (add_sched_out_event(atoms, 'S', timestamp))\n\t\t\tgoto out_put;\n\t}\n\n\tBUG_ON(list_empty(&atoms->work_list));\n\n\tatom = list_entry(atoms->work_list.prev, struct work_atom, list);\n\n\t \n\tif (sched->profile_cpu == -1 && atom->state != THREAD_SLEEPING)\n\t\tgoto out_ok;\n\n\tsched->nr_timestamps++;\n\tif (atom->sched_out_time > timestamp) {\n\t\tsched->nr_unordered_timestamps++;\n\t\tgoto out_ok;\n\t}\n\n\tatom->state = THREAD_WAIT_CPU;\n\tatom->wake_up_time = timestamp;\nout_ok:\n\terr = 0;\nout_put:\n\tthread__put(wakee);\n\treturn err;\n}\n\nstatic int latency_migrate_task_event(struct perf_sched *sched,\n\t\t\t\t      struct evsel *evsel,\n\t\t\t\t      struct perf_sample *sample,\n\t\t\t\t      struct machine *machine)\n{\n\tconst u32 pid = evsel__intval(evsel, sample, \"pid\");\n\tu64 timestamp = sample->time;\n\tstruct work_atoms *atoms;\n\tstruct work_atom *atom;\n\tstruct thread *migrant;\n\tint err = -1;\n\n\t \n\tif (sched->profile_cpu == -1)\n\t\treturn 0;\n\n\tmigrant = machine__findnew_thread(machine, -1, pid);\n\tif (migrant == NULL)\n\t\treturn -1;\n\tatoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);\n\tif (!atoms) {\n\t\tif (thread_atoms_insert(sched, migrant))\n\t\t\tgoto out_put;\n\t\tregister_pid(sched, thread__tid(migrant), thread__comm_str(migrant));\n\t\tatoms = thread_atoms_search(&sched->atom_root, migrant, &sched->cmp_pid);\n\t\tif (!atoms) {\n\t\t\tpr_err(\"migration-event: Internal tree error\");\n\t\t\tgoto out_put;\n\t\t}\n\t\tif (add_sched_out_event(atoms, 'R', timestamp))\n\t\t\tgoto out_put;\n\t}\n\n\tBUG_ON(list_empty(&atoms->work_list));\n\n\tatom = list_entry(atoms->work_list.prev, struct work_atom, list);\n\tatom->sched_in_time = atom->sched_out_time = atom->wake_up_time = timestamp;\n\n\tsched->nr_timestamps++;\n\n\tif (atom->sched_out_time > timestamp)\n\t\tsched->nr_unordered_timestamps++;\n\terr = 0;\nout_put:\n\tthread__put(migrant);\n\treturn err;\n}\n\nstatic void output_lat_thread(struct perf_sched *sched, struct work_atoms *work_list)\n{\n\tint i;\n\tint ret;\n\tu64 avg;\n\tchar max_lat_start[32], max_lat_end[32];\n\n\tif (!work_list->nb_atoms)\n\t\treturn;\n\t \n\tif (!strcmp(thread__comm_str(work_list->thread), \"swapper\"))\n\t\treturn;\n\n\tsched->all_runtime += work_list->total_runtime;\n\tsched->all_count   += work_list->nb_atoms;\n\n\tif (work_list->num_merged > 1) {\n\t\tret = printf(\"  %s:(%d) \", thread__comm_str(work_list->thread),\n\t\t\t     work_list->num_merged);\n\t} else {\n\t\tret = printf(\"  %s:%d \", thread__comm_str(work_list->thread),\n\t\t\t     thread__tid(work_list->thread));\n\t}\n\n\tfor (i = 0; i < 24 - ret; i++)\n\t\tprintf(\" \");\n\n\tavg = work_list->total_lat / work_list->nb_atoms;\n\ttimestamp__scnprintf_usec(work_list->max_lat_start, max_lat_start, sizeof(max_lat_start));\n\ttimestamp__scnprintf_usec(work_list->max_lat_end, max_lat_end, sizeof(max_lat_end));\n\n\tprintf(\"|%11.3f ms |%9\" PRIu64 \" | avg:%8.3f ms | max:%8.3f ms | max start: %12s s | max end: %12s s\\n\",\n\t      (double)work_list->total_runtime / NSEC_PER_MSEC,\n\t\t work_list->nb_atoms, (double)avg / NSEC_PER_MSEC,\n\t\t (double)work_list->max_lat / NSEC_PER_MSEC,\n\t\t max_lat_start, max_lat_end);\n}\n\nstatic int pid_cmp(struct work_atoms *l, struct work_atoms *r)\n{\n\tpid_t l_tid, r_tid;\n\n\tif (RC_CHK_ACCESS(l->thread) == RC_CHK_ACCESS(r->thread))\n\t\treturn 0;\n\tl_tid = thread__tid(l->thread);\n\tr_tid = thread__tid(r->thread);\n\tif (l_tid < r_tid)\n\t\treturn -1;\n\tif (l_tid > r_tid)\n\t\treturn 1;\n\treturn (int)(RC_CHK_ACCESS(l->thread) - RC_CHK_ACCESS(r->thread));\n}\n\nstatic int avg_cmp(struct work_atoms *l, struct work_atoms *r)\n{\n\tu64 avgl, avgr;\n\n\tif (!l->nb_atoms)\n\t\treturn -1;\n\n\tif (!r->nb_atoms)\n\t\treturn 1;\n\n\tavgl = l->total_lat / l->nb_atoms;\n\tavgr = r->total_lat / r->nb_atoms;\n\n\tif (avgl < avgr)\n\t\treturn -1;\n\tif (avgl > avgr)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int max_cmp(struct work_atoms *l, struct work_atoms *r)\n{\n\tif (l->max_lat < r->max_lat)\n\t\treturn -1;\n\tif (l->max_lat > r->max_lat)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int switch_cmp(struct work_atoms *l, struct work_atoms *r)\n{\n\tif (l->nb_atoms < r->nb_atoms)\n\t\treturn -1;\n\tif (l->nb_atoms > r->nb_atoms)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int runtime_cmp(struct work_atoms *l, struct work_atoms *r)\n{\n\tif (l->total_runtime < r->total_runtime)\n\t\treturn -1;\n\tif (l->total_runtime > r->total_runtime)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int sort_dimension__add(const char *tok, struct list_head *list)\n{\n\tsize_t i;\n\tstatic struct sort_dimension avg_sort_dimension = {\n\t\t.name = \"avg\",\n\t\t.cmp  = avg_cmp,\n\t};\n\tstatic struct sort_dimension max_sort_dimension = {\n\t\t.name = \"max\",\n\t\t.cmp  = max_cmp,\n\t};\n\tstatic struct sort_dimension pid_sort_dimension = {\n\t\t.name = \"pid\",\n\t\t.cmp  = pid_cmp,\n\t};\n\tstatic struct sort_dimension runtime_sort_dimension = {\n\t\t.name = \"runtime\",\n\t\t.cmp  = runtime_cmp,\n\t};\n\tstatic struct sort_dimension switch_sort_dimension = {\n\t\t.name = \"switch\",\n\t\t.cmp  = switch_cmp,\n\t};\n\tstruct sort_dimension *available_sorts[] = {\n\t\t&pid_sort_dimension,\n\t\t&avg_sort_dimension,\n\t\t&max_sort_dimension,\n\t\t&switch_sort_dimension,\n\t\t&runtime_sort_dimension,\n\t};\n\n\tfor (i = 0; i < ARRAY_SIZE(available_sorts); i++) {\n\t\tif (!strcmp(available_sorts[i]->name, tok)) {\n\t\t\tlist_add_tail(&available_sorts[i]->list, list);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -1;\n}\n\nstatic void perf_sched__sort_lat(struct perf_sched *sched)\n{\n\tstruct rb_node *node;\n\tstruct rb_root_cached *root = &sched->atom_root;\nagain:\n\tfor (;;) {\n\t\tstruct work_atoms *data;\n\t\tnode = rb_first_cached(root);\n\t\tif (!node)\n\t\t\tbreak;\n\n\t\trb_erase_cached(node, root);\n\t\tdata = rb_entry(node, struct work_atoms, node);\n\t\t__thread_latency_insert(&sched->sorted_atom_root, data, &sched->sort_list);\n\t}\n\tif (root == &sched->atom_root) {\n\t\troot = &sched->merged_atom_root;\n\t\tgoto again;\n\t}\n}\n\nstatic int process_sched_wakeup_event(struct perf_tool *tool,\n\t\t\t\t      struct evsel *evsel,\n\t\t\t\t      struct perf_sample *sample,\n\t\t\t\t      struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\n\tif (sched->tp_handler->wakeup_event)\n\t\treturn sched->tp_handler->wakeup_event(sched, evsel, sample, machine);\n\n\treturn 0;\n}\n\nstatic int process_sched_wakeup_ignore(struct perf_tool *tool __maybe_unused,\n\t\t\t\t      struct evsel *evsel __maybe_unused,\n\t\t\t\t      struct perf_sample *sample __maybe_unused,\n\t\t\t\t      struct machine *machine __maybe_unused)\n{\n\treturn 0;\n}\n\nunion map_priv {\n\tvoid\t*ptr;\n\tbool\t color;\n};\n\nstatic bool thread__has_color(struct thread *thread)\n{\n\tunion map_priv priv = {\n\t\t.ptr = thread__priv(thread),\n\t};\n\n\treturn priv.color;\n}\n\nstatic struct thread*\nmap__findnew_thread(struct perf_sched *sched, struct machine *machine, pid_t pid, pid_t tid)\n{\n\tstruct thread *thread = machine__findnew_thread(machine, pid, tid);\n\tunion map_priv priv = {\n\t\t.color = false,\n\t};\n\n\tif (!sched->map.color_pids || !thread || thread__priv(thread))\n\t\treturn thread;\n\n\tif (thread_map__has(sched->map.color_pids, tid))\n\t\tpriv.color = true;\n\n\tthread__set_priv(thread, priv.ptr);\n\treturn thread;\n}\n\nstatic int map_switch_event(struct perf_sched *sched, struct evsel *evsel,\n\t\t\t    struct perf_sample *sample, struct machine *machine)\n{\n\tconst u32 next_pid = evsel__intval(evsel, sample, \"next_pid\");\n\tstruct thread *sched_in;\n\tstruct thread_runtime *tr;\n\tint new_shortname;\n\tu64 timestamp0, timestamp = sample->time;\n\ts64 delta;\n\tint i;\n\tstruct perf_cpu this_cpu = {\n\t\t.cpu = sample->cpu,\n\t};\n\tint cpus_nr;\n\tbool new_cpu = false;\n\tconst char *color = PERF_COLOR_NORMAL;\n\tchar stimestamp[32];\n\n\tBUG_ON(this_cpu.cpu >= MAX_CPUS || this_cpu.cpu < 0);\n\n\tif (this_cpu.cpu > sched->max_cpu.cpu)\n\t\tsched->max_cpu = this_cpu;\n\n\tif (sched->map.comp) {\n\t\tcpus_nr = bitmap_weight(sched->map.comp_cpus_mask, MAX_CPUS);\n\t\tif (!__test_and_set_bit(this_cpu.cpu, sched->map.comp_cpus_mask)) {\n\t\t\tsched->map.comp_cpus[cpus_nr++] = this_cpu;\n\t\t\tnew_cpu = true;\n\t\t}\n\t} else\n\t\tcpus_nr = sched->max_cpu.cpu;\n\n\ttimestamp0 = sched->cpu_last_switched[this_cpu.cpu];\n\tsched->cpu_last_switched[this_cpu.cpu] = timestamp;\n\tif (timestamp0)\n\t\tdelta = timestamp - timestamp0;\n\telse\n\t\tdelta = 0;\n\n\tif (delta < 0) {\n\t\tpr_err(\"hm, delta: %\" PRIu64 \" < 0 ?\\n\", delta);\n\t\treturn -1;\n\t}\n\n\tsched_in = map__findnew_thread(sched, machine, -1, next_pid);\n\tif (sched_in == NULL)\n\t\treturn -1;\n\n\ttr = thread__get_runtime(sched_in);\n\tif (tr == NULL) {\n\t\tthread__put(sched_in);\n\t\treturn -1;\n\t}\n\n\tsched->curr_thread[this_cpu.cpu] = thread__get(sched_in);\n\n\tprintf(\"  \");\n\n\tnew_shortname = 0;\n\tif (!tr->shortname[0]) {\n\t\tif (!strcmp(thread__comm_str(sched_in), \"swapper\")) {\n\t\t\t \n\t\t\ttr->shortname[0] = '.';\n\t\t\ttr->shortname[1] = ' ';\n\t\t} else {\n\t\t\ttr->shortname[0] = sched->next_shortname1;\n\t\t\ttr->shortname[1] = sched->next_shortname2;\n\n\t\t\tif (sched->next_shortname1 < 'Z') {\n\t\t\t\tsched->next_shortname1++;\n\t\t\t} else {\n\t\t\t\tsched->next_shortname1 = 'A';\n\t\t\t\tif (sched->next_shortname2 < '9')\n\t\t\t\t\tsched->next_shortname2++;\n\t\t\t\telse\n\t\t\t\t\tsched->next_shortname2 = '0';\n\t\t\t}\n\t\t}\n\t\tnew_shortname = 1;\n\t}\n\n\tfor (i = 0; i < cpus_nr; i++) {\n\t\tstruct perf_cpu cpu = {\n\t\t\t.cpu = sched->map.comp ? sched->map.comp_cpus[i].cpu : i,\n\t\t};\n\t\tstruct thread *curr_thread = sched->curr_thread[cpu.cpu];\n\t\tstruct thread_runtime *curr_tr;\n\t\tconst char *pid_color = color;\n\t\tconst char *cpu_color = color;\n\n\t\tif (curr_thread && thread__has_color(curr_thread))\n\t\t\tpid_color = COLOR_PIDS;\n\n\t\tif (sched->map.cpus && !perf_cpu_map__has(sched->map.cpus, cpu))\n\t\t\tcontinue;\n\n\t\tif (sched->map.color_cpus && perf_cpu_map__has(sched->map.color_cpus, cpu))\n\t\t\tcpu_color = COLOR_CPUS;\n\n\t\tif (cpu.cpu != this_cpu.cpu)\n\t\t\tcolor_fprintf(stdout, color, \" \");\n\t\telse\n\t\t\tcolor_fprintf(stdout, cpu_color, \"*\");\n\n\t\tif (sched->curr_thread[cpu.cpu]) {\n\t\t\tcurr_tr = thread__get_runtime(sched->curr_thread[cpu.cpu]);\n\t\t\tif (curr_tr == NULL) {\n\t\t\t\tthread__put(sched_in);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tcolor_fprintf(stdout, pid_color, \"%2s \", curr_tr->shortname);\n\t\t} else\n\t\t\tcolor_fprintf(stdout, color, \"   \");\n\t}\n\n\tif (sched->map.cpus && !perf_cpu_map__has(sched->map.cpus, this_cpu))\n\t\tgoto out;\n\n\ttimestamp__scnprintf_usec(timestamp, stimestamp, sizeof(stimestamp));\n\tcolor_fprintf(stdout, color, \"  %12s secs \", stimestamp);\n\tif (new_shortname || tr->comm_changed || (verbose > 0 && thread__tid(sched_in))) {\n\t\tconst char *pid_color = color;\n\n\t\tif (thread__has_color(sched_in))\n\t\t\tpid_color = COLOR_PIDS;\n\n\t\tcolor_fprintf(stdout, pid_color, \"%s => %s:%d\",\n\t\t\ttr->shortname, thread__comm_str(sched_in), thread__tid(sched_in));\n\t\ttr->comm_changed = false;\n\t}\n\n\tif (sched->map.comp && new_cpu)\n\t\tcolor_fprintf(stdout, color, \" (CPU %d)\", this_cpu);\n\nout:\n\tcolor_fprintf(stdout, color, \"\\n\");\n\n\tthread__put(sched_in);\n\n\treturn 0;\n}\n\nstatic int process_sched_switch_event(struct perf_tool *tool,\n\t\t\t\t      struct evsel *evsel,\n\t\t\t\t      struct perf_sample *sample,\n\t\t\t\t      struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\tint this_cpu = sample->cpu, err = 0;\n\tu32 prev_pid = evsel__intval(evsel, sample, \"prev_pid\"),\n\t    next_pid = evsel__intval(evsel, sample, \"next_pid\");\n\n\tif (sched->curr_pid[this_cpu] != (u32)-1) {\n\t\t \n\t\tif (sched->curr_pid[this_cpu] != prev_pid)\n\t\t\tsched->nr_context_switch_bugs++;\n\t}\n\n\tif (sched->tp_handler->switch_event)\n\t\terr = sched->tp_handler->switch_event(sched, evsel, sample, machine);\n\n\tsched->curr_pid[this_cpu] = next_pid;\n\treturn err;\n}\n\nstatic int process_sched_runtime_event(struct perf_tool *tool,\n\t\t\t\t       struct evsel *evsel,\n\t\t\t\t       struct perf_sample *sample,\n\t\t\t\t       struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\n\tif (sched->tp_handler->runtime_event)\n\t\treturn sched->tp_handler->runtime_event(sched, evsel, sample, machine);\n\n\treturn 0;\n}\n\nstatic int perf_sched__process_fork_event(struct perf_tool *tool,\n\t\t\t\t\t  union perf_event *event,\n\t\t\t\t\t  struct perf_sample *sample,\n\t\t\t\t\t  struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\n\t \n\tperf_event__process_fork(tool, event, sample, machine);\n\n\t \n\tif (sched->tp_handler->fork_event)\n\t\treturn sched->tp_handler->fork_event(sched, event, machine);\n\n\treturn 0;\n}\n\nstatic int process_sched_migrate_task_event(struct perf_tool *tool,\n\t\t\t\t\t    struct evsel *evsel,\n\t\t\t\t\t    struct perf_sample *sample,\n\t\t\t\t\t    struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\n\tif (sched->tp_handler->migrate_task_event)\n\t\treturn sched->tp_handler->migrate_task_event(sched, evsel, sample, machine);\n\n\treturn 0;\n}\n\ntypedef int (*tracepoint_handler)(struct perf_tool *tool,\n\t\t\t\t  struct evsel *evsel,\n\t\t\t\t  struct perf_sample *sample,\n\t\t\t\t  struct machine *machine);\n\nstatic int perf_sched__process_tracepoint_sample(struct perf_tool *tool __maybe_unused,\n\t\t\t\t\t\t union perf_event *event __maybe_unused,\n\t\t\t\t\t\t struct perf_sample *sample,\n\t\t\t\t\t\t struct evsel *evsel,\n\t\t\t\t\t\t struct machine *machine)\n{\n\tint err = 0;\n\n\tif (evsel->handler != NULL) {\n\t\ttracepoint_handler f = evsel->handler;\n\t\terr = f(tool, evsel, sample, machine);\n\t}\n\n\treturn err;\n}\n\nstatic int perf_sched__process_comm(struct perf_tool *tool __maybe_unused,\n\t\t\t\t    union perf_event *event,\n\t\t\t\t    struct perf_sample *sample,\n\t\t\t\t    struct machine *machine)\n{\n\tstruct thread *thread;\n\tstruct thread_runtime *tr;\n\tint err;\n\n\terr = perf_event__process_comm(tool, event, sample, machine);\n\tif (err)\n\t\treturn err;\n\n\tthread = machine__find_thread(machine, sample->pid, sample->tid);\n\tif (!thread) {\n\t\tpr_err(\"Internal error: can't find thread\\n\");\n\t\treturn -1;\n\t}\n\n\ttr = thread__get_runtime(thread);\n\tif (tr == NULL) {\n\t\tthread__put(thread);\n\t\treturn -1;\n\t}\n\n\ttr->comm_changed = true;\n\tthread__put(thread);\n\n\treturn 0;\n}\n\nstatic int perf_sched__read_events(struct perf_sched *sched)\n{\n\tstruct evsel_str_handler handlers[] = {\n\t\t{ \"sched:sched_switch\",\t      process_sched_switch_event, },\n\t\t{ \"sched:sched_stat_runtime\", process_sched_runtime_event, },\n\t\t{ \"sched:sched_wakeup\",\t      process_sched_wakeup_event, },\n\t\t{ \"sched:sched_waking\",\t      process_sched_wakeup_event, },\n\t\t{ \"sched:sched_wakeup_new\",   process_sched_wakeup_event, },\n\t\t{ \"sched:sched_migrate_task\", process_sched_migrate_task_event, },\n\t};\n\tstruct perf_session *session;\n\tstruct perf_data data = {\n\t\t.path  = input_name,\n\t\t.mode  = PERF_DATA_MODE_READ,\n\t\t.force = sched->force,\n\t};\n\tint rc = -1;\n\n\tsession = perf_session__new(&data, &sched->tool);\n\tif (IS_ERR(session)) {\n\t\tpr_debug(\"Error creating perf session\");\n\t\treturn PTR_ERR(session);\n\t}\n\n\tsymbol__init(&session->header.env);\n\n\t \n\tif (evlist__find_tracepoint_by_name(session->evlist, \"sched:sched_waking\"))\n\t\thandlers[2].handler = process_sched_wakeup_ignore;\n\n\tif (perf_session__set_tracepoints_handlers(session, handlers))\n\t\tgoto out_delete;\n\n\tif (perf_session__has_traces(session, \"record -R\")) {\n\t\tint err = perf_session__process_events(session);\n\t\tif (err) {\n\t\t\tpr_err(\"Failed to process events, error %d\", err);\n\t\t\tgoto out_delete;\n\t\t}\n\n\t\tsched->nr_events      = session->evlist->stats.nr_events[0];\n\t\tsched->nr_lost_events = session->evlist->stats.total_lost;\n\t\tsched->nr_lost_chunks = session->evlist->stats.nr_events[PERF_RECORD_LOST];\n\t}\n\n\trc = 0;\nout_delete:\n\tperf_session__delete(session);\n\treturn rc;\n}\n\n \nstatic inline void print_sched_time(unsigned long long nsecs, int width)\n{\n\tunsigned long msecs;\n\tunsigned long usecs;\n\n\tmsecs  = nsecs / NSEC_PER_MSEC;\n\tnsecs -= msecs * NSEC_PER_MSEC;\n\tusecs  = nsecs / NSEC_PER_USEC;\n\tprintf(\"%*lu.%03lu \", width, msecs, usecs);\n}\n\n \nstatic struct evsel_runtime *evsel__get_runtime(struct evsel *evsel)\n{\n\tstruct evsel_runtime *r = evsel->priv;\n\n\tif (r == NULL) {\n\t\tr = zalloc(sizeof(struct evsel_runtime));\n\t\tevsel->priv = r;\n\t}\n\n\treturn r;\n}\n\n \nstatic void evsel__save_time(struct evsel *evsel, u64 timestamp, u32 cpu)\n{\n\tstruct evsel_runtime *r = evsel__get_runtime(evsel);\n\n\tif (r == NULL)\n\t\treturn;\n\n\tif ((cpu >= r->ncpu) || (r->last_time == NULL)) {\n\t\tint i, n = __roundup_pow_of_two(cpu+1);\n\t\tvoid *p = r->last_time;\n\n\t\tp = realloc(r->last_time, n * sizeof(u64));\n\t\tif (!p)\n\t\t\treturn;\n\n\t\tr->last_time = p;\n\t\tfor (i = r->ncpu; i < n; ++i)\n\t\t\tr->last_time[i] = (u64) 0;\n\n\t\tr->ncpu = n;\n\t}\n\n\tr->last_time[cpu] = timestamp;\n}\n\n \nstatic u64 evsel__get_time(struct evsel *evsel, u32 cpu)\n{\n\tstruct evsel_runtime *r = evsel__get_runtime(evsel);\n\n\tif ((r == NULL) || (r->last_time == NULL) || (cpu >= r->ncpu))\n\t\treturn 0;\n\n\treturn r->last_time[cpu];\n}\n\nstatic int comm_width = 30;\n\nstatic char *timehist_get_commstr(struct thread *thread)\n{\n\tstatic char str[32];\n\tconst char *comm = thread__comm_str(thread);\n\tpid_t tid = thread__tid(thread);\n\tpid_t pid = thread__pid(thread);\n\tint n;\n\n\tif (pid == 0)\n\t\tn = scnprintf(str, sizeof(str), \"%s\", comm);\n\n\telse if (tid != pid)\n\t\tn = scnprintf(str, sizeof(str), \"%s[%d/%d]\", comm, tid, pid);\n\n\telse\n\t\tn = scnprintf(str, sizeof(str), \"%s[%d]\", comm, tid);\n\n\tif (n > comm_width)\n\t\tcomm_width = n;\n\n\treturn str;\n}\n\nstatic void timehist_header(struct perf_sched *sched)\n{\n\tu32 ncpus = sched->max_cpu.cpu + 1;\n\tu32 i, j;\n\n\tprintf(\"%15s %6s \", \"time\", \"cpu\");\n\n\tif (sched->show_cpu_visual) {\n\t\tprintf(\" \");\n\t\tfor (i = 0, j = 0; i < ncpus; ++i) {\n\t\t\tprintf(\"%x\", j++);\n\t\t\tif (j > 15)\n\t\t\t\tj = 0;\n\t\t}\n\t\tprintf(\" \");\n\t}\n\n\tprintf(\" %-*s  %9s  %9s  %9s\", comm_width,\n\t\t\"task name\", \"wait time\", \"sch delay\", \"run time\");\n\n\tif (sched->show_state)\n\t\tprintf(\"  %s\", \"state\");\n\n\tprintf(\"\\n\");\n\n\t \n\tprintf(\"%15s %-6s \", \"\", \"\");\n\n\tif (sched->show_cpu_visual)\n\t\tprintf(\" %*s \", ncpus, \"\");\n\n\tprintf(\" %-*s  %9s  %9s  %9s\", comm_width,\n\t       \"[tid/pid]\", \"(msec)\", \"(msec)\", \"(msec)\");\n\n\tif (sched->show_state)\n\t\tprintf(\"  %5s\", \"\");\n\n\tprintf(\"\\n\");\n\n\t \n\tprintf(\"%.15s %.6s \", graph_dotted_line, graph_dotted_line);\n\n\tif (sched->show_cpu_visual)\n\t\tprintf(\" %.*s \", ncpus, graph_dotted_line);\n\n\tprintf(\" %.*s  %.9s  %.9s  %.9s\", comm_width,\n\t\tgraph_dotted_line, graph_dotted_line, graph_dotted_line,\n\t\tgraph_dotted_line);\n\n\tif (sched->show_state)\n\t\tprintf(\"  %.5s\", graph_dotted_line);\n\n\tprintf(\"\\n\");\n}\n\nstatic char task_state_char(struct thread *thread, int state)\n{\n\tstatic const char state_to_char[] = TASK_STATE_TO_CHAR_STR;\n\tunsigned bit = state ? ffs(state) : 0;\n\n\t \n\tif (thread__tid(thread) == 0)\n\t\treturn 'I';\n\n\treturn bit < sizeof(state_to_char) - 1 ? state_to_char[bit] : '?';\n}\n\nstatic void timehist_print_sample(struct perf_sched *sched,\n\t\t\t\t  struct evsel *evsel,\n\t\t\t\t  struct perf_sample *sample,\n\t\t\t\t  struct addr_location *al,\n\t\t\t\t  struct thread *thread,\n\t\t\t\t  u64 t, int state)\n{\n\tstruct thread_runtime *tr = thread__priv(thread);\n\tconst char *next_comm = evsel__strval(evsel, sample, \"next_comm\");\n\tconst u32 next_pid = evsel__intval(evsel, sample, \"next_pid\");\n\tu32 max_cpus = sched->max_cpu.cpu + 1;\n\tchar tstr[64];\n\tchar nstr[30];\n\tu64 wait_time;\n\n\tif (cpu_list && !test_bit(sample->cpu, cpu_bitmap))\n\t\treturn;\n\n\ttimestamp__scnprintf_usec(t, tstr, sizeof(tstr));\n\tprintf(\"%15s [%04d] \", tstr, sample->cpu);\n\n\tif (sched->show_cpu_visual) {\n\t\tu32 i;\n\t\tchar c;\n\n\t\tprintf(\" \");\n\t\tfor (i = 0; i < max_cpus; ++i) {\n\t\t\t \n\t\t\tif (i == sample->cpu)\n\t\t\t\tc = (thread__tid(thread) == 0) ? 'i' : 's';\n\t\t\telse\n\t\t\t\tc = ' ';\n\t\t\tprintf(\"%c\", c);\n\t\t}\n\t\tprintf(\" \");\n\t}\n\n\tprintf(\" %-*s \", comm_width, timehist_get_commstr(thread));\n\n\twait_time = tr->dt_sleep + tr->dt_iowait + tr->dt_preempt;\n\tprint_sched_time(wait_time, 6);\n\n\tprint_sched_time(tr->dt_delay, 6);\n\tprint_sched_time(tr->dt_run, 6);\n\n\tif (sched->show_state)\n\t\tprintf(\" %5c \", task_state_char(thread, state));\n\n\tif (sched->show_next) {\n\t\tsnprintf(nstr, sizeof(nstr), \"next: %s[%d]\", next_comm, next_pid);\n\t\tprintf(\" %-*s\", comm_width, nstr);\n\t}\n\n\tif (sched->show_wakeups && !sched->show_next)\n\t\tprintf(\"  %-*s\", comm_width, \"\");\n\n\tif (thread__tid(thread) == 0)\n\t\tgoto out;\n\n\tif (sched->show_callchain)\n\t\tprintf(\"  \");\n\n\tsample__fprintf_sym(sample, al, 0,\n\t\t\t    EVSEL__PRINT_SYM | EVSEL__PRINT_ONELINE |\n\t\t\t    EVSEL__PRINT_CALLCHAIN_ARROW |\n\t\t\t    EVSEL__PRINT_SKIP_IGNORED,\n\t\t\t    get_tls_callchain_cursor(), symbol_conf.bt_stop_list,  stdout);\n\nout:\n\tprintf(\"\\n\");\n}\n\n \n\nstatic void timehist_update_runtime_stats(struct thread_runtime *r,\n\t\t\t\t\t u64 t, u64 tprev)\n{\n\tr->dt_delay   = 0;\n\tr->dt_sleep   = 0;\n\tr->dt_iowait  = 0;\n\tr->dt_preempt = 0;\n\tr->dt_run     = 0;\n\n\tif (tprev) {\n\t\tr->dt_run = t - tprev;\n\t\tif (r->ready_to_run) {\n\t\t\tif (r->ready_to_run > tprev)\n\t\t\t\tpr_debug(\"time travel: wakeup time for task > previous sched_switch event\\n\");\n\t\t\telse\n\t\t\t\tr->dt_delay = tprev - r->ready_to_run;\n\t\t}\n\n\t\tif (r->last_time > tprev)\n\t\t\tpr_debug(\"time travel: last sched out time for task > previous sched_switch event\\n\");\n\t\telse if (r->last_time) {\n\t\t\tu64 dt_wait = tprev - r->last_time;\n\n\t\t\tif (r->last_state == TASK_RUNNING)\n\t\t\t\tr->dt_preempt = dt_wait;\n\t\t\telse if (r->last_state == TASK_UNINTERRUPTIBLE)\n\t\t\t\tr->dt_iowait = dt_wait;\n\t\t\telse\n\t\t\t\tr->dt_sleep = dt_wait;\n\t\t}\n\t}\n\n\tupdate_stats(&r->run_stats, r->dt_run);\n\n\tr->total_run_time     += r->dt_run;\n\tr->total_delay_time   += r->dt_delay;\n\tr->total_sleep_time   += r->dt_sleep;\n\tr->total_iowait_time  += r->dt_iowait;\n\tr->total_preempt_time += r->dt_preempt;\n}\n\nstatic bool is_idle_sample(struct perf_sample *sample,\n\t\t\t   struct evsel *evsel)\n{\n\t \n\tif (strcmp(evsel__name(evsel), \"sched:sched_switch\") == 0)\n\t\treturn evsel__intval(evsel, sample, \"prev_pid\") == 0;\n\n\treturn sample->pid == 0;\n}\n\nstatic void save_task_callchain(struct perf_sched *sched,\n\t\t\t\tstruct perf_sample *sample,\n\t\t\t\tstruct evsel *evsel,\n\t\t\t\tstruct machine *machine)\n{\n\tstruct callchain_cursor *cursor;\n\tstruct thread *thread;\n\n\t \n\tthread = machine__findnew_thread(machine, sample->pid, sample->pid);\n\tif (thread == NULL) {\n\t\tpr_debug(\"Failed to get thread for pid %d.\\n\", sample->pid);\n\t\treturn;\n\t}\n\n\tif (!sched->show_callchain || sample->callchain == NULL)\n\t\treturn;\n\n\tcursor = get_tls_callchain_cursor();\n\n\tif (thread__resolve_callchain(thread, cursor, evsel, sample,\n\t\t\t\t      NULL, NULL, sched->max_stack + 2) != 0) {\n\t\tif (verbose > 0)\n\t\t\tpr_err(\"Failed to resolve callchain. Skipping\\n\");\n\n\t\treturn;\n\t}\n\n\tcallchain_cursor_commit(cursor);\n\n\twhile (true) {\n\t\tstruct callchain_cursor_node *node;\n\t\tstruct symbol *sym;\n\n\t\tnode = callchain_cursor_current(cursor);\n\t\tif (node == NULL)\n\t\t\tbreak;\n\n\t\tsym = node->ms.sym;\n\t\tif (sym) {\n\t\t\tif (!strcmp(sym->name, \"schedule\") ||\n\t\t\t    !strcmp(sym->name, \"__schedule\") ||\n\t\t\t    !strcmp(sym->name, \"preempt_schedule\"))\n\t\t\t\tsym->ignore = 1;\n\t\t}\n\n\t\tcallchain_cursor_advance(cursor);\n\t}\n}\n\nstatic int init_idle_thread(struct thread *thread)\n{\n\tstruct idle_thread_runtime *itr;\n\n\tthread__set_comm(thread, idle_comm, 0);\n\n\titr = zalloc(sizeof(*itr));\n\tif (itr == NULL)\n\t\treturn -ENOMEM;\n\n\tinit_stats(&itr->tr.run_stats);\n\tcallchain_init(&itr->callchain);\n\tcallchain_cursor_reset(&itr->cursor);\n\tthread__set_priv(thread, itr);\n\n\treturn 0;\n}\n\n \nstatic int init_idle_threads(int ncpu)\n{\n\tint i, ret;\n\n\tidle_threads = zalloc(ncpu * sizeof(struct thread *));\n\tif (!idle_threads)\n\t\treturn -ENOMEM;\n\n\tidle_max_cpu = ncpu;\n\n\t \n\tfor (i = 0; i < ncpu; ++i) {\n\t\tidle_threads[i] = thread__new(0, 0);\n\t\tif (idle_threads[i] == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tret = init_idle_thread(idle_threads[i]);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void free_idle_threads(void)\n{\n\tint i;\n\n\tif (idle_threads == NULL)\n\t\treturn;\n\n\tfor (i = 0; i < idle_max_cpu; ++i) {\n\t\tif ((idle_threads[i]))\n\t\t\tthread__delete(idle_threads[i]);\n\t}\n\n\tfree(idle_threads);\n}\n\nstatic struct thread *get_idle_thread(int cpu)\n{\n\t \n\tif ((cpu >= idle_max_cpu) || (idle_threads == NULL)) {\n\t\tint i, j = __roundup_pow_of_two(cpu+1);\n\t\tvoid *p;\n\n\t\tp = realloc(idle_threads, j * sizeof(struct thread *));\n\t\tif (!p)\n\t\t\treturn NULL;\n\n\t\tidle_threads = (struct thread **) p;\n\t\tfor (i = idle_max_cpu; i < j; ++i)\n\t\t\tidle_threads[i] = NULL;\n\n\t\tidle_max_cpu = j;\n\t}\n\n\t \n\tif (idle_threads[cpu] == NULL) {\n\t\tidle_threads[cpu] = thread__new(0, 0);\n\n\t\tif (idle_threads[cpu]) {\n\t\t\tif (init_idle_thread(idle_threads[cpu]) < 0)\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn idle_threads[cpu];\n}\n\nstatic void save_idle_callchain(struct perf_sched *sched,\n\t\t\t\tstruct idle_thread_runtime *itr,\n\t\t\t\tstruct perf_sample *sample)\n{\n\tstruct callchain_cursor *cursor;\n\n\tif (!sched->show_callchain || sample->callchain == NULL)\n\t\treturn;\n\n\tcursor = get_tls_callchain_cursor();\n\tif (cursor == NULL)\n\t\treturn;\n\n\tcallchain_cursor__copy(&itr->cursor, cursor);\n}\n\nstatic struct thread *timehist_get_thread(struct perf_sched *sched,\n\t\t\t\t\t  struct perf_sample *sample,\n\t\t\t\t\t  struct machine *machine,\n\t\t\t\t\t  struct evsel *evsel)\n{\n\tstruct thread *thread;\n\n\tif (is_idle_sample(sample, evsel)) {\n\t\tthread = get_idle_thread(sample->cpu);\n\t\tif (thread == NULL)\n\t\t\tpr_err(\"Failed to get idle thread for cpu %d.\\n\", sample->cpu);\n\n\t} else {\n\t\t \n\t\tthread = machine__findnew_thread(machine, sample->pid,\n\t\t\t\t\t\t sample->tid ?: sample->pid);\n\t\tif (thread == NULL) {\n\t\t\tpr_debug(\"Failed to get thread for tid %d. skipping sample.\\n\",\n\t\t\t\t sample->tid);\n\t\t}\n\n\t\tsave_task_callchain(sched, sample, evsel, machine);\n\t\tif (sched->idle_hist) {\n\t\t\tstruct thread *idle;\n\t\t\tstruct idle_thread_runtime *itr;\n\n\t\t\tidle = get_idle_thread(sample->cpu);\n\t\t\tif (idle == NULL) {\n\t\t\t\tpr_err(\"Failed to get idle thread for cpu %d.\\n\", sample->cpu);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\titr = thread__priv(idle);\n\t\t\tif (itr == NULL)\n\t\t\t\treturn NULL;\n\n\t\t\titr->last_thread = thread;\n\n\t\t\t \n\t\t\tif (evsel__intval(evsel, sample, \"next_pid\") == 0)\n\t\t\t\tsave_idle_callchain(sched, itr, sample);\n\t\t}\n\t}\n\n\treturn thread;\n}\n\nstatic bool timehist_skip_sample(struct perf_sched *sched,\n\t\t\t\t struct thread *thread,\n\t\t\t\t struct evsel *evsel,\n\t\t\t\t struct perf_sample *sample)\n{\n\tbool rc = false;\n\n\tif (thread__is_filtered(thread)) {\n\t\trc = true;\n\t\tsched->skipped_samples++;\n\t}\n\n\tif (sched->idle_hist) {\n\t\tif (strcmp(evsel__name(evsel), \"sched:sched_switch\"))\n\t\t\trc = true;\n\t\telse if (evsel__intval(evsel, sample, \"prev_pid\") != 0 &&\n\t\t\t evsel__intval(evsel, sample, \"next_pid\") != 0)\n\t\t\trc = true;\n\t}\n\n\treturn rc;\n}\n\nstatic void timehist_print_wakeup_event(struct perf_sched *sched,\n\t\t\t\t\tstruct evsel *evsel,\n\t\t\t\t\tstruct perf_sample *sample,\n\t\t\t\t\tstruct machine *machine,\n\t\t\t\t\tstruct thread *awakened)\n{\n\tstruct thread *thread;\n\tchar tstr[64];\n\n\tthread = machine__findnew_thread(machine, sample->pid, sample->tid);\n\tif (thread == NULL)\n\t\treturn;\n\n\t \n\tif (timehist_skip_sample(sched, thread, evsel, sample) &&\n\t    timehist_skip_sample(sched, awakened, evsel, sample)) {\n\t\treturn;\n\t}\n\n\ttimestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));\n\tprintf(\"%15s [%04d] \", tstr, sample->cpu);\n\tif (sched->show_cpu_visual)\n\t\tprintf(\" %*s \", sched->max_cpu.cpu + 1, \"\");\n\n\tprintf(\" %-*s \", comm_width, timehist_get_commstr(thread));\n\n\t \n\tprintf(\"  %9s  %9s  %9s \", \"\", \"\", \"\");\n\n\tprintf(\"awakened: %s\", timehist_get_commstr(awakened));\n\n\tprintf(\"\\n\");\n}\n\nstatic int timehist_sched_wakeup_ignore(struct perf_tool *tool __maybe_unused,\n\t\t\t\t\tunion perf_event *event __maybe_unused,\n\t\t\t\t\tstruct evsel *evsel __maybe_unused,\n\t\t\t\t\tstruct perf_sample *sample __maybe_unused,\n\t\t\t\t\tstruct machine *machine __maybe_unused)\n{\n\treturn 0;\n}\n\nstatic int timehist_sched_wakeup_event(struct perf_tool *tool,\n\t\t\t\t       union perf_event *event __maybe_unused,\n\t\t\t\t       struct evsel *evsel,\n\t\t\t\t       struct perf_sample *sample,\n\t\t\t\t       struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\tstruct thread *thread;\n\tstruct thread_runtime *tr = NULL;\n\t \n\tconst u32 pid = evsel__intval(evsel, sample, \"pid\");\n\n\tthread = machine__findnew_thread(machine, 0, pid);\n\tif (thread == NULL)\n\t\treturn -1;\n\n\ttr = thread__get_runtime(thread);\n\tif (tr == NULL)\n\t\treturn -1;\n\n\tif (tr->ready_to_run == 0)\n\t\ttr->ready_to_run = sample->time;\n\n\t \n\tif (sched->show_wakeups &&\n\t    !perf_time__skip_sample(&sched->ptime, sample->time))\n\t\ttimehist_print_wakeup_event(sched, evsel, sample, machine, thread);\n\n\treturn 0;\n}\n\nstatic void timehist_print_migration_event(struct perf_sched *sched,\n\t\t\t\t\tstruct evsel *evsel,\n\t\t\t\t\tstruct perf_sample *sample,\n\t\t\t\t\tstruct machine *machine,\n\t\t\t\t\tstruct thread *migrated)\n{\n\tstruct thread *thread;\n\tchar tstr[64];\n\tu32 max_cpus;\n\tu32 ocpu, dcpu;\n\n\tif (sched->summary_only)\n\t\treturn;\n\n\tmax_cpus = sched->max_cpu.cpu + 1;\n\tocpu = evsel__intval(evsel, sample, \"orig_cpu\");\n\tdcpu = evsel__intval(evsel, sample, \"dest_cpu\");\n\n\tthread = machine__findnew_thread(machine, sample->pid, sample->tid);\n\tif (thread == NULL)\n\t\treturn;\n\n\tif (timehist_skip_sample(sched, thread, evsel, sample) &&\n\t    timehist_skip_sample(sched, migrated, evsel, sample)) {\n\t\treturn;\n\t}\n\n\ttimestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));\n\tprintf(\"%15s [%04d] \", tstr, sample->cpu);\n\n\tif (sched->show_cpu_visual) {\n\t\tu32 i;\n\t\tchar c;\n\n\t\tprintf(\"  \");\n\t\tfor (i = 0; i < max_cpus; ++i) {\n\t\t\tc = (i == sample->cpu) ? 'm' : ' ';\n\t\t\tprintf(\"%c\", c);\n\t\t}\n\t\tprintf(\"  \");\n\t}\n\n\tprintf(\" %-*s \", comm_width, timehist_get_commstr(thread));\n\n\t \n\tprintf(\"  %9s  %9s  %9s \", \"\", \"\", \"\");\n\n\tprintf(\"migrated: %s\", timehist_get_commstr(migrated));\n\tprintf(\" cpu %d => %d\", ocpu, dcpu);\n\n\tprintf(\"\\n\");\n}\n\nstatic int timehist_migrate_task_event(struct perf_tool *tool,\n\t\t\t\t       union perf_event *event __maybe_unused,\n\t\t\t\t       struct evsel *evsel,\n\t\t\t\t       struct perf_sample *sample,\n\t\t\t\t       struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\tstruct thread *thread;\n\tstruct thread_runtime *tr = NULL;\n\t \n\tconst u32 pid = evsel__intval(evsel, sample, \"pid\");\n\n\tthread = machine__findnew_thread(machine, 0, pid);\n\tif (thread == NULL)\n\t\treturn -1;\n\n\ttr = thread__get_runtime(thread);\n\tif (tr == NULL)\n\t\treturn -1;\n\n\ttr->migrations++;\n\n\t \n\ttimehist_print_migration_event(sched, evsel, sample, machine, thread);\n\n\treturn 0;\n}\n\nstatic int timehist_sched_change_event(struct perf_tool *tool,\n\t\t\t\t       union perf_event *event,\n\t\t\t\t       struct evsel *evsel,\n\t\t\t\t       struct perf_sample *sample,\n\t\t\t\t       struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\tstruct perf_time_interval *ptime = &sched->ptime;\n\tstruct addr_location al;\n\tstruct thread *thread;\n\tstruct thread_runtime *tr = NULL;\n\tu64 tprev, t = sample->time;\n\tint rc = 0;\n\tint state = evsel__intval(evsel, sample, \"prev_state\");\n\n\taddr_location__init(&al);\n\tif (machine__resolve(machine, &al, sample) < 0) {\n\t\tpr_err(\"problem processing %d event. skipping it\\n\",\n\t\t       event->header.type);\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\tthread = timehist_get_thread(sched, sample, machine, evsel);\n\tif (thread == NULL) {\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\tif (timehist_skip_sample(sched, thread, evsel, sample))\n\t\tgoto out;\n\n\ttr = thread__get_runtime(thread);\n\tif (tr == NULL) {\n\t\trc = -1;\n\t\tgoto out;\n\t}\n\n\ttprev = evsel__get_time(evsel, sample->cpu);\n\n\t \n\tif (ptime->start && ptime->start > t)\n\t\tgoto out;\n\n\tif (tprev && ptime->start > tprev)\n\t\ttprev = ptime->start;\n\n\t \n\tif (ptime->end) {\n\t\tif (tprev > ptime->end)\n\t\t\tgoto out;\n\n\t\tif (t > ptime->end)\n\t\t\tt = ptime->end;\n\t}\n\n\tif (!sched->idle_hist || thread__tid(thread) == 0) {\n\t\tif (!cpu_list || test_bit(sample->cpu, cpu_bitmap))\n\t\t\ttimehist_update_runtime_stats(tr, t, tprev);\n\n\t\tif (sched->idle_hist) {\n\t\t\tstruct idle_thread_runtime *itr = (void *)tr;\n\t\t\tstruct thread_runtime *last_tr;\n\n\t\t\tBUG_ON(thread__tid(thread) != 0);\n\n\t\t\tif (itr->last_thread == NULL)\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\tlast_tr = thread__get_runtime(itr->last_thread);\n\t\t\tif (last_tr == NULL)\n\t\t\t\tgoto out;\n\n\t\t\ttimehist_update_runtime_stats(last_tr, t, tprev);\n\t\t\t \n\t\t\tlast_tr->dt_run = 0;\n\t\t\tlast_tr->dt_delay = 0;\n\t\t\tlast_tr->dt_sleep = 0;\n\t\t\tlast_tr->dt_iowait = 0;\n\t\t\tlast_tr->dt_preempt = 0;\n\n\t\t\tif (itr->cursor.nr)\n\t\t\t\tcallchain_append(&itr->callchain, &itr->cursor, t - tprev);\n\n\t\t\titr->last_thread = NULL;\n\t\t}\n\t}\n\n\tif (!sched->summary_only)\n\t\ttimehist_print_sample(sched, evsel, sample, &al, thread, t, state);\n\nout:\n\tif (sched->hist_time.start == 0 && t >= ptime->start)\n\t\tsched->hist_time.start = t;\n\tif (ptime->end == 0 || t <= ptime->end)\n\t\tsched->hist_time.end = t;\n\n\tif (tr) {\n\t\t \n\t\ttr->last_time = sample->time;\n\n\t\t \n\t\ttr->last_state = state;\n\n\t\t \n\t\ttr->ready_to_run = 0;\n\t}\n\n\tevsel__save_time(evsel, sample->time, sample->cpu);\n\n\taddr_location__exit(&al);\n\treturn rc;\n}\n\nstatic int timehist_sched_switch_event(struct perf_tool *tool,\n\t\t\t     union perf_event *event,\n\t\t\t     struct evsel *evsel,\n\t\t\t     struct perf_sample *sample,\n\t\t\t     struct machine *machine __maybe_unused)\n{\n\treturn timehist_sched_change_event(tool, event, evsel, sample, machine);\n}\n\nstatic int process_lost(struct perf_tool *tool __maybe_unused,\n\t\t\tunion perf_event *event,\n\t\t\tstruct perf_sample *sample,\n\t\t\tstruct machine *machine __maybe_unused)\n{\n\tchar tstr[64];\n\n\ttimestamp__scnprintf_usec(sample->time, tstr, sizeof(tstr));\n\tprintf(\"%15s \", tstr);\n\tprintf(\"lost %\" PRI_lu64 \" events on cpu %d\\n\", event->lost.lost, sample->cpu);\n\n\treturn 0;\n}\n\n\nstatic void print_thread_runtime(struct thread *t,\n\t\t\t\t struct thread_runtime *r)\n{\n\tdouble mean = avg_stats(&r->run_stats);\n\tfloat stddev;\n\n\tprintf(\"%*s   %5d  %9\" PRIu64 \" \",\n\t       comm_width, timehist_get_commstr(t), thread__ppid(t),\n\t       (u64) r->run_stats.n);\n\n\tprint_sched_time(r->total_run_time, 8);\n\tstddev = rel_stddev_stats(stddev_stats(&r->run_stats), mean);\n\tprint_sched_time(r->run_stats.min, 6);\n\tprintf(\" \");\n\tprint_sched_time((u64) mean, 6);\n\tprintf(\" \");\n\tprint_sched_time(r->run_stats.max, 6);\n\tprintf(\"  \");\n\tprintf(\"%5.2f\", stddev);\n\tprintf(\"   %5\" PRIu64, r->migrations);\n\tprintf(\"\\n\");\n}\n\nstatic void print_thread_waittime(struct thread *t,\n\t\t\t\t  struct thread_runtime *r)\n{\n\tprintf(\"%*s   %5d  %9\" PRIu64 \" \",\n\t       comm_width, timehist_get_commstr(t), thread__ppid(t),\n\t       (u64) r->run_stats.n);\n\n\tprint_sched_time(r->total_run_time, 8);\n\tprint_sched_time(r->total_sleep_time, 6);\n\tprintf(\" \");\n\tprint_sched_time(r->total_iowait_time, 6);\n\tprintf(\" \");\n\tprint_sched_time(r->total_preempt_time, 6);\n\tprintf(\" \");\n\tprint_sched_time(r->total_delay_time, 6);\n\tprintf(\"\\n\");\n}\n\nstruct total_run_stats {\n\tstruct perf_sched *sched;\n\tu64  sched_count;\n\tu64  task_count;\n\tu64  total_run_time;\n};\n\nstatic int show_thread_runtime(struct thread *t, void *priv)\n{\n\tstruct total_run_stats *stats = priv;\n\tstruct thread_runtime *r;\n\n\tif (thread__is_filtered(t))\n\t\treturn 0;\n\n\tr = thread__priv(t);\n\tif (r && r->run_stats.n) {\n\t\tstats->task_count++;\n\t\tstats->sched_count += r->run_stats.n;\n\t\tstats->total_run_time += r->total_run_time;\n\n\t\tif (stats->sched->show_state)\n\t\t\tprint_thread_waittime(t, r);\n\t\telse\n\t\t\tprint_thread_runtime(t, r);\n\t}\n\n\treturn 0;\n}\n\nstatic size_t callchain__fprintf_folded(FILE *fp, struct callchain_node *node)\n{\n\tconst char *sep = \" <- \";\n\tstruct callchain_list *chain;\n\tsize_t ret = 0;\n\tchar bf[1024];\n\tbool first;\n\n\tif (node == NULL)\n\t\treturn 0;\n\n\tret = callchain__fprintf_folded(fp, node->parent);\n\tfirst = (ret == 0);\n\n\tlist_for_each_entry(chain, &node->val, list) {\n\t\tif (chain->ip >= PERF_CONTEXT_MAX)\n\t\t\tcontinue;\n\t\tif (chain->ms.sym && chain->ms.sym->ignore)\n\t\t\tcontinue;\n\t\tret += fprintf(fp, \"%s%s\", first ? \"\" : sep,\n\t\t\t       callchain_list__sym_name(chain, bf, sizeof(bf),\n\t\t\t\t\t\t\tfalse));\n\t\tfirst = false;\n\t}\n\n\treturn ret;\n}\n\nstatic size_t timehist_print_idlehist_callchain(struct rb_root_cached *root)\n{\n\tsize_t ret = 0;\n\tFILE *fp = stdout;\n\tstruct callchain_node *chain;\n\tstruct rb_node *rb_node = rb_first_cached(root);\n\n\tprintf(\"  %16s  %8s  %s\\n\", \"Idle time (msec)\", \"Count\", \"Callchains\");\n\tprintf(\"  %.16s  %.8s  %.50s\\n\", graph_dotted_line, graph_dotted_line,\n\t       graph_dotted_line);\n\n\twhile (rb_node) {\n\t\tchain = rb_entry(rb_node, struct callchain_node, rb_node);\n\t\trb_node = rb_next(rb_node);\n\n\t\tret += fprintf(fp, \"  \");\n\t\tprint_sched_time(chain->hit, 12);\n\t\tret += 16;   \n\t\tret += fprintf(fp, \" %8d  \", chain->count);\n\t\tret += callchain__fprintf_folded(fp, chain);\n\t\tret += fprintf(fp, \"\\n\");\n\t}\n\n\treturn ret;\n}\n\nstatic void timehist_print_summary(struct perf_sched *sched,\n\t\t\t\t   struct perf_session *session)\n{\n\tstruct machine *m = &session->machines.host;\n\tstruct total_run_stats totals;\n\tu64 task_count;\n\tstruct thread *t;\n\tstruct thread_runtime *r;\n\tint i;\n\tu64 hist_time = sched->hist_time.end - sched->hist_time.start;\n\n\tmemset(&totals, 0, sizeof(totals));\n\ttotals.sched = sched;\n\n\tif (sched->idle_hist) {\n\t\tprintf(\"\\nIdle-time summary\\n\");\n\t\tprintf(\"%*s  parent  sched-out  \", comm_width, \"comm\");\n\t\tprintf(\"  idle-time   min-idle    avg-idle    max-idle  stddev  migrations\\n\");\n\t} else if (sched->show_state) {\n\t\tprintf(\"\\nWait-time summary\\n\");\n\t\tprintf(\"%*s  parent   sched-in  \", comm_width, \"comm\");\n\t\tprintf(\"   run-time      sleep      iowait     preempt       delay\\n\");\n\t} else {\n\t\tprintf(\"\\nRuntime summary\\n\");\n\t\tprintf(\"%*s  parent   sched-in  \", comm_width, \"comm\");\n\t\tprintf(\"   run-time    min-run     avg-run     max-run  stddev  migrations\\n\");\n\t}\n\tprintf(\"%*s            (count)  \", comm_width, \"\");\n\tprintf(\"     (msec)     (msec)      (msec)      (msec)       %s\\n\",\n\t       sched->show_state ? \"(msec)\" : \"%\");\n\tprintf(\"%.117s\\n\", graph_dotted_line);\n\n\tmachine__for_each_thread(m, show_thread_runtime, &totals);\n\ttask_count = totals.task_count;\n\tif (!task_count)\n\t\tprintf(\"<no still running tasks>\\n\");\n\n\t \n\tif (sched->skipped_samples && !sched->idle_hist)\n\t\treturn;\n\n\tprintf(\"\\nIdle stats:\\n\");\n\tfor (i = 0; i < idle_max_cpu; ++i) {\n\t\tif (cpu_list && !test_bit(i, cpu_bitmap))\n\t\t\tcontinue;\n\n\t\tt = idle_threads[i];\n\t\tif (!t)\n\t\t\tcontinue;\n\n\t\tr = thread__priv(t);\n\t\tif (r && r->run_stats.n) {\n\t\t\ttotals.sched_count += r->run_stats.n;\n\t\t\tprintf(\"    CPU %2d idle for \", i);\n\t\t\tprint_sched_time(r->total_run_time, 6);\n\t\t\tprintf(\" msec  (%6.2f%%)\\n\", 100.0 * r->total_run_time / hist_time);\n\t\t} else\n\t\t\tprintf(\"    CPU %2d idle entire time window\\n\", i);\n\t}\n\n\tif (sched->idle_hist && sched->show_callchain) {\n\t\tcallchain_param.mode  = CHAIN_FOLDED;\n\t\tcallchain_param.value = CCVAL_PERIOD;\n\n\t\tcallchain_register_param(&callchain_param);\n\n\t\tprintf(\"\\nIdle stats by callchain:\\n\");\n\t\tfor (i = 0; i < idle_max_cpu; ++i) {\n\t\t\tstruct idle_thread_runtime *itr;\n\n\t\t\tt = idle_threads[i];\n\t\t\tif (!t)\n\t\t\t\tcontinue;\n\n\t\t\titr = thread__priv(t);\n\t\t\tif (itr == NULL)\n\t\t\t\tcontinue;\n\n\t\t\tcallchain_param.sort(&itr->sorted_root.rb_root, &itr->callchain,\n\t\t\t\t\t     0, &callchain_param);\n\n\t\t\tprintf(\"  CPU %2d:\", i);\n\t\t\tprint_sched_time(itr->tr.total_run_time, 6);\n\t\t\tprintf(\" msec\\n\");\n\t\t\ttimehist_print_idlehist_callchain(&itr->sorted_root);\n\t\t\tprintf(\"\\n\");\n\t\t}\n\t}\n\n\tprintf(\"\\n\"\n\t       \"    Total number of unique tasks: %\" PRIu64 \"\\n\"\n\t       \"Total number of context switches: %\" PRIu64 \"\\n\",\n\t       totals.task_count, totals.sched_count);\n\n\tprintf(\"           Total run time (msec): \");\n\tprint_sched_time(totals.total_run_time, 2);\n\tprintf(\"\\n\");\n\n\tprintf(\"    Total scheduling time (msec): \");\n\tprint_sched_time(hist_time, 2);\n\tprintf(\" (x %d)\\n\", sched->max_cpu.cpu);\n}\n\ntypedef int (*sched_handler)(struct perf_tool *tool,\n\t\t\t  union perf_event *event,\n\t\t\t  struct evsel *evsel,\n\t\t\t  struct perf_sample *sample,\n\t\t\t  struct machine *machine);\n\nstatic int perf_timehist__process_sample(struct perf_tool *tool,\n\t\t\t\t\t union perf_event *event,\n\t\t\t\t\t struct perf_sample *sample,\n\t\t\t\t\t struct evsel *evsel,\n\t\t\t\t\t struct machine *machine)\n{\n\tstruct perf_sched *sched = container_of(tool, struct perf_sched, tool);\n\tint err = 0;\n\tstruct perf_cpu this_cpu = {\n\t\t.cpu = sample->cpu,\n\t};\n\n\tif (this_cpu.cpu > sched->max_cpu.cpu)\n\t\tsched->max_cpu = this_cpu;\n\n\tif (evsel->handler != NULL) {\n\t\tsched_handler f = evsel->handler;\n\n\t\terr = f(tool, event, evsel, sample, machine);\n\t}\n\n\treturn err;\n}\n\nstatic int timehist_check_attr(struct perf_sched *sched,\n\t\t\t       struct evlist *evlist)\n{\n\tstruct evsel *evsel;\n\tstruct evsel_runtime *er;\n\n\tlist_for_each_entry(evsel, &evlist->core.entries, core.node) {\n\t\ter = evsel__get_runtime(evsel);\n\t\tif (er == NULL) {\n\t\t\tpr_err(\"Failed to allocate memory for evsel runtime data\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (sched->show_callchain && !evsel__has_callchain(evsel)) {\n\t\t\tpr_info(\"Samples do not have callchains.\\n\");\n\t\t\tsched->show_callchain = 0;\n\t\t\tsymbol_conf.use_callchain = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int perf_sched__timehist(struct perf_sched *sched)\n{\n\tstruct evsel_str_handler handlers[] = {\n\t\t{ \"sched:sched_switch\",       timehist_sched_switch_event, },\n\t\t{ \"sched:sched_wakeup\",\t      timehist_sched_wakeup_event, },\n\t\t{ \"sched:sched_waking\",       timehist_sched_wakeup_event, },\n\t\t{ \"sched:sched_wakeup_new\",   timehist_sched_wakeup_event, },\n\t};\n\tconst struct evsel_str_handler migrate_handlers[] = {\n\t\t{ \"sched:sched_migrate_task\", timehist_migrate_task_event, },\n\t};\n\tstruct perf_data data = {\n\t\t.path  = input_name,\n\t\t.mode  = PERF_DATA_MODE_READ,\n\t\t.force = sched->force,\n\t};\n\n\tstruct perf_session *session;\n\tstruct evlist *evlist;\n\tint err = -1;\n\n\t \n\tsched->tool.sample\t = perf_timehist__process_sample;\n\tsched->tool.mmap\t = perf_event__process_mmap;\n\tsched->tool.comm\t = perf_event__process_comm;\n\tsched->tool.exit\t = perf_event__process_exit;\n\tsched->tool.fork\t = perf_event__process_fork;\n\tsched->tool.lost\t = process_lost;\n\tsched->tool.attr\t = perf_event__process_attr;\n\tsched->tool.tracing_data = perf_event__process_tracing_data;\n\tsched->tool.build_id\t = perf_event__process_build_id;\n\n\tsched->tool.ordered_events = true;\n\tsched->tool.ordering_requires_timestamps = true;\n\n\tsymbol_conf.use_callchain = sched->show_callchain;\n\n\tsession = perf_session__new(&data, &sched->tool);\n\tif (IS_ERR(session))\n\t\treturn PTR_ERR(session);\n\n\tif (cpu_list) {\n\t\terr = perf_session__cpu_bitmap(session, cpu_list, cpu_bitmap);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t}\n\n\tevlist = session->evlist;\n\n\tsymbol__init(&session->header.env);\n\n\tif (perf_time__parse_str(&sched->ptime, sched->time_str) != 0) {\n\t\tpr_err(\"Invalid time string\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (timehist_check_attr(sched, evlist) != 0)\n\t\tgoto out;\n\n\tsetup_pager();\n\n\t \n\tif (evlist__find_tracepoint_by_name(session->evlist, \"sched:sched_waking\"))\n\t\thandlers[1].handler = timehist_sched_wakeup_ignore;\n\n\t \n\tif (perf_session__set_tracepoints_handlers(session, handlers))\n\t\tgoto out;\n\n\t \n\tif (!evlist__find_tracepoint_by_name(session->evlist, \"sched:sched_switch\")) {\n\t\tpr_err(\"No sched_switch events found. Have you run 'perf sched record'?\\n\");\n\t\tgoto out;\n\t}\n\n\tif (sched->show_migrations &&\n\t    perf_session__set_tracepoints_handlers(session, migrate_handlers))\n\t\tgoto out;\n\n\t \n\tsched->max_cpu.cpu = session->header.env.nr_cpus_online;\n\tif (sched->max_cpu.cpu == 0)\n\t\tsched->max_cpu.cpu = 4;\n\tif (init_idle_threads(sched->max_cpu.cpu))\n\t\tgoto out;\n\n\t \n\tif (sched->summary_only)\n\t\tsched->summary = sched->summary_only;\n\n\tif (!sched->summary_only)\n\t\ttimehist_header(sched);\n\n\terr = perf_session__process_events(session);\n\tif (err) {\n\t\tpr_err(\"Failed to process events, error %d\", err);\n\t\tgoto out;\n\t}\n\n\tsched->nr_events      = evlist->stats.nr_events[0];\n\tsched->nr_lost_events = evlist->stats.total_lost;\n\tsched->nr_lost_chunks = evlist->stats.nr_events[PERF_RECORD_LOST];\n\n\tif (sched->summary)\n\t\ttimehist_print_summary(sched, session);\n\nout:\n\tfree_idle_threads();\n\tperf_session__delete(session);\n\n\treturn err;\n}\n\n\nstatic void print_bad_events(struct perf_sched *sched)\n{\n\tif (sched->nr_unordered_timestamps && sched->nr_timestamps) {\n\t\tprintf(\"  INFO: %.3f%% unordered timestamps (%ld out of %ld)\\n\",\n\t\t\t(double)sched->nr_unordered_timestamps/(double)sched->nr_timestamps*100.0,\n\t\t\tsched->nr_unordered_timestamps, sched->nr_timestamps);\n\t}\n\tif (sched->nr_lost_events && sched->nr_events) {\n\t\tprintf(\"  INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\\n\",\n\t\t\t(double)sched->nr_lost_events/(double)sched->nr_events * 100.0,\n\t\t\tsched->nr_lost_events, sched->nr_events, sched->nr_lost_chunks);\n\t}\n\tif (sched->nr_context_switch_bugs && sched->nr_timestamps) {\n\t\tprintf(\"  INFO: %.3f%% context switch bugs (%ld out of %ld)\",\n\t\t\t(double)sched->nr_context_switch_bugs/(double)sched->nr_timestamps*100.0,\n\t\t\tsched->nr_context_switch_bugs, sched->nr_timestamps);\n\t\tif (sched->nr_lost_events)\n\t\t\tprintf(\" (due to lost events?)\");\n\t\tprintf(\"\\n\");\n\t}\n}\n\nstatic void __merge_work_atoms(struct rb_root_cached *root, struct work_atoms *data)\n{\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tstruct work_atoms *this;\n\tconst char *comm = thread__comm_str(data->thread), *this_comm;\n\tbool leftmost = true;\n\n\twhile (*new) {\n\t\tint cmp;\n\n\t\tthis = container_of(*new, struct work_atoms, node);\n\t\tparent = *new;\n\n\t\tthis_comm = thread__comm_str(this->thread);\n\t\tcmp = strcmp(comm, this_comm);\n\t\tif (cmp > 0) {\n\t\t\tnew = &((*new)->rb_left);\n\t\t} else if (cmp < 0) {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t} else {\n\t\t\tthis->num_merged++;\n\t\t\tthis->total_runtime += data->total_runtime;\n\t\t\tthis->nb_atoms += data->nb_atoms;\n\t\t\tthis->total_lat += data->total_lat;\n\t\t\tlist_splice(&data->work_list, &this->work_list);\n\t\t\tif (this->max_lat < data->max_lat) {\n\t\t\t\tthis->max_lat = data->max_lat;\n\t\t\t\tthis->max_lat_start = data->max_lat_start;\n\t\t\t\tthis->max_lat_end = data->max_lat_end;\n\t\t\t}\n\t\t\tzfree(&data);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tdata->num_merged++;\n\trb_link_node(&data->node, parent, new);\n\trb_insert_color_cached(&data->node, root, leftmost);\n}\n\nstatic void perf_sched__merge_lat(struct perf_sched *sched)\n{\n\tstruct work_atoms *data;\n\tstruct rb_node *node;\n\n\tif (sched->skip_merge)\n\t\treturn;\n\n\twhile ((node = rb_first_cached(&sched->atom_root))) {\n\t\trb_erase_cached(node, &sched->atom_root);\n\t\tdata = rb_entry(node, struct work_atoms, node);\n\t\t__merge_work_atoms(&sched->merged_atom_root, data);\n\t}\n}\n\nstatic int perf_sched__lat(struct perf_sched *sched)\n{\n\tstruct rb_node *next;\n\n\tsetup_pager();\n\n\tif (perf_sched__read_events(sched))\n\t\treturn -1;\n\n\tperf_sched__merge_lat(sched);\n\tperf_sched__sort_lat(sched);\n\n\tprintf(\"\\n -------------------------------------------------------------------------------------------------------------------------------------------\\n\");\n\tprintf(\"  Task                  |   Runtime ms  | Switches | Avg delay ms    | Max delay ms    | Max delay start           | Max delay end          |\\n\");\n\tprintf(\" -------------------------------------------------------------------------------------------------------------------------------------------\\n\");\n\n\tnext = rb_first_cached(&sched->sorted_atom_root);\n\n\twhile (next) {\n\t\tstruct work_atoms *work_list;\n\n\t\twork_list = rb_entry(next, struct work_atoms, node);\n\t\toutput_lat_thread(sched, work_list);\n\t\tnext = rb_next(next);\n\t\tthread__zput(work_list->thread);\n\t}\n\n\tprintf(\" -----------------------------------------------------------------------------------------------------------------\\n\");\n\tprintf(\"  TOTAL:                |%11.3f ms |%9\" PRIu64 \" |\\n\",\n\t\t(double)sched->all_runtime / NSEC_PER_MSEC, sched->all_count);\n\n\tprintf(\" ---------------------------------------------------\\n\");\n\n\tprint_bad_events(sched);\n\tprintf(\"\\n\");\n\n\treturn 0;\n}\n\nstatic int setup_map_cpus(struct perf_sched *sched)\n{\n\tstruct perf_cpu_map *map;\n\n\tsched->max_cpu.cpu  = sysconf(_SC_NPROCESSORS_CONF);\n\n\tif (sched->map.comp) {\n\t\tsched->map.comp_cpus = zalloc(sched->max_cpu.cpu * sizeof(int));\n\t\tif (!sched->map.comp_cpus)\n\t\t\treturn -1;\n\t}\n\n\tif (!sched->map.cpus_str)\n\t\treturn 0;\n\n\tmap = perf_cpu_map__new(sched->map.cpus_str);\n\tif (!map) {\n\t\tpr_err(\"failed to get cpus map from %s\\n\", sched->map.cpus_str);\n\t\treturn -1;\n\t}\n\n\tsched->map.cpus = map;\n\treturn 0;\n}\n\nstatic int setup_color_pids(struct perf_sched *sched)\n{\n\tstruct perf_thread_map *map;\n\n\tif (!sched->map.color_pids_str)\n\t\treturn 0;\n\n\tmap = thread_map__new_by_tid_str(sched->map.color_pids_str);\n\tif (!map) {\n\t\tpr_err(\"failed to get thread map from %s\\n\", sched->map.color_pids_str);\n\t\treturn -1;\n\t}\n\n\tsched->map.color_pids = map;\n\treturn 0;\n}\n\nstatic int setup_color_cpus(struct perf_sched *sched)\n{\n\tstruct perf_cpu_map *map;\n\n\tif (!sched->map.color_cpus_str)\n\t\treturn 0;\n\n\tmap = perf_cpu_map__new(sched->map.color_cpus_str);\n\tif (!map) {\n\t\tpr_err(\"failed to get thread map from %s\\n\", sched->map.color_cpus_str);\n\t\treturn -1;\n\t}\n\n\tsched->map.color_cpus = map;\n\treturn 0;\n}\n\nstatic int perf_sched__map(struct perf_sched *sched)\n{\n\tif (setup_map_cpus(sched))\n\t\treturn -1;\n\n\tif (setup_color_pids(sched))\n\t\treturn -1;\n\n\tif (setup_color_cpus(sched))\n\t\treturn -1;\n\n\tsetup_pager();\n\tif (perf_sched__read_events(sched))\n\t\treturn -1;\n\tprint_bad_events(sched);\n\treturn 0;\n}\n\nstatic int perf_sched__replay(struct perf_sched *sched)\n{\n\tunsigned long i;\n\n\tcalibrate_run_measurement_overhead(sched);\n\tcalibrate_sleep_measurement_overhead(sched);\n\n\ttest_calibrations(sched);\n\n\tif (perf_sched__read_events(sched))\n\t\treturn -1;\n\n\tprintf(\"nr_run_events:        %ld\\n\", sched->nr_run_events);\n\tprintf(\"nr_sleep_events:      %ld\\n\", sched->nr_sleep_events);\n\tprintf(\"nr_wakeup_events:     %ld\\n\", sched->nr_wakeup_events);\n\n\tif (sched->targetless_wakeups)\n\t\tprintf(\"target-less wakeups:  %ld\\n\", sched->targetless_wakeups);\n\tif (sched->multitarget_wakeups)\n\t\tprintf(\"multi-target wakeups: %ld\\n\", sched->multitarget_wakeups);\n\tif (sched->nr_run_events_optimized)\n\t\tprintf(\"run atoms optimized: %ld\\n\",\n\t\t\tsched->nr_run_events_optimized);\n\n\tprint_task_traces(sched);\n\tadd_cross_task_wakeups(sched);\n\n\tsched->thread_funcs_exit = false;\n\tcreate_tasks(sched);\n\tprintf(\"------------------------------------------------------------\\n\");\n\tfor (i = 0; i < sched->replay_repeat; i++)\n\t\trun_one_test(sched);\n\n\tsched->thread_funcs_exit = true;\n\tdestroy_tasks(sched);\n\treturn 0;\n}\n\nstatic void setup_sorting(struct perf_sched *sched, const struct option *options,\n\t\t\t  const char * const usage_msg[])\n{\n\tchar *tmp, *tok, *str = strdup(sched->sort_order);\n\n\tfor (tok = strtok_r(str, \", \", &tmp);\n\t\t\ttok; tok = strtok_r(NULL, \", \", &tmp)) {\n\t\tif (sort_dimension__add(tok, &sched->sort_list) < 0) {\n\t\t\tusage_with_options_msg(usage_msg, options,\n\t\t\t\t\t\"Unknown --sort key: `%s'\", tok);\n\t\t}\n\t}\n\n\tfree(str);\n\n\tsort_dimension__add(\"pid\", &sched->cmp_pid);\n}\n\nstatic bool schedstat_events_exposed(void)\n{\n\t \n\treturn IS_ERR(trace_event__tp_format(\"sched\", \"sched_stat_wait\")) ?\n\t\tfalse : true;\n}\n\nstatic int __cmd_record(int argc, const char **argv)\n{\n\tunsigned int rec_argc, i, j;\n\tchar **rec_argv;\n\tconst char **rec_argv_copy;\n\tconst char * const record_args[] = {\n\t\t\"record\",\n\t\t\"-a\",\n\t\t\"-R\",\n\t\t\"-m\", \"1024\",\n\t\t\"-c\", \"1\",\n\t\t\"-e\", \"sched:sched_switch\",\n\t\t\"-e\", \"sched:sched_stat_runtime\",\n\t\t\"-e\", \"sched:sched_process_fork\",\n\t\t\"-e\", \"sched:sched_wakeup_new\",\n\t\t\"-e\", \"sched:sched_migrate_task\",\n\t};\n\n\t \n\tconst char * const schedstat_args[] = {\n\t\t\"-e\", \"sched:sched_stat_wait\",\n\t\t\"-e\", \"sched:sched_stat_sleep\",\n\t\t\"-e\", \"sched:sched_stat_iowait\",\n\t};\n\tunsigned int schedstat_argc = schedstat_events_exposed() ?\n\t\tARRAY_SIZE(schedstat_args) : 0;\n\n\tstruct tep_event *waking_event;\n\tint ret;\n\n\t \n\trec_argc = ARRAY_SIZE(record_args) + 2 + schedstat_argc + argc - 1;\n\trec_argv = calloc(rec_argc + 1, sizeof(char *));\n\tif (rec_argv == NULL)\n\t\treturn -ENOMEM;\n\trec_argv_copy = calloc(rec_argc + 1, sizeof(char *));\n\tif (rec_argv_copy == NULL) {\n\t\tfree(rec_argv);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(record_args); i++)\n\t\trec_argv[i] = strdup(record_args[i]);\n\n\trec_argv[i++] = strdup(\"-e\");\n\twaking_event = trace_event__tp_format(\"sched\", \"sched_waking\");\n\tif (!IS_ERR(waking_event))\n\t\trec_argv[i++] = strdup(\"sched:sched_waking\");\n\telse\n\t\trec_argv[i++] = strdup(\"sched:sched_wakeup\");\n\n\tfor (j = 0; j < schedstat_argc; j++)\n\t\trec_argv[i++] = strdup(schedstat_args[j]);\n\n\tfor (j = 1; j < (unsigned int)argc; j++, i++)\n\t\trec_argv[i] = strdup(argv[j]);\n\n\tBUG_ON(i != rec_argc);\n\n\tmemcpy(rec_argv_copy, rec_argv, sizeof(char *) * rec_argc);\n\tret = cmd_record(rec_argc, rec_argv_copy);\n\n\tfor (i = 0; i < rec_argc; i++)\n\t\tfree(rec_argv[i]);\n\tfree(rec_argv);\n\tfree(rec_argv_copy);\n\n\treturn ret;\n}\n\nint cmd_sched(int argc, const char **argv)\n{\n\tstatic const char default_sort_order[] = \"avg, max, switch, runtime\";\n\tstruct perf_sched sched = {\n\t\t.tool = {\n\t\t\t.sample\t\t = perf_sched__process_tracepoint_sample,\n\t\t\t.comm\t\t = perf_sched__process_comm,\n\t\t\t.namespaces\t = perf_event__process_namespaces,\n\t\t\t.lost\t\t = perf_event__process_lost,\n\t\t\t.fork\t\t = perf_sched__process_fork_event,\n\t\t\t.ordered_events = true,\n\t\t},\n\t\t.cmp_pid\t      = LIST_HEAD_INIT(sched.cmp_pid),\n\t\t.sort_list\t      = LIST_HEAD_INIT(sched.sort_list),\n\t\t.sort_order\t      = default_sort_order,\n\t\t.replay_repeat\t      = 10,\n\t\t.profile_cpu\t      = -1,\n\t\t.next_shortname1      = 'A',\n\t\t.next_shortname2      = '0',\n\t\t.skip_merge           = 0,\n\t\t.show_callchain\t      = 1,\n\t\t.max_stack            = 5,\n\t};\n\tconst struct option sched_options[] = {\n\tOPT_STRING('i', \"input\", &input_name, \"file\",\n\t\t    \"input file name\"),\n\tOPT_INCR('v', \"verbose\", &verbose,\n\t\t    \"be more verbose (show symbol address, etc)\"),\n\tOPT_BOOLEAN('D', \"dump-raw-trace\", &dump_trace,\n\t\t    \"dump raw trace in ASCII\"),\n\tOPT_BOOLEAN('f', \"force\", &sched.force, \"don't complain, do it\"),\n\tOPT_END()\n\t};\n\tconst struct option latency_options[] = {\n\tOPT_STRING('s', \"sort\", &sched.sort_order, \"key[,key2...]\",\n\t\t   \"sort by key(s): runtime, switch, avg, max\"),\n\tOPT_INTEGER('C', \"CPU\", &sched.profile_cpu,\n\t\t    \"CPU to profile on\"),\n\tOPT_BOOLEAN('p', \"pids\", &sched.skip_merge,\n\t\t    \"latency stats per pid instead of per comm\"),\n\tOPT_PARENT(sched_options)\n\t};\n\tconst struct option replay_options[] = {\n\tOPT_UINTEGER('r', \"repeat\", &sched.replay_repeat,\n\t\t     \"repeat the workload replay N times (-1: infinite)\"),\n\tOPT_PARENT(sched_options)\n\t};\n\tconst struct option map_options[] = {\n\tOPT_BOOLEAN(0, \"compact\", &sched.map.comp,\n\t\t    \"map output in compact mode\"),\n\tOPT_STRING(0, \"color-pids\", &sched.map.color_pids_str, \"pids\",\n\t\t   \"highlight given pids in map\"),\n\tOPT_STRING(0, \"color-cpus\", &sched.map.color_cpus_str, \"cpus\",\n                    \"highlight given CPUs in map\"),\n\tOPT_STRING(0, \"cpus\", &sched.map.cpus_str, \"cpus\",\n                    \"display given CPUs in map\"),\n\tOPT_PARENT(sched_options)\n\t};\n\tconst struct option timehist_options[] = {\n\tOPT_STRING('k', \"vmlinux\", &symbol_conf.vmlinux_name,\n\t\t   \"file\", \"vmlinux pathname\"),\n\tOPT_STRING(0, \"kallsyms\", &symbol_conf.kallsyms_name,\n\t\t   \"file\", \"kallsyms pathname\"),\n\tOPT_BOOLEAN('g', \"call-graph\", &sched.show_callchain,\n\t\t    \"Display call chains if present (default on)\"),\n\tOPT_UINTEGER(0, \"max-stack\", &sched.max_stack,\n\t\t   \"Maximum number of functions to display backtrace.\"),\n\tOPT_STRING(0, \"symfs\", &symbol_conf.symfs, \"directory\",\n\t\t    \"Look for files with symbols relative to this directory\"),\n\tOPT_BOOLEAN('s', \"summary\", &sched.summary_only,\n\t\t    \"Show only syscall summary with statistics\"),\n\tOPT_BOOLEAN('S', \"with-summary\", &sched.summary,\n\t\t    \"Show all syscalls and summary with statistics\"),\n\tOPT_BOOLEAN('w', \"wakeups\", &sched.show_wakeups, \"Show wakeup events\"),\n\tOPT_BOOLEAN('n', \"next\", &sched.show_next, \"Show next task\"),\n\tOPT_BOOLEAN('M', \"migrations\", &sched.show_migrations, \"Show migration events\"),\n\tOPT_BOOLEAN('V', \"cpu-visual\", &sched.show_cpu_visual, \"Add CPU visual\"),\n\tOPT_BOOLEAN('I', \"idle-hist\", &sched.idle_hist, \"Show idle events only\"),\n\tOPT_STRING(0, \"time\", &sched.time_str, \"str\",\n\t\t   \"Time span for analysis (start,stop)\"),\n\tOPT_BOOLEAN(0, \"state\", &sched.show_state, \"Show task state when sched-out\"),\n\tOPT_STRING('p', \"pid\", &symbol_conf.pid_list_str, \"pid[,pid...]\",\n\t\t   \"analyze events only for given process id(s)\"),\n\tOPT_STRING('t', \"tid\", &symbol_conf.tid_list_str, \"tid[,tid...]\",\n\t\t   \"analyze events only for given thread id(s)\"),\n\tOPT_STRING('C', \"cpu\", &cpu_list, \"cpu\", \"list of cpus to profile\"),\n\tOPT_PARENT(sched_options)\n\t};\n\n\tconst char * const latency_usage[] = {\n\t\t\"perf sched latency [<options>]\",\n\t\tNULL\n\t};\n\tconst char * const replay_usage[] = {\n\t\t\"perf sched replay [<options>]\",\n\t\tNULL\n\t};\n\tconst char * const map_usage[] = {\n\t\t\"perf sched map [<options>]\",\n\t\tNULL\n\t};\n\tconst char * const timehist_usage[] = {\n\t\t\"perf sched timehist [<options>]\",\n\t\tNULL\n\t};\n\tconst char *const sched_subcommands[] = { \"record\", \"latency\", \"map\",\n\t\t\t\t\t\t  \"replay\", \"script\",\n\t\t\t\t\t\t  \"timehist\", NULL };\n\tconst char *sched_usage[] = {\n\t\tNULL,\n\t\tNULL\n\t};\n\tstruct trace_sched_handler lat_ops  = {\n\t\t.wakeup_event\t    = latency_wakeup_event,\n\t\t.switch_event\t    = latency_switch_event,\n\t\t.runtime_event\t    = latency_runtime_event,\n\t\t.migrate_task_event = latency_migrate_task_event,\n\t};\n\tstruct trace_sched_handler map_ops  = {\n\t\t.switch_event\t    = map_switch_event,\n\t};\n\tstruct trace_sched_handler replay_ops  = {\n\t\t.wakeup_event\t    = replay_wakeup_event,\n\t\t.switch_event\t    = replay_switch_event,\n\t\t.fork_event\t    = replay_fork_event,\n\t};\n\tunsigned int i;\n\tint ret = 0;\n\n\tmutex_init(&sched.start_work_mutex);\n\tmutex_init(&sched.work_done_wait_mutex);\n\tsched.curr_thread = calloc(MAX_CPUS, sizeof(*sched.curr_thread));\n\tif (!sched.curr_thread) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsched.cpu_last_switched = calloc(MAX_CPUS, sizeof(*sched.cpu_last_switched));\n\tif (!sched.cpu_last_switched) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tsched.curr_pid = malloc(MAX_CPUS * sizeof(*sched.curr_pid));\n\tif (!sched.curr_pid) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tfor (i = 0; i < MAX_CPUS; i++)\n\t\tsched.curr_pid[i] = -1;\n\n\targc = parse_options_subcommand(argc, argv, sched_options, sched_subcommands,\n\t\t\t\t\tsched_usage, PARSE_OPT_STOP_AT_NON_OPTION);\n\tif (!argc)\n\t\tusage_with_options(sched_usage, sched_options);\n\n\t \n\tif (!strcmp(argv[0], \"script\")) {\n\t\tret = cmd_script(argc, argv);\n\t} else if (strlen(argv[0]) > 2 && strstarts(\"record\", argv[0])) {\n\t\tret = __cmd_record(argc, argv);\n\t} else if (strlen(argv[0]) > 2 && strstarts(\"latency\", argv[0])) {\n\t\tsched.tp_handler = &lat_ops;\n\t\tif (argc > 1) {\n\t\t\targc = parse_options(argc, argv, latency_options, latency_usage, 0);\n\t\t\tif (argc)\n\t\t\t\tusage_with_options(latency_usage, latency_options);\n\t\t}\n\t\tsetup_sorting(&sched, latency_options, latency_usage);\n\t\tret = perf_sched__lat(&sched);\n\t} else if (!strcmp(argv[0], \"map\")) {\n\t\tif (argc) {\n\t\t\targc = parse_options(argc, argv, map_options, map_usage, 0);\n\t\t\tif (argc)\n\t\t\t\tusage_with_options(map_usage, map_options);\n\t\t}\n\t\tsched.tp_handler = &map_ops;\n\t\tsetup_sorting(&sched, latency_options, latency_usage);\n\t\tret = perf_sched__map(&sched);\n\t} else if (strlen(argv[0]) > 2 && strstarts(\"replay\", argv[0])) {\n\t\tsched.tp_handler = &replay_ops;\n\t\tif (argc) {\n\t\t\targc = parse_options(argc, argv, replay_options, replay_usage, 0);\n\t\t\tif (argc)\n\t\t\t\tusage_with_options(replay_usage, replay_options);\n\t\t}\n\t\tret = perf_sched__replay(&sched);\n\t} else if (!strcmp(argv[0], \"timehist\")) {\n\t\tif (argc) {\n\t\t\targc = parse_options(argc, argv, timehist_options,\n\t\t\t\t\t     timehist_usage, 0);\n\t\t\tif (argc)\n\t\t\t\tusage_with_options(timehist_usage, timehist_options);\n\t\t}\n\t\tif ((sched.show_wakeups || sched.show_next) &&\n\t\t    sched.summary_only) {\n\t\t\tpr_err(\" Error: -s and -[n|w] are mutually exclusive.\\n\");\n\t\t\tparse_options_usage(timehist_usage, timehist_options, \"s\", true);\n\t\t\tif (sched.show_wakeups)\n\t\t\t\tparse_options_usage(NULL, timehist_options, \"w\", true);\n\t\t\tif (sched.show_next)\n\t\t\t\tparse_options_usage(NULL, timehist_options, \"n\", true);\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = symbol__validate_sym_arguments();\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tret = perf_sched__timehist(&sched);\n\t} else {\n\t\tusage_with_options(sched_usage, sched_options);\n\t}\n\nout:\n\tfree(sched.curr_pid);\n\tfree(sched.cpu_last_switched);\n\tfree(sched.curr_thread);\n\tmutex_destroy(&sched.start_work_mutex);\n\tmutex_destroy(&sched.work_done_wait_mutex);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}