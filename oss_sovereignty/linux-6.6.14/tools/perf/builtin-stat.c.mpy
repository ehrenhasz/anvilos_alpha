{
  "module_name": "builtin-stat.c",
  "hash_id": "8f22f67ac736ff42d0de3c63711f033bd0c7475f50a93eaf209f0aca72678a7f",
  "original_prompt": "Ingested from linux-6.6.14/tools/perf/builtin-stat.c",
  "human_readable_source": "\n \n\n#include \"builtin.h\"\n#include \"util/cgroup.h\"\n#include <subcmd/parse-options.h>\n#include \"util/parse-events.h\"\n#include \"util/pmus.h\"\n#include \"util/pmu.h\"\n#include \"util/event.h\"\n#include \"util/evlist.h\"\n#include \"util/evsel.h\"\n#include \"util/debug.h\"\n#include \"util/color.h\"\n#include \"util/stat.h\"\n#include \"util/header.h\"\n#include \"util/cpumap.h\"\n#include \"util/thread_map.h\"\n#include \"util/counts.h\"\n#include \"util/topdown.h\"\n#include \"util/session.h\"\n#include \"util/tool.h\"\n#include \"util/string2.h\"\n#include \"util/metricgroup.h\"\n#include \"util/synthetic-events.h\"\n#include \"util/target.h\"\n#include \"util/time-utils.h\"\n#include \"util/top.h\"\n#include \"util/affinity.h\"\n#include \"util/pfm.h\"\n#include \"util/bpf_counter.h\"\n#include \"util/iostat.h\"\n#include \"util/util.h\"\n#include \"asm/bug.h\"\n\n#include <linux/time64.h>\n#include <linux/zalloc.h>\n#include <api/fs/fs.h>\n#include <errno.h>\n#include <signal.h>\n#include <stdlib.h>\n#include <sys/prctl.h>\n#include <inttypes.h>\n#include <locale.h>\n#include <math.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <sys/time.h>\n#include <sys/resource.h>\n#include <linux/err.h>\n\n#include <linux/ctype.h>\n#include <perf/evlist.h>\n#include <internal/threadmap.h>\n\n#define DEFAULT_SEPARATOR\t\" \"\n#define FREEZE_ON_SMI_PATH\t\"devices/cpu/freeze_on_smi\"\n\nstatic void print_counters(struct timespec *ts, int argc, const char **argv);\n\nstatic struct evlist\t*evsel_list;\nstatic struct parse_events_option_args parse_events_option_args = {\n\t.evlistp = &evsel_list,\n};\n\nstatic bool all_counters_use_bpf = true;\n\nstatic struct target target = {\n\t.uid\t= UINT_MAX,\n};\n\n#define METRIC_ONLY_LEN 20\n\nstatic volatile sig_atomic_t\tchild_pid\t\t\t= -1;\nstatic int\t\t\tdetailed_run\t\t\t=  0;\nstatic bool\t\t\ttransaction_run;\nstatic bool\t\t\ttopdown_run\t\t\t= false;\nstatic bool\t\t\tsmi_cost\t\t\t= false;\nstatic bool\t\t\tsmi_reset\t\t\t= false;\nstatic int\t\t\tbig_num_opt\t\t\t=  -1;\nstatic const char\t\t*pre_cmd\t\t\t= NULL;\nstatic const char\t\t*post_cmd\t\t\t= NULL;\nstatic bool\t\t\tsync_run\t\t\t= false;\nstatic bool\t\t\tforever\t\t\t\t= false;\nstatic bool\t\t\tforce_metric_only\t\t= false;\nstatic struct timespec\t\tref_time;\nstatic bool\t\t\tappend_file;\nstatic bool\t\t\tinterval_count;\nstatic const char\t\t*output_name;\nstatic int\t\t\toutput_fd;\nstatic char\t\t\t*metrics;\n\nstruct perf_stat {\n\tbool\t\t\t record;\n\tstruct perf_data\t data;\n\tstruct perf_session\t*session;\n\tu64\t\t\t bytes_written;\n\tstruct perf_tool\t tool;\n\tbool\t\t\t maps_allocated;\n\tstruct perf_cpu_map\t*cpus;\n\tstruct perf_thread_map *threads;\n\tenum aggr_mode\t\t aggr_mode;\n\tu32\t\t\t aggr_level;\n};\n\nstatic struct perf_stat\t\tperf_stat;\n#define STAT_RECORD\t\tperf_stat.record\n\nstatic volatile sig_atomic_t done = 0;\n\nstatic struct perf_stat_config stat_config = {\n\t.aggr_mode\t\t= AGGR_GLOBAL,\n\t.aggr_level\t\t= MAX_CACHE_LVL + 1,\n\t.scale\t\t\t= true,\n\t.unit_width\t\t= 4,  \n\t.run_count\t\t= 1,\n\t.metric_only_len\t= METRIC_ONLY_LEN,\n\t.walltime_nsecs_stats\t= &walltime_nsecs_stats,\n\t.ru_stats\t\t= &ru_stats,\n\t.big_num\t\t= true,\n\t.ctl_fd\t\t\t= -1,\n\t.ctl_fd_ack\t\t= -1,\n\t.iostat_run\t\t= false,\n};\n\nstatic bool cpus_map_matched(struct evsel *a, struct evsel *b)\n{\n\tif (!a->core.cpus && !b->core.cpus)\n\t\treturn true;\n\n\tif (!a->core.cpus || !b->core.cpus)\n\t\treturn false;\n\n\tif (perf_cpu_map__nr(a->core.cpus) != perf_cpu_map__nr(b->core.cpus))\n\t\treturn false;\n\n\tfor (int i = 0; i < perf_cpu_map__nr(a->core.cpus); i++) {\n\t\tif (perf_cpu_map__cpu(a->core.cpus, i).cpu !=\n\t\t    perf_cpu_map__cpu(b->core.cpus, i).cpu)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void evlist__check_cpu_maps(struct evlist *evlist)\n{\n\tstruct evsel *evsel, *warned_leader = NULL;\n\n\tevlist__for_each_entry(evlist, evsel) {\n\t\tstruct evsel *leader = evsel__leader(evsel);\n\n\t\t \n\t\tif (leader == evsel)\n\t\t\tcontinue;\n\t\tif (cpus_map_matched(leader, evsel))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (warned_leader != leader) {\n\t\t\tchar buf[200];\n\n\t\t\tpr_warning(\"WARNING: grouped events cpus do not match.\\n\"\n\t\t\t\t\"Events with CPUs not matching the leader will \"\n\t\t\t\t\"be removed from the group.\\n\");\n\t\t\tevsel__group_desc(leader, buf, sizeof(buf));\n\t\t\tpr_warning(\"  %s\\n\", buf);\n\t\t\twarned_leader = leader;\n\t\t}\n\t\tif (verbose > 0) {\n\t\t\tchar buf[200];\n\n\t\t\tcpu_map__snprint(leader->core.cpus, buf, sizeof(buf));\n\t\t\tpr_warning(\"     %s: %s\\n\", leader->name, buf);\n\t\t\tcpu_map__snprint(evsel->core.cpus, buf, sizeof(buf));\n\t\t\tpr_warning(\"     %s: %s\\n\", evsel->name, buf);\n\t\t}\n\n\t\tevsel__remove_from_group(evsel, leader);\n\t}\n}\n\nstatic inline void diff_timespec(struct timespec *r, struct timespec *a,\n\t\t\t\t struct timespec *b)\n{\n\tr->tv_sec = a->tv_sec - b->tv_sec;\n\tif (a->tv_nsec < b->tv_nsec) {\n\t\tr->tv_nsec = a->tv_nsec + NSEC_PER_SEC - b->tv_nsec;\n\t\tr->tv_sec--;\n\t} else {\n\t\tr->tv_nsec = a->tv_nsec - b->tv_nsec ;\n\t}\n}\n\nstatic void perf_stat__reset_stats(void)\n{\n\tevlist__reset_stats(evsel_list);\n\tperf_stat__reset_shadow_stats();\n}\n\nstatic int process_synthesized_event(struct perf_tool *tool __maybe_unused,\n\t\t\t\t     union perf_event *event,\n\t\t\t\t     struct perf_sample *sample __maybe_unused,\n\t\t\t\t     struct machine *machine __maybe_unused)\n{\n\tif (perf_data__write(&perf_stat.data, event, event->header.size) < 0) {\n\t\tpr_err(\"failed to write perf data, error: %m\\n\");\n\t\treturn -1;\n\t}\n\n\tperf_stat.bytes_written += event->header.size;\n\treturn 0;\n}\n\nstatic int write_stat_round_event(u64 tm, u64 type)\n{\n\treturn perf_event__synthesize_stat_round(NULL, tm, type,\n\t\t\t\t\t\t process_synthesized_event,\n\t\t\t\t\t\t NULL);\n}\n\n#define WRITE_STAT_ROUND_EVENT(time, interval) \\\n\twrite_stat_round_event(time, PERF_STAT_ROUND_TYPE__ ## interval)\n\n#define SID(e, x, y) xyarray__entry(e->core.sample_id, x, y)\n\nstatic int evsel__write_stat_event(struct evsel *counter, int cpu_map_idx, u32 thread,\n\t\t\t\t   struct perf_counts_values *count)\n{\n\tstruct perf_sample_id *sid = SID(counter, cpu_map_idx, thread);\n\tstruct perf_cpu cpu = perf_cpu_map__cpu(evsel__cpus(counter), cpu_map_idx);\n\n\treturn perf_event__synthesize_stat(NULL, cpu, thread, sid->id, count,\n\t\t\t\t\t   process_synthesized_event, NULL);\n}\n\nstatic int read_single_counter(struct evsel *counter, int cpu_map_idx,\n\t\t\t       int thread, struct timespec *rs)\n{\n\tswitch(counter->tool_event) {\n\t\tcase PERF_TOOL_DURATION_TIME: {\n\t\t\tu64 val = rs->tv_nsec + rs->tv_sec*1000000000ULL;\n\t\t\tstruct perf_counts_values *count =\n\t\t\t\tperf_counts(counter->counts, cpu_map_idx, thread);\n\t\t\tcount->ena = count->run = val;\n\t\t\tcount->val = val;\n\t\t\treturn 0;\n\t\t}\n\t\tcase PERF_TOOL_USER_TIME:\n\t\tcase PERF_TOOL_SYSTEM_TIME: {\n\t\t\tu64 val;\n\t\t\tstruct perf_counts_values *count =\n\t\t\t\tperf_counts(counter->counts, cpu_map_idx, thread);\n\t\t\tif (counter->tool_event == PERF_TOOL_USER_TIME)\n\t\t\t\tval = ru_stats.ru_utime_usec_stat.mean;\n\t\t\telse\n\t\t\t\tval = ru_stats.ru_stime_usec_stat.mean;\n\t\t\tcount->ena = count->run = val;\n\t\t\tcount->val = val;\n\t\t\treturn 0;\n\t\t}\n\t\tdefault:\n\t\tcase PERF_TOOL_NONE:\n\t\t\treturn evsel__read_counter(counter, cpu_map_idx, thread);\n\t\tcase PERF_TOOL_MAX:\n\t\t\t \n\t\t\treturn 0;\n\t}\n}\n\n \nstatic int read_counter_cpu(struct evsel *counter, struct timespec *rs, int cpu_map_idx)\n{\n\tint nthreads = perf_thread_map__nr(evsel_list->core.threads);\n\tint thread;\n\n\tif (!counter->supported)\n\t\treturn -ENOENT;\n\n\tfor (thread = 0; thread < nthreads; thread++) {\n\t\tstruct perf_counts_values *count;\n\n\t\tcount = perf_counts(counter->counts, cpu_map_idx, thread);\n\n\t\t \n\t\tif (!perf_counts__is_loaded(counter->counts, cpu_map_idx, thread) &&\n\t\t    read_single_counter(counter, cpu_map_idx, thread, rs)) {\n\t\t\tcounter->counts->scaled = -1;\n\t\t\tperf_counts(counter->counts, cpu_map_idx, thread)->ena = 0;\n\t\t\tperf_counts(counter->counts, cpu_map_idx, thread)->run = 0;\n\t\t\treturn -1;\n\t\t}\n\n\t\tperf_counts__set_loaded(counter->counts, cpu_map_idx, thread, false);\n\n\t\tif (STAT_RECORD) {\n\t\t\tif (evsel__write_stat_event(counter, cpu_map_idx, thread, count)) {\n\t\t\t\tpr_err(\"failed to write stat event\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\n\t\tif (verbose > 1) {\n\t\t\tfprintf(stat_config.output,\n\t\t\t\t\"%s: %d: %\" PRIu64 \" %\" PRIu64 \" %\" PRIu64 \"\\n\",\n\t\t\t\t\tevsel__name(counter),\n\t\t\t\t\tperf_cpu_map__cpu(evsel__cpus(counter),\n\t\t\t\t\t\t\t  cpu_map_idx).cpu,\n\t\t\t\t\tcount->val, count->ena, count->run);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int read_affinity_counters(struct timespec *rs)\n{\n\tstruct evlist_cpu_iterator evlist_cpu_itr;\n\tstruct affinity saved_affinity, *affinity;\n\n\tif (all_counters_use_bpf)\n\t\treturn 0;\n\n\tif (!target__has_cpu(&target) || target__has_per_thread(&target))\n\t\taffinity = NULL;\n\telse if (affinity__setup(&saved_affinity) < 0)\n\t\treturn -1;\n\telse\n\t\taffinity = &saved_affinity;\n\n\tevlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {\n\t\tstruct evsel *counter = evlist_cpu_itr.evsel;\n\n\t\tif (evsel__is_bpf(counter))\n\t\t\tcontinue;\n\n\t\tif (!counter->err) {\n\t\t\tcounter->err = read_counter_cpu(counter, rs,\n\t\t\t\t\t\t\tevlist_cpu_itr.cpu_map_idx);\n\t\t}\n\t}\n\tif (affinity)\n\t\taffinity__cleanup(&saved_affinity);\n\n\treturn 0;\n}\n\nstatic int read_bpf_map_counters(void)\n{\n\tstruct evsel *counter;\n\tint err;\n\n\tevlist__for_each_entry(evsel_list, counter) {\n\t\tif (!evsel__is_bpf(counter))\n\t\t\tcontinue;\n\n\t\terr = bpf_counter__read(counter);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn 0;\n}\n\nstatic int read_counters(struct timespec *rs)\n{\n\tif (!stat_config.stop_read_counter) {\n\t\tif (read_bpf_map_counters() ||\n\t\t    read_affinity_counters(rs))\n\t\t\treturn -1;\n\t}\n\treturn 0;\n}\n\nstatic void process_counters(void)\n{\n\tstruct evsel *counter;\n\n\tevlist__for_each_entry(evsel_list, counter) {\n\t\tif (counter->err)\n\t\t\tpr_debug(\"failed to read counter %s\\n\", counter->name);\n\t\tif (counter->err == 0 && perf_stat_process_counter(&stat_config, counter))\n\t\t\tpr_warning(\"failed to process counter %s\\n\", counter->name);\n\t\tcounter->err = 0;\n\t}\n\n\tperf_stat_merge_counters(&stat_config, evsel_list);\n\tperf_stat_process_percore(&stat_config, evsel_list);\n}\n\nstatic void process_interval(void)\n{\n\tstruct timespec ts, rs;\n\n\tclock_gettime(CLOCK_MONOTONIC, &ts);\n\tdiff_timespec(&rs, &ts, &ref_time);\n\n\tevlist__reset_aggr_stats(evsel_list);\n\n\tif (read_counters(&rs) == 0)\n\t\tprocess_counters();\n\n\tif (STAT_RECORD) {\n\t\tif (WRITE_STAT_ROUND_EVENT(rs.tv_sec * NSEC_PER_SEC + rs.tv_nsec, INTERVAL))\n\t\t\tpr_err(\"failed to write stat round event\\n\");\n\t}\n\n\tinit_stats(&walltime_nsecs_stats);\n\tupdate_stats(&walltime_nsecs_stats, stat_config.interval * 1000000ULL);\n\tprint_counters(&rs, 0, NULL);\n}\n\nstatic bool handle_interval(unsigned int interval, int *times)\n{\n\tif (interval) {\n\t\tprocess_interval();\n\t\tif (interval_count && !(--(*times)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int enable_counters(void)\n{\n\tstruct evsel *evsel;\n\tint err;\n\n\tevlist__for_each_entry(evsel_list, evsel) {\n\t\tif (!evsel__is_bpf(evsel))\n\t\t\tcontinue;\n\n\t\terr = bpf_counter__enable(evsel);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (!target__enable_on_exec(&target)) {\n\t\tif (!all_counters_use_bpf)\n\t\t\tevlist__enable(evsel_list);\n\t}\n\treturn 0;\n}\n\nstatic void disable_counters(void)\n{\n\tstruct evsel *counter;\n\n\t \n\tif (!target__none(&target)) {\n\t\tevlist__for_each_entry(evsel_list, counter)\n\t\t\tbpf_counter__disable(counter);\n\t\tif (!all_counters_use_bpf)\n\t\t\tevlist__disable(evsel_list);\n\t}\n}\n\nstatic volatile sig_atomic_t workload_exec_errno;\n\n \nstatic void workload_exec_failed_signal(int signo __maybe_unused, siginfo_t *info,\n\t\t\t\t\tvoid *ucontext __maybe_unused)\n{\n\tworkload_exec_errno = info->si_value.sival_int;\n}\n\nstatic bool evsel__should_store_id(struct evsel *counter)\n{\n\treturn STAT_RECORD || counter->core.attr.read_format & PERF_FORMAT_ID;\n}\n\nstatic bool is_target_alive(struct target *_target,\n\t\t\t    struct perf_thread_map *threads)\n{\n\tstruct stat st;\n\tint i;\n\n\tif (!target__has_task(_target))\n\t\treturn true;\n\n\tfor (i = 0; i < threads->nr; i++) {\n\t\tchar path[PATH_MAX];\n\n\t\tscnprintf(path, PATH_MAX, \"%s/%d\", procfs__mountpoint(),\n\t\t\t  threads->map[i].pid);\n\n\t\tif (!stat(path, &st))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void process_evlist(struct evlist *evlist, unsigned int interval)\n{\n\tenum evlist_ctl_cmd cmd = EVLIST_CTL_CMD_UNSUPPORTED;\n\n\tif (evlist__ctlfd_process(evlist, &cmd) > 0) {\n\t\tswitch (cmd) {\n\t\tcase EVLIST_CTL_CMD_ENABLE:\n\t\t\tfallthrough;\n\t\tcase EVLIST_CTL_CMD_DISABLE:\n\t\t\tif (interval)\n\t\t\t\tprocess_interval();\n\t\t\tbreak;\n\t\tcase EVLIST_CTL_CMD_SNAPSHOT:\n\t\tcase EVLIST_CTL_CMD_ACK:\n\t\tcase EVLIST_CTL_CMD_UNSUPPORTED:\n\t\tcase EVLIST_CTL_CMD_EVLIST:\n\t\tcase EVLIST_CTL_CMD_STOP:\n\t\tcase EVLIST_CTL_CMD_PING:\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void compute_tts(struct timespec *time_start, struct timespec *time_stop,\n\t\t\tint *time_to_sleep)\n{\n\tint tts = *time_to_sleep;\n\tstruct timespec time_diff;\n\n\tdiff_timespec(&time_diff, time_stop, time_start);\n\n\ttts -= time_diff.tv_sec * MSEC_PER_SEC +\n\t       time_diff.tv_nsec / NSEC_PER_MSEC;\n\n\tif (tts < 0)\n\t\ttts = 0;\n\n\t*time_to_sleep = tts;\n}\n\nstatic int dispatch_events(bool forks, int timeout, int interval, int *times)\n{\n\tint child_exited = 0, status = 0;\n\tint time_to_sleep, sleep_time;\n\tstruct timespec time_start, time_stop;\n\n\tif (interval)\n\t\tsleep_time = interval;\n\telse if (timeout)\n\t\tsleep_time = timeout;\n\telse\n\t\tsleep_time = 1000;\n\n\ttime_to_sleep = sleep_time;\n\n\twhile (!done) {\n\t\tif (forks)\n\t\t\tchild_exited = waitpid(child_pid, &status, WNOHANG);\n\t\telse\n\t\t\tchild_exited = !is_target_alive(&target, evsel_list->core.threads) ? 1 : 0;\n\n\t\tif (child_exited)\n\t\t\tbreak;\n\n\t\tclock_gettime(CLOCK_MONOTONIC, &time_start);\n\t\tif (!(evlist__poll(evsel_list, time_to_sleep) > 0)) {  \n\t\t\tif (timeout || handle_interval(interval, times))\n\t\t\t\tbreak;\n\t\t\ttime_to_sleep = sleep_time;\n\t\t} else {  \n\t\t\tprocess_evlist(evsel_list, interval);\n\t\t\tclock_gettime(CLOCK_MONOTONIC, &time_stop);\n\t\t\tcompute_tts(&time_start, &time_stop, &time_to_sleep);\n\t\t}\n\t}\n\n\treturn status;\n}\n\nenum counter_recovery {\n\tCOUNTER_SKIP,\n\tCOUNTER_RETRY,\n\tCOUNTER_FATAL,\n};\n\nstatic enum counter_recovery stat_handle_error(struct evsel *counter)\n{\n\tchar msg[BUFSIZ];\n\t \n\tif (errno == EINVAL || errno == ENOSYS ||\n\t    errno == ENOENT || errno == EOPNOTSUPP ||\n\t    errno == ENXIO) {\n\t\tif (verbose > 0)\n\t\t\tui__warning(\"%s event is not supported by the kernel.\\n\",\n\t\t\t\t    evsel__name(counter));\n\t\tcounter->supported = false;\n\t\t \n\t\tcounter->errored = true;\n\n\t\tif ((evsel__leader(counter) != counter) ||\n\t\t    !(counter->core.leader->nr_members > 1))\n\t\t\treturn COUNTER_SKIP;\n\t} else if (evsel__fallback(counter, errno, msg, sizeof(msg))) {\n\t\tif (verbose > 0)\n\t\t\tui__warning(\"%s\\n\", msg);\n\t\treturn COUNTER_RETRY;\n\t} else if (target__has_per_thread(&target) &&\n\t\t   evsel_list->core.threads &&\n\t\t   evsel_list->core.threads->err_thread != -1) {\n\t\t \n\t\tif (!thread_map__remove(evsel_list->core.threads,\n\t\t\t\t\tevsel_list->core.threads->err_thread)) {\n\t\t\tevsel_list->core.threads->err_thread = -1;\n\t\t\treturn COUNTER_RETRY;\n\t\t}\n\t} else if (counter->skippable) {\n\t\tif (verbose > 0)\n\t\t\tui__warning(\"skipping event %s that kernel failed to open .\\n\",\n\t\t\t\t    evsel__name(counter));\n\t\tcounter->supported = false;\n\t\tcounter->errored = true;\n\t\treturn COUNTER_SKIP;\n\t}\n\n\tevsel__open_strerror(counter, &target, errno, msg, sizeof(msg));\n\tui__error(\"%s\\n\", msg);\n\n\tif (child_pid != -1)\n\t\tkill(child_pid, SIGTERM);\n\treturn COUNTER_FATAL;\n}\n\nstatic int __run_perf_stat(int argc, const char **argv, int run_idx)\n{\n\tint interval = stat_config.interval;\n\tint times = stat_config.times;\n\tint timeout = stat_config.timeout;\n\tchar msg[BUFSIZ];\n\tunsigned long long t0, t1;\n\tstruct evsel *counter;\n\tsize_t l;\n\tint status = 0;\n\tconst bool forks = (argc > 0);\n\tbool is_pipe = STAT_RECORD ? perf_stat.data.is_pipe : false;\n\tstruct evlist_cpu_iterator evlist_cpu_itr;\n\tstruct affinity saved_affinity, *affinity = NULL;\n\tint err;\n\tbool second_pass = false;\n\n\tif (forks) {\n\t\tif (evlist__prepare_workload(evsel_list, &target, argv, is_pipe, workload_exec_failed_signal) < 0) {\n\t\t\tperror(\"failed to prepare workload\");\n\t\t\treturn -1;\n\t\t}\n\t\tchild_pid = evsel_list->workload.pid;\n\t}\n\n\tif (!cpu_map__is_dummy(evsel_list->core.user_requested_cpus)) {\n\t\tif (affinity__setup(&saved_affinity) < 0)\n\t\t\treturn -1;\n\t\taffinity = &saved_affinity;\n\t}\n\n\tevlist__for_each_entry(evsel_list, counter) {\n\t\tcounter->reset_group = false;\n\t\tif (bpf_counter__load(counter, &target))\n\t\t\treturn -1;\n\t\tif (!(evsel__is_bperf(counter)))\n\t\t\tall_counters_use_bpf = false;\n\t}\n\n\tevlist__reset_aggr_stats(evsel_list);\n\n\tevlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {\n\t\tcounter = evlist_cpu_itr.evsel;\n\n\t\t \n\t\tif (target.use_bpf)\n\t\t\tbreak;\n\n\t\tif (counter->reset_group || counter->errored)\n\t\t\tcontinue;\n\t\tif (evsel__is_bperf(counter))\n\t\t\tcontinue;\ntry_again:\n\t\tif (create_perf_stat_counter(counter, &stat_config, &target,\n\t\t\t\t\t     evlist_cpu_itr.cpu_map_idx) < 0) {\n\n\t\t\t \n\t\t\tif ((errno == EINVAL || errno == EBADF) &&\n\t\t\t\tevsel__leader(counter) != counter &&\n\t\t\t\tcounter->weak_group) {\n\t\t\t\tevlist__reset_weak_group(evsel_list, counter, false);\n\t\t\t\tassert(counter->reset_group);\n\t\t\t\tsecond_pass = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tswitch (stat_handle_error(counter)) {\n\t\t\tcase COUNTER_FATAL:\n\t\t\t\treturn -1;\n\t\t\tcase COUNTER_RETRY:\n\t\t\t\tgoto try_again;\n\t\t\tcase COUNTER_SKIP:\n\t\t\t\tcontinue;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t}\n\t\tcounter->supported = true;\n\t}\n\n\tif (second_pass) {\n\t\t \n\n\t\t \n\t\tevlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {\n\t\t\tcounter = evlist_cpu_itr.evsel;\n\n\t\t\tif (!counter->reset_group && !counter->errored)\n\t\t\t\tcontinue;\n\n\t\t\tperf_evsel__close_cpu(&counter->core, evlist_cpu_itr.cpu_map_idx);\n\t\t}\n\t\t \n\t\tevlist__for_each_cpu(evlist_cpu_itr, evsel_list, affinity) {\n\t\t\tcounter = evlist_cpu_itr.evsel;\n\n\t\t\tif (!counter->reset_group)\n\t\t\t\tcontinue;\ntry_again_reset:\n\t\t\tpr_debug2(\"reopening weak %s\\n\", evsel__name(counter));\n\t\t\tif (create_perf_stat_counter(counter, &stat_config, &target,\n\t\t\t\t\t\t     evlist_cpu_itr.cpu_map_idx) < 0) {\n\n\t\t\t\tswitch (stat_handle_error(counter)) {\n\t\t\t\tcase COUNTER_FATAL:\n\t\t\t\t\treturn -1;\n\t\t\t\tcase COUNTER_RETRY:\n\t\t\t\t\tgoto try_again_reset;\n\t\t\t\tcase COUNTER_SKIP:\n\t\t\t\t\tcontinue;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcounter->supported = true;\n\t\t}\n\t}\n\taffinity__cleanup(affinity);\n\n\tevlist__for_each_entry(evsel_list, counter) {\n\t\tif (!counter->supported) {\n\t\t\tperf_evsel__free_fd(&counter->core);\n\t\t\tcontinue;\n\t\t}\n\n\t\tl = strlen(counter->unit);\n\t\tif (l > stat_config.unit_width)\n\t\t\tstat_config.unit_width = l;\n\n\t\tif (evsel__should_store_id(counter) &&\n\t\t    evsel__store_ids(counter, evsel_list))\n\t\t\treturn -1;\n\t}\n\n\tif (evlist__apply_filters(evsel_list, &counter)) {\n\t\tpr_err(\"failed to set filter \\\"%s\\\" on event %s with %d (%s)\\n\",\n\t\t\tcounter->filter, evsel__name(counter), errno,\n\t\t\tstr_error_r(errno, msg, sizeof(msg)));\n\t\treturn -1;\n\t}\n\n\tif (STAT_RECORD) {\n\t\tint fd = perf_data__fd(&perf_stat.data);\n\n\t\tif (is_pipe) {\n\t\t\terr = perf_header__write_pipe(perf_data__fd(&perf_stat.data));\n\t\t} else {\n\t\t\terr = perf_session__write_header(perf_stat.session, evsel_list,\n\t\t\t\t\t\t\t fd, false);\n\t\t}\n\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = perf_event__synthesize_stat_events(&stat_config, NULL, evsel_list,\n\t\t\t\t\t\t\t process_synthesized_event, is_pipe);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tif (target.initial_delay) {\n\t\tpr_info(EVLIST_DISABLED_MSG);\n\t} else {\n\t\terr = enable_counters();\n\t\tif (err)\n\t\t\treturn -1;\n\t}\n\n\t \n\tif (forks)\n\t\tevlist__start_workload(evsel_list);\n\n\tif (target.initial_delay > 0) {\n\t\tusleep(target.initial_delay * USEC_PER_MSEC);\n\t\terr = enable_counters();\n\t\tif (err)\n\t\t\treturn -1;\n\n\t\tpr_info(EVLIST_ENABLED_MSG);\n\t}\n\n\tt0 = rdclock();\n\tclock_gettime(CLOCK_MONOTONIC, &ref_time);\n\n\tif (forks) {\n\t\tif (interval || timeout || evlist__ctlfd_initialized(evsel_list))\n\t\t\tstatus = dispatch_events(forks, timeout, interval, &times);\n\t\tif (child_pid != -1) {\n\t\t\tif (timeout)\n\t\t\t\tkill(child_pid, SIGTERM);\n\t\t\twait4(child_pid, &status, 0, &stat_config.ru_data);\n\t\t}\n\n\t\tif (workload_exec_errno) {\n\t\t\tconst char *emsg = str_error_r(workload_exec_errno, msg, sizeof(msg));\n\t\t\tpr_err(\"Workload failed: %s\\n\", emsg);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (WIFSIGNALED(status))\n\t\t\tpsignal(WTERMSIG(status), argv[0]);\n\t} else {\n\t\tstatus = dispatch_events(forks, timeout, interval, &times);\n\t}\n\n\tdisable_counters();\n\n\tt1 = rdclock();\n\n\tif (stat_config.walltime_run_table)\n\t\tstat_config.walltime_run[run_idx] = t1 - t0;\n\n\tif (interval && stat_config.summary) {\n\t\tstat_config.interval = 0;\n\t\tstat_config.stop_read_counter = true;\n\t\tinit_stats(&walltime_nsecs_stats);\n\t\tupdate_stats(&walltime_nsecs_stats, t1 - t0);\n\n\t\tevlist__copy_prev_raw_counts(evsel_list);\n\t\tevlist__reset_prev_raw_counts(evsel_list);\n\t\tevlist__reset_aggr_stats(evsel_list);\n\t} else {\n\t\tupdate_stats(&walltime_nsecs_stats, t1 - t0);\n\t\tupdate_rusage_stats(&ru_stats, &stat_config.ru_data);\n\t}\n\n\t \n\tif (read_counters(&(struct timespec) { .tv_nsec = t1-t0 }) == 0)\n\t\tprocess_counters();\n\n\t \n\tif (!STAT_RECORD)\n\t\tevlist__close(evsel_list);\n\n\treturn WEXITSTATUS(status);\n}\n\nstatic int run_perf_stat(int argc, const char **argv, int run_idx)\n{\n\tint ret;\n\n\tif (pre_cmd) {\n\t\tret = system(pre_cmd);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (sync_run)\n\t\tsync();\n\n\tret = __run_perf_stat(argc, argv, run_idx);\n\tif (ret)\n\t\treturn ret;\n\n\tif (post_cmd) {\n\t\tret = system(post_cmd);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\nstatic void print_counters(struct timespec *ts, int argc, const char **argv)\n{\n\t \n\tif (STAT_RECORD && perf_stat.data.is_pipe)\n\t\treturn;\n\tif (quiet)\n\t\treturn;\n\n\tevlist__print_counters(evsel_list, &stat_config, &target, ts, argc, argv);\n}\n\nstatic volatile sig_atomic_t signr = -1;\n\nstatic void skip_signal(int signo)\n{\n\tif ((child_pid == -1) || stat_config.interval)\n\t\tdone = 1;\n\n\tsignr = signo;\n\t \n\tchild_pid = -1;\n}\n\nstatic void sig_atexit(void)\n{\n\tsigset_t set, oset;\n\n\t \n\tsigemptyset(&set);\n\tsigaddset(&set, SIGCHLD);\n\tsigprocmask(SIG_BLOCK, &set, &oset);\n\n\tif (child_pid != -1)\n\t\tkill(child_pid, SIGTERM);\n\n\tsigprocmask(SIG_SETMASK, &oset, NULL);\n\n\tif (signr == -1)\n\t\treturn;\n\n\tsignal(signr, SIG_DFL);\n\tkill(getpid(), signr);\n}\n\nvoid perf_stat__set_big_num(int set)\n{\n\tstat_config.big_num = (set != 0);\n}\n\nvoid perf_stat__set_no_csv_summary(int set)\n{\n\tstat_config.no_csv_summary = (set != 0);\n}\n\nstatic int stat__set_big_num(const struct option *opt __maybe_unused,\n\t\t\t     const char *s __maybe_unused, int unset)\n{\n\tbig_num_opt = unset ? 0 : 1;\n\tperf_stat__set_big_num(!unset);\n\treturn 0;\n}\n\nstatic int enable_metric_only(const struct option *opt __maybe_unused,\n\t\t\t      const char *s __maybe_unused, int unset)\n{\n\tforce_metric_only = true;\n\tstat_config.metric_only = !unset;\n\treturn 0;\n}\n\nstatic int append_metric_groups(const struct option *opt __maybe_unused,\n\t\t\t       const char *str,\n\t\t\t       int unset __maybe_unused)\n{\n\tif (metrics) {\n\t\tchar *tmp;\n\n\t\tif (asprintf(&tmp, \"%s,%s\", metrics, str) < 0)\n\t\t\treturn -ENOMEM;\n\t\tfree(metrics);\n\t\tmetrics = tmp;\n\t} else {\n\t\tmetrics = strdup(str);\n\t\tif (!metrics)\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int parse_control_option(const struct option *opt,\n\t\t\t\tconst char *str,\n\t\t\t\tint unset __maybe_unused)\n{\n\tstruct perf_stat_config *config = opt->value;\n\n\treturn evlist__parse_control(str, &config->ctl_fd, &config->ctl_fd_ack, &config->ctl_fd_close);\n}\n\nstatic int parse_stat_cgroups(const struct option *opt,\n\t\t\t      const char *str, int unset)\n{\n\tif (stat_config.cgroup_list) {\n\t\tpr_err(\"--cgroup and --for-each-cgroup cannot be used together\\n\");\n\t\treturn -1;\n\t}\n\n\treturn parse_cgroups(opt, str, unset);\n}\n\nstatic int parse_cputype(const struct option *opt,\n\t\t\t     const char *str,\n\t\t\t     int unset __maybe_unused)\n{\n\tconst struct perf_pmu *pmu;\n\tstruct evlist *evlist = *(struct evlist **)opt->value;\n\n\tif (!list_empty(&evlist->core.entries)) {\n\t\tfprintf(stderr, \"Must define cputype before events/metrics\\n\");\n\t\treturn -1;\n\t}\n\n\tpmu = perf_pmus__pmu_for_pmu_filter(str);\n\tif (!pmu) {\n\t\tfprintf(stderr, \"--cputype %s is not supported!\\n\", str);\n\t\treturn -1;\n\t}\n\tparse_events_option_args.pmu_filter = pmu->name;\n\n\treturn 0;\n}\n\nstatic int parse_cache_level(const struct option *opt,\n\t\t\t     const char *str,\n\t\t\t     int unset __maybe_unused)\n{\n\tint level;\n\tu32 *aggr_mode = (u32 *)opt->value;\n\tu32 *aggr_level = (u32 *)opt->data;\n\n\t \n\tif (str == NULL) {\n\t\tlevel = MAX_CACHE_LVL + 1;\n\t\tgoto out;\n\t}\n\n\t \n\tif (strlen(str) != 2 || (str[0] != 'l' && str[0] != 'L')) {\n\t\tpr_err(\"Cache level must be of form L[1-%d], or l[1-%d]\\n\",\n\t\t       MAX_CACHE_LVL,\n\t\t       MAX_CACHE_LVL);\n\t\treturn -EINVAL;\n\t}\n\n\tlevel = atoi(&str[1]);\n\tif (level < 1) {\n\t\tpr_err(\"Cache level must be of form L[1-%d], or l[1-%d]\\n\",\n\t\t       MAX_CACHE_LVL,\n\t\t       MAX_CACHE_LVL);\n\t\treturn -EINVAL;\n\t}\n\n\tif (level > MAX_CACHE_LVL) {\n\t\tpr_err(\"perf only supports max cache level of %d.\\n\"\n\t\t       \"Consider increasing MAX_CACHE_LVL\\n\", MAX_CACHE_LVL);\n\t\treturn -EINVAL;\n\t}\nout:\n\t*aggr_mode = AGGR_CACHE;\n\t*aggr_level = level;\n\treturn 0;\n}\n\nstatic struct option stat_options[] = {\n\tOPT_BOOLEAN('T', \"transaction\", &transaction_run,\n\t\t    \"hardware transaction statistics\"),\n\tOPT_CALLBACK('e', \"event\", &parse_events_option_args, \"event\",\n\t\t     \"event selector. use 'perf list' to list available events\",\n\t\t     parse_events_option),\n\tOPT_CALLBACK(0, \"filter\", &evsel_list, \"filter\",\n\t\t     \"event filter\", parse_filter),\n\tOPT_BOOLEAN('i', \"no-inherit\", &stat_config.no_inherit,\n\t\t    \"child tasks do not inherit counters\"),\n\tOPT_STRING('p', \"pid\", &target.pid, \"pid\",\n\t\t   \"stat events on existing process id\"),\n\tOPT_STRING('t', \"tid\", &target.tid, \"tid\",\n\t\t   \"stat events on existing thread id\"),\n#ifdef HAVE_BPF_SKEL\n\tOPT_STRING('b', \"bpf-prog\", &target.bpf_str, \"bpf-prog-id\",\n\t\t   \"stat events on existing bpf program id\"),\n\tOPT_BOOLEAN(0, \"bpf-counters\", &target.use_bpf,\n\t\t    \"use bpf program to count events\"),\n\tOPT_STRING(0, \"bpf-attr-map\", &target.attr_map, \"attr-map-path\",\n\t\t   \"path to perf_event_attr map\"),\n#endif\n\tOPT_BOOLEAN('a', \"all-cpus\", &target.system_wide,\n\t\t    \"system-wide collection from all CPUs\"),\n\tOPT_BOOLEAN(0, \"scale\", &stat_config.scale,\n\t\t    \"Use --no-scale to disable counter scaling for multiplexing\"),\n\tOPT_INCR('v', \"verbose\", &verbose,\n\t\t    \"be more verbose (show counter open errors, etc)\"),\n\tOPT_INTEGER('r', \"repeat\", &stat_config.run_count,\n\t\t    \"repeat command and print average + stddev (max: 100, forever: 0)\"),\n\tOPT_BOOLEAN(0, \"table\", &stat_config.walltime_run_table,\n\t\t    \"display details about each run (only with -r option)\"),\n\tOPT_BOOLEAN('n', \"null\", &stat_config.null_run,\n\t\t    \"null run - dont start any counters\"),\n\tOPT_INCR('d', \"detailed\", &detailed_run,\n\t\t    \"detailed run - start a lot of events\"),\n\tOPT_BOOLEAN('S', \"sync\", &sync_run,\n\t\t    \"call sync() before starting a run\"),\n\tOPT_CALLBACK_NOOPT('B', \"big-num\", NULL, NULL,\n\t\t\t   \"print large numbers with thousands\\' separators\",\n\t\t\t   stat__set_big_num),\n\tOPT_STRING('C', \"cpu\", &target.cpu_list, \"cpu\",\n\t\t    \"list of cpus to monitor in system-wide\"),\n\tOPT_SET_UINT('A', \"no-aggr\", &stat_config.aggr_mode,\n\t\t    \"disable CPU count aggregation\", AGGR_NONE),\n\tOPT_BOOLEAN(0, \"no-merge\", &stat_config.no_merge, \"Do not merge identical named events\"),\n\tOPT_BOOLEAN(0, \"hybrid-merge\", &stat_config.hybrid_merge,\n\t\t    \"Merge identical named hybrid events\"),\n\tOPT_STRING('x', \"field-separator\", &stat_config.csv_sep, \"separator\",\n\t\t   \"print counts with custom separator\"),\n\tOPT_BOOLEAN('j', \"json-output\", &stat_config.json_output,\n\t\t   \"print counts in JSON format\"),\n\tOPT_CALLBACK('G', \"cgroup\", &evsel_list, \"name\",\n\t\t     \"monitor event in cgroup name only\", parse_stat_cgroups),\n\tOPT_STRING(0, \"for-each-cgroup\", &stat_config.cgroup_list, \"name\",\n\t\t    \"expand events for each cgroup\"),\n\tOPT_STRING('o', \"output\", &output_name, \"file\", \"output file name\"),\n\tOPT_BOOLEAN(0, \"append\", &append_file, \"append to the output file\"),\n\tOPT_INTEGER(0, \"log-fd\", &output_fd,\n\t\t    \"log output to fd, instead of stderr\"),\n\tOPT_STRING(0, \"pre\", &pre_cmd, \"command\",\n\t\t\t\"command to run prior to the measured command\"),\n\tOPT_STRING(0, \"post\", &post_cmd, \"command\",\n\t\t\t\"command to run after to the measured command\"),\n\tOPT_UINTEGER('I', \"interval-print\", &stat_config.interval,\n\t\t    \"print counts at regular interval in ms \"\n\t\t    \"(overhead is possible for values <= 100ms)\"),\n\tOPT_INTEGER(0, \"interval-count\", &stat_config.times,\n\t\t    \"print counts for fixed number of times\"),\n\tOPT_BOOLEAN(0, \"interval-clear\", &stat_config.interval_clear,\n\t\t    \"clear screen in between new interval\"),\n\tOPT_UINTEGER(0, \"timeout\", &stat_config.timeout,\n\t\t    \"stop workload and print counts after a timeout period in ms (>= 10ms)\"),\n\tOPT_SET_UINT(0, \"per-socket\", &stat_config.aggr_mode,\n\t\t     \"aggregate counts per processor socket\", AGGR_SOCKET),\n\tOPT_SET_UINT(0, \"per-die\", &stat_config.aggr_mode,\n\t\t     \"aggregate counts per processor die\", AGGR_DIE),\n\tOPT_CALLBACK_OPTARG(0, \"per-cache\", &stat_config.aggr_mode, &stat_config.aggr_level,\n\t\t\t    \"cache level\", \"aggregate count at this cache level (Default: LLC)\",\n\t\t\t    parse_cache_level),\n\tOPT_SET_UINT(0, \"per-core\", &stat_config.aggr_mode,\n\t\t     \"aggregate counts per physical processor core\", AGGR_CORE),\n\tOPT_SET_UINT(0, \"per-thread\", &stat_config.aggr_mode,\n\t\t     \"aggregate counts per thread\", AGGR_THREAD),\n\tOPT_SET_UINT(0, \"per-node\", &stat_config.aggr_mode,\n\t\t     \"aggregate counts per numa node\", AGGR_NODE),\n\tOPT_INTEGER('D', \"delay\", &target.initial_delay,\n\t\t    \"ms to wait before starting measurement after program start (-1: start with events disabled)\"),\n\tOPT_CALLBACK_NOOPT(0, \"metric-only\", &stat_config.metric_only, NULL,\n\t\t\t\"Only print computed metrics. No raw values\", enable_metric_only),\n\tOPT_BOOLEAN(0, \"metric-no-group\", &stat_config.metric_no_group,\n\t\t       \"don't group metric events, impacts multiplexing\"),\n\tOPT_BOOLEAN(0, \"metric-no-merge\", &stat_config.metric_no_merge,\n\t\t       \"don't try to share events between metrics in a group\"),\n\tOPT_BOOLEAN(0, \"metric-no-threshold\", &stat_config.metric_no_threshold,\n\t\t       \"don't try to share events between metrics in a group  \"),\n\tOPT_BOOLEAN(0, \"topdown\", &topdown_run,\n\t\t\t\"measure top-down statistics\"),\n\tOPT_UINTEGER(0, \"td-level\", &stat_config.topdown_level,\n\t\t\t\"Set the metrics level for the top-down statistics (0: max level)\"),\n\tOPT_BOOLEAN(0, \"smi-cost\", &smi_cost,\n\t\t\t\"measure SMI cost\"),\n\tOPT_CALLBACK('M', \"metrics\", &evsel_list, \"metric/metric group list\",\n\t\t     \"monitor specified metrics or metric groups (separated by ,)\",\n\t\t     append_metric_groups),\n\tOPT_BOOLEAN_FLAG(0, \"all-kernel\", &stat_config.all_kernel,\n\t\t\t \"Configure all used events to run in kernel space.\",\n\t\t\t PARSE_OPT_EXCLUSIVE),\n\tOPT_BOOLEAN_FLAG(0, \"all-user\", &stat_config.all_user,\n\t\t\t \"Configure all used events to run in user space.\",\n\t\t\t PARSE_OPT_EXCLUSIVE),\n\tOPT_BOOLEAN(0, \"percore-show-thread\", &stat_config.percore_show_thread,\n\t\t    \"Use with 'percore' event qualifier to show the event \"\n\t\t    \"counts of one hardware thread by sum up total hardware \"\n\t\t    \"threads of same physical core\"),\n\tOPT_BOOLEAN(0, \"summary\", &stat_config.summary,\n\t\t       \"print summary for interval mode\"),\n\tOPT_BOOLEAN(0, \"no-csv-summary\", &stat_config.no_csv_summary,\n\t\t       \"don't print 'summary' for CSV summary output\"),\n\tOPT_BOOLEAN(0, \"quiet\", &quiet,\n\t\t\t\"don't print any output, messages or warnings (useful with record)\"),\n\tOPT_CALLBACK(0, \"cputype\", &evsel_list, \"hybrid cpu type\",\n\t\t     \"Only enable events on applying cpu with this type \"\n\t\t     \"for hybrid platform (e.g. core or atom)\",\n\t\t     parse_cputype),\n#ifdef HAVE_LIBPFM\n\tOPT_CALLBACK(0, \"pfm-events\", &evsel_list, \"event\",\n\t\t\"libpfm4 event selector. use 'perf list' to list available events\",\n\t\tparse_libpfm_events_option),\n#endif\n\tOPT_CALLBACK(0, \"control\", &stat_config, \"fd:ctl-fd[,ack-fd] or fifo:ctl-fifo[,ack-fifo]\",\n\t\t     \"Listen on ctl-fd descriptor for command to control measurement ('enable': enable events, 'disable': disable events).\\n\"\n\t\t     \"\\t\\t\\t  Optionally send control command completion ('ack\\\\n') to ack-fd descriptor.\\n\"\n\t\t     \"\\t\\t\\t  Alternatively, ctl-fifo / ack-fifo will be opened and used as ctl-fd / ack-fd.\",\n\t\t      parse_control_option),\n\tOPT_CALLBACK_OPTARG(0, \"iostat\", &evsel_list, &stat_config, \"default\",\n\t\t\t    \"measure I/O performance metrics provided by arch/platform\",\n\t\t\t    iostat_parse),\n\tOPT_END()\n};\n\n \nstatic int cpu__get_cache_id_from_map(struct perf_cpu cpu, char *map)\n{\n\tint id;\n\tstruct perf_cpu_map *cpu_map = perf_cpu_map__new(map);\n\n\t \n\tif (perf_cpu_map__empty(cpu_map))\n\t\tid = cpu.cpu;\n\telse\n\t\tid = perf_cpu_map__cpu(cpu_map, 0).cpu;\n\n\t \n\tperf_cpu_map__put(cpu_map);\n\n\treturn id;\n}\n\n \nstatic int cpu__get_cache_details(struct perf_cpu cpu, struct perf_cache *cache)\n{\n\tint ret = 0;\n\tu32 cache_level = stat_config.aggr_level;\n\tstruct cpu_cache_level caches[MAX_CACHE_LVL];\n\tu32 i = 0, caches_cnt = 0;\n\n\tcache->cache_lvl = (cache_level > MAX_CACHE_LVL) ? 0 : cache_level;\n\tcache->cache = -1;\n\n\tret = build_caches_for_cpu(cpu.cpu, caches, &caches_cnt);\n\tif (ret) {\n\t\t \n\t\tif (caches_cnt)\n\t\t\tgoto free_caches;\n\n\t\treturn ret;\n\t}\n\n\tif (!caches_cnt)\n\t\treturn -1;\n\n\t \n\tif (cache_level > MAX_CACHE_LVL) {\n\t\tint max_level_index = 0;\n\n\t\tfor (i = 1; i < caches_cnt; ++i) {\n\t\t\tif (caches[i].level > caches[max_level_index].level)\n\t\t\t\tmax_level_index = i;\n\t\t}\n\n\t\tcache->cache_lvl = caches[max_level_index].level;\n\t\tcache->cache = cpu__get_cache_id_from_map(cpu, caches[max_level_index].map);\n\n\t\t \n\t\ti = 0;\n\t\tgoto free_caches;\n\t}\n\n\tfor (i = 0; i < caches_cnt; ++i) {\n\t\tif (caches[i].level == cache_level) {\n\t\t\tcache->cache_lvl = cache_level;\n\t\t\tcache->cache = cpu__get_cache_id_from_map(cpu, caches[i].map);\n\t\t}\n\n\t\tcpu_cache_level__free(&caches[i]);\n\t}\n\nfree_caches:\n\t \n\twhile (i < caches_cnt)\n\t\tcpu_cache_level__free(&caches[i++]);\n\n\treturn ret;\n}\n\n \nstatic struct aggr_cpu_id aggr_cpu_id__cache(struct perf_cpu cpu, void *data)\n{\n\tint ret;\n\tstruct aggr_cpu_id id;\n\tstruct perf_cache cache;\n\n\tid = aggr_cpu_id__die(cpu, data);\n\tif (aggr_cpu_id__is_empty(&id))\n\t\treturn id;\n\n\tret = cpu__get_cache_details(cpu, &cache);\n\tif (ret)\n\t\treturn id;\n\n\tid.cache_lvl = cache.cache_lvl;\n\tid.cache = cache.cache;\n\treturn id;\n}\n\nstatic const char *const aggr_mode__string[] = {\n\t[AGGR_CORE] = \"core\",\n\t[AGGR_CACHE] = \"cache\",\n\t[AGGR_DIE] = \"die\",\n\t[AGGR_GLOBAL] = \"global\",\n\t[AGGR_NODE] = \"node\",\n\t[AGGR_NONE] = \"none\",\n\t[AGGR_SOCKET] = \"socket\",\n\t[AGGR_THREAD] = \"thread\",\n\t[AGGR_UNSET] = \"unset\",\n};\n\nstatic struct aggr_cpu_id perf_stat__get_socket(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\tstruct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__socket(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_die(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t     struct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__die(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_cache_id(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t  struct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__cache(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_core(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t      struct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__core(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_node(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t      struct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__node(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_global(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\tstruct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__global(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_cpu(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t     struct perf_cpu cpu)\n{\n\treturn aggr_cpu_id__cpu(cpu,  NULL);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_aggr(struct perf_stat_config *config,\n\t\t\t\t\t      aggr_get_id_t get_id, struct perf_cpu cpu)\n{\n\tstruct aggr_cpu_id id;\n\n\t \n\tif (cpu.cpu == -1)\n\t\treturn get_id(config, cpu);\n\n\tif (aggr_cpu_id__is_empty(&config->cpus_aggr_map->map[cpu.cpu]))\n\t\tconfig->cpus_aggr_map->map[cpu.cpu] = get_id(config, cpu);\n\n\tid = config->cpus_aggr_map->map[cpu.cpu];\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_stat__get_socket_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t       struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_socket, cpu);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_die_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t    struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_die, cpu);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_cache_id_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t\t struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_cache_id, cpu);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_core_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t     struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_core, cpu);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_node_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t     struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_node, cpu);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_global_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t       struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_global, cpu);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_cpu_cached(struct perf_stat_config *config,\n\t\t\t\t\t\t    struct perf_cpu cpu)\n{\n\treturn perf_stat__get_aggr(config, perf_stat__get_cpu, cpu);\n}\n\nstatic aggr_cpu_id_get_t aggr_mode__get_aggr(enum aggr_mode aggr_mode)\n{\n\tswitch (aggr_mode) {\n\tcase AGGR_SOCKET:\n\t\treturn aggr_cpu_id__socket;\n\tcase AGGR_DIE:\n\t\treturn aggr_cpu_id__die;\n\tcase AGGR_CACHE:\n\t\treturn aggr_cpu_id__cache;\n\tcase AGGR_CORE:\n\t\treturn aggr_cpu_id__core;\n\tcase AGGR_NODE:\n\t\treturn aggr_cpu_id__node;\n\tcase AGGR_NONE:\n\t\treturn aggr_cpu_id__cpu;\n\tcase AGGR_GLOBAL:\n\t\treturn aggr_cpu_id__global;\n\tcase AGGR_THREAD:\n\tcase AGGR_UNSET:\n\tcase AGGR_MAX:\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic aggr_get_id_t aggr_mode__get_id(enum aggr_mode aggr_mode)\n{\n\tswitch (aggr_mode) {\n\tcase AGGR_SOCKET:\n\t\treturn perf_stat__get_socket_cached;\n\tcase AGGR_DIE:\n\t\treturn perf_stat__get_die_cached;\n\tcase AGGR_CACHE:\n\t\treturn perf_stat__get_cache_id_cached;\n\tcase AGGR_CORE:\n\t\treturn perf_stat__get_core_cached;\n\tcase AGGR_NODE:\n\t\treturn perf_stat__get_node_cached;\n\tcase AGGR_NONE:\n\t\treturn perf_stat__get_cpu_cached;\n\tcase AGGR_GLOBAL:\n\t\treturn perf_stat__get_global_cached;\n\tcase AGGR_THREAD:\n\tcase AGGR_UNSET:\n\tcase AGGR_MAX:\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic int perf_stat_init_aggr_mode(void)\n{\n\tint nr;\n\taggr_cpu_id_get_t get_id = aggr_mode__get_aggr(stat_config.aggr_mode);\n\n\tif (get_id) {\n\t\tbool needs_sort = stat_config.aggr_mode != AGGR_NONE;\n\t\tstat_config.aggr_map = cpu_aggr_map__new(evsel_list->core.user_requested_cpus,\n\t\t\t\t\t\t\t get_id,  NULL, needs_sort);\n\t\tif (!stat_config.aggr_map) {\n\t\t\tpr_err(\"cannot build %s map\\n\", aggr_mode__string[stat_config.aggr_mode]);\n\t\t\treturn -1;\n\t\t}\n\t\tstat_config.aggr_get_id = aggr_mode__get_id(stat_config.aggr_mode);\n\t}\n\n\tif (stat_config.aggr_mode == AGGR_THREAD) {\n\t\tnr = perf_thread_map__nr(evsel_list->core.threads);\n\t\tstat_config.aggr_map = cpu_aggr_map__empty_new(nr);\n\t\tif (stat_config.aggr_map == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tfor (int s = 0; s < nr; s++) {\n\t\t\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\t\t\tid.thread_idx = s;\n\t\t\tstat_config.aggr_map->map[s] = id;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tif (!perf_cpu_map__empty(evsel_list->core.user_requested_cpus))\n\t\tnr = perf_cpu_map__max(evsel_list->core.user_requested_cpus).cpu;\n\telse\n\t\tnr = 0;\n\tstat_config.cpus_aggr_map = cpu_aggr_map__empty_new(nr + 1);\n\treturn stat_config.cpus_aggr_map ? 0 : -ENOMEM;\n}\n\nstatic void cpu_aggr_map__delete(struct cpu_aggr_map *map)\n{\n\tif (map) {\n\t\tWARN_ONCE(refcount_read(&map->refcnt) != 0,\n\t\t\t  \"cpu_aggr_map refcnt unbalanced\\n\");\n\t\tfree(map);\n\t}\n}\n\nstatic void cpu_aggr_map__put(struct cpu_aggr_map *map)\n{\n\tif (map && refcount_dec_and_test(&map->refcnt))\n\t\tcpu_aggr_map__delete(map);\n}\n\nstatic void perf_stat__exit_aggr_mode(void)\n{\n\tcpu_aggr_map__put(stat_config.aggr_map);\n\tcpu_aggr_map__put(stat_config.cpus_aggr_map);\n\tstat_config.aggr_map = NULL;\n\tstat_config.cpus_aggr_map = NULL;\n}\n\nstatic struct aggr_cpu_id perf_env__get_socket_aggr_by_cpu(struct perf_cpu cpu, void *data)\n{\n\tstruct perf_env *env = data;\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tif (cpu.cpu != -1)\n\t\tid.socket = env->cpu[cpu.cpu].socket_id;\n\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_env__get_die_aggr_by_cpu(struct perf_cpu cpu, void *data)\n{\n\tstruct perf_env *env = data;\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tif (cpu.cpu != -1) {\n\t\t \n\t\tid.socket = env->cpu[cpu.cpu].socket_id;\n\t\tid.die = env->cpu[cpu.cpu].die_id;\n\t}\n\n\treturn id;\n}\n\nstatic void perf_env__get_cache_id_for_cpu(struct perf_cpu cpu, struct perf_env *env,\n\t\t\t\t\t   u32 cache_level, struct aggr_cpu_id *id)\n{\n\tint i;\n\tint caches_cnt = env->caches_cnt;\n\tstruct cpu_cache_level *caches = env->caches;\n\n\tid->cache_lvl = (cache_level > MAX_CACHE_LVL) ? 0 : cache_level;\n\tid->cache = -1;\n\n\tif (!caches_cnt)\n\t\treturn;\n\n\tfor (i = caches_cnt - 1; i > -1; --i) {\n\t\tstruct perf_cpu_map *cpu_map;\n\t\tint map_contains_cpu;\n\n\t\t \n\t\tif (cache_level <= MAX_CACHE_LVL && caches[i].level != cache_level)\n\t\t\tcontinue;\n\n\t\tcpu_map = perf_cpu_map__new(caches[i].map);\n\t\tmap_contains_cpu = perf_cpu_map__idx(cpu_map, cpu);\n\t\tperf_cpu_map__put(cpu_map);\n\n\t\tif (map_contains_cpu != -1) {\n\t\t\tid->cache_lvl = caches[i].level;\n\t\t\tid->cache = cpu__get_cache_id_from_map(cpu, caches[i].map);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic struct aggr_cpu_id perf_env__get_cache_aggr_by_cpu(struct perf_cpu cpu,\n\t\t\t\t\t\t\t  void *data)\n{\n\tstruct perf_env *env = data;\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tif (cpu.cpu != -1) {\n\t\tu32 cache_level = (perf_stat.aggr_level) ?: stat_config.aggr_level;\n\n\t\tid.socket = env->cpu[cpu.cpu].socket_id;\n\t\tid.die = env->cpu[cpu.cpu].die_id;\n\t\tperf_env__get_cache_id_for_cpu(cpu, env, cache_level, &id);\n\t}\n\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_env__get_core_aggr_by_cpu(struct perf_cpu cpu, void *data)\n{\n\tstruct perf_env *env = data;\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tif (cpu.cpu != -1) {\n\t\t \n\t\tid.socket = env->cpu[cpu.cpu].socket_id;\n\t\tid.die = env->cpu[cpu.cpu].die_id;\n\t\tid.core = env->cpu[cpu.cpu].core_id;\n\t}\n\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_env__get_cpu_aggr_by_cpu(struct perf_cpu cpu, void *data)\n{\n\tstruct perf_env *env = data;\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tif (cpu.cpu != -1) {\n\t\t \n\t\tid.socket = env->cpu[cpu.cpu].socket_id;\n\t\tid.die = env->cpu[cpu.cpu].die_id;\n\t\tid.core = env->cpu[cpu.cpu].core_id;\n\t\tid.cpu = cpu;\n\t}\n\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_env__get_node_aggr_by_cpu(struct perf_cpu cpu, void *data)\n{\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\tid.node = perf_env__numa_node(data, cpu);\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_env__get_global_aggr_by_cpu(struct perf_cpu cpu __maybe_unused,\n\t\t\t\t\t\t\t   void *data __maybe_unused)\n{\n\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\t \n\tid.cpu = (struct perf_cpu){ .cpu = 0 };\n\treturn id;\n}\n\nstatic struct aggr_cpu_id perf_stat__get_socket_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t     struct perf_cpu cpu)\n{\n\treturn perf_env__get_socket_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\nstatic struct aggr_cpu_id perf_stat__get_die_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t  struct perf_cpu cpu)\n{\n\treturn perf_env__get_die_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_cache_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t    struct perf_cpu cpu)\n{\n\treturn perf_env__get_cache_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_core_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t   struct perf_cpu cpu)\n{\n\treturn perf_env__get_core_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_cpu_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t  struct perf_cpu cpu)\n{\n\treturn perf_env__get_cpu_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_node_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t   struct perf_cpu cpu)\n{\n\treturn perf_env__get_node_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\n\nstatic struct aggr_cpu_id perf_stat__get_global_file(struct perf_stat_config *config __maybe_unused,\n\t\t\t\t\t\t     struct perf_cpu cpu)\n{\n\treturn perf_env__get_global_aggr_by_cpu(cpu, &perf_stat.session->header.env);\n}\n\nstatic aggr_cpu_id_get_t aggr_mode__get_aggr_file(enum aggr_mode aggr_mode)\n{\n\tswitch (aggr_mode) {\n\tcase AGGR_SOCKET:\n\t\treturn perf_env__get_socket_aggr_by_cpu;\n\tcase AGGR_DIE:\n\t\treturn perf_env__get_die_aggr_by_cpu;\n\tcase AGGR_CACHE:\n\t\treturn perf_env__get_cache_aggr_by_cpu;\n\tcase AGGR_CORE:\n\t\treturn perf_env__get_core_aggr_by_cpu;\n\tcase AGGR_NODE:\n\t\treturn perf_env__get_node_aggr_by_cpu;\n\tcase AGGR_GLOBAL:\n\t\treturn perf_env__get_global_aggr_by_cpu;\n\tcase AGGR_NONE:\n\t\treturn perf_env__get_cpu_aggr_by_cpu;\n\tcase AGGR_THREAD:\n\tcase AGGR_UNSET:\n\tcase AGGR_MAX:\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic aggr_get_id_t aggr_mode__get_id_file(enum aggr_mode aggr_mode)\n{\n\tswitch (aggr_mode) {\n\tcase AGGR_SOCKET:\n\t\treturn perf_stat__get_socket_file;\n\tcase AGGR_DIE:\n\t\treturn perf_stat__get_die_file;\n\tcase AGGR_CACHE:\n\t\treturn perf_stat__get_cache_file;\n\tcase AGGR_CORE:\n\t\treturn perf_stat__get_core_file;\n\tcase AGGR_NODE:\n\t\treturn perf_stat__get_node_file;\n\tcase AGGR_GLOBAL:\n\t\treturn perf_stat__get_global_file;\n\tcase AGGR_NONE:\n\t\treturn perf_stat__get_cpu_file;\n\tcase AGGR_THREAD:\n\tcase AGGR_UNSET:\n\tcase AGGR_MAX:\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic int perf_stat_init_aggr_mode_file(struct perf_stat *st)\n{\n\tstruct perf_env *env = &st->session->header.env;\n\taggr_cpu_id_get_t get_id = aggr_mode__get_aggr_file(stat_config.aggr_mode);\n\tbool needs_sort = stat_config.aggr_mode != AGGR_NONE;\n\n\tif (stat_config.aggr_mode == AGGR_THREAD) {\n\t\tint nr = perf_thread_map__nr(evsel_list->core.threads);\n\n\t\tstat_config.aggr_map = cpu_aggr_map__empty_new(nr);\n\t\tif (stat_config.aggr_map == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tfor (int s = 0; s < nr; s++) {\n\t\t\tstruct aggr_cpu_id id = aggr_cpu_id__empty();\n\n\t\t\tid.thread_idx = s;\n\t\t\tstat_config.aggr_map->map[s] = id;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (!get_id)\n\t\treturn 0;\n\n\tstat_config.aggr_map = cpu_aggr_map__new(evsel_list->core.user_requested_cpus,\n\t\t\t\t\t\t get_id, env, needs_sort);\n\tif (!stat_config.aggr_map) {\n\t\tpr_err(\"cannot build %s map\\n\", aggr_mode__string[stat_config.aggr_mode]);\n\t\treturn -1;\n\t}\n\tstat_config.aggr_get_id = aggr_mode__get_id_file(stat_config.aggr_mode);\n\treturn 0;\n}\n\n \nstatic int add_default_attributes(void)\n{\n\tstruct perf_event_attr default_attrs0[] = {\n\n  { .type = PERF_TYPE_SOFTWARE, .config = PERF_COUNT_SW_TASK_CLOCK\t\t},\n  { .type = PERF_TYPE_SOFTWARE, .config = PERF_COUNT_SW_CONTEXT_SWITCHES\t},\n  { .type = PERF_TYPE_SOFTWARE, .config = PERF_COUNT_SW_CPU_MIGRATIONS\t\t},\n  { .type = PERF_TYPE_SOFTWARE, .config = PERF_COUNT_SW_PAGE_FAULTS\t\t},\n\n  { .type = PERF_TYPE_HARDWARE, .config = PERF_COUNT_HW_CPU_CYCLES\t\t},\n};\n\tstruct perf_event_attr frontend_attrs[] = {\n  { .type = PERF_TYPE_HARDWARE, .config = PERF_COUNT_HW_STALLED_CYCLES_FRONTEND\t},\n};\n\tstruct perf_event_attr backend_attrs[] = {\n  { .type = PERF_TYPE_HARDWARE, .config = PERF_COUNT_HW_STALLED_CYCLES_BACKEND\t},\n};\n\tstruct perf_event_attr default_attrs1[] = {\n  { .type = PERF_TYPE_HARDWARE, .config = PERF_COUNT_HW_INSTRUCTIONS\t\t},\n  { .type = PERF_TYPE_HARDWARE, .config = PERF_COUNT_HW_BRANCH_INSTRUCTIONS\t},\n  { .type = PERF_TYPE_HARDWARE, .config = PERF_COUNT_HW_BRANCH_MISSES\t\t},\n\n};\n\n \n\tstruct perf_event_attr detailed_attrs[] = {\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_L1D\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_ACCESS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_L1D\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_MISS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_LL\t\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_ACCESS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_LL\t\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_MISS\t<< 16)\t\t\t\t},\n};\n\n \n\tstruct perf_event_attr very_detailed_attrs[] = {\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_L1I\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_ACCESS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_L1I\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_MISS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_DTLB\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_ACCESS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_DTLB\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_MISS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_ITLB\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_ACCESS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_ITLB\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_READ\t\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_MISS\t<< 16)\t\t\t\t},\n\n};\n\n \n\tstruct perf_event_attr very_very_detailed_attrs[] = {\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_L1D\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_PREFETCH\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_ACCESS\t<< 16)\t\t\t\t},\n\n  { .type = PERF_TYPE_HW_CACHE,\n    .config =\n\t PERF_COUNT_HW_CACHE_L1D\t\t<<  0  |\n\t(PERF_COUNT_HW_CACHE_OP_PREFETCH\t<<  8) |\n\t(PERF_COUNT_HW_CACHE_RESULT_MISS\t<< 16)\t\t\t\t},\n};\n\n\tstruct perf_event_attr default_null_attrs[] = {};\n\tconst char *pmu = parse_events_option_args.pmu_filter ?: \"all\";\n\n\t \n\tif (stat_config.null_run)\n\t\treturn 0;\n\n\tif (transaction_run) {\n\t\t \n\t\tif (!metricgroup__has_metric(pmu, \"transaction\")) {\n\t\t\tpr_err(\"Missing transaction metrics\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\treturn metricgroup__parse_groups(evsel_list, pmu, \"transaction\",\n\t\t\t\t\t\tstat_config.metric_no_group,\n\t\t\t\t\t\tstat_config.metric_no_merge,\n\t\t\t\t\t\tstat_config.metric_no_threshold,\n\t\t\t\t\t\tstat_config.user_requested_cpu_list,\n\t\t\t\t\t\tstat_config.system_wide,\n\t\t\t\t\t\t&stat_config.metric_events);\n\t}\n\n\tif (smi_cost) {\n\t\tint smi;\n\n\t\tif (sysfs__read_int(FREEZE_ON_SMI_PATH, &smi) < 0) {\n\t\t\tpr_err(\"freeze_on_smi is not supported.\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!smi) {\n\t\t\tif (sysfs__write_int(FREEZE_ON_SMI_PATH, 1) < 0) {\n\t\t\t\tfprintf(stderr, \"Failed to set freeze_on_smi.\\n\");\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tsmi_reset = true;\n\t\t}\n\n\t\tif (!metricgroup__has_metric(pmu, \"smi\")) {\n\t\t\tpr_err(\"Missing smi metrics\\n\");\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (!force_metric_only)\n\t\t\tstat_config.metric_only = true;\n\n\t\treturn metricgroup__parse_groups(evsel_list, pmu, \"smi\",\n\t\t\t\t\t\tstat_config.metric_no_group,\n\t\t\t\t\t\tstat_config.metric_no_merge,\n\t\t\t\t\t\tstat_config.metric_no_threshold,\n\t\t\t\t\t\tstat_config.user_requested_cpu_list,\n\t\t\t\t\t\tstat_config.system_wide,\n\t\t\t\t\t\t&stat_config.metric_events);\n\t}\n\n\tif (topdown_run) {\n\t\tunsigned int max_level = metricgroups__topdown_max_level();\n\t\tchar str[] = \"TopdownL1\";\n\n\t\tif (!force_metric_only)\n\t\t\tstat_config.metric_only = true;\n\n\t\tif (!max_level) {\n\t\t\tpr_err(\"Topdown requested but the topdown metric groups aren't present.\\n\"\n\t\t\t\t\"(See perf list the metric groups have names like TopdownL1)\\n\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (stat_config.topdown_level > max_level) {\n\t\t\tpr_err(\"Invalid top-down metrics level. The max level is %u.\\n\", max_level);\n\t\t\treturn -1;\n\t\t} else if (!stat_config.topdown_level)\n\t\t\tstat_config.topdown_level = 1;\n\n\t\tif (!stat_config.interval && !stat_config.metric_only) {\n\t\t\tfprintf(stat_config.output,\n\t\t\t\t\"Topdown accuracy may decrease when measuring long periods.\\n\"\n\t\t\t\t\"Please print the result regularly, e.g. -I1000\\n\");\n\t\t}\n\t\tstr[8] = stat_config.topdown_level + '0';\n\t\tif (metricgroup__parse_groups(evsel_list,\n\t\t\t\t\t\tpmu, str,\n\t\t\t\t\t\t false,\n\t\t\t\t\t\t false,\n\t\t\t\t\t\t true,\n\t\t\t\t\t\tstat_config.user_requested_cpu_list,\n\t\t\t\t\t\tstat_config.system_wide,\n\t\t\t\t\t\t&stat_config.metric_events) < 0)\n\t\t\treturn -1;\n\t}\n\n\tif (!stat_config.topdown_level)\n\t\tstat_config.topdown_level = 1;\n\n\tif (!evsel_list->core.nr_entries) {\n\t\t \n\t\tif (target__has_cpu(&target))\n\t\t\tdefault_attrs0[0].config = PERF_COUNT_SW_CPU_CLOCK;\n\n\t\tif (evlist__add_default_attrs(evsel_list, default_attrs0) < 0)\n\t\t\treturn -1;\n\t\tif (perf_pmus__have_event(\"cpu\", \"stalled-cycles-frontend\")) {\n\t\t\tif (evlist__add_default_attrs(evsel_list, frontend_attrs) < 0)\n\t\t\t\treturn -1;\n\t\t}\n\t\tif (perf_pmus__have_event(\"cpu\", \"stalled-cycles-backend\")) {\n\t\t\tif (evlist__add_default_attrs(evsel_list, backend_attrs) < 0)\n\t\t\t\treturn -1;\n\t\t}\n\t\tif (evlist__add_default_attrs(evsel_list, default_attrs1) < 0)\n\t\t\treturn -1;\n\t\t \n\t\tif (metricgroup__has_metric(pmu, \"Default\")) {\n\t\t\tstruct evlist *metric_evlist = evlist__new();\n\t\t\tstruct evsel *metric_evsel;\n\n\t\t\tif (!metric_evlist)\n\t\t\t\treturn -1;\n\n\t\t\tif (metricgroup__parse_groups(metric_evlist, pmu, \"Default\",\n\t\t\t\t\t\t\t false,\n\t\t\t\t\t\t\t false,\n\t\t\t\t\t\t\t true,\n\t\t\t\t\t\t\tstat_config.user_requested_cpu_list,\n\t\t\t\t\t\t\tstat_config.system_wide,\n\t\t\t\t\t\t\t&stat_config.metric_events) < 0)\n\t\t\t\treturn -1;\n\n\t\t\tevlist__for_each_entry(metric_evlist, metric_evsel) {\n\t\t\t\tmetric_evsel->skippable = true;\n\t\t\t\tmetric_evsel->default_metricgroup = true;\n\t\t\t}\n\t\t\tevlist__splice_list_tail(evsel_list, &metric_evlist->core.entries);\n\t\t\tevlist__delete(metric_evlist);\n\t\t}\n\n\t\t \n\t\tif (evlist__add_default_attrs(evsel_list, default_null_attrs) < 0)\n\t\t\treturn -1;\n\t}\n\n\t \n\n\tif (detailed_run <  1)\n\t\treturn 0;\n\n\t \n\tif (evlist__add_default_attrs(evsel_list, detailed_attrs) < 0)\n\t\treturn -1;\n\n\tif (detailed_run < 2)\n\t\treturn 0;\n\n\t \n\tif (evlist__add_default_attrs(evsel_list, very_detailed_attrs) < 0)\n\t\treturn -1;\n\n\tif (detailed_run < 3)\n\t\treturn 0;\n\n\t \n\treturn evlist__add_default_attrs(evsel_list, very_very_detailed_attrs);\n}\n\nstatic const char * const stat_record_usage[] = {\n\t\"perf stat record [<options>]\",\n\tNULL,\n};\n\nstatic void init_features(struct perf_session *session)\n{\n\tint feat;\n\n\tfor (feat = HEADER_FIRST_FEATURE; feat < HEADER_LAST_FEATURE; feat++)\n\t\tperf_header__set_feat(&session->header, feat);\n\n\tperf_header__clear_feat(&session->header, HEADER_DIR_FORMAT);\n\tperf_header__clear_feat(&session->header, HEADER_BUILD_ID);\n\tperf_header__clear_feat(&session->header, HEADER_TRACING_DATA);\n\tperf_header__clear_feat(&session->header, HEADER_BRANCH_STACK);\n\tperf_header__clear_feat(&session->header, HEADER_AUXTRACE);\n}\n\nstatic int __cmd_record(int argc, const char **argv)\n{\n\tstruct perf_session *session;\n\tstruct perf_data *data = &perf_stat.data;\n\n\targc = parse_options(argc, argv, stat_options, stat_record_usage,\n\t\t\t     PARSE_OPT_STOP_AT_NON_OPTION);\n\n\tif (output_name)\n\t\tdata->path = output_name;\n\n\tif (stat_config.run_count != 1 || forever) {\n\t\tpr_err(\"Cannot use -r option with perf stat record.\\n\");\n\t\treturn -1;\n\t}\n\n\tsession = perf_session__new(data, NULL);\n\tif (IS_ERR(session)) {\n\t\tpr_err(\"Perf session creation failed\\n\");\n\t\treturn PTR_ERR(session);\n\t}\n\n\tinit_features(session);\n\n\tsession->evlist   = evsel_list;\n\tperf_stat.session = session;\n\tperf_stat.record  = true;\n\treturn argc;\n}\n\nstatic int process_stat_round_event(struct perf_session *session,\n\t\t\t\t    union perf_event *event)\n{\n\tstruct perf_record_stat_round *stat_round = &event->stat_round;\n\tstruct timespec tsh, *ts = NULL;\n\tconst char **argv = session->header.env.cmdline_argv;\n\tint argc = session->header.env.nr_cmdline;\n\n\tprocess_counters();\n\n\tif (stat_round->type == PERF_STAT_ROUND_TYPE__FINAL)\n\t\tupdate_stats(&walltime_nsecs_stats, stat_round->time);\n\n\tif (stat_config.interval && stat_round->time) {\n\t\ttsh.tv_sec  = stat_round->time / NSEC_PER_SEC;\n\t\ttsh.tv_nsec = stat_round->time % NSEC_PER_SEC;\n\t\tts = &tsh;\n\t}\n\n\tprint_counters(ts, argc, argv);\n\treturn 0;\n}\n\nstatic\nint process_stat_config_event(struct perf_session *session,\n\t\t\t      union perf_event *event)\n{\n\tstruct perf_tool *tool = session->tool;\n\tstruct perf_stat *st = container_of(tool, struct perf_stat, tool);\n\n\tperf_event__read_stat_config(&stat_config, &event->stat_config);\n\n\tif (perf_cpu_map__empty(st->cpus)) {\n\t\tif (st->aggr_mode != AGGR_UNSET)\n\t\t\tpr_warning(\"warning: processing task data, aggregation mode not set\\n\");\n\t} else if (st->aggr_mode != AGGR_UNSET) {\n\t\tstat_config.aggr_mode = st->aggr_mode;\n\t}\n\n\tif (perf_stat.data.is_pipe)\n\t\tperf_stat_init_aggr_mode();\n\telse\n\t\tperf_stat_init_aggr_mode_file(st);\n\n\tif (stat_config.aggr_map) {\n\t\tint nr_aggr = stat_config.aggr_map->nr;\n\n\t\tif (evlist__alloc_aggr_stats(session->evlist, nr_aggr) < 0) {\n\t\t\tpr_err(\"cannot allocate aggr counts\\n\");\n\t\t\treturn -1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int set_maps(struct perf_stat *st)\n{\n\tif (!st->cpus || !st->threads)\n\t\treturn 0;\n\n\tif (WARN_ONCE(st->maps_allocated, \"stats double allocation\\n\"))\n\t\treturn -EINVAL;\n\n\tperf_evlist__set_maps(&evsel_list->core, st->cpus, st->threads);\n\n\tif (evlist__alloc_stats(&stat_config, evsel_list,  true))\n\t\treturn -ENOMEM;\n\n\tst->maps_allocated = true;\n\treturn 0;\n}\n\nstatic\nint process_thread_map_event(struct perf_session *session,\n\t\t\t     union perf_event *event)\n{\n\tstruct perf_tool *tool = session->tool;\n\tstruct perf_stat *st = container_of(tool, struct perf_stat, tool);\n\n\tif (st->threads) {\n\t\tpr_warning(\"Extra thread map event, ignoring.\\n\");\n\t\treturn 0;\n\t}\n\n\tst->threads = thread_map__new_event(&event->thread_map);\n\tif (!st->threads)\n\t\treturn -ENOMEM;\n\n\treturn set_maps(st);\n}\n\nstatic\nint process_cpu_map_event(struct perf_session *session,\n\t\t\t  union perf_event *event)\n{\n\tstruct perf_tool *tool = session->tool;\n\tstruct perf_stat *st = container_of(tool, struct perf_stat, tool);\n\tstruct perf_cpu_map *cpus;\n\n\tif (st->cpus) {\n\t\tpr_warning(\"Extra cpu map event, ignoring.\\n\");\n\t\treturn 0;\n\t}\n\n\tcpus = cpu_map__new_data(&event->cpu_map.data);\n\tif (!cpus)\n\t\treturn -ENOMEM;\n\n\tst->cpus = cpus;\n\treturn set_maps(st);\n}\n\nstatic const char * const stat_report_usage[] = {\n\t\"perf stat report [<options>]\",\n\tNULL,\n};\n\nstatic struct perf_stat perf_stat = {\n\t.tool = {\n\t\t.attr\t\t= perf_event__process_attr,\n\t\t.event_update\t= perf_event__process_event_update,\n\t\t.thread_map\t= process_thread_map_event,\n\t\t.cpu_map\t= process_cpu_map_event,\n\t\t.stat_config\t= process_stat_config_event,\n\t\t.stat\t\t= perf_event__process_stat_event,\n\t\t.stat_round\t= process_stat_round_event,\n\t},\n\t.aggr_mode\t= AGGR_UNSET,\n\t.aggr_level\t= 0,\n};\n\nstatic int __cmd_report(int argc, const char **argv)\n{\n\tstruct perf_session *session;\n\tconst struct option options[] = {\n\tOPT_STRING('i', \"input\", &input_name, \"file\", \"input file name\"),\n\tOPT_SET_UINT(0, \"per-socket\", &perf_stat.aggr_mode,\n\t\t     \"aggregate counts per processor socket\", AGGR_SOCKET),\n\tOPT_SET_UINT(0, \"per-die\", &perf_stat.aggr_mode,\n\t\t     \"aggregate counts per processor die\", AGGR_DIE),\n\tOPT_CALLBACK_OPTARG(0, \"per-cache\", &perf_stat.aggr_mode, &perf_stat.aggr_level,\n\t\t\t    \"cache level\",\n\t\t\t    \"aggregate count at this cache level (Default: LLC)\",\n\t\t\t    parse_cache_level),\n\tOPT_SET_UINT(0, \"per-core\", &perf_stat.aggr_mode,\n\t\t     \"aggregate counts per physical processor core\", AGGR_CORE),\n\tOPT_SET_UINT(0, \"per-node\", &perf_stat.aggr_mode,\n\t\t     \"aggregate counts per numa node\", AGGR_NODE),\n\tOPT_SET_UINT('A', \"no-aggr\", &perf_stat.aggr_mode,\n\t\t     \"disable CPU count aggregation\", AGGR_NONE),\n\tOPT_END()\n\t};\n\tstruct stat st;\n\tint ret;\n\n\targc = parse_options(argc, argv, options, stat_report_usage, 0);\n\n\tif (!input_name || !strlen(input_name)) {\n\t\tif (!fstat(STDIN_FILENO, &st) && S_ISFIFO(st.st_mode))\n\t\t\tinput_name = \"-\";\n\t\telse\n\t\t\tinput_name = \"perf.data\";\n\t}\n\n\tperf_stat.data.path = input_name;\n\tperf_stat.data.mode = PERF_DATA_MODE_READ;\n\n\tsession = perf_session__new(&perf_stat.data, &perf_stat.tool);\n\tif (IS_ERR(session))\n\t\treturn PTR_ERR(session);\n\n\tperf_stat.session  = session;\n\tstat_config.output = stderr;\n\tevlist__delete(evsel_list);\n\tevsel_list         = session->evlist;\n\n\tret = perf_session__process_events(session);\n\tif (ret)\n\t\treturn ret;\n\n\tperf_session__delete(session);\n\treturn 0;\n}\n\nstatic void setup_system_wide(int forks)\n{\n\t \n\tif (!target__none(&target))\n\t\treturn;\n\n\tif (!forks)\n\t\ttarget.system_wide = true;\n\telse {\n\t\tstruct evsel *counter;\n\n\t\tevlist__for_each_entry(evsel_list, counter) {\n\t\t\tif (!counter->core.requires_cpu &&\n\t\t\t    !evsel__name_is(counter, \"duration_time\")) {\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tif (evsel_list->core.nr_entries)\n\t\t\ttarget.system_wide = true;\n\t}\n}\n\nint cmd_stat(int argc, const char **argv)\n{\n\tconst char * const stat_usage[] = {\n\t\t\"perf stat [<options>] [<command>]\",\n\t\tNULL\n\t};\n\tint status = -EINVAL, run_idx, err;\n\tconst char *mode;\n\tFILE *output = stderr;\n\tunsigned int interval, timeout;\n\tconst char * const stat_subcommands[] = { \"record\", \"report\" };\n\tchar errbuf[BUFSIZ];\n\n\tsetlocale(LC_ALL, \"\");\n\n\tevsel_list = evlist__new();\n\tif (evsel_list == NULL)\n\t\treturn -ENOMEM;\n\n\tparse_events__shrink_config_terms();\n\n\t \n\tset_option_flag(stat_options, 'e', \"event\", PARSE_OPT_NONEG);\n\tset_option_flag(stat_options, 'M', \"metrics\", PARSE_OPT_NONEG);\n\tset_option_flag(stat_options, 'G', \"cgroup\", PARSE_OPT_NONEG);\n\n\targc = parse_options_subcommand(argc, argv, stat_options, stat_subcommands,\n\t\t\t\t\t(const char **) stat_usage,\n\t\t\t\t\tPARSE_OPT_STOP_AT_NON_OPTION);\n\n\tif (stat_config.csv_sep) {\n\t\tstat_config.csv_output = true;\n\t\tif (!strcmp(stat_config.csv_sep, \"\\\\t\"))\n\t\t\tstat_config.csv_sep = \"\\t\";\n\t} else\n\t\tstat_config.csv_sep = DEFAULT_SEPARATOR;\n\n\tif (argc && strlen(argv[0]) > 2 && strstarts(\"record\", argv[0])) {\n\t\targc = __cmd_record(argc, argv);\n\t\tif (argc < 0)\n\t\t\treturn -1;\n\t} else if (argc && strlen(argv[0]) > 2 && strstarts(\"report\", argv[0]))\n\t\treturn __cmd_report(argc, argv);\n\n\tinterval = stat_config.interval;\n\ttimeout = stat_config.timeout;\n\n\t \n\tif (!STAT_RECORD && output_name && strcmp(output_name, \"-\"))\n\t\toutput = NULL;\n\n\tif (output_name && output_fd) {\n\t\tfprintf(stderr, \"cannot use both --output and --log-fd\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"o\", 1);\n\t\tparse_options_usage(NULL, stat_options, \"log-fd\", 0);\n\t\tgoto out;\n\t}\n\n\tif (stat_config.metric_only && stat_config.aggr_mode == AGGR_THREAD) {\n\t\tfprintf(stderr, \"--metric-only is not supported with --per-thread\\n\");\n\t\tgoto out;\n\t}\n\n\tif (stat_config.metric_only && stat_config.run_count > 1) {\n\t\tfprintf(stderr, \"--metric-only is not supported with -r\\n\");\n\t\tgoto out;\n\t}\n\n\tif (stat_config.walltime_run_table && stat_config.run_count <= 1) {\n\t\tfprintf(stderr, \"--table is only supported with -r\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"r\", 1);\n\t\tparse_options_usage(NULL, stat_options, \"table\", 0);\n\t\tgoto out;\n\t}\n\n\tif (output_fd < 0) {\n\t\tfprintf(stderr, \"argument to --log-fd must be a > 0\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"log-fd\", 0);\n\t\tgoto out;\n\t}\n\n\tif (!output && !quiet) {\n\t\tstruct timespec tm;\n\t\tmode = append_file ? \"a\" : \"w\";\n\n\t\toutput = fopen(output_name, mode);\n\t\tif (!output) {\n\t\t\tperror(\"failed to create output file\");\n\t\t\treturn -1;\n\t\t}\n\t\tif (!stat_config.json_output) {\n\t\t\tclock_gettime(CLOCK_REALTIME, &tm);\n\t\t\tfprintf(output, \"# started on %s\\n\", ctime(&tm.tv_sec));\n\t\t}\n\t} else if (output_fd > 0) {\n\t\tmode = append_file ? \"a\" : \"w\";\n\t\toutput = fdopen(output_fd, mode);\n\t\tif (!output) {\n\t\t\tperror(\"Failed opening logfd\");\n\t\t\treturn -errno;\n\t\t}\n\t}\n\n\tif (stat_config.interval_clear && !isatty(fileno(output))) {\n\t\tfprintf(stderr, \"--interval-clear does not work with output\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"o\", 1);\n\t\tparse_options_usage(NULL, stat_options, \"log-fd\", 0);\n\t\tparse_options_usage(NULL, stat_options, \"interval-clear\", 0);\n\t\treturn -1;\n\t}\n\n\tstat_config.output = output;\n\n\t \n\tif (stat_config.csv_output) {\n\t\t \n\t\tif (big_num_opt == 1) {\n\t\t\tfprintf(stderr, \"-B option not supported with -x\\n\");\n\t\t\tparse_options_usage(stat_usage, stat_options, \"B\", 1);\n\t\t\tparse_options_usage(NULL, stat_options, \"x\", 1);\n\t\t\tgoto out;\n\t\t} else  \n\t\t\tstat_config.big_num = false;\n\t} else if (big_num_opt == 0)  \n\t\tstat_config.big_num = false;\n\n\terr = target__validate(&target);\n\tif (err) {\n\t\ttarget__strerror(&target, err, errbuf, BUFSIZ);\n\t\tpr_warning(\"%s\\n\", errbuf);\n\t}\n\n\tsetup_system_wide(argc);\n\n\t \n\tif ((stat_config.run_count == 1) && target__none(&target))\n\t\tstat_config.ru_display = true;\n\n\tif (stat_config.run_count < 0) {\n\t\tpr_err(\"Run count must be a positive number\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"r\", 1);\n\t\tgoto out;\n\t} else if (stat_config.run_count == 0) {\n\t\tforever = true;\n\t\tstat_config.run_count = 1;\n\t}\n\n\tif (stat_config.walltime_run_table) {\n\t\tstat_config.walltime_run = zalloc(stat_config.run_count * sizeof(stat_config.walltime_run[0]));\n\t\tif (!stat_config.walltime_run) {\n\t\t\tpr_err(\"failed to setup -r option\");\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif ((stat_config.aggr_mode == AGGR_THREAD) &&\n\t\t!target__has_task(&target)) {\n\t\tif (!target.system_wide || target.cpu_list) {\n\t\t\tfprintf(stderr, \"The --per-thread option is only \"\n\t\t\t\t\"available when monitoring via -p -t -a \"\n\t\t\t\t\"options or only --per-thread.\\n\");\n\t\t\tparse_options_usage(NULL, stat_options, \"p\", 1);\n\t\t\tparse_options_usage(NULL, stat_options, \"t\", 1);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif (((stat_config.aggr_mode != AGGR_GLOBAL &&\n\t      stat_config.aggr_mode != AGGR_THREAD) ||\n\t     (nr_cgroups || stat_config.cgroup_list)) &&\n\t    !target__has_cpu(&target)) {\n\t\tfprintf(stderr, \"both cgroup and no-aggregation \"\n\t\t\t\"modes only available in system-wide mode\\n\");\n\n\t\tparse_options_usage(stat_usage, stat_options, \"G\", 1);\n\t\tparse_options_usage(NULL, stat_options, \"A\", 1);\n\t\tparse_options_usage(NULL, stat_options, \"a\", 1);\n\t\tparse_options_usage(NULL, stat_options, \"for-each-cgroup\", 0);\n\t\tgoto out;\n\t}\n\n\tif (stat_config.iostat_run) {\n\t\tstatus = iostat_prepare(evsel_list, &stat_config);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tif (iostat_mode == IOSTAT_LIST) {\n\t\t\tiostat_list(evsel_list, &stat_config);\n\t\t\tgoto out;\n\t\t} else if (verbose > 0)\n\t\t\tiostat_list(evsel_list, &stat_config);\n\t\tif (iostat_mode == IOSTAT_RUN && !target__has_cpu(&target))\n\t\t\ttarget.system_wide = true;\n\t}\n\n\tif ((stat_config.aggr_mode == AGGR_THREAD) && (target.system_wide))\n\t\ttarget.per_thread = true;\n\n\tstat_config.system_wide = target.system_wide;\n\tif (target.cpu_list) {\n\t\tstat_config.user_requested_cpu_list = strdup(target.cpu_list);\n\t\tif (!stat_config.user_requested_cpu_list) {\n\t\t\tstatus = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif (metrics) {\n\t\tconst char *pmu = parse_events_option_args.pmu_filter ?: \"all\";\n\t\tint ret = metricgroup__parse_groups(evsel_list, pmu, metrics,\n\t\t\t\t\t\tstat_config.metric_no_group,\n\t\t\t\t\t\tstat_config.metric_no_merge,\n\t\t\t\t\t\tstat_config.metric_no_threshold,\n\t\t\t\t\t\tstat_config.user_requested_cpu_list,\n\t\t\t\t\t\tstat_config.system_wide,\n\t\t\t\t\t\t&stat_config.metric_events);\n\n\t\tzfree(&metrics);\n\t\tif (ret) {\n\t\t\tstatus = ret;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (add_default_attributes())\n\t\tgoto out;\n\n\tif (stat_config.cgroup_list) {\n\t\tif (nr_cgroups > 0) {\n\t\t\tpr_err(\"--cgroup and --for-each-cgroup cannot be used together\\n\");\n\t\t\tparse_options_usage(stat_usage, stat_options, \"G\", 1);\n\t\t\tparse_options_usage(NULL, stat_options, \"for-each-cgroup\", 0);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (evlist__expand_cgroup(evsel_list, stat_config.cgroup_list,\n\t\t\t\t\t  &stat_config.metric_events, true) < 0) {\n\t\t\tparse_options_usage(stat_usage, stat_options,\n\t\t\t\t\t    \"for-each-cgroup\", 0);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tevlist__warn_user_requested_cpus(evsel_list, target.cpu_list);\n\n\tif (evlist__create_maps(evsel_list, &target) < 0) {\n\t\tif (target__has_task(&target)) {\n\t\t\tpr_err(\"Problems finding threads of monitor\\n\");\n\t\t\tparse_options_usage(stat_usage, stat_options, \"p\", 1);\n\t\t\tparse_options_usage(NULL, stat_options, \"t\", 1);\n\t\t} else if (target__has_cpu(&target)) {\n\t\t\tperror(\"failed to parse CPUs map\");\n\t\t\tparse_options_usage(stat_usage, stat_options, \"C\", 1);\n\t\t\tparse_options_usage(NULL, stat_options, \"a\", 1);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tevlist__check_cpu_maps(evsel_list);\n\n\t \n\tif (stat_config.aggr_mode == AGGR_THREAD) {\n\t\tthread_map__read_comms(evsel_list->core.threads);\n\t}\n\n\tif (stat_config.aggr_mode == AGGR_NODE)\n\t\tcpu__setup_cpunode_map();\n\n\tif (stat_config.times && interval)\n\t\tinterval_count = true;\n\telse if (stat_config.times && !interval) {\n\t\tpr_err(\"interval-count option should be used together with \"\n\t\t\t\t\"interval-print.\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"interval-count\", 0);\n\t\tparse_options_usage(stat_usage, stat_options, \"I\", 1);\n\t\tgoto out;\n\t}\n\n\tif (timeout && timeout < 100) {\n\t\tif (timeout < 10) {\n\t\t\tpr_err(\"timeout must be >= 10ms.\\n\");\n\t\t\tparse_options_usage(stat_usage, stat_options, \"timeout\", 0);\n\t\t\tgoto out;\n\t\t} else\n\t\t\tpr_warning(\"timeout < 100ms. \"\n\t\t\t\t   \"The overhead percentage could be high in some cases. \"\n\t\t\t\t   \"Please proceed with caution.\\n\");\n\t}\n\tif (timeout && interval) {\n\t\tpr_err(\"timeout option is not supported with interval-print.\\n\");\n\t\tparse_options_usage(stat_usage, stat_options, \"timeout\", 0);\n\t\tparse_options_usage(stat_usage, stat_options, \"I\", 1);\n\t\tgoto out;\n\t}\n\n\tif (perf_stat_init_aggr_mode())\n\t\tgoto out;\n\n\tif (evlist__alloc_stats(&stat_config, evsel_list, interval))\n\t\tgoto out;\n\n\t \n\tstat_config.identifier = !(STAT_RECORD && perf_stat.data.is_pipe);\n\n\t \n\tatexit(sig_atexit);\n\tif (!forever)\n\t\tsignal(SIGINT,  skip_signal);\n\tsignal(SIGCHLD, skip_signal);\n\tsignal(SIGALRM, skip_signal);\n\tsignal(SIGABRT, skip_signal);\n\n\tif (evlist__initialize_ctlfd(evsel_list, stat_config.ctl_fd, stat_config.ctl_fd_ack))\n\t\tgoto out;\n\n\t \n\tevlist__first(evsel_list)->ignore_missing_thread = target.pid;\n\tstatus = 0;\n\tfor (run_idx = 0; forever || run_idx < stat_config.run_count; run_idx++) {\n\t\tif (stat_config.run_count != 1 && verbose > 0)\n\t\t\tfprintf(output, \"[ perf stat: executing run #%d ... ]\\n\",\n\t\t\t\trun_idx + 1);\n\n\t\tif (run_idx != 0)\n\t\t\tevlist__reset_prev_raw_counts(evsel_list);\n\n\t\tstatus = run_perf_stat(argc, argv, run_idx);\n\t\tif (forever && status != -1 && !interval) {\n\t\t\tprint_counters(NULL, argc, argv);\n\t\t\tperf_stat__reset_stats();\n\t\t}\n\t}\n\n\tif (!forever && status != -1 && (!interval || stat_config.summary)) {\n\t\tif (stat_config.run_count > 1)\n\t\t\tevlist__copy_res_stats(&stat_config, evsel_list);\n\t\tprint_counters(NULL, argc, argv);\n\t}\n\n\tevlist__finalize_ctlfd(evsel_list);\n\n\tif (STAT_RECORD) {\n\t\t \n\t\tint fd = perf_data__fd(&perf_stat.data);\n\n\t\terr = perf_event__synthesize_kernel_mmap((void *)&perf_stat,\n\t\t\t\t\t\t\t process_synthesized_event,\n\t\t\t\t\t\t\t &perf_stat.session->machines.host);\n\t\tif (err) {\n\t\t\tpr_warning(\"Couldn't synthesize the kernel mmap record, harmless, \"\n\t\t\t\t   \"older tools may produce warnings about this file\\n.\");\n\t\t}\n\n\t\tif (!interval) {\n\t\t\tif (WRITE_STAT_ROUND_EVENT(walltime_nsecs_stats.max, FINAL))\n\t\t\t\tpr_err(\"failed to write stat round event\\n\");\n\t\t}\n\n\t\tif (!perf_stat.data.is_pipe) {\n\t\t\tperf_stat.session->header.data_size += perf_stat.bytes_written;\n\t\t\tperf_session__write_header(perf_stat.session, evsel_list, fd, true);\n\t\t}\n\n\t\tevlist__close(evsel_list);\n\t\tperf_session__delete(perf_stat.session);\n\t}\n\n\tperf_stat__exit_aggr_mode();\n\tevlist__free_stats(evsel_list);\nout:\n\tif (stat_config.iostat_run)\n\t\tiostat_release(evsel_list);\n\n\tzfree(&stat_config.walltime_run);\n\tzfree(&stat_config.user_requested_cpu_list);\n\n\tif (smi_cost && smi_reset)\n\t\tsysfs__write_int(FREEZE_ON_SMI_PATH, 0);\n\n\tevlist__delete(evsel_list);\n\n\tmetricgroup__rblist_exit(&stat_config.metric_events);\n\tevlist__close_control(stat_config.ctl_fd, stat_config.ctl_fd_ack, &stat_config.ctl_fd_close);\n\n\treturn status;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}