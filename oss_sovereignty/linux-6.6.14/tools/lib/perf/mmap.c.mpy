{
  "module_name": "mmap.c",
  "hash_id": "c6edaa86060321617396e5afeef37dc28d0dd997c94c5dc3f95effcc5f55801a",
  "original_prompt": "Ingested from linux-6.6.14/tools/lib/perf/mmap.c",
  "human_readable_source": "\n#include <sys/mman.h>\n#include <inttypes.h>\n#include <asm/bug.h>\n#include <errno.h>\n#include <string.h>\n#include <linux/ring_buffer.h>\n#include <linux/perf_event.h>\n#include <perf/mmap.h>\n#include <perf/event.h>\n#include <perf/evsel.h>\n#include <internal/mmap.h>\n#include <internal/lib.h>\n#include <linux/kernel.h>\n#include <linux/math64.h>\n#include <linux/stringify.h>\n#include \"internal.h\"\n\nvoid perf_mmap__init(struct perf_mmap *map, struct perf_mmap *prev,\n\t\t     bool overwrite, libperf_unmap_cb_t unmap_cb)\n{\n\tmap->fd = -1;\n\tmap->overwrite = overwrite;\n\tmap->unmap_cb  = unmap_cb;\n\trefcount_set(&map->refcnt, 0);\n\tif (prev)\n\t\tprev->next = map;\n}\n\nsize_t perf_mmap__mmap_len(struct perf_mmap *map)\n{\n\treturn map->mask + 1 + page_size;\n}\n\nint perf_mmap__mmap(struct perf_mmap *map, struct perf_mmap_param *mp,\n\t\t    int fd, struct perf_cpu cpu)\n{\n\tmap->prev = 0;\n\tmap->mask = mp->mask;\n\tmap->base = mmap(NULL, perf_mmap__mmap_len(map), mp->prot,\n\t\t\t MAP_SHARED, fd, 0);\n\tif (map->base == MAP_FAILED) {\n\t\tmap->base = NULL;\n\t\treturn -1;\n\t}\n\n\tmap->fd  = fd;\n\tmap->cpu = cpu;\n\treturn 0;\n}\n\nvoid perf_mmap__munmap(struct perf_mmap *map)\n{\n\tif (map && map->base != NULL) {\n\t\tmunmap(map->base, perf_mmap__mmap_len(map));\n\t\tmap->base = NULL;\n\t\tmap->fd = -1;\n\t\trefcount_set(&map->refcnt, 0);\n\t}\n\tif (map && map->unmap_cb)\n\t\tmap->unmap_cb(map);\n}\n\nvoid perf_mmap__get(struct perf_mmap *map)\n{\n\trefcount_inc(&map->refcnt);\n}\n\nvoid perf_mmap__put(struct perf_mmap *map)\n{\n\tBUG_ON(map->base && refcount_read(&map->refcnt) == 0);\n\n\tif (refcount_dec_and_test(&map->refcnt))\n\t\tperf_mmap__munmap(map);\n}\n\nstatic inline void perf_mmap__write_tail(struct perf_mmap *md, u64 tail)\n{\n\tring_buffer_write_tail(md->base, tail);\n}\n\nu64 perf_mmap__read_head(struct perf_mmap *map)\n{\n\treturn ring_buffer_read_head(map->base);\n}\n\nstatic bool perf_mmap__empty(struct perf_mmap *map)\n{\n\tstruct perf_event_mmap_page *pc = map->base;\n\n\treturn perf_mmap__read_head(map) == map->prev && !pc->aux_size;\n}\n\nvoid perf_mmap__consume(struct perf_mmap *map)\n{\n\tif (!map->overwrite) {\n\t\tu64 old = map->prev;\n\n\t\tperf_mmap__write_tail(map, old);\n\t}\n\n\tif (refcount_read(&map->refcnt) == 1 && perf_mmap__empty(map))\n\t\tperf_mmap__put(map);\n}\n\nstatic int overwrite_rb_find_range(void *buf, int mask, u64 *start, u64 *end)\n{\n\tstruct perf_event_header *pheader;\n\tu64 evt_head = *start;\n\tint size = mask + 1;\n\n\tpr_debug2(\"%s: buf=%p, start=%\"PRIx64\"\\n\", __func__, buf, *start);\n\tpheader = (struct perf_event_header *)(buf + (*start & mask));\n\twhile (true) {\n\t\tif (evt_head - *start >= (unsigned int)size) {\n\t\t\tpr_debug(\"Finished reading overwrite ring buffer: rewind\\n\");\n\t\t\tif (evt_head - *start > (unsigned int)size)\n\t\t\t\tevt_head -= pheader->size;\n\t\t\t*end = evt_head;\n\t\t\treturn 0;\n\t\t}\n\n\t\tpheader = (struct perf_event_header *)(buf + (evt_head & mask));\n\n\t\tif (pheader->size == 0) {\n\t\t\tpr_debug(\"Finished reading overwrite ring buffer: get start\\n\");\n\t\t\t*end = evt_head;\n\t\t\treturn 0;\n\t\t}\n\n\t\tevt_head += pheader->size;\n\t\tpr_debug3(\"move evt_head: %\"PRIx64\"\\n\", evt_head);\n\t}\n\tWARN_ONCE(1, \"Shouldn't get here\\n\");\n\treturn -1;\n}\n\n \nstatic int __perf_mmap__read_init(struct perf_mmap *md)\n{\n\tu64 head = perf_mmap__read_head(md);\n\tu64 old = md->prev;\n\tunsigned char *data = md->base + page_size;\n\tunsigned long size;\n\n\tmd->start = md->overwrite ? head : old;\n\tmd->end = md->overwrite ? old : head;\n\n\tif ((md->end - md->start) < md->flush)\n\t\treturn -EAGAIN;\n\n\tsize = md->end - md->start;\n\tif (size > (unsigned long)(md->mask) + 1) {\n\t\tif (!md->overwrite) {\n\t\t\tWARN_ONCE(1, \"failed to keep up with mmap data. (warn only once)\\n\");\n\n\t\t\tmd->prev = head;\n\t\t\tperf_mmap__consume(md);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t \n\t\tif (overwrite_rb_find_range(data, md->mask, &md->start, &md->end))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint perf_mmap__read_init(struct perf_mmap *map)\n{\n\t \n\tif (!refcount_read(&map->refcnt))\n\t\treturn -ENOENT;\n\n\treturn __perf_mmap__read_init(map);\n}\n\n \nvoid perf_mmap__read_done(struct perf_mmap *map)\n{\n\t \n\tif (!refcount_read(&map->refcnt))\n\t\treturn;\n\n\tmap->prev = perf_mmap__read_head(map);\n}\n\n \nstatic union perf_event *perf_mmap__read(struct perf_mmap *map,\n\t\t\t\t\t u64 *startp, u64 end)\n{\n\tunsigned char *data = map->base + page_size;\n\tunion perf_event *event = NULL;\n\tint diff = end - *startp;\n\n\tif (diff >= (int)sizeof(event->header)) {\n\t\tsize_t size;\n\n\t\tevent = (union perf_event *)&data[*startp & map->mask];\n\t\tsize = event->header.size;\n\n\t\tif (size < sizeof(event->header) || diff < (int)size)\n\t\t\treturn NULL;\n\n\t\t \n\t\tif ((*startp & map->mask) + size != ((*startp + size) & map->mask)) {\n\t\t\tunsigned int offset = *startp;\n\t\t\tunsigned int len = min(sizeof(*event), size), cpy;\n\t\t\tvoid *dst = map->event_copy;\n\n\t\t\tdo {\n\t\t\t\tcpy = min(map->mask + 1 - (offset & map->mask), len);\n\t\t\t\tmemcpy(dst, &data[offset & map->mask], cpy);\n\t\t\t\toffset += cpy;\n\t\t\t\tdst += cpy;\n\t\t\t\tlen -= cpy;\n\t\t\t} while (len);\n\n\t\t\tevent = (union perf_event *)map->event_copy;\n\t\t}\n\n\t\t*startp += size;\n\t}\n\n\treturn event;\n}\n\n \nunion perf_event *perf_mmap__read_event(struct perf_mmap *map)\n{\n\tunion perf_event *event;\n\n\t \n\tif (!refcount_read(&map->refcnt))\n\t\treturn NULL;\n\n\t \n\tif (!map->overwrite)\n\t\tmap->end = perf_mmap__read_head(map);\n\n\tevent = perf_mmap__read(map, &map->start, map->end);\n\n\tif (!map->overwrite)\n\t\tmap->prev = map->start;\n\n\treturn event;\n}\n\n#if defined(__i386__) || defined(__x86_64__)\nstatic u64 read_perf_counter(unsigned int counter)\n{\n\tunsigned int low, high;\n\n\tasm volatile(\"rdpmc\" : \"=a\" (low), \"=d\" (high) : \"c\" (counter));\n\n\treturn low | ((u64)high) << 32;\n}\n\nstatic u64 read_timestamp(void)\n{\n\tunsigned int low, high;\n\n\tasm volatile(\"rdtsc\" : \"=a\" (low), \"=d\" (high));\n\n\treturn low | ((u64)high) << 32;\n}\n#elif defined(__aarch64__)\n#define read_sysreg(r) ({\t\t\t\t\t\t\\\n\tu64 __val;\t\t\t\t\t\t\t\\\n\tasm volatile(\"mrs %0, \" __stringify(r) : \"=r\" (__val));\t\t\\\n\t__val;\t\t\t\t\t\t\t\t\\\n})\n\nstatic u64 read_pmccntr(void)\n{\n\treturn read_sysreg(pmccntr_el0);\n}\n\n#define PMEVCNTR_READ(idx)\t\t\t\t\t\\\n\tstatic u64 read_pmevcntr_##idx(void) {\t\t\t\\\n\t\treturn read_sysreg(pmevcntr##idx##_el0);\t\\\n\t}\n\nPMEVCNTR_READ(0);\nPMEVCNTR_READ(1);\nPMEVCNTR_READ(2);\nPMEVCNTR_READ(3);\nPMEVCNTR_READ(4);\nPMEVCNTR_READ(5);\nPMEVCNTR_READ(6);\nPMEVCNTR_READ(7);\nPMEVCNTR_READ(8);\nPMEVCNTR_READ(9);\nPMEVCNTR_READ(10);\nPMEVCNTR_READ(11);\nPMEVCNTR_READ(12);\nPMEVCNTR_READ(13);\nPMEVCNTR_READ(14);\nPMEVCNTR_READ(15);\nPMEVCNTR_READ(16);\nPMEVCNTR_READ(17);\nPMEVCNTR_READ(18);\nPMEVCNTR_READ(19);\nPMEVCNTR_READ(20);\nPMEVCNTR_READ(21);\nPMEVCNTR_READ(22);\nPMEVCNTR_READ(23);\nPMEVCNTR_READ(24);\nPMEVCNTR_READ(25);\nPMEVCNTR_READ(26);\nPMEVCNTR_READ(27);\nPMEVCNTR_READ(28);\nPMEVCNTR_READ(29);\nPMEVCNTR_READ(30);\n\n \nstatic u64 read_perf_counter(unsigned int counter)\n{\n\tstatic u64 (* const read_f[])(void) = {\n\t\tread_pmevcntr_0,\n\t\tread_pmevcntr_1,\n\t\tread_pmevcntr_2,\n\t\tread_pmevcntr_3,\n\t\tread_pmevcntr_4,\n\t\tread_pmevcntr_5,\n\t\tread_pmevcntr_6,\n\t\tread_pmevcntr_7,\n\t\tread_pmevcntr_8,\n\t\tread_pmevcntr_9,\n\t\tread_pmevcntr_10,\n\t\tread_pmevcntr_11,\n\t\tread_pmevcntr_13,\n\t\tread_pmevcntr_12,\n\t\tread_pmevcntr_14,\n\t\tread_pmevcntr_15,\n\t\tread_pmevcntr_16,\n\t\tread_pmevcntr_17,\n\t\tread_pmevcntr_18,\n\t\tread_pmevcntr_19,\n\t\tread_pmevcntr_20,\n\t\tread_pmevcntr_21,\n\t\tread_pmevcntr_22,\n\t\tread_pmevcntr_23,\n\t\tread_pmevcntr_24,\n\t\tread_pmevcntr_25,\n\t\tread_pmevcntr_26,\n\t\tread_pmevcntr_27,\n\t\tread_pmevcntr_28,\n\t\tread_pmevcntr_29,\n\t\tread_pmevcntr_30,\n\t\tread_pmccntr\n\t};\n\n\tif (counter < ARRAY_SIZE(read_f))\n\t\treturn (read_f[counter])();\n\n\treturn 0;\n}\n\nstatic u64 read_timestamp(void) { return read_sysreg(cntvct_el0); }\n\n \n#elif defined(__riscv) && __riscv_xlen == 64\n\n \n\n#define CSR_CYCLE\t0xc00\n#define CSR_TIME\t0xc01\n\n#define csr_read(csr)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tregister unsigned long __v;\t\t\t\t\\\n\t\t__asm__ __volatile__ (\"csrr %0, %1\"\t\t\\\n\t\t : \"=r\" (__v)\t\t\t\t\t\\\n\t\t : \"i\" (csr) : );\t\t\t\t\\\n\t\t __v;\t\t\t\t\t\t\\\n})\n\nstatic unsigned long csr_read_num(int csr_num)\n{\n#define switchcase_csr_read(__csr_num, __val)           {\\\n\tcase __csr_num:                                 \\\n\t\t__val = csr_read(__csr_num);            \\\n\t\tbreak; }\n#define switchcase_csr_read_2(__csr_num, __val)         {\\\n\tswitchcase_csr_read(__csr_num + 0, __val)        \\\n\tswitchcase_csr_read(__csr_num + 1, __val)}\n#define switchcase_csr_read_4(__csr_num, __val)         {\\\n\tswitchcase_csr_read_2(__csr_num + 0, __val)      \\\n\tswitchcase_csr_read_2(__csr_num + 2, __val)}\n#define switchcase_csr_read_8(__csr_num, __val)         {\\\n\tswitchcase_csr_read_4(__csr_num + 0, __val)      \\\n\tswitchcase_csr_read_4(__csr_num + 4, __val)}\n#define switchcase_csr_read_16(__csr_num, __val)        {\\\n\tswitchcase_csr_read_8(__csr_num + 0, __val)      \\\n\tswitchcase_csr_read_8(__csr_num + 8, __val)}\n#define switchcase_csr_read_32(__csr_num, __val)        {\\\n\tswitchcase_csr_read_16(__csr_num + 0, __val)     \\\n\tswitchcase_csr_read_16(__csr_num + 16, __val)}\n\n\tunsigned long ret = 0;\n\n\tswitch (csr_num) {\n\tswitchcase_csr_read_32(CSR_CYCLE, ret)\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n#undef switchcase_csr_read_32\n#undef switchcase_csr_read_16\n#undef switchcase_csr_read_8\n#undef switchcase_csr_read_4\n#undef switchcase_csr_read_2\n#undef switchcase_csr_read\n}\n\nstatic u64 read_perf_counter(unsigned int counter)\n{\n\treturn csr_read_num(CSR_CYCLE + counter);\n}\n\nstatic u64 read_timestamp(void)\n{\n\treturn csr_read_num(CSR_TIME);\n}\n\n#else\nstatic u64 read_perf_counter(unsigned int counter __maybe_unused) { return 0; }\nstatic u64 read_timestamp(void) { return 0; }\n#endif\n\nint perf_mmap__read_self(struct perf_mmap *map, struct perf_counts_values *count)\n{\n\tstruct perf_event_mmap_page *pc = map->base;\n\tu32 seq, idx, time_mult = 0, time_shift = 0;\n\tu64 cnt, cyc = 0, time_offset = 0, time_cycles = 0, time_mask = ~0ULL;\n\n\tif (!pc || !pc->cap_user_rdpmc)\n\t\treturn -1;\n\n\tdo {\n\t\tseq = READ_ONCE(pc->lock);\n\t\tbarrier();\n\n\t\tcount->ena = READ_ONCE(pc->time_enabled);\n\t\tcount->run = READ_ONCE(pc->time_running);\n\n\t\tif (pc->cap_user_time && count->ena != count->run) {\n\t\t\tcyc = read_timestamp();\n\t\t\ttime_mult = READ_ONCE(pc->time_mult);\n\t\t\ttime_shift = READ_ONCE(pc->time_shift);\n\t\t\ttime_offset = READ_ONCE(pc->time_offset);\n\n\t\t\tif (pc->cap_user_time_short) {\n\t\t\t\ttime_cycles = READ_ONCE(pc->time_cycles);\n\t\t\t\ttime_mask = READ_ONCE(pc->time_mask);\n\t\t\t}\n\t\t}\n\n\t\tidx = READ_ONCE(pc->index);\n\t\tcnt = READ_ONCE(pc->offset);\n\t\tif (pc->cap_user_rdpmc && idx) {\n\t\t\ts64 evcnt = read_perf_counter(idx - 1);\n\t\t\tu16 width = READ_ONCE(pc->pmc_width);\n\n\t\t\tevcnt <<= 64 - width;\n\t\t\tevcnt >>= 64 - width;\n\t\t\tcnt += evcnt;\n\t\t} else\n\t\t\treturn -1;\n\n\t\tbarrier();\n\t} while (READ_ONCE(pc->lock) != seq);\n\n\tif (count->ena != count->run) {\n\t\tu64 delta;\n\n\t\t \n\t\tcyc = time_cycles + ((cyc - time_cycles) & time_mask);\n\n\t\tdelta = time_offset + mul_u64_u32_shr(cyc, time_mult, time_shift);\n\n\t\tcount->ena += delta;\n\t\tif (idx)\n\t\t\tcount->run += delta;\n\t}\n\n\tcount->val = cnt;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}