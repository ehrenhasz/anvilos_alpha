{
  "module_name": "cpumap.c",
  "hash_id": "18cbe064b16ac78a09693e055d6a29de1ec7db8753dc705e0ba8fcc7a6278313",
  "original_prompt": "Ingested from linux-6.6.14/tools/lib/perf/cpumap.c",
  "human_readable_source": "\n#include <perf/cpumap.h>\n#include <stdlib.h>\n#include <linux/refcount.h>\n#include <internal/cpumap.h>\n#include <asm/bug.h>\n#include <stdio.h>\n#include <string.h>\n#include <unistd.h>\n#include <ctype.h>\n#include <limits.h>\n\nvoid perf_cpu_map__set_nr(struct perf_cpu_map *map, int nr_cpus)\n{\n\tRC_CHK_ACCESS(map)->nr = nr_cpus;\n}\n\nstruct perf_cpu_map *perf_cpu_map__alloc(int nr_cpus)\n{\n\tRC_STRUCT(perf_cpu_map) *cpus = malloc(sizeof(*cpus) + sizeof(struct perf_cpu) * nr_cpus);\n\tstruct perf_cpu_map *result;\n\n\tif (ADD_RC_CHK(result, cpus)) {\n\t\tcpus->nr = nr_cpus;\n\t\trefcount_set(&cpus->refcnt, 1);\n\t}\n\treturn result;\n}\n\nstruct perf_cpu_map *perf_cpu_map__dummy_new(void)\n{\n\tstruct perf_cpu_map *cpus = perf_cpu_map__alloc(1);\n\n\tif (cpus)\n\t\tRC_CHK_ACCESS(cpus)->map[0].cpu = -1;\n\n\treturn cpus;\n}\n\nstatic void cpu_map__delete(struct perf_cpu_map *map)\n{\n\tif (map) {\n\t\tWARN_ONCE(refcount_read(perf_cpu_map__refcnt(map)) != 0,\n\t\t\t  \"cpu_map refcnt unbalanced\\n\");\n\t\tRC_CHK_FREE(map);\n\t}\n}\n\nstruct perf_cpu_map *perf_cpu_map__get(struct perf_cpu_map *map)\n{\n\tstruct perf_cpu_map *result;\n\n\tif (RC_CHK_GET(result, map))\n\t\trefcount_inc(perf_cpu_map__refcnt(map));\n\n\treturn result;\n}\n\nvoid perf_cpu_map__put(struct perf_cpu_map *map)\n{\n\tif (map) {\n\t\tif (refcount_dec_and_test(perf_cpu_map__refcnt(map)))\n\t\t\tcpu_map__delete(map);\n\t\telse\n\t\t\tRC_CHK_PUT(map);\n\t}\n}\n\nstatic struct perf_cpu_map *cpu_map__default_new(void)\n{\n\tstruct perf_cpu_map *cpus;\n\tint nr_cpus;\n\n\tnr_cpus = sysconf(_SC_NPROCESSORS_ONLN);\n\tif (nr_cpus < 0)\n\t\treturn NULL;\n\n\tcpus = perf_cpu_map__alloc(nr_cpus);\n\tif (cpus != NULL) {\n\t\tint i;\n\n\t\tfor (i = 0; i < nr_cpus; ++i)\n\t\t\tRC_CHK_ACCESS(cpus)->map[i].cpu = i;\n\t}\n\n\treturn cpus;\n}\n\nstruct perf_cpu_map *perf_cpu_map__default_new(void)\n{\n\treturn cpu_map__default_new();\n}\n\n\nstatic int cmp_cpu(const void *a, const void *b)\n{\n\tconst struct perf_cpu *cpu_a = a, *cpu_b = b;\n\n\treturn cpu_a->cpu - cpu_b->cpu;\n}\n\nstatic struct perf_cpu __perf_cpu_map__cpu(const struct perf_cpu_map *cpus, int idx)\n{\n\treturn RC_CHK_ACCESS(cpus)->map[idx];\n}\n\nstatic struct perf_cpu_map *cpu_map__trim_new(int nr_cpus, const struct perf_cpu *tmp_cpus)\n{\n\tsize_t payload_size = nr_cpus * sizeof(struct perf_cpu);\n\tstruct perf_cpu_map *cpus = perf_cpu_map__alloc(nr_cpus);\n\tint i, j;\n\n\tif (cpus != NULL) {\n\t\tmemcpy(RC_CHK_ACCESS(cpus)->map, tmp_cpus, payload_size);\n\t\tqsort(RC_CHK_ACCESS(cpus)->map, nr_cpus, sizeof(struct perf_cpu), cmp_cpu);\n\t\t \n\t\tj = 0;\n\t\tfor (i = 0; i < nr_cpus; i++) {\n\t\t\tif (i == 0 ||\n\t\t\t    __perf_cpu_map__cpu(cpus, i).cpu !=\n\t\t\t    __perf_cpu_map__cpu(cpus, i - 1).cpu) {\n\t\t\t\tRC_CHK_ACCESS(cpus)->map[j++].cpu =\n\t\t\t\t\t__perf_cpu_map__cpu(cpus, i).cpu;\n\t\t\t}\n\t\t}\n\t\tperf_cpu_map__set_nr(cpus, j);\n\t\tassert(j <= nr_cpus);\n\t}\n\treturn cpus;\n}\n\nstruct perf_cpu_map *perf_cpu_map__read(FILE *file)\n{\n\tstruct perf_cpu_map *cpus = NULL;\n\tint nr_cpus = 0;\n\tstruct perf_cpu *tmp_cpus = NULL, *tmp;\n\tint max_entries = 0;\n\tint n, cpu, prev;\n\tchar sep;\n\n\tsep = 0;\n\tprev = -1;\n\tfor (;;) {\n\t\tn = fscanf(file, \"%u%c\", &cpu, &sep);\n\t\tif (n <= 0)\n\t\t\tbreak;\n\t\tif (prev >= 0) {\n\t\t\tint new_max = nr_cpus + cpu - prev - 1;\n\n\t\t\tWARN_ONCE(new_max >= MAX_NR_CPUS, \"Perf can support %d CPUs. \"\n\t\t\t\t\t\t\t  \"Consider raising MAX_NR_CPUS\\n\", MAX_NR_CPUS);\n\n\t\t\tif (new_max >= max_entries) {\n\t\t\t\tmax_entries = new_max + MAX_NR_CPUS / 2;\n\t\t\t\ttmp = realloc(tmp_cpus, max_entries * sizeof(struct perf_cpu));\n\t\t\t\tif (tmp == NULL)\n\t\t\t\t\tgoto out_free_tmp;\n\t\t\t\ttmp_cpus = tmp;\n\t\t\t}\n\n\t\t\twhile (++prev < cpu)\n\t\t\t\ttmp_cpus[nr_cpus++].cpu = prev;\n\t\t}\n\t\tif (nr_cpus == max_entries) {\n\t\t\tmax_entries += MAX_NR_CPUS;\n\t\t\ttmp = realloc(tmp_cpus, max_entries * sizeof(struct perf_cpu));\n\t\t\tif (tmp == NULL)\n\t\t\t\tgoto out_free_tmp;\n\t\t\ttmp_cpus = tmp;\n\t\t}\n\n\t\ttmp_cpus[nr_cpus++].cpu = cpu;\n\t\tif (n == 2 && sep == '-')\n\t\t\tprev = cpu;\n\t\telse\n\t\t\tprev = -1;\n\t\tif (n == 1 || sep == '\\n')\n\t\t\tbreak;\n\t}\n\n\tif (nr_cpus > 0)\n\t\tcpus = cpu_map__trim_new(nr_cpus, tmp_cpus);\n\telse\n\t\tcpus = cpu_map__default_new();\nout_free_tmp:\n\tfree(tmp_cpus);\n\treturn cpus;\n}\n\nstatic struct perf_cpu_map *cpu_map__read_all_cpu_map(void)\n{\n\tstruct perf_cpu_map *cpus = NULL;\n\tFILE *onlnf;\n\n\tonlnf = fopen(\"/sys/devices/system/cpu/online\", \"r\");\n\tif (!onlnf)\n\t\treturn cpu_map__default_new();\n\n\tcpus = perf_cpu_map__read(onlnf);\n\tfclose(onlnf);\n\treturn cpus;\n}\n\nstruct perf_cpu_map *perf_cpu_map__new(const char *cpu_list)\n{\n\tstruct perf_cpu_map *cpus = NULL;\n\tunsigned long start_cpu, end_cpu = 0;\n\tchar *p = NULL;\n\tint i, nr_cpus = 0;\n\tstruct perf_cpu *tmp_cpus = NULL, *tmp;\n\tint max_entries = 0;\n\n\tif (!cpu_list)\n\t\treturn cpu_map__read_all_cpu_map();\n\n\t \n\tif (!isdigit(*cpu_list) && *cpu_list != '\\0')\n\t\tgoto out;\n\n\twhile (isdigit(*cpu_list)) {\n\t\tp = NULL;\n\t\tstart_cpu = strtoul(cpu_list, &p, 0);\n\t\tif (start_cpu >= INT_MAX\n\t\t    || (*p != '\\0' && *p != ',' && *p != '-'))\n\t\t\tgoto invalid;\n\n\t\tif (*p == '-') {\n\t\t\tcpu_list = ++p;\n\t\t\tp = NULL;\n\t\t\tend_cpu = strtoul(cpu_list, &p, 0);\n\n\t\t\tif (end_cpu >= INT_MAX || (*p != '\\0' && *p != ','))\n\t\t\t\tgoto invalid;\n\n\t\t\tif (end_cpu < start_cpu)\n\t\t\t\tgoto invalid;\n\t\t} else {\n\t\t\tend_cpu = start_cpu;\n\t\t}\n\n\t\tWARN_ONCE(end_cpu >= MAX_NR_CPUS, \"Perf can support %d CPUs. \"\n\t\t\t\t\t\t  \"Consider raising MAX_NR_CPUS\\n\", MAX_NR_CPUS);\n\n\t\tfor (; start_cpu <= end_cpu; start_cpu++) {\n\t\t\t \n\t\t\tfor (i = 0; i < nr_cpus; i++)\n\t\t\t\tif (tmp_cpus[i].cpu == (int)start_cpu)\n\t\t\t\t\tgoto invalid;\n\n\t\t\tif (nr_cpus == max_entries) {\n\t\t\t\tmax_entries += MAX_NR_CPUS;\n\t\t\t\ttmp = realloc(tmp_cpus, max_entries * sizeof(struct perf_cpu));\n\t\t\t\tif (tmp == NULL)\n\t\t\t\t\tgoto invalid;\n\t\t\t\ttmp_cpus = tmp;\n\t\t\t}\n\t\t\ttmp_cpus[nr_cpus++].cpu = (int)start_cpu;\n\t\t}\n\t\tif (*p)\n\t\t\t++p;\n\n\t\tcpu_list = p;\n\t}\n\n\tif (nr_cpus > 0)\n\t\tcpus = cpu_map__trim_new(nr_cpus, tmp_cpus);\n\telse if (*cpu_list != '\\0')\n\t\tcpus = cpu_map__default_new();\n\telse\n\t\tcpus = perf_cpu_map__dummy_new();\ninvalid:\n\tfree(tmp_cpus);\nout:\n\treturn cpus;\n}\n\nstatic int __perf_cpu_map__nr(const struct perf_cpu_map *cpus)\n{\n\treturn RC_CHK_ACCESS(cpus)->nr;\n}\n\nstruct perf_cpu perf_cpu_map__cpu(const struct perf_cpu_map *cpus, int idx)\n{\n\tstruct perf_cpu result = {\n\t\t.cpu = -1\n\t};\n\n\tif (cpus && idx < __perf_cpu_map__nr(cpus))\n\t\treturn __perf_cpu_map__cpu(cpus, idx);\n\n\treturn result;\n}\n\nint perf_cpu_map__nr(const struct perf_cpu_map *cpus)\n{\n\treturn cpus ? __perf_cpu_map__nr(cpus) : 1;\n}\n\nbool perf_cpu_map__empty(const struct perf_cpu_map *map)\n{\n\treturn map ? __perf_cpu_map__cpu(map, 0).cpu == -1 : true;\n}\n\nint perf_cpu_map__idx(const struct perf_cpu_map *cpus, struct perf_cpu cpu)\n{\n\tint low, high;\n\n\tif (!cpus)\n\t\treturn -1;\n\n\tlow = 0;\n\thigh = __perf_cpu_map__nr(cpus);\n\twhile (low < high) {\n\t\tint idx = (low + high) / 2;\n\t\tstruct perf_cpu cpu_at_idx = __perf_cpu_map__cpu(cpus, idx);\n\n\t\tif (cpu_at_idx.cpu == cpu.cpu)\n\t\t\treturn idx;\n\n\t\tif (cpu_at_idx.cpu > cpu.cpu)\n\t\t\thigh = idx;\n\t\telse\n\t\t\tlow = idx + 1;\n\t}\n\n\treturn -1;\n}\n\nbool perf_cpu_map__has(const struct perf_cpu_map *cpus, struct perf_cpu cpu)\n{\n\treturn perf_cpu_map__idx(cpus, cpu) != -1;\n}\n\nbool perf_cpu_map__equal(const struct perf_cpu_map *lhs, const struct perf_cpu_map *rhs)\n{\n\tint nr;\n\n\tif (lhs == rhs)\n\t\treturn true;\n\n\tif (!lhs || !rhs)\n\t\treturn false;\n\n\tnr = __perf_cpu_map__nr(lhs);\n\tif (nr != __perf_cpu_map__nr(rhs))\n\t\treturn false;\n\n\tfor (int idx = 0; idx < nr; idx++) {\n\t\tif (__perf_cpu_map__cpu(lhs, idx).cpu != __perf_cpu_map__cpu(rhs, idx).cpu)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nbool perf_cpu_map__has_any_cpu(const struct perf_cpu_map *map)\n{\n\treturn map && __perf_cpu_map__cpu(map, 0).cpu == -1;\n}\n\nstruct perf_cpu perf_cpu_map__max(const struct perf_cpu_map *map)\n{\n\tstruct perf_cpu result = {\n\t\t.cpu = -1\n\t};\n\n\t\n\treturn __perf_cpu_map__nr(map) > 0\n\t\t? __perf_cpu_map__cpu(map, __perf_cpu_map__nr(map) - 1)\n\t\t: result;\n}\n\n \nbool perf_cpu_map__is_subset(const struct perf_cpu_map *a, const struct perf_cpu_map *b)\n{\n\tif (a == b || !b)\n\t\treturn true;\n\tif (!a || __perf_cpu_map__nr(b) > __perf_cpu_map__nr(a))\n\t\treturn false;\n\n\tfor (int i = 0, j = 0; i < __perf_cpu_map__nr(a); i++) {\n\t\tif (__perf_cpu_map__cpu(a, i).cpu > __perf_cpu_map__cpu(b, j).cpu)\n\t\t\treturn false;\n\t\tif (__perf_cpu_map__cpu(a, i).cpu == __perf_cpu_map__cpu(b, j).cpu) {\n\t\t\tj++;\n\t\t\tif (j == __perf_cpu_map__nr(b))\n\t\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\n \n\nstruct perf_cpu_map *perf_cpu_map__merge(struct perf_cpu_map *orig,\n\t\t\t\t\t struct perf_cpu_map *other)\n{\n\tstruct perf_cpu *tmp_cpus;\n\tint tmp_len;\n\tint i, j, k;\n\tstruct perf_cpu_map *merged;\n\n\tif (perf_cpu_map__is_subset(orig, other))\n\t\treturn orig;\n\tif (perf_cpu_map__is_subset(other, orig)) {\n\t\tperf_cpu_map__put(orig);\n\t\treturn perf_cpu_map__get(other);\n\t}\n\n\ttmp_len = __perf_cpu_map__nr(orig) + __perf_cpu_map__nr(other);\n\ttmp_cpus = malloc(tmp_len * sizeof(struct perf_cpu));\n\tif (!tmp_cpus)\n\t\treturn NULL;\n\n\t \n\ti = j = k = 0;\n\twhile (i < __perf_cpu_map__nr(orig) && j < __perf_cpu_map__nr(other)) {\n\t\tif (__perf_cpu_map__cpu(orig, i).cpu <= __perf_cpu_map__cpu(other, j).cpu) {\n\t\t\tif (__perf_cpu_map__cpu(orig, i).cpu == __perf_cpu_map__cpu(other, j).cpu)\n\t\t\t\tj++;\n\t\t\ttmp_cpus[k++] = __perf_cpu_map__cpu(orig, i++);\n\t\t} else\n\t\t\ttmp_cpus[k++] = __perf_cpu_map__cpu(other, j++);\n\t}\n\n\twhile (i < __perf_cpu_map__nr(orig))\n\t\ttmp_cpus[k++] = __perf_cpu_map__cpu(orig, i++);\n\n\twhile (j < __perf_cpu_map__nr(other))\n\t\ttmp_cpus[k++] = __perf_cpu_map__cpu(other, j++);\n\tassert(k <= tmp_len);\n\n\tmerged = cpu_map__trim_new(k, tmp_cpus);\n\tfree(tmp_cpus);\n\tperf_cpu_map__put(orig);\n\treturn merged;\n}\n\nstruct perf_cpu_map *perf_cpu_map__intersect(struct perf_cpu_map *orig,\n\t\t\t\t\t     struct perf_cpu_map *other)\n{\n\tstruct perf_cpu *tmp_cpus;\n\tint tmp_len;\n\tint i, j, k;\n\tstruct perf_cpu_map *merged = NULL;\n\n\tif (perf_cpu_map__is_subset(other, orig))\n\t\treturn perf_cpu_map__get(orig);\n\tif (perf_cpu_map__is_subset(orig, other))\n\t\treturn perf_cpu_map__get(other);\n\n\ttmp_len = max(__perf_cpu_map__nr(orig), __perf_cpu_map__nr(other));\n\ttmp_cpus = malloc(tmp_len * sizeof(struct perf_cpu));\n\tif (!tmp_cpus)\n\t\treturn NULL;\n\n\ti = j = k = 0;\n\twhile (i < __perf_cpu_map__nr(orig) && j < __perf_cpu_map__nr(other)) {\n\t\tif (__perf_cpu_map__cpu(orig, i).cpu < __perf_cpu_map__cpu(other, j).cpu)\n\t\t\ti++;\n\t\telse if (__perf_cpu_map__cpu(orig, i).cpu > __perf_cpu_map__cpu(other, j).cpu)\n\t\t\tj++;\n\t\telse {\n\t\t\tj++;\n\t\t\ttmp_cpus[k++] = __perf_cpu_map__cpu(orig, i++);\n\t\t}\n\t}\n\tif (k)\n\t\tmerged = cpu_map__trim_new(k, tmp_cpus);\n\tfree(tmp_cpus);\n\treturn merged;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}