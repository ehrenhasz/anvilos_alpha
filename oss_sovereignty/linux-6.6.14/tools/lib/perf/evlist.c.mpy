{
  "module_name": "evlist.c",
  "hash_id": "f686a066bdd057cb235605835c4a9aa553b6fac5b234869d1b1131d464f53b88",
  "original_prompt": "Ingested from linux-6.6.14/tools/lib/perf/evlist.c",
  "human_readable_source": "\n#include <perf/evlist.h>\n#include <perf/evsel.h>\n#include <linux/bitops.h>\n#include <linux/list.h>\n#include <linux/hash.h>\n#include <sys/ioctl.h>\n#include <internal/evlist.h>\n#include <internal/evsel.h>\n#include <internal/xyarray.h>\n#include <internal/mmap.h>\n#include <internal/cpumap.h>\n#include <internal/threadmap.h>\n#include <internal/lib.h>\n#include <linux/zalloc.h>\n#include <stdlib.h>\n#include <errno.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <signal.h>\n#include <poll.h>\n#include <sys/mman.h>\n#include <perf/cpumap.h>\n#include <perf/threadmap.h>\n#include <api/fd/array.h>\n#include \"internal.h\"\n\nvoid perf_evlist__init(struct perf_evlist *evlist)\n{\n\tINIT_LIST_HEAD(&evlist->entries);\n\tevlist->nr_entries = 0;\n\tfdarray__init(&evlist->pollfd, 64);\n\tperf_evlist__reset_id_hash(evlist);\n}\n\nstatic void __perf_evlist__propagate_maps(struct perf_evlist *evlist,\n\t\t\t\t\t  struct perf_evsel *evsel)\n{\n\tif (evsel->system_wide) {\n\t\t \n\t\tperf_cpu_map__put(evsel->cpus);\n\t\tevsel->cpus = perf_cpu_map__new(NULL);\n\t} else if (evlist->has_user_cpus && evsel->is_pmu_core) {\n\t\t \n\t\tperf_cpu_map__put(evsel->cpus);\n\t\tevsel->cpus = perf_cpu_map__intersect(evlist->user_requested_cpus, evsel->own_cpus);\n\t} else if (!evsel->own_cpus || evlist->has_user_cpus ||\n\t\t(!evsel->requires_cpu && perf_cpu_map__has_any_cpu(evlist->user_requested_cpus))) {\n\t\t \n\t\tperf_cpu_map__put(evsel->cpus);\n\t\tevsel->cpus = perf_cpu_map__get(evlist->user_requested_cpus);\n\t} else if (evsel->cpus != evsel->own_cpus) {\n\t\t \n\t\tperf_cpu_map__put(evsel->cpus);\n\t\tevsel->cpus = perf_cpu_map__get(evsel->own_cpus);\n\t}\n\n\tif (evsel->system_wide) {\n\t\tperf_thread_map__put(evsel->threads);\n\t\tevsel->threads = perf_thread_map__new_dummy();\n\t} else {\n\t\tperf_thread_map__put(evsel->threads);\n\t\tevsel->threads = perf_thread_map__get(evlist->threads);\n\t}\n\n\tevlist->all_cpus = perf_cpu_map__merge(evlist->all_cpus, evsel->cpus);\n}\n\nstatic void perf_evlist__propagate_maps(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *evsel;\n\n\tevlist->needs_map_propagation = true;\n\n\tperf_evlist__for_each_evsel(evlist, evsel)\n\t\t__perf_evlist__propagate_maps(evlist, evsel);\n}\n\nvoid perf_evlist__add(struct perf_evlist *evlist,\n\t\t      struct perf_evsel *evsel)\n{\n\tevsel->idx = evlist->nr_entries;\n\tlist_add_tail(&evsel->node, &evlist->entries);\n\tevlist->nr_entries += 1;\n\n\tif (evlist->needs_map_propagation)\n\t\t__perf_evlist__propagate_maps(evlist, evsel);\n}\n\nvoid perf_evlist__remove(struct perf_evlist *evlist,\n\t\t\t struct perf_evsel *evsel)\n{\n\tlist_del_init(&evsel->node);\n\tevlist->nr_entries -= 1;\n}\n\nstruct perf_evlist *perf_evlist__new(void)\n{\n\tstruct perf_evlist *evlist = zalloc(sizeof(*evlist));\n\n\tif (evlist != NULL)\n\t\tperf_evlist__init(evlist);\n\n\treturn evlist;\n}\n\nstruct perf_evsel *\nperf_evlist__next(struct perf_evlist *evlist, struct perf_evsel *prev)\n{\n\tstruct perf_evsel *next;\n\n\tif (!prev) {\n\t\tnext = list_first_entry(&evlist->entries,\n\t\t\t\t\tstruct perf_evsel,\n\t\t\t\t\tnode);\n\t} else {\n\t\tnext = list_next_entry(prev, node);\n\t}\n\n\t \n\tif (&next->node == &evlist->entries)\n\t\treturn NULL;\n\n\treturn next;\n}\n\nstatic void perf_evlist__purge(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *pos, *n;\n\n\tperf_evlist__for_each_entry_safe(evlist, n, pos) {\n\t\tlist_del_init(&pos->node);\n\t\tperf_evsel__delete(pos);\n\t}\n\n\tevlist->nr_entries = 0;\n}\n\nvoid perf_evlist__exit(struct perf_evlist *evlist)\n{\n\tperf_cpu_map__put(evlist->user_requested_cpus);\n\tperf_cpu_map__put(evlist->all_cpus);\n\tperf_thread_map__put(evlist->threads);\n\tevlist->user_requested_cpus = NULL;\n\tevlist->all_cpus = NULL;\n\tevlist->threads = NULL;\n\tfdarray__exit(&evlist->pollfd);\n}\n\nvoid perf_evlist__delete(struct perf_evlist *evlist)\n{\n\tif (evlist == NULL)\n\t\treturn;\n\n\tperf_evlist__munmap(evlist);\n\tperf_evlist__close(evlist);\n\tperf_evlist__purge(evlist);\n\tperf_evlist__exit(evlist);\n\tfree(evlist);\n}\n\nvoid perf_evlist__set_maps(struct perf_evlist *evlist,\n\t\t\t   struct perf_cpu_map *cpus,\n\t\t\t   struct perf_thread_map *threads)\n{\n\t \n\tif (cpus != evlist->user_requested_cpus) {\n\t\tperf_cpu_map__put(evlist->user_requested_cpus);\n\t\tevlist->user_requested_cpus = perf_cpu_map__get(cpus);\n\t}\n\n\tif (threads != evlist->threads) {\n\t\tperf_thread_map__put(evlist->threads);\n\t\tevlist->threads = perf_thread_map__get(threads);\n\t}\n\n\tperf_evlist__propagate_maps(evlist);\n}\n\nint perf_evlist__open(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *evsel;\n\tint err;\n\n\tperf_evlist__for_each_entry(evlist, evsel) {\n\t\terr = perf_evsel__open(evsel, evsel->cpus, evsel->threads);\n\t\tif (err < 0)\n\t\t\tgoto out_err;\n\t}\n\n\treturn 0;\n\nout_err:\n\tperf_evlist__close(evlist);\n\treturn err;\n}\n\nvoid perf_evlist__close(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *evsel;\n\n\tperf_evlist__for_each_entry_reverse(evlist, evsel)\n\t\tperf_evsel__close(evsel);\n}\n\nvoid perf_evlist__enable(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *evsel;\n\n\tperf_evlist__for_each_entry(evlist, evsel)\n\t\tperf_evsel__enable(evsel);\n}\n\nvoid perf_evlist__disable(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *evsel;\n\n\tperf_evlist__for_each_entry(evlist, evsel)\n\t\tperf_evsel__disable(evsel);\n}\n\nu64 perf_evlist__read_format(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *first = perf_evlist__first(evlist);\n\n\treturn first->attr.read_format;\n}\n\n#define SID(e, x, y) xyarray__entry(e->sample_id, x, y)\n\nstatic void perf_evlist__id_hash(struct perf_evlist *evlist,\n\t\t\t\t struct perf_evsel *evsel,\n\t\t\t\t int cpu, int thread, u64 id)\n{\n\tint hash;\n\tstruct perf_sample_id *sid = SID(evsel, cpu, thread);\n\n\tsid->id = id;\n\tsid->evsel = evsel;\n\thash = hash_64(sid->id, PERF_EVLIST__HLIST_BITS);\n\thlist_add_head(&sid->node, &evlist->heads[hash]);\n}\n\nvoid perf_evlist__reset_id_hash(struct perf_evlist *evlist)\n{\n\tint i;\n\n\tfor (i = 0; i < PERF_EVLIST__HLIST_SIZE; ++i)\n\t\tINIT_HLIST_HEAD(&evlist->heads[i]);\n}\n\nvoid perf_evlist__id_add(struct perf_evlist *evlist,\n\t\t\t struct perf_evsel *evsel,\n\t\t\t int cpu, int thread, u64 id)\n{\n\tperf_evlist__id_hash(evlist, evsel, cpu, thread, id);\n\tevsel->id[evsel->ids++] = id;\n}\n\nint perf_evlist__id_add_fd(struct perf_evlist *evlist,\n\t\t\t   struct perf_evsel *evsel,\n\t\t\t   int cpu, int thread, int fd)\n{\n\tu64 read_data[4] = { 0, };\n\tint id_idx = 1;  \n\tu64 id;\n\tint ret;\n\n\tret = ioctl(fd, PERF_EVENT_IOC_ID, &id);\n\tif (!ret)\n\t\tgoto add;\n\n\tif (errno != ENOTTY)\n\t\treturn -1;\n\n\t \n\n\t \n\tif (perf_evlist__read_format(evlist) & PERF_FORMAT_GROUP)\n\t\treturn -1;\n\n\tif (!(evsel->attr.read_format & PERF_FORMAT_ID) ||\n\t    read(fd, &read_data, sizeof(read_data)) == -1)\n\t\treturn -1;\n\n\tif (evsel->attr.read_format & PERF_FORMAT_TOTAL_TIME_ENABLED)\n\t\t++id_idx;\n\tif (evsel->attr.read_format & PERF_FORMAT_TOTAL_TIME_RUNNING)\n\t\t++id_idx;\n\n\tid = read_data[id_idx];\n\nadd:\n\tperf_evlist__id_add(evlist, evsel, cpu, thread, id);\n\treturn 0;\n}\n\nint perf_evlist__alloc_pollfd(struct perf_evlist *evlist)\n{\n\tint nr_cpus = perf_cpu_map__nr(evlist->all_cpus);\n\tint nr_threads = perf_thread_map__nr(evlist->threads);\n\tint nfds = 0;\n\tstruct perf_evsel *evsel;\n\n\tperf_evlist__for_each_entry(evlist, evsel) {\n\t\tif (evsel->system_wide)\n\t\t\tnfds += nr_cpus;\n\t\telse\n\t\t\tnfds += nr_cpus * nr_threads;\n\t}\n\n\tif (fdarray__available_entries(&evlist->pollfd) < nfds &&\n\t    fdarray__grow(&evlist->pollfd, nfds) < 0)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nint perf_evlist__add_pollfd(struct perf_evlist *evlist, int fd,\n\t\t\t    void *ptr, short revent, enum fdarray_flags flags)\n{\n\tint pos = fdarray__add(&evlist->pollfd, fd, revent | POLLERR | POLLHUP, flags);\n\n\tif (pos >= 0) {\n\t\tevlist->pollfd.priv[pos].ptr = ptr;\n\t\tfcntl(fd, F_SETFL, O_NONBLOCK);\n\t}\n\n\treturn pos;\n}\n\nstatic void perf_evlist__munmap_filtered(struct fdarray *fda, int fd,\n\t\t\t\t\t void *arg __maybe_unused)\n{\n\tstruct perf_mmap *map = fda->priv[fd].ptr;\n\n\tif (map)\n\t\tperf_mmap__put(map);\n}\n\nint perf_evlist__filter_pollfd(struct perf_evlist *evlist, short revents_and_mask)\n{\n\treturn fdarray__filter(&evlist->pollfd, revents_and_mask,\n\t\t\t       perf_evlist__munmap_filtered, NULL);\n}\n\nint perf_evlist__poll(struct perf_evlist *evlist, int timeout)\n{\n\treturn fdarray__poll(&evlist->pollfd, timeout);\n}\n\nstatic struct perf_mmap* perf_evlist__alloc_mmap(struct perf_evlist *evlist, bool overwrite)\n{\n\tint i;\n\tstruct perf_mmap *map;\n\n\tmap = zalloc(evlist->nr_mmaps * sizeof(struct perf_mmap));\n\tif (!map)\n\t\treturn NULL;\n\n\tfor (i = 0; i < evlist->nr_mmaps; i++) {\n\t\tstruct perf_mmap *prev = i ? &map[i - 1] : NULL;\n\n\t\t \n\t\tperf_mmap__init(&map[i], prev, overwrite, NULL);\n\t}\n\n\treturn map;\n}\n\nstatic void perf_evsel__set_sid_idx(struct perf_evsel *evsel, int idx, int cpu, int thread)\n{\n\tstruct perf_sample_id *sid = SID(evsel, cpu, thread);\n\n\tsid->idx = idx;\n\tsid->cpu = perf_cpu_map__cpu(evsel->cpus, cpu);\n\tsid->tid = perf_thread_map__pid(evsel->threads, thread);\n}\n\nstatic struct perf_mmap*\nperf_evlist__mmap_cb_get(struct perf_evlist *evlist, bool overwrite, int idx)\n{\n\tstruct perf_mmap *maps;\n\n\tmaps = overwrite ? evlist->mmap_ovw : evlist->mmap;\n\n\tif (!maps) {\n\t\tmaps = perf_evlist__alloc_mmap(evlist, overwrite);\n\t\tif (!maps)\n\t\t\treturn NULL;\n\n\t\tif (overwrite)\n\t\t\tevlist->mmap_ovw = maps;\n\t\telse\n\t\t\tevlist->mmap = maps;\n\t}\n\n\treturn &maps[idx];\n}\n\n#define FD(e, x, y) (*(int *) xyarray__entry(e->fd, x, y))\n\nstatic int\nperf_evlist__mmap_cb_mmap(struct perf_mmap *map, struct perf_mmap_param *mp,\n\t\t\t  int output, struct perf_cpu cpu)\n{\n\treturn perf_mmap__mmap(map, mp, output, cpu);\n}\n\nstatic void perf_evlist__set_mmap_first(struct perf_evlist *evlist, struct perf_mmap *map,\n\t\t\t\t\tbool overwrite)\n{\n\tif (overwrite)\n\t\tevlist->mmap_ovw_first = map;\n\telse\n\t\tevlist->mmap_first = map;\n}\n\nstatic int\nmmap_per_evsel(struct perf_evlist *evlist, struct perf_evlist_mmap_ops *ops,\n\t       int idx, struct perf_mmap_param *mp, int cpu_idx,\n\t       int thread, int *_output, int *_output_overwrite, int *nr_mmaps)\n{\n\tstruct perf_cpu evlist_cpu = perf_cpu_map__cpu(evlist->all_cpus, cpu_idx);\n\tstruct perf_evsel *evsel;\n\tint revent;\n\n\tperf_evlist__for_each_entry(evlist, evsel) {\n\t\tbool overwrite = evsel->attr.write_backward;\n\t\tenum fdarray_flags flgs;\n\t\tstruct perf_mmap *map;\n\t\tint *output, fd, cpu;\n\n\t\tif (evsel->system_wide && thread)\n\t\t\tcontinue;\n\n\t\tcpu = perf_cpu_map__idx(evsel->cpus, evlist_cpu);\n\t\tif (cpu == -1)\n\t\t\tcontinue;\n\n\t\tmap = ops->get(evlist, overwrite, idx);\n\t\tif (map == NULL)\n\t\t\treturn -ENOMEM;\n\n\t\tif (overwrite) {\n\t\t\tmp->prot = PROT_READ;\n\t\t\toutput   = _output_overwrite;\n\t\t} else {\n\t\t\tmp->prot = PROT_READ | PROT_WRITE;\n\t\t\toutput   = _output;\n\t\t}\n\n\t\tfd = FD(evsel, cpu, thread);\n\n\t\tif (*output == -1) {\n\t\t\t*output = fd;\n\n\t\t\t \n\t\t\trefcount_set(&map->refcnt, 2);\n\n\t\t\tif (ops->idx)\n\t\t\t\tops->idx(evlist, evsel, mp, idx);\n\n\t\t\t \n\t\t\tpr_debug(\"idx %d: mmapping fd %d\\n\", idx, *output);\n\t\t\tif (ops->mmap(map, mp, *output, evlist_cpu) < 0)\n\t\t\t\treturn -1;\n\n\t\t\t*nr_mmaps += 1;\n\n\t\t\tif (!idx)\n\t\t\t\tperf_evlist__set_mmap_first(evlist, map, overwrite);\n\t\t} else {\n\t\t\t \n\t\t\tpr_debug(\"idx %d: set output fd %d -> %d\\n\", idx, fd, *output);\n\t\t\tif (ioctl(fd, PERF_EVENT_IOC_SET_OUTPUT, *output) != 0)\n\t\t\t\treturn -1;\n\n\t\t\tperf_mmap__get(map);\n\t\t}\n\n\t\trevent = !overwrite ? POLLIN : 0;\n\n\t\tflgs = evsel->system_wide ? fdarray_flag__nonfilterable : fdarray_flag__default;\n\t\tif (perf_evlist__add_pollfd(evlist, fd, map, revent, flgs) < 0) {\n\t\t\tperf_mmap__put(map);\n\t\t\treturn -1;\n\t\t}\n\n\t\tif (evsel->attr.read_format & PERF_FORMAT_ID) {\n\t\t\tif (perf_evlist__id_add_fd(evlist, evsel, cpu, thread,\n\t\t\t\t\t\t   fd) < 0)\n\t\t\t\treturn -1;\n\t\t\tperf_evsel__set_sid_idx(evsel, idx, cpu, thread);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int\nmmap_per_thread(struct perf_evlist *evlist, struct perf_evlist_mmap_ops *ops,\n\t\tstruct perf_mmap_param *mp)\n{\n\tint nr_threads = perf_thread_map__nr(evlist->threads);\n\tint nr_cpus    = perf_cpu_map__nr(evlist->all_cpus);\n\tint cpu, thread, idx = 0;\n\tint nr_mmaps = 0;\n\n\tpr_debug(\"%s: nr cpu values (may include -1) %d nr threads %d\\n\",\n\t\t __func__, nr_cpus, nr_threads);\n\n\t \n\tfor (thread = 0; thread < nr_threads; thread++, idx++) {\n\t\tint output = -1;\n\t\tint output_overwrite = -1;\n\n\t\tif (mmap_per_evsel(evlist, ops, idx, mp, 0, thread, &output,\n\t\t\t\t   &output_overwrite, &nr_mmaps))\n\t\t\tgoto out_unmap;\n\t}\n\n\t \n\tfor (cpu = 1; cpu < nr_cpus; cpu++, idx++) {\n\t\tint output = -1;\n\t\tint output_overwrite = -1;\n\n\t\tif (mmap_per_evsel(evlist, ops, idx, mp, cpu, 0, &output,\n\t\t\t\t   &output_overwrite, &nr_mmaps))\n\t\t\tgoto out_unmap;\n\t}\n\n\tif (nr_mmaps != evlist->nr_mmaps)\n\t\tpr_err(\"Miscounted nr_mmaps %d vs %d\\n\", nr_mmaps, evlist->nr_mmaps);\n\n\treturn 0;\n\nout_unmap:\n\tperf_evlist__munmap(evlist);\n\treturn -1;\n}\n\nstatic int\nmmap_per_cpu(struct perf_evlist *evlist, struct perf_evlist_mmap_ops *ops,\n\t     struct perf_mmap_param *mp)\n{\n\tint nr_threads = perf_thread_map__nr(evlist->threads);\n\tint nr_cpus    = perf_cpu_map__nr(evlist->all_cpus);\n\tint nr_mmaps = 0;\n\tint cpu, thread;\n\n\tpr_debug(\"%s: nr cpu values %d nr threads %d\\n\", __func__, nr_cpus, nr_threads);\n\n\tfor (cpu = 0; cpu < nr_cpus; cpu++) {\n\t\tint output = -1;\n\t\tint output_overwrite = -1;\n\n\t\tfor (thread = 0; thread < nr_threads; thread++) {\n\t\t\tif (mmap_per_evsel(evlist, ops, cpu, mp, cpu,\n\t\t\t\t\t   thread, &output, &output_overwrite, &nr_mmaps))\n\t\t\t\tgoto out_unmap;\n\t\t}\n\t}\n\n\tif (nr_mmaps != evlist->nr_mmaps)\n\t\tpr_err(\"Miscounted nr_mmaps %d vs %d\\n\", nr_mmaps, evlist->nr_mmaps);\n\n\treturn 0;\n\nout_unmap:\n\tperf_evlist__munmap(evlist);\n\treturn -1;\n}\n\nstatic int perf_evlist__nr_mmaps(struct perf_evlist *evlist)\n{\n\tint nr_mmaps;\n\n\t \n\tnr_mmaps = perf_cpu_map__nr(evlist->all_cpus);\n\tif (perf_cpu_map__empty(evlist->all_cpus)) {\n\t\t \n\t\tnr_mmaps += perf_thread_map__nr(evlist->threads);\n\t\t \n\t\tnr_mmaps -= 1;\n\t}\n\n\treturn nr_mmaps;\n}\n\nint perf_evlist__mmap_ops(struct perf_evlist *evlist,\n\t\t\t  struct perf_evlist_mmap_ops *ops,\n\t\t\t  struct perf_mmap_param *mp)\n{\n\tconst struct perf_cpu_map *cpus = evlist->all_cpus;\n\tstruct perf_evsel *evsel;\n\n\tif (!ops || !ops->get || !ops->mmap)\n\t\treturn -EINVAL;\n\n\tmp->mask = evlist->mmap_len - page_size - 1;\n\n\tevlist->nr_mmaps = perf_evlist__nr_mmaps(evlist);\n\n\tperf_evlist__for_each_entry(evlist, evsel) {\n\t\tif ((evsel->attr.read_format & PERF_FORMAT_ID) &&\n\t\t    evsel->sample_id == NULL &&\n\t\t    perf_evsel__alloc_id(evsel, evsel->fd->max_x, evsel->fd->max_y) < 0)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (evlist->pollfd.entries == NULL && perf_evlist__alloc_pollfd(evlist) < 0)\n\t\treturn -ENOMEM;\n\n\tif (perf_cpu_map__empty(cpus))\n\t\treturn mmap_per_thread(evlist, ops, mp);\n\n\treturn mmap_per_cpu(evlist, ops, mp);\n}\n\nint perf_evlist__mmap(struct perf_evlist *evlist, int pages)\n{\n\tstruct perf_mmap_param mp;\n\tstruct perf_evlist_mmap_ops ops = {\n\t\t.get  = perf_evlist__mmap_cb_get,\n\t\t.mmap = perf_evlist__mmap_cb_mmap,\n\t};\n\n\tevlist->mmap_len = (pages + 1) * page_size;\n\n\treturn perf_evlist__mmap_ops(evlist, &ops, &mp);\n}\n\nvoid perf_evlist__munmap(struct perf_evlist *evlist)\n{\n\tint i;\n\n\tif (evlist->mmap) {\n\t\tfor (i = 0; i < evlist->nr_mmaps; i++)\n\t\t\tperf_mmap__munmap(&evlist->mmap[i]);\n\t}\n\n\tif (evlist->mmap_ovw) {\n\t\tfor (i = 0; i < evlist->nr_mmaps; i++)\n\t\t\tperf_mmap__munmap(&evlist->mmap_ovw[i]);\n\t}\n\n\tzfree(&evlist->mmap);\n\tzfree(&evlist->mmap_ovw);\n}\n\nstruct perf_mmap*\nperf_evlist__next_mmap(struct perf_evlist *evlist, struct perf_mmap *map,\n\t\t       bool overwrite)\n{\n\tif (map)\n\t\treturn map->next;\n\n\treturn overwrite ? evlist->mmap_ovw_first : evlist->mmap_first;\n}\n\nvoid __perf_evlist__set_leader(struct list_head *list, struct perf_evsel *leader)\n{\n\tstruct perf_evsel *evsel;\n\tint n = 0;\n\n\t__perf_evlist__for_each_entry(list, evsel) {\n\t\tevsel->leader = leader;\n\t\tn++;\n\t}\n\tleader->nr_members = n;\n}\n\nvoid perf_evlist__set_leader(struct perf_evlist *evlist)\n{\n\tif (evlist->nr_entries) {\n\t\tstruct perf_evsel *first = list_entry(evlist->entries.next,\n\t\t\t\t\t\tstruct perf_evsel, node);\n\n\t\t__perf_evlist__set_leader(&evlist->entries, first);\n\t}\n}\n\nint perf_evlist__nr_groups(struct perf_evlist *evlist)\n{\n\tstruct perf_evsel *evsel;\n\tint nr_groups = 0;\n\n\tperf_evlist__for_each_evsel(evlist, evsel) {\n\t\t \n\t\tif (evsel->leader == evsel && evsel->nr_members > 1)\n\t\t\tnr_groups++;\n\t}\n\treturn nr_groups;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}