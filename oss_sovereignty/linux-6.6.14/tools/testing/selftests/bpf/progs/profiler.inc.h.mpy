{
  "module_name": "profiler.inc.h",
  "hash_id": "ec63c332c291b1d387e5ec98db193938b2f80f954b33a6d97582ee22cfde8536",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/bpf/progs/profiler.inc.h",
  "human_readable_source": "\n \n#include <vmlinux.h>\n#include <bpf/bpf_core_read.h>\n#include <bpf/bpf_helpers.h>\n#include <bpf/bpf_tracing.h>\n\n#include \"profiler.h\"\n#include \"err.h\"\n\n#ifndef NULL\n#define NULL 0\n#endif\n\n#define O_WRONLY 00000001\n#define O_RDWR 00000002\n#define O_DIRECTORY 00200000\n#define __O_TMPFILE 020000000\n#define O_TMPFILE (__O_TMPFILE | O_DIRECTORY)\n#define S_IFMT 00170000\n#define S_IFSOCK 0140000\n#define S_IFLNK 0120000\n#define S_IFREG 0100000\n#define S_IFBLK 0060000\n#define S_IFDIR 0040000\n#define S_IFCHR 0020000\n#define S_IFIFO 0010000\n#define S_ISUID 0004000\n#define S_ISGID 0002000\n#define S_ISVTX 0001000\n#define S_ISLNK(m) (((m)&S_IFMT) == S_IFLNK)\n#define S_ISDIR(m) (((m)&S_IFMT) == S_IFDIR)\n#define S_ISCHR(m) (((m)&S_IFMT) == S_IFCHR)\n#define S_ISBLK(m) (((m)&S_IFMT) == S_IFBLK)\n#define S_ISFIFO(m) (((m)&S_IFMT) == S_IFIFO)\n#define S_ISSOCK(m) (((m)&S_IFMT) == S_IFSOCK)\n\n#define KILL_DATA_ARRAY_SIZE 8\n\nstruct var_kill_data_arr_t {\n\tstruct var_kill_data_t array[KILL_DATA_ARRAY_SIZE];\n};\n\nunion any_profiler_data_t {\n\tstruct var_exec_data_t var_exec;\n\tstruct var_kill_data_t var_kill;\n\tstruct var_sysctl_data_t var_sysctl;\n\tstruct var_filemod_data_t var_filemod;\n\tstruct var_fork_data_t var_fork;\n\tstruct var_kill_data_arr_t var_kill_data_arr;\n};\n\nvolatile struct profiler_config_struct bpf_config = {};\n\n#define FETCH_CGROUPS_FROM_BPF (bpf_config.fetch_cgroups_from_bpf)\n#define CGROUP_FS_INODE (bpf_config.cgroup_fs_inode)\n#define CGROUP_LOGIN_SESSION_INODE \\\n\t(bpf_config.cgroup_login_session_inode)\n#define KILL_SIGNALS (bpf_config.kill_signals_mask)\n#define STALE_INFO (bpf_config.stale_info_secs)\n#define INODE_FILTER (bpf_config.inode_filter)\n#define READ_ENVIRON_FROM_EXEC (bpf_config.read_environ_from_exec)\n#define ENABLE_CGROUP_V1_RESOLVER (bpf_config.enable_cgroup_v1_resolver)\n\nstruct kernfs_iattrs___52 {\n\tstruct iattr ia_iattr;\n};\n\nstruct kernfs_node___52 {\n\tunion   {\n\t\tstruct {\n\t\t\tu32 ino;\n\t\t\tu32 generation;\n\t\t};\n\t\tu64 id;\n\t} id;\n};\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);\n\t__uint(max_entries, 1);\n\t__type(key, u32);\n\t__type(value, union any_profiler_data_t);\n} data_heap SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY);\n\t__uint(key_size, sizeof(int));\n\t__uint(value_size, sizeof(int));\n} events SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__uint(max_entries, KILL_DATA_ARRAY_SIZE);\n\t__type(key, u32);\n\t__type(value, struct var_kill_data_arr_t);\n} var_tpid_to_data SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_PERCPU_ARRAY);\n\t__uint(max_entries, profiler_bpf_max_function_id);\n\t__type(key, u32);\n\t__type(value, struct bpf_func_stats_data);\n} bpf_func_stats SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__type(key, u32);\n\t__type(value, bool);\n\t__uint(max_entries, 16);\n} allowed_devices SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__type(key, u64);\n\t__type(value, bool);\n\t__uint(max_entries, 1024);\n} allowed_file_inodes SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__type(key, u64);\n\t__type(value, bool);\n\t__uint(max_entries, 1024);\n} allowed_directory_inodes SEC(\".maps\");\n\nstruct {\n\t__uint(type, BPF_MAP_TYPE_HASH);\n\t__type(key, u32);\n\t__type(value, bool);\n\t__uint(max_entries, 16);\n} disallowed_exec_inodes SEC(\".maps\");\n\n#ifndef ARRAY_SIZE\n#define ARRAY_SIZE(arr) (sizeof(arr) / sizeof(arr[0]))\n#endif\n\nstatic INLINE bool IS_ERR(const void* ptr)\n{\n\treturn IS_ERR_VALUE((unsigned long)ptr);\n}\n\nstatic INLINE u32 get_userspace_pid()\n{\n\treturn bpf_get_current_pid_tgid() >> 32;\n}\n\nstatic INLINE bool is_init_process(u32 tgid)\n{\n\treturn tgid == 1 || tgid == 0;\n}\n\nstatic INLINE unsigned long\nprobe_read_lim(void* dst, void* src, unsigned long len, unsigned long max)\n{\n\tlen = len < max ? len : max;\n\tif (len > 1) {\n\t\tif (bpf_probe_read_kernel(dst, len, src))\n\t\t\treturn 0;\n\t} else if (len == 1) {\n\t\tif (bpf_probe_read_kernel(dst, 1, src))\n\t\t\treturn 0;\n\t}\n\treturn len;\n}\n\nstatic INLINE int get_var_spid_index(struct var_kill_data_arr_t* arr_struct,\n\t\t\t\t     int spid)\n{\n#ifdef UNROLL\n#pragma unroll\n#endif\n\tfor (int i = 0; i < ARRAY_SIZE(arr_struct->array); i++)\n\t\tif (arr_struct->array[i].meta.pid == spid)\n\t\t\treturn i;\n\treturn -1;\n}\n\nstatic INLINE void populate_ancestors(struct task_struct* task,\n\t\t\t\t      struct ancestors_data_t* ancestors_data)\n{\n\tstruct task_struct* parent = task;\n\tu32 num_ancestors, ppid;\n\n\tancestors_data->num_ancestors = 0;\n#ifdef UNROLL\n#pragma unroll\n#endif\n\tfor (num_ancestors = 0; num_ancestors < MAX_ANCESTORS; num_ancestors++) {\n\t\tparent = BPF_CORE_READ(parent, real_parent);\n\t\tif (parent == NULL)\n\t\t\tbreak;\n\t\tppid = BPF_CORE_READ(parent, tgid);\n\t\tif (is_init_process(ppid))\n\t\t\tbreak;\n\t\tancestors_data->ancestor_pids[num_ancestors] = ppid;\n\t\tancestors_data->ancestor_exec_ids[num_ancestors] =\n\t\t\tBPF_CORE_READ(parent, self_exec_id);\n\t\tancestors_data->ancestor_start_times[num_ancestors] =\n\t\t\tBPF_CORE_READ(parent, start_time);\n\t\tancestors_data->num_ancestors = num_ancestors;\n\t}\n}\n\nstatic INLINE void* read_full_cgroup_path(struct kernfs_node* cgroup_node,\n\t\t\t\t\t  struct kernfs_node* cgroup_root_node,\n\t\t\t\t\t  void* payload,\n\t\t\t\t\t  int* root_pos)\n{\n\tvoid* payload_start = payload;\n\tsize_t filepart_length;\n\n#ifdef UNROLL\n#pragma unroll\n#endif\n\tfor (int i = 0; i < MAX_CGROUPS_PATH_DEPTH; i++) {\n\t\tfilepart_length =\n\t\t\tbpf_probe_read_kernel_str(payload, MAX_PATH,\n\t\t\t\t\t\t  BPF_CORE_READ(cgroup_node, name));\n\t\tif (!cgroup_node)\n\t\t\treturn payload;\n\t\tif (cgroup_node == cgroup_root_node)\n\t\t\t*root_pos = payload - payload_start;\n\t\tif (filepart_length <= MAX_PATH) {\n\t\t\tbarrier_var(filepart_length);\n\t\t\tpayload += filepart_length;\n\t\t}\n\t\tcgroup_node = BPF_CORE_READ(cgroup_node, parent);\n\t}\n\treturn payload;\n}\n\nstatic ino_t get_inode_from_kernfs(struct kernfs_node* node)\n{\n\tstruct kernfs_node___52* node52 = (void*)node;\n\n\tif (bpf_core_field_exists(node52->id.ino)) {\n\t\tbarrier_var(node52);\n\t\treturn BPF_CORE_READ(node52, id.ino);\n\t} else {\n\t\tbarrier_var(node);\n\t\treturn (u64)BPF_CORE_READ(node, id);\n\t}\n}\n\nextern bool CONFIG_CGROUP_PIDS __kconfig __weak;\nenum cgroup_subsys_id___local {\n\tpids_cgrp_id___local = 123,  \n};\n\nstatic INLINE void* populate_cgroup_info(struct cgroup_data_t* cgroup_data,\n\t\t\t\t\t struct task_struct* task,\n\t\t\t\t\t void* payload)\n{\n\tstruct kernfs_node* root_kernfs =\n\t\tBPF_CORE_READ(task, nsproxy, cgroup_ns, root_cset, dfl_cgrp, kn);\n\tstruct kernfs_node* proc_kernfs = BPF_CORE_READ(task, cgroups, dfl_cgrp, kn);\n\n#if __has_builtin(__builtin_preserve_enum_value)\n\tif (ENABLE_CGROUP_V1_RESOLVER && CONFIG_CGROUP_PIDS) {\n\t\tint cgrp_id = bpf_core_enum_value(enum cgroup_subsys_id___local,\n\t\t\t\t\t\t  pids_cgrp_id___local);\n#ifdef UNROLL\n#pragma unroll\n#endif\n\t\tfor (int i = 0; i < CGROUP_SUBSYS_COUNT; i++) {\n\t\t\tstruct cgroup_subsys_state* subsys =\n\t\t\t\tBPF_CORE_READ(task, cgroups, subsys[i]);\n\t\t\tif (subsys != NULL) {\n\t\t\t\tint subsys_id = BPF_CORE_READ(subsys, ss, id);\n\t\t\t\tif (subsys_id == cgrp_id) {\n\t\t\t\t\tproc_kernfs = BPF_CORE_READ(subsys, cgroup, kn);\n\t\t\t\t\troot_kernfs = BPF_CORE_READ(subsys, ss, root, kf_root, kn);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n#endif\n\n\tcgroup_data->cgroup_root_inode = get_inode_from_kernfs(root_kernfs);\n\tcgroup_data->cgroup_proc_inode = get_inode_from_kernfs(proc_kernfs);\n\n\tif (bpf_core_field_exists(root_kernfs->iattr->ia_mtime)) {\n\t\tcgroup_data->cgroup_root_mtime =\n\t\t\tBPF_CORE_READ(root_kernfs, iattr, ia_mtime.tv_nsec);\n\t\tcgroup_data->cgroup_proc_mtime =\n\t\t\tBPF_CORE_READ(proc_kernfs, iattr, ia_mtime.tv_nsec);\n\t} else {\n\t\tstruct kernfs_iattrs___52* root_iattr =\n\t\t\t(struct kernfs_iattrs___52*)BPF_CORE_READ(root_kernfs, iattr);\n\t\tcgroup_data->cgroup_root_mtime =\n\t\t\tBPF_CORE_READ(root_iattr, ia_iattr.ia_mtime.tv_nsec);\n\n\t\tstruct kernfs_iattrs___52* proc_iattr =\n\t\t\t(struct kernfs_iattrs___52*)BPF_CORE_READ(proc_kernfs, iattr);\n\t\tcgroup_data->cgroup_proc_mtime =\n\t\t\tBPF_CORE_READ(proc_iattr, ia_iattr.ia_mtime.tv_nsec);\n\t}\n\n\tcgroup_data->cgroup_root_length = 0;\n\tcgroup_data->cgroup_proc_length = 0;\n\tcgroup_data->cgroup_full_length = 0;\n\n\tsize_t cgroup_root_length =\n\t\tbpf_probe_read_kernel_str(payload, MAX_PATH,\n\t\t\t\t\t  BPF_CORE_READ(root_kernfs, name));\n\tbarrier_var(cgroup_root_length);\n\tif (cgroup_root_length <= MAX_PATH) {\n\t\tbarrier_var(cgroup_root_length);\n\t\tcgroup_data->cgroup_root_length = cgroup_root_length;\n\t\tpayload += cgroup_root_length;\n\t}\n\n\tsize_t cgroup_proc_length =\n\t\tbpf_probe_read_kernel_str(payload, MAX_PATH,\n\t\t\t\t\t  BPF_CORE_READ(proc_kernfs, name));\n\tbarrier_var(cgroup_proc_length);\n\tif (cgroup_proc_length <= MAX_PATH) {\n\t\tbarrier_var(cgroup_proc_length);\n\t\tcgroup_data->cgroup_proc_length = cgroup_proc_length;\n\t\tpayload += cgroup_proc_length;\n\t}\n\n\tif (FETCH_CGROUPS_FROM_BPF) {\n\t\tcgroup_data->cgroup_full_path_root_pos = -1;\n\t\tvoid* payload_end_pos = read_full_cgroup_path(proc_kernfs, root_kernfs, payload,\n\t\t\t\t\t\t\t      &cgroup_data->cgroup_full_path_root_pos);\n\t\tcgroup_data->cgroup_full_length = payload_end_pos - payload;\n\t\tpayload = payload_end_pos;\n\t}\n\n\treturn (void*)payload;\n}\n\nstatic INLINE void* populate_var_metadata(struct var_metadata_t* metadata,\n\t\t\t\t\t  struct task_struct* task,\n\t\t\t\t\t  u32 pid, void* payload)\n{\n\tu64 uid_gid = bpf_get_current_uid_gid();\n\n\tmetadata->uid = (u32)uid_gid;\n\tmetadata->gid = uid_gid >> 32;\n\tmetadata->pid = pid;\n\tmetadata->exec_id = BPF_CORE_READ(task, self_exec_id);\n\tmetadata->start_time = BPF_CORE_READ(task, start_time);\n\tmetadata->comm_length = 0;\n\n\tsize_t comm_length = bpf_core_read_str(payload, TASK_COMM_LEN, &task->comm);\n\tbarrier_var(comm_length);\n\tif (comm_length <= TASK_COMM_LEN) {\n\t\tbarrier_var(comm_length);\n\t\tmetadata->comm_length = comm_length;\n\t\tpayload += comm_length;\n\t}\n\n\treturn (void*)payload;\n}\n\nstatic INLINE struct var_kill_data_t*\nget_var_kill_data(struct pt_regs* ctx, int spid, int tpid, int sig)\n{\n\tint zero = 0;\n\tstruct var_kill_data_t* kill_data = bpf_map_lookup_elem(&data_heap, &zero);\n\n\tif (kill_data == NULL)\n\t\treturn NULL;\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\n\tvoid* payload = populate_var_metadata(&kill_data->meta, task, spid, kill_data->payload);\n\tpayload = populate_cgroup_info(&kill_data->cgroup_data, task, payload);\n\tsize_t payload_length = payload - (void*)kill_data->payload;\n\tkill_data->payload_length = payload_length;\n\tpopulate_ancestors(task, &kill_data->ancestors_info);\n\tkill_data->meta.type = KILL_EVENT;\n\tkill_data->kill_target_pid = tpid;\n\tkill_data->kill_sig = sig;\n\tkill_data->kill_count = 1;\n\tkill_data->last_kill_time = bpf_ktime_get_ns();\n\treturn kill_data;\n}\n\nstatic INLINE int trace_var_sys_kill(void* ctx, int tpid, int sig)\n{\n\tif ((KILL_SIGNALS & (1ULL << sig)) == 0)\n\t\treturn 0;\n\n\tu32 spid = get_userspace_pid();\n\tstruct var_kill_data_arr_t* arr_struct = bpf_map_lookup_elem(&var_tpid_to_data, &tpid);\n\n\tif (arr_struct == NULL) {\n\t\tstruct var_kill_data_t* kill_data = get_var_kill_data(ctx, spid, tpid, sig);\n\t\tint zero = 0;\n\n\t\tif (kill_data == NULL)\n\t\t\treturn 0;\n\t\tarr_struct = bpf_map_lookup_elem(&data_heap, &zero);\n\t\tif (arr_struct == NULL)\n\t\t\treturn 0;\n\t\tbpf_probe_read_kernel(&arr_struct->array[0],\n\t\t\t\t      sizeof(arr_struct->array[0]), kill_data);\n\t} else {\n\t\tint index = get_var_spid_index(arr_struct, spid);\n\n\t\tif (index == -1) {\n\t\t\tstruct var_kill_data_t* kill_data =\n\t\t\t\tget_var_kill_data(ctx, spid, tpid, sig);\n\t\t\tif (kill_data == NULL)\n\t\t\t\treturn 0;\n#ifdef UNROLL\n#pragma unroll\n#endif\n\t\t\tfor (int i = 0; i < ARRAY_SIZE(arr_struct->array); i++)\n\t\t\t\tif (arr_struct->array[i].meta.pid == 0) {\n\t\t\t\t\tbpf_probe_read_kernel(&arr_struct->array[i],\n\t\t\t\t\t\t\t      sizeof(arr_struct->array[i]),\n\t\t\t\t\t\t\t      kill_data);\n\t\t\t\t\tbpf_map_update_elem(&var_tpid_to_data, &tpid,\n\t\t\t\t\t\t\t    arr_struct, 0);\n\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\n\t\tstruct var_kill_data_t* kill_data = &arr_struct->array[index];\n\n\t\tu64 delta_sec =\n\t\t\t(bpf_ktime_get_ns() - kill_data->last_kill_time) / 1000000000;\n\n\t\tif (delta_sec < STALE_INFO) {\n\t\t\tkill_data->kill_count++;\n\t\t\tkill_data->last_kill_time = bpf_ktime_get_ns();\n\t\t\tbpf_probe_read_kernel(&arr_struct->array[index],\n\t\t\t\t\t      sizeof(arr_struct->array[index]),\n\t\t\t\t\t      kill_data);\n\t\t} else {\n\t\t\tstruct var_kill_data_t* kill_data =\n\t\t\t\tget_var_kill_data(ctx, spid, tpid, sig);\n\t\t\tif (kill_data == NULL)\n\t\t\t\treturn 0;\n\t\t\tbpf_probe_read_kernel(&arr_struct->array[index],\n\t\t\t\t\t      sizeof(arr_struct->array[index]),\n\t\t\t\t\t      kill_data);\n\t\t}\n\t}\n\tbpf_map_update_elem(&var_tpid_to_data, &tpid, arr_struct, 0);\n\treturn 0;\n}\n\nstatic INLINE void bpf_stats_enter(struct bpf_func_stats_ctx* bpf_stat_ctx,\n\t\t\t\t   enum bpf_function_id func_id)\n{\n\tint func_id_key = func_id;\n\n\tbpf_stat_ctx->start_time_ns = bpf_ktime_get_ns();\n\tbpf_stat_ctx->bpf_func_stats_data_val =\n\t\tbpf_map_lookup_elem(&bpf_func_stats, &func_id_key);\n\tif (bpf_stat_ctx->bpf_func_stats_data_val)\n\t\tbpf_stat_ctx->bpf_func_stats_data_val->num_executions++;\n}\n\nstatic INLINE void bpf_stats_exit(struct bpf_func_stats_ctx* bpf_stat_ctx)\n{\n\tif (bpf_stat_ctx->bpf_func_stats_data_val)\n\t\tbpf_stat_ctx->bpf_func_stats_data_val->time_elapsed_ns +=\n\t\t\tbpf_ktime_get_ns() - bpf_stat_ctx->start_time_ns;\n}\n\nstatic INLINE void\nbpf_stats_pre_submit_var_perf_event(struct bpf_func_stats_ctx* bpf_stat_ctx,\n\t\t\t\t    struct var_metadata_t* meta)\n{\n\tif (bpf_stat_ctx->bpf_func_stats_data_val) {\n\t\tbpf_stat_ctx->bpf_func_stats_data_val->num_perf_events++;\n\t\tmeta->bpf_stats_num_perf_events =\n\t\t\tbpf_stat_ctx->bpf_func_stats_data_val->num_perf_events;\n\t}\n\tmeta->bpf_stats_start_ktime_ns = bpf_stat_ctx->start_time_ns;\n\tmeta->cpu_id = bpf_get_smp_processor_id();\n}\n\nstatic INLINE size_t\nread_absolute_file_path_from_dentry(struct dentry* filp_dentry, void* payload)\n{\n\tsize_t length = 0;\n\tsize_t filepart_length;\n\tstruct dentry* parent_dentry;\n\n#ifdef UNROLL\n#pragma unroll\n#endif\n\tfor (int i = 0; i < MAX_PATH_DEPTH; i++) {\n\t\tfilepart_length =\n\t\t\tbpf_probe_read_kernel_str(payload, MAX_PATH,\n\t\t\t\t\t\t  BPF_CORE_READ(filp_dentry, d_name.name));\n\t\tbarrier_var(filepart_length);\n\t\tif (filepart_length > MAX_PATH)\n\t\t\tbreak;\n\t\tbarrier_var(filepart_length);\n\t\tpayload += filepart_length;\n\t\tlength += filepart_length;\n\n\t\tparent_dentry = BPF_CORE_READ(filp_dentry, d_parent);\n\t\tif (filp_dentry == parent_dentry)\n\t\t\tbreak;\n\t\tfilp_dentry = parent_dentry;\n\t}\n\n\treturn length;\n}\n\nstatic INLINE bool\nis_ancestor_in_allowed_inodes(struct dentry* filp_dentry)\n{\n\tstruct dentry* parent_dentry;\n#ifdef UNROLL\n#pragma unroll\n#endif\n\tfor (int i = 0; i < MAX_PATH_DEPTH; i++) {\n\t\tu64 dir_ino = BPF_CORE_READ(filp_dentry, d_inode, i_ino);\n\t\tbool* allowed_dir = bpf_map_lookup_elem(&allowed_directory_inodes, &dir_ino);\n\n\t\tif (allowed_dir != NULL)\n\t\t\treturn true;\n\t\tparent_dentry = BPF_CORE_READ(filp_dentry, d_parent);\n\t\tif (filp_dentry == parent_dentry)\n\t\t\tbreak;\n\t\tfilp_dentry = parent_dentry;\n\t}\n\treturn false;\n}\n\nstatic INLINE bool is_dentry_allowed_for_filemod(struct dentry* file_dentry,\n\t\t\t\t\t\t u32* device_id,\n\t\t\t\t\t\t u64* file_ino)\n{\n\tu32 dev_id = BPF_CORE_READ(file_dentry, d_sb, s_dev);\n\t*device_id = dev_id;\n\tbool* allowed_device = bpf_map_lookup_elem(&allowed_devices, &dev_id);\n\n\tif (allowed_device == NULL)\n\t\treturn false;\n\n\tu64 ino = BPF_CORE_READ(file_dentry, d_inode, i_ino);\n\t*file_ino = ino;\n\tbool* allowed_file = bpf_map_lookup_elem(&allowed_file_inodes, &ino);\n\n\tif (allowed_file == NULL)\n\t\tif (!is_ancestor_in_allowed_inodes(BPF_CORE_READ(file_dentry, d_parent)))\n\t\t\treturn false;\n\treturn true;\n}\n\nSEC(\"kprobe/proc_sys_write\")\nssize_t BPF_KPROBE(kprobe__proc_sys_write,\n\t\t   struct file* filp, const char* buf,\n\t\t   size_t count, loff_t* ppos)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_proc_sys_write);\n\n\tu32 pid = get_userspace_pid();\n\tint zero = 0;\n\tstruct var_sysctl_data_t* sysctl_data =\n\t\tbpf_map_lookup_elem(&data_heap, &zero);\n\tif (!sysctl_data)\n\t\tgoto out;\n\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\tsysctl_data->meta.type = SYSCTL_EVENT;\n\tvoid* payload = populate_var_metadata(&sysctl_data->meta, task, pid, sysctl_data->payload);\n\tpayload = populate_cgroup_info(&sysctl_data->cgroup_data, task, payload);\n\n\tpopulate_ancestors(task, &sysctl_data->ancestors_info);\n\n\tsysctl_data->sysctl_val_length = 0;\n\tsysctl_data->sysctl_path_length = 0;\n\n\tsize_t sysctl_val_length = bpf_probe_read_kernel_str(payload,\n\t\t\t\t\t\t\t     CTL_MAXNAME, buf);\n\tbarrier_var(sysctl_val_length);\n\tif (sysctl_val_length <= CTL_MAXNAME) {\n\t\tbarrier_var(sysctl_val_length);\n\t\tsysctl_data->sysctl_val_length = sysctl_val_length;\n\t\tpayload += sysctl_val_length;\n\t}\n\n\tsize_t sysctl_path_length =\n\t\tbpf_probe_read_kernel_str(payload, MAX_PATH,\n\t\t\t\t\t  BPF_CORE_READ(filp, f_path.dentry,\n\t\t\t\t\t\t\td_name.name));\n\tbarrier_var(sysctl_path_length);\n\tif (sysctl_path_length <= MAX_PATH) {\n\t\tbarrier_var(sysctl_path_length);\n\t\tsysctl_data->sysctl_path_length = sysctl_path_length;\n\t\tpayload += sysctl_path_length;\n\t}\n\n\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &sysctl_data->meta);\n\tunsigned long data_len = payload - (void*)sysctl_data;\n\tdata_len = data_len > sizeof(struct var_sysctl_data_t)\n\t\t? sizeof(struct var_sysctl_data_t)\n\t\t: data_len;\n\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, sysctl_data, data_len);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\n\nSEC(\"tracepoint/syscalls/sys_enter_kill\")\nint tracepoint__syscalls__sys_enter_kill(struct trace_event_raw_sys_enter* ctx)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_sys_enter_kill);\n\tint pid = ctx->args[0];\n\tint sig = ctx->args[1];\n\tint ret = trace_var_sys_kill(ctx, pid, sig);\n\tbpf_stats_exit(&stats_ctx);\n\treturn ret;\n};\n\nSEC(\"raw_tracepoint/sched_process_exit\")\nint raw_tracepoint__sched_process_exit(void* ctx)\n{\n\tint zero = 0;\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_sched_process_exit);\n\n\tu32 tpid = get_userspace_pid();\n\n\tstruct var_kill_data_arr_t* arr_struct = bpf_map_lookup_elem(&var_tpid_to_data, &tpid);\n\tstruct var_kill_data_t* kill_data = bpf_map_lookup_elem(&data_heap, &zero);\n\n\tif (arr_struct == NULL || kill_data == NULL)\n\t\tgoto out;\n\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\tstruct kernfs_node* proc_kernfs = BPF_CORE_READ(task, cgroups, dfl_cgrp, kn);\n\n#ifdef UNROLL\n#pragma unroll\n#endif\n\tfor (int i = 0; i < ARRAY_SIZE(arr_struct->array); i++) {\n\t\tstruct var_kill_data_t* past_kill_data = &arr_struct->array[i];\n\n\t\tif (past_kill_data != NULL && past_kill_data->kill_target_pid == tpid) {\n\t\t\tbpf_probe_read_kernel(kill_data, sizeof(*past_kill_data),\n\t\t\t\t\t      past_kill_data);\n\t\t\tvoid* payload = kill_data->payload;\n\t\t\tsize_t offset = kill_data->payload_length;\n\t\t\tif (offset >= MAX_METADATA_PAYLOAD_LEN + MAX_CGROUP_PAYLOAD_LEN)\n\t\t\t\treturn 0;\n\t\t\tpayload += offset;\n\n\t\t\tkill_data->kill_target_name_length = 0;\n\t\t\tkill_data->kill_target_cgroup_proc_length = 0;\n\n\t\t\tsize_t comm_length = bpf_core_read_str(payload, TASK_COMM_LEN, &task->comm);\n\t\t\tbarrier_var(comm_length);\n\t\t\tif (comm_length <= TASK_COMM_LEN) {\n\t\t\t\tbarrier_var(comm_length);\n\t\t\t\tkill_data->kill_target_name_length = comm_length;\n\t\t\t\tpayload += comm_length;\n\t\t\t}\n\n\t\t\tsize_t cgroup_proc_length =\n\t\t\t\tbpf_probe_read_kernel_str(payload,\n\t\t\t\t\t\t\t  KILL_TARGET_LEN,\n\t\t\t\t\t\t\t  BPF_CORE_READ(proc_kernfs, name));\n\t\t\tbarrier_var(cgroup_proc_length);\n\t\t\tif (cgroup_proc_length <= KILL_TARGET_LEN) {\n\t\t\t\tbarrier_var(cgroup_proc_length);\n\t\t\t\tkill_data->kill_target_cgroup_proc_length = cgroup_proc_length;\n\t\t\t\tpayload += cgroup_proc_length;\n\t\t\t}\n\n\t\t\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &kill_data->meta);\n\t\t\tunsigned long data_len = (void*)payload - (void*)kill_data;\n\t\t\tdata_len = data_len > sizeof(struct var_kill_data_t)\n\t\t\t\t? sizeof(struct var_kill_data_t)\n\t\t\t\t: data_len;\n\t\t\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, kill_data, data_len);\n\t\t}\n\t}\n\tbpf_map_delete_elem(&var_tpid_to_data, &tpid);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\n\nSEC(\"raw_tracepoint/sched_process_exec\")\nint raw_tracepoint__sched_process_exec(struct bpf_raw_tracepoint_args* ctx)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_sched_process_exec);\n\n\tstruct linux_binprm* bprm = (struct linux_binprm*)ctx->args[2];\n\tu64 inode = BPF_CORE_READ(bprm, file, f_inode, i_ino);\n\n\tbool* should_filter_binprm = bpf_map_lookup_elem(&disallowed_exec_inodes, &inode);\n\tif (should_filter_binprm != NULL)\n\t\tgoto out;\n\n\tint zero = 0;\n\tstruct var_exec_data_t* proc_exec_data = bpf_map_lookup_elem(&data_heap, &zero);\n\tif (!proc_exec_data)\n\t\tgoto out;\n\n\tif (INODE_FILTER && inode != INODE_FILTER)\n\t\treturn 0;\n\n\tu32 pid = get_userspace_pid();\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\n\tproc_exec_data->meta.type = EXEC_EVENT;\n\tproc_exec_data->bin_path_length = 0;\n\tproc_exec_data->cmdline_length = 0;\n\tproc_exec_data->environment_length = 0;\n\tvoid* payload = populate_var_metadata(&proc_exec_data->meta, task, pid,\n\t\t\t\t\t      proc_exec_data->payload);\n\tpayload = populate_cgroup_info(&proc_exec_data->cgroup_data, task, payload);\n\n\tstruct task_struct* parent_task = BPF_CORE_READ(task, real_parent);\n\tproc_exec_data->parent_pid = BPF_CORE_READ(parent_task, tgid);\n\tproc_exec_data->parent_uid = BPF_CORE_READ(parent_task, real_cred, uid.val);\n\tproc_exec_data->parent_exec_id = BPF_CORE_READ(parent_task, self_exec_id);\n\tproc_exec_data->parent_start_time = BPF_CORE_READ(parent_task, start_time);\n\n\tconst char* filename = BPF_CORE_READ(bprm, filename);\n\tsize_t bin_path_length =\n\t\tbpf_probe_read_kernel_str(payload, MAX_FILENAME_LEN, filename);\n\tbarrier_var(bin_path_length);\n\tif (bin_path_length <= MAX_FILENAME_LEN) {\n\t\tbarrier_var(bin_path_length);\n\t\tproc_exec_data->bin_path_length = bin_path_length;\n\t\tpayload += bin_path_length;\n\t}\n\n\tvoid* arg_start = (void*)BPF_CORE_READ(task, mm, arg_start);\n\tvoid* arg_end = (void*)BPF_CORE_READ(task, mm, arg_end);\n\tunsigned int cmdline_length = probe_read_lim(payload, arg_start,\n\t\t\t\t\t\t     arg_end - arg_start, MAX_ARGS_LEN);\n\n\tif (cmdline_length <= MAX_ARGS_LEN) {\n\t\tbarrier_var(cmdline_length);\n\t\tproc_exec_data->cmdline_length = cmdline_length;\n\t\tpayload += cmdline_length;\n\t}\n\n\tif (READ_ENVIRON_FROM_EXEC) {\n\t\tvoid* env_start = (void*)BPF_CORE_READ(task, mm, env_start);\n\t\tvoid* env_end = (void*)BPF_CORE_READ(task, mm, env_end);\n\t\tunsigned long env_len = probe_read_lim(payload, env_start,\n\t\t\t\t\t\t       env_end - env_start, MAX_ENVIRON_LEN);\n\t\tif (cmdline_length <= MAX_ENVIRON_LEN) {\n\t\t\tproc_exec_data->environment_length = env_len;\n\t\t\tpayload += env_len;\n\t\t}\n\t}\n\n\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &proc_exec_data->meta);\n\tunsigned long data_len = payload - (void*)proc_exec_data;\n\tdata_len = data_len > sizeof(struct var_exec_data_t)\n\t\t? sizeof(struct var_exec_data_t)\n\t\t: data_len;\n\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, proc_exec_data, data_len);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\n\nSEC(\"kretprobe/do_filp_open\")\nint kprobe_ret__do_filp_open(struct pt_regs* ctx)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_do_filp_open_ret);\n\n\tstruct file* filp = (struct file*)PT_REGS_RC_CORE(ctx);\n\n\tif (filp == NULL || IS_ERR(filp))\n\t\tgoto out;\n\tunsigned int flags = BPF_CORE_READ(filp, f_flags);\n\tif ((flags & (O_RDWR | O_WRONLY)) == 0)\n\t\tgoto out;\n\tif ((flags & O_TMPFILE) > 0)\n\t\tgoto out;\n\tstruct inode* file_inode = BPF_CORE_READ(filp, f_inode);\n\tumode_t mode = BPF_CORE_READ(file_inode, i_mode);\n\tif (S_ISDIR(mode) || S_ISCHR(mode) || S_ISBLK(mode) || S_ISFIFO(mode) ||\n\t    S_ISSOCK(mode))\n\t\tgoto out;\n\n\tstruct dentry* filp_dentry = BPF_CORE_READ(filp, f_path.dentry);\n\tu32 device_id = 0;\n\tu64 file_ino = 0;\n\tif (!is_dentry_allowed_for_filemod(filp_dentry, &device_id, &file_ino))\n\t\tgoto out;\n\n\tint zero = 0;\n\tstruct var_filemod_data_t* filemod_data = bpf_map_lookup_elem(&data_heap, &zero);\n\tif (!filemod_data)\n\t\tgoto out;\n\n\tu32 pid = get_userspace_pid();\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\n\tfilemod_data->meta.type = FILEMOD_EVENT;\n\tfilemod_data->fmod_type = FMOD_OPEN;\n\tfilemod_data->dst_flags = flags;\n\tfilemod_data->src_inode = 0;\n\tfilemod_data->dst_inode = file_ino;\n\tfilemod_data->src_device_id = 0;\n\tfilemod_data->dst_device_id = device_id;\n\tfilemod_data->src_filepath_length = 0;\n\tfilemod_data->dst_filepath_length = 0;\n\n\tvoid* payload = populate_var_metadata(&filemod_data->meta, task, pid,\n\t\t\t\t\t      filemod_data->payload);\n\tpayload = populate_cgroup_info(&filemod_data->cgroup_data, task, payload);\n\n\tsize_t len = read_absolute_file_path_from_dentry(filp_dentry, payload);\n\tbarrier_var(len);\n\tif (len <= MAX_FILEPATH_LENGTH) {\n\t\tbarrier_var(len);\n\t\tpayload += len;\n\t\tfilemod_data->dst_filepath_length = len;\n\t}\n\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &filemod_data->meta);\n\tunsigned long data_len = payload - (void*)filemod_data;\n\tdata_len = data_len > sizeof(*filemod_data) ? sizeof(*filemod_data) : data_len;\n\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, filemod_data, data_len);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\n\nSEC(\"kprobe/vfs_link\")\nint BPF_KPROBE(kprobe__vfs_link,\n\t       struct dentry* old_dentry, struct mnt_idmap *idmap,\n\t       struct inode* dir, struct dentry* new_dentry,\n\t       struct inode** delegated_inode)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_vfs_link);\n\n\tu32 src_device_id = 0;\n\tu64 src_file_ino = 0;\n\tu32 dst_device_id = 0;\n\tu64 dst_file_ino = 0;\n\tif (!is_dentry_allowed_for_filemod(old_dentry, &src_device_id, &src_file_ino) &&\n\t    !is_dentry_allowed_for_filemod(new_dentry, &dst_device_id, &dst_file_ino))\n\t\tgoto out;\n\n\tint zero = 0;\n\tstruct var_filemod_data_t* filemod_data = bpf_map_lookup_elem(&data_heap, &zero);\n\tif (!filemod_data)\n\t\tgoto out;\n\n\tu32 pid = get_userspace_pid();\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\n\tfilemod_data->meta.type = FILEMOD_EVENT;\n\tfilemod_data->fmod_type = FMOD_LINK;\n\tfilemod_data->dst_flags = 0;\n\tfilemod_data->src_inode = src_file_ino;\n\tfilemod_data->dst_inode = dst_file_ino;\n\tfilemod_data->src_device_id = src_device_id;\n\tfilemod_data->dst_device_id = dst_device_id;\n\tfilemod_data->src_filepath_length = 0;\n\tfilemod_data->dst_filepath_length = 0;\n\n\tvoid* payload = populate_var_metadata(&filemod_data->meta, task, pid,\n\t\t\t\t\t      filemod_data->payload);\n\tpayload = populate_cgroup_info(&filemod_data->cgroup_data, task, payload);\n\n\tsize_t len = read_absolute_file_path_from_dentry(old_dentry, payload);\n\tbarrier_var(len);\n\tif (len <= MAX_FILEPATH_LENGTH) {\n\t\tbarrier_var(len);\n\t\tpayload += len;\n\t\tfilemod_data->src_filepath_length = len;\n\t}\n\n\tlen = read_absolute_file_path_from_dentry(new_dentry, payload);\n\tbarrier_var(len);\n\tif (len <= MAX_FILEPATH_LENGTH) {\n\t\tbarrier_var(len);\n\t\tpayload += len;\n\t\tfilemod_data->dst_filepath_length = len;\n\t}\n\n\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &filemod_data->meta);\n\tunsigned long data_len = payload - (void*)filemod_data;\n\tdata_len = data_len > sizeof(*filemod_data) ? sizeof(*filemod_data) : data_len;\n\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, filemod_data, data_len);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\n\nSEC(\"kprobe/vfs_symlink\")\nint BPF_KPROBE(kprobe__vfs_symlink, struct inode* dir, struct dentry* dentry,\n\t       const char* oldname)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_vfs_symlink);\n\n\tu32 dst_device_id = 0;\n\tu64 dst_file_ino = 0;\n\tif (!is_dentry_allowed_for_filemod(dentry, &dst_device_id, &dst_file_ino))\n\t\tgoto out;\n\n\tint zero = 0;\n\tstruct var_filemod_data_t* filemod_data = bpf_map_lookup_elem(&data_heap, &zero);\n\tif (!filemod_data)\n\t\tgoto out;\n\n\tu32 pid = get_userspace_pid();\n\tstruct task_struct* task = (struct task_struct*)bpf_get_current_task();\n\n\tfilemod_data->meta.type = FILEMOD_EVENT;\n\tfilemod_data->fmod_type = FMOD_SYMLINK;\n\tfilemod_data->dst_flags = 0;\n\tfilemod_data->src_inode = 0;\n\tfilemod_data->dst_inode = dst_file_ino;\n\tfilemod_data->src_device_id = 0;\n\tfilemod_data->dst_device_id = dst_device_id;\n\tfilemod_data->src_filepath_length = 0;\n\tfilemod_data->dst_filepath_length = 0;\n\n\tvoid* payload = populate_var_metadata(&filemod_data->meta, task, pid,\n\t\t\t\t\t      filemod_data->payload);\n\tpayload = populate_cgroup_info(&filemod_data->cgroup_data, task, payload);\n\n\tsize_t len = bpf_probe_read_kernel_str(payload, MAX_FILEPATH_LENGTH,\n\t\t\t\t\t       oldname);\n\tbarrier_var(len);\n\tif (len <= MAX_FILEPATH_LENGTH) {\n\t\tbarrier_var(len);\n\t\tpayload += len;\n\t\tfilemod_data->src_filepath_length = len;\n\t}\n\tlen = read_absolute_file_path_from_dentry(dentry, payload);\n\tbarrier_var(len);\n\tif (len <= MAX_FILEPATH_LENGTH) {\n\t\tbarrier_var(len);\n\t\tpayload += len;\n\t\tfilemod_data->dst_filepath_length = len;\n\t}\n\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &filemod_data->meta);\n\tunsigned long data_len = payload - (void*)filemod_data;\n\tdata_len = data_len > sizeof(*filemod_data) ? sizeof(*filemod_data) : data_len;\n\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, filemod_data, data_len);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\n\nSEC(\"raw_tracepoint/sched_process_fork\")\nint raw_tracepoint__sched_process_fork(struct bpf_raw_tracepoint_args* ctx)\n{\n\tstruct bpf_func_stats_ctx stats_ctx;\n\tbpf_stats_enter(&stats_ctx, profiler_bpf_sched_process_fork);\n\n\tint zero = 0;\n\tstruct var_fork_data_t* fork_data = bpf_map_lookup_elem(&data_heap, &zero);\n\tif (!fork_data)\n\t\tgoto out;\n\n\tstruct task_struct* parent = (struct task_struct*)ctx->args[0];\n\tstruct task_struct* child = (struct task_struct*)ctx->args[1];\n\tfork_data->meta.type = FORK_EVENT;\n\n\tvoid* payload = populate_var_metadata(&fork_data->meta, child,\n\t\t\t\t\t      BPF_CORE_READ(child, pid), fork_data->payload);\n\tfork_data->parent_pid = BPF_CORE_READ(parent, pid);\n\tfork_data->parent_exec_id = BPF_CORE_READ(parent, self_exec_id);\n\tfork_data->parent_start_time = BPF_CORE_READ(parent, start_time);\n\tbpf_stats_pre_submit_var_perf_event(&stats_ctx, &fork_data->meta);\n\n\tunsigned long data_len = payload - (void*)fork_data;\n\tdata_len = data_len > sizeof(*fork_data) ? sizeof(*fork_data) : data_len;\n\tbpf_perf_event_output(ctx, &events, BPF_F_CURRENT_CPU, fork_data, data_len);\nout:\n\tbpf_stats_exit(&stats_ctx);\n\treturn 0;\n}\nchar _license[] SEC(\"license\") = \"GPL\";\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}