{
  "module_name": "xsk.h",
  "hash_id": "1b18c8814ea015597b10be5eff8f4f647062ff20d52d150f999380f0b2e2ce30",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/bpf/xsk.h",
  "human_readable_source": " \n\n \n\n#ifndef __XSK_H\n#define __XSK_H\n\n#include <stdio.h>\n#include <stdint.h>\n#include <stdbool.h>\n#include <linux/if_xdp.h>\n\n#include <bpf/libbpf.h>\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n \n#define DEFINE_XSK_RING(name) \\\nstruct name { \\\n\t__u32 cached_prod; \\\n\t__u32 cached_cons; \\\n\t__u32 mask; \\\n\t__u32 size; \\\n\t__u32 *producer; \\\n\t__u32 *consumer; \\\n\tvoid *ring; \\\n\t__u32 *flags; \\\n}\n\nDEFINE_XSK_RING(xsk_ring_prod);\nDEFINE_XSK_RING(xsk_ring_cons);\n\n \n\nstruct xsk_umem;\nstruct xsk_socket;\n\nstatic inline __u64 *xsk_ring_prod__fill_addr(struct xsk_ring_prod *fill,\n\t\t\t\t\t      __u32 idx)\n{\n\t__u64 *addrs = (__u64 *)fill->ring;\n\n\treturn &addrs[idx & fill->mask];\n}\n\nstatic inline const __u64 *\nxsk_ring_cons__comp_addr(const struct xsk_ring_cons *comp, __u32 idx)\n{\n\tconst __u64 *addrs = (const __u64 *)comp->ring;\n\n\treturn &addrs[idx & comp->mask];\n}\n\nstatic inline struct xdp_desc *xsk_ring_prod__tx_desc(struct xsk_ring_prod *tx,\n\t\t\t\t\t\t      __u32 idx)\n{\n\tstruct xdp_desc *descs = (struct xdp_desc *)tx->ring;\n\n\treturn &descs[idx & tx->mask];\n}\n\nstatic inline const struct xdp_desc *\nxsk_ring_cons__rx_desc(const struct xsk_ring_cons *rx, __u32 idx)\n{\n\tconst struct xdp_desc *descs = (const struct xdp_desc *)rx->ring;\n\n\treturn &descs[idx & rx->mask];\n}\n\nstatic inline int xsk_ring_prod__needs_wakeup(const struct xsk_ring_prod *r)\n{\n\treturn *r->flags & XDP_RING_NEED_WAKEUP;\n}\n\nstatic inline __u32 xsk_prod_nb_free(struct xsk_ring_prod *r, __u32 nb)\n{\n\t__u32 free_entries = r->cached_cons - r->cached_prod;\n\n\tif (free_entries >= nb)\n\t\treturn free_entries;\n\n\t \n\tr->cached_cons = __atomic_load_n(r->consumer, __ATOMIC_ACQUIRE);\n\tr->cached_cons += r->size;\n\n\treturn r->cached_cons - r->cached_prod;\n}\n\nstatic inline __u32 xsk_cons_nb_avail(struct xsk_ring_cons *r, __u32 nb)\n{\n\t__u32 entries = r->cached_prod - r->cached_cons;\n\n\tif (entries == 0) {\n\t\tr->cached_prod = __atomic_load_n(r->producer, __ATOMIC_ACQUIRE);\n\t\tentries = r->cached_prod - r->cached_cons;\n\t}\n\n\treturn (entries > nb) ? nb : entries;\n}\n\nstatic inline __u32 xsk_ring_prod__reserve(struct xsk_ring_prod *prod, __u32 nb, __u32 *idx)\n{\n\tif (xsk_prod_nb_free(prod, nb) < nb)\n\t\treturn 0;\n\n\t*idx = prod->cached_prod;\n\tprod->cached_prod += nb;\n\n\treturn nb;\n}\n\nstatic inline void xsk_ring_prod__submit(struct xsk_ring_prod *prod, __u32 nb)\n{\n\t \n\t__atomic_store_n(prod->producer, *prod->producer + nb, __ATOMIC_RELEASE);\n}\n\nstatic inline void xsk_ring_prod__cancel(struct xsk_ring_prod *prod, __u32 nb)\n{\n\tprod->cached_prod -= nb;\n}\n\nstatic inline __u32 xsk_ring_cons__peek(struct xsk_ring_cons *cons, __u32 nb, __u32 *idx)\n{\n\t__u32 entries = xsk_cons_nb_avail(cons, nb);\n\n\tif (entries > 0) {\n\t\t*idx = cons->cached_cons;\n\t\tcons->cached_cons += entries;\n\t}\n\n\treturn entries;\n}\n\nstatic inline void xsk_ring_cons__cancel(struct xsk_ring_cons *cons, __u32 nb)\n{\n\tcons->cached_cons -= nb;\n}\n\nstatic inline void xsk_ring_cons__release(struct xsk_ring_cons *cons, __u32 nb)\n{\n\t \n\t__atomic_store_n(cons->consumer, *cons->consumer + nb, __ATOMIC_RELEASE);\n}\n\nstatic inline void *xsk_umem__get_data(void *umem_area, __u64 addr)\n{\n\treturn &((char *)umem_area)[addr];\n}\n\nstatic inline __u64 xsk_umem__extract_addr(__u64 addr)\n{\n\treturn addr & XSK_UNALIGNED_BUF_ADDR_MASK;\n}\n\nstatic inline __u64 xsk_umem__extract_offset(__u64 addr)\n{\n\treturn addr >> XSK_UNALIGNED_BUF_OFFSET_SHIFT;\n}\n\nstatic inline __u64 xsk_umem__add_offset_to_addr(__u64 addr)\n{\n\treturn xsk_umem__extract_addr(addr) + xsk_umem__extract_offset(addr);\n}\n\nint xsk_umem__fd(const struct xsk_umem *umem);\nint xsk_socket__fd(const struct xsk_socket *xsk);\n\n#define XSK_RING_CONS__DEFAULT_NUM_DESCS      2048\n#define XSK_RING_PROD__DEFAULT_NUM_DESCS      2048\n#define XSK_UMEM__DEFAULT_FRAME_SHIFT    12  \n#define XSK_UMEM__DEFAULT_FRAME_SIZE     (1 << XSK_UMEM__DEFAULT_FRAME_SHIFT)\n#define XSK_UMEM__DEFAULT_FRAME_HEADROOM 0\n#define XSK_UMEM__DEFAULT_FLAGS 0\n\nstruct xsk_umem_config {\n\t__u32 fill_size;\n\t__u32 comp_size;\n\t__u32 frame_size;\n\t__u32 frame_headroom;\n\t__u32 flags;\n};\n\nint xsk_attach_xdp_program(struct bpf_program *prog, int ifindex, u32 xdp_flags);\nvoid xsk_detach_xdp_program(int ifindex, u32 xdp_flags);\nint xsk_update_xskmap(struct bpf_map *map, struct xsk_socket *xsk);\nvoid xsk_clear_xskmap(struct bpf_map *map);\nbool xsk_is_in_mode(u32 ifindex, int mode);\n\nstruct xsk_socket_config {\n\t__u32 rx_size;\n\t__u32 tx_size;\n\t__u16 bind_flags;\n};\n\n \nint xsk_umem__create(struct xsk_umem **umem,\n\t\t     void *umem_area, __u64 size,\n\t\t     struct xsk_ring_prod *fill,\n\t\t     struct xsk_ring_cons *comp,\n\t\t     const struct xsk_umem_config *config);\nint xsk_socket__create(struct xsk_socket **xsk,\n\t\t       int ifindex, __u32 queue_id,\n\t\t       struct xsk_umem *umem,\n\t\t       struct xsk_ring_cons *rx,\n\t\t       struct xsk_ring_prod *tx,\n\t\t       const struct xsk_socket_config *config);\nint xsk_socket__create_shared(struct xsk_socket **xsk_ptr,\n\t\t\t      int ifindex,\n\t\t\t      __u32 queue_id, struct xsk_umem *umem,\n\t\t\t      struct xsk_ring_cons *rx,\n\t\t\t      struct xsk_ring_prod *tx,\n\t\t\t      struct xsk_ring_prod *fill,\n\t\t\t      struct xsk_ring_cons *comp,\n\t\t\t      const struct xsk_socket_config *config);\n\n \nint xsk_umem__delete(struct xsk_umem *umem);\nvoid xsk_socket__delete(struct xsk_socket *xsk);\n\nint xsk_set_mtu(int ifindex, int mtu);\n\n#ifdef __cplusplus\n}  \n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}