{
  "module_name": "unpriv_bpf_disabled.c",
  "hash_id": "532c81a4a46396d3f8fbe5a144c38d04e9ec71bd10b34e67a7fe9c640643d99e",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/bpf/prog_tests/unpriv_bpf_disabled.c",
  "human_readable_source": "\n \n\n#include <test_progs.h>\n#include <bpf/btf.h>\n\n#include \"test_unpriv_bpf_disabled.skel.h\"\n\n#include \"cap_helpers.h\"\n\n \n#define ALL_CAPS\t((2ULL << CAP_BPF) - 1)\n\n#define PINPATH\t\t\"/sys/fs/bpf/unpriv_bpf_disabled_\"\n#define NUM_MAPS\t7\n\nstatic __u32 got_perfbuf_val;\nstatic __u32 got_ringbuf_val;\n\nstatic int process_ringbuf(void *ctx, void *data, size_t len)\n{\n\tif (ASSERT_EQ(len, sizeof(__u32), \"ringbuf_size_valid\"))\n\t\tgot_ringbuf_val = *(__u32 *)data;\n\treturn 0;\n}\n\nstatic void process_perfbuf(void *ctx, int cpu, void *data, __u32 len)\n{\n\tif (ASSERT_EQ(len, sizeof(__u32), \"perfbuf_size_valid\"))\n\t\tgot_perfbuf_val = *(__u32 *)data;\n}\n\nstatic int sysctl_set(const char *sysctl_path, char *old_val, const char *new_val)\n{\n\tint ret = 0;\n\tFILE *fp;\n\n\tfp = fopen(sysctl_path, \"r+\");\n\tif (!fp)\n\t\treturn -errno;\n\tif (old_val && fscanf(fp, \"%s\", old_val) <= 0) {\n\t\tret = -ENOENT;\n\t} else if (!old_val || strcmp(old_val, new_val) != 0) {\n\t\tfseek(fp, 0, SEEK_SET);\n\t\tif (fprintf(fp, \"%s\", new_val) < 0)\n\t\t\tret = -errno;\n\t}\n\tfclose(fp);\n\n\treturn ret;\n}\n\nstatic void test_unpriv_bpf_disabled_positive(struct test_unpriv_bpf_disabled *skel,\n\t\t\t\t\t      __u32 prog_id, int prog_fd, int perf_fd,\n\t\t\t\t\t      char **map_paths, int *map_fds)\n{\n\tstruct perf_buffer *perfbuf = NULL;\n\tstruct ring_buffer *ringbuf = NULL;\n\tint i, nr_cpus, link_fd = -1;\n\n\tnr_cpus = bpf_num_possible_cpus();\n\n\tskel->bss->perfbuf_val = 1;\n\tskel->bss->ringbuf_val = 2;\n\n\t \n\tperfbuf = perf_buffer__new(bpf_map__fd(skel->maps.perfbuf), 8, process_perfbuf, NULL, NULL,\n\t\t\t\t   NULL);\n\tif (!ASSERT_OK_PTR(perfbuf, \"perf_buffer__new\"))\n\t\tgoto cleanup;\n\n\tringbuf = ring_buffer__new(bpf_map__fd(skel->maps.ringbuf), process_ringbuf, NULL, NULL);\n\tif (!ASSERT_OK_PTR(ringbuf, \"ring_buffer__new\"))\n\t\tgoto cleanup;\n\n\t \n\tusleep(1);\n\n\tASSERT_GT(perf_buffer__poll(perfbuf, 100), -1, \"perf_buffer__poll\");\n\tASSERT_EQ(got_perfbuf_val, skel->bss->perfbuf_val, \"check_perfbuf_val\");\n\tASSERT_EQ(ring_buffer__consume(ringbuf), 1, \"ring_buffer__consume\");\n\tASSERT_EQ(got_ringbuf_val, skel->bss->ringbuf_val, \"check_ringbuf_val\");\n\n\tfor (i = 0; i < NUM_MAPS; i++) {\n\t\tmap_fds[i] = bpf_obj_get(map_paths[i]);\n\t\tif (!ASSERT_GT(map_fds[i], -1, \"obj_get\"))\n\t\t\tgoto cleanup;\n\t}\n\n\tfor (i = 0; i < NUM_MAPS; i++) {\n\t\tbool prog_array = strstr(map_paths[i], \"prog_array\") != NULL;\n\t\tbool array = strstr(map_paths[i], \"array\") != NULL;\n\t\tbool buf = strstr(map_paths[i], \"buf\") != NULL;\n\t\t__u32 key = 0, vals[nr_cpus], lookup_vals[nr_cpus];\n\t\t__u32 expected_val = 1;\n\t\tint j;\n\n\t\t \n\t\tif (buf)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < nr_cpus; j++)\n\t\t\tvals[j] = expected_val;\n\n\t\tif (prog_array) {\n\t\t\t \n\t\t\tvals[0] = prog_fd;\n\t\t\t \n\t\t\texpected_val = prog_id;\n\t\t}\n\t\tASSERT_OK(bpf_map_update_elem(map_fds[i], &key, vals, 0), \"map_update_elem\");\n\t\tASSERT_OK(bpf_map_lookup_elem(map_fds[i], &key, &lookup_vals), \"map_lookup_elem\");\n\t\tASSERT_EQ(lookup_vals[0], expected_val, \"map_lookup_elem_values\");\n\t\tif (!array)\n\t\t\tASSERT_OK(bpf_map_delete_elem(map_fds[i], &key), \"map_delete_elem\");\n\t}\n\n\tlink_fd = bpf_link_create(bpf_program__fd(skel->progs.handle_perf_event), perf_fd,\n\t\t\t\t  BPF_PERF_EVENT, NULL);\n\tASSERT_GT(link_fd, 0, \"link_create\");\n\ncleanup:\n\tif (link_fd)\n\t\tclose(link_fd);\n\tif (perfbuf)\n\t\tperf_buffer__free(perfbuf);\n\tif (ringbuf)\n\t\tring_buffer__free(ringbuf);\n}\n\nstatic void test_unpriv_bpf_disabled_negative(struct test_unpriv_bpf_disabled *skel,\n\t\t\t\t\t      __u32 prog_id, int prog_fd, int perf_fd,\n\t\t\t\t\t      char **map_paths, int *map_fds)\n{\n\tconst struct bpf_insn prog_insns[] = {\n\t\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\t\tBPF_EXIT_INSN(),\n\t};\n\tconst size_t prog_insn_cnt = sizeof(prog_insns) / sizeof(struct bpf_insn);\n\tLIBBPF_OPTS(bpf_prog_load_opts, load_opts);\n\tstruct bpf_map_info map_info = {};\n\t__u32 map_info_len = sizeof(map_info);\n\tstruct bpf_link_info link_info = {};\n\t__u32 link_info_len = sizeof(link_info);\n\tstruct btf *btf = NULL;\n\t__u32 attach_flags = 0;\n\t__u32 prog_ids[3] = {};\n\t__u32 prog_cnt = 3;\n\t__u32 next;\n\tint i;\n\n\t \n\tASSERT_EQ(bpf_prog_load(BPF_PROG_TYPE_SOCKET_FILTER, \"simple_prog\", \"GPL\",\n\t\t\t\tprog_insns, prog_insn_cnt, &load_opts),\n\t\t  -EPERM, \"prog_load_fails\");\n\n\t \n\tfor (i = BPF_MAP_TYPE_HASH; i <= BPF_MAP_TYPE_ARRAY; i++)\n\t\tASSERT_EQ(bpf_map_create(i, NULL, sizeof(int), sizeof(int), 1, NULL),\n\t\t\t  -EPERM, \"map_create_fails\");\n\n\tASSERT_EQ(bpf_prog_get_fd_by_id(prog_id), -EPERM, \"prog_get_fd_by_id_fails\");\n\tASSERT_EQ(bpf_prog_get_next_id(prog_id, &next), -EPERM, \"prog_get_next_id_fails\");\n\tASSERT_EQ(bpf_prog_get_next_id(0, &next), -EPERM, \"prog_get_next_id_fails\");\n\n\tif (ASSERT_OK(bpf_map_get_info_by_fd(map_fds[0], &map_info, &map_info_len),\n\t\t      \"obj_get_info_by_fd\")) {\n\t\tASSERT_EQ(bpf_map_get_fd_by_id(map_info.id), -EPERM, \"map_get_fd_by_id_fails\");\n\t\tASSERT_EQ(bpf_map_get_next_id(map_info.id, &next), -EPERM,\n\t\t\t  \"map_get_next_id_fails\");\n\t}\n\tASSERT_EQ(bpf_map_get_next_id(0, &next), -EPERM, \"map_get_next_id_fails\");\n\n\tif (ASSERT_OK(bpf_link_get_info_by_fd(bpf_link__fd(skel->links.sys_nanosleep_enter),\n\t\t\t\t\t      &link_info, &link_info_len),\n\t\t      \"obj_get_info_by_fd\")) {\n\t\tASSERT_EQ(bpf_link_get_fd_by_id(link_info.id), -EPERM, \"link_get_fd_by_id_fails\");\n\t\tASSERT_EQ(bpf_link_get_next_id(link_info.id, &next), -EPERM,\n\t\t\t  \"link_get_next_id_fails\");\n\t}\n\tASSERT_EQ(bpf_link_get_next_id(0, &next), -EPERM, \"link_get_next_id_fails\");\n\n\tASSERT_EQ(bpf_prog_query(prog_fd, BPF_TRACE_FENTRY, 0, &attach_flags, prog_ids,\n\t\t\t\t &prog_cnt), -EPERM, \"prog_query_fails\");\n\n\tbtf = btf__new_empty();\n\tif (ASSERT_OK_PTR(btf, \"empty_btf\") &&\n\t    ASSERT_GT(btf__add_int(btf, \"int\", 4, 0), 0, \"unpriv_int_type\")) {\n\t\tconst void *raw_btf_data;\n\t\t__u32 raw_btf_size;\n\n\t\traw_btf_data = btf__raw_data(btf, &raw_btf_size);\n\t\tif (ASSERT_OK_PTR(raw_btf_data, \"raw_btf_data_good\"))\n\t\t\tASSERT_EQ(bpf_btf_load(raw_btf_data, raw_btf_size, NULL), -EPERM,\n\t\t\t\t  \"bpf_btf_load_fails\");\n\t}\n\tbtf__free(btf);\n}\n\nvoid test_unpriv_bpf_disabled(void)\n{\n\tchar *map_paths[NUM_MAPS] = {\tPINPATH\t\"array\",\n\t\t\t\t\tPINPATH \"percpu_array\",\n\t\t\t\t\tPINPATH \"hash\",\n\t\t\t\t\tPINPATH \"percpu_hash\",\n\t\t\t\t\tPINPATH \"perfbuf\",\n\t\t\t\t\tPINPATH \"ringbuf\",\n\t\t\t\t\tPINPATH \"prog_array\" };\n\tint map_fds[NUM_MAPS];\n\tstruct test_unpriv_bpf_disabled *skel;\n\tchar unprivileged_bpf_disabled_orig[32] = {};\n\tchar perf_event_paranoid_orig[32] = {};\n\tstruct bpf_prog_info prog_info = {};\n\t__u32 prog_info_len = sizeof(prog_info);\n\tstruct perf_event_attr attr = {};\n\tint prog_fd, perf_fd = -1, i, ret;\n\t__u64 save_caps = 0;\n\t__u32 prog_id;\n\n\tskel = test_unpriv_bpf_disabled__open_and_load();\n\tif (!ASSERT_OK_PTR(skel, \"skel_open\"))\n\t\treturn;\n\n\tskel->bss->test_pid = getpid();\n\n\tmap_fds[0] = bpf_map__fd(skel->maps.array);\n\tmap_fds[1] = bpf_map__fd(skel->maps.percpu_array);\n\tmap_fds[2] = bpf_map__fd(skel->maps.hash);\n\tmap_fds[3] = bpf_map__fd(skel->maps.percpu_hash);\n\tmap_fds[4] = bpf_map__fd(skel->maps.perfbuf);\n\tmap_fds[5] = bpf_map__fd(skel->maps.ringbuf);\n\tmap_fds[6] = bpf_map__fd(skel->maps.prog_array);\n\n\tfor (i = 0; i < NUM_MAPS; i++)\n\t\tASSERT_OK(bpf_obj_pin(map_fds[i], map_paths[i]), \"pin map_fd\");\n\n\t \n\tif (!ASSERT_OK(sysctl_set(\"/proc/sys/kernel/perf_event_paranoid\", perf_event_paranoid_orig,\n\t\t\t\t  \"-1\"),\n\t\t       \"set_perf_event_paranoid\"))\n\t\tgoto cleanup;\n\t \n\tret = sysctl_set(\"/proc/sys/kernel/unprivileged_bpf_disabled\",\n\t\t\t unprivileged_bpf_disabled_orig, \"2\");\n\tif (ret == -EPERM) {\n\t\t \n\t\tif (!ASSERT_OK(strcmp(unprivileged_bpf_disabled_orig, \"1\"),\n\t\t\t       \"unprivileged_bpf_disabled_on\"))\n\t\t\tgoto cleanup;\n\t} else {\n\t\tif (!ASSERT_OK(ret, \"set unprivileged_bpf_disabled\"))\n\t\t\tgoto cleanup;\n\t}\n\n\tprog_fd = bpf_program__fd(skel->progs.sys_nanosleep_enter);\n\tASSERT_OK(bpf_prog_get_info_by_fd(prog_fd, &prog_info, &prog_info_len),\n\t\t  \"obj_get_info_by_fd\");\n\tprog_id = prog_info.id;\n\tASSERT_GT(prog_id, 0, \"valid_prog_id\");\n\n\tattr.size = sizeof(attr);\n\tattr.type = PERF_TYPE_SOFTWARE;\n\tattr.config = PERF_COUNT_SW_CPU_CLOCK;\n\tattr.freq = 1;\n\tattr.sample_freq = 1000;\n\tperf_fd = syscall(__NR_perf_event_open, &attr, -1, 0, -1, PERF_FLAG_FD_CLOEXEC);\n\tif (!ASSERT_GE(perf_fd, 0, \"perf_fd\"))\n\t\tgoto cleanup;\n\n\tif (!ASSERT_OK(test_unpriv_bpf_disabled__attach(skel), \"skel_attach\"))\n\t\tgoto cleanup;\n\n\tif (!ASSERT_OK(cap_disable_effective(ALL_CAPS, &save_caps), \"disable caps\"))\n\t\tgoto cleanup;\n\n\tif (test__start_subtest(\"unpriv_bpf_disabled_positive\"))\n\t\ttest_unpriv_bpf_disabled_positive(skel, prog_id, prog_fd, perf_fd, map_paths,\n\t\t\t\t\t\t  map_fds);\n\n\tif (test__start_subtest(\"unpriv_bpf_disabled_negative\"))\n\t\ttest_unpriv_bpf_disabled_negative(skel, prog_id, prog_fd, perf_fd, map_paths,\n\t\t\t\t\t\t  map_fds);\n\ncleanup:\n\tclose(perf_fd);\n\tif (save_caps)\n\t\tcap_enable_effective(save_caps, NULL);\n\tif (strlen(perf_event_paranoid_orig) > 0)\n\t\tsysctl_set(\"/proc/sys/kernel/perf_event_paranoid\", NULL, perf_event_paranoid_orig);\n\tif (strlen(unprivileged_bpf_disabled_orig) > 0)\n\t\tsysctl_set(\"/proc/sys/kernel/unprivileged_bpf_disabled\", NULL,\n\t\t\t   unprivileged_bpf_disabled_orig);\n\tfor (i = 0; i < NUM_MAPS; i++)\n\t\tunlink(map_paths[i]);\n\ttest_unpriv_bpf_disabled__destroy(skel);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}