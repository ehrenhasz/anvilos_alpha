{
  "module_name": "bench_ringbufs.c",
  "hash_id": "32b313b20c0e93ee59bb36a4182adedb9a14584567b0b37bf15e6c47c0445dad",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/bpf/benchs/bench_ringbufs.c",
  "human_readable_source": "\n \n#include <asm/barrier.h>\n#include <linux/perf_event.h>\n#include <linux/ring_buffer.h>\n#include <sys/epoll.h>\n#include <sys/mman.h>\n#include <argp.h>\n#include <stdlib.h>\n#include \"bench.h\"\n#include \"ringbuf_bench.skel.h\"\n#include \"perfbuf_bench.skel.h\"\n\nstatic struct {\n\tbool back2back;\n\tint batch_cnt;\n\tbool sampled;\n\tint sample_rate;\n\tint ringbuf_sz;  \n\tbool ringbuf_use_output;  \n\tint perfbuf_sz;  \n} args = {\n\t.back2back = false,\n\t.batch_cnt = 500,\n\t.sampled = false,\n\t.sample_rate = 500,\n\t.ringbuf_sz = 512 * 1024,\n\t.ringbuf_use_output = false,\n\t.perfbuf_sz = 128,\n};\n\nenum {\n\tARG_RB_BACK2BACK = 2000,\n\tARG_RB_USE_OUTPUT = 2001,\n\tARG_RB_BATCH_CNT = 2002,\n\tARG_RB_SAMPLED = 2003,\n\tARG_RB_SAMPLE_RATE = 2004,\n};\n\nstatic const struct argp_option opts[] = {\n\t{ \"rb-b2b\", ARG_RB_BACK2BACK, NULL, 0, \"Back-to-back mode\"},\n\t{ \"rb-use-output\", ARG_RB_USE_OUTPUT, NULL, 0, \"Use bpf_ringbuf_output() instead of bpf_ringbuf_reserve()\"},\n\t{ \"rb-batch-cnt\", ARG_RB_BATCH_CNT, \"CNT\", 0, \"Set BPF-side record batch count\"},\n\t{ \"rb-sampled\", ARG_RB_SAMPLED, NULL, 0, \"Notification sampling\"},\n\t{ \"rb-sample-rate\", ARG_RB_SAMPLE_RATE, \"RATE\", 0, \"Notification sample rate\"},\n\t{},\n};\n\nstatic error_t parse_arg(int key, char *arg, struct argp_state *state)\n{\n\tswitch (key) {\n\tcase ARG_RB_BACK2BACK:\n\t\targs.back2back = true;\n\t\tbreak;\n\tcase ARG_RB_USE_OUTPUT:\n\t\targs.ringbuf_use_output = true;\n\t\tbreak;\n\tcase ARG_RB_BATCH_CNT:\n\t\targs.batch_cnt = strtol(arg, NULL, 10);\n\t\tif (args.batch_cnt < 0) {\n\t\t\tfprintf(stderr, \"Invalid batch count.\");\n\t\t\targp_usage(state);\n\t\t}\n\t\tbreak;\n\tcase ARG_RB_SAMPLED:\n\t\targs.sampled = true;\n\t\tbreak;\n\tcase ARG_RB_SAMPLE_RATE:\n\t\targs.sample_rate = strtol(arg, NULL, 10);\n\t\tif (args.sample_rate < 0) {\n\t\t\tfprintf(stderr, \"Invalid perfbuf sample rate.\");\n\t\t\targp_usage(state);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn ARGP_ERR_UNKNOWN;\n\t}\n\treturn 0;\n}\n\n \nconst struct argp bench_ringbufs_argp = {\n\t.options = opts,\n\t.parser = parse_arg,\n};\n\n \n\nstatic struct counter buf_hits;\n\nstatic inline void bufs_trigger_batch(void)\n{\n\t(void)syscall(__NR_getpgid);\n}\n\nstatic void bufs_validate(void)\n{\n\tif (env.consumer_cnt != 1) {\n\t\tfprintf(stderr, \"rb-libbpf benchmark needs one consumer!\\n\");\n\t\texit(1);\n\t}\n\n\tif (args.back2back && env.producer_cnt > 1) {\n\t\tfprintf(stderr, \"back-to-back mode makes sense only for single-producer case!\\n\");\n\t\texit(1);\n\t}\n}\n\nstatic void *bufs_sample_producer(void *input)\n{\n\tif (args.back2back) {\n\t\t \n\t\tbufs_trigger_batch();\n\t\treturn NULL;\n\t}\n\n\twhile (true)\n\t\tbufs_trigger_batch();\n\treturn NULL;\n}\n\nstatic struct ringbuf_libbpf_ctx {\n\tstruct ringbuf_bench *skel;\n\tstruct ring_buffer *ringbuf;\n} ringbuf_libbpf_ctx;\n\nstatic void ringbuf_libbpf_measure(struct bench_res *res)\n{\n\tstruct ringbuf_libbpf_ctx *ctx = &ringbuf_libbpf_ctx;\n\n\tres->hits = atomic_swap(&buf_hits.value, 0);\n\tres->drops = atomic_swap(&ctx->skel->bss->dropped, 0);\n}\n\nstatic struct ringbuf_bench *ringbuf_setup_skeleton(void)\n{\n\tstruct ringbuf_bench *skel;\n\n\tsetup_libbpf();\n\n\tskel = ringbuf_bench__open();\n\tif (!skel) {\n\t\tfprintf(stderr, \"failed to open skeleton\\n\");\n\t\texit(1);\n\t}\n\n\tskel->rodata->batch_cnt = args.batch_cnt;\n\tskel->rodata->use_output = args.ringbuf_use_output ? 1 : 0;\n\n\tif (args.sampled)\n\t\t \n\t\tskel->rodata->wakeup_data_size = args.sample_rate * 16;\n\n\tbpf_map__set_max_entries(skel->maps.ringbuf, args.ringbuf_sz);\n\n\tif (ringbuf_bench__load(skel)) {\n\t\tfprintf(stderr, \"failed to load skeleton\\n\");\n\t\texit(1);\n\t}\n\n\treturn skel;\n}\n\nstatic int buf_process_sample(void *ctx, void *data, size_t len)\n{\n\tatomic_inc(&buf_hits.value);\n\treturn 0;\n}\n\nstatic void ringbuf_libbpf_setup(void)\n{\n\tstruct ringbuf_libbpf_ctx *ctx = &ringbuf_libbpf_ctx;\n\tstruct bpf_link *link;\n\n\tctx->skel = ringbuf_setup_skeleton();\n\tctx->ringbuf = ring_buffer__new(bpf_map__fd(ctx->skel->maps.ringbuf),\n\t\t\t\t\tbuf_process_sample, NULL, NULL);\n\tif (!ctx->ringbuf) {\n\t\tfprintf(stderr, \"failed to create ringbuf\\n\");\n\t\texit(1);\n\t}\n\n\tlink = bpf_program__attach(ctx->skel->progs.bench_ringbuf);\n\tif (!link) {\n\t\tfprintf(stderr, \"failed to attach program!\\n\");\n\t\texit(1);\n\t}\n}\n\nstatic void *ringbuf_libbpf_consumer(void *input)\n{\n\tstruct ringbuf_libbpf_ctx *ctx = &ringbuf_libbpf_ctx;\n\n\twhile (ring_buffer__poll(ctx->ringbuf, -1) >= 0) {\n\t\tif (args.back2back)\n\t\t\tbufs_trigger_batch();\n\t}\n\tfprintf(stderr, \"ringbuf polling failed!\\n\");\n\treturn NULL;\n}\n\n \nstruct ringbuf_custom {\n\t__u64 *consumer_pos;\n\t__u64 *producer_pos;\n\t__u64 mask;\n\tvoid *data;\n\tint map_fd;\n};\n\nstatic struct ringbuf_custom_ctx {\n\tstruct ringbuf_bench *skel;\n\tstruct ringbuf_custom ringbuf;\n\tint epoll_fd;\n\tstruct epoll_event event;\n} ringbuf_custom_ctx;\n\nstatic void ringbuf_custom_measure(struct bench_res *res)\n{\n\tstruct ringbuf_custom_ctx *ctx = &ringbuf_custom_ctx;\n\n\tres->hits = atomic_swap(&buf_hits.value, 0);\n\tres->drops = atomic_swap(&ctx->skel->bss->dropped, 0);\n}\n\nstatic void ringbuf_custom_setup(void)\n{\n\tstruct ringbuf_custom_ctx *ctx = &ringbuf_custom_ctx;\n\tconst size_t page_size = getpagesize();\n\tstruct bpf_link *link;\n\tstruct ringbuf_custom *r;\n\tvoid *tmp;\n\tint err;\n\n\tctx->skel = ringbuf_setup_skeleton();\n\n\tctx->epoll_fd = epoll_create1(EPOLL_CLOEXEC);\n\tif (ctx->epoll_fd < 0) {\n\t\tfprintf(stderr, \"failed to create epoll fd: %d\\n\", -errno);\n\t\texit(1);\n\t}\n\n\tr = &ctx->ringbuf;\n\tr->map_fd = bpf_map__fd(ctx->skel->maps.ringbuf);\n\tr->mask = args.ringbuf_sz - 1;\n\n\t \n\ttmp = mmap(NULL, page_size, PROT_READ | PROT_WRITE, MAP_SHARED,\n\t\t   r->map_fd, 0);\n\tif (tmp == MAP_FAILED) {\n\t\tfprintf(stderr, \"failed to mmap consumer page: %d\\n\", -errno);\n\t\texit(1);\n\t}\n\tr->consumer_pos = tmp;\n\n\t \n\ttmp = mmap(NULL, page_size + 2 * args.ringbuf_sz, PROT_READ, MAP_SHARED,\n\t\t   r->map_fd, page_size);\n\tif (tmp == MAP_FAILED) {\n\t\tfprintf(stderr, \"failed to mmap data pages: %d\\n\", -errno);\n\t\texit(1);\n\t}\n\tr->producer_pos = tmp;\n\tr->data = tmp + page_size;\n\n\tctx->event.events = EPOLLIN;\n\terr = epoll_ctl(ctx->epoll_fd, EPOLL_CTL_ADD, r->map_fd, &ctx->event);\n\tif (err < 0) {\n\t\tfprintf(stderr, \"failed to epoll add ringbuf: %d\\n\", -errno);\n\t\texit(1);\n\t}\n\n\tlink = bpf_program__attach(ctx->skel->progs.bench_ringbuf);\n\tif (!link) {\n\t\tfprintf(stderr, \"failed to attach program\\n\");\n\t\texit(1);\n\t}\n}\n\n#define RINGBUF_BUSY_BIT (1 << 31)\n#define RINGBUF_DISCARD_BIT (1 << 30)\n#define RINGBUF_META_LEN 8\n\nstatic inline int roundup_len(__u32 len)\n{\n\t \n\tlen <<= 2;\n\tlen >>= 2;\n\t \n\tlen += RINGBUF_META_LEN;\n\t \n\treturn (len + 7) / 8 * 8;\n}\n\nstatic void ringbuf_custom_process_ring(struct ringbuf_custom *r)\n{\n\tunsigned long cons_pos, prod_pos;\n\tint *len_ptr, len;\n\tbool got_new_data;\n\n\tcons_pos = smp_load_acquire(r->consumer_pos);\n\twhile (true) {\n\t\tgot_new_data = false;\n\t\tprod_pos = smp_load_acquire(r->producer_pos);\n\t\twhile (cons_pos < prod_pos) {\n\t\t\tlen_ptr = r->data + (cons_pos & r->mask);\n\t\t\tlen = smp_load_acquire(len_ptr);\n\n\t\t\t \n\t\t\tif (len & RINGBUF_BUSY_BIT)\n\t\t\t\treturn;\n\n\t\t\tgot_new_data = true;\n\t\t\tcons_pos += roundup_len(len);\n\n\t\t\tatomic_inc(&buf_hits.value);\n\t\t}\n\t\tif (got_new_data)\n\t\t\tsmp_store_release(r->consumer_pos, cons_pos);\n\t\telse\n\t\t\tbreak;\n\t}\n}\n\nstatic void *ringbuf_custom_consumer(void *input)\n{\n\tstruct ringbuf_custom_ctx *ctx = &ringbuf_custom_ctx;\n\tint cnt;\n\n\tdo {\n\t\tif (args.back2back)\n\t\t\tbufs_trigger_batch();\n\t\tcnt = epoll_wait(ctx->epoll_fd, &ctx->event, 1, -1);\n\t\tif (cnt > 0)\n\t\t\tringbuf_custom_process_ring(&ctx->ringbuf);\n\t} while (cnt >= 0);\n\tfprintf(stderr, \"ringbuf polling failed!\\n\");\n\treturn 0;\n}\n\n \nstatic struct perfbuf_libbpf_ctx {\n\tstruct perfbuf_bench *skel;\n\tstruct perf_buffer *perfbuf;\n} perfbuf_libbpf_ctx;\n\nstatic void perfbuf_measure(struct bench_res *res)\n{\n\tstruct perfbuf_libbpf_ctx *ctx = &perfbuf_libbpf_ctx;\n\n\tres->hits = atomic_swap(&buf_hits.value, 0);\n\tres->drops = atomic_swap(&ctx->skel->bss->dropped, 0);\n}\n\nstatic struct perfbuf_bench *perfbuf_setup_skeleton(void)\n{\n\tstruct perfbuf_bench *skel;\n\n\tsetup_libbpf();\n\n\tskel = perfbuf_bench__open();\n\tif (!skel) {\n\t\tfprintf(stderr, \"failed to open skeleton\\n\");\n\t\texit(1);\n\t}\n\n\tskel->rodata->batch_cnt = args.batch_cnt;\n\n\tif (perfbuf_bench__load(skel)) {\n\t\tfprintf(stderr, \"failed to load skeleton\\n\");\n\t\texit(1);\n\t}\n\n\treturn skel;\n}\n\nstatic enum bpf_perf_event_ret\nperfbuf_process_sample_raw(void *input_ctx, int cpu,\n\t\t\t   struct perf_event_header *e)\n{\n\tswitch (e->type) {\n\tcase PERF_RECORD_SAMPLE:\n\t\tatomic_inc(&buf_hits.value);\n\t\tbreak;\n\tcase PERF_RECORD_LOST:\n\t\tbreak;\n\tdefault:\n\t\treturn LIBBPF_PERF_EVENT_ERROR;\n\t}\n\treturn LIBBPF_PERF_EVENT_CONT;\n}\n\nstatic void perfbuf_libbpf_setup(void)\n{\n\tstruct perfbuf_libbpf_ctx *ctx = &perfbuf_libbpf_ctx;\n\tstruct perf_event_attr attr;\n\tstruct bpf_link *link;\n\n\tctx->skel = perfbuf_setup_skeleton();\n\n\tmemset(&attr, 0, sizeof(attr));\n\tattr.config = PERF_COUNT_SW_BPF_OUTPUT;\n\tattr.type = PERF_TYPE_SOFTWARE;\n\tattr.sample_type = PERF_SAMPLE_RAW;\n\t \n\tif (args.sampled) {\n\t\tattr.sample_period = args.sample_rate;\n\t\tattr.wakeup_events = args.sample_rate;\n\t} else {\n\t\tattr.sample_period = 1;\n\t\tattr.wakeup_events = 1;\n\t}\n\n\tif (args.sample_rate > args.batch_cnt) {\n\t\tfprintf(stderr, \"sample rate %d is too high for given batch count %d\\n\",\n\t\t\targs.sample_rate, args.batch_cnt);\n\t\texit(1);\n\t}\n\n\tctx->perfbuf = perf_buffer__new_raw(bpf_map__fd(ctx->skel->maps.perfbuf),\n\t\t\t\t\t    args.perfbuf_sz, &attr,\n\t\t\t\t\t    perfbuf_process_sample_raw, NULL, NULL);\n\tif (!ctx->perfbuf) {\n\t\tfprintf(stderr, \"failed to create perfbuf\\n\");\n\t\texit(1);\n\t}\n\n\tlink = bpf_program__attach(ctx->skel->progs.bench_perfbuf);\n\tif (!link) {\n\t\tfprintf(stderr, \"failed to attach program\\n\");\n\t\texit(1);\n\t}\n}\n\nstatic void *perfbuf_libbpf_consumer(void *input)\n{\n\tstruct perfbuf_libbpf_ctx *ctx = &perfbuf_libbpf_ctx;\n\n\twhile (perf_buffer__poll(ctx->perfbuf, -1) >= 0) {\n\t\tif (args.back2back)\n\t\t\tbufs_trigger_batch();\n\t}\n\tfprintf(stderr, \"perfbuf polling failed!\\n\");\n\treturn NULL;\n}\n\n \n\n \nstruct perf_cpu_buf {\n\tstruct perf_buffer *pb;\n\tvoid *base;  \n\tvoid *buf;  \n\tsize_t buf_size;\n\tint fd;\n\tint cpu;\n\tint map_key;\n};\n\nstruct perf_buffer {\n\tperf_buffer_event_fn event_cb;\n\tperf_buffer_sample_fn sample_cb;\n\tperf_buffer_lost_fn lost_cb;\n\tvoid *ctx;  \n\n\tsize_t page_size;\n\tsize_t mmap_size;\n\tstruct perf_cpu_buf **cpu_bufs;\n\tstruct epoll_event *events;\n\tint cpu_cnt;  \n\tint epoll_fd;  \n\tint map_fd;  \n};\n\nstatic void *perfbuf_custom_consumer(void *input)\n{\n\tstruct perfbuf_libbpf_ctx *ctx = &perfbuf_libbpf_ctx;\n\tstruct perf_buffer *pb = ctx->perfbuf;\n\tstruct perf_cpu_buf *cpu_buf;\n\tstruct perf_event_mmap_page *header;\n\tsize_t mmap_mask = pb->mmap_size - 1;\n\tstruct perf_event_header *ehdr;\n\t__u64 data_head, data_tail;\n\tsize_t ehdr_size;\n\tvoid *base;\n\tint i, cnt;\n\n\twhile (true) {\n\t\tif (args.back2back)\n\t\t\tbufs_trigger_batch();\n\t\tcnt = epoll_wait(pb->epoll_fd, pb->events, pb->cpu_cnt, -1);\n\t\tif (cnt <= 0) {\n\t\t\tfprintf(stderr, \"perf epoll failed: %d\\n\", -errno);\n\t\t\texit(1);\n\t\t}\n\n\t\tfor (i = 0; i < cnt; ++i) {\n\t\t\tcpu_buf = pb->events[i].data.ptr;\n\t\t\theader = cpu_buf->base;\n\t\t\tbase = ((void *)header) + pb->page_size;\n\n\t\t\tdata_head = ring_buffer_read_head(header);\n\t\t\tdata_tail = header->data_tail;\n\t\t\twhile (data_head != data_tail) {\n\t\t\t\tehdr = base + (data_tail & mmap_mask);\n\t\t\t\tehdr_size = ehdr->size;\n\n\t\t\t\tif (ehdr->type == PERF_RECORD_SAMPLE)\n\t\t\t\t\tatomic_inc(&buf_hits.value);\n\n\t\t\t\tdata_tail += ehdr_size;\n\t\t\t}\n\t\t\tring_buffer_write_tail(header, data_tail);\n\t\t}\n\t}\n\treturn NULL;\n}\n\nconst struct bench bench_rb_libbpf = {\n\t.name = \"rb-libbpf\",\n\t.argp = &bench_ringbufs_argp,\n\t.validate = bufs_validate,\n\t.setup = ringbuf_libbpf_setup,\n\t.producer_thread = bufs_sample_producer,\n\t.consumer_thread = ringbuf_libbpf_consumer,\n\t.measure = ringbuf_libbpf_measure,\n\t.report_progress = hits_drops_report_progress,\n\t.report_final = hits_drops_report_final,\n};\n\nconst struct bench bench_rb_custom = {\n\t.name = \"rb-custom\",\n\t.argp = &bench_ringbufs_argp,\n\t.validate = bufs_validate,\n\t.setup = ringbuf_custom_setup,\n\t.producer_thread = bufs_sample_producer,\n\t.consumer_thread = ringbuf_custom_consumer,\n\t.measure = ringbuf_custom_measure,\n\t.report_progress = hits_drops_report_progress,\n\t.report_final = hits_drops_report_final,\n};\n\nconst struct bench bench_pb_libbpf = {\n\t.name = \"pb-libbpf\",\n\t.argp = &bench_ringbufs_argp,\n\t.validate = bufs_validate,\n\t.setup = perfbuf_libbpf_setup,\n\t.producer_thread = bufs_sample_producer,\n\t.consumer_thread = perfbuf_libbpf_consumer,\n\t.measure = perfbuf_measure,\n\t.report_progress = hits_drops_report_progress,\n\t.report_final = hits_drops_report_final,\n};\n\nconst struct bench bench_pb_custom = {\n\t.name = \"pb-custom\",\n\t.argp = &bench_ringbufs_argp,\n\t.validate = bufs_validate,\n\t.setup = perfbuf_libbpf_setup,\n\t.producer_thread = bufs_sample_producer,\n\t.consumer_thread = perfbuf_custom_consumer,\n\t.measure = perfbuf_measure,\n\t.report_progress = hits_drops_report_progress,\n\t.report_final = hits_drops_report_final,\n};\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}