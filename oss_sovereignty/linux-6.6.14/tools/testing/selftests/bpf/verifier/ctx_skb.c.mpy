{
  "module_name": "ctx_skb.c",
  "hash_id": "02668244e2101b8a10a08c9cd72a9cb73f17b4b4c016603caf99871d42bc2352",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/bpf/verifier/ctx_skb.c",
  "human_readable_source": "{\n\t\"access skb fields ok\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, len)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 1),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, mark)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 1),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, pkt_type)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 1),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, queue_mapping)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 0),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, protocol)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 0),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, vlan_present)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 0),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, vlan_tci)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 0),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, napi_id)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"access skb fields bad1\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1, -4),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"access skb fields bad2\",\n\t.insns = {\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_1, 0, 9),\n\tBPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),\n\tBPF_MOV64_REG(BPF_REG_2, BPF_REG_10),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),\n\tBPF_LD_MAP_FD(BPF_REG_1, 0),\n\tBPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n\tBPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),\n\tBPF_EXIT_INSN(),\n\tBPF_MOV64_REG(BPF_REG_1, BPF_REG_0),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, pkt_type)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.fixup_map_hash_8b = { 4 },\n\t.errstr = \"different pointers\",\n\t.errstr_unpriv = \"R1 pointer comparison\",\n\t.result = REJECT,\n},\n{\n\t\"access skb fields bad3\",\n\t.insns = {\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_1, 0, 2),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, pkt_type)),\n\tBPF_EXIT_INSN(),\n\tBPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),\n\tBPF_MOV64_REG(BPF_REG_2, BPF_REG_10),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),\n\tBPF_LD_MAP_FD(BPF_REG_1, 0),\n\tBPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n\tBPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),\n\tBPF_EXIT_INSN(),\n\tBPF_MOV64_REG(BPF_REG_1, BPF_REG_0),\n\tBPF_JMP_IMM(BPF_JA, 0, 0, -12),\n\t},\n\t.fixup_map_hash_8b = { 6 },\n\t.errstr = \"different pointers\",\n\t.errstr_unpriv = \"R1 pointer comparison\",\n\t.result = REJECT,\n},\n{\n\t\"access skb fields bad4\",\n\t.insns = {\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_1, 0, 3),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, len)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\tBPF_ST_MEM(BPF_DW, BPF_REG_10, -8, 0),\n\tBPF_MOV64_REG(BPF_REG_2, BPF_REG_10),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),\n\tBPF_LD_MAP_FD(BPF_REG_1, 0),\n\tBPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),\n\tBPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),\n\tBPF_EXIT_INSN(),\n\tBPF_MOV64_REG(BPF_REG_1, BPF_REG_0),\n\tBPF_JMP_IMM(BPF_JA, 0, 0, -13),\n\t},\n\t.fixup_map_hash_8b = { 7 },\n\t.errstr = \"different pointers\",\n\t.errstr_unpriv = \"R1 pointer comparison\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff family\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, family)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff remote_ip4\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip4)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff local_ip4\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip4)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff remote_ip6\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip6)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff local_ip6\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip6)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff remote_port\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_port)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"invalid access __sk_buff remote_port\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_port)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"valid access __sk_buff family\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, family)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"valid access __sk_buff remote_ip4\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip4)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"valid access __sk_buff local_ip4\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip4)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"valid access __sk_buff remote_ip6\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip6[0])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip6[1])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip6[2])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_ip6[3])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"valid access __sk_buff local_ip6\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip6[0])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip6[1])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip6[2])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_ip6[3])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"valid access __sk_buff remote_port\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, remote_port)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"valid access __sk_buff remote_port\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, local_port)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"invalid access of tc_classid for SK_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, tc_classid)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = REJECT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n\t.errstr = \"invalid bpf_context access\",\n},\n{\n\t\"invalid access of skb->mark for SK_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, mark)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result =  REJECT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n\t.errstr = \"invalid bpf_context access\",\n},\n{\n\t\"check skb->mark is not writeable by SK_SKB\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, mark)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result =  REJECT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n\t.errstr = \"invalid bpf_context access\",\n},\n{\n\t\"check skb->tc_index is writeable by SK_SKB\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, tc_index)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"check skb->priority is writeable by SK_SKB\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, priority)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"direct packet read for SK_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data)),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_3, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data_end)),\n\tBPF_MOV64_REG(BPF_REG_0, BPF_REG_2),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 8),\n\tBPF_JMP_REG(BPF_JGT, BPF_REG_0, BPF_REG_3, 1),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_2, 0),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"direct packet write for SK_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data)),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_3, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data_end)),\n\tBPF_MOV64_REG(BPF_REG_0, BPF_REG_2),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 8),\n\tBPF_JMP_REG(BPF_JGT, BPF_REG_0, BPF_REG_3, 1),\n\tBPF_STX_MEM(BPF_B, BPF_REG_2, BPF_REG_2, 0),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"overlapping checks for direct packet access SK_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data)),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_3, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data_end)),\n\tBPF_MOV64_REG(BPF_REG_0, BPF_REG_2),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_0, 8),\n\tBPF_JMP_REG(BPF_JGT, BPF_REG_0, BPF_REG_3, 4),\n\tBPF_MOV64_REG(BPF_REG_1, BPF_REG_2),\n\tBPF_ALU64_IMM(BPF_ADD, BPF_REG_1, 6),\n\tBPF_JMP_REG(BPF_JGT, BPF_REG_1, BPF_REG_3, 1),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_2, 6),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SK_SKB,\n},\n{\n\t\"check skb->mark is not writeable by sockets\",\n\t.insns = {\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, mark)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.errstr_unpriv = \"R1 leaks addr\",\n\t.result = REJECT,\n},\n{\n\t\"check skb->tc_index is not writeable by sockets\",\n\t.insns = {\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, tc_index)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.errstr_unpriv = \"R1 leaks addr\",\n\t.result = REJECT,\n},\n{\n\t\"check cb access: byte\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 1),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 2),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 3),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 1),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 2),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 3),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 1),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 2),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 3),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 1),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 2),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 3),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 1),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 2),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 3),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 1),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 2),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 3),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 1),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 2),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 3),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 1),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 2),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 3),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 1),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 2),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 3),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 1),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 2),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 3),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"__sk_buff->hash, offset 0, byte store not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, hash)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"__sk_buff->tc_index, offset 3, byte store not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, tc_index) + 3),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check skb->hash byte load permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash)),\n#else\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 3),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check skb->hash byte load permitted 1\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 1),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check skb->hash byte load permitted 2\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 2),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check skb->hash byte load permitted 3\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 3),\n#else\n\tBPF_LDX_MEM(BPF_B, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash)),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check cb access: byte, wrong type\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SOCK,\n},\n{\n\t\"check cb access: half\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 2),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 2),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 2),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 2),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 2),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 2),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1]) + 2),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2]) + 2),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3]) + 2),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 2),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check cb access: half, unaligned\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 1),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check __sk_buff->hash, offset 0, half store not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, hash)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check __sk_buff->tc_index, offset 2, half store not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, tc_index) + 2),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check skb->hash half load permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash)),\n#else\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 2),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check skb->hash half load permitted 2\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 2),\n#else\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash)),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check skb->hash half load not permitted, unaligned 1\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 1),\n#else\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 3),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n\t.flags = F_NEEDS_EFFICIENT_UNALIGNED_ACCESS,\n},\n{\n\t\"check skb->hash half load not permitted, unaligned 3\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 3),\n#else\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hash) + 1),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n\t.flags = F_NEEDS_EFFICIENT_UNALIGNED_ACCESS,\n},\n{\n\t\"check cb access: half, wrong type\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_H, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SOCK,\n},\n{\n\t\"check cb access: word\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check cb access: word, unaligned 1\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 2),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check cb access: word, unaligned 2\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 1),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check cb access: word, unaligned 3\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 2),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check cb access: word, unaligned 4\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4]) + 3),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check cb access: double\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n},\n{\n\t\"check cb access: double, unaligned 1\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[1])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check cb access: double, unaligned 2\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"misaligned context access\",\n\t.result = REJECT,\n\t.flags = F_LOAD_WITH_STRICT_ALIGNMENT,\n},\n{\n\t\"check cb access: double, oob 1\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check cb access: double, oob 2\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check __sk_buff->ifindex dw store not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, ifindex)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check __sk_buff->ifindex dw load not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, ifindex)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check cb access: double, wrong type\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SOCK,\n},\n{\n\t\"check out of range skb->cb access\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0]) + 256),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.errstr_unpriv = \"\",\n\t.result = REJECT,\n\t.prog_type = BPF_PROG_TYPE_SCHED_ACT,\n},\n{\n\t\"write skb fields from socket prog\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[4])),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 1),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, mark)),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, tc_index)),\n\tBPF_JMP_IMM(BPF_JGE, BPF_REG_0, 0, 1),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[2])),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.errstr_unpriv = \"R1 leaks addr\",\n\t.result_unpriv = REJECT,\n},\n{\n\t\"write skb fields from tc_cls_act prog\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, cb[0])),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, mark)),\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, tc_index)),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, tc_index)),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, cb[3])),\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, tstamp)),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, tstamp)),\n\tBPF_EXIT_INSN(),\n\t},\n\t.errstr_unpriv = \"\",\n\t.result_unpriv = REJECT,\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n},\n{\n\t\"check skb->data half load not permitted\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data)),\n#else\n\tBPF_LDX_MEM(BPF_H, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, data) + 2),\n#endif\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = REJECT,\n\t.errstr = \"invalid bpf_context access\",\n},\n{\n\t\"read gso_segs from CGROUP_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, gso_segs)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"read gso_segs from CGROUP_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, gso_segs)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"write gso_segs from CGROUP_SKB\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, gso_segs)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = REJECT,\n\t.result_unpriv = REJECT,\n\t.errstr = \"invalid bpf_context access off=164 size=4\",\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"read gso_segs from CLS\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, gso_segs)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n},\n{\n\t\"read gso_size from CGROUP_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, gso_size)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"read gso_size from CGROUP_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, gso_size)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"write gso_size from CGROUP_SKB\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, gso_size)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = REJECT,\n\t.result_unpriv = REJECT,\n\t.errstr = \"invalid bpf_context access off=176 size=4\",\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"read gso_size from CLS\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, gso_size)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n},\n{\n\t\"padding after gso_size is not accessible\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t    offsetofend(struct __sk_buff, gso_size)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = REJECT,\n\t.result_unpriv = REJECT,\n\t.errstr = \"invalid bpf_context access off=180 size=4\",\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n},\n{\n\t\"read hwtstamp from CGROUP_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hwtstamp)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"read hwtstamp from CGROUP_SKB\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_1, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hwtstamp)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"write hwtstamp from CGROUP_SKB\",\n\t.insns = {\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_STX_MEM(BPF_DW, BPF_REG_1, BPF_REG_0,\n\t\t    offsetof(struct __sk_buff, hwtstamp)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = REJECT,\n\t.result_unpriv = REJECT,\n\t.errstr = \"invalid bpf_context access off=184 size=8\",\n\t.prog_type = BPF_PROG_TYPE_CGROUP_SKB,\n},\n{\n\t\"read hwtstamp from CLS\",\n\t.insns = {\n\tBPF_LDX_MEM(BPF_DW, BPF_REG_0, BPF_REG_1,\n\t\t    offsetof(struct __sk_buff, hwtstamp)),\n\tBPF_MOV64_IMM(BPF_REG_0, 0),\n\tBPF_EXIT_INSN(),\n\t},\n\t.result = ACCEPT,\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n},\n{\n\t\"check wire_len is not readable by sockets\",\n\t.insns = {\n\t\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t\t    offsetof(struct __sk_buff, wire_len)),\n\t\tBPF_EXIT_INSN(),\n\t},\n\t.errstr = \"invalid bpf_context access\",\n\t.result = REJECT,\n},\n{\n\t\"check wire_len is readable by tc classifier\",\n\t.insns = {\n\t\tBPF_LDX_MEM(BPF_W, BPF_REG_0, BPF_REG_1,\n\t\t\t    offsetof(struct __sk_buff, wire_len)),\n\t\tBPF_EXIT_INSN(),\n\t},\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n\t.result = ACCEPT,\n},\n{\n\t\"check wire_len is not writable by tc classifier\",\n\t.insns = {\n\t\tBPF_STX_MEM(BPF_W, BPF_REG_1, BPF_REG_1,\n\t\t\t    offsetof(struct __sk_buff, wire_len)),\n\t\tBPF_EXIT_INSN(),\n\t},\n\t.prog_type = BPF_PROG_TYPE_SCHED_CLS,\n\t.errstr = \"invalid bpf_context access\",\n\t.errstr_unpriv = \"R1 leaks addr\",\n\t.result = REJECT,\n},\n{\n       \"pkt > pkt_end taken check\",\n       .insns = {\n       BPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_1,                \n                   offsetof(struct __sk_buff, data_end)),\n       BPF_LDX_MEM(BPF_W, BPF_REG_4, BPF_REG_1,                \n                   offsetof(struct __sk_buff, data)),\n       BPF_MOV64_REG(BPF_REG_3, BPF_REG_4),                    \n       BPF_ALU64_IMM(BPF_ADD, BPF_REG_3, 42),                  \n       BPF_MOV64_IMM(BPF_REG_1, 0),                            \n       BPF_JMP_REG(BPF_JGT, BPF_REG_3, BPF_REG_2, 2),          \n       BPF_ALU64_IMM(BPF_ADD, BPF_REG_4, 14),                  \n       BPF_MOV64_REG(BPF_REG_1, BPF_REG_4),                    \n       BPF_JMP_REG(BPF_JGT, BPF_REG_3, BPF_REG_2, 1),          \n       BPF_LDX_MEM(BPF_H, BPF_REG_2, BPF_REG_1, 9),            \n       BPF_MOV64_IMM(BPF_REG_0, 0),                            \n       BPF_EXIT_INSN(),                                        \n       },\n       .result = ACCEPT,\n       .prog_type = BPF_PROG_TYPE_SK_SKB,\n       .flags = F_NEEDS_EFFICIENT_UNALIGNED_ACCESS,\n},\n{\n       \"pkt_end < pkt taken check\",\n       .insns = {\n       BPF_LDX_MEM(BPF_W, BPF_REG_2, BPF_REG_1,                \n                   offsetof(struct __sk_buff, data_end)),\n       BPF_LDX_MEM(BPF_W, BPF_REG_4, BPF_REG_1,                \n                   offsetof(struct __sk_buff, data)),\n       BPF_MOV64_REG(BPF_REG_3, BPF_REG_4),                    \n       BPF_ALU64_IMM(BPF_ADD, BPF_REG_3, 42),                  \n       BPF_MOV64_IMM(BPF_REG_1, 0),                            \n       BPF_JMP_REG(BPF_JGT, BPF_REG_3, BPF_REG_2, 2),          \n       BPF_ALU64_IMM(BPF_ADD, BPF_REG_4, 14),                  \n       BPF_MOV64_REG(BPF_REG_1, BPF_REG_4),                    \n       BPF_JMP_REG(BPF_JLT, BPF_REG_2, BPF_REG_3, 1),          \n       BPF_LDX_MEM(BPF_H, BPF_REG_2, BPF_REG_1, 9),            \n       BPF_MOV64_IMM(BPF_REG_0, 0),                            \n       BPF_EXIT_INSN(),                                        \n       },\n       .result = ACCEPT,\n       .prog_type = BPF_PROG_TYPE_SK_SKB,\n       .flags = F_NEEDS_EFFICIENT_UNALIGNED_ACCESS,\n},\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}