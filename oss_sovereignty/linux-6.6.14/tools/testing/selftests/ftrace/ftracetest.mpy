{
  "module_name": "ftracetest",
  "hash_id": "884818e49800b78c35623936eed6a68f48555c72b6edaa65de00237a6227bb45",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/ftrace/ftracetest",
  "human_readable_source": "#!/bin/sh\n# SPDX-License-Identifier: GPL-2.0-only\n\n# ftracetest - Ftrace test shell scripts\n#\n# Copyright (C) Hitachi Ltd., 2014\n#  Written by Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>\n#\n\nusage() { # errno [message]\n[ ! -z \"$2\" ] && echo $2\necho \"Usage: ftracetest [options] [testcase(s)] [testcase-directory(s)]\"\necho \" Options:\"\necho \"\t\t-h|--help  Show help message\"\necho \"\t\t-k|--keep  Keep passed test logs\"\necho \"\t\t-K|--ktap  Output in KTAP format\"\necho \"\t\t-v|--verbose Increase verbosity of test messages\"\necho \"\t\t-vv        Alias of -v -v (Show all results in stdout)\"\necho \"\t\t-vvv       Alias of -v -v -v (Show all commands immediately)\"\necho \"\t\t--fail-unsupported Treat UNSUPPORTED as a failure\"\necho \"\t\t--fail-unresolved Treat UNRESOLVED as a failure\"\necho \"\t\t-d|--debug Debug mode (trace all shell commands)\"\necho \"\t\t-l|--logdir <dir> Save logs on the <dir>\"\necho \"\t\t            If <dir> is -, all logs output in console only\"\nexit $1\n}\n\n# default error\nerr_ret=1\n\n# kselftest skip code is 4\nerr_skip=4\n\n# umount required\nUMOUNT_DIR=\"\"\n\n# cgroup RT scheduling prevents chrt commands from succeeding, which\n# induces failures in test wakeup tests.  Disable for the duration of\n# the tests.\n\nreadonly sched_rt_runtime=/proc/sys/kernel/sched_rt_runtime_us\n\nsched_rt_runtime_orig=$(cat $sched_rt_runtime)\n\nsetup() {\n  echo -1 > $sched_rt_runtime\n}\n\ncleanup() {\n  echo $sched_rt_runtime_orig > $sched_rt_runtime\n  if [ -n \"${UMOUNT_DIR}\" ]; then\n    umount ${UMOUNT_DIR} ||:\n  fi\n}\n\nerrexit() { # message\n  echo \"Error: $1\" 1>&2\n  cleanup\n  exit $err_ret\n}\n\n# Ensuring user privilege\nif [ `id -u` -ne 0 ]; then\n  errexit \"this must be run by root user\"\nfi\n\nsetup\n\n# Utilities\nabsdir() { # file_path\n  (cd `dirname $1`; pwd)\n}\n\nabspath() {\n  echo `absdir $1`/`basename $1`\n}\n\nfind_testcases() { #directory\n  echo `find $1 -name \\*.tc | sort`\n}\n\nparse_opts() { # opts\n  local OPT_TEST_CASES=\n  local OPT_TEST_DIR=\n\n  while [ ! -z \"$1\" ]; do\n    case \"$1\" in\n    --help|-h)\n      usage 0\n    ;;\n    --keep|-k)\n      KEEP_LOG=1\n      shift 1\n    ;;\n    --ktap|-K)\n      KTAP=1\n      shift 1\n    ;;\n    --verbose|-v|-vv|-vvv)\n      if [ $VERBOSE -eq -1 ]; then\n\tusage \"--console can not use with --verbose\"\n      fi\n      VERBOSE=$((VERBOSE + 1))\n      [ $1 = '-vv' ] && VERBOSE=$((VERBOSE + 1))\n      [ $1 = '-vvv' ] && VERBOSE=$((VERBOSE + 2))\n      shift 1\n    ;;\n    --console)\n      if [ $VERBOSE -ne 0 ]; then\n\tusage \"--console can not use with --verbose\"\n      fi\n      VERBOSE=-1\n      shift 1\n    ;;\n    --debug|-d)\n      DEBUG=1\n      shift 1\n    ;;\n    --stop-fail)\n      STOP_FAILURE=1\n      shift 1\n    ;;\n    --fail-unsupported)\n      UNSUPPORTED_RESULT=1\n      shift 1\n    ;;\n    --fail-unresolved)\n      UNRESOLVED_RESULT=1\n      shift 1\n    ;;\n    --logdir|-l)\n      LOG_DIR=$2\n      LINK_PTR=\n      shift 2\n    ;;\n    *.tc)\n      if [ -f \"$1\" ]; then\n        OPT_TEST_CASES=\"$OPT_TEST_CASES `abspath $1`\"\n        shift 1\n      else\n        usage 1 \"$1 is not a testcase\"\n      fi\n      ;;\n    *)\n      if [ -d \"$1\" ]; then\n        OPT_TEST_DIR=`abspath $1`\n        OPT_TEST_CASES=\"$OPT_TEST_CASES `find_testcases $OPT_TEST_DIR`\"\n        shift 1\n      else\n        usage 1 \"Invalid option ($1)\"\n      fi\n    ;;\n    esac\n  done\n  if [ ! -z \"$OPT_TEST_CASES\" ]; then\n    TEST_CASES=$OPT_TEST_CASES\n  fi\n}\n\n# Parameters\nTRACING_DIR=`grep tracefs /proc/mounts | cut -f2 -d' ' | head -1`\nif [ -z \"$TRACING_DIR\" ]; then\n    DEBUGFS_DIR=`grep debugfs /proc/mounts | cut -f2 -d' ' | head -1`\n    if [ -z \"$DEBUGFS_DIR\" ]; then\n\t# If tracefs exists, then so does /sys/kernel/tracing\n\tif [ -d \"/sys/kernel/tracing\" ]; then\n\t    mount -t tracefs nodev /sys/kernel/tracing ||\n\t      errexit \"Failed to mount /sys/kernel/tracing\"\n\t    TRACING_DIR=\"/sys/kernel/tracing\"\n\t    UMOUNT_DIR=${TRACING_DIR}\n\t# If debugfs exists, then so does /sys/kernel/debug\n\telif [ -d \"/sys/kernel/debug\" ]; then\n\t    mount -t debugfs nodev /sys/kernel/debug ||\n\t      errexit \"Failed to mount /sys/kernel/debug\"\n\t    TRACING_DIR=\"/sys/kernel/debug/tracing\"\n\t    UMOUNT_DIR=${TRACING_DIR}\n\telse\n\t    err_ret=$err_skip\n\t    errexit \"debugfs and tracefs are not configured in this kernel\"\n\tfi\n    else\n\tTRACING_DIR=\"$DEBUGFS_DIR/tracing\"\n    fi\nfi\nif [ ! -d \"$TRACING_DIR\" ]; then\n    err_ret=$err_skip\n    errexit \"ftrace is not configured in this kernel\"\nfi\n\nTOP_DIR=`absdir $0`\nTEST_DIR=$TOP_DIR/test.d\nTEST_CASES=`find_testcases $TEST_DIR`\nLOG_TOP_DIR=$TOP_DIR/logs\nLOG_DATE=`date +%Y%m%d-%H%M%S`\nLOG_DIR=$LOG_TOP_DIR/$LOG_DATE/\nLINK_PTR=$LOG_TOP_DIR/latest\nKEEP_LOG=0\nKTAP=0\nDEBUG=0\nVERBOSE=0\nUNSUPPORTED_RESULT=0\nUNRESOLVED_RESULT=0\nSTOP_FAILURE=0\n# Parse command-line options\nparse_opts $*\n\n[ $DEBUG -ne 0 ] && set -x\n\n# Verify parameters\nif [ -z \"$TRACING_DIR\" -o ! -d \"$TRACING_DIR\" ]; then\n  errexit \"No ftrace directory found\"\nfi\n\n# Preparing logs\nif [ \"x$LOG_DIR\" = \"x-\" ]; then\n  LOG_FILE=\n  date\nelse\n  LOG_FILE=$LOG_DIR/ftracetest.log\n  mkdir -p $LOG_DIR || errexit \"Failed to make a log directory: $LOG_DIR\"\n  date > $LOG_FILE\n  if [ \"x-$LINK_PTR\" != \"x-\" ]; then\n    unlink $LINK_PTR\n    ln -fs $LOG_DATE $LINK_PTR\n  fi\nfi\n\n# Define text colors\n# Check available colors on the terminal, if any\nncolors=`tput colors 2>/dev/null || echo 0`\ncolor_reset=\ncolor_red=\ncolor_green=\ncolor_blue=\n# If stdout exists and number of colors is eight or more, use them\nif [ -t 1 -a \"$ncolors\" -ge 8 ]; then\n  color_reset=\"\\033[0m\"\n  color_red=\"\\033[31m\"\n  color_green=\"\\033[32m\"\n  color_blue=\"\\033[34m\"\nfi\n\nstrip_esc() {\n  # busybox sed implementation doesn't accept \"\\x1B\", so use [:cntrl:] instead.\n  sed -E \"s/[[:cntrl:]]\\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g\"\n}\n\nprlog() { # messages\n  newline=\"\\n\"\n  if [ \"$1\" = \"-n\" ] ; then\n    newline=\n    shift\n  fi\n  [ \"$KTAP\" != \"1\" ] && printf \"$*$newline\"\n  [ \"$LOG_FILE\" ] && printf \"$*$newline\" | strip_esc >> $LOG_FILE\n}\ncatlog() { #file\n  cat $1\n  [ \"$LOG_FILE\" ] && cat $1 | strip_esc >> $LOG_FILE\n}\nprlog \"=== Ftrace unit tests ===\"\n\n\n# Testcase management\n# Test result codes - Dejagnu extended code\nPASS=0\t# The test succeeded.\nFAIL=1\t# The test failed, but was expected to succeed.\nUNRESOLVED=2  # The test produced indeterminate results. (e.g. interrupted)\nUNTESTED=3    # The test was not run, currently just a placeholder.\nUNSUPPORTED=4 # The test failed because of lack of feature.\nXFAIL=5\t# The test failed, and was expected to fail.\n\n# Accumulations\nPASSED_CASES=\nFAILED_CASES=\nUNRESOLVED_CASES=\nUNTESTED_CASES=\nUNSUPPORTED_CASES=\nXFAILED_CASES=\nUNDEFINED_CASES=\nTOTAL_RESULT=0\n\nINSTANCE=\nCASENO=0\nCASENAME=\n\ntestcase() { # testfile\n  CASENO=$((CASENO+1))\n  CASENAME=`grep \"^#[ \\t]*description:\" $1 | cut -f2- -d:`\n}\n\ncheckreq() { # testfile\n  requires=`grep \"^#[ \\t]*requires:\" $1 | cut -f2- -d:`\n  # Use eval to pass quoted-patterns correctly.\n  eval check_requires \"$requires\"\n}\n\ntest_on_instance() { # testfile\n  grep -q \"^#[ \\t]*flags:.*instance\" $1\n}\n\nktaptest() { # result comment\n  if [ \"$KTAP\" != \"1\" ]; then\n    return\n  fi\n\n  local result=\n  if [ \"$1\" = \"1\" ]; then\n    result=\"ok\"\n  else\n    result=\"not ok\"\n  fi\n  shift\n\n  local comment=$*\n  if [ \"$comment\" != \"\" ]; then\n    comment=\"# $comment\"\n  fi\n\n  echo $result $CASENO $INSTANCE$CASENAME $comment\n}\n\neval_result() { # sigval\n  case $1 in\n    $PASS)\n      prlog \"\t[${color_green}PASS${color_reset}]\"\n      ktaptest 1\n      PASSED_CASES=\"$PASSED_CASES $CASENO\"\n      return 0\n    ;;\n    $FAIL)\n      prlog \"\t[${color_red}FAIL${color_reset}]\"\n      ktaptest 0\n      FAILED_CASES=\"$FAILED_CASES $CASENO\"\n      return 1 # this is a bug.\n    ;;\n    $UNRESOLVED)\n      prlog \"\t[${color_blue}UNRESOLVED${color_reset}]\"\n      ktaptest 0 UNRESOLVED\n      UNRESOLVED_CASES=\"$UNRESOLVED_CASES $CASENO\"\n      return $UNRESOLVED_RESULT # depends on use case\n    ;;\n    $UNTESTED)\n      prlog \"\t[${color_blue}UNTESTED${color_reset}]\"\n      ktaptest 1 SKIP\n      UNTESTED_CASES=\"$UNTESTED_CASES $CASENO\"\n      return 0\n    ;;\n    $UNSUPPORTED)\n      prlog \"\t[${color_blue}UNSUPPORTED${color_reset}]\"\n      ktaptest 1 SKIP\n      UNSUPPORTED_CASES=\"$UNSUPPORTED_CASES $CASENO\"\n      return $UNSUPPORTED_RESULT # depends on use case\n    ;;\n    $XFAIL)\n      prlog \"\t[${color_green}XFAIL${color_reset}]\"\n      ktaptest 1 XFAIL\n      XFAILED_CASES=\"$XFAILED_CASES $CASENO\"\n      return 0\n    ;;\n    *)\n      prlog \"\t[${color_blue}UNDEFINED${color_reset}]\"\n      ktaptest 0 error\n      UNDEFINED_CASES=\"$UNDEFINED_CASES $CASENO\"\n      return 1 # this must be a test bug\n    ;;\n  esac\n}\n\n# Signal handling for result codes\nSIG_RESULT=\nSIG_BASE=36\t# Use realtime signals\nSIG_PID=$$\n\nexit_pass () {\n  exit 0\n}\n\nSIG_FAIL=$((SIG_BASE + FAIL))\nexit_fail () {\n  exit 1\n}\ntrap 'SIG_RESULT=$FAIL' $SIG_FAIL\n\nSIG_UNRESOLVED=$((SIG_BASE + UNRESOLVED))\nexit_unresolved () {\n  kill -s $SIG_UNRESOLVED $SIG_PID\n  exit 0\n}\ntrap 'SIG_RESULT=$UNRESOLVED' $SIG_UNRESOLVED\n\nSIG_UNTESTED=$((SIG_BASE + UNTESTED))\nexit_untested () {\n  kill -s $SIG_UNTESTED $SIG_PID\n  exit 0\n}\ntrap 'SIG_RESULT=$UNTESTED' $SIG_UNTESTED\n\nSIG_UNSUPPORTED=$((SIG_BASE + UNSUPPORTED))\nexit_unsupported () {\n  kill -s $SIG_UNSUPPORTED $SIG_PID\n  exit 0\n}\ntrap 'SIG_RESULT=$UNSUPPORTED' $SIG_UNSUPPORTED\n\nSIG_XFAIL=$((SIG_BASE + XFAIL))\nexit_xfail () {\n  kill -s $SIG_XFAIL $SIG_PID\n  exit 0\n}\ntrap 'SIG_RESULT=$XFAIL' $SIG_XFAIL\n\n__run_test() { # testfile\n  # setup PID and PPID, $$ is not updated.\n  (cd $TRACING_DIR; read PID _ < /proc/self/stat; set -e; set -x;\n   checkreq $1; initialize_ftrace; . $1)\n  [ $? -ne 0 ] && kill -s $SIG_FAIL $SIG_PID\n}\n\n# Run one test case\nrun_test() { # testfile\n  local testname=`basename $1`\n  testcase $1\n  prlog -n \"[$CASENO]$INSTANCE$CASENAME\"\n  if [ ! -z \"$LOG_FILE\" ] ; then\n    local testlog=`mktemp $LOG_DIR/${CASENO}-${testname}-log.XXXXXX`\n  else\n    local testlog=/proc/self/fd/1\n  fi\n  export TMPDIR=`mktemp -d /tmp/ftracetest-dir.XXXXXX`\n  export FTRACETEST_ROOT=$TOP_DIR\n  echo \"execute$INSTANCE: \"$1 > $testlog\n  SIG_RESULT=0\n  if [ $VERBOSE -eq -1 ]; then\n    __run_test $1\n  elif [ -z \"$LOG_FILE\" ]; then\n    __run_test $1 2>&1\n  elif [ $VERBOSE -ge 3 ]; then\n    __run_test $1 | tee -a $testlog 2>&1\n  elif [ $VERBOSE -eq 2 ]; then\n    __run_test $1 2>> $testlog | tee -a $testlog\n  else\n    __run_test $1 >> $testlog 2>&1\n  fi\n  eval_result $SIG_RESULT\n  if [ $? -eq 0 ]; then\n    # Remove test log if the test was done as it was expected.\n    [ $KEEP_LOG -eq 0 -a ! -z \"$LOG_FILE\" ] && rm $testlog\n  else\n    [ $VERBOSE -eq 1 -o $VERBOSE -eq 2 ] && catlog $testlog\n    TOTAL_RESULT=1\n  fi\n  rm -rf $TMPDIR\n}\n\n# load in the helper functions\n. $TEST_DIR/functions\n\nif [ \"$KTAP\" = \"1\" ]; then\n  echo \"TAP version 13\"\n\n  casecount=`echo $TEST_CASES | wc -w`\n  for t in $TEST_CASES; do\n    test_on_instance $t || continue\n    casecount=$((casecount+1))\n  done\n  echo \"1..${casecount}\"\nfi\n\n# Main loop\nfor t in $TEST_CASES; do\n  run_test $t\n  if [ $STOP_FAILURE -ne 0 -a $TOTAL_RESULT -ne 0 ]; then\n    echo \"A failure detected. Stop test.\"\n    exit 1\n  fi\ndone\n\n# Test on instance loop\nINSTANCE=\" (instance) \"\nfor t in $TEST_CASES; do\n  test_on_instance $t || continue\n  SAVED_TRACING_DIR=$TRACING_DIR\n  export TRACING_DIR=`mktemp -d $TRACING_DIR/instances/ftracetest.XXXXXX`\n  run_test $t\n  rmdir $TRACING_DIR\n  TRACING_DIR=$SAVED_TRACING_DIR\n  if [ $STOP_FAILURE -ne 0 -a $TOTAL_RESULT -ne 0 ]; then\n    echo \"A failure detected. Stop test.\"\n    exit 1\n  fi\ndone\n(cd $TRACING_DIR; finish_ftrace) # for cleanup\n\nprlog \"\"\nprlog \"# of passed: \" `echo $PASSED_CASES | wc -w`\nprlog \"# of failed: \" `echo $FAILED_CASES | wc -w`\nprlog \"# of unresolved: \" `echo $UNRESOLVED_CASES | wc -w`\nprlog \"# of untested: \" `echo $UNTESTED_CASES | wc -w`\nprlog \"# of unsupported: \" `echo $UNSUPPORTED_CASES | wc -w`\nprlog \"# of xfailed: \" `echo $XFAILED_CASES | wc -w`\nprlog \"# of undefined(test bug): \" `echo $UNDEFINED_CASES | wc -w`\n\nif [ \"$KTAP\" = \"1\" ]; then\n  echo -n \"# Totals:\"\n  echo -n \" pass:\"`echo $PASSED_CASES | wc -w`\n  echo -n \" faii:\"`echo $FAILED_CASES | wc -w`\n  echo -n \" xfail:\"`echo $XFAILED_CASES | wc -w`\n  echo -n \" xpass:0\"\n  echo -n \" skip:\"`echo $UNTESTED_CASES $UNSUPPORTED_CASES | wc -w`\n  echo -n \" error:\"`echo $UNRESOLVED_CASES $UNDEFINED_CASES | wc -w`\n  echo\nfi\n\ncleanup\n\n# if no error, return 0\nexit $TOTAL_RESULT\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}