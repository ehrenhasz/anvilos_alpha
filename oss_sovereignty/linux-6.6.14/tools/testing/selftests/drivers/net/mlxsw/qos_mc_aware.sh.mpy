{
  "module_name": "qos_mc_aware.sh",
  "hash_id": "be0a5f7c31e25dbab977d64dc5a1c0b1965b470784ef70e38dc221d64ff878c6",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/drivers/net/mlxsw/qos_mc_aware.sh",
  "human_readable_source": "#!/bin/bash\n# SPDX-License-Identifier: GPL-2.0\n#\n# A test for switch behavior under MC overload. An issue in Spectrum chips\n# causes throughput of UC traffic to drop severely when a switch is under heavy\n# MC load. This issue can be overcome by putting the switch to MC-aware mode.\n# This test verifies that UC performance stays intact even as the switch is\n# under MC flood, and therefore that the MC-aware mode is enabled and correctly\n# configured.\n#\n# Because mlxsw throttles CPU port, the traffic can't actually reach userspace\n# at full speed. That makes it impossible to use iperf3 to simply measure the\n# throughput, because many packets (that reach $h3) don't get to the kernel at\n# all even in UDP mode (the situation is even worse in TCP mode, where one can't\n# hope to see more than a couple Mbps).\n#\n# So instead we send traffic with mausezahn and use RX ethtool counters at $h3.\n# Multicast traffic is untagged, unicast traffic is tagged with PCP 1. Therefore\n# each gets a different priority and we can use per-prio ethtool counters to\n# measure the throughput. In order to avoid prioritizing unicast traffic, prio\n# qdisc is installed on $swp3 and maps all priorities to the same band #7 (and\n# thus TC 0).\n#\n# Mausezahn can't actually saturate the links unless it's using large frames.\n# Thus we set MTU to 10K on all involved interfaces. Then both unicast and\n# multicast traffic uses 8K frames.\n#\n# +---------------------------+            +----------------------------------+\n# | H1                        |            |                               H2 |\n# |                           |            |  unicast --> + $h2.111           |\n# |                 multicast |            |  traffic     | 192.0.2.129/28    |\n# |                 traffic   |            |              | e-qos-map 0:1     |\n# |           $h1 + <-----    |            |              |                   |\n# | 192.0.2.65/28 |           |            |              + $h2               |\n# +---------------|-----------+            +--------------|-------------------+\n#                 |                                       |\n# +---------------|---------------------------------------|-------------------+\n# |         $swp1 +                                       + $swp2             |\n# |        >1Gbps |                                       | >1Gbps            |\n# | +-------------|------+                     +----------|----------------+  |\n# | |     $swp1.1 +      |                     |          + $swp2.111      |  |\n# | |                BR1 |             SW      | BR111                     |  |\n# | |     $swp3.1 +      |                     |          + $swp3.111      |  |\n# | +-------------|------+                     +----------|----------------+  |\n# |               \\_______________________________________/                   |\n# |                                    |                                      |\n# |                                    + $swp3                                |\n# |                                    | 1Gbps bottleneck                     |\n# |                                    | prio qdisc: {0..7} -> 7              |\n# +------------------------------------|--------------------------------------+\n#                                      |\n#                                   +--|-----------------+\n#                                   |  + $h3          H3 |\n#                                   |  | 192.0.2.66/28   |\n#                                   |  |                 |\n#                                   |  + $h3.111         |\n#                                   |    192.0.2.130/28  |\n#                                   +--------------------+\n\nALL_TESTS=\"\n\tping_ipv4\n\ttest_mc_aware\n\ttest_uc_aware\n\"\n\nlib_dir=$(dirname $0)/../../../net/forwarding\n\nNUM_NETIFS=6\nsource $lib_dir/lib.sh\nsource $lib_dir/devlink_lib.sh\nsource qos_lib.sh\n\nh1_create()\n{\n\tsimple_if_init $h1 192.0.2.65/28\n\tmtu_set $h1 10000\n}\n\nh1_destroy()\n{\n\tmtu_restore $h1\n\tsimple_if_fini $h1 192.0.2.65/28\n}\n\nh2_create()\n{\n\tsimple_if_init $h2\n\tmtu_set $h2 10000\n\n\tvlan_create $h2 111 v$h2 192.0.2.129/28\n\tip link set dev $h2.111 type vlan egress-qos-map 0:1\n}\n\nh2_destroy()\n{\n\tvlan_destroy $h2 111\n\n\tmtu_restore $h2\n\tsimple_if_fini $h2\n}\n\nh3_create()\n{\n\tsimple_if_init $h3 192.0.2.66/28\n\tmtu_set $h3 10000\n\n\tvlan_create $h3 111 v$h3 192.0.2.130/28\n}\n\nh3_destroy()\n{\n\tvlan_destroy $h3 111\n\n\tmtu_restore $h3\n\tsimple_if_fini $h3 192.0.2.66/28\n}\n\nswitch_create()\n{\n\tip link set dev $swp1 up\n\tmtu_set $swp1 10000\n\n\tip link set dev $swp2 up\n\tmtu_set $swp2 10000\n\n\tip link set dev $swp3 up\n\tmtu_set $swp3 10000\n\n\tvlan_create $swp2 111\n\tvlan_create $swp3 111\n\n\ttc qdisc replace dev $swp3 root handle 3: tbf rate 1gbit \\\n\t\tburst 128K limit 1G\n\ttc qdisc replace dev $swp3 parent 3:3 handle 33: \\\n\t\tprio bands 8 priomap 7 7 7 7 7 7 7 7\n\n\tip link add name br1 type bridge vlan_filtering 0\n\tip link set dev br1 addrgenmode none\n\tip link set dev br1 up\n\tip link set dev $swp1 master br1\n\tip link set dev $swp3 master br1\n\n\tip link add name br111 type bridge vlan_filtering 0\n\tip link set dev br111 addrgenmode none\n\tip link set dev br111 up\n\tip link set dev $swp2.111 master br111\n\tip link set dev $swp3.111 master br111\n\n\t# Make sure that ingress quotas are smaller than egress so that there is\n\t# room for both streams of traffic to be admitted to shared buffer.\n\tdevlink_port_pool_th_save $swp1 0\n\tdevlink_port_pool_th_set $swp1 0 5\n\tdevlink_tc_bind_pool_th_save $swp1 0 ingress\n\tdevlink_tc_bind_pool_th_set $swp1 0 ingress 0 5\n\n\tdevlink_port_pool_th_save $swp2 0\n\tdevlink_port_pool_th_set $swp2 0 5\n\tdevlink_tc_bind_pool_th_save $swp2 1 ingress\n\tdevlink_tc_bind_pool_th_set $swp2 1 ingress 0 5\n\n\tdevlink_port_pool_th_save $swp3 4\n\tdevlink_port_pool_th_set $swp3 4 12\n}\n\nswitch_destroy()\n{\n\tdevlink_port_pool_th_restore $swp3 4\n\n\tdevlink_tc_bind_pool_th_restore $swp2 1 ingress\n\tdevlink_port_pool_th_restore $swp2 0\n\n\tdevlink_tc_bind_pool_th_restore $swp1 0 ingress\n\tdevlink_port_pool_th_restore $swp1 0\n\n\tip link del dev br111\n\tip link del dev br1\n\n\ttc qdisc del dev $swp3 parent 3:3 handle 33:\n\ttc qdisc del dev $swp3 root handle 3:\n\n\tvlan_destroy $swp3 111\n\tvlan_destroy $swp2 111\n\n\tmtu_restore $swp3\n\tip link set dev $swp3 down\n\n\tmtu_restore $swp2\n\tip link set dev $swp2 down\n\n\tmtu_restore $swp1\n\tip link set dev $swp1 down\n}\n\nsetup_prepare()\n{\n\th1=${NETIFS[p1]}\n\tswp1=${NETIFS[p2]}\n\n\tswp2=${NETIFS[p3]}\n\th2=${NETIFS[p4]}\n\n\tswp3=${NETIFS[p5]}\n\th3=${NETIFS[p6]}\n\n\th3mac=$(mac_get $h3)\n\n\tvrf_prepare\n\n\th1_create\n\th2_create\n\th3_create\n\tswitch_create\n}\n\ncleanup()\n{\n\tpre_cleanup\n\n\tswitch_destroy\n\th3_destroy\n\th2_destroy\n\th1_destroy\n\n\tvrf_cleanup\n}\n\nping_ipv4()\n{\n\tping_test $h2 192.0.2.130\n}\n\ntest_mc_aware()\n{\n\tRET=0\n\n\tlocal -a uc_rate\n\tstart_traffic $h2.111 192.0.2.129 192.0.2.130 $h3mac\n\tuc_rate=($(measure_rate $swp2 $h3 rx_octets_prio_1 \"UC-only\"))\n\tcheck_err $? \"Could not get high enough UC-only ingress rate\"\n\tstop_traffic\n\tlocal ucth1=${uc_rate[1]}\n\n\tstart_traffic $h1 192.0.2.65 bc bc\n\n\tlocal d0=$(date +%s)\n\tlocal t0=$(ethtool_stats_get $h3 rx_octets_prio_0)\n\tlocal u0=$(ethtool_stats_get $swp1 rx_octets_prio_0)\n\n\tlocal -a uc_rate_2\n\tstart_traffic $h2.111 192.0.2.129 192.0.2.130 $h3mac\n\tuc_rate_2=($(measure_rate $swp2 $h3 rx_octets_prio_1 \"UC+MC\"))\n\tcheck_err $? \"Could not get high enough UC+MC ingress rate\"\n\tstop_traffic\n\tlocal ucth2=${uc_rate_2[1]}\n\n\tlocal d1=$(date +%s)\n\tlocal t1=$(ethtool_stats_get $h3 rx_octets_prio_0)\n\tlocal u1=$(ethtool_stats_get $swp1 rx_octets_prio_0)\n\n\tlocal deg=$(bc <<< \"\n\t\t\tscale=2\n\t\t\tret = 100 * ($ucth1 - $ucth2) / $ucth1\n\t\t\tif (ret > 0) { ret } else { 0 }\n\t\t    \")\n\n\t# Minimum shaper of 200Mbps on MC TCs should cause about 20% of\n\t# degradation on 1Gbps link.\n\tcheck_err $(bc <<< \"$deg < 15\") \"Minimum shaper not in effect\"\n\tcheck_err $(bc <<< \"$deg > 25\") \"MC traffic degrades UC performance too much\"\n\n\tlocal interval=$((d1 - d0))\n\tlocal mc_ir=$(rate $u0 $u1 $interval)\n\tlocal mc_er=$(rate $t0 $t1 $interval)\n\n\tstop_traffic\n\n\tlog_test \"UC performance under MC overload\"\n\n\techo \"UC-only throughput  $(humanize $ucth1)\"\n\techo \"UC+MC throughput    $(humanize $ucth2)\"\n\techo \"Degradation         $deg %\"\n\techo\n\techo \"Full report:\"\n\techo \"  UC only:\"\n\techo \"    ingress UC throughput $(humanize ${uc_rate[0]})\"\n\techo \"    egress UC throughput  $(humanize ${uc_rate[1]})\"\n\techo \"  UC+MC:\"\n\techo \"    ingress UC throughput $(humanize ${uc_rate_2[0]})\"\n\techo \"    egress UC throughput  $(humanize ${uc_rate_2[1]})\"\n\techo \"    ingress MC throughput $(humanize $mc_ir)\"\n\techo \"    egress MC throughput  $(humanize $mc_er)\"\n\techo\n}\n\ntest_uc_aware()\n{\n\tRET=0\n\n\tstart_traffic $h2.111 192.0.2.129 192.0.2.130 $h3mac\n\n\tlocal d0=$(date +%s)\n\tlocal t0=$(ethtool_stats_get $h3 rx_octets_prio_1)\n\tlocal u0=$(ethtool_stats_get $swp2 rx_octets_prio_1)\n\tsleep 1\n\n\tlocal attempts=50\n\tlocal passes=0\n\tlocal i\n\n\tfor ((i = 0; i < attempts; ++i)); do\n\t\tif $ARPING -c 1 -I $h1 -b 192.0.2.66 -q -w 1; then\n\t\t\t((passes++))\n\t\tfi\n\n\t\tsleep 0.1\n\tdone\n\n\tlocal d1=$(date +%s)\n\tlocal t1=$(ethtool_stats_get $h3 rx_octets_prio_1)\n\tlocal u1=$(ethtool_stats_get $swp2 rx_octets_prio_1)\n\n\tlocal interval=$((d1 - d0))\n\tlocal uc_ir=$(rate $u0 $u1 $interval)\n\tlocal uc_er=$(rate $t0 $t1 $interval)\n\n\t((attempts == passes))\n\tcheck_err $?\n\n\tstop_traffic\n\n\tlog_test \"MC performance under UC overload\"\n\techo \"    ingress UC throughput $(humanize ${uc_ir})\"\n\techo \"    egress UC throughput  $(humanize ${uc_er})\"\n\techo \"    sent $attempts BC ARPs, got $passes responses\"\n}\n\ntrap cleanup EXIT\n\nsetup_prepare\nsetup_wait\n\ntests_run\n\nexit $EXIT_STATUS\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}