{
  "module_name": "state_test.c",
  "hash_id": "c8276feb7023f0f24e94e0c126d35ac1cf3a8b8136a1d526370659ffad39f8cf",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/x86_64/state_test.c",
  "human_readable_source": "\n \n#define _GNU_SOURCE  \n#include <fcntl.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/ioctl.h>\n\n#include \"test_util.h\"\n\n#include \"kvm_util.h\"\n#include \"processor.h\"\n#include \"vmx.h\"\n#include \"svm_util.h\"\n\n#define L2_GUEST_STACK_SIZE 256\n\nvoid svm_l2_guest_code(void)\n{\n\tGUEST_SYNC(4);\n\t \n\tvmcall();\n\tGUEST_SYNC(6);\n\t \n\tvmcall();\n}\n\nstatic void svm_l1_guest_code(struct svm_test_data *svm)\n{\n\tunsigned long l2_guest_stack[L2_GUEST_STACK_SIZE];\n\tstruct vmcb *vmcb = svm->vmcb;\n\n\tGUEST_ASSERT(svm->vmcb_gpa);\n\t \n\tgeneric_svm_setup(svm, svm_l2_guest_code,\n\t\t\t  &l2_guest_stack[L2_GUEST_STACK_SIZE]);\n\n\tGUEST_SYNC(3);\n\trun_guest(vmcb, svm->vmcb_gpa);\n\tGUEST_ASSERT(vmcb->control.exit_code == SVM_EXIT_VMMCALL);\n\tGUEST_SYNC(5);\n\tvmcb->save.rip += 3;\n\trun_guest(vmcb, svm->vmcb_gpa);\n\tGUEST_ASSERT(vmcb->control.exit_code == SVM_EXIT_VMMCALL);\n\tGUEST_SYNC(7);\n}\n\nvoid vmx_l2_guest_code(void)\n{\n\tGUEST_SYNC(6);\n\n\t \n\tvmcall();\n\n\t \n\tGUEST_ASSERT(vmreadz(GUEST_RIP) == 0xc0ffee);\n\tGUEST_SYNC(10);\n\tGUEST_ASSERT(vmreadz(GUEST_RIP) == 0xc0ffee);\n\tGUEST_ASSERT(!vmwrite(GUEST_RIP, 0xc0fffee));\n\tGUEST_SYNC(11);\n\tGUEST_ASSERT(vmreadz(GUEST_RIP) == 0xc0fffee);\n\tGUEST_ASSERT(!vmwrite(GUEST_RIP, 0xc0ffffee));\n\tGUEST_SYNC(12);\n\n\t \n\tvmcall();\n}\n\nstatic void vmx_l1_guest_code(struct vmx_pages *vmx_pages)\n{\n\tunsigned long l2_guest_stack[L2_GUEST_STACK_SIZE];\n\n\tGUEST_ASSERT(vmx_pages->vmcs_gpa);\n\tGUEST_ASSERT(prepare_for_vmx_operation(vmx_pages));\n\tGUEST_SYNC(3);\n\tGUEST_ASSERT(load_vmcs(vmx_pages));\n\tGUEST_ASSERT(vmptrstz() == vmx_pages->vmcs_gpa);\n\n\tGUEST_SYNC(4);\n\tGUEST_ASSERT(vmptrstz() == vmx_pages->vmcs_gpa);\n\n\tprepare_vmcs(vmx_pages, vmx_l2_guest_code,\n\t\t     &l2_guest_stack[L2_GUEST_STACK_SIZE]);\n\n\tGUEST_SYNC(5);\n\tGUEST_ASSERT(vmptrstz() == vmx_pages->vmcs_gpa);\n\tGUEST_ASSERT(!vmlaunch());\n\tGUEST_ASSERT(vmptrstz() == vmx_pages->vmcs_gpa);\n\tGUEST_ASSERT(vmreadz(VM_EXIT_REASON) == EXIT_REASON_VMCALL);\n\n\t \n\tGUEST_ASSERT(vmlaunch());\n\n\tGUEST_ASSERT(!vmresume());\n\tGUEST_ASSERT(vmreadz(VM_EXIT_REASON) == EXIT_REASON_VMCALL);\n\n\tGUEST_SYNC(7);\n\tGUEST_ASSERT(vmreadz(VM_EXIT_REASON) == EXIT_REASON_VMCALL);\n\n\tGUEST_ASSERT(!vmresume());\n\tGUEST_ASSERT(vmreadz(VM_EXIT_REASON) == EXIT_REASON_VMCALL);\n\n\tvmwrite(GUEST_RIP, vmreadz(GUEST_RIP) + 3);\n\n\tvmwrite(SECONDARY_VM_EXEC_CONTROL, SECONDARY_EXEC_SHADOW_VMCS);\n\tvmwrite(VMCS_LINK_POINTER, vmx_pages->shadow_vmcs_gpa);\n\n\tGUEST_ASSERT(!vmptrld(vmx_pages->shadow_vmcs_gpa));\n\tGUEST_ASSERT(vmlaunch());\n\tGUEST_SYNC(8);\n\tGUEST_ASSERT(vmlaunch());\n\tGUEST_ASSERT(vmresume());\n\n\tvmwrite(GUEST_RIP, 0xc0ffee);\n\tGUEST_SYNC(9);\n\tGUEST_ASSERT(vmreadz(GUEST_RIP) == 0xc0ffee);\n\n\tGUEST_ASSERT(!vmptrld(vmx_pages->vmcs_gpa));\n\tGUEST_ASSERT(!vmresume());\n\tGUEST_ASSERT(vmreadz(VM_EXIT_REASON) == EXIT_REASON_VMCALL);\n\n\tGUEST_ASSERT(!vmptrld(vmx_pages->shadow_vmcs_gpa));\n\tGUEST_ASSERT(vmreadz(GUEST_RIP) == 0xc0ffffee);\n\tGUEST_ASSERT(vmlaunch());\n\tGUEST_ASSERT(vmresume());\n\tGUEST_SYNC(13);\n\tGUEST_ASSERT(vmreadz(GUEST_RIP) == 0xc0ffffee);\n\tGUEST_ASSERT(vmlaunch());\n\tGUEST_ASSERT(vmresume());\n}\n\nstatic void __attribute__((__flatten__)) guest_code(void *arg)\n{\n\tGUEST_SYNC(1);\n\n\tif (this_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tuint64_t supported_xcr0 = this_cpu_supported_xcr0();\n\t\tuint8_t buffer[4096];\n\n\t\tmemset(buffer, 0xcc, sizeof(buffer));\n\n\t\tset_cr4(get_cr4() | X86_CR4_OSXSAVE);\n\t\tGUEST_ASSERT(this_cpu_has(X86_FEATURE_OSXSAVE));\n\n\t\txsetbv(0, xgetbv(0) | supported_xcr0);\n\n\t\t \n\t\tGUEST_ASSERT(supported_xcr0 & XFEATURE_MASK_FP);\n\t\tasm volatile (\"fincstp\");\n\n\t\tGUEST_ASSERT(supported_xcr0 & XFEATURE_MASK_SSE);\n\t\tasm volatile (\"vmovdqu %0, %%xmm0\" :: \"m\" (buffer));\n\n\t\tif (supported_xcr0 & XFEATURE_MASK_YMM)\n\t\t\tasm volatile (\"vmovdqu %0, %%ymm0\" :: \"m\" (buffer));\n\n\t\tif (supported_xcr0 & XFEATURE_MASK_AVX512) {\n\t\t\tasm volatile (\"kmovq %0, %%k1\" :: \"r\" (-1ull));\n\t\t\tasm volatile (\"vmovupd %0, %%zmm0\" :: \"m\" (buffer));\n\t\t\tasm volatile (\"vmovupd %0, %%zmm16\" :: \"m\" (buffer));\n\t\t}\n\n\t\tif (this_cpu_has(X86_FEATURE_MPX)) {\n\t\t\tuint64_t bounds[2] = { 10, 0xffffffffull };\n\t\t\tuint64_t output[2] = { };\n\n\t\t\tGUEST_ASSERT(supported_xcr0 & XFEATURE_MASK_BNDREGS);\n\t\t\tGUEST_ASSERT(supported_xcr0 & XFEATURE_MASK_BNDCSR);\n\n\t\t\t \n\t\t\twrmsr(MSR_IA32_BNDCFGS, BIT_ULL(0));\n\t\t\tasm volatile (\".byte 0x66,0x0f,0x1a,0x08\" :: \"a\" (bounds));\n\t\t\t \n\t\t\tasm volatile (\".byte 0x66,0x0f,0x1b,0x08\" :: \"a\" (output));\n\t\t\twrmsr(MSR_IA32_BNDCFGS, 0);\n\n\t\t\tGUEST_ASSERT_EQ(bounds[0], output[0]);\n\t\t\tGUEST_ASSERT_EQ(bounds[1], output[1]);\n\t\t}\n\t\tif (this_cpu_has(X86_FEATURE_PKU)) {\n\t\t\tGUEST_ASSERT(supported_xcr0 & XFEATURE_MASK_PKRU);\n\t\t\tset_cr4(get_cr4() | X86_CR4_PKE);\n\t\t\tGUEST_ASSERT(this_cpu_has(X86_FEATURE_OSPKE));\n\n\t\t\twrpkru(-1u);\n\t\t}\n\t}\n\n\tGUEST_SYNC(2);\n\n\tif (arg) {\n\t\tif (this_cpu_has(X86_FEATURE_SVM))\n\t\t\tsvm_l1_guest_code(arg);\n\t\telse\n\t\t\tvmx_l1_guest_code(arg);\n\t}\n\n\tGUEST_DONE();\n}\n\nint main(int argc, char *argv[])\n{\n\tuint64_t *xstate_bv, saved_xstate_bv;\n\tvm_vaddr_t nested_gva = 0;\n\tstruct kvm_cpuid2 empty_cpuid = {};\n\tstruct kvm_regs regs1, regs2;\n\tstruct kvm_vcpu *vcpu, *vcpuN;\n\tstruct kvm_vm *vm;\n\tstruct kvm_x86_state *state;\n\tstruct ucall uc;\n\tint stage;\n\n\t \n\tvm = vm_create_with_one_vcpu(&vcpu, guest_code);\n\n\tvcpu_regs_get(vcpu, &regs1);\n\n\tif (kvm_has_cap(KVM_CAP_NESTED_STATE)) {\n\t\tif (kvm_cpu_has(X86_FEATURE_SVM))\n\t\t\tvcpu_alloc_svm(vm, &nested_gva);\n\t\telse if (kvm_cpu_has(X86_FEATURE_VMX))\n\t\t\tvcpu_alloc_vmx(vm, &nested_gva);\n\t}\n\n\tif (!nested_gva)\n\t\tpr_info(\"will skip nested state checks\\n\");\n\n\tvcpu_args_set(vcpu, 1, nested_gva);\n\n\tfor (stage = 1;; stage++) {\n\t\tvcpu_run(vcpu);\n\t\tTEST_ASSERT_KVM_EXIT_REASON(vcpu, KVM_EXIT_IO);\n\n\t\tswitch (get_ucall(vcpu, &uc)) {\n\t\tcase UCALL_ABORT:\n\t\t\tREPORT_GUEST_ASSERT(uc);\n\t\t\t \n\t\tcase UCALL_SYNC:\n\t\t\tbreak;\n\t\tcase UCALL_DONE:\n\t\t\tgoto done;\n\t\tdefault:\n\t\t\tTEST_FAIL(\"Unknown ucall %lu\", uc.cmd);\n\t\t}\n\n\t\t \n\t\tTEST_ASSERT(!strcmp((const char *)uc.args[0], \"hello\") &&\n\t\t\t    uc.args[1] == stage, \"Stage %d: Unexpected register values vmexit, got %lx\",\n\t\t\t    stage, (ulong)uc.args[1]);\n\n\t\tstate = vcpu_save_state(vcpu);\n\t\tmemset(&regs1, 0, sizeof(regs1));\n\t\tvcpu_regs_get(vcpu, &regs1);\n\n\t\tkvm_vm_release(vm);\n\n\t\t \n\t\tvcpu = vm_recreate_with_one_vcpu(vm);\n\t\tvcpu_load_state(vcpu, state);\n\n\t\t \n\t\txstate_bv = (void *)&((uint8_t *)state->xsave->region)[512];\n\t\tsaved_xstate_bv = *xstate_bv;\n\n\t\tvcpuN = __vm_vcpu_add(vm, vcpu->id + 1);\n\t\tvcpu_xsave_set(vcpuN, state->xsave);\n\t\t*xstate_bv = kvm_cpu_supported_xcr0();\n\t\tvcpu_xsave_set(vcpuN, state->xsave);\n\n\t\tvcpu_init_cpuid(vcpuN, &empty_cpuid);\n\t\tvcpu_xsave_set(vcpuN, state->xsave);\n\t\t*xstate_bv = saved_xstate_bv;\n\t\tvcpu_xsave_set(vcpuN, state->xsave);\n\n\t\tkvm_x86_state_cleanup(state);\n\n\t\tmemset(&regs2, 0, sizeof(regs2));\n\t\tvcpu_regs_get(vcpu, &regs2);\n\t\tTEST_ASSERT(!memcmp(&regs1, &regs2, sizeof(regs2)),\n\t\t\t    \"Unexpected register values after vcpu_load_state; rdi: %lx rsi: %lx\",\n\t\t\t    (ulong) regs2.rdi, (ulong) regs2.rsi);\n\t}\n\ndone:\n\tkvm_vm_free(vm);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}