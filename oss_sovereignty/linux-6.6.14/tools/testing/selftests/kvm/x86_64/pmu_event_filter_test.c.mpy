{
  "module_name": "pmu_event_filter_test.c",
  "hash_id": "3df9d7a2411ca0886705819fc173b80346bb0705273ca1d47ac645dfa9e4a34b",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/x86_64/pmu_event_filter_test.c",
  "human_readable_source": "\n \n\n#define _GNU_SOURCE  \n#include \"test_util.h\"\n#include \"kvm_util.h\"\n#include \"processor.h\"\n\n \n#define ARCH_PERFMON_EVENTSEL_OS\t\t\t(1ULL << 17)\n#define ARCH_PERFMON_EVENTSEL_ENABLE\t\t\t(1ULL << 22)\n\n \n\n \n#define ARCH_PERFMON_BRANCHES_RETIRED\t\t5\n\n#define NUM_BRANCHES 42\n#define INTEL_PMC_IDX_FIXED\t\t32\n\n \n#define MAX_FILTER_EVENTS\t\t300\n#define MAX_TEST_EVENTS\t\t10\n\n#define PMU_EVENT_FILTER_INVALID_ACTION\t\t(KVM_PMU_EVENT_DENY + 1)\n#define PMU_EVENT_FILTER_INVALID_FLAGS\t\t\t(KVM_PMU_EVENT_FLAGS_VALID_MASK << 1)\n#define PMU_EVENT_FILTER_INVALID_NEVENTS\t\t(MAX_FILTER_EVENTS + 1)\n\n \n#define EVENT(select, umask) ((select & 0xf00UL) << 24 | (select & 0xff) | \\\n\t\t\t      (umask & 0xff) << 8)\n\n \n\n#define INTEL_BR_RETIRED EVENT(0xc4, 0)\n\n \n\n#define AMD_ZEN_BR_RETIRED EVENT(0xc2, 0)\n\n\n \n\n#define INST_RETIRED EVENT(0xc0, 0)\n\nstruct __kvm_pmu_event_filter {\n\t__u32 action;\n\t__u32 nevents;\n\t__u32 fixed_counter_bitmap;\n\t__u32 flags;\n\t__u32 pad[4];\n\t__u64 events[MAX_FILTER_EVENTS];\n};\n\n \nstatic const struct __kvm_pmu_event_filter base_event_filter = {\n\t.nevents = ARRAY_SIZE(base_event_filter.events),\n\t.events = {\n\t\tEVENT(0x3c, 0),\n\t\tINST_RETIRED,\n\t\tEVENT(0x3c, 1),\n\t\tEVENT(0x2e, 0x4f),\n\t\tEVENT(0x2e, 0x41),\n\t\tEVENT(0xc4, 0),\n\t\tEVENT(0xc5, 0),\n\t\tEVENT(0xa4, 1),\n\t\tAMD_ZEN_BR_RETIRED,\n\t},\n};\n\nstruct {\n\tuint64_t loads;\n\tuint64_t stores;\n\tuint64_t loads_stores;\n\tuint64_t branches_retired;\n\tuint64_t instructions_retired;\n} pmc_results;\n\n \nstatic void guest_gp_handler(struct ex_regs *regs)\n{\n\tGUEST_SYNC(-EFAULT);\n}\n\n \nstatic void check_msr(uint32_t msr, uint64_t bits_to_flip)\n{\n\tuint64_t v = rdmsr(msr) ^ bits_to_flip;\n\n\twrmsr(msr, v);\n\tif (rdmsr(msr) != v)\n\t\tGUEST_SYNC(-EIO);\n\n\tv ^= bits_to_flip;\n\twrmsr(msr, v);\n\tif (rdmsr(msr) != v)\n\t\tGUEST_SYNC(-EIO);\n}\n\nstatic void run_and_measure_loop(uint32_t msr_base)\n{\n\tconst uint64_t branches_retired = rdmsr(msr_base + 0);\n\tconst uint64_t insn_retired = rdmsr(msr_base + 1);\n\n\t__asm__ __volatile__(\"loop .\" : \"+c\"((int){NUM_BRANCHES}));\n\n\tpmc_results.branches_retired = rdmsr(msr_base + 0) - branches_retired;\n\tpmc_results.instructions_retired = rdmsr(msr_base + 1) - insn_retired;\n}\n\nstatic void intel_guest_code(void)\n{\n\tcheck_msr(MSR_CORE_PERF_GLOBAL_CTRL, 1);\n\tcheck_msr(MSR_P6_EVNTSEL0, 0xffff);\n\tcheck_msr(MSR_IA32_PMC0, 0xffff);\n\tGUEST_SYNC(0);\n\n\tfor (;;) {\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\t\twrmsr(MSR_P6_EVNTSEL0, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | INTEL_BR_RETIRED);\n\t\twrmsr(MSR_P6_EVNTSEL1, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | INST_RETIRED);\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL, 0x3);\n\n\t\trun_and_measure_loop(MSR_IA32_PMC0);\n\t\tGUEST_SYNC(0);\n\t}\n}\n\n \nstatic void amd_guest_code(void)\n{\n\tcheck_msr(MSR_K7_EVNTSEL0, 0xffff);\n\tcheck_msr(MSR_K7_PERFCTR0, 0xffff);\n\tGUEST_SYNC(0);\n\n\tfor (;;) {\n\t\twrmsr(MSR_K7_EVNTSEL0, 0);\n\t\twrmsr(MSR_K7_EVNTSEL0, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | AMD_ZEN_BR_RETIRED);\n\t\twrmsr(MSR_K7_EVNTSEL1, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | INST_RETIRED);\n\n\t\trun_and_measure_loop(MSR_K7_PERFCTR0);\n\t\tGUEST_SYNC(0);\n\t}\n}\n\n \nstatic uint64_t run_vcpu_to_sync(struct kvm_vcpu *vcpu)\n{\n\tstruct ucall uc;\n\n\tvcpu_run(vcpu);\n\tTEST_ASSERT_KVM_EXIT_REASON(vcpu, KVM_EXIT_IO);\n\tget_ucall(vcpu, &uc);\n\tTEST_ASSERT(uc.cmd == UCALL_SYNC,\n\t\t    \"Received ucall other than UCALL_SYNC: %lu\", uc.cmd);\n\treturn uc.args[1];\n}\n\nstatic void run_vcpu_and_sync_pmc_results(struct kvm_vcpu *vcpu)\n{\n\tuint64_t r;\n\n\tmemset(&pmc_results, 0, sizeof(pmc_results));\n\tsync_global_to_guest(vcpu->vm, pmc_results);\n\n\tr = run_vcpu_to_sync(vcpu);\n\tTEST_ASSERT(!r, \"Unexpected sync value: 0x%lx\", r);\n\n\tsync_global_from_guest(vcpu->vm, pmc_results);\n}\n\n \nstatic bool sanity_check_pmu(struct kvm_vcpu *vcpu)\n{\n\tuint64_t r;\n\n\tvm_install_exception_handler(vcpu->vm, GP_VECTOR, guest_gp_handler);\n\tr = run_vcpu_to_sync(vcpu);\n\tvm_install_exception_handler(vcpu->vm, GP_VECTOR, NULL);\n\n\treturn !r;\n}\n\n \nstatic void remove_event(struct __kvm_pmu_event_filter *f, uint64_t event)\n{\n\tbool found = false;\n\tint i;\n\n\tfor (i = 0; i < f->nevents; i++) {\n\t\tif (found)\n\t\t\tf->events[i - 1] = f->events[i];\n\t\telse\n\t\t\tfound = f->events[i] == event;\n\t}\n\tif (found)\n\t\tf->nevents--;\n}\n\n#define ASSERT_PMC_COUNTING_INSTRUCTIONS()\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\t\\\n\tuint64_t br = pmc_results.branches_retired;\t\t\t\t\t\\\n\tuint64_t ir = pmc_results.instructions_retired;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\t\\\n\tif (br && br != NUM_BRANCHES)\t\t\t\t\t\t\t\\\n\t\tpr_info(\"%s: Branch instructions retired = %lu (expected %u)\\n\",\t\\\n\t\t\t__func__, br, NUM_BRANCHES);\t\t\t\t\t\\\n\tTEST_ASSERT(br, \"%s: Branch instructions retired = %lu (expected > 0)\",\t\t\\\n\t\t    __func__, br);\t\t\t\t\t\t\t\\\n\tTEST_ASSERT(ir,\t\"%s: Instructions retired = %lu (expected > 0)\",\t\t\\\n\t\t    __func__, ir);\t\t\t\t\t\t\t\\\n} while (0)\n\n#define ASSERT_PMC_NOT_COUNTING_INSTRUCTIONS()\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\t\\\n\tuint64_t br = pmc_results.branches_retired;\t\t\t\t\t\\\n\tuint64_t ir = pmc_results.instructions_retired;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\t\\\n\tTEST_ASSERT(!br, \"%s: Branch instructions retired = %lu (expected 0)\",\t\t\\\n\t\t    __func__, br);\t\t\t\t\t\t\t\\\n\tTEST_ASSERT(!ir, \"%s: Instructions retired = %lu (expected 0)\",\t\t\t\\\n\t\t    __func__, ir);\t\t\t\t\t\t\t\\\n} while (0)\n\nstatic void test_without_filter(struct kvm_vcpu *vcpu)\n{\n\trun_vcpu_and_sync_pmc_results(vcpu);\n\n\tASSERT_PMC_COUNTING_INSTRUCTIONS();\n}\n\nstatic void test_with_filter(struct kvm_vcpu *vcpu,\n\t\t\t     struct __kvm_pmu_event_filter *__f)\n{\n\tstruct kvm_pmu_event_filter *f = (void *)__f;\n\n\tvm_ioctl(vcpu->vm, KVM_SET_PMU_EVENT_FILTER, f);\n\trun_vcpu_and_sync_pmc_results(vcpu);\n}\n\nstatic void test_amd_deny_list(struct kvm_vcpu *vcpu)\n{\n\tstruct __kvm_pmu_event_filter f = {\n\t\t.action = KVM_PMU_EVENT_DENY,\n\t\t.nevents = 1,\n\t\t.events = {\n\t\t\tEVENT(0x1C2, 0),\n\t\t},\n\t};\n\n\ttest_with_filter(vcpu, &f);\n\n\tASSERT_PMC_COUNTING_INSTRUCTIONS();\n}\n\nstatic void test_member_deny_list(struct kvm_vcpu *vcpu)\n{\n\tstruct __kvm_pmu_event_filter f = base_event_filter;\n\n\tf.action = KVM_PMU_EVENT_DENY;\n\ttest_with_filter(vcpu, &f);\n\n\tASSERT_PMC_NOT_COUNTING_INSTRUCTIONS();\n}\n\nstatic void test_member_allow_list(struct kvm_vcpu *vcpu)\n{\n\tstruct __kvm_pmu_event_filter f = base_event_filter;\n\n\tf.action = KVM_PMU_EVENT_ALLOW;\n\ttest_with_filter(vcpu, &f);\n\n\tASSERT_PMC_COUNTING_INSTRUCTIONS();\n}\n\nstatic void test_not_member_deny_list(struct kvm_vcpu *vcpu)\n{\n\tstruct __kvm_pmu_event_filter f = base_event_filter;\n\n\tf.action = KVM_PMU_EVENT_DENY;\n\n\tremove_event(&f, INST_RETIRED);\n\tremove_event(&f, INTEL_BR_RETIRED);\n\tremove_event(&f, AMD_ZEN_BR_RETIRED);\n\ttest_with_filter(vcpu, &f);\n\n\tASSERT_PMC_COUNTING_INSTRUCTIONS();\n}\n\nstatic void test_not_member_allow_list(struct kvm_vcpu *vcpu)\n{\n\tstruct __kvm_pmu_event_filter f = base_event_filter;\n\n\tf.action = KVM_PMU_EVENT_ALLOW;\n\n\tremove_event(&f, INST_RETIRED);\n\tremove_event(&f, INTEL_BR_RETIRED);\n\tremove_event(&f, AMD_ZEN_BR_RETIRED);\n\ttest_with_filter(vcpu, &f);\n\n\tASSERT_PMC_NOT_COUNTING_INSTRUCTIONS();\n}\n\n \nstatic void test_pmu_config_disable(void (*guest_code)(void))\n{\n\tstruct kvm_vcpu *vcpu;\n\tint r;\n\tstruct kvm_vm *vm;\n\n\tr = kvm_check_cap(KVM_CAP_PMU_CAPABILITY);\n\tif (!(r & KVM_PMU_CAP_DISABLE))\n\t\treturn;\n\n\tvm = vm_create(1);\n\n\tvm_enable_cap(vm, KVM_CAP_PMU_CAPABILITY, KVM_PMU_CAP_DISABLE);\n\n\tvcpu = vm_vcpu_add(vm, 0, guest_code);\n\tvm_init_descriptor_tables(vm);\n\tvcpu_init_descriptor_tables(vcpu);\n\n\tTEST_ASSERT(!sanity_check_pmu(vcpu),\n\t\t    \"Guest should not be able to use disabled PMU.\");\n\n\tkvm_vm_free(vm);\n}\n\n \nstatic bool use_intel_pmu(void)\n{\n\treturn host_cpu_is_intel &&\n\t       kvm_cpu_property(X86_PROPERTY_PMU_VERSION) &&\n\t       kvm_cpu_property(X86_PROPERTY_PMU_NR_GP_COUNTERS) &&\n\t       kvm_pmu_has(X86_PMU_FEATURE_BRANCH_INSNS_RETIRED);\n}\n\nstatic bool is_zen1(uint32_t family, uint32_t model)\n{\n\treturn family == 0x17 && model <= 0x0f;\n}\n\nstatic bool is_zen2(uint32_t family, uint32_t model)\n{\n\treturn family == 0x17 && model >= 0x30 && model <= 0x3f;\n}\n\nstatic bool is_zen3(uint32_t family, uint32_t model)\n{\n\treturn family == 0x19 && model <= 0x0f;\n}\n\n \nstatic bool use_amd_pmu(void)\n{\n\tuint32_t family = kvm_cpu_family();\n\tuint32_t model = kvm_cpu_model();\n\n\treturn host_cpu_is_amd &&\n\t\t(is_zen1(family, model) ||\n\t\t is_zen2(family, model) ||\n\t\t is_zen3(family, model));\n}\n\n \n#define MEM_INST_RETIRED\t\t0xD0\n#define MEM_INST_RETIRED_LOAD\t\tEVENT(MEM_INST_RETIRED, 0x81)\n#define MEM_INST_RETIRED_STORE\t\tEVENT(MEM_INST_RETIRED, 0x82)\n#define MEM_INST_RETIRED_LOAD_STORE\tEVENT(MEM_INST_RETIRED, 0x83)\n\nstatic bool supports_event_mem_inst_retired(void)\n{\n\tuint32_t eax, ebx, ecx, edx;\n\n\tcpuid(1, &eax, &ebx, &ecx, &edx);\n\tif (x86_family(eax) == 0x6) {\n\t\tswitch (x86_model(eax)) {\n\t\t \n\t\tcase 0x8F:\n\t\t \n\t\tcase 0x6A:\n\t\t \n\t\t \n\t\tcase 0x55:\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\n \n#define LS_DISPATCH\t\t0x29\n#define LS_DISPATCH_LOAD\tEVENT(LS_DISPATCH, BIT(0))\n#define LS_DISPATCH_STORE\tEVENT(LS_DISPATCH, BIT(1))\n#define LS_DISPATCH_LOAD_STORE\tEVENT(LS_DISPATCH, BIT(2))\n\n#define INCLUDE_MASKED_ENTRY(event_select, mask, match) \\\n\tKVM_PMU_ENCODE_MASKED_ENTRY(event_select, mask, match, false)\n#define EXCLUDE_MASKED_ENTRY(event_select, mask, match) \\\n\tKVM_PMU_ENCODE_MASKED_ENTRY(event_select, mask, match, true)\n\nstatic void masked_events_guest_test(uint32_t msr_base)\n{\n\t \n\tconst uint64_t loads = rdmsr(msr_base + 0);\n\tconst uint64_t stores = rdmsr(msr_base + 1);\n\tconst uint64_t loads_stores = rdmsr(msr_base + 2);\n\tint val;\n\n\n\t__asm__ __volatile__(\"movl $0, %[v];\"\n\t\t\t     \"movl %[v], %%eax;\"\n\t\t\t     \"incl %[v];\"\n\t\t\t     : [v]\"+m\"(val) :: \"eax\");\n\n\tpmc_results.loads = rdmsr(msr_base + 0) - loads;\n\tpmc_results.stores = rdmsr(msr_base + 1) - stores;\n\tpmc_results.loads_stores = rdmsr(msr_base + 2) - loads_stores;\n}\n\nstatic void intel_masked_events_guest_code(void)\n{\n\tfor (;;) {\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\n\t\twrmsr(MSR_P6_EVNTSEL0 + 0, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | MEM_INST_RETIRED_LOAD);\n\t\twrmsr(MSR_P6_EVNTSEL0 + 1, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | MEM_INST_RETIRED_STORE);\n\t\twrmsr(MSR_P6_EVNTSEL0 + 2, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | MEM_INST_RETIRED_LOAD_STORE);\n\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL, 0x7);\n\n\t\tmasked_events_guest_test(MSR_IA32_PMC0);\n\t\tGUEST_SYNC(0);\n\t}\n}\n\nstatic void amd_masked_events_guest_code(void)\n{\n\tfor (;;) {\n\t\twrmsr(MSR_K7_EVNTSEL0, 0);\n\t\twrmsr(MSR_K7_EVNTSEL1, 0);\n\t\twrmsr(MSR_K7_EVNTSEL2, 0);\n\n\t\twrmsr(MSR_K7_EVNTSEL0, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | LS_DISPATCH_LOAD);\n\t\twrmsr(MSR_K7_EVNTSEL1, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | LS_DISPATCH_STORE);\n\t\twrmsr(MSR_K7_EVNTSEL2, ARCH_PERFMON_EVENTSEL_ENABLE |\n\t\t      ARCH_PERFMON_EVENTSEL_OS | LS_DISPATCH_LOAD_STORE);\n\n\t\tmasked_events_guest_test(MSR_K7_PERFCTR0);\n\t\tGUEST_SYNC(0);\n\t}\n}\n\nstatic void run_masked_events_test(struct kvm_vcpu *vcpu,\n\t\t\t\t   const uint64_t masked_events[],\n\t\t\t\t   const int nmasked_events)\n{\n\tstruct __kvm_pmu_event_filter f = {\n\t\t.nevents = nmasked_events,\n\t\t.action = KVM_PMU_EVENT_ALLOW,\n\t\t.flags = KVM_PMU_EVENT_FLAG_MASKED_EVENTS,\n\t};\n\n\tmemcpy(f.events, masked_events, sizeof(uint64_t) * nmasked_events);\n\ttest_with_filter(vcpu, &f);\n}\n\n#define ALLOW_LOADS\t\tBIT(0)\n#define ALLOW_STORES\t\tBIT(1)\n#define ALLOW_LOADS_STORES\tBIT(2)\n\nstruct masked_events_test {\n\tuint64_t intel_events[MAX_TEST_EVENTS];\n\tuint64_t intel_event_end;\n\tuint64_t amd_events[MAX_TEST_EVENTS];\n\tuint64_t amd_event_end;\n\tconst char *msg;\n\tuint32_t flags;\n};\n\n \nconst struct masked_events_test test_cases[] = {\n\t{\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0xFF, 0x81),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xFF, BIT(0)),\n\t\t},\n\t\t.msg = \"Only allow loads.\",\n\t\t.flags = ALLOW_LOADS,\n\t}, {\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0xFF, 0x82),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xFF, BIT(1)),\n\t\t},\n\t\t.msg = \"Only allow stores.\",\n\t\t.flags = ALLOW_STORES,\n\t}, {\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0xFF, 0x83),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xFF, BIT(2)),\n\t\t},\n\t\t.msg = \"Only allow loads + stores.\",\n\t\t.flags = ALLOW_LOADS_STORES,\n\t}, {\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0x7C, 0),\n\t\t\tEXCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0xFF, 0x83),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, ~(BIT(0) | BIT(1)), 0),\n\t\t},\n\t\t.msg = \"Only allow loads and stores.\",\n\t\t.flags = ALLOW_LOADS | ALLOW_STORES,\n\t}, {\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0x7C, 0),\n\t\t\tEXCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0xFF, 0x82),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xF8, 0),\n\t\t\tEXCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xFF, BIT(1)),\n\t\t},\n\t\t.msg = \"Only allow loads and loads + stores.\",\n\t\t.flags = ALLOW_LOADS | ALLOW_LOADS_STORES\n\t}, {\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0xFE, 0x82),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xF8, 0),\n\t\t\tEXCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xFF, BIT(0)),\n\t\t},\n\t\t.msg = \"Only allow stores and loads + stores.\",\n\t\t.flags = ALLOW_STORES | ALLOW_LOADS_STORES\n\t}, {\n\t\t.intel_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(MEM_INST_RETIRED, 0x7C, 0),\n\t\t},\n\t\t.amd_events = {\n\t\t\tINCLUDE_MASKED_ENTRY(LS_DISPATCH, 0xF8, 0),\n\t\t},\n\t\t.msg = \"Only allow loads, stores, and loads + stores.\",\n\t\t.flags = ALLOW_LOADS | ALLOW_STORES | ALLOW_LOADS_STORES\n\t},\n};\n\nstatic int append_test_events(const struct masked_events_test *test,\n\t\t\t      uint64_t *events, int nevents)\n{\n\tconst uint64_t *evts;\n\tint i;\n\n\tevts = use_intel_pmu() ? test->intel_events : test->amd_events;\n\tfor (i = 0; i < MAX_TEST_EVENTS; i++) {\n\t\tif (evts[i] == 0)\n\t\t\tbreak;\n\n\t\tevents[nevents + i] = evts[i];\n\t}\n\n\treturn nevents + i;\n}\n\nstatic bool bool_eq(bool a, bool b)\n{\n\treturn a == b;\n}\n\nstatic void run_masked_events_tests(struct kvm_vcpu *vcpu, uint64_t *events,\n\t\t\t\t    int nevents)\n{\n\tint ntests = ARRAY_SIZE(test_cases);\n\tint i, n;\n\n\tfor (i = 0; i < ntests; i++) {\n\t\tconst struct masked_events_test *test = &test_cases[i];\n\n\t\t \n\t\tassert(test->intel_event_end == 0);\n\t\tassert(test->amd_event_end == 0);\n\n\t\tn = append_test_events(test, events, nevents);\n\n\t\trun_masked_events_test(vcpu, events, n);\n\n\t\tTEST_ASSERT(bool_eq(pmc_results.loads, test->flags & ALLOW_LOADS) &&\n\t\t\t    bool_eq(pmc_results.stores, test->flags & ALLOW_STORES) &&\n\t\t\t    bool_eq(pmc_results.loads_stores,\n\t\t\t\t    test->flags & ALLOW_LOADS_STORES),\n\t\t\t    \"%s  loads: %lu, stores: %lu, loads + stores: %lu\",\n\t\t\t    test->msg, pmc_results.loads, pmc_results.stores,\n\t\t\t    pmc_results.loads_stores);\n\t}\n}\n\nstatic void add_dummy_events(uint64_t *events, int nevents)\n{\n\tint i;\n\n\tfor (i = 0; i < nevents; i++) {\n\t\tint event_select = i % 0xFF;\n\t\tbool exclude = ((i % 4) == 0);\n\n\t\tif (event_select == MEM_INST_RETIRED ||\n\t\t    event_select == LS_DISPATCH)\n\t\t\tevent_select++;\n\n\t\tevents[i] = KVM_PMU_ENCODE_MASKED_ENTRY(event_select, 0,\n\t\t\t\t\t\t\t0, exclude);\n\t}\n}\n\nstatic void test_masked_events(struct kvm_vcpu *vcpu)\n{\n\tint nevents = MAX_FILTER_EVENTS - MAX_TEST_EVENTS;\n\tuint64_t events[MAX_FILTER_EVENTS];\n\n\t \n\trun_masked_events_tests(vcpu, events, 0);\n\n\t \n\tadd_dummy_events(events, MAX_FILTER_EVENTS);\n\trun_masked_events_tests(vcpu, events, nevents);\n}\n\nstatic int set_pmu_event_filter(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct __kvm_pmu_event_filter *__f)\n{\n\tstruct kvm_pmu_event_filter *f = (void *)__f;\n\n\treturn __vm_ioctl(vcpu->vm, KVM_SET_PMU_EVENT_FILTER, f);\n}\n\nstatic int set_pmu_single_event_filter(struct kvm_vcpu *vcpu, uint64_t event,\n\t\t\t\t       uint32_t flags, uint32_t action)\n{\n\tstruct __kvm_pmu_event_filter f = {\n\t\t.nevents = 1,\n\t\t.flags = flags,\n\t\t.action = action,\n\t\t.events = {\n\t\t\tevent,\n\t\t},\n\t};\n\n\treturn set_pmu_event_filter(vcpu, &f);\n}\n\nstatic void test_filter_ioctl(struct kvm_vcpu *vcpu)\n{\n\tuint8_t nr_fixed_counters = kvm_cpu_property(X86_PROPERTY_PMU_NR_FIXED_COUNTERS);\n\tstruct __kvm_pmu_event_filter f;\n\tuint64_t e = ~0ul;\n\tint r;\n\n\t \n\tr = set_pmu_single_event_filter(vcpu, e, 0, KVM_PMU_EVENT_ALLOW);\n\tTEST_ASSERT(r == 0, \"Valid PMU Event Filter is failing\");\n\n\tr = set_pmu_single_event_filter(vcpu, e,\n\t\t\t\t\tKVM_PMU_EVENT_FLAG_MASKED_EVENTS,\n\t\t\t\t\tKVM_PMU_EVENT_ALLOW);\n\tTEST_ASSERT(r != 0, \"Invalid PMU Event Filter is expected to fail\");\n\n\te = KVM_PMU_ENCODE_MASKED_ENTRY(0xff, 0xff, 0xff, 0xf);\n\tr = set_pmu_single_event_filter(vcpu, e,\n\t\t\t\t\tKVM_PMU_EVENT_FLAG_MASKED_EVENTS,\n\t\t\t\t\tKVM_PMU_EVENT_ALLOW);\n\tTEST_ASSERT(r == 0, \"Valid PMU Event Filter is failing\");\n\n\tf = base_event_filter;\n\tf.action = PMU_EVENT_FILTER_INVALID_ACTION;\n\tr = set_pmu_event_filter(vcpu, &f);\n\tTEST_ASSERT(r, \"Set invalid action is expected to fail\");\n\n\tf = base_event_filter;\n\tf.flags = PMU_EVENT_FILTER_INVALID_FLAGS;\n\tr = set_pmu_event_filter(vcpu, &f);\n\tTEST_ASSERT(r, \"Set invalid flags is expected to fail\");\n\n\tf = base_event_filter;\n\tf.nevents = PMU_EVENT_FILTER_INVALID_NEVENTS;\n\tr = set_pmu_event_filter(vcpu, &f);\n\tTEST_ASSERT(r, \"Exceeding the max number of filter events should fail\");\n\n\tf = base_event_filter;\n\tf.fixed_counter_bitmap = ~GENMASK_ULL(nr_fixed_counters, 0);\n\tr = set_pmu_event_filter(vcpu, &f);\n\tTEST_ASSERT(!r, \"Masking non-existent fixed counters should be allowed\");\n}\n\nstatic void intel_run_fixed_counter_guest_code(uint8_t fixed_ctr_idx)\n{\n\tfor (;;) {\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\t\twrmsr(MSR_CORE_PERF_FIXED_CTR0 + fixed_ctr_idx, 0);\n\n\t\t \n\t\twrmsr(MSR_CORE_PERF_FIXED_CTR_CTRL, BIT_ULL(4 * fixed_ctr_idx));\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL,\n\t\t      BIT_ULL(INTEL_PMC_IDX_FIXED + fixed_ctr_idx));\n\t\t__asm__ __volatile__(\"loop .\" : \"+c\"((int){NUM_BRANCHES}));\n\t\twrmsr(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\n\t\tGUEST_SYNC(rdmsr(MSR_CORE_PERF_FIXED_CTR0 + fixed_ctr_idx));\n\t}\n}\n\nstatic uint64_t test_with_fixed_counter_filter(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       uint32_t action, uint32_t bitmap)\n{\n\tstruct __kvm_pmu_event_filter f = {\n\t\t.action = action,\n\t\t.fixed_counter_bitmap = bitmap,\n\t};\n\tset_pmu_event_filter(vcpu, &f);\n\n\treturn run_vcpu_to_sync(vcpu);\n}\n\nstatic uint64_t test_set_gp_and_fixed_event_filter(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t   uint32_t action,\n\t\t\t\t\t\t   uint32_t bitmap)\n{\n\tstruct __kvm_pmu_event_filter f = base_event_filter;\n\n\tf.action = action;\n\tf.fixed_counter_bitmap = bitmap;\n\tset_pmu_event_filter(vcpu, &f);\n\n\treturn run_vcpu_to_sync(vcpu);\n}\n\nstatic void __test_fixed_counter_bitmap(struct kvm_vcpu *vcpu, uint8_t idx,\n\t\t\t\t\tuint8_t nr_fixed_counters)\n{\n\tunsigned int i;\n\tuint32_t bitmap;\n\tuint64_t count;\n\n\tTEST_ASSERT(nr_fixed_counters < sizeof(bitmap) * 8,\n\t\t    \"Invalid nr_fixed_counters\");\n\n\t \n\tcount = run_vcpu_to_sync(vcpu);\n\tTEST_ASSERT(count, \"Unexpected count value: %ld\\n\", count);\n\n\tfor (i = 0; i < BIT(nr_fixed_counters); i++) {\n\t\tbitmap = BIT(i);\n\t\tcount = test_with_fixed_counter_filter(vcpu, KVM_PMU_EVENT_ALLOW,\n\t\t\t\t\t\t       bitmap);\n\t\tTEST_ASSERT_EQ(!!count, !!(bitmap & BIT(idx)));\n\n\t\tcount = test_with_fixed_counter_filter(vcpu, KVM_PMU_EVENT_DENY,\n\t\t\t\t\t\t       bitmap);\n\t\tTEST_ASSERT_EQ(!!count, !(bitmap & BIT(idx)));\n\n\t\t \n\t\tcount = test_set_gp_and_fixed_event_filter(vcpu,\n\t\t\t\t\t\t\t   KVM_PMU_EVENT_ALLOW,\n\t\t\t\t\t\t\t   bitmap);\n\t\tTEST_ASSERT_EQ(!!count, !!(bitmap & BIT(idx)));\n\n\t\tcount = test_set_gp_and_fixed_event_filter(vcpu,\n\t\t\t\t\t\t\t   KVM_PMU_EVENT_DENY,\n\t\t\t\t\t\t\t   bitmap);\n\t\tTEST_ASSERT_EQ(!!count, !(bitmap & BIT(idx)));\n\t}\n}\n\nstatic void test_fixed_counter_bitmap(void)\n{\n\tuint8_t nr_fixed_counters = kvm_cpu_property(X86_PROPERTY_PMU_NR_FIXED_COUNTERS);\n\tstruct kvm_vm *vm;\n\tstruct kvm_vcpu *vcpu;\n\tuint8_t idx;\n\n\t \n\tfor (idx = 0; idx < nr_fixed_counters; idx++) {\n\t\tvm = vm_create_with_one_vcpu(&vcpu,\n\t\t\t\t\t     intel_run_fixed_counter_guest_code);\n\t\tvcpu_args_set(vcpu, 1, idx);\n\t\t__test_fixed_counter_bitmap(vcpu, idx, nr_fixed_counters);\n\t\tkvm_vm_free(vm);\n\t}\n}\n\nint main(int argc, char *argv[])\n{\n\tvoid (*guest_code)(void);\n\tstruct kvm_vcpu *vcpu, *vcpu2 = NULL;\n\tstruct kvm_vm *vm;\n\n\tTEST_REQUIRE(get_kvm_param_bool(\"enable_pmu\"));\n\tTEST_REQUIRE(kvm_has_cap(KVM_CAP_PMU_EVENT_FILTER));\n\tTEST_REQUIRE(kvm_has_cap(KVM_CAP_PMU_EVENT_MASKED_EVENTS));\n\n\tTEST_REQUIRE(use_intel_pmu() || use_amd_pmu());\n\tguest_code = use_intel_pmu() ? intel_guest_code : amd_guest_code;\n\n\tvm = vm_create_with_one_vcpu(&vcpu, guest_code);\n\n\tvm_init_descriptor_tables(vm);\n\tvcpu_init_descriptor_tables(vcpu);\n\n\tTEST_REQUIRE(sanity_check_pmu(vcpu));\n\n\tif (use_amd_pmu())\n\t\ttest_amd_deny_list(vcpu);\n\n\ttest_without_filter(vcpu);\n\ttest_member_deny_list(vcpu);\n\ttest_member_allow_list(vcpu);\n\ttest_not_member_deny_list(vcpu);\n\ttest_not_member_allow_list(vcpu);\n\n\tif (use_intel_pmu() &&\n\t    supports_event_mem_inst_retired() &&\n\t    kvm_cpu_property(X86_PROPERTY_PMU_NR_GP_COUNTERS) >= 3)\n\t\tvcpu2 = vm_vcpu_add(vm, 2, intel_masked_events_guest_code);\n\telse if (use_amd_pmu())\n\t\tvcpu2 = vm_vcpu_add(vm, 2, amd_masked_events_guest_code);\n\n\tif (vcpu2)\n\t\ttest_masked_events(vcpu2);\n\ttest_filter_ioctl(vcpu);\n\n\tkvm_vm_free(vm);\n\n\ttest_pmu_config_disable(guest_code);\n\ttest_fixed_counter_bitmap();\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}