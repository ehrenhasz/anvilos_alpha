{
  "module_name": "memslot_perf_test.c",
  "hash_id": "f2b6a7ae2fa550ce2e2dcf1d1318ca7e86e51d54fa6cbf7ed201ec84868ef983",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/memslot_perf_test.c",
  "human_readable_source": "\n \n#include <pthread.h>\n#include <sched.h>\n#include <semaphore.h>\n#include <stdatomic.h>\n#include <stdbool.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/mman.h>\n#include <time.h>\n#include <unistd.h>\n\n#include <linux/compiler.h>\n#include <linux/sizes.h>\n\n#include <test_util.h>\n#include <kvm_util.h>\n#include <processor.h>\n\n#define MEM_EXTRA_SIZE\t\tSZ_64K\n\n#define MEM_SIZE\t\t(SZ_512M + MEM_EXTRA_SIZE)\n#define MEM_GPA\t\t\tSZ_256M\n#define MEM_AUX_GPA\t\tMEM_GPA\n#define MEM_SYNC_GPA\t\tMEM_AUX_GPA\n#define MEM_TEST_GPA\t\t(MEM_AUX_GPA + MEM_EXTRA_SIZE)\n#define MEM_TEST_SIZE\t\t(MEM_SIZE - MEM_EXTRA_SIZE)\n\n \n#define MEM_SIZE_MAP\t\t(SZ_32M + MEM_EXTRA_SIZE)\n#define MEM_TEST_MAP_SIZE\t(MEM_SIZE_MAP - MEM_EXTRA_SIZE)\n\n \n#define MEM_TEST_UNMAP_SIZE\t\tSZ_128M\n#define MEM_TEST_UNMAP_CHUNK_SIZE\tSZ_2M\n\n \n#define MEM_TEST_MOVE_SIZE\t\t(3 * SZ_64K)\n#define MEM_TEST_MOVE_GPA_DEST\t\t(MEM_GPA + MEM_SIZE)\nstatic_assert(MEM_TEST_MOVE_SIZE <= MEM_TEST_SIZE,\n\t      \"invalid move test region size\");\n\n#define MEM_TEST_VAL_1 0x1122334455667788\n#define MEM_TEST_VAL_2 0x99AABBCCDDEEFF00\n\nstruct vm_data {\n\tstruct kvm_vm *vm;\n\tstruct kvm_vcpu *vcpu;\n\tpthread_t vcpu_thread;\n\tuint32_t nslots;\n\tuint64_t npages;\n\tuint64_t pages_per_slot;\n\tvoid **hva_slots;\n\tbool mmio_ok;\n\tuint64_t mmio_gpa_min;\n\tuint64_t mmio_gpa_max;\n};\n\nstruct sync_area {\n\tuint32_t    guest_page_size;\n\tatomic_bool start_flag;\n\tatomic_bool exit_flag;\n\tatomic_bool sync_flag;\n\tvoid *move_area_ptr;\n};\n\n \nstatic_assert(ATOMIC_BOOL_LOCK_FREE == 2, \"atomic bool is not lockless\");\n\nstatic sem_t vcpu_ready;\n\nstatic bool map_unmap_verify;\n\nstatic bool verbose;\n#define pr_info_v(...)\t\t\t\t\\\n\tdo {\t\t\t\t\t\\\n\t\tif (verbose)\t\t\t\\\n\t\t\tpr_info(__VA_ARGS__);\t\\\n\t} while (0)\n\nstatic void check_mmio_access(struct vm_data *data, struct kvm_run *run)\n{\n\tTEST_ASSERT(data->mmio_ok, \"Unexpected mmio exit\");\n\tTEST_ASSERT(run->mmio.is_write, \"Unexpected mmio read\");\n\tTEST_ASSERT(run->mmio.len == 8,\n\t\t    \"Unexpected exit mmio size = %u\", run->mmio.len);\n\tTEST_ASSERT(run->mmio.phys_addr >= data->mmio_gpa_min &&\n\t\t    run->mmio.phys_addr <= data->mmio_gpa_max,\n\t\t    \"Unexpected exit mmio address = 0x%llx\",\n\t\t    run->mmio.phys_addr);\n}\n\nstatic void *vcpu_worker(void *__data)\n{\n\tstruct vm_data *data = __data;\n\tstruct kvm_vcpu *vcpu = data->vcpu;\n\tstruct kvm_run *run = vcpu->run;\n\tstruct ucall uc;\n\n\twhile (1) {\n\t\tvcpu_run(vcpu);\n\n\t\tswitch (get_ucall(vcpu, &uc)) {\n\t\tcase UCALL_SYNC:\n\t\t\tTEST_ASSERT(uc.args[1] == 0,\n\t\t\t\t\"Unexpected sync ucall, got %lx\",\n\t\t\t\t(ulong)uc.args[1]);\n\t\t\tsem_post(&vcpu_ready);\n\t\t\tcontinue;\n\t\tcase UCALL_NONE:\n\t\t\tif (run->exit_reason == KVM_EXIT_MMIO)\n\t\t\t\tcheck_mmio_access(data, run);\n\t\t\telse\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\tcase UCALL_ABORT:\n\t\t\tREPORT_GUEST_ASSERT(uc);\n\t\t\tbreak;\n\t\tcase UCALL_DONE:\n\t\t\tgoto done;\n\t\tdefault:\n\t\t\tTEST_FAIL(\"Unknown ucall %lu\", uc.cmd);\n\t\t}\n\t}\n\ndone:\n\treturn NULL;\n}\n\nstatic void wait_for_vcpu(void)\n{\n\tstruct timespec ts;\n\n\tTEST_ASSERT(!clock_gettime(CLOCK_REALTIME, &ts),\n\t\t    \"clock_gettime() failed: %d\\n\", errno);\n\n\tts.tv_sec += 2;\n\tTEST_ASSERT(!sem_timedwait(&vcpu_ready, &ts),\n\t\t    \"sem_timedwait() failed: %d\\n\", errno);\n}\n\nstatic void *vm_gpa2hva(struct vm_data *data, uint64_t gpa, uint64_t *rempages)\n{\n\tuint64_t gpage, pgoffs;\n\tuint32_t slot, slotoffs;\n\tvoid *base;\n\tuint32_t guest_page_size = data->vm->page_size;\n\n\tTEST_ASSERT(gpa >= MEM_GPA, \"Too low gpa to translate\");\n\tTEST_ASSERT(gpa < MEM_GPA + data->npages * guest_page_size,\n\t\t    \"Too high gpa to translate\");\n\tgpa -= MEM_GPA;\n\n\tgpage = gpa / guest_page_size;\n\tpgoffs = gpa % guest_page_size;\n\tslot = min(gpage / data->pages_per_slot, (uint64_t)data->nslots - 1);\n\tslotoffs = gpage - (slot * data->pages_per_slot);\n\n\tif (rempages) {\n\t\tuint64_t slotpages;\n\n\t\tif (slot == data->nslots - 1)\n\t\t\tslotpages = data->npages - slot * data->pages_per_slot;\n\t\telse\n\t\t\tslotpages = data->pages_per_slot;\n\n\t\tTEST_ASSERT(!pgoffs,\n\t\t\t    \"Asking for remaining pages in slot but gpa not page aligned\");\n\t\t*rempages = slotpages - slotoffs;\n\t}\n\n\tbase = data->hva_slots[slot];\n\treturn (uint8_t *)base + slotoffs * guest_page_size + pgoffs;\n}\n\nstatic uint64_t vm_slot2gpa(struct vm_data *data, uint32_t slot)\n{\n\tuint32_t guest_page_size = data->vm->page_size;\n\n\tTEST_ASSERT(slot < data->nslots, \"Too high slot number\");\n\n\treturn MEM_GPA + slot * data->pages_per_slot * guest_page_size;\n}\n\nstatic struct vm_data *alloc_vm(void)\n{\n\tstruct vm_data *data;\n\n\tdata = malloc(sizeof(*data));\n\tTEST_ASSERT(data, \"malloc(vmdata) failed\");\n\n\tdata->vm = NULL;\n\tdata->vcpu = NULL;\n\tdata->hva_slots = NULL;\n\n\treturn data;\n}\n\nstatic bool check_slot_pages(uint32_t host_page_size, uint32_t guest_page_size,\n\t\t\t     uint64_t pages_per_slot, uint64_t rempages)\n{\n\tif (!pages_per_slot)\n\t\treturn false;\n\n\tif ((pages_per_slot * guest_page_size) % host_page_size)\n\t\treturn false;\n\n\tif ((rempages * guest_page_size) % host_page_size)\n\t\treturn false;\n\n\treturn true;\n}\n\n\nstatic uint64_t get_max_slots(struct vm_data *data, uint32_t host_page_size)\n{\n\tuint32_t guest_page_size = data->vm->page_size;\n\tuint64_t mempages, pages_per_slot, rempages;\n\tuint64_t slots;\n\n\tmempages = data->npages;\n\tslots = data->nslots;\n\twhile (--slots > 1) {\n\t\tpages_per_slot = mempages / slots;\n\t\tif (!pages_per_slot)\n\t\t\tcontinue;\n\n\t\trempages = mempages % pages_per_slot;\n\t\tif (check_slot_pages(host_page_size, guest_page_size,\n\t\t\t\t     pages_per_slot, rempages))\n\t\t\treturn slots + 1;\t \n\t}\n\n\treturn 0;\n}\n\nstatic bool prepare_vm(struct vm_data *data, int nslots, uint64_t *maxslots,\n\t\t       void *guest_code, uint64_t mem_size,\n\t\t       struct timespec *slot_runtime)\n{\n\tuint64_t mempages, rempages;\n\tuint64_t guest_addr;\n\tuint32_t slot, host_page_size, guest_page_size;\n\tstruct timespec tstart;\n\tstruct sync_area *sync;\n\n\thost_page_size = getpagesize();\n\tguest_page_size = vm_guest_mode_params[VM_MODE_DEFAULT].page_size;\n\tmempages = mem_size / guest_page_size;\n\n\tdata->vm = __vm_create_with_one_vcpu(&data->vcpu, mempages, guest_code);\n\tTEST_ASSERT(data->vm->page_size == guest_page_size, \"Invalid VM page size\");\n\n\tdata->npages = mempages;\n\tTEST_ASSERT(data->npages > 1, \"Can't test without any memory\");\n\tdata->nslots = nslots;\n\tdata->pages_per_slot = data->npages / data->nslots;\n\trempages = data->npages % data->nslots;\n\tif (!check_slot_pages(host_page_size, guest_page_size,\n\t\t\t      data->pages_per_slot, rempages)) {\n\t\t*maxslots = get_max_slots(data, host_page_size);\n\t\treturn false;\n\t}\n\n\tdata->hva_slots = malloc(sizeof(*data->hva_slots) * data->nslots);\n\tTEST_ASSERT(data->hva_slots, \"malloc() fail\");\n\n\tpr_info_v(\"Adding slots 1..%i, each slot with %\"PRIu64\" pages + %\"PRIu64\" extra pages last\\n\",\n\t\tdata->nslots, data->pages_per_slot, rempages);\n\n\tclock_gettime(CLOCK_MONOTONIC, &tstart);\n\tfor (slot = 1, guest_addr = MEM_GPA; slot <= data->nslots; slot++) {\n\t\tuint64_t npages;\n\n\t\tnpages = data->pages_per_slot;\n\t\tif (slot == data->nslots)\n\t\t\tnpages += rempages;\n\n\t\tvm_userspace_mem_region_add(data->vm, VM_MEM_SRC_ANONYMOUS,\n\t\t\t\t\t    guest_addr, slot, npages,\n\t\t\t\t\t    0);\n\t\tguest_addr += npages * guest_page_size;\n\t}\n\t*slot_runtime = timespec_elapsed(tstart);\n\n\tfor (slot = 1, guest_addr = MEM_GPA; slot <= data->nslots; slot++) {\n\t\tuint64_t npages;\n\t\tuint64_t gpa;\n\n\t\tnpages = data->pages_per_slot;\n\t\tif (slot == data->nslots)\n\t\t\tnpages += rempages;\n\n\t\tgpa = vm_phy_pages_alloc(data->vm, npages, guest_addr, slot);\n\t\tTEST_ASSERT(gpa == guest_addr,\n\t\t\t    \"vm_phy_pages_alloc() failed\\n\");\n\n\t\tdata->hva_slots[slot - 1] = addr_gpa2hva(data->vm, guest_addr);\n\t\tmemset(data->hva_slots[slot - 1], 0, npages * guest_page_size);\n\n\t\tguest_addr += npages * guest_page_size;\n\t}\n\n\tvirt_map(data->vm, MEM_GPA, MEM_GPA, data->npages);\n\n\tsync = (typeof(sync))vm_gpa2hva(data, MEM_SYNC_GPA, NULL);\n\tsync->guest_page_size = data->vm->page_size;\n\tatomic_init(&sync->start_flag, false);\n\tatomic_init(&sync->exit_flag, false);\n\tatomic_init(&sync->sync_flag, false);\n\n\tdata->mmio_ok = false;\n\n\treturn true;\n}\n\nstatic void launch_vm(struct vm_data *data)\n{\n\tpr_info_v(\"Launching the test VM\\n\");\n\n\tpthread_create(&data->vcpu_thread, NULL, vcpu_worker, data);\n\n\t \n\twait_for_vcpu();\n}\n\nstatic void free_vm(struct vm_data *data)\n{\n\tkvm_vm_free(data->vm);\n\tfree(data->hva_slots);\n\tfree(data);\n}\n\nstatic void wait_guest_exit(struct vm_data *data)\n{\n\tpthread_join(data->vcpu_thread, NULL);\n}\n\nstatic void let_guest_run(struct sync_area *sync)\n{\n\tatomic_store_explicit(&sync->start_flag, true, memory_order_release);\n}\n\nstatic void guest_spin_until_start(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\n\twhile (!atomic_load_explicit(&sync->start_flag, memory_order_acquire))\n\t\t;\n}\n\nstatic void make_guest_exit(struct sync_area *sync)\n{\n\tatomic_store_explicit(&sync->exit_flag, true, memory_order_release);\n}\n\nstatic bool _guest_should_exit(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\n\treturn atomic_load_explicit(&sync->exit_flag, memory_order_acquire);\n}\n\n#define guest_should_exit() unlikely(_guest_should_exit())\n\n \nstatic noinline void host_perform_sync(struct sync_area *sync)\n{\n\talarm(2);\n\n\tatomic_store_explicit(&sync->sync_flag, true, memory_order_release);\n\twhile (atomic_load_explicit(&sync->sync_flag, memory_order_acquire))\n\t\t;\n\n\talarm(0);\n}\n\nstatic bool guest_perform_sync(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\tbool expected;\n\n\tdo {\n\t\tif (guest_should_exit())\n\t\t\treturn false;\n\n\t\texpected = true;\n\t} while (!atomic_compare_exchange_weak_explicit(&sync->sync_flag,\n\t\t\t\t\t\t\t&expected, false,\n\t\t\t\t\t\t\tmemory_order_acq_rel,\n\t\t\t\t\t\t\tmemory_order_relaxed));\n\n\treturn true;\n}\n\nstatic void guest_code_test_memslot_move(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\tuint32_t page_size = (typeof(page_size))READ_ONCE(sync->guest_page_size);\n\tuintptr_t base = (typeof(base))READ_ONCE(sync->move_area_ptr);\n\n\tGUEST_SYNC(0);\n\n\tguest_spin_until_start();\n\n\twhile (!guest_should_exit()) {\n\t\tuintptr_t ptr;\n\n\t\tfor (ptr = base; ptr < base + MEM_TEST_MOVE_SIZE;\n\t\t     ptr += page_size)\n\t\t\t*(uint64_t *)ptr = MEM_TEST_VAL_1;\n\n\t\t \n\t}\n\n\tGUEST_DONE();\n}\n\nstatic void guest_code_test_memslot_map(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\tuint32_t page_size = (typeof(page_size))READ_ONCE(sync->guest_page_size);\n\n\tGUEST_SYNC(0);\n\n\tguest_spin_until_start();\n\n\twhile (1) {\n\t\tuintptr_t ptr;\n\n\t\tfor (ptr = MEM_TEST_GPA;\n\t\t     ptr < MEM_TEST_GPA + MEM_TEST_MAP_SIZE / 2;\n\t\t     ptr += page_size)\n\t\t\t*(uint64_t *)ptr = MEM_TEST_VAL_1;\n\n\t\tif (!guest_perform_sync())\n\t\t\tbreak;\n\n\t\tfor (ptr = MEM_TEST_GPA + MEM_TEST_MAP_SIZE / 2;\n\t\t     ptr < MEM_TEST_GPA + MEM_TEST_MAP_SIZE;\n\t\t     ptr += page_size)\n\t\t\t*(uint64_t *)ptr = MEM_TEST_VAL_2;\n\n\t\tif (!guest_perform_sync())\n\t\t\tbreak;\n\t}\n\n\tGUEST_DONE();\n}\n\nstatic void guest_code_test_memslot_unmap(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\n\tGUEST_SYNC(0);\n\n\tguest_spin_until_start();\n\n\twhile (1) {\n\t\tuintptr_t ptr = MEM_TEST_GPA;\n\n\t\t \n\t\t*(uint64_t *)ptr = MEM_TEST_VAL_1;\n\n\t\tif (!guest_perform_sync())\n\t\t\tbreak;\n\n\t\tptr += MEM_TEST_UNMAP_SIZE / 2;\n\t\t*(uint64_t *)ptr = MEM_TEST_VAL_2;\n\n\t\tif (!guest_perform_sync())\n\t\t\tbreak;\n\t}\n\n\tGUEST_DONE();\n}\n\nstatic void guest_code_test_memslot_rw(void)\n{\n\tstruct sync_area *sync = (typeof(sync))MEM_SYNC_GPA;\n\tuint32_t page_size = (typeof(page_size))READ_ONCE(sync->guest_page_size);\n\n\tGUEST_SYNC(0);\n\n\tguest_spin_until_start();\n\n\twhile (1) {\n\t\tuintptr_t ptr;\n\n\t\tfor (ptr = MEM_TEST_GPA;\n\t\t     ptr < MEM_TEST_GPA + MEM_TEST_SIZE; ptr += page_size)\n\t\t\t*(uint64_t *)ptr = MEM_TEST_VAL_1;\n\n\t\tif (!guest_perform_sync())\n\t\t\tbreak;\n\n\t\tfor (ptr = MEM_TEST_GPA + page_size / 2;\n\t\t     ptr < MEM_TEST_GPA + MEM_TEST_SIZE; ptr += page_size) {\n\t\t\tuint64_t val = *(uint64_t *)ptr;\n\n\t\t\tGUEST_ASSERT_EQ(val, MEM_TEST_VAL_2);\n\t\t\t*(uint64_t *)ptr = 0;\n\t\t}\n\n\t\tif (!guest_perform_sync())\n\t\t\tbreak;\n\t}\n\n\tGUEST_DONE();\n}\n\nstatic bool test_memslot_move_prepare(struct vm_data *data,\n\t\t\t\t      struct sync_area *sync,\n\t\t\t\t      uint64_t *maxslots, bool isactive)\n{\n\tuint32_t guest_page_size = data->vm->page_size;\n\tuint64_t movesrcgpa, movetestgpa;\n\n\tmovesrcgpa = vm_slot2gpa(data, data->nslots - 1);\n\n\tif (isactive) {\n\t\tuint64_t lastpages;\n\n\t\tvm_gpa2hva(data, movesrcgpa, &lastpages);\n\t\tif (lastpages * guest_page_size < MEM_TEST_MOVE_SIZE / 2) {\n\t\t\t*maxslots = 0;\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tmovetestgpa = movesrcgpa - (MEM_TEST_MOVE_SIZE / (isactive ? 2 : 1));\n\tsync->move_area_ptr = (void *)movetestgpa;\n\n\tif (isactive) {\n\t\tdata->mmio_ok = true;\n\t\tdata->mmio_gpa_min = movesrcgpa;\n\t\tdata->mmio_gpa_max = movesrcgpa + MEM_TEST_MOVE_SIZE / 2 - 1;\n\t}\n\n\treturn true;\n}\n\nstatic bool test_memslot_move_prepare_active(struct vm_data *data,\n\t\t\t\t\t     struct sync_area *sync,\n\t\t\t\t\t     uint64_t *maxslots)\n{\n\treturn test_memslot_move_prepare(data, sync, maxslots, true);\n}\n\nstatic bool test_memslot_move_prepare_inactive(struct vm_data *data,\n\t\t\t\t\t       struct sync_area *sync,\n\t\t\t\t\t       uint64_t *maxslots)\n{\n\treturn test_memslot_move_prepare(data, sync, maxslots, false);\n}\n\nstatic void test_memslot_move_loop(struct vm_data *data, struct sync_area *sync)\n{\n\tuint64_t movesrcgpa;\n\n\tmovesrcgpa = vm_slot2gpa(data, data->nslots - 1);\n\tvm_mem_region_move(data->vm, data->nslots - 1 + 1,\n\t\t\t   MEM_TEST_MOVE_GPA_DEST);\n\tvm_mem_region_move(data->vm, data->nslots - 1 + 1, movesrcgpa);\n}\n\nstatic void test_memslot_do_unmap(struct vm_data *data,\n\t\t\t\t  uint64_t offsp, uint64_t count)\n{\n\tuint64_t gpa, ctr;\n\tuint32_t guest_page_size = data->vm->page_size;\n\n\tfor (gpa = MEM_TEST_GPA + offsp * guest_page_size, ctr = 0; ctr < count; ) {\n\t\tuint64_t npages;\n\t\tvoid *hva;\n\t\tint ret;\n\n\t\thva = vm_gpa2hva(data, gpa, &npages);\n\t\tTEST_ASSERT(npages, \"Empty memory slot at gptr 0x%\"PRIx64, gpa);\n\t\tnpages = min(npages, count - ctr);\n\t\tret = madvise(hva, npages * guest_page_size, MADV_DONTNEED);\n\t\tTEST_ASSERT(!ret,\n\t\t\t    \"madvise(%p, MADV_DONTNEED) on VM memory should not fail for gptr 0x%\"PRIx64,\n\t\t\t    hva, gpa);\n\t\tctr += npages;\n\t\tgpa += npages * guest_page_size;\n\t}\n\tTEST_ASSERT(ctr == count,\n\t\t    \"madvise(MADV_DONTNEED) should exactly cover all of the requested area\");\n}\n\nstatic void test_memslot_map_unmap_check(struct vm_data *data,\n\t\t\t\t\t uint64_t offsp, uint64_t valexp)\n{\n\tuint64_t gpa;\n\tuint64_t *val;\n\tuint32_t guest_page_size = data->vm->page_size;\n\n\tif (!map_unmap_verify)\n\t\treturn;\n\n\tgpa = MEM_TEST_GPA + offsp * guest_page_size;\n\tval = (typeof(val))vm_gpa2hva(data, gpa, NULL);\n\tTEST_ASSERT(*val == valexp,\n\t\t    \"Guest written values should read back correctly before unmap (%\"PRIu64\" vs %\"PRIu64\" @ %\"PRIx64\")\",\n\t\t    *val, valexp, gpa);\n\t*val = 0;\n}\n\nstatic void test_memslot_map_loop(struct vm_data *data, struct sync_area *sync)\n{\n\tuint32_t guest_page_size = data->vm->page_size;\n\tuint64_t guest_pages = MEM_TEST_MAP_SIZE / guest_page_size;\n\n\t \n\ttest_memslot_do_unmap(data, guest_pages / 2, guest_pages / 2);\n\n\t \n\thost_perform_sync(sync);\n\ttest_memslot_map_unmap_check(data, 0, MEM_TEST_VAL_1);\n\ttest_memslot_map_unmap_check(data, guest_pages / 2 - 1, MEM_TEST_VAL_1);\n\ttest_memslot_do_unmap(data, 0, guest_pages / 2);\n\n\n\t \n\thost_perform_sync(sync);\n\ttest_memslot_map_unmap_check(data, guest_pages / 2, MEM_TEST_VAL_2);\n\ttest_memslot_map_unmap_check(data, guest_pages - 1, MEM_TEST_VAL_2);\n}\n\nstatic void test_memslot_unmap_loop_common(struct vm_data *data,\n\t\t\t\t\t   struct sync_area *sync,\n\t\t\t\t\t   uint64_t chunk)\n{\n\tuint32_t guest_page_size = data->vm->page_size;\n\tuint64_t guest_pages = MEM_TEST_UNMAP_SIZE / guest_page_size;\n\tuint64_t ctr;\n\n\t \n\thost_perform_sync(sync);\n\ttest_memslot_map_unmap_check(data, 0, MEM_TEST_VAL_1);\n\tfor (ctr = 0; ctr < guest_pages / 2; ctr += chunk)\n\t\ttest_memslot_do_unmap(data, ctr, chunk);\n\n\t \n\thost_perform_sync(sync);\n\ttest_memslot_map_unmap_check(data, guest_pages / 2, MEM_TEST_VAL_2);\n\tfor (ctr = guest_pages / 2; ctr < guest_pages; ctr += chunk)\n\t\ttest_memslot_do_unmap(data, ctr, chunk);\n}\n\nstatic void test_memslot_unmap_loop(struct vm_data *data,\n\t\t\t\t    struct sync_area *sync)\n{\n\tuint32_t host_page_size = getpagesize();\n\tuint32_t guest_page_size = data->vm->page_size;\n\tuint64_t guest_chunk_pages = guest_page_size >= host_page_size ?\n\t\t\t\t\t1 : host_page_size / guest_page_size;\n\n\ttest_memslot_unmap_loop_common(data, sync, guest_chunk_pages);\n}\n\nstatic void test_memslot_unmap_loop_chunked(struct vm_data *data,\n\t\t\t\t\t    struct sync_area *sync)\n{\n\tuint32_t guest_page_size = data->vm->page_size;\n\tuint64_t guest_chunk_pages = MEM_TEST_UNMAP_CHUNK_SIZE / guest_page_size;\n\n\ttest_memslot_unmap_loop_common(data, sync, guest_chunk_pages);\n}\n\nstatic void test_memslot_rw_loop(struct vm_data *data, struct sync_area *sync)\n{\n\tuint64_t gptr;\n\tuint32_t guest_page_size = data->vm->page_size;\n\n\tfor (gptr = MEM_TEST_GPA + guest_page_size / 2;\n\t     gptr < MEM_TEST_GPA + MEM_TEST_SIZE; gptr += guest_page_size)\n\t\t*(uint64_t *)vm_gpa2hva(data, gptr, NULL) = MEM_TEST_VAL_2;\n\n\thost_perform_sync(sync);\n\n\tfor (gptr = MEM_TEST_GPA;\n\t     gptr < MEM_TEST_GPA + MEM_TEST_SIZE; gptr += guest_page_size) {\n\t\tuint64_t *vptr = (typeof(vptr))vm_gpa2hva(data, gptr, NULL);\n\t\tuint64_t val = *vptr;\n\n\t\tTEST_ASSERT(val == MEM_TEST_VAL_1,\n\t\t\t    \"Guest written values should read back correctly (is %\"PRIu64\" @ %\"PRIx64\")\",\n\t\t\t    val, gptr);\n\t\t*vptr = 0;\n\t}\n\n\thost_perform_sync(sync);\n}\n\nstruct test_data {\n\tconst char *name;\n\tuint64_t mem_size;\n\tvoid (*guest_code)(void);\n\tbool (*prepare)(struct vm_data *data, struct sync_area *sync,\n\t\t\tuint64_t *maxslots);\n\tvoid (*loop)(struct vm_data *data, struct sync_area *sync);\n};\n\nstatic bool test_execute(int nslots, uint64_t *maxslots,\n\t\t\t unsigned int maxtime,\n\t\t\t const struct test_data *tdata,\n\t\t\t uint64_t *nloops,\n\t\t\t struct timespec *slot_runtime,\n\t\t\t struct timespec *guest_runtime)\n{\n\tuint64_t mem_size = tdata->mem_size ? : MEM_SIZE;\n\tstruct vm_data *data;\n\tstruct sync_area *sync;\n\tstruct timespec tstart;\n\tbool ret = true;\n\n\tdata = alloc_vm();\n\tif (!prepare_vm(data, nslots, maxslots, tdata->guest_code,\n\t\t\tmem_size, slot_runtime)) {\n\t\tret = false;\n\t\tgoto exit_free;\n\t}\n\n\tsync = (typeof(sync))vm_gpa2hva(data, MEM_SYNC_GPA, NULL);\n\tif (tdata->prepare &&\n\t    !tdata->prepare(data, sync, maxslots)) {\n\t\tret = false;\n\t\tgoto exit_free;\n\t}\n\n\tlaunch_vm(data);\n\n\tclock_gettime(CLOCK_MONOTONIC, &tstart);\n\tlet_guest_run(sync);\n\n\twhile (1) {\n\t\t*guest_runtime = timespec_elapsed(tstart);\n\t\tif (guest_runtime->tv_sec >= maxtime)\n\t\t\tbreak;\n\n\t\ttdata->loop(data, sync);\n\n\t\t(*nloops)++;\n\t}\n\n\tmake_guest_exit(sync);\n\twait_guest_exit(data);\n\nexit_free:\n\tfree_vm(data);\n\n\treturn ret;\n}\n\nstatic const struct test_data tests[] = {\n\t{\n\t\t.name = \"map\",\n\t\t.mem_size = MEM_SIZE_MAP,\n\t\t.guest_code = guest_code_test_memslot_map,\n\t\t.loop = test_memslot_map_loop,\n\t},\n\t{\n\t\t.name = \"unmap\",\n\t\t.mem_size = MEM_TEST_UNMAP_SIZE + MEM_EXTRA_SIZE,\n\t\t.guest_code = guest_code_test_memslot_unmap,\n\t\t.loop = test_memslot_unmap_loop,\n\t},\n\t{\n\t\t.name = \"unmap chunked\",\n\t\t.mem_size = MEM_TEST_UNMAP_SIZE + MEM_EXTRA_SIZE,\n\t\t.guest_code = guest_code_test_memslot_unmap,\n\t\t.loop = test_memslot_unmap_loop_chunked,\n\t},\n\t{\n\t\t.name = \"move active area\",\n\t\t.guest_code = guest_code_test_memslot_move,\n\t\t.prepare = test_memslot_move_prepare_active,\n\t\t.loop = test_memslot_move_loop,\n\t},\n\t{\n\t\t.name = \"move inactive area\",\n\t\t.guest_code = guest_code_test_memslot_move,\n\t\t.prepare = test_memslot_move_prepare_inactive,\n\t\t.loop = test_memslot_move_loop,\n\t},\n\t{\n\t\t.name = \"RW\",\n\t\t.guest_code = guest_code_test_memslot_rw,\n\t\t.loop = test_memslot_rw_loop\n\t},\n};\n\n#define NTESTS ARRAY_SIZE(tests)\n\nstruct test_args {\n\tint tfirst;\n\tint tlast;\n\tint nslots;\n\tint seconds;\n\tint runs;\n};\n\nstatic void help(char *name, struct test_args *targs)\n{\n\tint ctr;\n\n\tpr_info(\"usage: %s [-h] [-v] [-d] [-s slots] [-f first_test] [-e last_test] [-l test_length] [-r run_count]\\n\",\n\t\tname);\n\tpr_info(\" -h: print this help screen.\\n\");\n\tpr_info(\" -v: enable verbose mode (not for benchmarking).\\n\");\n\tpr_info(\" -d: enable extra debug checks.\\n\");\n\tpr_info(\" -s: specify memslot count cap (-1 means no cap; currently: %i)\\n\",\n\t\ttargs->nslots);\n\tpr_info(\" -f: specify the first test to run (currently: %i; max %zu)\\n\",\n\t\ttargs->tfirst, NTESTS - 1);\n\tpr_info(\" -e: specify the last test to run (currently: %i; max %zu)\\n\",\n\t\ttargs->tlast, NTESTS - 1);\n\tpr_info(\" -l: specify the test length in seconds (currently: %i)\\n\",\n\t\ttargs->seconds);\n\tpr_info(\" -r: specify the number of runs per test (currently: %i)\\n\",\n\t\ttargs->runs);\n\n\tpr_info(\"\\nAvailable tests:\\n\");\n\tfor (ctr = 0; ctr < NTESTS; ctr++)\n\t\tpr_info(\"%d: %s\\n\", ctr, tests[ctr].name);\n}\n\nstatic bool check_memory_sizes(void)\n{\n\tuint32_t host_page_size = getpagesize();\n\tuint32_t guest_page_size = vm_guest_mode_params[VM_MODE_DEFAULT].page_size;\n\n\tif (host_page_size > SZ_64K || guest_page_size > SZ_64K) {\n\t\tpr_info(\"Unsupported page size on host (0x%x) or guest (0x%x)\\n\",\n\t\t\thost_page_size, guest_page_size);\n\t\treturn false;\n\t}\n\n\tif (MEM_SIZE % guest_page_size ||\n\t    MEM_TEST_SIZE % guest_page_size) {\n\t\tpr_info(\"invalid MEM_SIZE or MEM_TEST_SIZE\\n\");\n\t\treturn false;\n\t}\n\n\tif (MEM_SIZE_MAP % guest_page_size\t\t||\n\t    MEM_TEST_MAP_SIZE % guest_page_size\t\t||\n\t    (MEM_TEST_MAP_SIZE / guest_page_size) <= 2\t||\n\t    (MEM_TEST_MAP_SIZE / guest_page_size) % 2) {\n\t\tpr_info(\"invalid MEM_SIZE_MAP or MEM_TEST_MAP_SIZE\\n\");\n\t\treturn false;\n\t}\n\n\tif (MEM_TEST_UNMAP_SIZE > MEM_TEST_SIZE\t\t||\n\t    MEM_TEST_UNMAP_SIZE % guest_page_size\t||\n\t    (MEM_TEST_UNMAP_SIZE / guest_page_size) %\n\t    (2 * MEM_TEST_UNMAP_CHUNK_SIZE / guest_page_size)) {\n\t\tpr_info(\"invalid MEM_TEST_UNMAP_SIZE or MEM_TEST_UNMAP_CHUNK_SIZE\\n\");\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool parse_args(int argc, char *argv[],\n\t\t       struct test_args *targs)\n{\n\tuint32_t max_mem_slots;\n\tint opt;\n\n\twhile ((opt = getopt(argc, argv, \"hvds:f:e:l:r:\")) != -1) {\n\t\tswitch (opt) {\n\t\tcase 'h':\n\t\tdefault:\n\t\t\thelp(argv[0], targs);\n\t\t\treturn false;\n\t\tcase 'v':\n\t\t\tverbose = true;\n\t\t\tbreak;\n\t\tcase 'd':\n\t\t\tmap_unmap_verify = true;\n\t\t\tbreak;\n\t\tcase 's':\n\t\t\ttargs->nslots = atoi_paranoid(optarg);\n\t\t\tif (targs->nslots <= 1 && targs->nslots != -1) {\n\t\t\t\tpr_info(\"Slot count cap must be larger than 1 or -1 for no cap\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 'f':\n\t\t\ttargs->tfirst = atoi_non_negative(\"First test\", optarg);\n\t\t\tbreak;\n\t\tcase 'e':\n\t\t\ttargs->tlast = atoi_non_negative(\"Last test\", optarg);\n\t\t\tif (targs->tlast >= NTESTS) {\n\t\t\t\tpr_info(\"Last test to run has to be non-negative and less than %zu\\n\",\n\t\t\t\t\tNTESTS);\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 'l':\n\t\t\ttargs->seconds = atoi_non_negative(\"Test length\", optarg);\n\t\t\tbreak;\n\t\tcase 'r':\n\t\t\ttargs->runs = atoi_positive(\"Runs per test\", optarg);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (optind < argc) {\n\t\thelp(argv[0], targs);\n\t\treturn false;\n\t}\n\n\tif (targs->tfirst > targs->tlast) {\n\t\tpr_info(\"First test to run cannot be greater than the last test to run\\n\");\n\t\treturn false;\n\t}\n\n\tmax_mem_slots = kvm_check_cap(KVM_CAP_NR_MEMSLOTS);\n\tif (max_mem_slots <= 1) {\n\t\tpr_info(\"KVM_CAP_NR_MEMSLOTS should be greater than 1\\n\");\n\t\treturn false;\n\t}\n\n\t \n\tif (targs->nslots == -1)\n\t\ttargs->nslots = max_mem_slots - 1;\n\telse\n\t\ttargs->nslots = min_t(int, targs->nslots, max_mem_slots) - 1;\n\n\tpr_info_v(\"Allowed Number of memory slots: %\"PRIu32\"\\n\",\n\t\t  targs->nslots + 1);\n\n\treturn true;\n}\n\nstruct test_result {\n\tstruct timespec slot_runtime, guest_runtime, iter_runtime;\n\tint64_t slottimens, runtimens;\n\tuint64_t nloops;\n};\n\nstatic bool test_loop(const struct test_data *data,\n\t\t      const struct test_args *targs,\n\t\t      struct test_result *rbestslottime,\n\t\t      struct test_result *rbestruntime)\n{\n\tuint64_t maxslots;\n\tstruct test_result result = {};\n\n\tif (!test_execute(targs->nslots, &maxslots, targs->seconds, data,\n\t\t\t  &result.nloops,\n\t\t\t  &result.slot_runtime, &result.guest_runtime)) {\n\t\tif (maxslots)\n\t\t\tpr_info(\"Memslot count too high for this test, decrease the cap (max is %\"PRIu64\")\\n\",\n\t\t\t\tmaxslots);\n\t\telse\n\t\t\tpr_info(\"Memslot count may be too high for this test, try adjusting the cap\\n\");\n\n\t\treturn false;\n\t}\n\n\tpr_info(\"Test took %ld.%.9lds for slot setup + %ld.%.9lds all iterations\\n\",\n\t\tresult.slot_runtime.tv_sec, result.slot_runtime.tv_nsec,\n\t\tresult.guest_runtime.tv_sec, result.guest_runtime.tv_nsec);\n\tif (!result.nloops) {\n\t\tpr_info(\"No full loops done - too short test time or system too loaded?\\n\");\n\t\treturn true;\n\t}\n\n\tresult.iter_runtime = timespec_div(result.guest_runtime,\n\t\t\t\t\t   result.nloops);\n\tpr_info(\"Done %\"PRIu64\" iterations, avg %ld.%.9lds each\\n\",\n\t\tresult.nloops,\n\t\tresult.iter_runtime.tv_sec,\n\t\tresult.iter_runtime.tv_nsec);\n\tresult.slottimens = timespec_to_ns(result.slot_runtime);\n\tresult.runtimens = timespec_to_ns(result.iter_runtime);\n\n\t \n\tif (!data->mem_size &&\n\t    (!rbestslottime->slottimens ||\n\t     result.slottimens < rbestslottime->slottimens))\n\t\t*rbestslottime = result;\n\tif (!rbestruntime->runtimens ||\n\t    result.runtimens < rbestruntime->runtimens)\n\t\t*rbestruntime = result;\n\n\treturn true;\n}\n\nint main(int argc, char *argv[])\n{\n\tstruct test_args targs = {\n\t\t.tfirst = 0,\n\t\t.tlast = NTESTS - 1,\n\t\t.nslots = -1,\n\t\t.seconds = 5,\n\t\t.runs = 1,\n\t};\n\tstruct test_result rbestslottime = {};\n\tint tctr;\n\n\tif (!check_memory_sizes())\n\t\treturn -1;\n\n\tif (!parse_args(argc, argv, &targs))\n\t\treturn -1;\n\n\tfor (tctr = targs.tfirst; tctr <= targs.tlast; tctr++) {\n\t\tconst struct test_data *data = &tests[tctr];\n\t\tunsigned int runctr;\n\t\tstruct test_result rbestruntime = {};\n\n\t\tif (tctr > targs.tfirst)\n\t\t\tpr_info(\"\\n\");\n\n\t\tpr_info(\"Testing %s performance with %i runs, %d seconds each\\n\",\n\t\t\tdata->name, targs.runs, targs.seconds);\n\n\t\tfor (runctr = 0; runctr < targs.runs; runctr++)\n\t\t\tif (!test_loop(data, &targs,\n\t\t\t\t       &rbestslottime, &rbestruntime))\n\t\t\t\tbreak;\n\n\t\tif (rbestruntime.runtimens)\n\t\t\tpr_info(\"Best runtime result was %ld.%.9lds per iteration (with %\"PRIu64\" iterations)\\n\",\n\t\t\t\trbestruntime.iter_runtime.tv_sec,\n\t\t\t\trbestruntime.iter_runtime.tv_nsec,\n\t\t\t\trbestruntime.nloops);\n\t}\n\n\tif (rbestslottime.slottimens)\n\t\tpr_info(\"Best slot setup time for the whole test area was %ld.%.9lds\\n\",\n\t\t\trbestslottime.slot_runtime.tv_sec,\n\t\t\trbestslottime.slot_runtime.tv_nsec);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}