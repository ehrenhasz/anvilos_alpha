{
  "module_name": "processor.h",
  "hash_id": "db79b64ec449a0f17c1261b0f528c0c0aa9b662e01a1c88a12fc97149cc41219",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/include/x86_64/processor.h",
  "human_readable_source": " \n \n\n#ifndef SELFTEST_KVM_PROCESSOR_H\n#define SELFTEST_KVM_PROCESSOR_H\n\n#include <assert.h>\n#include <stdint.h>\n#include <syscall.h>\n\n#include <asm/msr-index.h>\n#include <asm/prctl.h>\n\n#include <linux/stringify.h>\n\n#include \"../kvm_util.h\"\n\nextern bool host_cpu_is_intel;\nextern bool host_cpu_is_amd;\n\n#define NMI_VECTOR\t\t0x02\n\n#define X86_EFLAGS_FIXED\t (1u << 1)\n\n#define X86_CR4_VME\t\t(1ul << 0)\n#define X86_CR4_PVI\t\t(1ul << 1)\n#define X86_CR4_TSD\t\t(1ul << 2)\n#define X86_CR4_DE\t\t(1ul << 3)\n#define X86_CR4_PSE\t\t(1ul << 4)\n#define X86_CR4_PAE\t\t(1ul << 5)\n#define X86_CR4_MCE\t\t(1ul << 6)\n#define X86_CR4_PGE\t\t(1ul << 7)\n#define X86_CR4_PCE\t\t(1ul << 8)\n#define X86_CR4_OSFXSR\t\t(1ul << 9)\n#define X86_CR4_OSXMMEXCPT\t(1ul << 10)\n#define X86_CR4_UMIP\t\t(1ul << 11)\n#define X86_CR4_LA57\t\t(1ul << 12)\n#define X86_CR4_VMXE\t\t(1ul << 13)\n#define X86_CR4_SMXE\t\t(1ul << 14)\n#define X86_CR4_FSGSBASE\t(1ul << 16)\n#define X86_CR4_PCIDE\t\t(1ul << 17)\n#define X86_CR4_OSXSAVE\t\t(1ul << 18)\n#define X86_CR4_SMEP\t\t(1ul << 20)\n#define X86_CR4_SMAP\t\t(1ul << 21)\n#define X86_CR4_PKE\t\t(1ul << 22)\n\nstruct xstate_header {\n\tu64\t\t\t\txstate_bv;\n\tu64\t\t\t\txcomp_bv;\n\tu64\t\t\t\treserved[6];\n} __attribute__((packed));\n\nstruct xstate {\n\tu8\t\t\t\ti387[512];\n\tstruct xstate_header\t\theader;\n\tu8\t\t\t\textended_state_area[0];\n} __attribute__ ((packed, aligned (64)));\n\n#define XFEATURE_MASK_FP\t\tBIT_ULL(0)\n#define XFEATURE_MASK_SSE\t\tBIT_ULL(1)\n#define XFEATURE_MASK_YMM\t\tBIT_ULL(2)\n#define XFEATURE_MASK_BNDREGS\t\tBIT_ULL(3)\n#define XFEATURE_MASK_BNDCSR\t\tBIT_ULL(4)\n#define XFEATURE_MASK_OPMASK\t\tBIT_ULL(5)\n#define XFEATURE_MASK_ZMM_Hi256\t\tBIT_ULL(6)\n#define XFEATURE_MASK_Hi16_ZMM\t\tBIT_ULL(7)\n#define XFEATURE_MASK_PT\t\tBIT_ULL(8)\n#define XFEATURE_MASK_PKRU\t\tBIT_ULL(9)\n#define XFEATURE_MASK_PASID\t\tBIT_ULL(10)\n#define XFEATURE_MASK_CET_USER\t\tBIT_ULL(11)\n#define XFEATURE_MASK_CET_KERNEL\tBIT_ULL(12)\n#define XFEATURE_MASK_LBR\t\tBIT_ULL(15)\n#define XFEATURE_MASK_XTILE_CFG\t\tBIT_ULL(17)\n#define XFEATURE_MASK_XTILE_DATA\tBIT_ULL(18)\n\n#define XFEATURE_MASK_AVX512\t\t(XFEATURE_MASK_OPMASK | \\\n\t\t\t\t\t XFEATURE_MASK_ZMM_Hi256 | \\\n\t\t\t\t\t XFEATURE_MASK_Hi16_ZMM)\n#define XFEATURE_MASK_XTILE\t\t(XFEATURE_MASK_XTILE_DATA | \\\n\t\t\t\t\t XFEATURE_MASK_XTILE_CFG)\n\n \nenum cpuid_output_regs {\n\tKVM_CPUID_EAX,\n\tKVM_CPUID_EBX,\n\tKVM_CPUID_ECX,\n\tKVM_CPUID_EDX\n};\n\n \nstruct kvm_x86_cpu_feature {\n\tu32\tfunction;\n\tu16\tindex;\n\tu8\treg;\n\tu8\tbit;\n};\n#define\tKVM_X86_CPU_FEATURE(fn, idx, gpr, __bit)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\\\n\tstruct kvm_x86_cpu_feature feature = {\t\t\t\t\t\\\n\t\t.function = fn,\t\t\t\t\t\t\t\\\n\t\t.index = idx,\t\t\t\t\t\t\t\\\n\t\t.reg = KVM_CPUID_##gpr,\t\t\t\t\t\t\\\n\t\t.bit = __bit,\t\t\t\t\t\t\t\\\n\t};\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tkvm_static_assert((fn & 0xc0000000) == 0 ||\t\t\t\t\\\n\t\t\t  (fn & 0xc0000000) == 0x40000000 ||\t\t\t\\\n\t\t\t  (fn & 0xc0000000) == 0x80000000 ||\t\t\t\\\n\t\t\t  (fn & 0xc0000000) == 0xc0000000);\t\t\t\\\n\tkvm_static_assert(idx < BIT(sizeof(feature.index) * BITS_PER_BYTE));\t\\\n\tfeature;\t\t\t\t\t\t\t\t\\\n})\n\n \n#define\tX86_FEATURE_MWAIT\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 3)\n#define\tX86_FEATURE_VMX\t\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 5)\n#define\tX86_FEATURE_SMX\t\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 6)\n#define\tX86_FEATURE_PDCM\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 15)\n#define\tX86_FEATURE_PCID\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 17)\n#define X86_FEATURE_X2APIC\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 21)\n#define\tX86_FEATURE_MOVBE\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 22)\n#define\tX86_FEATURE_TSC_DEADLINE_TIMER\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 24)\n#define\tX86_FEATURE_XSAVE\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 26)\n#define\tX86_FEATURE_OSXSAVE\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 27)\n#define\tX86_FEATURE_RDRAND\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 30)\n#define\tX86_FEATURE_HYPERVISOR\t\tKVM_X86_CPU_FEATURE(0x1, 0, ECX, 31)\n#define X86_FEATURE_PAE\t\t\tKVM_X86_CPU_FEATURE(0x1, 0, EDX, 6)\n#define\tX86_FEATURE_MCE\t\t\tKVM_X86_CPU_FEATURE(0x1, 0, EDX, 7)\n#define\tX86_FEATURE_APIC\t\tKVM_X86_CPU_FEATURE(0x1, 0, EDX, 9)\n#define\tX86_FEATURE_CLFLUSH\t\tKVM_X86_CPU_FEATURE(0x1, 0, EDX, 19)\n#define\tX86_FEATURE_XMM\t\t\tKVM_X86_CPU_FEATURE(0x1, 0, EDX, 25)\n#define\tX86_FEATURE_XMM2\t\tKVM_X86_CPU_FEATURE(0x1, 0, EDX, 26)\n#define\tX86_FEATURE_FSGSBASE\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 0)\n#define\tX86_FEATURE_TSC_ADJUST\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 1)\n#define\tX86_FEATURE_SGX\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 2)\n#define\tX86_FEATURE_HLE\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 4)\n#define\tX86_FEATURE_SMEP\t        KVM_X86_CPU_FEATURE(0x7, 0, EBX, 7)\n#define\tX86_FEATURE_INVPCID\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 10)\n#define\tX86_FEATURE_RTM\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 11)\n#define\tX86_FEATURE_MPX\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 14)\n#define\tX86_FEATURE_SMAP\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 20)\n#define\tX86_FEATURE_PCOMMIT\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 22)\n#define\tX86_FEATURE_CLFLUSHOPT\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 23)\n#define\tX86_FEATURE_CLWB\t\tKVM_X86_CPU_FEATURE(0x7, 0, EBX, 24)\n#define\tX86_FEATURE_UMIP\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 2)\n#define\tX86_FEATURE_PKU\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 3)\n#define\tX86_FEATURE_OSPKE\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 4)\n#define\tX86_FEATURE_LA57\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 16)\n#define\tX86_FEATURE_RDPID\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 22)\n#define\tX86_FEATURE_SGX_LC\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 30)\n#define\tX86_FEATURE_SHSTK\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 7)\n#define\tX86_FEATURE_IBT\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, EDX, 20)\n#define\tX86_FEATURE_AMX_TILE\t\tKVM_X86_CPU_FEATURE(0x7, 0, EDX, 24)\n#define\tX86_FEATURE_SPEC_CTRL\t\tKVM_X86_CPU_FEATURE(0x7, 0, EDX, 26)\n#define\tX86_FEATURE_ARCH_CAPABILITIES\tKVM_X86_CPU_FEATURE(0x7, 0, EDX, 29)\n#define\tX86_FEATURE_PKS\t\t\tKVM_X86_CPU_FEATURE(0x7, 0, ECX, 31)\n#define\tX86_FEATURE_XTILECFG\t\tKVM_X86_CPU_FEATURE(0xD, 0, EAX, 17)\n#define\tX86_FEATURE_XTILEDATA\t\tKVM_X86_CPU_FEATURE(0xD, 0, EAX, 18)\n#define\tX86_FEATURE_XSAVES\t\tKVM_X86_CPU_FEATURE(0xD, 1, EAX, 3)\n#define\tX86_FEATURE_XFD\t\t\tKVM_X86_CPU_FEATURE(0xD, 1, EAX, 4)\n#define X86_FEATURE_XTILEDATA_XFD\tKVM_X86_CPU_FEATURE(0xD, 18, ECX, 2)\n\n \n#define\tX86_FEATURE_SVM\t\t\tKVM_X86_CPU_FEATURE(0x80000001, 0, ECX, 2)\n#define\tX86_FEATURE_NX\t\t\tKVM_X86_CPU_FEATURE(0x80000001, 0, EDX, 20)\n#define\tX86_FEATURE_GBPAGES\t\tKVM_X86_CPU_FEATURE(0x80000001, 0, EDX, 26)\n#define\tX86_FEATURE_RDTSCP\t\tKVM_X86_CPU_FEATURE(0x80000001, 0, EDX, 27)\n#define\tX86_FEATURE_LM\t\t\tKVM_X86_CPU_FEATURE(0x80000001, 0, EDX, 29)\n#define\tX86_FEATURE_INVTSC\t\tKVM_X86_CPU_FEATURE(0x80000007, 0, EDX, 8)\n#define\tX86_FEATURE_RDPRU\t\tKVM_X86_CPU_FEATURE(0x80000008, 0, EBX, 4)\n#define\tX86_FEATURE_AMD_IBPB\t\tKVM_X86_CPU_FEATURE(0x80000008, 0, EBX, 12)\n#define\tX86_FEATURE_NPT\t\t\tKVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 0)\n#define\tX86_FEATURE_LBRV\t\tKVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 1)\n#define\tX86_FEATURE_NRIPS\t\tKVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 3)\n#define X86_FEATURE_TSCRATEMSR          KVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 4)\n#define X86_FEATURE_PAUSEFILTER         KVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 10)\n#define X86_FEATURE_PFTHRESHOLD         KVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 12)\n#define\tX86_FEATURE_VGIF\t\tKVM_X86_CPU_FEATURE(0x8000000A, 0, EDX, 16)\n#define X86_FEATURE_SEV\t\t\tKVM_X86_CPU_FEATURE(0x8000001F, 0, EAX, 1)\n#define X86_FEATURE_SEV_ES\t\tKVM_X86_CPU_FEATURE(0x8000001F, 0, EAX, 3)\n\n \n#define X86_FEATURE_KVM_CLOCKSOURCE\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 0)\n#define X86_FEATURE_KVM_NOP_IO_DELAY\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 1)\n#define X86_FEATURE_KVM_MMU_OP\t\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 2)\n#define X86_FEATURE_KVM_CLOCKSOURCE2\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 3)\n#define X86_FEATURE_KVM_ASYNC_PF\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 4)\n#define X86_FEATURE_KVM_STEAL_TIME\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 5)\n#define X86_FEATURE_KVM_PV_EOI\t\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 6)\n#define X86_FEATURE_KVM_PV_UNHALT\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 7)\n \n#define X86_FEATURE_KVM_PV_TLB_FLUSH\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 9)\n#define X86_FEATURE_KVM_ASYNC_PF_VMEXIT\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 10)\n#define X86_FEATURE_KVM_PV_SEND_IPI\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 11)\n#define X86_FEATURE_KVM_POLL_CONTROL\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 12)\n#define X86_FEATURE_KVM_PV_SCHED_YIELD\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 13)\n#define X86_FEATURE_KVM_ASYNC_PF_INT\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 14)\n#define X86_FEATURE_KVM_MSI_EXT_DEST_ID\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 15)\n#define X86_FEATURE_KVM_HC_MAP_GPA_RANGE\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 16)\n#define X86_FEATURE_KVM_MIGRATION_CONTROL\tKVM_X86_CPU_FEATURE(0x40000001, 0, EAX, 17)\n\n \nstruct kvm_x86_cpu_property {\n\tu32\tfunction;\n\tu8\tindex;\n\tu8\treg;\n\tu8\tlo_bit;\n\tu8\thi_bit;\n};\n#define\tKVM_X86_CPU_PROPERTY(fn, idx, gpr, low_bit, high_bit)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\\\n\tstruct kvm_x86_cpu_property property = {\t\t\t\t\\\n\t\t.function = fn,\t\t\t\t\t\t\t\\\n\t\t.index = idx,\t\t\t\t\t\t\t\\\n\t\t.reg = KVM_CPUID_##gpr,\t\t\t\t\t\t\\\n\t\t.lo_bit = low_bit,\t\t\t\t\t\t\\\n\t\t.hi_bit = high_bit,\t\t\t\t\t\t\\\n\t};\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tkvm_static_assert(low_bit < high_bit);\t\t\t\t\t\\\n\tkvm_static_assert((fn & 0xc0000000) == 0 ||\t\t\t\t\\\n\t\t\t  (fn & 0xc0000000) == 0x40000000 ||\t\t\t\\\n\t\t\t  (fn & 0xc0000000) == 0x80000000 ||\t\t\t\\\n\t\t\t  (fn & 0xc0000000) == 0xc0000000);\t\t\t\\\n\tkvm_static_assert(idx < BIT(sizeof(property.index) * BITS_PER_BYTE));\t\\\n\tproperty;\t\t\t\t\t\t\t\t\\\n})\n\n#define X86_PROPERTY_MAX_BASIC_LEAF\t\tKVM_X86_CPU_PROPERTY(0, 0, EAX, 0, 31)\n#define X86_PROPERTY_PMU_VERSION\t\tKVM_X86_CPU_PROPERTY(0xa, 0, EAX, 0, 7)\n#define X86_PROPERTY_PMU_NR_GP_COUNTERS\t\tKVM_X86_CPU_PROPERTY(0xa, 0, EAX, 8, 15)\n#define X86_PROPERTY_PMU_GP_COUNTERS_BIT_WIDTH\tKVM_X86_CPU_PROPERTY(0xa, 0, EAX, 16, 23)\n#define X86_PROPERTY_PMU_EBX_BIT_VECTOR_LENGTH\tKVM_X86_CPU_PROPERTY(0xa, 0, EAX, 24, 31)\n#define X86_PROPERTY_PMU_EVENTS_MASK\t\tKVM_X86_CPU_PROPERTY(0xa, 0, EBX, 0, 7)\n#define X86_PROPERTY_PMU_FIXED_COUNTERS_BITMASK\tKVM_X86_CPU_PROPERTY(0xa, 0, ECX, 0, 31)\n#define X86_PROPERTY_PMU_NR_FIXED_COUNTERS\tKVM_X86_CPU_PROPERTY(0xa, 0, EDX, 0, 4)\n#define X86_PROPERTY_PMU_FIXED_COUNTERS_BIT_WIDTH\tKVM_X86_CPU_PROPERTY(0xa, 0, EDX, 5, 12)\n\n#define X86_PROPERTY_SUPPORTED_XCR0_LO\t\tKVM_X86_CPU_PROPERTY(0xd,  0, EAX,  0, 31)\n#define X86_PROPERTY_XSTATE_MAX_SIZE_XCR0\tKVM_X86_CPU_PROPERTY(0xd,  0, EBX,  0, 31)\n#define X86_PROPERTY_XSTATE_MAX_SIZE\t\tKVM_X86_CPU_PROPERTY(0xd,  0, ECX,  0, 31)\n#define X86_PROPERTY_SUPPORTED_XCR0_HI\t\tKVM_X86_CPU_PROPERTY(0xd,  0, EDX,  0, 31)\n\n#define X86_PROPERTY_XSTATE_TILE_SIZE\t\tKVM_X86_CPU_PROPERTY(0xd, 18, EAX,  0, 31)\n#define X86_PROPERTY_XSTATE_TILE_OFFSET\t\tKVM_X86_CPU_PROPERTY(0xd, 18, EBX,  0, 31)\n#define X86_PROPERTY_AMX_MAX_PALETTE_TABLES\tKVM_X86_CPU_PROPERTY(0x1d, 0, EAX,  0, 31)\n#define X86_PROPERTY_AMX_TOTAL_TILE_BYTES\tKVM_X86_CPU_PROPERTY(0x1d, 1, EAX,  0, 15)\n#define X86_PROPERTY_AMX_BYTES_PER_TILE\t\tKVM_X86_CPU_PROPERTY(0x1d, 1, EAX, 16, 31)\n#define X86_PROPERTY_AMX_BYTES_PER_ROW\t\tKVM_X86_CPU_PROPERTY(0x1d, 1, EBX, 0,  15)\n#define X86_PROPERTY_AMX_NR_TILE_REGS\t\tKVM_X86_CPU_PROPERTY(0x1d, 1, EBX, 16, 31)\n#define X86_PROPERTY_AMX_MAX_ROWS\t\tKVM_X86_CPU_PROPERTY(0x1d, 1, ECX, 0,  15)\n\n#define X86_PROPERTY_MAX_KVM_LEAF\t\tKVM_X86_CPU_PROPERTY(0x40000000, 0, EAX, 0, 31)\n\n#define X86_PROPERTY_MAX_EXT_LEAF\t\tKVM_X86_CPU_PROPERTY(0x80000000, 0, EAX, 0, 31)\n#define X86_PROPERTY_MAX_PHY_ADDR\t\tKVM_X86_CPU_PROPERTY(0x80000008, 0, EAX, 0, 7)\n#define X86_PROPERTY_MAX_VIRT_ADDR\t\tKVM_X86_CPU_PROPERTY(0x80000008, 0, EAX, 8, 15)\n#define X86_PROPERTY_PHYS_ADDR_REDUCTION\tKVM_X86_CPU_PROPERTY(0x8000001F, 0, EBX, 6, 11)\n\n#define X86_PROPERTY_MAX_CENTAUR_LEAF\t\tKVM_X86_CPU_PROPERTY(0xC0000000, 0, EAX, 0, 31)\n\n \nstruct kvm_x86_pmu_feature {\n\tstruct kvm_x86_cpu_feature anti_feature;\n};\n#define\tKVM_X86_PMU_FEATURE(name, __bit)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\\\n\tstruct kvm_x86_pmu_feature feature = {\t\t\t\t\t\\\n\t\t.anti_feature = KVM_X86_CPU_FEATURE(0xa, 0, EBX, __bit),\t\\\n\t};\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tfeature;\t\t\t\t\t\t\t\t\\\n})\n\n#define X86_PMU_FEATURE_BRANCH_INSNS_RETIRED\tKVM_X86_PMU_FEATURE(BRANCH_INSNS_RETIRED, 5)\n\nstatic inline unsigned int x86_family(unsigned int eax)\n{\n\tunsigned int x86;\n\n\tx86 = (eax >> 8) & 0xf;\n\n\tif (x86 == 0xf)\n\t\tx86 += (eax >> 20) & 0xff;\n\n\treturn x86;\n}\n\nstatic inline unsigned int x86_model(unsigned int eax)\n{\n\treturn ((eax >> 12) & 0xf0) | ((eax >> 4) & 0x0f);\n}\n\n \n#define PTE_PRESENT_MASK        BIT_ULL(0)\n#define PTE_WRITABLE_MASK       BIT_ULL(1)\n#define PTE_USER_MASK           BIT_ULL(2)\n#define PTE_ACCESSED_MASK       BIT_ULL(5)\n#define PTE_DIRTY_MASK          BIT_ULL(6)\n#define PTE_LARGE_MASK          BIT_ULL(7)\n#define PTE_GLOBAL_MASK         BIT_ULL(8)\n#define PTE_NX_MASK             BIT_ULL(63)\n\n#define PHYSICAL_PAGE_MASK      GENMASK_ULL(51, 12)\n\n#define PAGE_SHIFT\t\t12\n#define PAGE_SIZE\t\t(1ULL << PAGE_SHIFT)\n#define PAGE_MASK\t\t(~(PAGE_SIZE-1) & PHYSICAL_PAGE_MASK)\n\n#define HUGEPAGE_SHIFT(x)\t(PAGE_SHIFT + (((x) - 1) * 9))\n#define HUGEPAGE_SIZE(x)\t(1UL << HUGEPAGE_SHIFT(x))\n#define HUGEPAGE_MASK(x)\t(~(HUGEPAGE_SIZE(x) - 1) & PHYSICAL_PAGE_MASK)\n\n#define PTE_GET_PA(pte)\t\t((pte) & PHYSICAL_PAGE_MASK)\n#define PTE_GET_PFN(pte)        (PTE_GET_PA(pte) >> PAGE_SHIFT)\n\n \nstruct gpr64_regs {\n\tu64 rax;\n\tu64 rcx;\n\tu64 rdx;\n\tu64 rbx;\n\tu64 rsp;\n\tu64 rbp;\n\tu64 rsi;\n\tu64 rdi;\n\tu64 r8;\n\tu64 r9;\n\tu64 r10;\n\tu64 r11;\n\tu64 r12;\n\tu64 r13;\n\tu64 r14;\n\tu64 r15;\n};\n\nstruct desc64 {\n\tuint16_t limit0;\n\tuint16_t base0;\n\tunsigned base1:8, type:4, s:1, dpl:2, p:1;\n\tunsigned limit1:4, avl:1, l:1, db:1, g:1, base2:8;\n\tuint32_t base3;\n\tuint32_t zero1;\n} __attribute__((packed));\n\nstruct desc_ptr {\n\tuint16_t size;\n\tuint64_t address;\n} __attribute__((packed));\n\nstruct kvm_x86_state {\n\tstruct kvm_xsave *xsave;\n\tstruct kvm_vcpu_events events;\n\tstruct kvm_mp_state mp_state;\n\tstruct kvm_regs regs;\n\tstruct kvm_xcrs xcrs;\n\tstruct kvm_sregs sregs;\n\tstruct kvm_debugregs debugregs;\n\tunion {\n\t\tstruct kvm_nested_state nested;\n\t\tchar nested_[16384];\n\t};\n\tstruct kvm_msrs msrs;\n};\n\nstatic inline uint64_t get_desc64_base(const struct desc64 *desc)\n{\n\treturn ((uint64_t)desc->base3 << 32) |\n\t\t(desc->base0 | ((desc->base1) << 16) | ((desc->base2) << 24));\n}\n\nstatic inline uint64_t rdtsc(void)\n{\n\tuint32_t eax, edx;\n\tuint64_t tsc_val;\n\t \n\t__asm__ __volatile__(\"lfence; rdtsc; lfence\" : \"=a\"(eax), \"=d\"(edx));\n\ttsc_val = ((uint64_t)edx) << 32 | eax;\n\treturn tsc_val;\n}\n\nstatic inline uint64_t rdtscp(uint32_t *aux)\n{\n\tuint32_t eax, edx;\n\n\t__asm__ __volatile__(\"rdtscp\" : \"=a\"(eax), \"=d\"(edx), \"=c\"(*aux));\n\treturn ((uint64_t)edx) << 32 | eax;\n}\n\nstatic inline uint64_t rdmsr(uint32_t msr)\n{\n\tuint32_t a, d;\n\n\t__asm__ __volatile__(\"rdmsr\" : \"=a\"(a), \"=d\"(d) : \"c\"(msr) : \"memory\");\n\n\treturn a | ((uint64_t) d << 32);\n}\n\nstatic inline void wrmsr(uint32_t msr, uint64_t value)\n{\n\tuint32_t a = value;\n\tuint32_t d = value >> 32;\n\n\t__asm__ __volatile__(\"wrmsr\" :: \"a\"(a), \"d\"(d), \"c\"(msr) : \"memory\");\n}\n\n\nstatic inline uint16_t inw(uint16_t port)\n{\n\tuint16_t tmp;\n\n\t__asm__ __volatile__(\"in %%dx, %%ax\"\n\t\t:   \"=a\" (tmp)\n\t\t:   \"d\" (port));\n\n\treturn tmp;\n}\n\nstatic inline uint16_t get_es(void)\n{\n\tuint16_t es;\n\n\t__asm__ __volatile__(\"mov %%es, %[es]\"\n\t\t\t     :   [es]\"=rm\"(es));\n\treturn es;\n}\n\nstatic inline uint16_t get_cs(void)\n{\n\tuint16_t cs;\n\n\t__asm__ __volatile__(\"mov %%cs, %[cs]\"\n\t\t\t     :   [cs]\"=rm\"(cs));\n\treturn cs;\n}\n\nstatic inline uint16_t get_ss(void)\n{\n\tuint16_t ss;\n\n\t__asm__ __volatile__(\"mov %%ss, %[ss]\"\n\t\t\t     :   [ss]\"=rm\"(ss));\n\treturn ss;\n}\n\nstatic inline uint16_t get_ds(void)\n{\n\tuint16_t ds;\n\n\t__asm__ __volatile__(\"mov %%ds, %[ds]\"\n\t\t\t     :   [ds]\"=rm\"(ds));\n\treturn ds;\n}\n\nstatic inline uint16_t get_fs(void)\n{\n\tuint16_t fs;\n\n\t__asm__ __volatile__(\"mov %%fs, %[fs]\"\n\t\t\t     :   [fs]\"=rm\"(fs));\n\treturn fs;\n}\n\nstatic inline uint16_t get_gs(void)\n{\n\tuint16_t gs;\n\n\t__asm__ __volatile__(\"mov %%gs, %[gs]\"\n\t\t\t     :   [gs]\"=rm\"(gs));\n\treturn gs;\n}\n\nstatic inline uint16_t get_tr(void)\n{\n\tuint16_t tr;\n\n\t__asm__ __volatile__(\"str %[tr]\"\n\t\t\t     :   [tr]\"=rm\"(tr));\n\treturn tr;\n}\n\nstatic inline uint64_t get_cr0(void)\n{\n\tuint64_t cr0;\n\n\t__asm__ __volatile__(\"mov %%cr0, %[cr0]\"\n\t\t\t     :   [cr0]\"=r\"(cr0));\n\treturn cr0;\n}\n\nstatic inline uint64_t get_cr3(void)\n{\n\tuint64_t cr3;\n\n\t__asm__ __volatile__(\"mov %%cr3, %[cr3]\"\n\t\t\t     :   [cr3]\"=r\"(cr3));\n\treturn cr3;\n}\n\nstatic inline uint64_t get_cr4(void)\n{\n\tuint64_t cr4;\n\n\t__asm__ __volatile__(\"mov %%cr4, %[cr4]\"\n\t\t\t     :   [cr4]\"=r\"(cr4));\n\treturn cr4;\n}\n\nstatic inline void set_cr4(uint64_t val)\n{\n\t__asm__ __volatile__(\"mov %0, %%cr4\" : : \"r\" (val) : \"memory\");\n}\n\nstatic inline u64 xgetbv(u32 index)\n{\n\tu32 eax, edx;\n\n\t__asm__ __volatile__(\"xgetbv;\"\n\t\t     : \"=a\" (eax), \"=d\" (edx)\n\t\t     : \"c\" (index));\n\treturn eax | ((u64)edx << 32);\n}\n\nstatic inline void xsetbv(u32 index, u64 value)\n{\n\tu32 eax = value;\n\tu32 edx = value >> 32;\n\n\t__asm__ __volatile__(\"xsetbv\" :: \"a\" (eax), \"d\" (edx), \"c\" (index));\n}\n\nstatic inline void wrpkru(u32 pkru)\n{\n\t \n\tasm volatile(\".byte 0x0f,0x01,0xef\\n\\t\"\n\t\t     : : \"a\" (pkru), \"c\"(0), \"d\"(0));\n}\n\nstatic inline struct desc_ptr get_gdt(void)\n{\n\tstruct desc_ptr gdt;\n\t__asm__ __volatile__(\"sgdt %[gdt]\"\n\t\t\t     :   [gdt]\"=m\"(gdt));\n\treturn gdt;\n}\n\nstatic inline struct desc_ptr get_idt(void)\n{\n\tstruct desc_ptr idt;\n\t__asm__ __volatile__(\"sidt %[idt]\"\n\t\t\t     :   [idt]\"=m\"(idt));\n\treturn idt;\n}\n\nstatic inline void outl(uint16_t port, uint32_t value)\n{\n\t__asm__ __volatile__(\"outl %%eax, %%dx\" : : \"d\"(port), \"a\"(value));\n}\n\nstatic inline void __cpuid(uint32_t function, uint32_t index,\n\t\t\t   uint32_t *eax, uint32_t *ebx,\n\t\t\t   uint32_t *ecx, uint32_t *edx)\n{\n\t*eax = function;\n\t*ecx = index;\n\n\tasm volatile(\"cpuid\"\n\t    : \"=a\" (*eax),\n\t      \"=b\" (*ebx),\n\t      \"=c\" (*ecx),\n\t      \"=d\" (*edx)\n\t    : \"0\" (*eax), \"2\" (*ecx)\n\t    : \"memory\");\n}\n\nstatic inline void cpuid(uint32_t function,\n\t\t\t uint32_t *eax, uint32_t *ebx,\n\t\t\t uint32_t *ecx, uint32_t *edx)\n{\n\treturn __cpuid(function, 0, eax, ebx, ecx, edx);\n}\n\nstatic inline uint32_t this_cpu_fms(void)\n{\n\tuint32_t eax, ebx, ecx, edx;\n\n\tcpuid(1, &eax, &ebx, &ecx, &edx);\n\treturn eax;\n}\n\nstatic inline uint32_t this_cpu_family(void)\n{\n\treturn x86_family(this_cpu_fms());\n}\n\nstatic inline uint32_t this_cpu_model(void)\n{\n\treturn x86_model(this_cpu_fms());\n}\n\nstatic inline bool this_cpu_vendor_string_is(const char *vendor)\n{\n\tconst uint32_t *chunk = (const uint32_t *)vendor;\n\tuint32_t eax, ebx, ecx, edx;\n\n\tcpuid(0, &eax, &ebx, &ecx, &edx);\n\treturn (ebx == chunk[0] && edx == chunk[1] && ecx == chunk[2]);\n}\n\nstatic inline bool this_cpu_is_intel(void)\n{\n\treturn this_cpu_vendor_string_is(\"GenuineIntel\");\n}\n\n \nstatic inline bool this_cpu_is_amd(void)\n{\n\treturn this_cpu_vendor_string_is(\"AuthenticAMD\");\n}\n\nstatic inline uint32_t __this_cpu_has(uint32_t function, uint32_t index,\n\t\t\t\t      uint8_t reg, uint8_t lo, uint8_t hi)\n{\n\tuint32_t gprs[4];\n\n\t__cpuid(function, index,\n\t\t&gprs[KVM_CPUID_EAX], &gprs[KVM_CPUID_EBX],\n\t\t&gprs[KVM_CPUID_ECX], &gprs[KVM_CPUID_EDX]);\n\n\treturn (gprs[reg] & GENMASK(hi, lo)) >> lo;\n}\n\nstatic inline bool this_cpu_has(struct kvm_x86_cpu_feature feature)\n{\n\treturn __this_cpu_has(feature.function, feature.index,\n\t\t\t      feature.reg, feature.bit, feature.bit);\n}\n\nstatic inline uint32_t this_cpu_property(struct kvm_x86_cpu_property property)\n{\n\treturn __this_cpu_has(property.function, property.index,\n\t\t\t      property.reg, property.lo_bit, property.hi_bit);\n}\n\nstatic __always_inline bool this_cpu_has_p(struct kvm_x86_cpu_property property)\n{\n\tuint32_t max_leaf;\n\n\tswitch (property.function & 0xc0000000) {\n\tcase 0:\n\t\tmax_leaf = this_cpu_property(X86_PROPERTY_MAX_BASIC_LEAF);\n\t\tbreak;\n\tcase 0x40000000:\n\t\tmax_leaf = this_cpu_property(X86_PROPERTY_MAX_KVM_LEAF);\n\t\tbreak;\n\tcase 0x80000000:\n\t\tmax_leaf = this_cpu_property(X86_PROPERTY_MAX_EXT_LEAF);\n\t\tbreak;\n\tcase 0xc0000000:\n\t\tmax_leaf = this_cpu_property(X86_PROPERTY_MAX_CENTAUR_LEAF);\n\t}\n\treturn max_leaf >= property.function;\n}\n\nstatic inline bool this_pmu_has(struct kvm_x86_pmu_feature feature)\n{\n\tuint32_t nr_bits = this_cpu_property(X86_PROPERTY_PMU_EBX_BIT_VECTOR_LENGTH);\n\n\treturn nr_bits > feature.anti_feature.bit &&\n\t       !this_cpu_has(feature.anti_feature);\n}\n\nstatic __always_inline uint64_t this_cpu_supported_xcr0(void)\n{\n\tif (!this_cpu_has_p(X86_PROPERTY_SUPPORTED_XCR0_LO))\n\t\treturn 0;\n\n\treturn this_cpu_property(X86_PROPERTY_SUPPORTED_XCR0_LO) |\n\t       ((uint64_t)this_cpu_property(X86_PROPERTY_SUPPORTED_XCR0_HI) << 32);\n}\n\ntypedef u32\t\t__attribute__((vector_size(16))) sse128_t;\n#define __sse128_u\tunion { sse128_t vec; u64 as_u64[2]; u32 as_u32[4]; }\n#define sse128_lo(x)\t({ __sse128_u t; t.vec = x; t.as_u64[0]; })\n#define sse128_hi(x)\t({ __sse128_u t; t.vec = x; t.as_u64[1]; })\n\nstatic inline void read_sse_reg(int reg, sse128_t *data)\n{\n\tswitch (reg) {\n\tcase 0:\n\t\tasm(\"movdqa %%xmm0, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 1:\n\t\tasm(\"movdqa %%xmm1, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 2:\n\t\tasm(\"movdqa %%xmm2, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 3:\n\t\tasm(\"movdqa %%xmm3, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 4:\n\t\tasm(\"movdqa %%xmm4, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 5:\n\t\tasm(\"movdqa %%xmm5, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 6:\n\t\tasm(\"movdqa %%xmm6, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tcase 7:\n\t\tasm(\"movdqa %%xmm7, %0\" : \"=m\"(*data));\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic inline void write_sse_reg(int reg, const sse128_t *data)\n{\n\tswitch (reg) {\n\tcase 0:\n\t\tasm(\"movdqa %0, %%xmm0\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 1:\n\t\tasm(\"movdqa %0, %%xmm1\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 2:\n\t\tasm(\"movdqa %0, %%xmm2\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 3:\n\t\tasm(\"movdqa %0, %%xmm3\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 4:\n\t\tasm(\"movdqa %0, %%xmm4\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 5:\n\t\tasm(\"movdqa %0, %%xmm5\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 6:\n\t\tasm(\"movdqa %0, %%xmm6\" : : \"m\"(*data));\n\t\tbreak;\n\tcase 7:\n\t\tasm(\"movdqa %0, %%xmm7\" : : \"m\"(*data));\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic inline void cpu_relax(void)\n{\n\tasm volatile(\"rep; nop\" ::: \"memory\");\n}\n\n#define ud2()\t\t\t\\\n\t__asm__ __volatile__(\t\\\n\t\t\"ud2\\n\"\t\\\n\t\t)\n\n#define hlt()\t\t\t\\\n\t__asm__ __volatile__(\t\\\n\t\t\"hlt\\n\"\t\\\n\t\t)\n\nstruct kvm_x86_state *vcpu_save_state(struct kvm_vcpu *vcpu);\nvoid vcpu_load_state(struct kvm_vcpu *vcpu, struct kvm_x86_state *state);\nvoid kvm_x86_state_cleanup(struct kvm_x86_state *state);\n\nconst struct kvm_msr_list *kvm_get_msr_index_list(void);\nconst struct kvm_msr_list *kvm_get_feature_msr_index_list(void);\nbool kvm_msr_is_in_save_restore_list(uint32_t msr_index);\nuint64_t kvm_get_feature_msr(uint64_t msr_index);\n\nstatic inline void vcpu_msrs_get(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_msrs *msrs)\n{\n\tint r = __vcpu_ioctl(vcpu, KVM_GET_MSRS, msrs);\n\n\tTEST_ASSERT(r == msrs->nmsrs,\n\t\t    \"KVM_GET_MSRS failed, r: %i (failed on MSR %x)\",\n\t\t    r, r < 0 || r >= msrs->nmsrs ? -1 : msrs->entries[r].index);\n}\nstatic inline void vcpu_msrs_set(struct kvm_vcpu *vcpu, struct kvm_msrs *msrs)\n{\n\tint r = __vcpu_ioctl(vcpu, KVM_SET_MSRS, msrs);\n\n\tTEST_ASSERT(r == msrs->nmsrs,\n\t\t    \"KVM_SET_MSRS failed, r: %i (failed on MSR %x)\",\n\t\t    r, r < 0 || r >= msrs->nmsrs ? -1 : msrs->entries[r].index);\n}\nstatic inline void vcpu_debugregs_get(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct kvm_debugregs *debugregs)\n{\n\tvcpu_ioctl(vcpu, KVM_GET_DEBUGREGS, debugregs);\n}\nstatic inline void vcpu_debugregs_set(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct kvm_debugregs *debugregs)\n{\n\tvcpu_ioctl(vcpu, KVM_SET_DEBUGREGS, debugregs);\n}\nstatic inline void vcpu_xsave_get(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_xsave *xsave)\n{\n\tvcpu_ioctl(vcpu, KVM_GET_XSAVE, xsave);\n}\nstatic inline void vcpu_xsave2_get(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct kvm_xsave *xsave)\n{\n\tvcpu_ioctl(vcpu, KVM_GET_XSAVE2, xsave);\n}\nstatic inline void vcpu_xsave_set(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_xsave *xsave)\n{\n\tvcpu_ioctl(vcpu, KVM_SET_XSAVE, xsave);\n}\nstatic inline void vcpu_xcrs_get(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_xcrs *xcrs)\n{\n\tvcpu_ioctl(vcpu, KVM_GET_XCRS, xcrs);\n}\nstatic inline void vcpu_xcrs_set(struct kvm_vcpu *vcpu, struct kvm_xcrs *xcrs)\n{\n\tvcpu_ioctl(vcpu, KVM_SET_XCRS, xcrs);\n}\n\nconst struct kvm_cpuid_entry2 *get_cpuid_entry(const struct kvm_cpuid2 *cpuid,\n\t\t\t\t\t       uint32_t function, uint32_t index);\nconst struct kvm_cpuid2 *kvm_get_supported_cpuid(void);\nconst struct kvm_cpuid2 *kvm_get_supported_hv_cpuid(void);\nconst struct kvm_cpuid2 *vcpu_get_supported_hv_cpuid(struct kvm_vcpu *vcpu);\n\nstatic inline uint32_t kvm_cpu_fms(void)\n{\n\treturn get_cpuid_entry(kvm_get_supported_cpuid(), 0x1, 0)->eax;\n}\n\nstatic inline uint32_t kvm_cpu_family(void)\n{\n\treturn x86_family(kvm_cpu_fms());\n}\n\nstatic inline uint32_t kvm_cpu_model(void)\n{\n\treturn x86_model(kvm_cpu_fms());\n}\n\nbool kvm_cpuid_has(const struct kvm_cpuid2 *cpuid,\n\t\t   struct kvm_x86_cpu_feature feature);\n\nstatic inline bool kvm_cpu_has(struct kvm_x86_cpu_feature feature)\n{\n\treturn kvm_cpuid_has(kvm_get_supported_cpuid(), feature);\n}\n\nuint32_t kvm_cpuid_property(const struct kvm_cpuid2 *cpuid,\n\t\t\t    struct kvm_x86_cpu_property property);\n\nstatic inline uint32_t kvm_cpu_property(struct kvm_x86_cpu_property property)\n{\n\treturn kvm_cpuid_property(kvm_get_supported_cpuid(), property);\n}\n\nstatic __always_inline bool kvm_cpu_has_p(struct kvm_x86_cpu_property property)\n{\n\tuint32_t max_leaf;\n\n\tswitch (property.function & 0xc0000000) {\n\tcase 0:\n\t\tmax_leaf = kvm_cpu_property(X86_PROPERTY_MAX_BASIC_LEAF);\n\t\tbreak;\n\tcase 0x40000000:\n\t\tmax_leaf = kvm_cpu_property(X86_PROPERTY_MAX_KVM_LEAF);\n\t\tbreak;\n\tcase 0x80000000:\n\t\tmax_leaf = kvm_cpu_property(X86_PROPERTY_MAX_EXT_LEAF);\n\t\tbreak;\n\tcase 0xc0000000:\n\t\tmax_leaf = kvm_cpu_property(X86_PROPERTY_MAX_CENTAUR_LEAF);\n\t}\n\treturn max_leaf >= property.function;\n}\n\nstatic inline bool kvm_pmu_has(struct kvm_x86_pmu_feature feature)\n{\n\tuint32_t nr_bits = kvm_cpu_property(X86_PROPERTY_PMU_EBX_BIT_VECTOR_LENGTH);\n\n\treturn nr_bits > feature.anti_feature.bit &&\n\t       !kvm_cpu_has(feature.anti_feature);\n}\n\nstatic __always_inline uint64_t kvm_cpu_supported_xcr0(void)\n{\n\tif (!kvm_cpu_has_p(X86_PROPERTY_SUPPORTED_XCR0_LO))\n\t\treturn 0;\n\n\treturn kvm_cpu_property(X86_PROPERTY_SUPPORTED_XCR0_LO) |\n\t       ((uint64_t)kvm_cpu_property(X86_PROPERTY_SUPPORTED_XCR0_HI) << 32);\n}\n\nstatic inline size_t kvm_cpuid2_size(int nr_entries)\n{\n\treturn sizeof(struct kvm_cpuid2) +\n\t       sizeof(struct kvm_cpuid_entry2) * nr_entries;\n}\n\n \nstatic inline struct kvm_cpuid2 *allocate_kvm_cpuid2(int nr_entries)\n{\n\tstruct kvm_cpuid2 *cpuid;\n\n\tcpuid = malloc(kvm_cpuid2_size(nr_entries));\n\tTEST_ASSERT(cpuid, \"-ENOMEM when allocating kvm_cpuid2\");\n\n\tcpuid->nent = nr_entries;\n\n\treturn cpuid;\n}\n\nvoid vcpu_init_cpuid(struct kvm_vcpu *vcpu, const struct kvm_cpuid2 *cpuid);\nvoid vcpu_set_hv_cpuid(struct kvm_vcpu *vcpu);\n\nstatic inline struct kvm_cpuid_entry2 *__vcpu_get_cpuid_entry(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\t      uint32_t function,\n\t\t\t\t\t\t\t      uint32_t index)\n{\n\treturn (struct kvm_cpuid_entry2 *)get_cpuid_entry(vcpu->cpuid,\n\t\t\t\t\t\t\t  function, index);\n}\n\nstatic inline struct kvm_cpuid_entry2 *vcpu_get_cpuid_entry(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\t    uint32_t function)\n{\n\treturn __vcpu_get_cpuid_entry(vcpu, function, 0);\n}\n\nstatic inline int __vcpu_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tTEST_ASSERT(vcpu->cpuid, \"Must do vcpu_init_cpuid() first\");\n\tr = __vcpu_ioctl(vcpu, KVM_SET_CPUID2, vcpu->cpuid);\n\tif (r)\n\t\treturn r;\n\n\t \n\tvcpu_ioctl(vcpu, KVM_GET_CPUID2, vcpu->cpuid);\n\treturn 0;\n}\n\nstatic inline void vcpu_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\tTEST_ASSERT(vcpu->cpuid, \"Must do vcpu_init_cpuid() first\");\n\tvcpu_ioctl(vcpu, KVM_SET_CPUID2, vcpu->cpuid);\n\n\t \n\tvcpu_ioctl(vcpu, KVM_GET_CPUID2, vcpu->cpuid);\n}\n\nvoid vcpu_set_cpuid_maxphyaddr(struct kvm_vcpu *vcpu, uint8_t maxphyaddr);\n\nvoid vcpu_clear_cpuid_entry(struct kvm_vcpu *vcpu, uint32_t function);\nvoid vcpu_set_or_clear_cpuid_feature(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_x86_cpu_feature feature,\n\t\t\t\t     bool set);\n\nstatic inline void vcpu_set_cpuid_feature(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct kvm_x86_cpu_feature feature)\n{\n\tvcpu_set_or_clear_cpuid_feature(vcpu, feature, true);\n\n}\n\nstatic inline void vcpu_clear_cpuid_feature(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct kvm_x86_cpu_feature feature)\n{\n\tvcpu_set_or_clear_cpuid_feature(vcpu, feature, false);\n}\n\nuint64_t vcpu_get_msr(struct kvm_vcpu *vcpu, uint64_t msr_index);\nint _vcpu_set_msr(struct kvm_vcpu *vcpu, uint64_t msr_index, uint64_t msr_value);\n\n \n#define TEST_ASSERT_MSR(cond, fmt, msr, str, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\\\n\tif (__builtin_constant_p(msr)) {\t\t\t\t\t\\\n\t\tTEST_ASSERT(cond, fmt, str, args);\t\t\t\t\\\n\t} else if (!(cond)) {\t\t\t\t\t\t\t\\\n\t\tchar buf[16];\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t\tsnprintf(buf, sizeof(buf), \"MSR 0x%x\", msr);\t\t\t\\\n\t\tTEST_ASSERT(cond, fmt, buf, args);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n \nstatic inline bool is_durable_msr(uint32_t msr)\n{\n\treturn msr != MSR_IA32_TSC;\n}\n\n#define vcpu_set_msr(vcpu, msr, val)\t\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\t\t\\\n\tuint64_t r, v = val;\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\t\\\n\tTEST_ASSERT_MSR(_vcpu_set_msr(vcpu, msr, v) == 1,\t\t\t\t\\\n\t\t\t\"KVM_SET_MSRS failed on %s, value = 0x%lx\", msr, #msr, v);\t\\\n\tif (!is_durable_msr(msr))\t\t\t\t\t\t\t\\\n\t\tbreak;\t\t\t\t\t\t\t\t\t\\\n\tr = vcpu_get_msr(vcpu, msr);\t\t\t\t\t\t\t\\\n\tTEST_ASSERT_MSR(r == v, \"Set %s to '0x%lx', got back '0x%lx'\", msr, #msr, v, r);\\\n} while (0)\n\nvoid kvm_get_cpu_address_width(unsigned int *pa_bits, unsigned int *va_bits);\nbool vm_is_unrestricted_guest(struct kvm_vm *vm);\n\nstruct ex_regs {\n\tuint64_t rax, rcx, rdx, rbx;\n\tuint64_t rbp, rsi, rdi;\n\tuint64_t r8, r9, r10, r11;\n\tuint64_t r12, r13, r14, r15;\n\tuint64_t vector;\n\tuint64_t error_code;\n\tuint64_t rip;\n\tuint64_t cs;\n\tuint64_t rflags;\n};\n\nstruct idt_entry {\n\tuint16_t offset0;\n\tuint16_t selector;\n\tuint16_t ist : 3;\n\tuint16_t : 5;\n\tuint16_t type : 4;\n\tuint16_t : 1;\n\tuint16_t dpl : 2;\n\tuint16_t p : 1;\n\tuint16_t offset1;\n\tuint32_t offset2; uint32_t reserved;\n};\n\nvoid vm_init_descriptor_tables(struct kvm_vm *vm);\nvoid vcpu_init_descriptor_tables(struct kvm_vcpu *vcpu);\nvoid vm_install_exception_handler(struct kvm_vm *vm, int vector,\n\t\t\tvoid (*handler)(struct ex_regs *));\n\n \n#define KVM_EXCEPTION_MAGIC 0xabacadabaULL\n\n \n#define KVM_ASM_SAFE(insn)\t\t\t\t\t\\\n\t\"mov $\" __stringify(KVM_EXCEPTION_MAGIC) \", %%r9\\n\\t\"\t\\\n\t\"lea 1f(%%rip), %%r10\\n\\t\"\t\t\t\t\\\n\t\"lea 2f(%%rip), %%r11\\n\\t\"\t\t\t\t\\\n\t\"1: \" insn \"\\n\\t\"\t\t\t\t\t\\\n\t\"xor %%r9, %%r9\\n\\t\"\t\t\t\t\t\\\n\t\"2:\\n\\t\"\t\t\t\t\t\t\\\n\t\"mov  %%r9b, %[vector]\\n\\t\"\t\t\t\t\\\n\t\"mov  %%r10, %[error_code]\\n\\t\"\n\n#define KVM_ASM_SAFE_OUTPUTS(v, ec)\t[vector] \"=qm\"(v), [error_code] \"=rm\"(ec)\n#define KVM_ASM_SAFE_CLOBBERS\t\"r9\", \"r10\", \"r11\"\n\n#define kvm_asm_safe(insn, inputs...)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tuint64_t ign_error_code;\t\t\t\t\t\\\n\tuint8_t vector;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm volatile(KVM_ASM_SAFE(insn)\t\t\t\t\t\\\n\t\t     : KVM_ASM_SAFE_OUTPUTS(vector, ign_error_code)\t\\\n\t\t     : inputs\t\t\t\t\t\t\\\n\t\t     : KVM_ASM_SAFE_CLOBBERS);\t\t\t\t\\\n\tvector;\t\t\t\t\t\t\t\t\\\n})\n\n#define kvm_asm_safe_ec(insn, error_code, inputs...)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tuint8_t vector;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm volatile(KVM_ASM_SAFE(insn)\t\t\t\t\t\\\n\t\t     : KVM_ASM_SAFE_OUTPUTS(vector, error_code)\t\t\\\n\t\t     : inputs\t\t\t\t\t\t\\\n\t\t     : KVM_ASM_SAFE_CLOBBERS);\t\t\t\t\\\n\tvector;\t\t\t\t\t\t\t\t\\\n})\n\nstatic inline uint8_t rdmsr_safe(uint32_t msr, uint64_t *val)\n{\n\tuint64_t error_code;\n\tuint8_t vector;\n\tuint32_t a, d;\n\n\tasm volatile(KVM_ASM_SAFE(\"rdmsr\")\n\t\t     : \"=a\"(a), \"=d\"(d), KVM_ASM_SAFE_OUTPUTS(vector, error_code)\n\t\t     : \"c\"(msr)\n\t\t     : KVM_ASM_SAFE_CLOBBERS);\n\n\t*val = (uint64_t)a | ((uint64_t)d << 32);\n\treturn vector;\n}\n\nstatic inline uint8_t wrmsr_safe(uint32_t msr, uint64_t val)\n{\n\treturn kvm_asm_safe(\"wrmsr\", \"a\"(val & -1u), \"d\"(val >> 32), \"c\"(msr));\n}\n\nstatic inline uint8_t xsetbv_safe(uint32_t index, uint64_t value)\n{\n\tu32 eax = value;\n\tu32 edx = value >> 32;\n\n\treturn kvm_asm_safe(\"xsetbv\", \"a\" (eax), \"d\" (edx), \"c\" (index));\n}\n\nbool kvm_is_tdp_enabled(void);\n\nuint64_t *__vm_get_page_table_entry(struct kvm_vm *vm, uint64_t vaddr,\n\t\t\t\t    int *level);\nuint64_t *vm_get_page_table_entry(struct kvm_vm *vm, uint64_t vaddr);\n\nuint64_t kvm_hypercall(uint64_t nr, uint64_t a0, uint64_t a1, uint64_t a2,\n\t\t       uint64_t a3);\nuint64_t __xen_hypercall(uint64_t nr, uint64_t a0, void *a1);\nvoid xen_hypercall(uint64_t nr, uint64_t a0, void *a1);\n\nvoid __vm_xsave_require_permission(uint64_t xfeature, const char *name);\n\n#define vm_xsave_require_permission(xfeature)\t\\\n\t__vm_xsave_require_permission(xfeature, #xfeature)\n\nenum pg_level {\n\tPG_LEVEL_NONE,\n\tPG_LEVEL_4K,\n\tPG_LEVEL_2M,\n\tPG_LEVEL_1G,\n\tPG_LEVEL_512G,\n\tPG_LEVEL_NUM\n};\n\n#define PG_LEVEL_SHIFT(_level) ((_level - 1) * 9 + 12)\n#define PG_LEVEL_SIZE(_level) (1ull << PG_LEVEL_SHIFT(_level))\n\n#define PG_SIZE_4K PG_LEVEL_SIZE(PG_LEVEL_4K)\n#define PG_SIZE_2M PG_LEVEL_SIZE(PG_LEVEL_2M)\n#define PG_SIZE_1G PG_LEVEL_SIZE(PG_LEVEL_1G)\n\nvoid __virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr, int level);\nvoid virt_map_level(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,\n\t\t    uint64_t nr_bytes, int level);\n\n \n#define X86_CR0_PE          (1UL<<0)  \n#define X86_CR0_MP          (1UL<<1)  \n#define X86_CR0_EM          (1UL<<2)  \n#define X86_CR0_TS          (1UL<<3)  \n#define X86_CR0_ET          (1UL<<4)  \n#define X86_CR0_NE          (1UL<<5)  \n#define X86_CR0_WP          (1UL<<16)  \n#define X86_CR0_AM          (1UL<<18)  \n#define X86_CR0_NW          (1UL<<29)  \n#define X86_CR0_CD          (1UL<<30)  \n#define X86_CR0_PG          (1UL<<31)  \n\n#define PFERR_PRESENT_BIT 0\n#define PFERR_WRITE_BIT 1\n#define PFERR_USER_BIT 2\n#define PFERR_RSVD_BIT 3\n#define PFERR_FETCH_BIT 4\n#define PFERR_PK_BIT 5\n#define PFERR_SGX_BIT 15\n#define PFERR_GUEST_FINAL_BIT 32\n#define PFERR_GUEST_PAGE_BIT 33\n#define PFERR_IMPLICIT_ACCESS_BIT 48\n\n#define PFERR_PRESENT_MASK\tBIT(PFERR_PRESENT_BIT)\n#define PFERR_WRITE_MASK\tBIT(PFERR_WRITE_BIT)\n#define PFERR_USER_MASK\t\tBIT(PFERR_USER_BIT)\n#define PFERR_RSVD_MASK\t\tBIT(PFERR_RSVD_BIT)\n#define PFERR_FETCH_MASK\tBIT(PFERR_FETCH_BIT)\n#define PFERR_PK_MASK\t\tBIT(PFERR_PK_BIT)\n#define PFERR_SGX_MASK\t\tBIT(PFERR_SGX_BIT)\n#define PFERR_GUEST_FINAL_MASK\tBIT_ULL(PFERR_GUEST_FINAL_BIT)\n#define PFERR_GUEST_PAGE_MASK\tBIT_ULL(PFERR_GUEST_PAGE_BIT)\n#define PFERR_IMPLICIT_ACCESS\tBIT_ULL(PFERR_IMPLICIT_ACCESS_BIT)\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}