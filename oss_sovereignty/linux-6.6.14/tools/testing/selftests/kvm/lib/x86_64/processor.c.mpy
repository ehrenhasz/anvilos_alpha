{
  "module_name": "processor.c",
  "hash_id": "d5b8ee60328a63ae7f663cbbc4375263d17942ed5a07e2c405f020b40904aa9d",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/x86_64/processor.c",
  "human_readable_source": "\n \n\n#include \"linux/bitmap.h\"\n#include \"test_util.h\"\n#include \"kvm_util.h\"\n#include \"processor.h\"\n\n#ifndef NUM_INTERRUPTS\n#define NUM_INTERRUPTS 256\n#endif\n\n#define DEFAULT_CODE_SELECTOR 0x8\n#define DEFAULT_DATA_SELECTOR 0x10\n\n#define MAX_NR_CPUID_ENTRIES 100\n\nvm_vaddr_t exception_handlers;\nbool host_cpu_is_amd;\nbool host_cpu_is_intel;\n\nstatic void regs_dump(FILE *stream, struct kvm_regs *regs, uint8_t indent)\n{\n\tfprintf(stream, \"%*srax: 0x%.16llx rbx: 0x%.16llx \"\n\t\t\"rcx: 0x%.16llx rdx: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tregs->rax, regs->rbx, regs->rcx, regs->rdx);\n\tfprintf(stream, \"%*srsi: 0x%.16llx rdi: 0x%.16llx \"\n\t\t\"rsp: 0x%.16llx rbp: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tregs->rsi, regs->rdi, regs->rsp, regs->rbp);\n\tfprintf(stream, \"%*sr8:  0x%.16llx r9:  0x%.16llx \"\n\t\t\"r10: 0x%.16llx r11: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tregs->r8, regs->r9, regs->r10, regs->r11);\n\tfprintf(stream, \"%*sr12: 0x%.16llx r13: 0x%.16llx \"\n\t\t\"r14: 0x%.16llx r15: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tregs->r12, regs->r13, regs->r14, regs->r15);\n\tfprintf(stream, \"%*srip: 0x%.16llx rfl: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tregs->rip, regs->rflags);\n}\n\nstatic void segment_dump(FILE *stream, struct kvm_segment *segment,\n\t\t\t uint8_t indent)\n{\n\tfprintf(stream, \"%*sbase: 0x%.16llx limit: 0x%.8x \"\n\t\t\"selector: 0x%.4x type: 0x%.2x\\n\",\n\t\tindent, \"\", segment->base, segment->limit,\n\t\tsegment->selector, segment->type);\n\tfprintf(stream, \"%*spresent: 0x%.2x dpl: 0x%.2x \"\n\t\t\"db: 0x%.2x s: 0x%.2x l: 0x%.2x\\n\",\n\t\tindent, \"\", segment->present, segment->dpl,\n\t\tsegment->db, segment->s, segment->l);\n\tfprintf(stream, \"%*sg: 0x%.2x avl: 0x%.2x \"\n\t\t\"unusable: 0x%.2x padding: 0x%.2x\\n\",\n\t\tindent, \"\", segment->g, segment->avl,\n\t\tsegment->unusable, segment->padding);\n}\n\nstatic void dtable_dump(FILE *stream, struct kvm_dtable *dtable,\n\t\t\tuint8_t indent)\n{\n\tfprintf(stream, \"%*sbase: 0x%.16llx limit: 0x%.4x \"\n\t\t\"padding: 0x%.4x 0x%.4x 0x%.4x\\n\",\n\t\tindent, \"\", dtable->base, dtable->limit,\n\t\tdtable->padding[0], dtable->padding[1], dtable->padding[2]);\n}\n\nstatic void sregs_dump(FILE *stream, struct kvm_sregs *sregs, uint8_t indent)\n{\n\tunsigned int i;\n\n\tfprintf(stream, \"%*scs:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->cs, indent + 2);\n\tfprintf(stream, \"%*sds:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->ds, indent + 2);\n\tfprintf(stream, \"%*ses:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->es, indent + 2);\n\tfprintf(stream, \"%*sfs:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->fs, indent + 2);\n\tfprintf(stream, \"%*sgs:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->gs, indent + 2);\n\tfprintf(stream, \"%*sss:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->ss, indent + 2);\n\tfprintf(stream, \"%*str:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->tr, indent + 2);\n\tfprintf(stream, \"%*sldt:\\n\", indent, \"\");\n\tsegment_dump(stream, &sregs->ldt, indent + 2);\n\n\tfprintf(stream, \"%*sgdt:\\n\", indent, \"\");\n\tdtable_dump(stream, &sregs->gdt, indent + 2);\n\tfprintf(stream, \"%*sidt:\\n\", indent, \"\");\n\tdtable_dump(stream, &sregs->idt, indent + 2);\n\n\tfprintf(stream, \"%*scr0: 0x%.16llx cr2: 0x%.16llx \"\n\t\t\"cr3: 0x%.16llx cr4: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tsregs->cr0, sregs->cr2, sregs->cr3, sregs->cr4);\n\tfprintf(stream, \"%*scr8: 0x%.16llx efer: 0x%.16llx \"\n\t\t\"apic_base: 0x%.16llx\\n\",\n\t\tindent, \"\",\n\t\tsregs->cr8, sregs->efer, sregs->apic_base);\n\n\tfprintf(stream, \"%*sinterrupt_bitmap:\\n\", indent, \"\");\n\tfor (i = 0; i < (KVM_NR_INTERRUPTS + 63) / 64; i++) {\n\t\tfprintf(stream, \"%*s%.16llx\\n\", indent + 2, \"\",\n\t\t\tsregs->interrupt_bitmap[i]);\n\t}\n}\n\nbool kvm_is_tdp_enabled(void)\n{\n\tif (host_cpu_is_intel)\n\t\treturn get_kvm_intel_param_bool(\"ept\");\n\telse\n\t\treturn get_kvm_amd_param_bool(\"npt\");\n}\n\nvoid virt_arch_pgd_alloc(struct kvm_vm *vm)\n{\n\tTEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, \"Attempt to use \"\n\t\t\"unknown or unsupported guest mode, mode: 0x%x\", vm->mode);\n\n\t \n\tif (!vm->pgd_created) {\n\t\tvm->pgd = vm_alloc_page_table(vm);\n\t\tvm->pgd_created = true;\n\t}\n}\n\nstatic void *virt_get_pte(struct kvm_vm *vm, uint64_t *parent_pte,\n\t\t\t  uint64_t vaddr, int level)\n{\n\tuint64_t pt_gpa = PTE_GET_PA(*parent_pte);\n\tuint64_t *page_table = addr_gpa2hva(vm, pt_gpa);\n\tint index = (vaddr >> PG_LEVEL_SHIFT(level)) & 0x1ffu;\n\n\tTEST_ASSERT((*parent_pte & PTE_PRESENT_MASK) || parent_pte == &vm->pgd,\n\t\t    \"Parent PTE (level %d) not PRESENT for gva: 0x%08lx\",\n\t\t    level + 1, vaddr);\n\n\treturn &page_table[index];\n}\n\nstatic uint64_t *virt_create_upper_pte(struct kvm_vm *vm,\n\t\t\t\t       uint64_t *parent_pte,\n\t\t\t\t       uint64_t vaddr,\n\t\t\t\t       uint64_t paddr,\n\t\t\t\t       int current_level,\n\t\t\t\t       int target_level)\n{\n\tuint64_t *pte = virt_get_pte(vm, parent_pte, vaddr, current_level);\n\n\tif (!(*pte & PTE_PRESENT_MASK)) {\n\t\t*pte = PTE_PRESENT_MASK | PTE_WRITABLE_MASK;\n\t\tif (current_level == target_level)\n\t\t\t*pte |= PTE_LARGE_MASK | (paddr & PHYSICAL_PAGE_MASK);\n\t\telse\n\t\t\t*pte |= vm_alloc_page_table(vm) & PHYSICAL_PAGE_MASK;\n\t} else {\n\t\t \n\t\tTEST_ASSERT(current_level != target_level,\n\t\t\t    \"Cannot create hugepage at level: %u, vaddr: 0x%lx\\n\",\n\t\t\t    current_level, vaddr);\n\t\tTEST_ASSERT(!(*pte & PTE_LARGE_MASK),\n\t\t\t    \"Cannot create page table at level: %u, vaddr: 0x%lx\\n\",\n\t\t\t    current_level, vaddr);\n\t}\n\treturn pte;\n}\n\nvoid __virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr, int level)\n{\n\tconst uint64_t pg_size = PG_LEVEL_SIZE(level);\n\tuint64_t *pml4e, *pdpe, *pde;\n\tuint64_t *pte;\n\n\tTEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K,\n\t\t    \"Unknown or unsupported guest mode, mode: 0x%x\", vm->mode);\n\n\tTEST_ASSERT((vaddr % pg_size) == 0,\n\t\t    \"Virtual address not aligned,\\n\"\n\t\t    \"vaddr: 0x%lx page size: 0x%lx\", vaddr, pg_size);\n\tTEST_ASSERT(sparsebit_is_set(vm->vpages_valid, (vaddr >> vm->page_shift)),\n\t\t    \"Invalid virtual address, vaddr: 0x%lx\", vaddr);\n\tTEST_ASSERT((paddr % pg_size) == 0,\n\t\t    \"Physical address not aligned,\\n\"\n\t\t    \"  paddr: 0x%lx page size: 0x%lx\", paddr, pg_size);\n\tTEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,\n\t\t    \"Physical address beyond maximum supported,\\n\"\n\t\t    \"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\t    paddr, vm->max_gfn, vm->page_size);\n\n\t \n\tpml4e = virt_create_upper_pte(vm, &vm->pgd, vaddr, paddr, PG_LEVEL_512G, level);\n\tif (*pml4e & PTE_LARGE_MASK)\n\t\treturn;\n\n\tpdpe = virt_create_upper_pte(vm, pml4e, vaddr, paddr, PG_LEVEL_1G, level);\n\tif (*pdpe & PTE_LARGE_MASK)\n\t\treturn;\n\n\tpde = virt_create_upper_pte(vm, pdpe, vaddr, paddr, PG_LEVEL_2M, level);\n\tif (*pde & PTE_LARGE_MASK)\n\t\treturn;\n\n\t \n\tpte = virt_get_pte(vm, pde, vaddr, PG_LEVEL_4K);\n\tTEST_ASSERT(!(*pte & PTE_PRESENT_MASK),\n\t\t    \"PTE already present for 4k page at vaddr: 0x%lx\\n\", vaddr);\n\t*pte = PTE_PRESENT_MASK | PTE_WRITABLE_MASK | (paddr & PHYSICAL_PAGE_MASK);\n}\n\nvoid virt_arch_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)\n{\n\t__virt_pg_map(vm, vaddr, paddr, PG_LEVEL_4K);\n}\n\nvoid virt_map_level(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,\n\t\t    uint64_t nr_bytes, int level)\n{\n\tuint64_t pg_size = PG_LEVEL_SIZE(level);\n\tuint64_t nr_pages = nr_bytes / pg_size;\n\tint i;\n\n\tTEST_ASSERT(nr_bytes % pg_size == 0,\n\t\t    \"Region size not aligned: nr_bytes: 0x%lx, page size: 0x%lx\",\n\t\t    nr_bytes, pg_size);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\t__virt_pg_map(vm, vaddr, paddr, level);\n\n\t\tvaddr += pg_size;\n\t\tpaddr += pg_size;\n\t}\n}\n\nstatic bool vm_is_target_pte(uint64_t *pte, int *level, int current_level)\n{\n\tif (*pte & PTE_LARGE_MASK) {\n\t\tTEST_ASSERT(*level == PG_LEVEL_NONE ||\n\t\t\t    *level == current_level,\n\t\t\t    \"Unexpected hugepage at level %d\\n\", current_level);\n\t\t*level = current_level;\n\t}\n\n\treturn *level == current_level;\n}\n\nuint64_t *__vm_get_page_table_entry(struct kvm_vm *vm, uint64_t vaddr,\n\t\t\t\t    int *level)\n{\n\tuint64_t *pml4e, *pdpe, *pde;\n\n\tTEST_ASSERT(*level >= PG_LEVEL_NONE && *level < PG_LEVEL_NUM,\n\t\t    \"Invalid PG_LEVEL_* '%d'\", *level);\n\n\tTEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, \"Attempt to use \"\n\t\t\"unknown or unsupported guest mode, mode: 0x%x\", vm->mode);\n\tTEST_ASSERT(sparsebit_is_set(vm->vpages_valid,\n\t\t(vaddr >> vm->page_shift)),\n\t\t\"Invalid virtual address, vaddr: 0x%lx\",\n\t\tvaddr);\n\t \n\tTEST_ASSERT(vaddr == (((int64_t)vaddr << 16) >> 16),\n\t\t\"Canonical check failed.  The virtual address is invalid.\");\n\n\tpml4e = virt_get_pte(vm, &vm->pgd, vaddr, PG_LEVEL_512G);\n\tif (vm_is_target_pte(pml4e, level, PG_LEVEL_512G))\n\t\treturn pml4e;\n\n\tpdpe = virt_get_pte(vm, pml4e, vaddr, PG_LEVEL_1G);\n\tif (vm_is_target_pte(pdpe, level, PG_LEVEL_1G))\n\t\treturn pdpe;\n\n\tpde = virt_get_pte(vm, pdpe, vaddr, PG_LEVEL_2M);\n\tif (vm_is_target_pte(pde, level, PG_LEVEL_2M))\n\t\treturn pde;\n\n\treturn virt_get_pte(vm, pde, vaddr, PG_LEVEL_4K);\n}\n\nuint64_t *vm_get_page_table_entry(struct kvm_vm *vm, uint64_t vaddr)\n{\n\tint level = PG_LEVEL_4K;\n\n\treturn __vm_get_page_table_entry(vm, vaddr, &level);\n}\n\nvoid virt_arch_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)\n{\n\tuint64_t *pml4e, *pml4e_start;\n\tuint64_t *pdpe, *pdpe_start;\n\tuint64_t *pde, *pde_start;\n\tuint64_t *pte, *pte_start;\n\n\tif (!vm->pgd_created)\n\t\treturn;\n\n\tfprintf(stream, \"%*s                                          \"\n\t\t\"                no\\n\", indent, \"\");\n\tfprintf(stream, \"%*s      index hvaddr         gpaddr         \"\n\t\t\"addr         w exec dirty\\n\",\n\t\tindent, \"\");\n\tpml4e_start = (uint64_t *) addr_gpa2hva(vm, vm->pgd);\n\tfor (uint16_t n1 = 0; n1 <= 0x1ffu; n1++) {\n\t\tpml4e = &pml4e_start[n1];\n\t\tif (!(*pml4e & PTE_PRESENT_MASK))\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*spml4e 0x%-3zx %p 0x%-12lx 0x%-10llx %u \"\n\t\t\t\" %u\\n\",\n\t\t\tindent, \"\",\n\t\t\tpml4e - pml4e_start, pml4e,\n\t\t\taddr_hva2gpa(vm, pml4e), PTE_GET_PFN(*pml4e),\n\t\t\t!!(*pml4e & PTE_WRITABLE_MASK), !!(*pml4e & PTE_NX_MASK));\n\n\t\tpdpe_start = addr_gpa2hva(vm, *pml4e & PHYSICAL_PAGE_MASK);\n\t\tfor (uint16_t n2 = 0; n2 <= 0x1ffu; n2++) {\n\t\t\tpdpe = &pdpe_start[n2];\n\t\t\tif (!(*pdpe & PTE_PRESENT_MASK))\n\t\t\t\tcontinue;\n\t\t\tfprintf(stream, \"%*spdpe  0x%-3zx %p 0x%-12lx 0x%-10llx \"\n\t\t\t\t\"%u  %u\\n\",\n\t\t\t\tindent, \"\",\n\t\t\t\tpdpe - pdpe_start, pdpe,\n\t\t\t\taddr_hva2gpa(vm, pdpe),\n\t\t\t\tPTE_GET_PFN(*pdpe), !!(*pdpe & PTE_WRITABLE_MASK),\n\t\t\t\t!!(*pdpe & PTE_NX_MASK));\n\n\t\t\tpde_start = addr_gpa2hva(vm, *pdpe & PHYSICAL_PAGE_MASK);\n\t\t\tfor (uint16_t n3 = 0; n3 <= 0x1ffu; n3++) {\n\t\t\t\tpde = &pde_start[n3];\n\t\t\t\tif (!(*pde & PTE_PRESENT_MASK))\n\t\t\t\t\tcontinue;\n\t\t\t\tfprintf(stream, \"%*spde   0x%-3zx %p \"\n\t\t\t\t\t\"0x%-12lx 0x%-10llx %u  %u\\n\",\n\t\t\t\t\tindent, \"\", pde - pde_start, pde,\n\t\t\t\t\taddr_hva2gpa(vm, pde),\n\t\t\t\t\tPTE_GET_PFN(*pde), !!(*pde & PTE_WRITABLE_MASK),\n\t\t\t\t\t!!(*pde & PTE_NX_MASK));\n\n\t\t\t\tpte_start = addr_gpa2hva(vm, *pde & PHYSICAL_PAGE_MASK);\n\t\t\t\tfor (uint16_t n4 = 0; n4 <= 0x1ffu; n4++) {\n\t\t\t\t\tpte = &pte_start[n4];\n\t\t\t\t\tif (!(*pte & PTE_PRESENT_MASK))\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tfprintf(stream, \"%*spte   0x%-3zx %p \"\n\t\t\t\t\t\t\"0x%-12lx 0x%-10llx %u  %u \"\n\t\t\t\t\t\t\"    %u    0x%-10lx\\n\",\n\t\t\t\t\t\tindent, \"\",\n\t\t\t\t\t\tpte - pte_start, pte,\n\t\t\t\t\t\taddr_hva2gpa(vm, pte),\n\t\t\t\t\t\tPTE_GET_PFN(*pte),\n\t\t\t\t\t\t!!(*pte & PTE_WRITABLE_MASK),\n\t\t\t\t\t\t!!(*pte & PTE_NX_MASK),\n\t\t\t\t\t\t!!(*pte & PTE_DIRTY_MASK),\n\t\t\t\t\t\t((uint64_t) n1 << 27)\n\t\t\t\t\t\t\t| ((uint64_t) n2 << 18)\n\t\t\t\t\t\t\t| ((uint64_t) n3 << 9)\n\t\t\t\t\t\t\t| ((uint64_t) n4));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic void kvm_seg_set_unusable(struct kvm_segment *segp)\n{\n\tmemset(segp, 0, sizeof(*segp));\n\tsegp->unusable = true;\n}\n\nstatic void kvm_seg_fill_gdt_64bit(struct kvm_vm *vm, struct kvm_segment *segp)\n{\n\tvoid *gdt = addr_gva2hva(vm, vm->gdt);\n\tstruct desc64 *desc = gdt + (segp->selector >> 3) * 8;\n\n\tdesc->limit0 = segp->limit & 0xFFFF;\n\tdesc->base0 = segp->base & 0xFFFF;\n\tdesc->base1 = segp->base >> 16;\n\tdesc->type = segp->type;\n\tdesc->s = segp->s;\n\tdesc->dpl = segp->dpl;\n\tdesc->p = segp->present;\n\tdesc->limit1 = segp->limit >> 16;\n\tdesc->avl = segp->avl;\n\tdesc->l = segp->l;\n\tdesc->db = segp->db;\n\tdesc->g = segp->g;\n\tdesc->base2 = segp->base >> 24;\n\tif (!segp->s)\n\t\tdesc->base3 = segp->base >> 32;\n}\n\n\n \nstatic void kvm_seg_set_kernel_code_64bit(struct kvm_vm *vm, uint16_t selector,\n\tstruct kvm_segment *segp)\n{\n\tmemset(segp, 0, sizeof(*segp));\n\tsegp->selector = selector;\n\tsegp->limit = 0xFFFFFFFFu;\n\tsegp->s = 0x1;  \n\tsegp->type = 0x08 | 0x01 | 0x02;  \n\tsegp->g = true;\n\tsegp->l = true;\n\tsegp->present = 1;\n\tif (vm)\n\t\tkvm_seg_fill_gdt_64bit(vm, segp);\n}\n\n \nstatic void kvm_seg_set_kernel_data_64bit(struct kvm_vm *vm, uint16_t selector,\n\tstruct kvm_segment *segp)\n{\n\tmemset(segp, 0, sizeof(*segp));\n\tsegp->selector = selector;\n\tsegp->limit = 0xFFFFFFFFu;\n\tsegp->s = 0x1;  \n\tsegp->type = 0x00 | 0x01 | 0x02;  \n\tsegp->g = true;\n\tsegp->present = true;\n\tif (vm)\n\t\tkvm_seg_fill_gdt_64bit(vm, segp);\n}\n\nvm_paddr_t addr_arch_gva2gpa(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tint level = PG_LEVEL_NONE;\n\tuint64_t *pte = __vm_get_page_table_entry(vm, gva, &level);\n\n\tTEST_ASSERT(*pte & PTE_PRESENT_MASK,\n\t\t    \"Leaf PTE not PRESENT for gva: 0x%08lx\", gva);\n\n\t \n\treturn PTE_GET_PA(*pte) | (gva & ~HUGEPAGE_MASK(level));\n}\n\nstatic void kvm_setup_gdt(struct kvm_vm *vm, struct kvm_dtable *dt)\n{\n\tif (!vm->gdt)\n\t\tvm->gdt = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);\n\n\tdt->base = vm->gdt;\n\tdt->limit = getpagesize();\n}\n\nstatic void kvm_setup_tss_64bit(struct kvm_vm *vm, struct kvm_segment *segp,\n\t\t\t\tint selector)\n{\n\tif (!vm->tss)\n\t\tvm->tss = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);\n\n\tmemset(segp, 0, sizeof(*segp));\n\tsegp->base = vm->tss;\n\tsegp->limit = 0x67;\n\tsegp->selector = selector;\n\tsegp->type = 0xb;\n\tsegp->present = 1;\n\tkvm_seg_fill_gdt_64bit(vm, segp);\n}\n\nstatic void vcpu_setup(struct kvm_vm *vm, struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_sregs sregs;\n\n\t \n\tvcpu_sregs_get(vcpu, &sregs);\n\n\tsregs.idt.limit = 0;\n\n\tkvm_setup_gdt(vm, &sregs.gdt);\n\n\tswitch (vm->mode) {\n\tcase VM_MODE_PXXV48_4K:\n\t\tsregs.cr0 = X86_CR0_PE | X86_CR0_NE | X86_CR0_PG;\n\t\tsregs.cr4 |= X86_CR4_PAE | X86_CR4_OSFXSR;\n\t\tsregs.efer |= (EFER_LME | EFER_LMA | EFER_NX);\n\n\t\tkvm_seg_set_unusable(&sregs.ldt);\n\t\tkvm_seg_set_kernel_code_64bit(vm, DEFAULT_CODE_SELECTOR, &sregs.cs);\n\t\tkvm_seg_set_kernel_data_64bit(vm, DEFAULT_DATA_SELECTOR, &sregs.ds);\n\t\tkvm_seg_set_kernel_data_64bit(vm, DEFAULT_DATA_SELECTOR, &sregs.es);\n\t\tkvm_setup_tss_64bit(vm, &sregs.tr, 0x18);\n\t\tbreak;\n\n\tdefault:\n\t\tTEST_FAIL(\"Unknown guest mode, mode: 0x%x\", vm->mode);\n\t}\n\n\tsregs.cr3 = vm->pgd;\n\tvcpu_sregs_set(vcpu, &sregs);\n}\n\nvoid kvm_arch_vm_post_create(struct kvm_vm *vm)\n{\n\tvm_create_irqchip(vm);\n\tsync_global_to_guest(vm, host_cpu_is_intel);\n\tsync_global_to_guest(vm, host_cpu_is_amd);\n}\n\nstruct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,\n\t\t\t\t  void *guest_code)\n{\n\tstruct kvm_mp_state mp_state;\n\tstruct kvm_regs regs;\n\tvm_vaddr_t stack_vaddr;\n\tstruct kvm_vcpu *vcpu;\n\n\tstack_vaddr = __vm_vaddr_alloc(vm, DEFAULT_STACK_PGS * getpagesize(),\n\t\t\t\t       DEFAULT_GUEST_STACK_VADDR_MIN,\n\t\t\t\t       MEM_REGION_DATA);\n\n\tstack_vaddr += DEFAULT_STACK_PGS * getpagesize();\n\n\t \n\tTEST_ASSERT(IS_ALIGNED(stack_vaddr, PAGE_SIZE),\n\t\t    \"__vm_vaddr_alloc() did not provide a page-aligned address\");\n\tstack_vaddr -= 8;\n\n\tvcpu = __vm_vcpu_add(vm, vcpu_id);\n\tvcpu_init_cpuid(vcpu, kvm_get_supported_cpuid());\n\tvcpu_setup(vm, vcpu);\n\n\t \n\tvcpu_regs_get(vcpu, &regs);\n\tregs.rflags = regs.rflags | 0x2;\n\tregs.rsp = stack_vaddr;\n\tregs.rip = (unsigned long) guest_code;\n\tvcpu_regs_set(vcpu, &regs);\n\n\t \n\tmp_state.mp_state = 0;\n\tvcpu_mp_state_set(vcpu, &mp_state);\n\n\treturn vcpu;\n}\n\nstruct kvm_vcpu *vm_arch_vcpu_recreate(struct kvm_vm *vm, uint32_t vcpu_id)\n{\n\tstruct kvm_vcpu *vcpu = __vm_vcpu_add(vm, vcpu_id);\n\n\tvcpu_init_cpuid(vcpu, kvm_get_supported_cpuid());\n\n\treturn vcpu;\n}\n\nvoid vcpu_arch_free(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->cpuid)\n\t\tfree(vcpu->cpuid);\n}\n\n \nstatic void *kvm_supported_cpuid;\n\nconst struct kvm_cpuid2 *kvm_get_supported_cpuid(void)\n{\n\tint kvm_fd;\n\n\tif (kvm_supported_cpuid)\n\t\treturn kvm_supported_cpuid;\n\n\tkvm_supported_cpuid = allocate_kvm_cpuid2(MAX_NR_CPUID_ENTRIES);\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\n\tkvm_ioctl(kvm_fd, KVM_GET_SUPPORTED_CPUID,\n\t\t  (struct kvm_cpuid2 *)kvm_supported_cpuid);\n\n\tclose(kvm_fd);\n\treturn kvm_supported_cpuid;\n}\n\nstatic uint32_t __kvm_cpu_has(const struct kvm_cpuid2 *cpuid,\n\t\t\t      uint32_t function, uint32_t index,\n\t\t\t      uint8_t reg, uint8_t lo, uint8_t hi)\n{\n\tconst struct kvm_cpuid_entry2 *entry;\n\tint i;\n\n\tfor (i = 0; i < cpuid->nent; i++) {\n\t\tentry = &cpuid->entries[i];\n\n\t\t \n\t\tif (entry->function == function && entry->index == index)\n\t\t\treturn ((&entry->eax)[reg] & GENMASK(hi, lo)) >> lo;\n\t}\n\n\treturn 0;\n}\n\nbool kvm_cpuid_has(const struct kvm_cpuid2 *cpuid,\n\t\t   struct kvm_x86_cpu_feature feature)\n{\n\treturn __kvm_cpu_has(cpuid, feature.function, feature.index,\n\t\t\t     feature.reg, feature.bit, feature.bit);\n}\n\nuint32_t kvm_cpuid_property(const struct kvm_cpuid2 *cpuid,\n\t\t\t    struct kvm_x86_cpu_property property)\n{\n\treturn __kvm_cpu_has(cpuid, property.function, property.index,\n\t\t\t     property.reg, property.lo_bit, property.hi_bit);\n}\n\nuint64_t kvm_get_feature_msr(uint64_t msr_index)\n{\n\tstruct {\n\t\tstruct kvm_msrs header;\n\t\tstruct kvm_msr_entry entry;\n\t} buffer = {};\n\tint r, kvm_fd;\n\n\tbuffer.header.nmsrs = 1;\n\tbuffer.entry.index = msr_index;\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\n\tr = __kvm_ioctl(kvm_fd, KVM_GET_MSRS, &buffer.header);\n\tTEST_ASSERT(r == 1, KVM_IOCTL_ERROR(KVM_GET_MSRS, r));\n\n\tclose(kvm_fd);\n\treturn buffer.entry.data;\n}\n\nvoid __vm_xsave_require_permission(uint64_t xfeature, const char *name)\n{\n\tint kvm_fd;\n\tu64 bitmask;\n\tlong rc;\n\tstruct kvm_device_attr attr = {\n\t\t.group = 0,\n\t\t.attr = KVM_X86_XCOMP_GUEST_SUPP,\n\t\t.addr = (unsigned long) &bitmask,\n\t};\n\n\tTEST_ASSERT(!kvm_supported_cpuid,\n\t\t    \"kvm_get_supported_cpuid() cannot be used before ARCH_REQ_XCOMP_GUEST_PERM\");\n\n\tTEST_ASSERT(is_power_of_2(xfeature),\n\t\t    \"Dynamic XFeatures must be enabled one at a time\");\n\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\trc = __kvm_ioctl(kvm_fd, KVM_GET_DEVICE_ATTR, &attr);\n\tclose(kvm_fd);\n\n\tif (rc == -1 && (errno == ENXIO || errno == EINVAL))\n\t\t__TEST_REQUIRE(0, \"KVM_X86_XCOMP_GUEST_SUPP not supported\");\n\n\tTEST_ASSERT(rc == 0, \"KVM_GET_DEVICE_ATTR(0, KVM_X86_XCOMP_GUEST_SUPP) error: %ld\", rc);\n\n\t__TEST_REQUIRE(bitmask & xfeature,\n\t\t       \"Required XSAVE feature '%s' not supported\", name);\n\n\tTEST_REQUIRE(!syscall(SYS_arch_prctl, ARCH_REQ_XCOMP_GUEST_PERM, ilog2(xfeature)));\n\n\trc = syscall(SYS_arch_prctl, ARCH_GET_XCOMP_GUEST_PERM, &bitmask);\n\tTEST_ASSERT(rc == 0, \"prctl(ARCH_GET_XCOMP_GUEST_PERM) error: %ld\", rc);\n\tTEST_ASSERT(bitmask & xfeature,\n\t\t    \"'%s' (0x%lx) not permitted after prctl(ARCH_REQ_XCOMP_GUEST_PERM) permitted=0x%lx\",\n\t\t    name, xfeature, bitmask);\n}\n\nvoid vcpu_init_cpuid(struct kvm_vcpu *vcpu, const struct kvm_cpuid2 *cpuid)\n{\n\tTEST_ASSERT(cpuid != vcpu->cpuid, \"@cpuid can't be the vCPU's CPUID\");\n\n\t \n\tif (vcpu->cpuid && vcpu->cpuid->nent < cpuid->nent) {\n\t\tfree(vcpu->cpuid);\n\t\tvcpu->cpuid = NULL;\n\t}\n\n\tif (!vcpu->cpuid)\n\t\tvcpu->cpuid = allocate_kvm_cpuid2(cpuid->nent);\n\n\tmemcpy(vcpu->cpuid, cpuid, kvm_cpuid2_size(cpuid->nent));\n\tvcpu_set_cpuid(vcpu);\n}\n\nvoid vcpu_set_cpuid_maxphyaddr(struct kvm_vcpu *vcpu, uint8_t maxphyaddr)\n{\n\tstruct kvm_cpuid_entry2 *entry = vcpu_get_cpuid_entry(vcpu, 0x80000008);\n\n\tentry->eax = (entry->eax & ~0xff) | maxphyaddr;\n\tvcpu_set_cpuid(vcpu);\n}\n\nvoid vcpu_clear_cpuid_entry(struct kvm_vcpu *vcpu, uint32_t function)\n{\n\tstruct kvm_cpuid_entry2 *entry = vcpu_get_cpuid_entry(vcpu, function);\n\n\tentry->eax = 0;\n\tentry->ebx = 0;\n\tentry->ecx = 0;\n\tentry->edx = 0;\n\tvcpu_set_cpuid(vcpu);\n}\n\nvoid vcpu_set_or_clear_cpuid_feature(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_x86_cpu_feature feature,\n\t\t\t\t     bool set)\n{\n\tstruct kvm_cpuid_entry2 *entry;\n\tu32 *reg;\n\n\tentry = __vcpu_get_cpuid_entry(vcpu, feature.function, feature.index);\n\treg = (&entry->eax) + feature.reg;\n\n\tif (set)\n\t\t*reg |= BIT(feature.bit);\n\telse\n\t\t*reg &= ~BIT(feature.bit);\n\n\tvcpu_set_cpuid(vcpu);\n}\n\nuint64_t vcpu_get_msr(struct kvm_vcpu *vcpu, uint64_t msr_index)\n{\n\tstruct {\n\t\tstruct kvm_msrs header;\n\t\tstruct kvm_msr_entry entry;\n\t} buffer = {};\n\n\tbuffer.header.nmsrs = 1;\n\tbuffer.entry.index = msr_index;\n\n\tvcpu_msrs_get(vcpu, &buffer.header);\n\n\treturn buffer.entry.data;\n}\n\nint _vcpu_set_msr(struct kvm_vcpu *vcpu, uint64_t msr_index, uint64_t msr_value)\n{\n\tstruct {\n\t\tstruct kvm_msrs header;\n\t\tstruct kvm_msr_entry entry;\n\t} buffer = {};\n\n\tmemset(&buffer, 0, sizeof(buffer));\n\tbuffer.header.nmsrs = 1;\n\tbuffer.entry.index = msr_index;\n\tbuffer.entry.data = msr_value;\n\n\treturn __vcpu_ioctl(vcpu, KVM_SET_MSRS, &buffer.header);\n}\n\nvoid vcpu_args_set(struct kvm_vcpu *vcpu, unsigned int num, ...)\n{\n\tva_list ap;\n\tstruct kvm_regs regs;\n\n\tTEST_ASSERT(num >= 1 && num <= 6, \"Unsupported number of args,\\n\"\n\t\t    \"  num: %u\\n\",\n\t\t    num);\n\n\tva_start(ap, num);\n\tvcpu_regs_get(vcpu, &regs);\n\n\tif (num >= 1)\n\t\tregs.rdi = va_arg(ap, uint64_t);\n\n\tif (num >= 2)\n\t\tregs.rsi = va_arg(ap, uint64_t);\n\n\tif (num >= 3)\n\t\tregs.rdx = va_arg(ap, uint64_t);\n\n\tif (num >= 4)\n\t\tregs.rcx = va_arg(ap, uint64_t);\n\n\tif (num >= 5)\n\t\tregs.r8 = va_arg(ap, uint64_t);\n\n\tif (num >= 6)\n\t\tregs.r9 = va_arg(ap, uint64_t);\n\n\tvcpu_regs_set(vcpu, &regs);\n\tva_end(ap);\n}\n\nvoid vcpu_arch_dump(FILE *stream, struct kvm_vcpu *vcpu, uint8_t indent)\n{\n\tstruct kvm_regs regs;\n\tstruct kvm_sregs sregs;\n\n\tfprintf(stream, \"%*svCPU ID: %u\\n\", indent, \"\", vcpu->id);\n\n\tfprintf(stream, \"%*sregs:\\n\", indent + 2, \"\");\n\tvcpu_regs_get(vcpu, &regs);\n\tregs_dump(stream, &regs, indent + 4);\n\n\tfprintf(stream, \"%*ssregs:\\n\", indent + 2, \"\");\n\tvcpu_sregs_get(vcpu, &sregs);\n\tsregs_dump(stream, &sregs, indent + 4);\n}\n\nstatic struct kvm_msr_list *__kvm_get_msr_index_list(bool feature_msrs)\n{\n\tstruct kvm_msr_list *list;\n\tstruct kvm_msr_list nmsrs;\n\tint kvm_fd, r;\n\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\n\tnmsrs.nmsrs = 0;\n\tif (!feature_msrs)\n\t\tr = __kvm_ioctl(kvm_fd, KVM_GET_MSR_INDEX_LIST, &nmsrs);\n\telse\n\t\tr = __kvm_ioctl(kvm_fd, KVM_GET_MSR_FEATURE_INDEX_LIST, &nmsrs);\n\n\tTEST_ASSERT(r == -1 && errno == E2BIG,\n\t\t    \"Expected -E2BIG, got rc: %i errno: %i (%s)\",\n\t\t    r, errno, strerror(errno));\n\n\tlist = malloc(sizeof(*list) + nmsrs.nmsrs * sizeof(list->indices[0]));\n\tTEST_ASSERT(list, \"-ENOMEM when allocating MSR index list\");\n\tlist->nmsrs = nmsrs.nmsrs;\n\n\tif (!feature_msrs)\n\t\tkvm_ioctl(kvm_fd, KVM_GET_MSR_INDEX_LIST, list);\n\telse\n\t\tkvm_ioctl(kvm_fd, KVM_GET_MSR_FEATURE_INDEX_LIST, list);\n\tclose(kvm_fd);\n\n\tTEST_ASSERT(list->nmsrs == nmsrs.nmsrs,\n\t\t    \"Number of MSRs in list changed, was %d, now %d\",\n\t\t    nmsrs.nmsrs, list->nmsrs);\n\treturn list;\n}\n\nconst struct kvm_msr_list *kvm_get_msr_index_list(void)\n{\n\tstatic const struct kvm_msr_list *list;\n\n\tif (!list)\n\t\tlist = __kvm_get_msr_index_list(false);\n\treturn list;\n}\n\n\nconst struct kvm_msr_list *kvm_get_feature_msr_index_list(void)\n{\n\tstatic const struct kvm_msr_list *list;\n\n\tif (!list)\n\t\tlist = __kvm_get_msr_index_list(true);\n\treturn list;\n}\n\nbool kvm_msr_is_in_save_restore_list(uint32_t msr_index)\n{\n\tconst struct kvm_msr_list *list = kvm_get_msr_index_list();\n\tint i;\n\n\tfor (i = 0; i < list->nmsrs; ++i) {\n\t\tif (list->indices[i] == msr_index)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void vcpu_save_xsave_state(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_x86_state *state)\n{\n\tint size = vm_check_cap(vcpu->vm, KVM_CAP_XSAVE2);\n\n\tif (size) {\n\t\tstate->xsave = malloc(size);\n\t\tvcpu_xsave2_get(vcpu, state->xsave);\n\t} else {\n\t\tstate->xsave = malloc(sizeof(struct kvm_xsave));\n\t\tvcpu_xsave_get(vcpu, state->xsave);\n\t}\n}\n\nstruct kvm_x86_state *vcpu_save_state(struct kvm_vcpu *vcpu)\n{\n\tconst struct kvm_msr_list *msr_list = kvm_get_msr_index_list();\n\tstruct kvm_x86_state *state;\n\tint i;\n\n\tstatic int nested_size = -1;\n\n\tif (nested_size == -1) {\n\t\tnested_size = kvm_check_cap(KVM_CAP_NESTED_STATE);\n\t\tTEST_ASSERT(nested_size <= sizeof(state->nested_),\n\t\t\t    \"Nested state size too big, %i > %zi\",\n\t\t\t    nested_size, sizeof(state->nested_));\n\t}\n\n\t \n\tvcpu_run_complete_io(vcpu);\n\n\tstate = malloc(sizeof(*state) + msr_list->nmsrs * sizeof(state->msrs.entries[0]));\n\tTEST_ASSERT(state, \"-ENOMEM when allocating kvm state\");\n\n\tvcpu_events_get(vcpu, &state->events);\n\tvcpu_mp_state_get(vcpu, &state->mp_state);\n\tvcpu_regs_get(vcpu, &state->regs);\n\tvcpu_save_xsave_state(vcpu, state);\n\n\tif (kvm_has_cap(KVM_CAP_XCRS))\n\t\tvcpu_xcrs_get(vcpu, &state->xcrs);\n\n\tvcpu_sregs_get(vcpu, &state->sregs);\n\n\tif (nested_size) {\n\t\tstate->nested.size = sizeof(state->nested_);\n\n\t\tvcpu_nested_state_get(vcpu, &state->nested);\n\t\tTEST_ASSERT(state->nested.size <= nested_size,\n\t\t\t    \"Nested state size too big, %i (KVM_CHECK_CAP gave %i)\",\n\t\t\t    state->nested.size, nested_size);\n\t} else {\n\t\tstate->nested.size = 0;\n\t}\n\n\tstate->msrs.nmsrs = msr_list->nmsrs;\n\tfor (i = 0; i < msr_list->nmsrs; i++)\n\t\tstate->msrs.entries[i].index = msr_list->indices[i];\n\tvcpu_msrs_get(vcpu, &state->msrs);\n\n\tvcpu_debugregs_get(vcpu, &state->debugregs);\n\n\treturn state;\n}\n\nvoid vcpu_load_state(struct kvm_vcpu *vcpu, struct kvm_x86_state *state)\n{\n\tvcpu_sregs_set(vcpu, &state->sregs);\n\tvcpu_msrs_set(vcpu, &state->msrs);\n\n\tif (kvm_has_cap(KVM_CAP_XCRS))\n\t\tvcpu_xcrs_set(vcpu, &state->xcrs);\n\n\tvcpu_xsave_set(vcpu,  state->xsave);\n\tvcpu_events_set(vcpu, &state->events);\n\tvcpu_mp_state_set(vcpu, &state->mp_state);\n\tvcpu_debugregs_set(vcpu, &state->debugregs);\n\tvcpu_regs_set(vcpu, &state->regs);\n\n\tif (state->nested.size)\n\t\tvcpu_nested_state_set(vcpu, &state->nested);\n}\n\nvoid kvm_x86_state_cleanup(struct kvm_x86_state *state)\n{\n\tfree(state->xsave);\n\tfree(state);\n}\n\nvoid kvm_get_cpu_address_width(unsigned int *pa_bits, unsigned int *va_bits)\n{\n\tif (!kvm_cpu_has_p(X86_PROPERTY_MAX_PHY_ADDR)) {\n\t\t*pa_bits = kvm_cpu_has(X86_FEATURE_PAE) ? 36 : 32;\n\t\t*va_bits = 32;\n\t} else {\n\t\t*pa_bits = kvm_cpu_property(X86_PROPERTY_MAX_PHY_ADDR);\n\t\t*va_bits = kvm_cpu_property(X86_PROPERTY_MAX_VIRT_ADDR);\n\t}\n}\n\nstatic void set_idt_entry(struct kvm_vm *vm, int vector, unsigned long addr,\n\t\t\t  int dpl, unsigned short selector)\n{\n\tstruct idt_entry *base =\n\t\t(struct idt_entry *)addr_gva2hva(vm, vm->idt);\n\tstruct idt_entry *e = &base[vector];\n\n\tmemset(e, 0, sizeof(*e));\n\te->offset0 = addr;\n\te->selector = selector;\n\te->ist = 0;\n\te->type = 14;\n\te->dpl = dpl;\n\te->p = 1;\n\te->offset1 = addr >> 16;\n\te->offset2 = addr >> 32;\n}\n\n\nstatic bool kvm_fixup_exception(struct ex_regs *regs)\n{\n\tif (regs->r9 != KVM_EXCEPTION_MAGIC || regs->rip != regs->r10)\n\t\treturn false;\n\n\tif (regs->vector == DE_VECTOR)\n\t\treturn false;\n\n\tregs->rip = regs->r11;\n\tregs->r9 = regs->vector;\n\tregs->r10 = regs->error_code;\n\treturn true;\n}\n\nvoid route_exception(struct ex_regs *regs)\n{\n\ttypedef void(*handler)(struct ex_regs *);\n\thandler *handlers = (handler *)exception_handlers;\n\n\tif (handlers && handlers[regs->vector]) {\n\t\thandlers[regs->vector](regs);\n\t\treturn;\n\t}\n\n\tif (kvm_fixup_exception(regs))\n\t\treturn;\n\n\tucall_assert(UCALL_UNHANDLED,\n\t\t     \"Unhandled exception in guest\", __FILE__, __LINE__,\n\t\t     \"Unhandled exception '0x%lx' at guest RIP '0x%lx'\",\n\t\t     regs->vector, regs->rip);\n}\n\nvoid vm_init_descriptor_tables(struct kvm_vm *vm)\n{\n\textern void *idt_handlers;\n\tint i;\n\n\tvm->idt = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);\n\tvm->handlers = __vm_vaddr_alloc_page(vm, MEM_REGION_DATA);\n\t \n\tfor (i = 0; i < NUM_INTERRUPTS; i++)\n\t\tset_idt_entry(vm, i, (unsigned long)(&idt_handlers)[i], 0,\n\t\t\tDEFAULT_CODE_SELECTOR);\n}\n\nvoid vcpu_init_descriptor_tables(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_vm *vm = vcpu->vm;\n\tstruct kvm_sregs sregs;\n\n\tvcpu_sregs_get(vcpu, &sregs);\n\tsregs.idt.base = vm->idt;\n\tsregs.idt.limit = NUM_INTERRUPTS * sizeof(struct idt_entry) - 1;\n\tsregs.gdt.base = vm->gdt;\n\tsregs.gdt.limit = getpagesize() - 1;\n\tkvm_seg_set_kernel_data_64bit(NULL, DEFAULT_DATA_SELECTOR, &sregs.gs);\n\tvcpu_sregs_set(vcpu, &sregs);\n\t*(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;\n}\n\nvoid vm_install_exception_handler(struct kvm_vm *vm, int vector,\n\t\t\t       void (*handler)(struct ex_regs *))\n{\n\tvm_vaddr_t *handlers = (vm_vaddr_t *)addr_gva2hva(vm, vm->handlers);\n\n\thandlers[vector] = (vm_vaddr_t)handler;\n}\n\nvoid assert_on_unhandled_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct ucall uc;\n\n\tif (get_ucall(vcpu, &uc) == UCALL_UNHANDLED)\n\t\tREPORT_GUEST_ASSERT(uc);\n}\n\nconst struct kvm_cpuid_entry2 *get_cpuid_entry(const struct kvm_cpuid2 *cpuid,\n\t\t\t\t\t       uint32_t function, uint32_t index)\n{\n\tint i;\n\n\tfor (i = 0; i < cpuid->nent; i++) {\n\t\tif (cpuid->entries[i].function == function &&\n\t\t    cpuid->entries[i].index == index)\n\t\t\treturn &cpuid->entries[i];\n\t}\n\n\tTEST_FAIL(\"CPUID function 0x%x index 0x%x not found \", function, index);\n\n\treturn NULL;\n}\n\n#define X86_HYPERCALL(inputs...)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tuint64_t r;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm volatile(\"test %[use_vmmcall], %[use_vmmcall]\\n\\t\"\t\t\\\n\t\t     \"jnz 1f\\n\\t\"\t\t\t\t\t\\\n\t\t     \"vmcall\\n\\t\"\t\t\t\t\t\\\n\t\t     \"jmp 2f\\n\\t\"\t\t\t\t\t\\\n\t\t     \"1: vmmcall\\n\\t\"\t\t\t\t\t\\\n\t\t     \"2:\"\t\t\t\t\t\t\\\n\t\t     : \"=a\"(r)\t\t\t\t\t\t\\\n\t\t     : [use_vmmcall] \"r\" (host_cpu_is_amd), inputs);\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tr;\t\t\t\t\t\t\t\t\\\n})\n\nuint64_t kvm_hypercall(uint64_t nr, uint64_t a0, uint64_t a1, uint64_t a2,\n\t\t       uint64_t a3)\n{\n\treturn X86_HYPERCALL(\"a\"(nr), \"b\"(a0), \"c\"(a1), \"d\"(a2), \"S\"(a3));\n}\n\nuint64_t __xen_hypercall(uint64_t nr, uint64_t a0, void *a1)\n{\n\treturn X86_HYPERCALL(\"a\"(nr), \"D\"(a0), \"S\"(a1));\n}\n\nvoid xen_hypercall(uint64_t nr, uint64_t a0, void *a1)\n{\n\tGUEST_ASSERT(!__xen_hypercall(nr, a0, a1));\n}\n\nconst struct kvm_cpuid2 *kvm_get_supported_hv_cpuid(void)\n{\n\tstatic struct kvm_cpuid2 *cpuid;\n\tint kvm_fd;\n\n\tif (cpuid)\n\t\treturn cpuid;\n\n\tcpuid = allocate_kvm_cpuid2(MAX_NR_CPUID_ENTRIES);\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\n\tkvm_ioctl(kvm_fd, KVM_GET_SUPPORTED_HV_CPUID, cpuid);\n\n\tclose(kvm_fd);\n\treturn cpuid;\n}\n\nvoid vcpu_set_hv_cpuid(struct kvm_vcpu *vcpu)\n{\n\tstatic struct kvm_cpuid2 *cpuid_full;\n\tconst struct kvm_cpuid2 *cpuid_sys, *cpuid_hv;\n\tint i, nent = 0;\n\n\tif (!cpuid_full) {\n\t\tcpuid_sys = kvm_get_supported_cpuid();\n\t\tcpuid_hv = kvm_get_supported_hv_cpuid();\n\n\t\tcpuid_full = allocate_kvm_cpuid2(cpuid_sys->nent + cpuid_hv->nent);\n\t\tif (!cpuid_full) {\n\t\t\tperror(\"malloc\");\n\t\t\tabort();\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < cpuid_sys->nent; i++) {\n\t\t\tif (cpuid_sys->entries[i].function >= 0x40000000 &&\n\t\t\t    cpuid_sys->entries[i].function < 0x40000100)\n\t\t\t\tcontinue;\n\t\t\tcpuid_full->entries[nent] = cpuid_sys->entries[i];\n\t\t\tnent++;\n\t\t}\n\n\t\tmemcpy(&cpuid_full->entries[nent], cpuid_hv->entries,\n\t\t       cpuid_hv->nent * sizeof(struct kvm_cpuid_entry2));\n\t\tcpuid_full->nent = nent + cpuid_hv->nent;\n\t}\n\n\tvcpu_init_cpuid(vcpu, cpuid_full);\n}\n\nconst struct kvm_cpuid2 *vcpu_get_supported_hv_cpuid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_cpuid2 *cpuid = allocate_kvm_cpuid2(MAX_NR_CPUID_ENTRIES);\n\n\tvcpu_ioctl(vcpu, KVM_GET_SUPPORTED_HV_CPUID, cpuid);\n\n\treturn cpuid;\n}\n\nunsigned long vm_compute_max_gfn(struct kvm_vm *vm)\n{\n\tconst unsigned long num_ht_pages = 12 << (30 - vm->page_shift);  \n\tunsigned long ht_gfn, max_gfn, max_pfn;\n\tuint8_t maxphyaddr;\n\n\tmax_gfn = (1ULL << (vm->pa_bits - vm->page_shift)) - 1;\n\n\t \n\tif (!host_cpu_is_amd)\n\t\treturn max_gfn;\n\n\t \n\tif (vm->pa_bits < 40)\n\t\treturn max_gfn;\n\n\t \n\tht_gfn = (1 << 28) - num_ht_pages;\n\tif (this_cpu_family() < 0x17)\n\t\tgoto done;\n\n\t \n\tif (!this_cpu_has_p(X86_PROPERTY_MAX_PHY_ADDR))\n\t\tgoto done;\n\n\tmaxphyaddr = this_cpu_property(X86_PROPERTY_MAX_PHY_ADDR);\n\tmax_pfn = (1ULL << (maxphyaddr - vm->page_shift)) - 1;\n\n\tif (this_cpu_has_p(X86_PROPERTY_PHYS_ADDR_REDUCTION))\n\t\tmax_pfn >>= this_cpu_property(X86_PROPERTY_PHYS_ADDR_REDUCTION);\n\n\tht_gfn = max_pfn - num_ht_pages;\ndone:\n\treturn min(max_gfn, ht_gfn - 1);\n}\n\n \nbool vm_is_unrestricted_guest(struct kvm_vm *vm)\n{\n\t \n\tif (vm == NULL)\n\t\tclose(open_kvm_dev_path_or_exit());\n\n\treturn get_kvm_intel_param_bool(\"unrestricted_guest\");\n}\n\nvoid kvm_selftest_arch_init(void)\n{\n\thost_cpu_is_intel = this_cpu_is_intel();\n\thost_cpu_is_amd = this_cpu_is_amd();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}