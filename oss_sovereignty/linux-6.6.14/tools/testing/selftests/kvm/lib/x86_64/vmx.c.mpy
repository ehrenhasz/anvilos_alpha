{
  "module_name": "vmx.c",
  "hash_id": "6e02182cbdb3cd1edd56026065f6afd069fdb1f44ca379a47bfb136689c2c113",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/x86_64/vmx.c",
  "human_readable_source": "\n \n\n#include <asm/msr-index.h>\n\n#include \"test_util.h\"\n#include \"kvm_util.h\"\n#include \"processor.h\"\n#include \"vmx.h\"\n\n#define PAGE_SHIFT_4K  12\n\n#define KVM_EPT_PAGE_TABLE_MIN_PADDR 0x1c0000\n\nbool enable_evmcs;\n\nstruct hv_enlightened_vmcs *current_evmcs;\nstruct hv_vp_assist_page *current_vp_assist;\n\nstruct eptPageTableEntry {\n\tuint64_t readable:1;\n\tuint64_t writable:1;\n\tuint64_t executable:1;\n\tuint64_t memory_type:3;\n\tuint64_t ignore_pat:1;\n\tuint64_t page_size:1;\n\tuint64_t accessed:1;\n\tuint64_t dirty:1;\n\tuint64_t ignored_11_10:2;\n\tuint64_t address:40;\n\tuint64_t ignored_62_52:11;\n\tuint64_t suppress_ve:1;\n};\n\nstruct eptPageTablePointer {\n\tuint64_t memory_type:3;\n\tuint64_t page_walk_length:3;\n\tuint64_t ad_enabled:1;\n\tuint64_t reserved_11_07:5;\n\tuint64_t address:40;\n\tuint64_t reserved_63_52:12;\n};\nint vcpu_enable_evmcs(struct kvm_vcpu *vcpu)\n{\n\tuint16_t evmcs_ver;\n\n\tvcpu_enable_cap(vcpu, KVM_CAP_HYPERV_ENLIGHTENED_VMCS,\n\t\t\t(unsigned long)&evmcs_ver);\n\n\t \n\tTEST_ASSERT(((evmcs_ver >> 8) >= (evmcs_ver & 0xff)) &&\n\t\t    (evmcs_ver & 0xff) > 0,\n\t\t    \"Incorrect EVMCS version range: %x:%x\\n\",\n\t\t    evmcs_ver & 0xff, evmcs_ver >> 8);\n\n\treturn evmcs_ver;\n}\n\n \nstruct vmx_pages *\nvcpu_alloc_vmx(struct kvm_vm *vm, vm_vaddr_t *p_vmx_gva)\n{\n\tvm_vaddr_t vmx_gva = vm_vaddr_alloc_page(vm);\n\tstruct vmx_pages *vmx = addr_gva2hva(vm, vmx_gva);\n\n\t \n\tvmx->vmxon = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->vmxon_hva = addr_gva2hva(vm, (uintptr_t)vmx->vmxon);\n\tvmx->vmxon_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->vmxon);\n\n\t \n\tvmx->vmcs = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->vmcs_hva = addr_gva2hva(vm, (uintptr_t)vmx->vmcs);\n\tvmx->vmcs_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->vmcs);\n\n\t \n\tvmx->msr = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->msr_hva = addr_gva2hva(vm, (uintptr_t)vmx->msr);\n\tvmx->msr_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->msr);\n\tmemset(vmx->msr_hva, 0, getpagesize());\n\n\t \n\tvmx->shadow_vmcs = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->shadow_vmcs_hva = addr_gva2hva(vm, (uintptr_t)vmx->shadow_vmcs);\n\tvmx->shadow_vmcs_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->shadow_vmcs);\n\n\t \n\tvmx->vmread = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->vmread_hva = addr_gva2hva(vm, (uintptr_t)vmx->vmread);\n\tvmx->vmread_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->vmread);\n\tmemset(vmx->vmread_hva, 0, getpagesize());\n\n\tvmx->vmwrite = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->vmwrite_hva = addr_gva2hva(vm, (uintptr_t)vmx->vmwrite);\n\tvmx->vmwrite_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->vmwrite);\n\tmemset(vmx->vmwrite_hva, 0, getpagesize());\n\n\t*p_vmx_gva = vmx_gva;\n\treturn vmx;\n}\n\nbool prepare_for_vmx_operation(struct vmx_pages *vmx)\n{\n\tuint64_t feature_control;\n\tuint64_t required;\n\tunsigned long cr0;\n\tunsigned long cr4;\n\n\t \n\t__asm__ __volatile__(\"mov %%cr0, %0\" : \"=r\"(cr0) : : \"memory\");\n\tcr0 &= rdmsr(MSR_IA32_VMX_CR0_FIXED1);\n\tcr0 |= rdmsr(MSR_IA32_VMX_CR0_FIXED0);\n\t__asm__ __volatile__(\"mov %0, %%cr0\" : : \"r\"(cr0) : \"memory\");\n\n\t__asm__ __volatile__(\"mov %%cr4, %0\" : \"=r\"(cr4) : : \"memory\");\n\tcr4 &= rdmsr(MSR_IA32_VMX_CR4_FIXED1);\n\tcr4 |= rdmsr(MSR_IA32_VMX_CR4_FIXED0);\n\t \n\tcr4 |= X86_CR4_VMXE;\n\t__asm__ __volatile__(\"mov %0, %%cr4\" : : \"r\"(cr4) : \"memory\");\n\n\t \n\trequired = FEAT_CTL_VMX_ENABLED_OUTSIDE_SMX;\n\trequired |= FEAT_CTL_LOCKED;\n\tfeature_control = rdmsr(MSR_IA32_FEAT_CTL);\n\tif ((feature_control & required) != required)\n\t\twrmsr(MSR_IA32_FEAT_CTL, feature_control | required);\n\n\t \n\t*(uint32_t *)(vmx->vmxon) = vmcs_revision();\n\tif (vmxon(vmx->vmxon_gpa))\n\t\treturn false;\n\n\treturn true;\n}\n\nbool load_vmcs(struct vmx_pages *vmx)\n{\n\t \n\t*(uint32_t *)(vmx->vmcs) = vmcs_revision();\n\tif (vmclear(vmx->vmcs_gpa))\n\t\treturn false;\n\n\tif (vmptrld(vmx->vmcs_gpa))\n\t\treturn false;\n\n\t \n\t*(uint32_t *)(vmx->shadow_vmcs) = vmcs_revision() | 0x80000000ul;\n\tif (vmclear(vmx->shadow_vmcs_gpa))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool ept_vpid_cap_supported(uint64_t mask)\n{\n\treturn rdmsr(MSR_IA32_VMX_EPT_VPID_CAP) & mask;\n}\n\nbool ept_1g_pages_supported(void)\n{\n\treturn ept_vpid_cap_supported(VMX_EPT_VPID_CAP_1G_PAGES);\n}\n\n \nstatic inline void init_vmcs_control_fields(struct vmx_pages *vmx)\n{\n\tuint32_t sec_exec_ctl = 0;\n\n\tvmwrite(VIRTUAL_PROCESSOR_ID, 0);\n\tvmwrite(POSTED_INTR_NV, 0);\n\n\tvmwrite(PIN_BASED_VM_EXEC_CONTROL, rdmsr(MSR_IA32_VMX_TRUE_PINBASED_CTLS));\n\n\tif (vmx->eptp_gpa) {\n\t\tuint64_t ept_paddr;\n\t\tstruct eptPageTablePointer eptp = {\n\t\t\t.memory_type = VMX_BASIC_MEM_TYPE_WB,\n\t\t\t.page_walk_length = 3,  \n\t\t\t.ad_enabled = ept_vpid_cap_supported(VMX_EPT_VPID_CAP_AD_BITS),\n\t\t\t.address = vmx->eptp_gpa >> PAGE_SHIFT_4K,\n\t\t};\n\n\t\tmemcpy(&ept_paddr, &eptp, sizeof(ept_paddr));\n\t\tvmwrite(EPT_POINTER, ept_paddr);\n\t\tsec_exec_ctl |= SECONDARY_EXEC_ENABLE_EPT;\n\t}\n\n\tif (!vmwrite(SECONDARY_VM_EXEC_CONTROL, sec_exec_ctl))\n\t\tvmwrite(CPU_BASED_VM_EXEC_CONTROL,\n\t\t\trdmsr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS) | CPU_BASED_ACTIVATE_SECONDARY_CONTROLS);\n\telse {\n\t\tvmwrite(CPU_BASED_VM_EXEC_CONTROL, rdmsr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS));\n\t\tGUEST_ASSERT(!sec_exec_ctl);\n\t}\n\n\tvmwrite(EXCEPTION_BITMAP, 0);\n\tvmwrite(PAGE_FAULT_ERROR_CODE_MASK, 0);\n\tvmwrite(PAGE_FAULT_ERROR_CODE_MATCH, -1);  \n\tvmwrite(CR3_TARGET_COUNT, 0);\n\tvmwrite(VM_EXIT_CONTROLS, rdmsr(MSR_IA32_VMX_EXIT_CTLS) |\n\t\tVM_EXIT_HOST_ADDR_SPACE_SIZE);\t   \n\tvmwrite(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmwrite(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tvmwrite(VM_ENTRY_CONTROLS, rdmsr(MSR_IA32_VMX_ENTRY_CTLS) |\n\t\tVM_ENTRY_IA32E_MODE);\t\t   \n\tvmwrite(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\tvmwrite(VM_ENTRY_INTR_INFO_FIELD, 0);\n\tvmwrite(TPR_THRESHOLD, 0);\n\n\tvmwrite(CR0_GUEST_HOST_MASK, 0);\n\tvmwrite(CR4_GUEST_HOST_MASK, 0);\n\tvmwrite(CR0_READ_SHADOW, get_cr0());\n\tvmwrite(CR4_READ_SHADOW, get_cr4());\n\n\tvmwrite(MSR_BITMAP, vmx->msr_gpa);\n\tvmwrite(VMREAD_BITMAP, vmx->vmread_gpa);\n\tvmwrite(VMWRITE_BITMAP, vmx->vmwrite_gpa);\n}\n\n \nstatic inline void init_vmcs_host_state(void)\n{\n\tuint32_t exit_controls = vmreadz(VM_EXIT_CONTROLS);\n\n\tvmwrite(HOST_ES_SELECTOR, get_es());\n\tvmwrite(HOST_CS_SELECTOR, get_cs());\n\tvmwrite(HOST_SS_SELECTOR, get_ss());\n\tvmwrite(HOST_DS_SELECTOR, get_ds());\n\tvmwrite(HOST_FS_SELECTOR, get_fs());\n\tvmwrite(HOST_GS_SELECTOR, get_gs());\n\tvmwrite(HOST_TR_SELECTOR, get_tr());\n\n\tif (exit_controls & VM_EXIT_LOAD_IA32_PAT)\n\t\tvmwrite(HOST_IA32_PAT, rdmsr(MSR_IA32_CR_PAT));\n\tif (exit_controls & VM_EXIT_LOAD_IA32_EFER)\n\t\tvmwrite(HOST_IA32_EFER, rdmsr(MSR_EFER));\n\tif (exit_controls & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tvmwrite(HOST_IA32_PERF_GLOBAL_CTRL,\n\t\t\trdmsr(MSR_CORE_PERF_GLOBAL_CTRL));\n\n\tvmwrite(HOST_IA32_SYSENTER_CS, rdmsr(MSR_IA32_SYSENTER_CS));\n\n\tvmwrite(HOST_CR0, get_cr0());\n\tvmwrite(HOST_CR3, get_cr3());\n\tvmwrite(HOST_CR4, get_cr4());\n\tvmwrite(HOST_FS_BASE, rdmsr(MSR_FS_BASE));\n\tvmwrite(HOST_GS_BASE, rdmsr(MSR_GS_BASE));\n\tvmwrite(HOST_TR_BASE,\n\t\tget_desc64_base((struct desc64 *)(get_gdt().address + get_tr())));\n\tvmwrite(HOST_GDTR_BASE, get_gdt().address);\n\tvmwrite(HOST_IDTR_BASE, get_idt().address);\n\tvmwrite(HOST_IA32_SYSENTER_ESP, rdmsr(MSR_IA32_SYSENTER_ESP));\n\tvmwrite(HOST_IA32_SYSENTER_EIP, rdmsr(MSR_IA32_SYSENTER_EIP));\n}\n\n \nstatic inline void init_vmcs_guest_state(void *rip, void *rsp)\n{\n\tvmwrite(GUEST_ES_SELECTOR, vmreadz(HOST_ES_SELECTOR));\n\tvmwrite(GUEST_CS_SELECTOR, vmreadz(HOST_CS_SELECTOR));\n\tvmwrite(GUEST_SS_SELECTOR, vmreadz(HOST_SS_SELECTOR));\n\tvmwrite(GUEST_DS_SELECTOR, vmreadz(HOST_DS_SELECTOR));\n\tvmwrite(GUEST_FS_SELECTOR, vmreadz(HOST_FS_SELECTOR));\n\tvmwrite(GUEST_GS_SELECTOR, vmreadz(HOST_GS_SELECTOR));\n\tvmwrite(GUEST_LDTR_SELECTOR, 0);\n\tvmwrite(GUEST_TR_SELECTOR, vmreadz(HOST_TR_SELECTOR));\n\tvmwrite(GUEST_INTR_STATUS, 0);\n\tvmwrite(GUEST_PML_INDEX, 0);\n\n\tvmwrite(VMCS_LINK_POINTER, -1ll);\n\tvmwrite(GUEST_IA32_DEBUGCTL, 0);\n\tvmwrite(GUEST_IA32_PAT, vmreadz(HOST_IA32_PAT));\n\tvmwrite(GUEST_IA32_EFER, vmreadz(HOST_IA32_EFER));\n\tvmwrite(GUEST_IA32_PERF_GLOBAL_CTRL,\n\t\tvmreadz(HOST_IA32_PERF_GLOBAL_CTRL));\n\n\tvmwrite(GUEST_ES_LIMIT, -1);\n\tvmwrite(GUEST_CS_LIMIT, -1);\n\tvmwrite(GUEST_SS_LIMIT, -1);\n\tvmwrite(GUEST_DS_LIMIT, -1);\n\tvmwrite(GUEST_FS_LIMIT, -1);\n\tvmwrite(GUEST_GS_LIMIT, -1);\n\tvmwrite(GUEST_LDTR_LIMIT, -1);\n\tvmwrite(GUEST_TR_LIMIT, 0x67);\n\tvmwrite(GUEST_GDTR_LIMIT, 0xffff);\n\tvmwrite(GUEST_IDTR_LIMIT, 0xffff);\n\tvmwrite(GUEST_ES_AR_BYTES,\n\t\tvmreadz(GUEST_ES_SELECTOR) == 0 ? 0x10000 : 0xc093);\n\tvmwrite(GUEST_CS_AR_BYTES, 0xa09b);\n\tvmwrite(GUEST_SS_AR_BYTES, 0xc093);\n\tvmwrite(GUEST_DS_AR_BYTES,\n\t\tvmreadz(GUEST_DS_SELECTOR) == 0 ? 0x10000 : 0xc093);\n\tvmwrite(GUEST_FS_AR_BYTES,\n\t\tvmreadz(GUEST_FS_SELECTOR) == 0 ? 0x10000 : 0xc093);\n\tvmwrite(GUEST_GS_AR_BYTES,\n\t\tvmreadz(GUEST_GS_SELECTOR) == 0 ? 0x10000 : 0xc093);\n\tvmwrite(GUEST_LDTR_AR_BYTES, 0x10000);\n\tvmwrite(GUEST_TR_AR_BYTES, 0x8b);\n\tvmwrite(GUEST_INTERRUPTIBILITY_INFO, 0);\n\tvmwrite(GUEST_ACTIVITY_STATE, 0);\n\tvmwrite(GUEST_SYSENTER_CS, vmreadz(HOST_IA32_SYSENTER_CS));\n\tvmwrite(VMX_PREEMPTION_TIMER_VALUE, 0);\n\n\tvmwrite(GUEST_CR0, vmreadz(HOST_CR0));\n\tvmwrite(GUEST_CR3, vmreadz(HOST_CR3));\n\tvmwrite(GUEST_CR4, vmreadz(HOST_CR4));\n\tvmwrite(GUEST_ES_BASE, 0);\n\tvmwrite(GUEST_CS_BASE, 0);\n\tvmwrite(GUEST_SS_BASE, 0);\n\tvmwrite(GUEST_DS_BASE, 0);\n\tvmwrite(GUEST_FS_BASE, vmreadz(HOST_FS_BASE));\n\tvmwrite(GUEST_GS_BASE, vmreadz(HOST_GS_BASE));\n\tvmwrite(GUEST_LDTR_BASE, 0);\n\tvmwrite(GUEST_TR_BASE, vmreadz(HOST_TR_BASE));\n\tvmwrite(GUEST_GDTR_BASE, vmreadz(HOST_GDTR_BASE));\n\tvmwrite(GUEST_IDTR_BASE, vmreadz(HOST_IDTR_BASE));\n\tvmwrite(GUEST_DR7, 0x400);\n\tvmwrite(GUEST_RSP, (uint64_t)rsp);\n\tvmwrite(GUEST_RIP, (uint64_t)rip);\n\tvmwrite(GUEST_RFLAGS, 2);\n\tvmwrite(GUEST_PENDING_DBG_EXCEPTIONS, 0);\n\tvmwrite(GUEST_SYSENTER_ESP, vmreadz(HOST_IA32_SYSENTER_ESP));\n\tvmwrite(GUEST_SYSENTER_EIP, vmreadz(HOST_IA32_SYSENTER_EIP));\n}\n\nvoid prepare_vmcs(struct vmx_pages *vmx, void *guest_rip, void *guest_rsp)\n{\n\tinit_vmcs_control_fields(vmx);\n\tinit_vmcs_host_state();\n\tinit_vmcs_guest_state(guest_rip, guest_rsp);\n}\n\nstatic void nested_create_pte(struct kvm_vm *vm,\n\t\t\t      struct eptPageTableEntry *pte,\n\t\t\t      uint64_t nested_paddr,\n\t\t\t      uint64_t paddr,\n\t\t\t      int current_level,\n\t\t\t      int target_level)\n{\n\tif (!pte->readable) {\n\t\tpte->writable = true;\n\t\tpte->readable = true;\n\t\tpte->executable = true;\n\t\tpte->page_size = (current_level == target_level);\n\t\tif (pte->page_size)\n\t\t\tpte->address = paddr >> vm->page_shift;\n\t\telse\n\t\t\tpte->address = vm_alloc_page_table(vm) >> vm->page_shift;\n\t} else {\n\t\t \n\t\tTEST_ASSERT(current_level != target_level,\n\t\t\t    \"Cannot create hugepage at level: %u, nested_paddr: 0x%lx\\n\",\n\t\t\t    current_level, nested_paddr);\n\t\tTEST_ASSERT(!pte->page_size,\n\t\t\t    \"Cannot create page table at level: %u, nested_paddr: 0x%lx\\n\",\n\t\t\t    current_level, nested_paddr);\n\t}\n}\n\n\nvoid __nested_pg_map(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\t     uint64_t nested_paddr, uint64_t paddr, int target_level)\n{\n\tconst uint64_t page_size = PG_LEVEL_SIZE(target_level);\n\tstruct eptPageTableEntry *pt = vmx->eptp_hva, *pte;\n\tuint16_t index;\n\n\tTEST_ASSERT(vm->mode == VM_MODE_PXXV48_4K, \"Attempt to use \"\n\t\t    \"unknown or unsupported guest mode, mode: 0x%x\", vm->mode);\n\n\tTEST_ASSERT((nested_paddr >> 48) == 0,\n\t\t    \"Nested physical address 0x%lx requires 5-level paging\",\n\t\t    nested_paddr);\n\tTEST_ASSERT((nested_paddr % page_size) == 0,\n\t\t    \"Nested physical address not on page boundary,\\n\"\n\t\t    \"  nested_paddr: 0x%lx page_size: 0x%lx\",\n\t\t    nested_paddr, page_size);\n\tTEST_ASSERT((nested_paddr >> vm->page_shift) <= vm->max_gfn,\n\t\t    \"Physical address beyond beyond maximum supported,\\n\"\n\t\t    \"  nested_paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\t    paddr, vm->max_gfn, vm->page_size);\n\tTEST_ASSERT((paddr % page_size) == 0,\n\t\t    \"Physical address not on page boundary,\\n\"\n\t\t    \"  paddr: 0x%lx page_size: 0x%lx\",\n\t\t    paddr, page_size);\n\tTEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,\n\t\t    \"Physical address beyond beyond maximum supported,\\n\"\n\t\t    \"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\t    paddr, vm->max_gfn, vm->page_size);\n\n\tfor (int level = PG_LEVEL_512G; level >= PG_LEVEL_4K; level--) {\n\t\tindex = (nested_paddr >> PG_LEVEL_SHIFT(level)) & 0x1ffu;\n\t\tpte = &pt[index];\n\n\t\tnested_create_pte(vm, pte, nested_paddr, paddr, level, target_level);\n\n\t\tif (pte->page_size)\n\t\t\tbreak;\n\n\t\tpt = addr_gpa2hva(vm, pte->address * vm->page_size);\n\t}\n\n\t \n\tpte->accessed = true;\n\tpte->dirty = true;\n\n}\n\nvoid nested_pg_map(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\t   uint64_t nested_paddr, uint64_t paddr)\n{\n\t__nested_pg_map(vmx, vm, nested_paddr, paddr, PG_LEVEL_4K);\n}\n\n \nvoid __nested_map(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\t  uint64_t nested_paddr, uint64_t paddr, uint64_t size,\n\t\t  int level)\n{\n\tsize_t page_size = PG_LEVEL_SIZE(level);\n\tsize_t npages = size / page_size;\n\n\tTEST_ASSERT(nested_paddr + size > nested_paddr, \"Vaddr overflow\");\n\tTEST_ASSERT(paddr + size > paddr, \"Paddr overflow\");\n\n\twhile (npages--) {\n\t\t__nested_pg_map(vmx, vm, nested_paddr, paddr, level);\n\t\tnested_paddr += page_size;\n\t\tpaddr += page_size;\n\t}\n}\n\nvoid nested_map(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\tuint64_t nested_paddr, uint64_t paddr, uint64_t size)\n{\n\t__nested_map(vmx, vm, nested_paddr, paddr, size, PG_LEVEL_4K);\n}\n\n \nvoid nested_map_memslot(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\t\tuint32_t memslot)\n{\n\tsparsebit_idx_t i, last;\n\tstruct userspace_mem_region *region =\n\t\tmemslot2region(vm, memslot);\n\n\ti = (region->region.guest_phys_addr >> vm->page_shift) - 1;\n\tlast = i + (region->region.memory_size >> vm->page_shift);\n\tfor (;;) {\n\t\ti = sparsebit_next_clear(region->unused_phy_pages, i);\n\t\tif (i > last)\n\t\t\tbreak;\n\n\t\tnested_map(vmx, vm,\n\t\t\t   (uint64_t)i << vm->page_shift,\n\t\t\t   (uint64_t)i << vm->page_shift,\n\t\t\t   1 << vm->page_shift);\n\t}\n}\n\n \nvoid nested_identity_map_1g(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\t\t    uint64_t addr, uint64_t size)\n{\n\t__nested_map(vmx, vm, addr, addr, size, PG_LEVEL_1G);\n}\n\nbool kvm_cpu_has_ept(void)\n{\n\tuint64_t ctrl;\n\n\tctrl = kvm_get_feature_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS) >> 32;\n\tif (!(ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS))\n\t\treturn false;\n\n\tctrl = kvm_get_feature_msr(MSR_IA32_VMX_PROCBASED_CTLS2) >> 32;\n\treturn ctrl & SECONDARY_EXEC_ENABLE_EPT;\n}\n\nvoid prepare_eptp(struct vmx_pages *vmx, struct kvm_vm *vm,\n\t\t  uint32_t eptp_memslot)\n{\n\tTEST_ASSERT(kvm_cpu_has_ept(), \"KVM doesn't support nested EPT\");\n\n\tvmx->eptp = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->eptp_hva = addr_gva2hva(vm, (uintptr_t)vmx->eptp);\n\tvmx->eptp_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->eptp);\n}\n\nvoid prepare_virtualize_apic_accesses(struct vmx_pages *vmx, struct kvm_vm *vm)\n{\n\tvmx->apic_access = (void *)vm_vaddr_alloc_page(vm);\n\tvmx->apic_access_hva = addr_gva2hva(vm, (uintptr_t)vmx->apic_access);\n\tvmx->apic_access_gpa = addr_gva2gpa(vm, (uintptr_t)vmx->apic_access);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}