{
  "module_name": "memstress.c",
  "hash_id": "205a72649c1c8e3540b2d92446403842d2a27ee73b8cbbc4215843dbdce90049",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/memstress.c",
  "human_readable_source": "\n \n#define _GNU_SOURCE\n\n#include <inttypes.h>\n#include <linux/bitmap.h>\n\n#include \"kvm_util.h\"\n#include \"memstress.h\"\n#include \"processor.h\"\n\nstruct memstress_args memstress_args;\n\n \nstatic uint64_t guest_test_virt_mem = DEFAULT_GUEST_TEST_MEM;\n\nstruct vcpu_thread {\n\t \n\tint vcpu_idx;\n\n\t \n\tpthread_t thread;\n\n\t \n\tbool running;\n};\n\n \nstatic struct vcpu_thread vcpu_threads[KVM_MAX_VCPUS];\n\n \nstatic void (*vcpu_thread_fn)(struct memstress_vcpu_args *);\n\n \nstatic bool all_vcpu_threads_running;\n\nstatic struct kvm_vcpu *vcpus[KVM_MAX_VCPUS];\n\n \nvoid memstress_guest_code(uint32_t vcpu_idx)\n{\n\tstruct memstress_args *args = &memstress_args;\n\tstruct memstress_vcpu_args *vcpu_args = &args->vcpu_args[vcpu_idx];\n\tstruct guest_random_state rand_state;\n\tuint64_t gva;\n\tuint64_t pages;\n\tuint64_t addr;\n\tuint64_t page;\n\tint i;\n\n\trand_state = new_guest_random_state(args->random_seed + vcpu_idx);\n\n\tgva = vcpu_args->gva;\n\tpages = vcpu_args->pages;\n\n\t \n\tGUEST_ASSERT(vcpu_args->vcpu_idx == vcpu_idx);\n\n\twhile (true) {\n\t\tfor (i = 0; i < sizeof(memstress_args); i += args->guest_page_size)\n\t\t\t(void) *((volatile char *)args + i);\n\n\t\tfor (i = 0; i < pages; i++) {\n\t\t\tif (args->random_access)\n\t\t\t\tpage = guest_random_u32(&rand_state) % pages;\n\t\t\telse\n\t\t\t\tpage = i;\n\n\t\t\taddr = gva + (page * args->guest_page_size);\n\n\t\t\tif (guest_random_u32(&rand_state) % 100 < args->write_percent)\n\t\t\t\t*(uint64_t *)addr = 0x0123456789ABCDEF;\n\t\t\telse\n\t\t\t\tREAD_ONCE(*(uint64_t *)addr);\n\t\t}\n\n\t\tGUEST_SYNC(1);\n\t}\n}\n\nvoid memstress_setup_vcpus(struct kvm_vm *vm, int nr_vcpus,\n\t\t\t   struct kvm_vcpu *vcpus[],\n\t\t\t   uint64_t vcpu_memory_bytes,\n\t\t\t   bool partition_vcpu_memory_access)\n{\n\tstruct memstress_args *args = &memstress_args;\n\tstruct memstress_vcpu_args *vcpu_args;\n\tint i;\n\n\tfor (i = 0; i < nr_vcpus; i++) {\n\t\tvcpu_args = &args->vcpu_args[i];\n\n\t\tvcpu_args->vcpu = vcpus[i];\n\t\tvcpu_args->vcpu_idx = i;\n\n\t\tif (partition_vcpu_memory_access) {\n\t\t\tvcpu_args->gva = guest_test_virt_mem +\n\t\t\t\t\t (i * vcpu_memory_bytes);\n\t\t\tvcpu_args->pages = vcpu_memory_bytes /\n\t\t\t\t\t   args->guest_page_size;\n\t\t\tvcpu_args->gpa = args->gpa + (i * vcpu_memory_bytes);\n\t\t} else {\n\t\t\tvcpu_args->gva = guest_test_virt_mem;\n\t\t\tvcpu_args->pages = (nr_vcpus * vcpu_memory_bytes) /\n\t\t\t\t\t   args->guest_page_size;\n\t\t\tvcpu_args->gpa = args->gpa;\n\t\t}\n\n\t\tvcpu_args_set(vcpus[i], 1, i);\n\n\t\tpr_debug(\"Added VCPU %d with test mem gpa [%lx, %lx)\\n\",\n\t\t\t i, vcpu_args->gpa, vcpu_args->gpa +\n\t\t\t (vcpu_args->pages * args->guest_page_size));\n\t}\n}\n\nstruct kvm_vm *memstress_create_vm(enum vm_guest_mode mode, int nr_vcpus,\n\t\t\t\t   uint64_t vcpu_memory_bytes, int slots,\n\t\t\t\t   enum vm_mem_backing_src_type backing_src,\n\t\t\t\t   bool partition_vcpu_memory_access)\n{\n\tstruct memstress_args *args = &memstress_args;\n\tstruct kvm_vm *vm;\n\tuint64_t guest_num_pages, slot0_pages = 0;\n\tuint64_t backing_src_pagesz = get_backing_src_pagesz(backing_src);\n\tuint64_t region_end_gfn;\n\tint i;\n\n\tpr_info(\"Testing guest mode: %s\\n\", vm_guest_mode_string(mode));\n\n\t \n\targs->write_percent = 100;\n\n\t \n\targs->guest_page_size = vm_guest_mode_params[mode].page_size;\n\n\tguest_num_pages = vm_adjust_num_guest_pages(mode,\n\t\t\t\t(nr_vcpus * vcpu_memory_bytes) / args->guest_page_size);\n\n\tTEST_ASSERT(vcpu_memory_bytes % getpagesize() == 0,\n\t\t    \"Guest memory size is not host page size aligned.\");\n\tTEST_ASSERT(vcpu_memory_bytes % args->guest_page_size == 0,\n\t\t    \"Guest memory size is not guest page size aligned.\");\n\tTEST_ASSERT(guest_num_pages % slots == 0,\n\t\t    \"Guest memory cannot be evenly divided into %d slots.\",\n\t\t    slots);\n\n\t \n\tif (args->nested)\n\t\tslot0_pages += memstress_nested_pages(nr_vcpus);\n\n\t \n\tvm = __vm_create_with_vcpus(mode, nr_vcpus, slot0_pages + guest_num_pages,\n\t\t\t\t    memstress_guest_code, vcpus);\n\n\targs->vm = vm;\n\n\t \n\tregion_end_gfn = vm->max_gfn + 1;\n\n#ifdef __x86_64__\n\t \n\tif (args->nested)\n\t\tregion_end_gfn = min(region_end_gfn, (1UL << 48) / args->guest_page_size);\n#endif\n\t \n\tTEST_ASSERT(guest_num_pages < region_end_gfn,\n\t\t    \"Requested more guest memory than address space allows.\\n\"\n\t\t    \"    guest pages: %\" PRIx64 \" max gfn: %\" PRIx64\n\t\t    \" nr_vcpus: %d wss: %\" PRIx64 \"]\\n\",\n\t\t    guest_num_pages, region_end_gfn - 1, nr_vcpus, vcpu_memory_bytes);\n\n\targs->gpa = (region_end_gfn - guest_num_pages - 1) * args->guest_page_size;\n\targs->gpa = align_down(args->gpa, backing_src_pagesz);\n#ifdef __s390x__\n\t \n\targs->gpa = align_down(args->gpa, 1 << 20);\n#endif\n\targs->size = guest_num_pages * args->guest_page_size;\n\tpr_info(\"guest physical test memory: [0x%lx, 0x%lx)\\n\",\n\t\targs->gpa, args->gpa + args->size);\n\n\t \n\tfor (i = 0; i < slots; i++) {\n\t\tuint64_t region_pages = guest_num_pages / slots;\n\t\tvm_paddr_t region_start = args->gpa + region_pages * args->guest_page_size * i;\n\n\t\tvm_userspace_mem_region_add(vm, backing_src, region_start,\n\t\t\t\t\t    MEMSTRESS_MEM_SLOT_INDEX + i,\n\t\t\t\t\t    region_pages, 0);\n\t}\n\n\t \n\tvirt_map(vm, guest_test_virt_mem, args->gpa, guest_num_pages);\n\n\tmemstress_setup_vcpus(vm, nr_vcpus, vcpus, vcpu_memory_bytes,\n\t\t\t      partition_vcpu_memory_access);\n\n\tif (args->nested) {\n\t\tpr_info(\"Configuring vCPUs to run in L2 (nested).\\n\");\n\t\tmemstress_setup_nested(vm, nr_vcpus, vcpus);\n\t}\n\n\t \n\tsync_global_to_guest(vm, memstress_args);\n\n\treturn vm;\n}\n\nvoid memstress_destroy_vm(struct kvm_vm *vm)\n{\n\tkvm_vm_free(vm);\n}\n\nvoid memstress_set_write_percent(struct kvm_vm *vm, uint32_t write_percent)\n{\n\tmemstress_args.write_percent = write_percent;\n\tsync_global_to_guest(vm, memstress_args.write_percent);\n}\n\nvoid memstress_set_random_seed(struct kvm_vm *vm, uint32_t random_seed)\n{\n\tmemstress_args.random_seed = random_seed;\n\tsync_global_to_guest(vm, memstress_args.random_seed);\n}\n\nvoid memstress_set_random_access(struct kvm_vm *vm, bool random_access)\n{\n\tmemstress_args.random_access = random_access;\n\tsync_global_to_guest(vm, memstress_args.random_access);\n}\n\nuint64_t __weak memstress_nested_pages(int nr_vcpus)\n{\n\treturn 0;\n}\n\nvoid __weak memstress_setup_nested(struct kvm_vm *vm, int nr_vcpus, struct kvm_vcpu **vcpus)\n{\n\tpr_info(\"%s() not support on this architecture, skipping.\\n\", __func__);\n\texit(KSFT_SKIP);\n}\n\nstatic void *vcpu_thread_main(void *data)\n{\n\tstruct vcpu_thread *vcpu = data;\n\tint vcpu_idx = vcpu->vcpu_idx;\n\n\tif (memstress_args.pin_vcpus)\n\t\tkvm_pin_this_task_to_pcpu(memstress_args.vcpu_to_pcpu[vcpu_idx]);\n\n\tWRITE_ONCE(vcpu->running, true);\n\n\t \n\twhile (!READ_ONCE(all_vcpu_threads_running))\n\t\t;\n\n\tvcpu_thread_fn(&memstress_args.vcpu_args[vcpu_idx]);\n\n\treturn NULL;\n}\n\nvoid memstress_start_vcpu_threads(int nr_vcpus,\n\t\t\t\t  void (*vcpu_fn)(struct memstress_vcpu_args *))\n{\n\tint i;\n\n\tvcpu_thread_fn = vcpu_fn;\n\tWRITE_ONCE(all_vcpu_threads_running, false);\n\tWRITE_ONCE(memstress_args.stop_vcpus, false);\n\n\tfor (i = 0; i < nr_vcpus; i++) {\n\t\tstruct vcpu_thread *vcpu = &vcpu_threads[i];\n\n\t\tvcpu->vcpu_idx = i;\n\t\tWRITE_ONCE(vcpu->running, false);\n\n\t\tpthread_create(&vcpu->thread, NULL, vcpu_thread_main, vcpu);\n\t}\n\n\tfor (i = 0; i < nr_vcpus; i++) {\n\t\twhile (!READ_ONCE(vcpu_threads[i].running))\n\t\t\t;\n\t}\n\n\tWRITE_ONCE(all_vcpu_threads_running, true);\n}\n\nvoid memstress_join_vcpu_threads(int nr_vcpus)\n{\n\tint i;\n\n\tWRITE_ONCE(memstress_args.stop_vcpus, true);\n\n\tfor (i = 0; i < nr_vcpus; i++)\n\t\tpthread_join(vcpu_threads[i].thread, NULL);\n}\n\nstatic void toggle_dirty_logging(struct kvm_vm *vm, int slots, bool enable)\n{\n\tint i;\n\n\tfor (i = 0; i < slots; i++) {\n\t\tint slot = MEMSTRESS_MEM_SLOT_INDEX + i;\n\t\tint flags = enable ? KVM_MEM_LOG_DIRTY_PAGES : 0;\n\n\t\tvm_mem_region_set_flags(vm, slot, flags);\n\t}\n}\n\nvoid memstress_enable_dirty_logging(struct kvm_vm *vm, int slots)\n{\n\ttoggle_dirty_logging(vm, slots, true);\n}\n\nvoid memstress_disable_dirty_logging(struct kvm_vm *vm, int slots)\n{\n\ttoggle_dirty_logging(vm, slots, false);\n}\n\nvoid memstress_get_dirty_log(struct kvm_vm *vm, unsigned long *bitmaps[], int slots)\n{\n\tint i;\n\n\tfor (i = 0; i < slots; i++) {\n\t\tint slot = MEMSTRESS_MEM_SLOT_INDEX + i;\n\n\t\tkvm_vm_get_dirty_log(vm, slot, bitmaps[i]);\n\t}\n}\n\nvoid memstress_clear_dirty_log(struct kvm_vm *vm, unsigned long *bitmaps[],\n\t\t\t       int slots, uint64_t pages_per_slot)\n{\n\tint i;\n\n\tfor (i = 0; i < slots; i++) {\n\t\tint slot = MEMSTRESS_MEM_SLOT_INDEX + i;\n\n\t\tkvm_vm_clear_dirty_log(vm, slot, bitmaps[i], 0, pages_per_slot);\n\t}\n}\n\nunsigned long **memstress_alloc_bitmaps(int slots, uint64_t pages_per_slot)\n{\n\tunsigned long **bitmaps;\n\tint i;\n\n\tbitmaps = malloc(slots * sizeof(bitmaps[0]));\n\tTEST_ASSERT(bitmaps, \"Failed to allocate bitmaps array.\");\n\n\tfor (i = 0; i < slots; i++) {\n\t\tbitmaps[i] = bitmap_zalloc(pages_per_slot);\n\t\tTEST_ASSERT(bitmaps[i], \"Failed to allocate slot bitmap.\");\n\t}\n\n\treturn bitmaps;\n}\n\nvoid memstress_free_bitmaps(unsigned long *bitmaps[], int slots)\n{\n\tint i;\n\n\tfor (i = 0; i < slots; i++)\n\t\tfree(bitmaps[i]);\n\n\tfree(bitmaps);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}