{
  "module_name": "processor.c",
  "hash_id": "edaa87874c94c7aeaa6752c6852ef8ab4c597fceea723c5dda64cb92a5e9852a",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/riscv/processor.c",
  "human_readable_source": "\n \n\n#include <linux/compiler.h>\n#include <assert.h>\n\n#include \"kvm_util.h\"\n#include \"processor.h\"\n\n#define DEFAULT_RISCV_GUEST_STACK_VADDR_MIN\t0xac0000\n\nstatic uint64_t page_align(struct kvm_vm *vm, uint64_t v)\n{\n\treturn (v + vm->page_size) & ~(vm->page_size - 1);\n}\n\nstatic uint64_t pte_addr(struct kvm_vm *vm, uint64_t entry)\n{\n\treturn ((entry & PGTBL_PTE_ADDR_MASK) >> PGTBL_PTE_ADDR_SHIFT) <<\n\t\tPGTBL_PAGE_SIZE_SHIFT;\n}\n\nstatic uint64_t ptrs_per_pte(struct kvm_vm *vm)\n{\n\treturn PGTBL_PAGE_SIZE / sizeof(uint64_t);\n}\n\nstatic uint64_t pte_index_mask[] = {\n\tPGTBL_L0_INDEX_MASK,\n\tPGTBL_L1_INDEX_MASK,\n\tPGTBL_L2_INDEX_MASK,\n\tPGTBL_L3_INDEX_MASK,\n};\n\nstatic uint32_t pte_index_shift[] = {\n\tPGTBL_L0_INDEX_SHIFT,\n\tPGTBL_L1_INDEX_SHIFT,\n\tPGTBL_L2_INDEX_SHIFT,\n\tPGTBL_L3_INDEX_SHIFT,\n};\n\nstatic uint64_t pte_index(struct kvm_vm *vm, vm_vaddr_t gva, int level)\n{\n\tTEST_ASSERT(level > -1,\n\t\t\"Negative page table level (%d) not possible\", level);\n\tTEST_ASSERT(level < vm->pgtable_levels,\n\t\t\"Invalid page table level (%d)\", level);\n\n\treturn (gva & pte_index_mask[level]) >> pte_index_shift[level];\n}\n\nvoid virt_arch_pgd_alloc(struct kvm_vm *vm)\n{\n\tsize_t nr_pages = page_align(vm, ptrs_per_pte(vm) * 8) / vm->page_size;\n\n\tif (vm->pgd_created)\n\t\treturn;\n\n\tvm->pgd = vm_phy_pages_alloc(vm, nr_pages,\n\t\t\t\t     KVM_GUEST_PAGE_TABLE_MIN_PADDR,\n\t\t\t\t     vm->memslots[MEM_REGION_PT]);\n\tvm->pgd_created = true;\n}\n\nvoid virt_arch_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)\n{\n\tuint64_t *ptep, next_ppn;\n\tint level = vm->pgtable_levels - 1;\n\n\tTEST_ASSERT((vaddr % vm->page_size) == 0,\n\t\t\"Virtual address not on page boundary,\\n\"\n\t\t\"  vaddr: 0x%lx vm->page_size: 0x%x\", vaddr, vm->page_size);\n\tTEST_ASSERT(sparsebit_is_set(vm->vpages_valid,\n\t\t(vaddr >> vm->page_shift)),\n\t\t\"Invalid virtual address, vaddr: 0x%lx\", vaddr);\n\tTEST_ASSERT((paddr % vm->page_size) == 0,\n\t\t\"Physical address not on page boundary,\\n\"\n\t\t\"  paddr: 0x%lx vm->page_size: 0x%x\", paddr, vm->page_size);\n\tTEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,\n\t\t\"Physical address beyond maximum supported,\\n\"\n\t\t\"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\tpaddr, vm->max_gfn, vm->page_size);\n\n\tptep = addr_gpa2hva(vm, vm->pgd) + pte_index(vm, vaddr, level) * 8;\n\tif (!*ptep) {\n\t\tnext_ppn = vm_alloc_page_table(vm) >> PGTBL_PAGE_SIZE_SHIFT;\n\t\t*ptep = (next_ppn << PGTBL_PTE_ADDR_SHIFT) |\n\t\t\tPGTBL_PTE_VALID_MASK;\n\t}\n\tlevel--;\n\n\twhile (level > -1) {\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) +\n\t\t       pte_index(vm, vaddr, level) * 8;\n\t\tif (!*ptep && level > 0) {\n\t\t\tnext_ppn = vm_alloc_page_table(vm) >>\n\t\t\t\t   PGTBL_PAGE_SIZE_SHIFT;\n\t\t\t*ptep = (next_ppn << PGTBL_PTE_ADDR_SHIFT) |\n\t\t\t\tPGTBL_PTE_VALID_MASK;\n\t\t}\n\t\tlevel--;\n\t}\n\n\tpaddr = paddr >> PGTBL_PAGE_SIZE_SHIFT;\n\t*ptep = (paddr << PGTBL_PTE_ADDR_SHIFT) |\n\t\tPGTBL_PTE_PERM_MASK | PGTBL_PTE_VALID_MASK;\n}\n\nvm_paddr_t addr_arch_gva2gpa(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tuint64_t *ptep;\n\tint level = vm->pgtable_levels - 1;\n\n\tif (!vm->pgd_created)\n\t\tgoto unmapped_gva;\n\n\tptep = addr_gpa2hva(vm, vm->pgd) + pte_index(vm, gva, level) * 8;\n\tif (!ptep)\n\t\tgoto unmapped_gva;\n\tlevel--;\n\n\twhile (level > -1) {\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) +\n\t\t       pte_index(vm, gva, level) * 8;\n\t\tif (!ptep)\n\t\t\tgoto unmapped_gva;\n\t\tlevel--;\n\t}\n\n\treturn pte_addr(vm, *ptep) + (gva & (vm->page_size - 1));\n\nunmapped_gva:\n\tTEST_FAIL(\"No mapping for vm virtual address gva: 0x%lx level: %d\",\n\t\t  gva, level);\n\texit(1);\n}\n\nstatic void pte_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent,\n\t\t     uint64_t page, int level)\n{\n#ifdef DEBUG\n\tstatic const char *const type[] = { \"pte\", \"pmd\", \"pud\", \"p4d\"};\n\tuint64_t pte, *ptep;\n\n\tif (level < 0)\n\t\treturn;\n\n\tfor (pte = page; pte < page + ptrs_per_pte(vm) * 8; pte += 8) {\n\t\tptep = addr_gpa2hva(vm, pte);\n\t\tif (!*ptep)\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*s%s: %lx: %lx at %p\\n\", indent, \"\",\n\t\t\ttype[level], pte, *ptep, ptep);\n\t\tpte_dump(stream, vm, indent + 1,\n\t\t\t pte_addr(vm, *ptep), level - 1);\n\t}\n#endif\n}\n\nvoid virt_arch_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)\n{\n\tint level = vm->pgtable_levels - 1;\n\tuint64_t pgd, *ptep;\n\n\tif (!vm->pgd_created)\n\t\treturn;\n\n\tfor (pgd = vm->pgd; pgd < vm->pgd + ptrs_per_pte(vm) * 8; pgd += 8) {\n\t\tptep = addr_gpa2hva(vm, pgd);\n\t\tif (!*ptep)\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*spgd: %lx: %lx at %p\\n\", indent, \"\",\n\t\t\tpgd, *ptep, ptep);\n\t\tpte_dump(stream, vm, indent + 1,\n\t\t\t pte_addr(vm, *ptep), level - 1);\n\t}\n}\n\nvoid riscv_vcpu_mmu_setup(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_vm *vm = vcpu->vm;\n\tunsigned long satp;\n\n\t \n\tswitch (vm->mode) {\n\tcase VM_MODE_P52V48_4K:\n\tcase VM_MODE_P48V48_4K:\n\tcase VM_MODE_P40V48_4K:\n\t\tbreak;\n\tdefault:\n\t\tTEST_FAIL(\"Unknown guest mode, mode: 0x%x\", vm->mode);\n\t}\n\n\tsatp = (vm->pgd >> PGTBL_PAGE_SIZE_SHIFT) & SATP_PPN;\n\tsatp |= SATP_MODE_48;\n\n\tvcpu_set_reg(vcpu, RISCV_CSR_REG(satp), satp);\n}\n\nvoid vcpu_arch_dump(FILE *stream, struct kvm_vcpu *vcpu, uint8_t indent)\n{\n\tstruct kvm_riscv_core core;\n\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(mode), &core.mode);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.pc), &core.regs.pc);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.ra), &core.regs.ra);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.sp), &core.regs.sp);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.gp), &core.regs.gp);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.tp), &core.regs.tp);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t0), &core.regs.t0);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t1), &core.regs.t1);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t2), &core.regs.t2);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s0), &core.regs.s0);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s1), &core.regs.s1);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a0), &core.regs.a0);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a1), &core.regs.a1);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a2), &core.regs.a2);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a3), &core.regs.a3);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a4), &core.regs.a4);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a5), &core.regs.a5);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a6), &core.regs.a6);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.a7), &core.regs.a7);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s2), &core.regs.s2);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s3), &core.regs.s3);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s4), &core.regs.s4);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s5), &core.regs.s5);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s6), &core.regs.s6);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s7), &core.regs.s7);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s8), &core.regs.s8);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s9), &core.regs.s9);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s10), &core.regs.s10);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.s11), &core.regs.s11);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t3), &core.regs.t3);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t4), &core.regs.t4);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t5), &core.regs.t5);\n\tvcpu_get_reg(vcpu, RISCV_CORE_REG(regs.t6), &core.regs.t6);\n\n\tfprintf(stream,\n\t\t\" MODE:  0x%lx\\n\", core.mode);\n\tfprintf(stream,\n\t\t\" PC: 0x%016lx   RA: 0x%016lx SP: 0x%016lx GP: 0x%016lx\\n\",\n\t\tcore.regs.pc, core.regs.ra, core.regs.sp, core.regs.gp);\n\tfprintf(stream,\n\t\t\" TP: 0x%016lx   T0: 0x%016lx T1: 0x%016lx T2: 0x%016lx\\n\",\n\t\tcore.regs.tp, core.regs.t0, core.regs.t1, core.regs.t2);\n\tfprintf(stream,\n\t\t\" S0: 0x%016lx   S1: 0x%016lx A0: 0x%016lx A1: 0x%016lx\\n\",\n\t\tcore.regs.s0, core.regs.s1, core.regs.a0, core.regs.a1);\n\tfprintf(stream,\n\t\t\" A2: 0x%016lx   A3: 0x%016lx A4: 0x%016lx A5: 0x%016lx\\n\",\n\t\tcore.regs.a2, core.regs.a3, core.regs.a4, core.regs.a5);\n\tfprintf(stream,\n\t\t\" A6: 0x%016lx   A7: 0x%016lx S2: 0x%016lx S3: 0x%016lx\\n\",\n\t\tcore.regs.a6, core.regs.a7, core.regs.s2, core.regs.s3);\n\tfprintf(stream,\n\t\t\" S4: 0x%016lx   S5: 0x%016lx S6: 0x%016lx S7: 0x%016lx\\n\",\n\t\tcore.regs.s4, core.regs.s5, core.regs.s6, core.regs.s7);\n\tfprintf(stream,\n\t\t\" S8: 0x%016lx   S9: 0x%016lx S10: 0x%016lx S11: 0x%016lx\\n\",\n\t\tcore.regs.s8, core.regs.s9, core.regs.s10, core.regs.s11);\n\tfprintf(stream,\n\t\t\" T3: 0x%016lx   T4: 0x%016lx T5: 0x%016lx T6: 0x%016lx\\n\",\n\t\tcore.regs.t3, core.regs.t4, core.regs.t5, core.regs.t6);\n}\n\nstatic void __aligned(16) guest_unexp_trap(void)\n{\n\tsbi_ecall(KVM_RISCV_SELFTESTS_SBI_EXT,\n\t\t  KVM_RISCV_SELFTESTS_SBI_UNEXP,\n\t\t  0, 0, 0, 0, 0, 0);\n}\n\nstruct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,\n\t\t\t\t  void *guest_code)\n{\n\tint r;\n\tsize_t stack_size;\n\tunsigned long stack_vaddr;\n\tunsigned long current_gp = 0;\n\tstruct kvm_mp_state mps;\n\tstruct kvm_vcpu *vcpu;\n\n\tstack_size = vm->page_size == 4096 ? DEFAULT_STACK_PGS * vm->page_size :\n\t\t\t\t\t     vm->page_size;\n\tstack_vaddr = __vm_vaddr_alloc(vm, stack_size,\n\t\t\t\t       DEFAULT_RISCV_GUEST_STACK_VADDR_MIN,\n\t\t\t\t       MEM_REGION_DATA);\n\n\tvcpu = __vm_vcpu_add(vm, vcpu_id);\n\triscv_vcpu_mmu_setup(vcpu);\n\n\t \n\tmps.mp_state = KVM_MP_STATE_RUNNABLE;\n\tr = __vcpu_ioctl(vcpu, KVM_SET_MP_STATE, &mps);\n\tTEST_ASSERT(!r, \"IOCTL KVM_SET_MP_STATE failed (error %d)\", r);\n\n\t \n\tasm volatile (\n\t\t\"add %0, gp, zero\" : \"=r\" (current_gp) : : \"memory\");\n\tvcpu_set_reg(vcpu, RISCV_CORE_REG(regs.gp), current_gp);\n\n\t \n\tvcpu_set_reg(vcpu, RISCV_CORE_REG(regs.sp), stack_vaddr + stack_size);\n\tvcpu_set_reg(vcpu, RISCV_CORE_REG(regs.pc), (unsigned long)guest_code);\n\n\t \n\tvcpu_set_reg(vcpu, RISCV_CSR_REG(stvec), (unsigned long)guest_unexp_trap);\n\n\treturn vcpu;\n}\n\nvoid vcpu_args_set(struct kvm_vcpu *vcpu, unsigned int num, ...)\n{\n\tva_list ap;\n\tuint64_t id = RISCV_CORE_REG(regs.a0);\n\tint i;\n\n\tTEST_ASSERT(num >= 1 && num <= 8, \"Unsupported number of args,\\n\"\n\t\t    \"  num: %u\\n\", num);\n\n\tva_start(ap, num);\n\n\tfor (i = 0; i < num; i++) {\n\t\tswitch (i) {\n\t\tcase 0:\n\t\t\tid = RISCV_CORE_REG(regs.a0);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tid = RISCV_CORE_REG(regs.a1);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tid = RISCV_CORE_REG(regs.a2);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tid = RISCV_CORE_REG(regs.a3);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tid = RISCV_CORE_REG(regs.a4);\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tid = RISCV_CORE_REG(regs.a5);\n\t\t\tbreak;\n\t\tcase 6:\n\t\t\tid = RISCV_CORE_REG(regs.a6);\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tid = RISCV_CORE_REG(regs.a7);\n\t\t\tbreak;\n\t\t}\n\t\tvcpu_set_reg(vcpu, id, va_arg(ap, uint64_t));\n\t}\n\n\tva_end(ap);\n}\n\nvoid assert_on_unhandled_exception(struct kvm_vcpu *vcpu)\n{\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}