{
  "module_name": "processor.c",
  "hash_id": "35aef2369ce987b18229e586642d0721a153f13a8382e90149faa5801a9feb4e",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/aarch64/processor.c",
  "human_readable_source": "\n \n\n#include <linux/compiler.h>\n#include <assert.h>\n\n#include \"guest_modes.h\"\n#include \"kvm_util.h\"\n#include \"processor.h\"\n#include <linux/bitfield.h>\n\n#define DEFAULT_ARM64_GUEST_STACK_VADDR_MIN\t0xac0000\n\nstatic vm_vaddr_t exception_handlers;\n\nstatic uint64_t page_align(struct kvm_vm *vm, uint64_t v)\n{\n\treturn (v + vm->page_size) & ~(vm->page_size - 1);\n}\n\nstatic uint64_t pgd_index(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tunsigned int shift = (vm->pgtable_levels - 1) * (vm->page_shift - 3) + vm->page_shift;\n\tuint64_t mask = (1UL << (vm->va_bits - shift)) - 1;\n\n\treturn (gva >> shift) & mask;\n}\n\nstatic uint64_t pud_index(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tunsigned int shift = 2 * (vm->page_shift - 3) + vm->page_shift;\n\tuint64_t mask = (1UL << (vm->page_shift - 3)) - 1;\n\n\tTEST_ASSERT(vm->pgtable_levels == 4,\n\t\t\"Mode %d does not have 4 page table levels\", vm->mode);\n\n\treturn (gva >> shift) & mask;\n}\n\nstatic uint64_t pmd_index(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tunsigned int shift = (vm->page_shift - 3) + vm->page_shift;\n\tuint64_t mask = (1UL << (vm->page_shift - 3)) - 1;\n\n\tTEST_ASSERT(vm->pgtable_levels >= 3,\n\t\t\"Mode %d does not have >= 3 page table levels\", vm->mode);\n\n\treturn (gva >> shift) & mask;\n}\n\nstatic uint64_t pte_index(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tuint64_t mask = (1UL << (vm->page_shift - 3)) - 1;\n\treturn (gva >> vm->page_shift) & mask;\n}\n\nstatic uint64_t addr_pte(struct kvm_vm *vm, uint64_t pa, uint64_t attrs)\n{\n\tuint64_t pte;\n\n\tpte = pa & GENMASK(47, vm->page_shift);\n\tif (vm->page_shift == 16)\n\t\tpte |= FIELD_GET(GENMASK(51, 48), pa) << 12;\n\tpte |= attrs;\n\n\treturn pte;\n}\n\nstatic uint64_t pte_addr(struct kvm_vm *vm, uint64_t pte)\n{\n\tuint64_t pa;\n\n\tpa = pte & GENMASK(47, vm->page_shift);\n\tif (vm->page_shift == 16)\n\t\tpa |= FIELD_GET(GENMASK(15, 12), pte) << 48;\n\n\treturn pa;\n}\n\nstatic uint64_t ptrs_per_pgd(struct kvm_vm *vm)\n{\n\tunsigned int shift = (vm->pgtable_levels - 1) * (vm->page_shift - 3) + vm->page_shift;\n\treturn 1 << (vm->va_bits - shift);\n}\n\nstatic uint64_t __maybe_unused ptrs_per_pte(struct kvm_vm *vm)\n{\n\treturn 1 << (vm->page_shift - 3);\n}\n\nvoid virt_arch_pgd_alloc(struct kvm_vm *vm)\n{\n\tsize_t nr_pages = page_align(vm, ptrs_per_pgd(vm) * 8) / vm->page_size;\n\n\tif (vm->pgd_created)\n\t\treturn;\n\n\tvm->pgd = vm_phy_pages_alloc(vm, nr_pages,\n\t\t\t\t     KVM_GUEST_PAGE_TABLE_MIN_PADDR,\n\t\t\t\t     vm->memslots[MEM_REGION_PT]);\n\tvm->pgd_created = true;\n}\n\nstatic void _virt_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,\n\t\t\t uint64_t flags)\n{\n\tuint8_t attr_idx = flags & 7;\n\tuint64_t *ptep;\n\n\tTEST_ASSERT((vaddr % vm->page_size) == 0,\n\t\t\"Virtual address not on page boundary,\\n\"\n\t\t\"  vaddr: 0x%lx vm->page_size: 0x%x\", vaddr, vm->page_size);\n\tTEST_ASSERT(sparsebit_is_set(vm->vpages_valid,\n\t\t(vaddr >> vm->page_shift)),\n\t\t\"Invalid virtual address, vaddr: 0x%lx\", vaddr);\n\tTEST_ASSERT((paddr % vm->page_size) == 0,\n\t\t\"Physical address not on page boundary,\\n\"\n\t\t\"  paddr: 0x%lx vm->page_size: 0x%x\", paddr, vm->page_size);\n\tTEST_ASSERT((paddr >> vm->page_shift) <= vm->max_gfn,\n\t\t\"Physical address beyond beyond maximum supported,\\n\"\n\t\t\"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\tpaddr, vm->max_gfn, vm->page_size);\n\n\tptep = addr_gpa2hva(vm, vm->pgd) + pgd_index(vm, vaddr) * 8;\n\tif (!*ptep)\n\t\t*ptep = addr_pte(vm, vm_alloc_page_table(vm), 3);\n\n\tswitch (vm->pgtable_levels) {\n\tcase 4:\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) + pud_index(vm, vaddr) * 8;\n\t\tif (!*ptep)\n\t\t\t*ptep = addr_pte(vm, vm_alloc_page_table(vm), 3);\n\t\t \n\tcase 3:\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) + pmd_index(vm, vaddr) * 8;\n\t\tif (!*ptep)\n\t\t\t*ptep = addr_pte(vm, vm_alloc_page_table(vm), 3);\n\t\t \n\tcase 2:\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) + pte_index(vm, vaddr) * 8;\n\t\tbreak;\n\tdefault:\n\t\tTEST_FAIL(\"Page table levels must be 2, 3, or 4\");\n\t}\n\n\t*ptep = addr_pte(vm, paddr, (attr_idx << 2) | (1 << 10) | 3);   \n}\n\nvoid virt_arch_pg_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr)\n{\n\tuint64_t attr_idx = MT_NORMAL;\n\n\t_virt_pg_map(vm, vaddr, paddr, attr_idx);\n}\n\nuint64_t *virt_get_pte_hva(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tuint64_t *ptep;\n\n\tif (!vm->pgd_created)\n\t\tgoto unmapped_gva;\n\n\tptep = addr_gpa2hva(vm, vm->pgd) + pgd_index(vm, gva) * 8;\n\tif (!ptep)\n\t\tgoto unmapped_gva;\n\n\tswitch (vm->pgtable_levels) {\n\tcase 4:\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) + pud_index(vm, gva) * 8;\n\t\tif (!ptep)\n\t\t\tgoto unmapped_gva;\n\t\t \n\tcase 3:\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) + pmd_index(vm, gva) * 8;\n\t\tif (!ptep)\n\t\t\tgoto unmapped_gva;\n\t\t \n\tcase 2:\n\t\tptep = addr_gpa2hva(vm, pte_addr(vm, *ptep)) + pte_index(vm, gva) * 8;\n\t\tif (!ptep)\n\t\t\tgoto unmapped_gva;\n\t\tbreak;\n\tdefault:\n\t\tTEST_FAIL(\"Page table levels must be 2, 3, or 4\");\n\t}\n\n\treturn ptep;\n\nunmapped_gva:\n\tTEST_FAIL(\"No mapping for vm virtual address, gva: 0x%lx\", gva);\n\texit(EXIT_FAILURE);\n}\n\nvm_paddr_t addr_arch_gva2gpa(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tuint64_t *ptep = virt_get_pte_hva(vm, gva);\n\n\treturn pte_addr(vm, *ptep) + (gva & (vm->page_size - 1));\n}\n\nstatic void pte_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent, uint64_t page, int level)\n{\n#ifdef DEBUG\n\tstatic const char * const type[] = { \"\", \"pud\", \"pmd\", \"pte\" };\n\tuint64_t pte, *ptep;\n\n\tif (level == 4)\n\t\treturn;\n\n\tfor (pte = page; pte < page + ptrs_per_pte(vm) * 8; pte += 8) {\n\t\tptep = addr_gpa2hva(vm, pte);\n\t\tif (!*ptep)\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*s%s: %lx: %lx at %p\\n\", indent, \"\", type[level], pte, *ptep, ptep);\n\t\tpte_dump(stream, vm, indent + 1, pte_addr(vm, *ptep), level + 1);\n\t}\n#endif\n}\n\nvoid virt_arch_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)\n{\n\tint level = 4 - (vm->pgtable_levels - 1);\n\tuint64_t pgd, *ptep;\n\n\tif (!vm->pgd_created)\n\t\treturn;\n\n\tfor (pgd = vm->pgd; pgd < vm->pgd + ptrs_per_pgd(vm) * 8; pgd += 8) {\n\t\tptep = addr_gpa2hva(vm, pgd);\n\t\tif (!*ptep)\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*spgd: %lx: %lx at %p\\n\", indent, \"\", pgd, *ptep, ptep);\n\t\tpte_dump(stream, vm, indent + 1, pte_addr(vm, *ptep), level);\n\t}\n}\n\nvoid aarch64_vcpu_setup(struct kvm_vcpu *vcpu, struct kvm_vcpu_init *init)\n{\n\tstruct kvm_vcpu_init default_init = { .target = -1, };\n\tstruct kvm_vm *vm = vcpu->vm;\n\tuint64_t sctlr_el1, tcr_el1, ttbr0_el1;\n\n\tif (!init)\n\t\tinit = &default_init;\n\n\tif (init->target == -1) {\n\t\tstruct kvm_vcpu_init preferred;\n\t\tvm_ioctl(vm, KVM_ARM_PREFERRED_TARGET, &preferred);\n\t\tinit->target = preferred.target;\n\t}\n\n\tvcpu_ioctl(vcpu, KVM_ARM_VCPU_INIT, init);\n\n\t \n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_CPACR_EL1), 3 << 20);\n\n\tvcpu_get_reg(vcpu, KVM_ARM64_SYS_REG(SYS_SCTLR_EL1), &sctlr_el1);\n\tvcpu_get_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TCR_EL1), &tcr_el1);\n\n\t \n\tswitch (vm->mode) {\n\tcase VM_MODE_P52V48_4K:\n\t\tTEST_FAIL(\"AArch64 does not support 4K sized pages \"\n\t\t\t  \"with 52-bit physical address ranges\");\n\tcase VM_MODE_PXXV48_4K:\n\t\tTEST_FAIL(\"AArch64 does not support 4K sized pages \"\n\t\t\t  \"with ANY-bit physical address ranges\");\n\tcase VM_MODE_P52V48_64K:\n\tcase VM_MODE_P48V48_64K:\n\tcase VM_MODE_P40V48_64K:\n\tcase VM_MODE_P36V48_64K:\n\t\ttcr_el1 |= 1ul << 14;  \n\t\tbreak;\n\tcase VM_MODE_P48V48_16K:\n\tcase VM_MODE_P40V48_16K:\n\tcase VM_MODE_P36V48_16K:\n\tcase VM_MODE_P36V47_16K:\n\t\ttcr_el1 |= 2ul << 14;  \n\t\tbreak;\n\tcase VM_MODE_P48V48_4K:\n\tcase VM_MODE_P40V48_4K:\n\tcase VM_MODE_P36V48_4K:\n\t\ttcr_el1 |= 0ul << 14;  \n\t\tbreak;\n\tdefault:\n\t\tTEST_FAIL(\"Unknown guest mode, mode: 0x%x\", vm->mode);\n\t}\n\n\tttbr0_el1 = vm->pgd & GENMASK(47, vm->page_shift);\n\n\t \n\tswitch (vm->mode) {\n\tcase VM_MODE_P52V48_64K:\n\t\ttcr_el1 |= 6ul << 32;  \n\t\tttbr0_el1 |= FIELD_GET(GENMASK(51, 48), vm->pgd) << 2;\n\t\tbreak;\n\tcase VM_MODE_P48V48_4K:\n\tcase VM_MODE_P48V48_16K:\n\tcase VM_MODE_P48V48_64K:\n\t\ttcr_el1 |= 5ul << 32;  \n\t\tbreak;\n\tcase VM_MODE_P40V48_4K:\n\tcase VM_MODE_P40V48_16K:\n\tcase VM_MODE_P40V48_64K:\n\t\ttcr_el1 |= 2ul << 32;  \n\t\tbreak;\n\tcase VM_MODE_P36V48_4K:\n\tcase VM_MODE_P36V48_16K:\n\tcase VM_MODE_P36V48_64K:\n\tcase VM_MODE_P36V47_16K:\n\t\ttcr_el1 |= 1ul << 32;  \n\t\tbreak;\n\tdefault:\n\t\tTEST_FAIL(\"Unknown guest mode, mode: 0x%x\", vm->mode);\n\t}\n\n\tsctlr_el1 |= (1 << 0) | (1 << 2) | (1 << 12)  ;\n\t ;\n\ttcr_el1 |= (1 << 8) | (1 << 10) | (3 << 12);\n\ttcr_el1 |= (64 - vm->va_bits)  ;\n\n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_SCTLR_EL1), sctlr_el1);\n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TCR_EL1), tcr_el1);\n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_MAIR_EL1), DEFAULT_MAIR_EL1);\n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TTBR0_EL1), ttbr0_el1);\n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_TPIDR_EL1), vcpu->id);\n}\n\nvoid vcpu_arch_dump(FILE *stream, struct kvm_vcpu *vcpu, uint8_t indent)\n{\n\tuint64_t pstate, pc;\n\n\tvcpu_get_reg(vcpu, ARM64_CORE_REG(regs.pstate), &pstate);\n\tvcpu_get_reg(vcpu, ARM64_CORE_REG(regs.pc), &pc);\n\n\tfprintf(stream, \"%*spstate: 0x%.16lx pc: 0x%.16lx\\n\",\n\t\tindent, \"\", pstate, pc);\n}\n\nstruct kvm_vcpu *aarch64_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,\n\t\t\t\t  struct kvm_vcpu_init *init, void *guest_code)\n{\n\tsize_t stack_size;\n\tuint64_t stack_vaddr;\n\tstruct kvm_vcpu *vcpu = __vm_vcpu_add(vm, vcpu_id);\n\n\tstack_size = vm->page_size == 4096 ? DEFAULT_STACK_PGS * vm->page_size :\n\t\t\t\t\t     vm->page_size;\n\tstack_vaddr = __vm_vaddr_alloc(vm, stack_size,\n\t\t\t\t       DEFAULT_ARM64_GUEST_STACK_VADDR_MIN,\n\t\t\t\t       MEM_REGION_DATA);\n\n\taarch64_vcpu_setup(vcpu, init);\n\n\tvcpu_set_reg(vcpu, ARM64_CORE_REG(sp_el1), stack_vaddr + stack_size);\n\tvcpu_set_reg(vcpu, ARM64_CORE_REG(regs.pc), (uint64_t)guest_code);\n\n\treturn vcpu;\n}\n\nstruct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,\n\t\t\t\t  void *guest_code)\n{\n\treturn aarch64_vcpu_add(vm, vcpu_id, NULL, guest_code);\n}\n\nvoid vcpu_args_set(struct kvm_vcpu *vcpu, unsigned int num, ...)\n{\n\tva_list ap;\n\tint i;\n\n\tTEST_ASSERT(num >= 1 && num <= 8, \"Unsupported number of args,\\n\"\n\t\t    \"  num: %u\\n\", num);\n\n\tva_start(ap, num);\n\n\tfor (i = 0; i < num; i++) {\n\t\tvcpu_set_reg(vcpu, ARM64_CORE_REG(regs.regs[i]),\n\t\t\t     va_arg(ap, uint64_t));\n\t}\n\n\tva_end(ap);\n}\n\nvoid kvm_exit_unexpected_exception(int vector, uint64_t ec, bool valid_ec)\n{\n\tucall(UCALL_UNHANDLED, 3, vector, ec, valid_ec);\n\twhile (1)\n\t\t;\n}\n\nvoid assert_on_unhandled_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct ucall uc;\n\n\tif (get_ucall(vcpu, &uc) != UCALL_UNHANDLED)\n\t\treturn;\n\n\tif (uc.args[2])   {\n\t\tassert(VECTOR_IS_SYNC(uc.args[0]));\n\t\tTEST_FAIL(\"Unexpected exception (vector:0x%lx, ec:0x%lx)\",\n\t\t\t  uc.args[0], uc.args[1]);\n\t} else {\n\t\tassert(!VECTOR_IS_SYNC(uc.args[0]));\n\t\tTEST_FAIL(\"Unexpected exception (vector:0x%lx)\",\n\t\t\t  uc.args[0]);\n\t}\n}\n\nstruct handlers {\n\thandler_fn exception_handlers[VECTOR_NUM][ESR_EC_NUM];\n};\n\nvoid vcpu_init_descriptor_tables(struct kvm_vcpu *vcpu)\n{\n\textern char vectors;\n\n\tvcpu_set_reg(vcpu, KVM_ARM64_SYS_REG(SYS_VBAR_EL1), (uint64_t)&vectors);\n}\n\nvoid route_exception(struct ex_regs *regs, int vector)\n{\n\tstruct handlers *handlers = (struct handlers *)exception_handlers;\n\tbool valid_ec;\n\tint ec = 0;\n\n\tswitch (vector) {\n\tcase VECTOR_SYNC_CURRENT:\n\tcase VECTOR_SYNC_LOWER_64:\n\t\tec = (read_sysreg(esr_el1) >> ESR_EC_SHIFT) & ESR_EC_MASK;\n\t\tvalid_ec = true;\n\t\tbreak;\n\tcase VECTOR_IRQ_CURRENT:\n\tcase VECTOR_IRQ_LOWER_64:\n\tcase VECTOR_FIQ_CURRENT:\n\tcase VECTOR_FIQ_LOWER_64:\n\tcase VECTOR_ERROR_CURRENT:\n\tcase VECTOR_ERROR_LOWER_64:\n\t\tec = 0;\n\t\tvalid_ec = false;\n\t\tbreak;\n\tdefault:\n\t\tvalid_ec = false;\n\t\tgoto unexpected_exception;\n\t}\n\n\tif (handlers && handlers->exception_handlers[vector][ec])\n\t\treturn handlers->exception_handlers[vector][ec](regs);\n\nunexpected_exception:\n\tkvm_exit_unexpected_exception(vector, ec, valid_ec);\n}\n\nvoid vm_init_descriptor_tables(struct kvm_vm *vm)\n{\n\tvm->handlers = __vm_vaddr_alloc(vm, sizeof(struct handlers),\n\t\t\t\t\tvm->page_size, MEM_REGION_DATA);\n\n\t*(vm_vaddr_t *)addr_gva2hva(vm, (vm_vaddr_t)(&exception_handlers)) = vm->handlers;\n}\n\nvoid vm_install_sync_handler(struct kvm_vm *vm, int vector, int ec,\n\t\t\t void (*handler)(struct ex_regs *))\n{\n\tstruct handlers *handlers = addr_gva2hva(vm, vm->handlers);\n\n\tassert(VECTOR_IS_SYNC(vector));\n\tassert(vector < VECTOR_NUM);\n\tassert(ec < ESR_EC_NUM);\n\thandlers->exception_handlers[vector][ec] = handler;\n}\n\nvoid vm_install_exception_handler(struct kvm_vm *vm, int vector,\n\t\t\t void (*handler)(struct ex_regs *))\n{\n\tstruct handlers *handlers = addr_gva2hva(vm, vm->handlers);\n\n\tassert(!VECTOR_IS_SYNC(vector));\n\tassert(vector < VECTOR_NUM);\n\thandlers->exception_handlers[vector][0] = handler;\n}\n\nuint32_t guest_get_vcpuid(void)\n{\n\treturn read_sysreg(tpidr_el1);\n}\n\nvoid aarch64_get_supported_page_sizes(uint32_t ipa,\n\t\t\t\t      bool *ps4k, bool *ps16k, bool *ps64k)\n{\n\tstruct kvm_vcpu_init preferred_init;\n\tint kvm_fd, vm_fd, vcpu_fd, err;\n\tuint64_t val;\n\tstruct kvm_one_reg reg = {\n\t\t.id\t= KVM_ARM64_SYS_REG(SYS_ID_AA64MMFR0_EL1),\n\t\t.addr\t= (uint64_t)&val,\n\t};\n\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\tvm_fd = __kvm_ioctl(kvm_fd, KVM_CREATE_VM, (void *)(unsigned long)ipa);\n\tTEST_ASSERT(vm_fd >= 0, KVM_IOCTL_ERROR(KVM_CREATE_VM, vm_fd));\n\n\tvcpu_fd = ioctl(vm_fd, KVM_CREATE_VCPU, 0);\n\tTEST_ASSERT(vcpu_fd >= 0, KVM_IOCTL_ERROR(KVM_CREATE_VCPU, vcpu_fd));\n\n\terr = ioctl(vm_fd, KVM_ARM_PREFERRED_TARGET, &preferred_init);\n\tTEST_ASSERT(err == 0, KVM_IOCTL_ERROR(KVM_ARM_PREFERRED_TARGET, err));\n\terr = ioctl(vcpu_fd, KVM_ARM_VCPU_INIT, &preferred_init);\n\tTEST_ASSERT(err == 0, KVM_IOCTL_ERROR(KVM_ARM_VCPU_INIT, err));\n\n\terr = ioctl(vcpu_fd, KVM_GET_ONE_REG, &reg);\n\tTEST_ASSERT(err == 0, KVM_IOCTL_ERROR(KVM_GET_ONE_REG, vcpu_fd));\n\n\t*ps4k = FIELD_GET(ARM64_FEATURE_MASK(ID_AA64MMFR0_TGRAN4), val) != 0xf;\n\t*ps64k = FIELD_GET(ARM64_FEATURE_MASK(ID_AA64MMFR0_TGRAN64), val) == 0;\n\t*ps16k = FIELD_GET(ARM64_FEATURE_MASK(ID_AA64MMFR0_TGRAN16), val) != 0;\n\n\tclose(vcpu_fd);\n\tclose(vm_fd);\n\tclose(kvm_fd);\n}\n\n#define __smccc_call(insn, function_id, arg0, arg1, arg2, arg3, arg4, arg5,\t\\\n\t\t     arg6, res)\t\t\t\t\t\t\t\\\n\tasm volatile(\"mov   w0, %w[function_id]\\n\"\t\t\t\t\\\n\t\t     \"mov   x1, %[arg0]\\n\"\t\t\t\t\t\\\n\t\t     \"mov   x2, %[arg1]\\n\"\t\t\t\t\t\\\n\t\t     \"mov   x3, %[arg2]\\n\"\t\t\t\t\t\\\n\t\t     \"mov   x4, %[arg3]\\n\"\t\t\t\t\t\\\n\t\t     \"mov   x5, %[arg4]\\n\"\t\t\t\t\t\\\n\t\t     \"mov   x6, %[arg5]\\n\"\t\t\t\t\t\\\n\t\t     \"mov   x7, %[arg6]\\n\"\t\t\t\t\t\\\n\t\t     #insn  \"#0\\n\"\t\t\t\t\t\t\\\n\t\t     \"mov   %[res0], x0\\n\"\t\t\t\t\t\\\n\t\t     \"mov   %[res1], x1\\n\"\t\t\t\t\t\\\n\t\t     \"mov   %[res2], x2\\n\"\t\t\t\t\t\\\n\t\t     \"mov   %[res3], x3\\n\"\t\t\t\t\t\\\n\t\t     : [res0] \"=r\"(res->a0), [res1] \"=r\"(res->a1),\t\t\\\n\t\t       [res2] \"=r\"(res->a2), [res3] \"=r\"(res->a3)\t\t\\\n\t\t     : [function_id] \"r\"(function_id), [arg0] \"r\"(arg0),\t\\\n\t\t       [arg1] \"r\"(arg1), [arg2] \"r\"(arg2), [arg3] \"r\"(arg3),\t\\\n\t\t       [arg4] \"r\"(arg4), [arg5] \"r\"(arg5), [arg6] \"r\"(arg6)\t\\\n\t\t     : \"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\")\n\n\nvoid smccc_hvc(uint32_t function_id, uint64_t arg0, uint64_t arg1,\n\t       uint64_t arg2, uint64_t arg3, uint64_t arg4, uint64_t arg5,\n\t       uint64_t arg6, struct arm_smccc_res *res)\n{\n\t__smccc_call(hvc, function_id, arg0, arg1, arg2, arg3, arg4, arg5,\n\t\t     arg6, res);\n}\n\nvoid smccc_smc(uint32_t function_id, uint64_t arg0, uint64_t arg1,\n\t       uint64_t arg2, uint64_t arg3, uint64_t arg4, uint64_t arg5,\n\t       uint64_t arg6, struct arm_smccc_res *res)\n{\n\t__smccc_call(smc, function_id, arg0, arg1, arg2, arg3, arg4, arg5,\n\t\t     arg6, res);\n}\n\nvoid kvm_selftest_arch_init(void)\n{\n\t \n\tguest_modes_append_default();\n}\n\nvoid vm_vaddr_populate_bitmap(struct kvm_vm *vm)\n{\n\t \n\tsparsebit_set_num(vm->vpages_valid, 0,\n\t\t\t  (1ULL << vm->va_bits) >> vm->page_shift);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}