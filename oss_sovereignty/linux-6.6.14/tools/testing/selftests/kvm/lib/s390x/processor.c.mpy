{
  "module_name": "processor.c",
  "hash_id": "63b3e769e279191eb1a7278e2c7e22c9f6a52b9f1d098299f2791e16b6e3f85e",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/s390x/processor.c",
  "human_readable_source": "\n \n\n#include \"processor.h\"\n#include \"kvm_util.h\"\n\n#define PAGES_PER_REGION 4\n\nvoid virt_arch_pgd_alloc(struct kvm_vm *vm)\n{\n\tvm_paddr_t paddr;\n\n\tTEST_ASSERT(vm->page_size == 4096, \"Unsupported page size: 0x%x\",\n\t\t    vm->page_size);\n\n\tif (vm->pgd_created)\n\t\treturn;\n\n\tpaddr = vm_phy_pages_alloc(vm, PAGES_PER_REGION,\n\t\t\t\t   KVM_GUEST_PAGE_TABLE_MIN_PADDR,\n\t\t\t\t   vm->memslots[MEM_REGION_PT]);\n\tmemset(addr_gpa2hva(vm, paddr), 0xff, PAGES_PER_REGION * vm->page_size);\n\n\tvm->pgd = paddr;\n\tvm->pgd_created = true;\n}\n\n \nstatic uint64_t virt_alloc_region(struct kvm_vm *vm, int ri)\n{\n\tuint64_t taddr;\n\n\ttaddr = vm_phy_pages_alloc(vm,  ri < 4 ? PAGES_PER_REGION : 1,\n\t\t\t\t   KVM_GUEST_PAGE_TABLE_MIN_PADDR, 0);\n\tmemset(addr_gpa2hva(vm, taddr), 0xff, PAGES_PER_REGION * vm->page_size);\n\n\treturn (taddr & REGION_ENTRY_ORIGIN)\n\t\t| (((4 - ri) << 2) & REGION_ENTRY_TYPE)\n\t\t| ((ri < 4 ? (PAGES_PER_REGION - 1) : 0) & REGION_ENTRY_LENGTH);\n}\n\nvoid virt_arch_pg_map(struct kvm_vm *vm, uint64_t gva, uint64_t gpa)\n{\n\tint ri, idx;\n\tuint64_t *entry;\n\n\tTEST_ASSERT((gva % vm->page_size) == 0,\n\t\t\"Virtual address not on page boundary,\\n\"\n\t\t\"  vaddr: 0x%lx vm->page_size: 0x%x\",\n\t\tgva, vm->page_size);\n\tTEST_ASSERT(sparsebit_is_set(vm->vpages_valid,\n\t\t(gva >> vm->page_shift)),\n\t\t\"Invalid virtual address, vaddr: 0x%lx\",\n\t\tgva);\n\tTEST_ASSERT((gpa % vm->page_size) == 0,\n\t\t\"Physical address not on page boundary,\\n\"\n\t\t\"  paddr: 0x%lx vm->page_size: 0x%x\",\n\t\tgva, vm->page_size);\n\tTEST_ASSERT((gpa >> vm->page_shift) <= vm->max_gfn,\n\t\t\"Physical address beyond beyond maximum supported,\\n\"\n\t\t\"  paddr: 0x%lx vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\tgva, vm->max_gfn, vm->page_size);\n\n\t \n\tentry = addr_gpa2hva(vm, vm->pgd);\n\tfor (ri = 1; ri <= 4; ri++) {\n\t\tidx = (gva >> (64 - 11 * ri)) & 0x7ffu;\n\t\tif (entry[idx] & REGION_ENTRY_INVALID)\n\t\t\tentry[idx] = virt_alloc_region(vm, ri);\n\t\tentry = addr_gpa2hva(vm, entry[idx] & REGION_ENTRY_ORIGIN);\n\t}\n\n\t \n\tidx = (gva >> 12) & 0x0ffu;\t\t \n\tif (!(entry[idx] & PAGE_INVALID))\n\t\tfprintf(stderr,\n\t\t\t\"WARNING: PTE for gpa=0x%\"PRIx64\" already set!\\n\", gpa);\n\tentry[idx] = gpa;\n}\n\nvm_paddr_t addr_arch_gva2gpa(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\tint ri, idx;\n\tuint64_t *entry;\n\n\tTEST_ASSERT(vm->page_size == 4096, \"Unsupported page size: 0x%x\",\n\t\t    vm->page_size);\n\n\tentry = addr_gpa2hva(vm, vm->pgd);\n\tfor (ri = 1; ri <= 4; ri++) {\n\t\tidx = (gva >> (64 - 11 * ri)) & 0x7ffu;\n\t\tTEST_ASSERT(!(entry[idx] & REGION_ENTRY_INVALID),\n\t\t\t    \"No region mapping for vm virtual address 0x%lx\",\n\t\t\t    gva);\n\t\tentry = addr_gpa2hva(vm, entry[idx] & REGION_ENTRY_ORIGIN);\n\t}\n\n\tidx = (gva >> 12) & 0x0ffu;\t\t \n\n\tTEST_ASSERT(!(entry[idx] & PAGE_INVALID),\n\t\t    \"No page mapping for vm virtual address 0x%lx\", gva);\n\n\treturn (entry[idx] & ~0xffful) + (gva & 0xffful);\n}\n\nstatic void virt_dump_ptes(FILE *stream, struct kvm_vm *vm, uint8_t indent,\n\t\t\t   uint64_t ptea_start)\n{\n\tuint64_t *pte, ptea;\n\n\tfor (ptea = ptea_start; ptea < ptea_start + 0x100 * 8; ptea += 8) {\n\t\tpte = addr_gpa2hva(vm, ptea);\n\t\tif (*pte & PAGE_INVALID)\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*spte @ 0x%lx: 0x%016lx\\n\",\n\t\t\tindent, \"\", ptea, *pte);\n\t}\n}\n\nstatic void virt_dump_region(FILE *stream, struct kvm_vm *vm, uint8_t indent,\n\t\t\t     uint64_t reg_tab_addr)\n{\n\tuint64_t addr, *entry;\n\n\tfor (addr = reg_tab_addr; addr < reg_tab_addr + 0x400 * 8; addr += 8) {\n\t\tentry = addr_gpa2hva(vm, addr);\n\t\tif (*entry & REGION_ENTRY_INVALID)\n\t\t\tcontinue;\n\t\tfprintf(stream, \"%*srt%lde @ 0x%lx: 0x%016lx\\n\",\n\t\t\tindent, \"\", 4 - ((*entry & REGION_ENTRY_TYPE) >> 2),\n\t\t\taddr, *entry);\n\t\tif (*entry & REGION_ENTRY_TYPE) {\n\t\t\tvirt_dump_region(stream, vm, indent + 2,\n\t\t\t\t\t *entry & REGION_ENTRY_ORIGIN);\n\t\t} else {\n\t\t\tvirt_dump_ptes(stream, vm, indent + 2,\n\t\t\t\t       *entry & REGION_ENTRY_ORIGIN);\n\t\t}\n\t}\n}\n\nvoid virt_arch_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)\n{\n\tif (!vm->pgd_created)\n\t\treturn;\n\n\tvirt_dump_region(stream, vm, indent, vm->pgd);\n}\n\nstruct kvm_vcpu *vm_arch_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id,\n\t\t\t\t  void *guest_code)\n{\n\tsize_t stack_size =  DEFAULT_STACK_PGS * getpagesize();\n\tuint64_t stack_vaddr;\n\tstruct kvm_regs regs;\n\tstruct kvm_sregs sregs;\n\tstruct kvm_vcpu *vcpu;\n\tstruct kvm_run *run;\n\n\tTEST_ASSERT(vm->page_size == 4096, \"Unsupported page size: 0x%x\",\n\t\t    vm->page_size);\n\n\tstack_vaddr = __vm_vaddr_alloc(vm, stack_size,\n\t\t\t\t       DEFAULT_GUEST_STACK_VADDR_MIN,\n\t\t\t\t       MEM_REGION_DATA);\n\n\tvcpu = __vm_vcpu_add(vm, vcpu_id);\n\n\t \n\tvcpu_regs_get(vcpu, &regs);\n\tregs.gprs[15] = stack_vaddr + (DEFAULT_STACK_PGS * getpagesize()) - 160;\n\tvcpu_regs_set(vcpu, &regs);\n\n\tvcpu_sregs_get(vcpu, &sregs);\n\tsregs.crs[0] |= 0x00040000;\t\t \n\tsregs.crs[1] = vm->pgd | 0xf;\t\t \n\tvcpu_sregs_set(vcpu, &sregs);\n\n\trun = vcpu->run;\n\trun->psw_mask = 0x0400000180000000ULL;   \n\trun->psw_addr = (uintptr_t)guest_code;\n\n\treturn vcpu;\n}\n\nvoid vcpu_args_set(struct kvm_vcpu *vcpu, unsigned int num, ...)\n{\n\tva_list ap;\n\tstruct kvm_regs regs;\n\tint i;\n\n\tTEST_ASSERT(num >= 1 && num <= 5, \"Unsupported number of args,\\n\"\n\t\t    \"  num: %u\\n\",\n\t\t    num);\n\n\tva_start(ap, num);\n\tvcpu_regs_get(vcpu, &regs);\n\n\tfor (i = 0; i < num; i++)\n\t\tregs.gprs[i + 2] = va_arg(ap, uint64_t);\n\n\tvcpu_regs_set(vcpu, &regs);\n\tva_end(ap);\n}\n\nvoid vcpu_arch_dump(FILE *stream, struct kvm_vcpu *vcpu, uint8_t indent)\n{\n\tfprintf(stream, \"%*spstate: psw: 0x%.16llx:0x%.16llx\\n\",\n\t\tindent, \"\", vcpu->run->psw_mask, vcpu->run->psw_addr);\n}\n\nvoid assert_on_unhandled_exception(struct kvm_vcpu *vcpu)\n{\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}