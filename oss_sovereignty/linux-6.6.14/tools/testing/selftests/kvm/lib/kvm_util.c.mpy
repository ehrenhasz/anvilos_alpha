{
  "module_name": "kvm_util.c",
  "hash_id": "61ec8fc99dc59db9909f5e2961015175703165bbd4cd7439090ae0696cd08325",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/lib/kvm_util.c",
  "human_readable_source": "\n \n\n#define _GNU_SOURCE  \n#include \"test_util.h\"\n#include \"kvm_util.h\"\n#include \"processor.h\"\n\n#include <assert.h>\n#include <sched.h>\n#include <sys/mman.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n#include <linux/kernel.h>\n\n#define KVM_UTIL_MIN_PFN\t2\n\nstatic int vcpu_mmap_sz(void);\n\nint open_path_or_exit(const char *path, int flags)\n{\n\tint fd;\n\n\tfd = open(path, flags);\n\t__TEST_REQUIRE(fd >= 0, \"%s not available (errno: %d)\", path, errno);\n\n\treturn fd;\n}\n\n \nstatic int _open_kvm_dev_path_or_exit(int flags)\n{\n\treturn open_path_or_exit(KVM_DEV_PATH, flags);\n}\n\nint open_kvm_dev_path_or_exit(void)\n{\n\treturn _open_kvm_dev_path_or_exit(O_RDONLY);\n}\n\nstatic bool get_module_param_bool(const char *module_name, const char *param)\n{\n\tconst int path_size = 128;\n\tchar path[path_size];\n\tchar value;\n\tssize_t r;\n\tint fd;\n\n\tr = snprintf(path, path_size, \"/sys/module/%s/parameters/%s\",\n\t\t     module_name, param);\n\tTEST_ASSERT(r < path_size,\n\t\t    \"Failed to construct sysfs path in %d bytes.\", path_size);\n\n\tfd = open_path_or_exit(path, O_RDONLY);\n\n\tr = read(fd, &value, 1);\n\tTEST_ASSERT(r == 1, \"read(%s) failed\", path);\n\n\tr = close(fd);\n\tTEST_ASSERT(!r, \"close(%s) failed\", path);\n\n\tif (value == 'Y')\n\t\treturn true;\n\telse if (value == 'N')\n\t\treturn false;\n\n\tTEST_FAIL(\"Unrecognized value '%c' for boolean module param\", value);\n}\n\nbool get_kvm_param_bool(const char *param)\n{\n\treturn get_module_param_bool(\"kvm\", param);\n}\n\nbool get_kvm_intel_param_bool(const char *param)\n{\n\treturn get_module_param_bool(\"kvm_intel\", param);\n}\n\nbool get_kvm_amd_param_bool(const char *param)\n{\n\treturn get_module_param_bool(\"kvm_amd\", param);\n}\n\n \nunsigned int kvm_check_cap(long cap)\n{\n\tint ret;\n\tint kvm_fd;\n\n\tkvm_fd = open_kvm_dev_path_or_exit();\n\tret = __kvm_ioctl(kvm_fd, KVM_CHECK_EXTENSION, (void *)cap);\n\tTEST_ASSERT(ret >= 0, KVM_IOCTL_ERROR(KVM_CHECK_EXTENSION, ret));\n\n\tclose(kvm_fd);\n\n\treturn (unsigned int)ret;\n}\n\nvoid vm_enable_dirty_ring(struct kvm_vm *vm, uint32_t ring_size)\n{\n\tif (vm_check_cap(vm, KVM_CAP_DIRTY_LOG_RING_ACQ_REL))\n\t\tvm_enable_cap(vm, KVM_CAP_DIRTY_LOG_RING_ACQ_REL, ring_size);\n\telse\n\t\tvm_enable_cap(vm, KVM_CAP_DIRTY_LOG_RING, ring_size);\n\tvm->dirty_ring_size = ring_size;\n}\n\nstatic void vm_open(struct kvm_vm *vm)\n{\n\tvm->kvm_fd = _open_kvm_dev_path_or_exit(O_RDWR);\n\n\tTEST_REQUIRE(kvm_has_cap(KVM_CAP_IMMEDIATE_EXIT));\n\n\tvm->fd = __kvm_ioctl(vm->kvm_fd, KVM_CREATE_VM, (void *)vm->type);\n\tTEST_ASSERT(vm->fd >= 0, KVM_IOCTL_ERROR(KVM_CREATE_VM, vm->fd));\n}\n\nconst char *vm_guest_mode_string(uint32_t i)\n{\n\tstatic const char * const strings[] = {\n\t\t[VM_MODE_P52V48_4K]\t= \"PA-bits:52,  VA-bits:48,  4K pages\",\n\t\t[VM_MODE_P52V48_64K]\t= \"PA-bits:52,  VA-bits:48, 64K pages\",\n\t\t[VM_MODE_P48V48_4K]\t= \"PA-bits:48,  VA-bits:48,  4K pages\",\n\t\t[VM_MODE_P48V48_16K]\t= \"PA-bits:48,  VA-bits:48, 16K pages\",\n\t\t[VM_MODE_P48V48_64K]\t= \"PA-bits:48,  VA-bits:48, 64K pages\",\n\t\t[VM_MODE_P40V48_4K]\t= \"PA-bits:40,  VA-bits:48,  4K pages\",\n\t\t[VM_MODE_P40V48_16K]\t= \"PA-bits:40,  VA-bits:48, 16K pages\",\n\t\t[VM_MODE_P40V48_64K]\t= \"PA-bits:40,  VA-bits:48, 64K pages\",\n\t\t[VM_MODE_PXXV48_4K]\t= \"PA-bits:ANY, VA-bits:48,  4K pages\",\n\t\t[VM_MODE_P47V64_4K]\t= \"PA-bits:47,  VA-bits:64,  4K pages\",\n\t\t[VM_MODE_P44V64_4K]\t= \"PA-bits:44,  VA-bits:64,  4K pages\",\n\t\t[VM_MODE_P36V48_4K]\t= \"PA-bits:36,  VA-bits:48,  4K pages\",\n\t\t[VM_MODE_P36V48_16K]\t= \"PA-bits:36,  VA-bits:48, 16K pages\",\n\t\t[VM_MODE_P36V48_64K]\t= \"PA-bits:36,  VA-bits:48, 64K pages\",\n\t\t[VM_MODE_P36V47_16K]\t= \"PA-bits:36,  VA-bits:47, 16K pages\",\n\t};\n\t_Static_assert(sizeof(strings)/sizeof(char *) == NUM_VM_MODES,\n\t\t       \"Missing new mode strings?\");\n\n\tTEST_ASSERT(i < NUM_VM_MODES, \"Guest mode ID %d too big\", i);\n\n\treturn strings[i];\n}\n\nconst struct vm_guest_mode_params vm_guest_mode_params[] = {\n\t[VM_MODE_P52V48_4K]\t= { 52, 48,  0x1000, 12 },\n\t[VM_MODE_P52V48_64K]\t= { 52, 48, 0x10000, 16 },\n\t[VM_MODE_P48V48_4K]\t= { 48, 48,  0x1000, 12 },\n\t[VM_MODE_P48V48_16K]\t= { 48, 48,  0x4000, 14 },\n\t[VM_MODE_P48V48_64K]\t= { 48, 48, 0x10000, 16 },\n\t[VM_MODE_P40V48_4K]\t= { 40, 48,  0x1000, 12 },\n\t[VM_MODE_P40V48_16K]\t= { 40, 48,  0x4000, 14 },\n\t[VM_MODE_P40V48_64K]\t= { 40, 48, 0x10000, 16 },\n\t[VM_MODE_PXXV48_4K]\t= {  0,  0,  0x1000, 12 },\n\t[VM_MODE_P47V64_4K]\t= { 47, 64,  0x1000, 12 },\n\t[VM_MODE_P44V64_4K]\t= { 44, 64,  0x1000, 12 },\n\t[VM_MODE_P36V48_4K]\t= { 36, 48,  0x1000, 12 },\n\t[VM_MODE_P36V48_16K]\t= { 36, 48,  0x4000, 14 },\n\t[VM_MODE_P36V48_64K]\t= { 36, 48, 0x10000, 16 },\n\t[VM_MODE_P36V47_16K]\t= { 36, 47,  0x4000, 14 },\n};\n_Static_assert(sizeof(vm_guest_mode_params)/sizeof(struct vm_guest_mode_params) == NUM_VM_MODES,\n\t       \"Missing new mode params?\");\n\n \n__weak void vm_vaddr_populate_bitmap(struct kvm_vm *vm)\n{\n\tsparsebit_set_num(vm->vpages_valid,\n\t\t0, (1ULL << (vm->va_bits - 1)) >> vm->page_shift);\n\tsparsebit_set_num(vm->vpages_valid,\n\t\t(~((1ULL << (vm->va_bits - 1)) - 1)) >> vm->page_shift,\n\t\t(1ULL << (vm->va_bits - 1)) >> vm->page_shift);\n}\n\nstruct kvm_vm *____vm_create(enum vm_guest_mode mode)\n{\n\tstruct kvm_vm *vm;\n\n\tvm = calloc(1, sizeof(*vm));\n\tTEST_ASSERT(vm != NULL, \"Insufficient Memory\");\n\n\tINIT_LIST_HEAD(&vm->vcpus);\n\tvm->regions.gpa_tree = RB_ROOT;\n\tvm->regions.hva_tree = RB_ROOT;\n\thash_init(vm->regions.slot_hash);\n\n\tvm->mode = mode;\n\tvm->type = 0;\n\n\tvm->pa_bits = vm_guest_mode_params[mode].pa_bits;\n\tvm->va_bits = vm_guest_mode_params[mode].va_bits;\n\tvm->page_size = vm_guest_mode_params[mode].page_size;\n\tvm->page_shift = vm_guest_mode_params[mode].page_shift;\n\n\t \n\tswitch (vm->mode) {\n\tcase VM_MODE_P52V48_4K:\n\t\tvm->pgtable_levels = 4;\n\t\tbreak;\n\tcase VM_MODE_P52V48_64K:\n\t\tvm->pgtable_levels = 3;\n\t\tbreak;\n\tcase VM_MODE_P48V48_4K:\n\t\tvm->pgtable_levels = 4;\n\t\tbreak;\n\tcase VM_MODE_P48V48_64K:\n\t\tvm->pgtable_levels = 3;\n\t\tbreak;\n\tcase VM_MODE_P40V48_4K:\n\tcase VM_MODE_P36V48_4K:\n\t\tvm->pgtable_levels = 4;\n\t\tbreak;\n\tcase VM_MODE_P40V48_64K:\n\tcase VM_MODE_P36V48_64K:\n\t\tvm->pgtable_levels = 3;\n\t\tbreak;\n\tcase VM_MODE_P48V48_16K:\n\tcase VM_MODE_P40V48_16K:\n\tcase VM_MODE_P36V48_16K:\n\t\tvm->pgtable_levels = 4;\n\t\tbreak;\n\tcase VM_MODE_P36V47_16K:\n\t\tvm->pgtable_levels = 3;\n\t\tbreak;\n\tcase VM_MODE_PXXV48_4K:\n#ifdef __x86_64__\n\t\tkvm_get_cpu_address_width(&vm->pa_bits, &vm->va_bits);\n\t\t \n\t\tTEST_ASSERT(vm->va_bits == 48 || vm->va_bits == 57,\n\t\t\t    \"Linear address width (%d bits) not supported\",\n\t\t\t    vm->va_bits);\n\t\tpr_debug(\"Guest physical address width detected: %d\\n\",\n\t\t\t vm->pa_bits);\n\t\tvm->pgtable_levels = 4;\n\t\tvm->va_bits = 48;\n#else\n\t\tTEST_FAIL(\"VM_MODE_PXXV48_4K not supported on non-x86 platforms\");\n#endif\n\t\tbreak;\n\tcase VM_MODE_P47V64_4K:\n\t\tvm->pgtable_levels = 5;\n\t\tbreak;\n\tcase VM_MODE_P44V64_4K:\n\t\tvm->pgtable_levels = 5;\n\t\tbreak;\n\tdefault:\n\t\tTEST_FAIL(\"Unknown guest mode, mode: 0x%x\", mode);\n\t}\n\n#ifdef __aarch64__\n\tif (vm->pa_bits != 40)\n\t\tvm->type = KVM_VM_TYPE_ARM_IPA_SIZE(vm->pa_bits);\n#endif\n\n\tvm_open(vm);\n\n\t \n\tvm->vpages_valid = sparsebit_alloc();\n\tvm_vaddr_populate_bitmap(vm);\n\n\t \n\tvm->max_gfn = vm_compute_max_gfn(vm);\n\n\t \n\tvm->vpages_mapped = sparsebit_alloc();\n\n\treturn vm;\n}\n\nstatic uint64_t vm_nr_pages_required(enum vm_guest_mode mode,\n\t\t\t\t     uint32_t nr_runnable_vcpus,\n\t\t\t\t     uint64_t extra_mem_pages)\n{\n\tuint64_t page_size = vm_guest_mode_params[mode].page_size;\n\tuint64_t nr_pages;\n\n\tTEST_ASSERT(nr_runnable_vcpus,\n\t\t    \"Use vm_create_barebones() for VMs that _never_ have vCPUs\\n\");\n\n\tTEST_ASSERT(nr_runnable_vcpus <= kvm_check_cap(KVM_CAP_MAX_VCPUS),\n\t\t    \"nr_vcpus = %d too large for host, max-vcpus = %d\",\n\t\t    nr_runnable_vcpus, kvm_check_cap(KVM_CAP_MAX_VCPUS));\n\n\t \n\tnr_pages = 512;\n\n\t \n\tnr_pages += nr_runnable_vcpus * DEFAULT_STACK_PGS;\n\n\t \n\tnr_pages += (nr_pages + extra_mem_pages) / PTES_PER_MIN_PAGE * 2;\n\n\t \n\tnr_pages += ucall_nr_pages_required(page_size);\n\n\treturn vm_adjust_num_guest_pages(mode, nr_pages);\n}\n\nstruct kvm_vm *__vm_create(enum vm_guest_mode mode, uint32_t nr_runnable_vcpus,\n\t\t\t   uint64_t nr_extra_pages)\n{\n\tuint64_t nr_pages = vm_nr_pages_required(mode, nr_runnable_vcpus,\n\t\t\t\t\t\t nr_extra_pages);\n\tstruct userspace_mem_region *slot0;\n\tstruct kvm_vm *vm;\n\tint i;\n\n\tpr_debug(\"%s: mode='%s' pages='%ld'\\n\", __func__,\n\t\t vm_guest_mode_string(mode), nr_pages);\n\n\tvm = ____vm_create(mode);\n\n\tvm_userspace_mem_region_add(vm, VM_MEM_SRC_ANONYMOUS, 0, 0, nr_pages, 0);\n\tfor (i = 0; i < NR_MEM_REGIONS; i++)\n\t\tvm->memslots[i] = 0;\n\n\tkvm_vm_elf_load(vm, program_invocation_name);\n\n\t \n\tslot0 = memslot2region(vm, 0);\n\tucall_init(vm, slot0->region.guest_phys_addr + slot0->region.memory_size);\n\n\tkvm_arch_vm_post_create(vm);\n\n\treturn vm;\n}\n\n \nstruct kvm_vm *__vm_create_with_vcpus(enum vm_guest_mode mode, uint32_t nr_vcpus,\n\t\t\t\t      uint64_t extra_mem_pages,\n\t\t\t\t      void *guest_code, struct kvm_vcpu *vcpus[])\n{\n\tstruct kvm_vm *vm;\n\tint i;\n\n\tTEST_ASSERT(!nr_vcpus || vcpus, \"Must provide vCPU array\");\n\n\tvm = __vm_create(mode, nr_vcpus, extra_mem_pages);\n\n\tfor (i = 0; i < nr_vcpus; ++i)\n\t\tvcpus[i] = vm_vcpu_add(vm, i, guest_code);\n\n\treturn vm;\n}\n\nstruct kvm_vm *__vm_create_with_one_vcpu(struct kvm_vcpu **vcpu,\n\t\t\t\t\t uint64_t extra_mem_pages,\n\t\t\t\t\t void *guest_code)\n{\n\tstruct kvm_vcpu *vcpus[1];\n\tstruct kvm_vm *vm;\n\n\tvm = __vm_create_with_vcpus(VM_MODE_DEFAULT, 1, extra_mem_pages,\n\t\t\t\t    guest_code, vcpus);\n\n\t*vcpu = vcpus[0];\n\treturn vm;\n}\n\n \nvoid kvm_vm_restart(struct kvm_vm *vmp)\n{\n\tint ctr;\n\tstruct userspace_mem_region *region;\n\n\tvm_open(vmp);\n\tif (vmp->has_irqchip)\n\t\tvm_create_irqchip(vmp);\n\n\thash_for_each(vmp->regions.slot_hash, ctr, region, slot_node) {\n\t\tint ret = ioctl(vmp->fd, KVM_SET_USER_MEMORY_REGION, &region->region);\n\t\tTEST_ASSERT(ret == 0, \"KVM_SET_USER_MEMORY_REGION IOCTL failed,\\n\"\n\t\t\t    \"  rc: %i errno: %i\\n\"\n\t\t\t    \"  slot: %u flags: 0x%x\\n\"\n\t\t\t    \"  guest_phys_addr: 0x%llx size: 0x%llx\",\n\t\t\t    ret, errno, region->region.slot,\n\t\t\t    region->region.flags,\n\t\t\t    region->region.guest_phys_addr,\n\t\t\t    region->region.memory_size);\n\t}\n}\n\n__weak struct kvm_vcpu *vm_arch_vcpu_recreate(struct kvm_vm *vm,\n\t\t\t\t\t      uint32_t vcpu_id)\n{\n\treturn __vm_vcpu_add(vm, vcpu_id);\n}\n\nstruct kvm_vcpu *vm_recreate_with_one_vcpu(struct kvm_vm *vm)\n{\n\tkvm_vm_restart(vm);\n\n\treturn vm_vcpu_recreate(vm, 0);\n}\n\nvoid kvm_pin_this_task_to_pcpu(uint32_t pcpu)\n{\n\tcpu_set_t mask;\n\tint r;\n\n\tCPU_ZERO(&mask);\n\tCPU_SET(pcpu, &mask);\n\tr = sched_setaffinity(0, sizeof(mask), &mask);\n\tTEST_ASSERT(!r, \"sched_setaffinity() failed for pCPU '%u'.\\n\", pcpu);\n}\n\nstatic uint32_t parse_pcpu(const char *cpu_str, const cpu_set_t *allowed_mask)\n{\n\tuint32_t pcpu = atoi_non_negative(\"CPU number\", cpu_str);\n\n\tTEST_ASSERT(CPU_ISSET(pcpu, allowed_mask),\n\t\t    \"Not allowed to run on pCPU '%d', check cgroups?\\n\", pcpu);\n\treturn pcpu;\n}\n\nvoid kvm_print_vcpu_pinning_help(void)\n{\n\tconst char *name = program_invocation_name;\n\n\tprintf(\" -c: Pin tasks to physical CPUs.  Takes a list of comma separated\\n\"\n\t       \"     values (target pCPU), one for each vCPU, plus an optional\\n\"\n\t       \"     entry for the main application task (specified via entry\\n\"\n\t       \"     <nr_vcpus + 1>).  If used, entries must be provided for all\\n\"\n\t       \"     vCPUs, i.e. pinning vCPUs is all or nothing.\\n\\n\"\n\t       \"     E.g. to create 3 vCPUs, pin vCPU0=>pCPU22, vCPU1=>pCPU23,\\n\"\n\t       \"     vCPU2=>pCPU24, and pin the application task to pCPU50:\\n\\n\"\n\t       \"         %s -v 3 -c 22,23,24,50\\n\\n\"\n\t       \"     To leave the application task unpinned, drop the final entry:\\n\\n\"\n\t       \"         %s -v 3 -c 22,23,24\\n\\n\"\n\t       \"     (default: no pinning)\\n\", name, name);\n}\n\nvoid kvm_parse_vcpu_pinning(const char *pcpus_string, uint32_t vcpu_to_pcpu[],\n\t\t\t    int nr_vcpus)\n{\n\tcpu_set_t allowed_mask;\n\tchar *cpu, *cpu_list;\n\tchar delim[2] = \",\";\n\tint i, r;\n\n\tcpu_list = strdup(pcpus_string);\n\tTEST_ASSERT(cpu_list, \"strdup() allocation failed.\\n\");\n\n\tr = sched_getaffinity(0, sizeof(allowed_mask), &allowed_mask);\n\tTEST_ASSERT(!r, \"sched_getaffinity() failed\");\n\n\tcpu = strtok(cpu_list, delim);\n\n\t \n\tfor (i = 0; i < nr_vcpus; i++) {\n\t\tTEST_ASSERT(cpu, \"pCPU not provided for vCPU '%d'\\n\", i);\n\t\tvcpu_to_pcpu[i] = parse_pcpu(cpu, &allowed_mask);\n\t\tcpu = strtok(NULL, delim);\n\t}\n\n\t \n\tif (cpu) {\n\t\tkvm_pin_this_task_to_pcpu(parse_pcpu(cpu, &allowed_mask));\n\t\tcpu = strtok(NULL, delim);\n\t}\n\n\tTEST_ASSERT(!cpu, \"pCPU list contains trailing garbage characters '%s'\", cpu);\n\tfree(cpu_list);\n}\n\n \nstatic struct userspace_mem_region *\nuserspace_mem_region_find(struct kvm_vm *vm, uint64_t start, uint64_t end)\n{\n\tstruct rb_node *node;\n\n\tfor (node = vm->regions.gpa_tree.rb_node; node; ) {\n\t\tstruct userspace_mem_region *region =\n\t\t\tcontainer_of(node, struct userspace_mem_region, gpa_node);\n\t\tuint64_t existing_start = region->region.guest_phys_addr;\n\t\tuint64_t existing_end = region->region.guest_phys_addr\n\t\t\t+ region->region.memory_size - 1;\n\t\tif (start <= existing_end && end >= existing_start)\n\t\t\treturn region;\n\n\t\tif (start < existing_start)\n\t\t\tnode = node->rb_left;\n\t\telse\n\t\t\tnode = node->rb_right;\n\t}\n\n\treturn NULL;\n}\n\n \nstruct kvm_userspace_memory_region *\nkvm_userspace_memory_region_find(struct kvm_vm *vm, uint64_t start,\n\t\t\t\t uint64_t end)\n{\n\tstruct userspace_mem_region *region;\n\n\tregion = userspace_mem_region_find(vm, start, end);\n\tif (!region)\n\t\treturn NULL;\n\n\treturn &region->region;\n}\n\n__weak void vcpu_arch_free(struct kvm_vcpu *vcpu)\n{\n\n}\n\n \nstatic void vm_vcpu_rm(struct kvm_vm *vm, struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\tif (vcpu->dirty_gfns) {\n\t\tret = munmap(vcpu->dirty_gfns, vm->dirty_ring_size);\n\t\tTEST_ASSERT(!ret, __KVM_SYSCALL_ERROR(\"munmap()\", ret));\n\t\tvcpu->dirty_gfns = NULL;\n\t}\n\n\tret = munmap(vcpu->run, vcpu_mmap_sz());\n\tTEST_ASSERT(!ret, __KVM_SYSCALL_ERROR(\"munmap()\", ret));\n\n\tret = close(vcpu->fd);\n\tTEST_ASSERT(!ret,  __KVM_SYSCALL_ERROR(\"close()\", ret));\n\n\tlist_del(&vcpu->list);\n\n\tvcpu_arch_free(vcpu);\n\tfree(vcpu);\n}\n\nvoid kvm_vm_release(struct kvm_vm *vmp)\n{\n\tstruct kvm_vcpu *vcpu, *tmp;\n\tint ret;\n\n\tlist_for_each_entry_safe(vcpu, tmp, &vmp->vcpus, list)\n\t\tvm_vcpu_rm(vmp, vcpu);\n\n\tret = close(vmp->fd);\n\tTEST_ASSERT(!ret,  __KVM_SYSCALL_ERROR(\"close()\", ret));\n\n\tret = close(vmp->kvm_fd);\n\tTEST_ASSERT(!ret,  __KVM_SYSCALL_ERROR(\"close()\", ret));\n}\n\nstatic void __vm_mem_region_delete(struct kvm_vm *vm,\n\t\t\t\t   struct userspace_mem_region *region,\n\t\t\t\t   bool unlink)\n{\n\tint ret;\n\n\tif (unlink) {\n\t\trb_erase(&region->gpa_node, &vm->regions.gpa_tree);\n\t\trb_erase(&region->hva_node, &vm->regions.hva_tree);\n\t\thash_del(&region->slot_node);\n\t}\n\n\tregion->region.memory_size = 0;\n\tvm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);\n\n\tsparsebit_free(&region->unused_phy_pages);\n\tret = munmap(region->mmap_start, region->mmap_size);\n\tTEST_ASSERT(!ret, __KVM_SYSCALL_ERROR(\"munmap()\", ret));\n\tif (region->fd >= 0) {\n\t\t \n\t\tret = munmap(region->mmap_alias, region->mmap_size);\n\t\tTEST_ASSERT(!ret, __KVM_SYSCALL_ERROR(\"munmap()\", ret));\n\t\tclose(region->fd);\n\t}\n\n\tfree(region);\n}\n\n \nvoid kvm_vm_free(struct kvm_vm *vmp)\n{\n\tint ctr;\n\tstruct hlist_node *node;\n\tstruct userspace_mem_region *region;\n\n\tif (vmp == NULL)\n\t\treturn;\n\n\t \n\tif (vmp->stats_fd) {\n\t\tfree(vmp->stats_desc);\n\t\tclose(vmp->stats_fd);\n\t}\n\n\t \n\thash_for_each_safe(vmp->regions.slot_hash, ctr, node, region, slot_node)\n\t\t__vm_mem_region_delete(vmp, region, false);\n\n\t \n\tsparsebit_free(&vmp->vpages_valid);\n\tsparsebit_free(&vmp->vpages_mapped);\n\n\tkvm_vm_release(vmp);\n\n\t \n\tfree(vmp);\n}\n\nint kvm_memfd_alloc(size_t size, bool hugepages)\n{\n\tint memfd_flags = MFD_CLOEXEC;\n\tint fd, r;\n\n\tif (hugepages)\n\t\tmemfd_flags |= MFD_HUGETLB;\n\n\tfd = memfd_create(\"kvm_selftest\", memfd_flags);\n\tTEST_ASSERT(fd != -1, __KVM_SYSCALL_ERROR(\"memfd_create()\", fd));\n\n\tr = ftruncate(fd, size);\n\tTEST_ASSERT(!r, __KVM_SYSCALL_ERROR(\"ftruncate()\", r));\n\n\tr = fallocate(fd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, 0, size);\n\tTEST_ASSERT(!r, __KVM_SYSCALL_ERROR(\"fallocate()\", r));\n\n\treturn fd;\n}\n\n \nint kvm_memcmp_hva_gva(void *hva, struct kvm_vm *vm, vm_vaddr_t gva, size_t len)\n{\n\tsize_t amt;\n\n\t \n\tfor (uintptr_t offset = 0; offset < len; offset += amt) {\n\t\tuintptr_t ptr1 = (uintptr_t)hva + offset;\n\n\t\t \n\t\tuintptr_t ptr2 = (uintptr_t)addr_gva2hva(vm, gva + offset);\n\n\t\t \n\t\tamt = len - offset;\n\t\tif ((ptr1 >> vm->page_shift) != ((ptr1 + amt) >> vm->page_shift))\n\t\t\tamt = vm->page_size - (ptr1 % vm->page_size);\n\t\tif ((ptr2 >> vm->page_shift) != ((ptr2 + amt) >> vm->page_shift))\n\t\t\tamt = vm->page_size - (ptr2 % vm->page_size);\n\n\t\tassert((ptr1 >> vm->page_shift) == ((ptr1 + amt - 1) >> vm->page_shift));\n\t\tassert((ptr2 >> vm->page_shift) == ((ptr2 + amt - 1) >> vm->page_shift));\n\n\t\t \n\t\tint ret = memcmp((void *)ptr1, (void *)ptr2, amt);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic void vm_userspace_mem_region_gpa_insert(struct rb_root *gpa_tree,\n\t\t\t\t\t       struct userspace_mem_region *region)\n{\n\tstruct rb_node **cur, *parent;\n\n\tfor (cur = &gpa_tree->rb_node, parent = NULL; *cur; ) {\n\t\tstruct userspace_mem_region *cregion;\n\n\t\tcregion = container_of(*cur, typeof(*cregion), gpa_node);\n\t\tparent = *cur;\n\t\tif (region->region.guest_phys_addr <\n\t\t    cregion->region.guest_phys_addr)\n\t\t\tcur = &(*cur)->rb_left;\n\t\telse {\n\t\t\tTEST_ASSERT(region->region.guest_phys_addr !=\n\t\t\t\t    cregion->region.guest_phys_addr,\n\t\t\t\t    \"Duplicate GPA in region tree\");\n\n\t\t\tcur = &(*cur)->rb_right;\n\t\t}\n\t}\n\n\trb_link_node(&region->gpa_node, parent, cur);\n\trb_insert_color(&region->gpa_node, gpa_tree);\n}\n\nstatic void vm_userspace_mem_region_hva_insert(struct rb_root *hva_tree,\n\t\t\t\t\t       struct userspace_mem_region *region)\n{\n\tstruct rb_node **cur, *parent;\n\n\tfor (cur = &hva_tree->rb_node, parent = NULL; *cur; ) {\n\t\tstruct userspace_mem_region *cregion;\n\n\t\tcregion = container_of(*cur, typeof(*cregion), hva_node);\n\t\tparent = *cur;\n\t\tif (region->host_mem < cregion->host_mem)\n\t\t\tcur = &(*cur)->rb_left;\n\t\telse {\n\t\t\tTEST_ASSERT(region->host_mem !=\n\t\t\t\t    cregion->host_mem,\n\t\t\t\t    \"Duplicate HVA in region tree\");\n\n\t\t\tcur = &(*cur)->rb_right;\n\t\t}\n\t}\n\n\trb_link_node(&region->hva_node, parent, cur);\n\trb_insert_color(&region->hva_node, hva_tree);\n}\n\n\nint __vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,\n\t\t\t\tuint64_t gpa, uint64_t size, void *hva)\n{\n\tstruct kvm_userspace_memory_region region = {\n\t\t.slot = slot,\n\t\t.flags = flags,\n\t\t.guest_phys_addr = gpa,\n\t\t.memory_size = size,\n\t\t.userspace_addr = (uintptr_t)hva,\n\t};\n\n\treturn ioctl(vm->fd, KVM_SET_USER_MEMORY_REGION, &region);\n}\n\nvoid vm_set_user_memory_region(struct kvm_vm *vm, uint32_t slot, uint32_t flags,\n\t\t\t       uint64_t gpa, uint64_t size, void *hva)\n{\n\tint ret = __vm_set_user_memory_region(vm, slot, flags, gpa, size, hva);\n\n\tTEST_ASSERT(!ret, \"KVM_SET_USER_MEMORY_REGION failed, errno = %d (%s)\",\n\t\t    errno, strerror(errno));\n}\n\n \nvoid vm_userspace_mem_region_add(struct kvm_vm *vm,\n\tenum vm_mem_backing_src_type src_type,\n\tuint64_t guest_paddr, uint32_t slot, uint64_t npages,\n\tuint32_t flags)\n{\n\tint ret;\n\tstruct userspace_mem_region *region;\n\tsize_t backing_src_pagesz = get_backing_src_pagesz(src_type);\n\tsize_t alignment;\n\n\tTEST_ASSERT(vm_adjust_num_guest_pages(vm->mode, npages) == npages,\n\t\t\"Number of guest pages is not compatible with the host. \"\n\t\t\"Try npages=%d\", vm_adjust_num_guest_pages(vm->mode, npages));\n\n\tTEST_ASSERT((guest_paddr % vm->page_size) == 0, \"Guest physical \"\n\t\t\"address not on a page boundary.\\n\"\n\t\t\"  guest_paddr: 0x%lx vm->page_size: 0x%x\",\n\t\tguest_paddr, vm->page_size);\n\tTEST_ASSERT((((guest_paddr >> vm->page_shift) + npages) - 1)\n\t\t<= vm->max_gfn, \"Physical range beyond maximum \"\n\t\t\"supported physical address,\\n\"\n\t\t\"  guest_paddr: 0x%lx npages: 0x%lx\\n\"\n\t\t\"  vm->max_gfn: 0x%lx vm->page_size: 0x%x\",\n\t\tguest_paddr, npages, vm->max_gfn, vm->page_size);\n\n\t \n\tregion = (struct userspace_mem_region *) userspace_mem_region_find(\n\t\tvm, guest_paddr, (guest_paddr + npages * vm->page_size) - 1);\n\tif (region != NULL)\n\t\tTEST_FAIL(\"overlapping userspace_mem_region already \"\n\t\t\t\"exists\\n\"\n\t\t\t\"  requested guest_paddr: 0x%lx npages: 0x%lx \"\n\t\t\t\"page_size: 0x%x\\n\"\n\t\t\t\"  existing guest_paddr: 0x%lx size: 0x%lx\",\n\t\t\tguest_paddr, npages, vm->page_size,\n\t\t\t(uint64_t) region->region.guest_phys_addr,\n\t\t\t(uint64_t) region->region.memory_size);\n\n\t \n\thash_for_each_possible(vm->regions.slot_hash, region, slot_node,\n\t\t\t       slot) {\n\t\tif (region->region.slot != slot)\n\t\t\tcontinue;\n\n\t\tTEST_FAIL(\"A mem region with the requested slot \"\n\t\t\t\"already exists.\\n\"\n\t\t\t\"  requested slot: %u paddr: 0x%lx npages: 0x%lx\\n\"\n\t\t\t\"  existing slot: %u paddr: 0x%lx size: 0x%lx\",\n\t\t\tslot, guest_paddr, npages,\n\t\t\tregion->region.slot,\n\t\t\t(uint64_t) region->region.guest_phys_addr,\n\t\t\t(uint64_t) region->region.memory_size);\n\t}\n\n\t \n\tregion = calloc(1, sizeof(*region));\n\tTEST_ASSERT(region != NULL, \"Insufficient Memory\");\n\tregion->mmap_size = npages * vm->page_size;\n\n#ifdef __s390x__\n\t \n\talignment = 0x100000;\n#else\n\talignment = 1;\n#endif\n\n\t \n\tif (src_type == VM_MEM_SRC_ANONYMOUS_THP)\n\t\talignment = max(backing_src_pagesz, alignment);\n\n\tTEST_ASSERT_EQ(guest_paddr, align_up(guest_paddr, backing_src_pagesz));\n\n\t \n\tif (alignment > 1)\n\t\tregion->mmap_size += alignment;\n\n\tregion->fd = -1;\n\tif (backing_src_is_shared(src_type))\n\t\tregion->fd = kvm_memfd_alloc(region->mmap_size,\n\t\t\t\t\t     src_type == VM_MEM_SRC_SHARED_HUGETLB);\n\n\tregion->mmap_start = mmap(NULL, region->mmap_size,\n\t\t\t\t  PROT_READ | PROT_WRITE,\n\t\t\t\t  vm_mem_backing_src_alias(src_type)->flag,\n\t\t\t\t  region->fd, 0);\n\tTEST_ASSERT(region->mmap_start != MAP_FAILED,\n\t\t    __KVM_SYSCALL_ERROR(\"mmap()\", (int)(unsigned long)MAP_FAILED));\n\n\tTEST_ASSERT(!is_backing_src_hugetlb(src_type) ||\n\t\t    region->mmap_start == align_ptr_up(region->mmap_start, backing_src_pagesz),\n\t\t    \"mmap_start %p is not aligned to HugeTLB page size 0x%lx\",\n\t\t    region->mmap_start, backing_src_pagesz);\n\n\t \n\tregion->host_mem = align_ptr_up(region->mmap_start, alignment);\n\n\t \n\tif ((src_type == VM_MEM_SRC_ANONYMOUS ||\n\t     src_type == VM_MEM_SRC_ANONYMOUS_THP) && thp_configured()) {\n\t\tret = madvise(region->host_mem, npages * vm->page_size,\n\t\t\t      src_type == VM_MEM_SRC_ANONYMOUS ? MADV_NOHUGEPAGE : MADV_HUGEPAGE);\n\t\tTEST_ASSERT(ret == 0, \"madvise failed, addr: %p length: 0x%lx src_type: %s\",\n\t\t\t    region->host_mem, npages * vm->page_size,\n\t\t\t    vm_mem_backing_src_alias(src_type)->name);\n\t}\n\n\tregion->backing_src_type = src_type;\n\tregion->unused_phy_pages = sparsebit_alloc();\n\tsparsebit_set_num(region->unused_phy_pages,\n\t\tguest_paddr >> vm->page_shift, npages);\n\tregion->region.slot = slot;\n\tregion->region.flags = flags;\n\tregion->region.guest_phys_addr = guest_paddr;\n\tregion->region.memory_size = npages * vm->page_size;\n\tregion->region.userspace_addr = (uintptr_t) region->host_mem;\n\tret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);\n\tTEST_ASSERT(ret == 0, \"KVM_SET_USER_MEMORY_REGION IOCTL failed,\\n\"\n\t\t\"  rc: %i errno: %i\\n\"\n\t\t\"  slot: %u flags: 0x%x\\n\"\n\t\t\"  guest_phys_addr: 0x%lx size: 0x%lx\",\n\t\tret, errno, slot, flags,\n\t\tguest_paddr, (uint64_t) region->region.memory_size);\n\n\t \n\tvm_userspace_mem_region_gpa_insert(&vm->regions.gpa_tree, region);\n\tvm_userspace_mem_region_hva_insert(&vm->regions.hva_tree, region);\n\thash_add(vm->regions.slot_hash, &region->slot_node, slot);\n\n\t \n\tif (region->fd >= 0) {\n\t\tregion->mmap_alias = mmap(NULL, region->mmap_size,\n\t\t\t\t\t  PROT_READ | PROT_WRITE,\n\t\t\t\t\t  vm_mem_backing_src_alias(src_type)->flag,\n\t\t\t\t\t  region->fd, 0);\n\t\tTEST_ASSERT(region->mmap_alias != MAP_FAILED,\n\t\t\t    __KVM_SYSCALL_ERROR(\"mmap()\",  (int)(unsigned long)MAP_FAILED));\n\n\t\t \n\t\tregion->host_alias = align_ptr_up(region->mmap_alias, alignment);\n\t}\n}\n\n \nstruct userspace_mem_region *\nmemslot2region(struct kvm_vm *vm, uint32_t memslot)\n{\n\tstruct userspace_mem_region *region;\n\n\thash_for_each_possible(vm->regions.slot_hash, region, slot_node,\n\t\t\t       memslot)\n\t\tif (region->region.slot == memslot)\n\t\t\treturn region;\n\n\tfprintf(stderr, \"No mem region with the requested slot found,\\n\"\n\t\t\"  requested slot: %u\\n\", memslot);\n\tfputs(\"---- vm dump ----\\n\", stderr);\n\tvm_dump(stderr, vm, 2);\n\tTEST_FAIL(\"Mem region not found\");\n\treturn NULL;\n}\n\n \nvoid vm_mem_region_set_flags(struct kvm_vm *vm, uint32_t slot, uint32_t flags)\n{\n\tint ret;\n\tstruct userspace_mem_region *region;\n\n\tregion = memslot2region(vm, slot);\n\n\tregion->region.flags = flags;\n\n\tret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);\n\n\tTEST_ASSERT(ret == 0, \"KVM_SET_USER_MEMORY_REGION IOCTL failed,\\n\"\n\t\t\"  rc: %i errno: %i slot: %u flags: 0x%x\",\n\t\tret, errno, slot, flags);\n}\n\n \nvoid vm_mem_region_move(struct kvm_vm *vm, uint32_t slot, uint64_t new_gpa)\n{\n\tstruct userspace_mem_region *region;\n\tint ret;\n\n\tregion = memslot2region(vm, slot);\n\n\tregion->region.guest_phys_addr = new_gpa;\n\n\tret = __vm_ioctl(vm, KVM_SET_USER_MEMORY_REGION, &region->region);\n\n\tTEST_ASSERT(!ret, \"KVM_SET_USER_MEMORY_REGION failed\\n\"\n\t\t    \"ret: %i errno: %i slot: %u new_gpa: 0x%lx\",\n\t\t    ret, errno, slot, new_gpa);\n}\n\n \nvoid vm_mem_region_delete(struct kvm_vm *vm, uint32_t slot)\n{\n\t__vm_mem_region_delete(vm, memslot2region(vm, slot), true);\n}\n\n \nstatic int vcpu_mmap_sz(void)\n{\n\tint dev_fd, ret;\n\n\tdev_fd = open_kvm_dev_path_or_exit();\n\n\tret = ioctl(dev_fd, KVM_GET_VCPU_MMAP_SIZE, NULL);\n\tTEST_ASSERT(ret >= sizeof(struct kvm_run),\n\t\t    KVM_IOCTL_ERROR(KVM_GET_VCPU_MMAP_SIZE, ret));\n\n\tclose(dev_fd);\n\n\treturn ret;\n}\n\nstatic bool vcpu_exists(struct kvm_vm *vm, uint32_t vcpu_id)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tlist_for_each_entry(vcpu, &vm->vcpus, list) {\n\t\tif (vcpu->id == vcpu_id)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstruct kvm_vcpu *__vm_vcpu_add(struct kvm_vm *vm, uint32_t vcpu_id)\n{\n\tstruct kvm_vcpu *vcpu;\n\n\t \n\tTEST_ASSERT(!vcpu_exists(vm, vcpu_id), \"vCPU%d already exists\\n\", vcpu_id);\n\n\t \n\tvcpu = calloc(1, sizeof(*vcpu));\n\tTEST_ASSERT(vcpu != NULL, \"Insufficient Memory\");\n\n\tvcpu->vm = vm;\n\tvcpu->id = vcpu_id;\n\tvcpu->fd = __vm_ioctl(vm, KVM_CREATE_VCPU, (void *)(unsigned long)vcpu_id);\n\tTEST_ASSERT(vcpu->fd >= 0, KVM_IOCTL_ERROR(KVM_CREATE_VCPU, vcpu->fd));\n\n\tTEST_ASSERT(vcpu_mmap_sz() >= sizeof(*vcpu->run), \"vcpu mmap size \"\n\t\t\"smaller than expected, vcpu_mmap_sz: %i expected_min: %zi\",\n\t\tvcpu_mmap_sz(), sizeof(*vcpu->run));\n\tvcpu->run = (struct kvm_run *) mmap(NULL, vcpu_mmap_sz(),\n\t\tPROT_READ | PROT_WRITE, MAP_SHARED, vcpu->fd, 0);\n\tTEST_ASSERT(vcpu->run != MAP_FAILED,\n\t\t    __KVM_SYSCALL_ERROR(\"mmap()\", (int)(unsigned long)MAP_FAILED));\n\n\t \n\tlist_add(&vcpu->list, &vm->vcpus);\n\n\treturn vcpu;\n}\n\n \nvm_vaddr_t vm_vaddr_unused_gap(struct kvm_vm *vm, size_t sz,\n\t\t\t       vm_vaddr_t vaddr_min)\n{\n\tuint64_t pages = (sz + vm->page_size - 1) >> vm->page_shift;\n\n\t \n\tuint64_t pgidx_start = (vaddr_min + vm->page_size - 1) >> vm->page_shift;\n\tif ((pgidx_start * vm->page_size) < vaddr_min)\n\t\tgoto no_va_found;\n\n\t \n\tif (!sparsebit_is_set_num(vm->vpages_valid,\n\t\tpgidx_start, pages))\n\t\tpgidx_start = sparsebit_next_set_num(vm->vpages_valid,\n\t\t\tpgidx_start, pages);\n\tdo {\n\t\t \n\t\tif (sparsebit_is_clear_num(vm->vpages_mapped,\n\t\t\tpgidx_start, pages))\n\t\t\tgoto va_found;\n\t\tpgidx_start = sparsebit_next_clear_num(vm->vpages_mapped,\n\t\t\tpgidx_start, pages);\n\t\tif (pgidx_start == 0)\n\t\t\tgoto no_va_found;\n\n\t\t \n\t\tif (!sparsebit_is_set_num(vm->vpages_valid,\n\t\t\tpgidx_start, pages)) {\n\t\t\tpgidx_start = sparsebit_next_set_num(\n\t\t\t\tvm->vpages_valid, pgidx_start, pages);\n\t\t\tif (pgidx_start == 0)\n\t\t\t\tgoto no_va_found;\n\t\t}\n\t} while (pgidx_start != 0);\n\nno_va_found:\n\tTEST_FAIL(\"No vaddr of specified pages available, pages: 0x%lx\", pages);\n\n\t \n\treturn -1;\n\nva_found:\n\tTEST_ASSERT(sparsebit_is_set_num(vm->vpages_valid,\n\t\tpgidx_start, pages),\n\t\t\"Unexpected, invalid virtual page index range,\\n\"\n\t\t\"  pgidx_start: 0x%lx\\n\"\n\t\t\"  pages: 0x%lx\",\n\t\tpgidx_start, pages);\n\tTEST_ASSERT(sparsebit_is_clear_num(vm->vpages_mapped,\n\t\tpgidx_start, pages),\n\t\t\"Unexpected, pages already mapped,\\n\"\n\t\t\"  pgidx_start: 0x%lx\\n\"\n\t\t\"  pages: 0x%lx\",\n\t\tpgidx_start, pages);\n\n\treturn pgidx_start * vm->page_size;\n}\n\nvm_vaddr_t __vm_vaddr_alloc(struct kvm_vm *vm, size_t sz, vm_vaddr_t vaddr_min,\n\t\t\t    enum kvm_mem_region_type type)\n{\n\tuint64_t pages = (sz >> vm->page_shift) + ((sz % vm->page_size) != 0);\n\n\tvirt_pgd_alloc(vm);\n\tvm_paddr_t paddr = vm_phy_pages_alloc(vm, pages,\n\t\t\t\t\t      KVM_UTIL_MIN_PFN * vm->page_size,\n\t\t\t\t\t      vm->memslots[type]);\n\n\t \n\tvm_vaddr_t vaddr_start = vm_vaddr_unused_gap(vm, sz, vaddr_min);\n\n\t \n\tfor (vm_vaddr_t vaddr = vaddr_start; pages > 0;\n\t\tpages--, vaddr += vm->page_size, paddr += vm->page_size) {\n\n\t\tvirt_pg_map(vm, vaddr, paddr);\n\n\t\tsparsebit_set(vm->vpages_mapped, vaddr >> vm->page_shift);\n\t}\n\n\treturn vaddr_start;\n}\n\n \nvm_vaddr_t vm_vaddr_alloc(struct kvm_vm *vm, size_t sz, vm_vaddr_t vaddr_min)\n{\n\treturn __vm_vaddr_alloc(vm, sz, vaddr_min, MEM_REGION_TEST_DATA);\n}\n\n \nvm_vaddr_t vm_vaddr_alloc_pages(struct kvm_vm *vm, int nr_pages)\n{\n\treturn vm_vaddr_alloc(vm, nr_pages * getpagesize(), KVM_UTIL_MIN_VADDR);\n}\n\nvm_vaddr_t __vm_vaddr_alloc_page(struct kvm_vm *vm, enum kvm_mem_region_type type)\n{\n\treturn __vm_vaddr_alloc(vm, getpagesize(), KVM_UTIL_MIN_VADDR, type);\n}\n\n \nvm_vaddr_t vm_vaddr_alloc_page(struct kvm_vm *vm)\n{\n\treturn vm_vaddr_alloc_pages(vm, 1);\n}\n\n \nvoid virt_map(struct kvm_vm *vm, uint64_t vaddr, uint64_t paddr,\n\t      unsigned int npages)\n{\n\tsize_t page_size = vm->page_size;\n\tsize_t size = npages * page_size;\n\n\tTEST_ASSERT(vaddr + size > vaddr, \"Vaddr overflow\");\n\tTEST_ASSERT(paddr + size > paddr, \"Paddr overflow\");\n\n\twhile (npages--) {\n\t\tvirt_pg_map(vm, vaddr, paddr);\n\t\tsparsebit_set(vm->vpages_mapped, vaddr >> vm->page_shift);\n\n\t\tvaddr += page_size;\n\t\tpaddr += page_size;\n\t}\n}\n\n \nvoid *addr_gpa2hva(struct kvm_vm *vm, vm_paddr_t gpa)\n{\n\tstruct userspace_mem_region *region;\n\n\tregion = userspace_mem_region_find(vm, gpa, gpa);\n\tif (!region) {\n\t\tTEST_FAIL(\"No vm physical memory at 0x%lx\", gpa);\n\t\treturn NULL;\n\t}\n\n\treturn (void *)((uintptr_t)region->host_mem\n\t\t+ (gpa - region->region.guest_phys_addr));\n}\n\n \nvm_paddr_t addr_hva2gpa(struct kvm_vm *vm, void *hva)\n{\n\tstruct rb_node *node;\n\n\tfor (node = vm->regions.hva_tree.rb_node; node; ) {\n\t\tstruct userspace_mem_region *region =\n\t\t\tcontainer_of(node, struct userspace_mem_region, hva_node);\n\n\t\tif (hva >= region->host_mem) {\n\t\t\tif (hva <= (region->host_mem\n\t\t\t\t+ region->region.memory_size - 1))\n\t\t\t\treturn (vm_paddr_t)((uintptr_t)\n\t\t\t\t\tregion->region.guest_phys_addr\n\t\t\t\t\t+ (hva - (uintptr_t)region->host_mem));\n\n\t\t\tnode = node->rb_right;\n\t\t} else\n\t\t\tnode = node->rb_left;\n\t}\n\n\tTEST_FAIL(\"No mapping to a guest physical address, hva: %p\", hva);\n\treturn -1;\n}\n\n \nvoid *addr_gpa2alias(struct kvm_vm *vm, vm_paddr_t gpa)\n{\n\tstruct userspace_mem_region *region;\n\tuintptr_t offset;\n\n\tregion = userspace_mem_region_find(vm, gpa, gpa);\n\tif (!region)\n\t\treturn NULL;\n\n\tif (!region->host_alias)\n\t\treturn NULL;\n\n\toffset = gpa - region->region.guest_phys_addr;\n\treturn (void *) ((uintptr_t) region->host_alias + offset);\n}\n\n \nvoid vm_create_irqchip(struct kvm_vm *vm)\n{\n\tvm_ioctl(vm, KVM_CREATE_IRQCHIP, NULL);\n\n\tvm->has_irqchip = true;\n}\n\nint _vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tint rc;\n\n\tdo {\n\t\trc = __vcpu_run(vcpu);\n\t} while (rc == -1 && errno == EINTR);\n\n\tassert_on_unhandled_exception(vcpu);\n\n\treturn rc;\n}\n\n \nvoid vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tint ret = _vcpu_run(vcpu);\n\n\tTEST_ASSERT(!ret, KVM_IOCTL_ERROR(KVM_RUN, ret));\n}\n\nvoid vcpu_run_complete_io(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\tvcpu->run->immediate_exit = 1;\n\tret = __vcpu_run(vcpu);\n\tvcpu->run->immediate_exit = 0;\n\n\tTEST_ASSERT(ret == -1 && errno == EINTR,\n\t\t    \"KVM_RUN IOCTL didn't exit immediately, rc: %i, errno: %i\",\n\t\t    ret, errno);\n}\n\n \nstruct kvm_reg_list *vcpu_get_reg_list(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_reg_list reg_list_n = { .n = 0 }, *reg_list;\n\tint ret;\n\n\tret = __vcpu_ioctl(vcpu, KVM_GET_REG_LIST, &reg_list_n);\n\tTEST_ASSERT(ret == -1 && errno == E2BIG, \"KVM_GET_REG_LIST n=0\");\n\n\treg_list = calloc(1, sizeof(*reg_list) + reg_list_n.n * sizeof(__u64));\n\treg_list->n = reg_list_n.n;\n\tvcpu_ioctl(vcpu, KVM_GET_REG_LIST, reg_list);\n\treturn reg_list;\n}\n\nvoid *vcpu_map_dirty_ring(struct kvm_vcpu *vcpu)\n{\n\tuint32_t page_size = getpagesize();\n\tuint32_t size = vcpu->vm->dirty_ring_size;\n\n\tTEST_ASSERT(size > 0, \"Should enable dirty ring first\");\n\n\tif (!vcpu->dirty_gfns) {\n\t\tvoid *addr;\n\n\t\taddr = mmap(NULL, size, PROT_READ, MAP_PRIVATE, vcpu->fd,\n\t\t\t    page_size * KVM_DIRTY_LOG_PAGE_OFFSET);\n\t\tTEST_ASSERT(addr == MAP_FAILED, \"Dirty ring mapped private\");\n\n\t\taddr = mmap(NULL, size, PROT_READ | PROT_EXEC, MAP_PRIVATE, vcpu->fd,\n\t\t\t    page_size * KVM_DIRTY_LOG_PAGE_OFFSET);\n\t\tTEST_ASSERT(addr == MAP_FAILED, \"Dirty ring mapped exec\");\n\n\t\taddr = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED, vcpu->fd,\n\t\t\t    page_size * KVM_DIRTY_LOG_PAGE_OFFSET);\n\t\tTEST_ASSERT(addr != MAP_FAILED, \"Dirty ring map failed\");\n\n\t\tvcpu->dirty_gfns = addr;\n\t\tvcpu->dirty_gfns_count = size / sizeof(struct kvm_dirty_gfn);\n\t}\n\n\treturn vcpu->dirty_gfns;\n}\n\n \n\nint __kvm_has_device_attr(int dev_fd, uint32_t group, uint64_t attr)\n{\n\tstruct kvm_device_attr attribute = {\n\t\t.group = group,\n\t\t.attr = attr,\n\t\t.flags = 0,\n\t};\n\n\treturn ioctl(dev_fd, KVM_HAS_DEVICE_ATTR, &attribute);\n}\n\nint __kvm_test_create_device(struct kvm_vm *vm, uint64_t type)\n{\n\tstruct kvm_create_device create_dev = {\n\t\t.type = type,\n\t\t.flags = KVM_CREATE_DEVICE_TEST,\n\t};\n\n\treturn __vm_ioctl(vm, KVM_CREATE_DEVICE, &create_dev);\n}\n\nint __kvm_create_device(struct kvm_vm *vm, uint64_t type)\n{\n\tstruct kvm_create_device create_dev = {\n\t\t.type = type,\n\t\t.fd = -1,\n\t\t.flags = 0,\n\t};\n\tint err;\n\n\terr = __vm_ioctl(vm, KVM_CREATE_DEVICE, &create_dev);\n\tTEST_ASSERT(err <= 0, \"KVM_CREATE_DEVICE shouldn't return a positive value\");\n\treturn err ? : create_dev.fd;\n}\n\nint __kvm_device_attr_get(int dev_fd, uint32_t group, uint64_t attr, void *val)\n{\n\tstruct kvm_device_attr kvmattr = {\n\t\t.group = group,\n\t\t.attr = attr,\n\t\t.flags = 0,\n\t\t.addr = (uintptr_t)val,\n\t};\n\n\treturn __kvm_ioctl(dev_fd, KVM_GET_DEVICE_ATTR, &kvmattr);\n}\n\nint __kvm_device_attr_set(int dev_fd, uint32_t group, uint64_t attr, void *val)\n{\n\tstruct kvm_device_attr kvmattr = {\n\t\t.group = group,\n\t\t.attr = attr,\n\t\t.flags = 0,\n\t\t.addr = (uintptr_t)val,\n\t};\n\n\treturn __kvm_ioctl(dev_fd, KVM_SET_DEVICE_ATTR, &kvmattr);\n}\n\n \n\nint _kvm_irq_line(struct kvm_vm *vm, uint32_t irq, int level)\n{\n\tstruct kvm_irq_level irq_level = {\n\t\t.irq    = irq,\n\t\t.level  = level,\n\t};\n\n\treturn __vm_ioctl(vm, KVM_IRQ_LINE, &irq_level);\n}\n\nvoid kvm_irq_line(struct kvm_vm *vm, uint32_t irq, int level)\n{\n\tint ret = _kvm_irq_line(vm, irq, level);\n\n\tTEST_ASSERT(ret >= 0, KVM_IOCTL_ERROR(KVM_IRQ_LINE, ret));\n}\n\nstruct kvm_irq_routing *kvm_gsi_routing_create(void)\n{\n\tstruct kvm_irq_routing *routing;\n\tsize_t size;\n\n\tsize = sizeof(struct kvm_irq_routing);\n\t \n\tsize += KVM_MAX_IRQ_ROUTES * sizeof(struct kvm_irq_routing_entry);\n\trouting = calloc(1, size);\n\tassert(routing);\n\n\treturn routing;\n}\n\nvoid kvm_gsi_routing_irqchip_add(struct kvm_irq_routing *routing,\n\t\tuint32_t gsi, uint32_t pin)\n{\n\tint i;\n\n\tassert(routing);\n\tassert(routing->nr < KVM_MAX_IRQ_ROUTES);\n\n\ti = routing->nr;\n\trouting->entries[i].gsi = gsi;\n\trouting->entries[i].type = KVM_IRQ_ROUTING_IRQCHIP;\n\trouting->entries[i].flags = 0;\n\trouting->entries[i].u.irqchip.irqchip = 0;\n\trouting->entries[i].u.irqchip.pin = pin;\n\trouting->nr++;\n}\n\nint _kvm_gsi_routing_write(struct kvm_vm *vm, struct kvm_irq_routing *routing)\n{\n\tint ret;\n\n\tassert(routing);\n\tret = __vm_ioctl(vm, KVM_SET_GSI_ROUTING, routing);\n\tfree(routing);\n\n\treturn ret;\n}\n\nvoid kvm_gsi_routing_write(struct kvm_vm *vm, struct kvm_irq_routing *routing)\n{\n\tint ret;\n\n\tret = _kvm_gsi_routing_write(vm, routing);\n\tTEST_ASSERT(!ret, KVM_IOCTL_ERROR(KVM_SET_GSI_ROUTING, ret));\n}\n\n \nvoid vm_dump(FILE *stream, struct kvm_vm *vm, uint8_t indent)\n{\n\tint ctr;\n\tstruct userspace_mem_region *region;\n\tstruct kvm_vcpu *vcpu;\n\n\tfprintf(stream, \"%*smode: 0x%x\\n\", indent, \"\", vm->mode);\n\tfprintf(stream, \"%*sfd: %i\\n\", indent, \"\", vm->fd);\n\tfprintf(stream, \"%*spage_size: 0x%x\\n\", indent, \"\", vm->page_size);\n\tfprintf(stream, \"%*sMem Regions:\\n\", indent, \"\");\n\thash_for_each(vm->regions.slot_hash, ctr, region, slot_node) {\n\t\tfprintf(stream, \"%*sguest_phys: 0x%lx size: 0x%lx \"\n\t\t\t\"host_virt: %p\\n\", indent + 2, \"\",\n\t\t\t(uint64_t) region->region.guest_phys_addr,\n\t\t\t(uint64_t) region->region.memory_size,\n\t\t\tregion->host_mem);\n\t\tfprintf(stream, \"%*sunused_phy_pages: \", indent + 2, \"\");\n\t\tsparsebit_dump(stream, region->unused_phy_pages, 0);\n\t}\n\tfprintf(stream, \"%*sMapped Virtual Pages:\\n\", indent, \"\");\n\tsparsebit_dump(stream, vm->vpages_mapped, indent + 2);\n\tfprintf(stream, \"%*spgd_created: %u\\n\", indent, \"\",\n\t\tvm->pgd_created);\n\tif (vm->pgd_created) {\n\t\tfprintf(stream, \"%*sVirtual Translation Tables:\\n\",\n\t\t\tindent + 2, \"\");\n\t\tvirt_dump(stream, vm, indent + 4);\n\t}\n\tfprintf(stream, \"%*sVCPUs:\\n\", indent, \"\");\n\n\tlist_for_each_entry(vcpu, &vm->vcpus, list)\n\t\tvcpu_dump(stream, vcpu, indent + 2);\n}\n\n#define KVM_EXIT_STRING(x) {KVM_EXIT_##x, #x}\n\n \nstatic struct exit_reason {\n\tunsigned int reason;\n\tconst char *name;\n} exit_reasons_known[] = {\n\tKVM_EXIT_STRING(UNKNOWN),\n\tKVM_EXIT_STRING(EXCEPTION),\n\tKVM_EXIT_STRING(IO),\n\tKVM_EXIT_STRING(HYPERCALL),\n\tKVM_EXIT_STRING(DEBUG),\n\tKVM_EXIT_STRING(HLT),\n\tKVM_EXIT_STRING(MMIO),\n\tKVM_EXIT_STRING(IRQ_WINDOW_OPEN),\n\tKVM_EXIT_STRING(SHUTDOWN),\n\tKVM_EXIT_STRING(FAIL_ENTRY),\n\tKVM_EXIT_STRING(INTR),\n\tKVM_EXIT_STRING(SET_TPR),\n\tKVM_EXIT_STRING(TPR_ACCESS),\n\tKVM_EXIT_STRING(S390_SIEIC),\n\tKVM_EXIT_STRING(S390_RESET),\n\tKVM_EXIT_STRING(DCR),\n\tKVM_EXIT_STRING(NMI),\n\tKVM_EXIT_STRING(INTERNAL_ERROR),\n\tKVM_EXIT_STRING(OSI),\n\tKVM_EXIT_STRING(PAPR_HCALL),\n\tKVM_EXIT_STRING(S390_UCONTROL),\n\tKVM_EXIT_STRING(WATCHDOG),\n\tKVM_EXIT_STRING(S390_TSCH),\n\tKVM_EXIT_STRING(EPR),\n\tKVM_EXIT_STRING(SYSTEM_EVENT),\n\tKVM_EXIT_STRING(S390_STSI),\n\tKVM_EXIT_STRING(IOAPIC_EOI),\n\tKVM_EXIT_STRING(HYPERV),\n\tKVM_EXIT_STRING(ARM_NISV),\n\tKVM_EXIT_STRING(X86_RDMSR),\n\tKVM_EXIT_STRING(X86_WRMSR),\n\tKVM_EXIT_STRING(DIRTY_RING_FULL),\n\tKVM_EXIT_STRING(AP_RESET_HOLD),\n\tKVM_EXIT_STRING(X86_BUS_LOCK),\n\tKVM_EXIT_STRING(XEN),\n\tKVM_EXIT_STRING(RISCV_SBI),\n\tKVM_EXIT_STRING(RISCV_CSR),\n\tKVM_EXIT_STRING(NOTIFY),\n#ifdef KVM_EXIT_MEMORY_NOT_PRESENT\n\tKVM_EXIT_STRING(MEMORY_NOT_PRESENT),\n#endif\n};\n\n \nconst char *exit_reason_str(unsigned int exit_reason)\n{\n\tunsigned int n1;\n\n\tfor (n1 = 0; n1 < ARRAY_SIZE(exit_reasons_known); n1++) {\n\t\tif (exit_reason == exit_reasons_known[n1].reason)\n\t\t\treturn exit_reasons_known[n1].name;\n\t}\n\n\treturn \"Unknown\";\n}\n\n \nvm_paddr_t vm_phy_pages_alloc(struct kvm_vm *vm, size_t num,\n\t\t\t      vm_paddr_t paddr_min, uint32_t memslot)\n{\n\tstruct userspace_mem_region *region;\n\tsparsebit_idx_t pg, base;\n\n\tTEST_ASSERT(num > 0, \"Must allocate at least one page\");\n\n\tTEST_ASSERT((paddr_min % vm->page_size) == 0, \"Min physical address \"\n\t\t\"not divisible by page size.\\n\"\n\t\t\"  paddr_min: 0x%lx page_size: 0x%x\",\n\t\tpaddr_min, vm->page_size);\n\n\tregion = memslot2region(vm, memslot);\n\tbase = pg = paddr_min >> vm->page_shift;\n\n\tdo {\n\t\tfor (; pg < base + num; ++pg) {\n\t\t\tif (!sparsebit_is_set(region->unused_phy_pages, pg)) {\n\t\t\t\tbase = pg = sparsebit_next_set(region->unused_phy_pages, pg);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} while (pg && pg != base + num);\n\n\tif (pg == 0) {\n\t\tfprintf(stderr, \"No guest physical page available, \"\n\t\t\t\"paddr_min: 0x%lx page_size: 0x%x memslot: %u\\n\",\n\t\t\tpaddr_min, vm->page_size, memslot);\n\t\tfputs(\"---- vm dump ----\\n\", stderr);\n\t\tvm_dump(stderr, vm, 2);\n\t\tabort();\n\t}\n\n\tfor (pg = base; pg < base + num; ++pg)\n\t\tsparsebit_clear(region->unused_phy_pages, pg);\n\n\treturn base * vm->page_size;\n}\n\nvm_paddr_t vm_phy_page_alloc(struct kvm_vm *vm, vm_paddr_t paddr_min,\n\t\t\t     uint32_t memslot)\n{\n\treturn vm_phy_pages_alloc(vm, 1, paddr_min, memslot);\n}\n\nvm_paddr_t vm_alloc_page_table(struct kvm_vm *vm)\n{\n\treturn vm_phy_page_alloc(vm, KVM_GUEST_PAGE_TABLE_MIN_PADDR,\n\t\t\t\t vm->memslots[MEM_REGION_PT]);\n}\n\n \nvoid *addr_gva2hva(struct kvm_vm *vm, vm_vaddr_t gva)\n{\n\treturn addr_gpa2hva(vm, addr_gva2gpa(vm, gva));\n}\n\nunsigned long __weak vm_compute_max_gfn(struct kvm_vm *vm)\n{\n\treturn ((1ULL << vm->pa_bits) >> vm->page_shift) - 1;\n}\n\nstatic unsigned int vm_calc_num_pages(unsigned int num_pages,\n\t\t\t\t      unsigned int page_shift,\n\t\t\t\t      unsigned int new_page_shift,\n\t\t\t\t      bool ceil)\n{\n\tunsigned int n = 1 << (new_page_shift - page_shift);\n\n\tif (page_shift >= new_page_shift)\n\t\treturn num_pages * (1 << (page_shift - new_page_shift));\n\n\treturn num_pages / n + !!(ceil && num_pages % n);\n}\n\nstatic inline int getpageshift(void)\n{\n\treturn __builtin_ffs(getpagesize()) - 1;\n}\n\nunsigned int\nvm_num_host_pages(enum vm_guest_mode mode, unsigned int num_guest_pages)\n{\n\treturn vm_calc_num_pages(num_guest_pages,\n\t\t\t\t vm_guest_mode_params[mode].page_shift,\n\t\t\t\t getpageshift(), true);\n}\n\nunsigned int\nvm_num_guest_pages(enum vm_guest_mode mode, unsigned int num_host_pages)\n{\n\treturn vm_calc_num_pages(num_host_pages, getpageshift(),\n\t\t\t\t vm_guest_mode_params[mode].page_shift, false);\n}\n\nunsigned int vm_calc_num_guest_pages(enum vm_guest_mode mode, size_t size)\n{\n\tunsigned int n;\n\tn = DIV_ROUND_UP(size, vm_guest_mode_params[mode].page_size);\n\treturn vm_adjust_num_guest_pages(mode, n);\n}\n\n \nstruct kvm_stats_desc *read_stats_descriptors(int stats_fd,\n\t\t\t\t\t      struct kvm_stats_header *header)\n{\n\tstruct kvm_stats_desc *stats_desc;\n\tssize_t desc_size, total_size, ret;\n\n\tdesc_size = get_stats_descriptor_size(header);\n\ttotal_size = header->num_desc * desc_size;\n\n\tstats_desc = calloc(header->num_desc, desc_size);\n\tTEST_ASSERT(stats_desc, \"Allocate memory for stats descriptors\");\n\n\tret = pread(stats_fd, stats_desc, total_size, header->desc_offset);\n\tTEST_ASSERT(ret == total_size, \"Read KVM stats descriptors\");\n\n\treturn stats_desc;\n}\n\n \nvoid read_stat_data(int stats_fd, struct kvm_stats_header *header,\n\t\t    struct kvm_stats_desc *desc, uint64_t *data,\n\t\t    size_t max_elements)\n{\n\tsize_t nr_elements = min_t(ssize_t, desc->size, max_elements);\n\tsize_t size = nr_elements * sizeof(*data);\n\tssize_t ret;\n\n\tTEST_ASSERT(desc->size, \"No elements in stat '%s'\", desc->name);\n\tTEST_ASSERT(max_elements, \"Zero elements requested for stat '%s'\", desc->name);\n\n\tret = pread(stats_fd, data, size,\n\t\t    header->data_offset + desc->offset);\n\n\tTEST_ASSERT(ret >= 0, \"pread() failed on stat '%s', errno: %i (%s)\",\n\t\t    desc->name, errno, strerror(errno));\n\tTEST_ASSERT(ret == size,\n\t\t    \"pread() on stat '%s' read %ld bytes, wanted %lu bytes\",\n\t\t    desc->name, size, ret);\n}\n\n \nvoid __vm_get_stat(struct kvm_vm *vm, const char *stat_name, uint64_t *data,\n\t\t   size_t max_elements)\n{\n\tstruct kvm_stats_desc *desc;\n\tsize_t size_desc;\n\tint i;\n\n\tif (!vm->stats_fd) {\n\t\tvm->stats_fd = vm_get_stats_fd(vm);\n\t\tread_stats_header(vm->stats_fd, &vm->stats_header);\n\t\tvm->stats_desc = read_stats_descriptors(vm->stats_fd,\n\t\t\t\t\t\t\t&vm->stats_header);\n\t}\n\n\tsize_desc = get_stats_descriptor_size(&vm->stats_header);\n\n\tfor (i = 0; i < vm->stats_header.num_desc; ++i) {\n\t\tdesc = (void *)vm->stats_desc + (i * size_desc);\n\n\t\tif (strcmp(desc->name, stat_name))\n\t\t\tcontinue;\n\n\t\tread_stat_data(vm->stats_fd, &vm->stats_header, desc,\n\t\t\t       data, max_elements);\n\n\t\tbreak;\n\t}\n}\n\n__weak void kvm_arch_vm_post_create(struct kvm_vm *vm)\n{\n}\n\n__weak void kvm_selftest_arch_init(void)\n{\n}\n\nvoid __attribute((constructor)) kvm_selftest_init(void)\n{\n\t \n\tsetbuf(stdout, NULL);\n\n\tkvm_selftest_arch_init();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}