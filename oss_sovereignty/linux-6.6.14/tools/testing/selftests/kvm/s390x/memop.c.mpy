{
  "module_name": "memop.c",
  "hash_id": "26f60abcc095b22b1f8ab15b371af8646f0ae3ffaa7b014f34029c1b05dc4bf9",
  "original_prompt": "Ingested from linux-6.6.14/tools/testing/selftests/kvm/s390x/memop.c",
  "human_readable_source": "\n \n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/ioctl.h>\n#include <pthread.h>\n\n#include <linux/bits.h>\n\n#include \"test_util.h\"\n#include \"kvm_util.h\"\n#include \"kselftest.h\"\n\nenum mop_target {\n\tLOGICAL,\n\tSIDA,\n\tABSOLUTE,\n\tINVALID,\n};\n\nenum mop_access_mode {\n\tREAD,\n\tWRITE,\n\tCMPXCHG,\n};\n\nstruct mop_desc {\n\tuintptr_t gaddr;\n\tuintptr_t gaddr_v;\n\tuint64_t set_flags;\n\tunsigned int f_check : 1;\n\tunsigned int f_inject : 1;\n\tunsigned int f_key : 1;\n\tunsigned int _gaddr_v : 1;\n\tunsigned int _set_flags : 1;\n\tunsigned int _sida_offset : 1;\n\tunsigned int _ar : 1;\n\tuint32_t size;\n\tenum mop_target target;\n\tenum mop_access_mode mode;\n\tvoid *buf;\n\tuint32_t sida_offset;\n\tvoid *old;\n\tuint8_t old_value[16];\n\tbool *cmpxchg_success;\n\tuint8_t ar;\n\tuint8_t key;\n};\n\nconst uint8_t NO_KEY = 0xff;\n\nstatic struct kvm_s390_mem_op ksmo_from_desc(struct mop_desc *desc)\n{\n\tstruct kvm_s390_mem_op ksmo = {\n\t\t.gaddr = (uintptr_t)desc->gaddr,\n\t\t.size = desc->size,\n\t\t.buf = ((uintptr_t)desc->buf),\n\t\t.reserved = \"ignored_ignored_ignored_ignored\"\n\t};\n\n\tswitch (desc->target) {\n\tcase LOGICAL:\n\t\tif (desc->mode == READ)\n\t\t\tksmo.op = KVM_S390_MEMOP_LOGICAL_READ;\n\t\tif (desc->mode == WRITE)\n\t\t\tksmo.op = KVM_S390_MEMOP_LOGICAL_WRITE;\n\t\tbreak;\n\tcase SIDA:\n\t\tif (desc->mode == READ)\n\t\t\tksmo.op = KVM_S390_MEMOP_SIDA_READ;\n\t\tif (desc->mode == WRITE)\n\t\t\tksmo.op = KVM_S390_MEMOP_SIDA_WRITE;\n\t\tbreak;\n\tcase ABSOLUTE:\n\t\tif (desc->mode == READ)\n\t\t\tksmo.op = KVM_S390_MEMOP_ABSOLUTE_READ;\n\t\tif (desc->mode == WRITE)\n\t\t\tksmo.op = KVM_S390_MEMOP_ABSOLUTE_WRITE;\n\t\tif (desc->mode == CMPXCHG) {\n\t\t\tksmo.op = KVM_S390_MEMOP_ABSOLUTE_CMPXCHG;\n\t\t\tksmo.old_addr = (uint64_t)desc->old;\n\t\t\tmemcpy(desc->old_value, desc->old, desc->size);\n\t\t}\n\t\tbreak;\n\tcase INVALID:\n\t\tksmo.op = -1;\n\t}\n\tif (desc->f_check)\n\t\tksmo.flags |= KVM_S390_MEMOP_F_CHECK_ONLY;\n\tif (desc->f_inject)\n\t\tksmo.flags |= KVM_S390_MEMOP_F_INJECT_EXCEPTION;\n\tif (desc->_set_flags)\n\t\tksmo.flags = desc->set_flags;\n\tif (desc->f_key && desc->key != NO_KEY) {\n\t\tksmo.flags |= KVM_S390_MEMOP_F_SKEY_PROTECTION;\n\t\tksmo.key = desc->key;\n\t}\n\tif (desc->_ar)\n\t\tksmo.ar = desc->ar;\n\telse\n\t\tksmo.ar = 0;\n\tif (desc->_sida_offset)\n\t\tksmo.sida_offset = desc->sida_offset;\n\n\treturn ksmo;\n}\n\nstruct test_info {\n\tstruct kvm_vm *vm;\n\tstruct kvm_vcpu *vcpu;\n};\n\n#define PRINT_MEMOP false\nstatic void print_memop(struct kvm_vcpu *vcpu, const struct kvm_s390_mem_op *ksmo)\n{\n\tif (!PRINT_MEMOP)\n\t\treturn;\n\n\tif (!vcpu)\n\t\tprintf(\"vm memop(\");\n\telse\n\t\tprintf(\"vcpu memop(\");\n\tswitch (ksmo->op) {\n\tcase KVM_S390_MEMOP_LOGICAL_READ:\n\t\tprintf(\"LOGICAL, READ, \");\n\t\tbreak;\n\tcase KVM_S390_MEMOP_LOGICAL_WRITE:\n\t\tprintf(\"LOGICAL, WRITE, \");\n\t\tbreak;\n\tcase KVM_S390_MEMOP_SIDA_READ:\n\t\tprintf(\"SIDA, READ, \");\n\t\tbreak;\n\tcase KVM_S390_MEMOP_SIDA_WRITE:\n\t\tprintf(\"SIDA, WRITE, \");\n\t\tbreak;\n\tcase KVM_S390_MEMOP_ABSOLUTE_READ:\n\t\tprintf(\"ABSOLUTE, READ, \");\n\t\tbreak;\n\tcase KVM_S390_MEMOP_ABSOLUTE_WRITE:\n\t\tprintf(\"ABSOLUTE, WRITE, \");\n\t\tbreak;\n\tcase KVM_S390_MEMOP_ABSOLUTE_CMPXCHG:\n\t\tprintf(\"ABSOLUTE, CMPXCHG, \");\n\t\tbreak;\n\t}\n\tprintf(\"gaddr=%llu, size=%u, buf=%llu, ar=%u, key=%u, old_addr=%llx\",\n\t       ksmo->gaddr, ksmo->size, ksmo->buf, ksmo->ar, ksmo->key,\n\t       ksmo->old_addr);\n\tif (ksmo->flags & KVM_S390_MEMOP_F_CHECK_ONLY)\n\t\tprintf(\", CHECK_ONLY\");\n\tif (ksmo->flags & KVM_S390_MEMOP_F_INJECT_EXCEPTION)\n\t\tprintf(\", INJECT_EXCEPTION\");\n\tif (ksmo->flags & KVM_S390_MEMOP_F_SKEY_PROTECTION)\n\t\tprintf(\", SKEY_PROTECTION\");\n\tputs(\")\");\n}\n\nstatic int err_memop_ioctl(struct test_info info, struct kvm_s390_mem_op *ksmo,\n\t\t\t   struct mop_desc *desc)\n{\n\tstruct kvm_vcpu *vcpu = info.vcpu;\n\n\tif (!vcpu)\n\t\treturn __vm_ioctl(info.vm, KVM_S390_MEM_OP, ksmo);\n\telse\n\t\treturn __vcpu_ioctl(vcpu, KVM_S390_MEM_OP, ksmo);\n}\n\nstatic void memop_ioctl(struct test_info info, struct kvm_s390_mem_op *ksmo,\n\t\t\tstruct mop_desc *desc)\n{\n\tint r;\n\n\tr = err_memop_ioctl(info, ksmo, desc);\n\tif (ksmo->op == KVM_S390_MEMOP_ABSOLUTE_CMPXCHG) {\n\t\tif (desc->cmpxchg_success) {\n\t\t\tint diff = memcmp(desc->old_value, desc->old, desc->size);\n\t\t\t*desc->cmpxchg_success = !diff;\n\t\t}\n\t}\n\tTEST_ASSERT(!r, __KVM_IOCTL_ERROR(\"KVM_S390_MEM_OP\", r));\n}\n\n#define MEMOP(err, info_p, mop_target_p, access_mode_p, buf_p, size_p, ...)\t\\\n({\t\t\t\t\t\t\t\t\t\t\\\n\tstruct test_info __info = (info_p);\t\t\t\t\t\\\n\tstruct mop_desc __desc = {\t\t\t\t\t\t\\\n\t\t.target = (mop_target_p),\t\t\t\t\t\\\n\t\t.mode = (access_mode_p),\t\t\t\t\t\\\n\t\t.buf = (buf_p),\t\t\t\t\t\t\t\\\n\t\t.size = (size_p),\t\t\t\t\t\t\\\n\t\t__VA_ARGS__\t\t\t\t\t\t\t\\\n\t};\t\t\t\t\t\t\t\t\t\\\n\tstruct kvm_s390_mem_op __ksmo;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tif (__desc._gaddr_v) {\t\t\t\t\t\t\t\\\n\t\tif (__desc.target == ABSOLUTE)\t\t\t\t\t\\\n\t\t\t__desc.gaddr = addr_gva2gpa(__info.vm, __desc.gaddr_v);\t\\\n\t\telse\t\t\t\t\t\t\t\t\\\n\t\t\t__desc.gaddr = __desc.gaddr_v;\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n\t__ksmo = ksmo_from_desc(&__desc);\t\t\t\t\t\\\n\tprint_memop(__info.vcpu, &__ksmo);\t\t\t\t\t\\\n\terr##memop_ioctl(__info, &__ksmo, &__desc);\t\t\t\t\\\n})\n\n#define MOP(...) MEMOP(, __VA_ARGS__)\n#define ERR_MOP(...) MEMOP(err_, __VA_ARGS__)\n\n#define GADDR(a) .gaddr = ((uintptr_t)a)\n#define GADDR_V(v) ._gaddr_v = 1, .gaddr_v = ((uintptr_t)v)\n#define CHECK_ONLY .f_check = 1\n#define SET_FLAGS(f) ._set_flags = 1, .set_flags = (f)\n#define SIDA_OFFSET(o) ._sida_offset = 1, .sida_offset = (o)\n#define AR(a) ._ar = 1, .ar = (a)\n#define KEY(a) .f_key = 1, .key = (a)\n#define INJECT .f_inject = 1\n#define CMPXCHG_OLD(o) .old = (o)\n#define CMPXCHG_SUCCESS(s) .cmpxchg_success = (s)\n\n#define CHECK_N_DO(f, ...) ({ f(__VA_ARGS__, CHECK_ONLY); f(__VA_ARGS__); })\n\n#define PAGE_SHIFT 12\n#define PAGE_SIZE (1ULL << PAGE_SHIFT)\n#define PAGE_MASK (~(PAGE_SIZE - 1))\n#define CR0_FETCH_PROTECTION_OVERRIDE\t(1UL << (63 - 38))\n#define CR0_STORAGE_PROTECTION_OVERRIDE\t(1UL << (63 - 39))\n\nstatic uint8_t __aligned(PAGE_SIZE) mem1[65536];\nstatic uint8_t __aligned(PAGE_SIZE) mem2[65536];\n\nstruct test_default {\n\tstruct kvm_vm *kvm_vm;\n\tstruct test_info vm;\n\tstruct test_info vcpu;\n\tstruct kvm_run *run;\n\tint size;\n};\n\nstatic struct test_default test_default_init(void *guest_code)\n{\n\tstruct kvm_vcpu *vcpu;\n\tstruct test_default t;\n\n\tt.size = min((size_t)kvm_check_cap(KVM_CAP_S390_MEM_OP), sizeof(mem1));\n\tt.kvm_vm = vm_create_with_one_vcpu(&vcpu, guest_code);\n\tt.vm = (struct test_info) { t.kvm_vm, NULL };\n\tt.vcpu = (struct test_info) { t.kvm_vm, vcpu };\n\tt.run = vcpu->run;\n\treturn t;\n}\n\nenum stage {\n\t \n\tSTAGE_INITED,\n\t \n\tSTAGE_IDLED,\n\t \n\tSTAGE_SKEYS_SET,\n\t \n\tSTAGE_COPIED,\n\t \n\tSTAGE_DONE,\n};\n\n#define HOST_SYNC(info_p, stage)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct test_info __info = (info_p);\t\t\t\t\\\n\tstruct kvm_vcpu *__vcpu = __info.vcpu;\t\t\t\t\\\n\tstruct ucall uc;\t\t\t\t\t\t\\\n\tint __stage = (stage);\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tvcpu_run(__vcpu);\t\t\t\t\t\t\\\n\tget_ucall(__vcpu, &uc);\t\t\t\t\t\t\\\n\tif (uc.cmd == UCALL_ABORT) {\t\t\t\t\t\\\n\t\tREPORT_GUEST_ASSERT(uc);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tTEST_ASSERT_EQ(uc.cmd, UCALL_SYNC);\t\t\t\t\\\n\tTEST_ASSERT_EQ(uc.args[1], __stage);\t\t\t\t\\\n})\t\t\t\t\t\t\t\t\t\\\n\nstatic void prepare_mem12(void)\n{\n\tint i;\n\n\tfor (i = 0; i < sizeof(mem1); i++)\n\t\tmem1[i] = rand();\n\tmemset(mem2, 0xaa, sizeof(mem2));\n}\n\n#define ASSERT_MEM_EQ(p1, p2, size) \\\n\tTEST_ASSERT(!memcmp(p1, p2, size), \"Memory contents do not match!\")\n\nstatic void default_write_read(struct test_info copy_cpu, struct test_info mop_cpu,\n\t\t\t       enum mop_target mop_target, uint32_t size, uint8_t key)\n{\n\tprepare_mem12();\n\tCHECK_N_DO(MOP, mop_cpu, mop_target, WRITE, mem1, size,\n\t\t   GADDR_V(mem1), KEY(key));\n\tHOST_SYNC(copy_cpu, STAGE_COPIED);\n\tCHECK_N_DO(MOP, mop_cpu, mop_target, READ, mem2, size,\n\t\t   GADDR_V(mem2), KEY(key));\n\tASSERT_MEM_EQ(mem1, mem2, size);\n}\n\nstatic void default_read(struct test_info copy_cpu, struct test_info mop_cpu,\n\t\t\t enum mop_target mop_target, uint32_t size, uint8_t key)\n{\n\tprepare_mem12();\n\tCHECK_N_DO(MOP, mop_cpu, mop_target, WRITE, mem1, size, GADDR_V(mem1));\n\tHOST_SYNC(copy_cpu, STAGE_COPIED);\n\tCHECK_N_DO(MOP, mop_cpu, mop_target, READ, mem2, size,\n\t\t   GADDR_V(mem2), KEY(key));\n\tASSERT_MEM_EQ(mem1, mem2, size);\n}\n\nstatic void default_cmpxchg(struct test_default *test, uint8_t key)\n{\n\tfor (int size = 1; size <= 16; size *= 2) {\n\t\tfor (int offset = 0; offset < 16; offset += size) {\n\t\t\tuint8_t __aligned(16) new[16] = {};\n\t\t\tuint8_t __aligned(16) old[16];\n\t\t\tbool succ;\n\n\t\t\tprepare_mem12();\n\t\t\tdefault_write_read(test->vcpu, test->vcpu, LOGICAL, 16, NO_KEY);\n\n\t\t\tmemcpy(&old, mem1, 16);\n\t\t\tMOP(test->vm, ABSOLUTE, CMPXCHG, new + offset,\n\t\t\t    size, GADDR_V(mem1 + offset),\n\t\t\t    CMPXCHG_OLD(old + offset),\n\t\t\t    CMPXCHG_SUCCESS(&succ), KEY(key));\n\t\t\tHOST_SYNC(test->vcpu, STAGE_COPIED);\n\t\t\tMOP(test->vm, ABSOLUTE, READ, mem2, 16, GADDR_V(mem2));\n\t\t\tTEST_ASSERT(succ, \"exchange of values should succeed\");\n\t\t\tmemcpy(mem1 + offset, new + offset, size);\n\t\t\tASSERT_MEM_EQ(mem1, mem2, 16);\n\n\t\t\tmemcpy(&old, mem1, 16);\n\t\t\tnew[offset]++;\n\t\t\told[offset]++;\n\t\t\tMOP(test->vm, ABSOLUTE, CMPXCHG, new + offset,\n\t\t\t    size, GADDR_V(mem1 + offset),\n\t\t\t    CMPXCHG_OLD(old + offset),\n\t\t\t    CMPXCHG_SUCCESS(&succ), KEY(key));\n\t\t\tHOST_SYNC(test->vcpu, STAGE_COPIED);\n\t\t\tMOP(test->vm, ABSOLUTE, READ, mem2, 16, GADDR_V(mem2));\n\t\t\tTEST_ASSERT(!succ, \"exchange of values should not succeed\");\n\t\t\tASSERT_MEM_EQ(mem1, mem2, 16);\n\t\t\tASSERT_MEM_EQ(&old, mem1, 16);\n\t\t}\n\t}\n}\n\nstatic void guest_copy(void)\n{\n\tGUEST_SYNC(STAGE_INITED);\n\tmemcpy(&mem2, &mem1, sizeof(mem2));\n\tGUEST_SYNC(STAGE_COPIED);\n}\n\nstatic void test_copy(void)\n{\n\tstruct test_default t = test_default_init(guest_copy);\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\n\tdefault_write_read(t.vcpu, t.vcpu, LOGICAL, t.size, NO_KEY);\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void set_storage_key_range(void *addr, size_t len, uint8_t key)\n{\n\tuintptr_t _addr, abs, i;\n\tint not_mapped = 0;\n\n\t_addr = (uintptr_t)addr;\n\tfor (i = _addr & PAGE_MASK; i < _addr + len; i += PAGE_SIZE) {\n\t\tabs = i;\n\t\tasm volatile (\n\t\t\t       \"lra\t%[abs], 0(0,%[abs])\\n\"\n\t\t\t\"\tjz\t0f\\n\"\n\t\t\t\"\tllill\t%[not_mapped],1\\n\"\n\t\t\t\"\tj\t1f\\n\"\n\t\t\t\"0:\tsske\t%[key], %[abs]\\n\"\n\t\t\t\"1:\"\n\t\t\t: [abs] \"+&a\" (abs), [not_mapped] \"+r\" (not_mapped)\n\t\t\t: [key] \"r\" (key)\n\t\t\t: \"cc\"\n\t\t);\n\t\tGUEST_ASSERT_EQ(not_mapped, 0);\n\t}\n}\n\nstatic void guest_copy_key(void)\n{\n\tset_storage_key_range(mem1, sizeof(mem1), 0x90);\n\tset_storage_key_range(mem2, sizeof(mem2), 0x90);\n\tGUEST_SYNC(STAGE_SKEYS_SET);\n\n\tfor (;;) {\n\t\tmemcpy(&mem2, &mem1, sizeof(mem2));\n\t\tGUEST_SYNC(STAGE_COPIED);\n\t}\n}\n\nstatic void test_copy_key(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key);\n\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tdefault_write_read(t.vcpu, t.vm, ABSOLUTE, t.size, NO_KEY);\n\n\t \n\tdefault_write_read(t.vcpu, t.vcpu, LOGICAL, t.size, 0);\n\tdefault_write_read(t.vcpu, t.vcpu, LOGICAL, t.size, 9);\n\tdefault_write_read(t.vcpu, t.vm, ABSOLUTE, t.size, 0);\n\tdefault_write_read(t.vcpu, t.vm, ABSOLUTE, t.size, 9);\n\t \n\tdefault_write_read(t.vcpu, t.vcpu, LOGICAL, 1, 0);\n\tdefault_write_read(t.vcpu, t.vcpu, LOGICAL, 1, 9);\n\tdefault_write_read(t.vcpu, t.vm, ABSOLUTE, 1, 0);\n\tdefault_write_read(t.vcpu, t.vm, ABSOLUTE, 1, 9);\n\n\t \n\tdefault_read(t.vcpu, t.vcpu, LOGICAL, t.size, 2);\n\tdefault_read(t.vcpu, t.vm, ABSOLUTE, t.size, 2);\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_cmpxchg_key(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key);\n\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\tdefault_cmpxchg(&t, NO_KEY);\n\tdefault_cmpxchg(&t, 0);\n\tdefault_cmpxchg(&t, 9);\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic __uint128_t cut_to_size(int size, __uint128_t val)\n{\n\tswitch (size) {\n\tcase 1:\n\t\treturn (uint8_t)val;\n\tcase 2:\n\t\treturn (uint16_t)val;\n\tcase 4:\n\t\treturn (uint32_t)val;\n\tcase 8:\n\t\treturn (uint64_t)val;\n\tcase 16:\n\t\treturn val;\n\t}\n\tGUEST_FAIL(\"Invalid size = %u\", size);\n\treturn 0;\n}\n\nstatic bool popcount_eq(__uint128_t a, __uint128_t b)\n{\n\tunsigned int count_a, count_b;\n\n\tcount_a = __builtin_popcountl((uint64_t)(a >> 64)) +\n\t\t  __builtin_popcountl((uint64_t)a);\n\tcount_b = __builtin_popcountl((uint64_t)(b >> 64)) +\n\t\t  __builtin_popcountl((uint64_t)b);\n\treturn count_a == count_b;\n}\n\nstatic __uint128_t rotate(int size, __uint128_t val, int amount)\n{\n\tunsigned int bits = size * 8;\n\n\tamount = (amount + bits) % bits;\n\tval = cut_to_size(size, val);\n\treturn (val << (bits - amount)) | (val >> amount);\n}\n\nconst unsigned int max_block = 16;\n\nstatic void choose_block(bool guest, int i, int *size, int *offset)\n{\n\tunsigned int rand;\n\n\trand = i;\n\tif (guest) {\n\t\trand = rand * 19 + 11;\n\t\t*size = 1 << ((rand % 3) + 2);\n\t\trand = rand * 19 + 11;\n\t\t*offset = (rand % max_block) & ~(*size - 1);\n\t} else {\n\t\trand = rand * 17 + 5;\n\t\t*size = 1 << (rand % 5);\n\t\trand = rand * 17 + 5;\n\t\t*offset = (rand % max_block) & ~(*size - 1);\n\t}\n}\n\nstatic __uint128_t permutate_bits(bool guest, int i, int size, __uint128_t old)\n{\n\tunsigned int rand;\n\tint amount;\n\tbool swap;\n\n\trand = i;\n\trand = rand * 3 + 1;\n\tif (guest)\n\t\trand = rand * 3 + 1;\n\tswap = rand % 2 == 0;\n\tif (swap) {\n\t\tint i, j;\n\t\t__uint128_t new;\n\t\tuint8_t byte0, byte1;\n\n\t\trand = rand * 3 + 1;\n\t\ti = rand % size;\n\t\trand = rand * 3 + 1;\n\t\tj = rand % size;\n\t\tif (i == j)\n\t\t\treturn old;\n\t\tnew = rotate(16, old, i * 8);\n\t\tbyte0 = new & 0xff;\n\t\tnew &= ~0xff;\n\t\tnew = rotate(16, new, -i * 8);\n\t\tnew = rotate(16, new, j * 8);\n\t\tbyte1 = new & 0xff;\n\t\tnew = (new & ~0xff) | byte0;\n\t\tnew = rotate(16, new, -j * 8);\n\t\tnew = rotate(16, new, i * 8);\n\t\tnew = new | byte1;\n\t\tnew = rotate(16, new, -i * 8);\n\t\treturn new;\n\t}\n\trand = rand * 3 + 1;\n\tamount = rand % (size * 8);\n\treturn rotate(size, old, amount);\n}\n\nstatic bool _cmpxchg(int size, void *target, __uint128_t *old_addr, __uint128_t new)\n{\n\tbool ret;\n\n\tswitch (size) {\n\tcase 4: {\n\t\t\tuint32_t old = *old_addr;\n\n\t\t\tasm volatile (\"cs %[old],%[new],%[address]\"\n\t\t\t    : [old] \"+d\" (old),\n\t\t\t      [address] \"+Q\" (*(uint32_t *)(target))\n\t\t\t    : [new] \"d\" ((uint32_t)new)\n\t\t\t    : \"cc\"\n\t\t\t);\n\t\t\tret = old == (uint32_t)*old_addr;\n\t\t\t*old_addr = old;\n\t\t\treturn ret;\n\t\t}\n\tcase 8: {\n\t\t\tuint64_t old = *old_addr;\n\n\t\t\tasm volatile (\"csg %[old],%[new],%[address]\"\n\t\t\t    : [old] \"+d\" (old),\n\t\t\t      [address] \"+Q\" (*(uint64_t *)(target))\n\t\t\t    : [new] \"d\" ((uint64_t)new)\n\t\t\t    : \"cc\"\n\t\t\t);\n\t\t\tret = old == (uint64_t)*old_addr;\n\t\t\t*old_addr = old;\n\t\t\treturn ret;\n\t\t}\n\tcase 16: {\n\t\t\t__uint128_t old = *old_addr;\n\n\t\t\tasm volatile (\"cdsg %[old],%[new],%[address]\"\n\t\t\t    : [old] \"+d\" (old),\n\t\t\t      [address] \"+Q\" (*(__uint128_t *)(target))\n\t\t\t    : [new] \"d\" (new)\n\t\t\t    : \"cc\"\n\t\t\t);\n\t\t\tret = old == *old_addr;\n\t\t\t*old_addr = old;\n\t\t\treturn ret;\n\t\t}\n\t}\n\tGUEST_FAIL(\"Invalid size = %u\", size);\n\treturn 0;\n}\n\nconst unsigned int cmpxchg_iter_outer = 100, cmpxchg_iter_inner = 10000;\n\nstatic void guest_cmpxchg_key(void)\n{\n\tint size, offset;\n\t__uint128_t old, new;\n\n\tset_storage_key_range(mem1, max_block, 0x10);\n\tset_storage_key_range(mem2, max_block, 0x10);\n\tGUEST_SYNC(STAGE_SKEYS_SET);\n\n\tfor (int i = 0; i < cmpxchg_iter_outer; i++) {\n\t\tdo {\n\t\t\told = 1;\n\t\t} while (!_cmpxchg(16, mem1, &old, 0));\n\t\tfor (int j = 0; j < cmpxchg_iter_inner; j++) {\n\t\t\tchoose_block(true, i + j, &size, &offset);\n\t\t\tdo {\n\t\t\t\tnew = permutate_bits(true, i + j, size, old);\n\t\t\t} while (!_cmpxchg(size, mem2 + offset, &old, new));\n\t\t}\n\t}\n\n\tGUEST_SYNC(STAGE_DONE);\n}\n\nstatic void *run_guest(void *data)\n{\n\tstruct test_info *info = data;\n\n\tHOST_SYNC(*info, STAGE_DONE);\n\treturn NULL;\n}\n\nstatic char *quad_to_char(__uint128_t *quad, int size)\n{\n\treturn ((char *)quad) + (sizeof(*quad) - size);\n}\n\nstatic void test_cmpxchg_key_concurrent(void)\n{\n\tstruct test_default t = test_default_init(guest_cmpxchg_key);\n\tint size, offset;\n\t__uint128_t old, new;\n\tbool success;\n\tpthread_t thread;\n\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\tprepare_mem12();\n\tMOP(t.vcpu, LOGICAL, WRITE, mem1, max_block, GADDR_V(mem2));\n\tpthread_create(&thread, NULL, run_guest, &t.vcpu);\n\n\tfor (int i = 0; i < cmpxchg_iter_outer; i++) {\n\t\tdo {\n\t\t\told = 0;\n\t\t\tnew = 1;\n\t\t\tMOP(t.vm, ABSOLUTE, CMPXCHG, &new,\n\t\t\t    sizeof(new), GADDR_V(mem1),\n\t\t\t    CMPXCHG_OLD(&old),\n\t\t\t    CMPXCHG_SUCCESS(&success), KEY(1));\n\t\t} while (!success);\n\t\tfor (int j = 0; j < cmpxchg_iter_inner; j++) {\n\t\t\tchoose_block(false, i + j, &size, &offset);\n\t\t\tdo {\n\t\t\t\tnew = permutate_bits(false, i + j, size, old);\n\t\t\t\tMOP(t.vm, ABSOLUTE, CMPXCHG, quad_to_char(&new, size),\n\t\t\t\t    size, GADDR_V(mem2 + offset),\n\t\t\t\t    CMPXCHG_OLD(quad_to_char(&old, size)),\n\t\t\t\t    CMPXCHG_SUCCESS(&success), KEY(1));\n\t\t\t} while (!success);\n\t\t}\n\t}\n\n\tpthread_join(thread, NULL);\n\n\tMOP(t.vcpu, LOGICAL, READ, mem2, max_block, GADDR_V(mem2));\n\tTEST_ASSERT(popcount_eq(*(__uint128_t *)mem1, *(__uint128_t *)mem2),\n\t\t    \"Must retain number of set bits\");\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void guest_copy_key_fetch_prot(void)\n{\n\t \n\tGUEST_SYNC(STAGE_INITED);\n\t \n\tset_storage_key_range(mem1, sizeof(mem1), 0x98);\n\tset_storage_key_range(mem2, sizeof(mem2), 0x98);\n\tGUEST_SYNC(STAGE_SKEYS_SET);\n\n\tfor (;;) {\n\t\tmemcpy(&mem2, &mem1, sizeof(mem2));\n\t\tGUEST_SYNC(STAGE_COPIED);\n\t}\n}\n\nstatic void test_copy_key_storage_prot_override(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot);\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tt.run->s.regs.crs[0] |= CR0_STORAGE_PROTECTION_OVERRIDE;\n\tt.run->kvm_dirty_regs = KVM_SYNC_CRS;\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tdefault_write_read(t.vcpu, t.vcpu, LOGICAL, t.size, 2);\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_copy_key_fetch_prot(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot);\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tdefault_read(t.vcpu, t.vcpu, LOGICAL, t.size, 9);\n\tdefault_read(t.vcpu, t.vm, ABSOLUTE, t.size, 9);\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\n#define ERR_PROT_MOP(...)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\t\\\n\tint rv;\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\trv = ERR_MOP(__VA_ARGS__);\t\t\t\t\t\t\\\n\tTEST_ASSERT(rv == 4, \"Should result in protection exception\");\t\t\\\n})\n\nstatic void guest_error_key(void)\n{\n\tGUEST_SYNC(STAGE_INITED);\n\tset_storage_key_range(mem1, PAGE_SIZE, 0x18);\n\tset_storage_key_range(mem1 + PAGE_SIZE, sizeof(mem1) - PAGE_SIZE, 0x98);\n\tGUEST_SYNC(STAGE_SKEYS_SET);\n\tGUEST_SYNC(STAGE_IDLED);\n}\n\nstatic void test_errors_key(void)\n{\n\tstruct test_default t = test_default_init(guest_error_key);\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tCHECK_N_DO(ERR_PROT_MOP, t.vcpu, LOGICAL, WRITE, mem1, t.size, GADDR_V(mem1), KEY(2));\n\tCHECK_N_DO(ERR_PROT_MOP, t.vcpu, LOGICAL, READ, mem2, t.size, GADDR_V(mem1), KEY(2));\n\tCHECK_N_DO(ERR_PROT_MOP, t.vm, ABSOLUTE, WRITE, mem1, t.size, GADDR_V(mem1), KEY(2));\n\tCHECK_N_DO(ERR_PROT_MOP, t.vm, ABSOLUTE, READ, mem2, t.size, GADDR_V(mem1), KEY(2));\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_errors_cmpxchg_key(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot);\n\tint i;\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\tfor (i = 1; i <= 16; i *= 2) {\n\t\t__uint128_t old = 0;\n\n\t\tERR_PROT_MOP(t.vm, ABSOLUTE, CMPXCHG, mem2, i, GADDR_V(mem2),\n\t\t\t     CMPXCHG_OLD(&old), KEY(2));\n\t}\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_termination(void)\n{\n\tstruct test_default t = test_default_init(guest_error_key);\n\tuint64_t prefix;\n\tuint64_t teid;\n\tuint64_t teid_mask = BIT(63 - 56) | BIT(63 - 60) | BIT(63 - 61);\n\tuint64_t psw[2];\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tERR_PROT_MOP(t.vcpu, LOGICAL, WRITE, mem1, t.size, GADDR_V(mem1), KEY(1), INJECT);\n\t \n\tprefix = t.run->s.regs.prefix;\n\tpsw[0] = t.run->psw_mask;\n\tpsw[1] = t.run->psw_addr;\n\tMOP(t.vm, ABSOLUTE, WRITE, psw, sizeof(psw), GADDR(prefix + 464));\n\tHOST_SYNC(t.vcpu, STAGE_IDLED);\n\tMOP(t.vm, ABSOLUTE, READ, &teid, sizeof(teid), GADDR(prefix + 168));\n\t \n\tTEST_ASSERT_EQ(teid & teid_mask, 0);\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_errors_key_storage_prot_override(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot);\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tt.run->s.regs.crs[0] |= CR0_STORAGE_PROTECTION_OVERRIDE;\n\tt.run->kvm_dirty_regs = KVM_SYNC_CRS;\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tCHECK_N_DO(ERR_PROT_MOP, t.vm, ABSOLUTE, WRITE, mem1, t.size, GADDR_V(mem1), KEY(2));\n\tCHECK_N_DO(ERR_PROT_MOP, t.vm, ABSOLUTE, READ, mem2, t.size, GADDR_V(mem2), KEY(2));\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nconst uint64_t last_page_addr = -PAGE_SIZE;\n\nstatic void guest_copy_key_fetch_prot_override(void)\n{\n\tint i;\n\tchar *page_0 = 0;\n\n\tGUEST_SYNC(STAGE_INITED);\n\tset_storage_key_range(0, PAGE_SIZE, 0x18);\n\tset_storage_key_range((void *)last_page_addr, PAGE_SIZE, 0x0);\n\tasm volatile (\"sske %[key],%[addr]\\n\" :: [addr] \"r\"(0L), [key] \"r\"(0x18) : \"cc\");\n\tGUEST_SYNC(STAGE_SKEYS_SET);\n\n\tfor (;;) {\n\t\tfor (i = 0; i < PAGE_SIZE; i++)\n\t\t\tpage_0[i] = mem1[i];\n\t\tGUEST_SYNC(STAGE_COPIED);\n\t}\n}\n\nstatic void test_copy_key_fetch_prot_override(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot_override);\n\tvm_vaddr_t guest_0_page, guest_last_page;\n\n\tguest_0_page = vm_vaddr_alloc(t.kvm_vm, PAGE_SIZE, 0);\n\tguest_last_page = vm_vaddr_alloc(t.kvm_vm, PAGE_SIZE, last_page_addr);\n\tif (guest_0_page != 0 || guest_last_page != last_page_addr) {\n\t\tprint_skip(\"did not allocate guest pages at required positions\");\n\t\tgoto out;\n\t}\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tt.run->s.regs.crs[0] |= CR0_FETCH_PROTECTION_OVERRIDE;\n\tt.run->kvm_dirty_regs = KVM_SYNC_CRS;\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tprepare_mem12();\n\tMOP(t.vcpu, LOGICAL, WRITE, mem1, PAGE_SIZE, GADDR_V(mem1));\n\tHOST_SYNC(t.vcpu, STAGE_COPIED);\n\tCHECK_N_DO(MOP, t.vcpu, LOGICAL, READ, mem2, 2048, GADDR_V(guest_0_page), KEY(2));\n\tASSERT_MEM_EQ(mem1, mem2, 2048);\n\n\t \n\tprepare_mem12();\n\tMOP(t.vcpu, LOGICAL, WRITE, mem1, 2 * PAGE_SIZE, GADDR_V(guest_last_page));\n\tHOST_SYNC(t.vcpu, STAGE_COPIED);\n\tCHECK_N_DO(MOP, t.vcpu, LOGICAL, READ, mem2, PAGE_SIZE + 2048,\n\t\t   GADDR_V(guest_last_page), KEY(2));\n\tASSERT_MEM_EQ(mem1, mem2, 2048);\n\nout:\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_errors_key_fetch_prot_override_not_enabled(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot_override);\n\tvm_vaddr_t guest_0_page, guest_last_page;\n\n\tguest_0_page = vm_vaddr_alloc(t.kvm_vm, PAGE_SIZE, 0);\n\tguest_last_page = vm_vaddr_alloc(t.kvm_vm, PAGE_SIZE, last_page_addr);\n\tif (guest_0_page != 0 || guest_last_page != last_page_addr) {\n\t\tprint_skip(\"did not allocate guest pages at required positions\");\n\t\tgoto out;\n\t}\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tCHECK_N_DO(ERR_PROT_MOP, t.vcpu, LOGICAL, READ, mem2, 2048, GADDR_V(0), KEY(2));\n\nout:\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_errors_key_fetch_prot_override_enabled(void)\n{\n\tstruct test_default t = test_default_init(guest_copy_key_fetch_prot_override);\n\tvm_vaddr_t guest_0_page, guest_last_page;\n\n\tguest_0_page = vm_vaddr_alloc(t.kvm_vm, PAGE_SIZE, 0);\n\tguest_last_page = vm_vaddr_alloc(t.kvm_vm, PAGE_SIZE, last_page_addr);\n\tif (guest_0_page != 0 || guest_last_page != last_page_addr) {\n\t\tprint_skip(\"did not allocate guest pages at required positions\");\n\t\tgoto out;\n\t}\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\tt.run->s.regs.crs[0] |= CR0_FETCH_PROTECTION_OVERRIDE;\n\tt.run->kvm_dirty_regs = KVM_SYNC_CRS;\n\tHOST_SYNC(t.vcpu, STAGE_SKEYS_SET);\n\n\t \n\tCHECK_N_DO(ERR_PROT_MOP, t.vcpu, LOGICAL, READ, mem2, 2048 + 1, GADDR_V(0), KEY(2));\n\tCHECK_N_DO(ERR_PROT_MOP, t.vcpu, LOGICAL, READ, mem2, PAGE_SIZE + 2048 + 1,\n\t\t   GADDR_V(guest_last_page), KEY(2));\n\t \n\tCHECK_N_DO(ERR_PROT_MOP, t.vm, ABSOLUTE, READ, mem2, 2048, GADDR(0), KEY(2));\n\tCHECK_N_DO(ERR_PROT_MOP, t.vm, ABSOLUTE, READ, mem2, 2048, GADDR_V(guest_0_page), KEY(2));\n\nout:\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void guest_idle(void)\n{\n\tGUEST_SYNC(STAGE_INITED);  \n\tfor (;;)\n\t\tGUEST_SYNC(STAGE_IDLED);\n}\n\nstatic void _test_errors_common(struct test_info info, enum mop_target target, int size)\n{\n\tint rv;\n\n\t \n\trv = ERR_MOP(info, target, WRITE, mem1, -1, GADDR_V(mem1));\n\tTEST_ASSERT(rv == -1 && errno == E2BIG, \"ioctl allows insane sizes\");\n\n\t \n\trv = ERR_MOP(info, target, WRITE, mem1, 0, GADDR_V(mem1));\n\tTEST_ASSERT(rv == -1 && (errno == EINVAL || errno == ENOMEM),\n\t\t    \"ioctl allows 0 as size\");\n\n\t \n\trv = ERR_MOP(info, target, WRITE, mem1, size, GADDR_V(mem1), SET_FLAGS(-1));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL, \"ioctl allows all flags\");\n\n\t \n\trv = ERR_MOP(info, target, WRITE, mem1, size, GADDR((void *)~0xfffUL), CHECK_ONLY);\n\tTEST_ASSERT(rv > 0, \"ioctl does not report bad guest memory address with CHECK_ONLY\");\n\trv = ERR_MOP(info, target, WRITE, mem1, size, GADDR((void *)~0xfffUL));\n\tTEST_ASSERT(rv > 0, \"ioctl does not report bad guest memory address on write\");\n\n\t \n\trv = ERR_MOP(info, target, WRITE, 0, size, GADDR_V(mem1));\n\tTEST_ASSERT(rv == -1 && errno == EFAULT,\n\t\t    \"ioctl does not report bad host memory address\");\n\n\t \n\trv = ERR_MOP(info, target, WRITE, mem1, size, GADDR_V(mem1), KEY(17));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL, \"ioctl allows invalid key\");\n}\n\nstatic void test_errors(void)\n{\n\tstruct test_default t = test_default_init(guest_idle);\n\tint rv;\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\n\t_test_errors_common(t.vcpu, LOGICAL, t.size);\n\t_test_errors_common(t.vm, ABSOLUTE, t.size);\n\n\t \n\trv = ERR_MOP(t.vcpu, INVALID, WRITE, mem1, t.size, GADDR_V(mem1));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL, \"ioctl allows bad operations\");\n\t \n\trv = ERR_MOP(t.vm, INVALID, WRITE, mem1, PAGE_SIZE, GADDR(0));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL, \"ioctl allows bad operations\");\n\n\t \n\tt.run->psw_mask &= ~(3UL << (63 - 17));\n\tt.run->psw_mask |= 1UL << (63 - 17);   \n\tHOST_SYNC(t.vcpu, STAGE_IDLED);  \n\trv = ERR_MOP(t.vcpu, LOGICAL, WRITE, mem1, t.size, GADDR_V(mem1), AR(17));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL, \"ioctl allows ARs > 15\");\n\tt.run->psw_mask &= ~(3UL << (63 - 17));    \n\tHOST_SYNC(t.vcpu, STAGE_IDLED);  \n\n\t \n\trv = ERR_MOP(t.vcpu, SIDA, READ, mem1, 8, GADDR(0), SIDA_OFFSET(0x1c0));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL,\n\t\t    \"ioctl does not reject SIDA_READ in non-protected mode\");\n\trv = ERR_MOP(t.vcpu, SIDA, WRITE, mem1, 8, GADDR(0), SIDA_OFFSET(0x1c0));\n\tTEST_ASSERT(rv == -1 && errno == EINVAL,\n\t\t    \"ioctl does not reject SIDA_WRITE in non-protected mode\");\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nstatic void test_errors_cmpxchg(void)\n{\n\tstruct test_default t = test_default_init(guest_idle);\n\t__uint128_t old;\n\tint rv, i, power = 1;\n\n\tHOST_SYNC(t.vcpu, STAGE_INITED);\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (i == power) {\n\t\t\tpower *= 2;\n\t\t\tcontinue;\n\t\t}\n\t\trv = ERR_MOP(t.vm, ABSOLUTE, CMPXCHG, mem1, i, GADDR_V(mem1),\n\t\t\t     CMPXCHG_OLD(&old));\n\t\tTEST_ASSERT(rv == -1 && errno == EINVAL,\n\t\t\t    \"ioctl allows bad size for cmpxchg\");\n\t}\n\tfor (i = 1; i <= 16; i *= 2) {\n\t\trv = ERR_MOP(t.vm, ABSOLUTE, CMPXCHG, mem1, i, GADDR((void *)~0xfffUL),\n\t\t\t     CMPXCHG_OLD(&old));\n\t\tTEST_ASSERT(rv > 0, \"ioctl allows bad guest address for cmpxchg\");\n\t}\n\tfor (i = 2; i <= 16; i *= 2) {\n\t\trv = ERR_MOP(t.vm, ABSOLUTE, CMPXCHG, mem1, i, GADDR_V(mem1 + 1),\n\t\t\t     CMPXCHG_OLD(&old));\n\t\tTEST_ASSERT(rv == -1 && errno == EINVAL,\n\t\t\t    \"ioctl allows bad alignment for cmpxchg\");\n\t}\n\n\tkvm_vm_free(t.kvm_vm);\n}\n\nint main(int argc, char *argv[])\n{\n\tint extension_cap, idx;\n\n\tTEST_REQUIRE(kvm_has_cap(KVM_CAP_S390_MEM_OP));\n\textension_cap = kvm_check_cap(KVM_CAP_S390_MEM_OP_EXTENSION);\n\n\tstruct testdef {\n\t\tconst char *name;\n\t\tvoid (*test)(void);\n\t\tbool requirements_met;\n\t} testlist[] = {\n\t\t{\n\t\t\t.name = \"simple copy\",\n\t\t\t.test = test_copy,\n\t\t\t.requirements_met = true,\n\t\t},\n\t\t{\n\t\t\t.name = \"generic error checks\",\n\t\t\t.test = test_errors,\n\t\t\t.requirements_met = true,\n\t\t},\n\t\t{\n\t\t\t.name = \"copy with storage keys\",\n\t\t\t.test = test_copy_key,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"cmpxchg with storage keys\",\n\t\t\t.test = test_cmpxchg_key,\n\t\t\t.requirements_met = extension_cap & 0x2,\n\t\t},\n\t\t{\n\t\t\t.name = \"concurrently cmpxchg with storage keys\",\n\t\t\t.test = test_cmpxchg_key_concurrent,\n\t\t\t.requirements_met = extension_cap & 0x2,\n\t\t},\n\t\t{\n\t\t\t.name = \"copy with key storage protection override\",\n\t\t\t.test = test_copy_key_storage_prot_override,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"copy with key fetch protection\",\n\t\t\t.test = test_copy_key_fetch_prot,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"copy with key fetch protection override\",\n\t\t\t.test = test_copy_key_fetch_prot_override,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"error checks with key\",\n\t\t\t.test = test_errors_key,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"error checks for cmpxchg with key\",\n\t\t\t.test = test_errors_cmpxchg_key,\n\t\t\t.requirements_met = extension_cap & 0x2,\n\t\t},\n\t\t{\n\t\t\t.name = \"error checks for cmpxchg\",\n\t\t\t.test = test_errors_cmpxchg,\n\t\t\t.requirements_met = extension_cap & 0x2,\n\t\t},\n\t\t{\n\t\t\t.name = \"termination\",\n\t\t\t.test = test_termination,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"error checks with key storage protection override\",\n\t\t\t.test = test_errors_key_storage_prot_override,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"error checks without key fetch prot override\",\n\t\t\t.test = test_errors_key_fetch_prot_override_not_enabled,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t\t{\n\t\t\t.name = \"error checks with key fetch prot override\",\n\t\t\t.test = test_errors_key_fetch_prot_override_enabled,\n\t\t\t.requirements_met = extension_cap > 0,\n\t\t},\n\t};\n\n\tksft_print_header();\n\tksft_set_plan(ARRAY_SIZE(testlist));\n\n\tfor (idx = 0; idx < ARRAY_SIZE(testlist); idx++) {\n\t\tif (testlist[idx].requirements_met) {\n\t\t\ttestlist[idx].test();\n\t\t\tksft_test_result_pass(\"%s\\n\", testlist[idx].name);\n\t\t} else {\n\t\t\tksft_test_result_skip(\"%s - requirements not met (kernel has extension cap %#x)\\n\",\n\t\t\t\t\t      testlist[idx].name, extension_cap);\n\t\t}\n\t}\n\n\tksft_finished();\t \n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}