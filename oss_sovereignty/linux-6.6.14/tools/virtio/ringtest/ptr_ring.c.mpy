{
  "module_name": "ptr_ring.c",
  "hash_id": "20705ad0f85d9ffb4f26f4106ce645208153ab63a7400c42c22251921d10433a",
  "original_prompt": "Ingested from linux-6.6.14/tools/virtio/ringtest/ptr_ring.c",
  "human_readable_source": "\n#define _GNU_SOURCE\n#include \"main.h\"\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <pthread.h>\n#include <malloc.h>\n#include <assert.h>\n#include <errno.h>\n#include <limits.h>\n\n#define SMP_CACHE_BYTES 64\n#define cache_line_size() SMP_CACHE_BYTES\n#define ____cacheline_aligned_in_smp __attribute__ ((aligned (SMP_CACHE_BYTES)))\n#define unlikely(x)    (__builtin_expect(!!(x), 0))\n#define likely(x)    (__builtin_expect(!!(x), 1))\n#define ALIGN(x, a) (((x) + (a) - 1) / (a) * (a))\n#define SIZE_MAX        (~(size_t)0)\n#define KMALLOC_MAX_SIZE SIZE_MAX\n\ntypedef pthread_spinlock_t  spinlock_t;\n\ntypedef int gfp_t;\n#define __GFP_ZERO 0x1\n\nstatic void *kmalloc(unsigned size, gfp_t gfp)\n{\n\tvoid *p = memalign(64, size);\n\tif (!p)\n\t\treturn p;\n\n\tif (gfp & __GFP_ZERO)\n\t\tmemset(p, 0, size);\n\treturn p;\n}\n\nstatic inline void *kzalloc(unsigned size, gfp_t flags)\n{\n\treturn kmalloc(size, flags | __GFP_ZERO);\n}\n\nstatic inline void *kmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tif (size != 0 && n > SIZE_MAX / size)\n\t\treturn NULL;\n\treturn kmalloc(n * size, flags);\n}\n\nstatic inline void *kcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn kmalloc_array(n, size, flags | __GFP_ZERO);\n}\n\nstatic void kfree(void *p)\n{\n\tif (p)\n\t\tfree(p);\n}\n\n#define kvmalloc_array kmalloc_array\n#define kvfree kfree\n\nstatic void spin_lock_init(spinlock_t *lock)\n{\n\tint r = pthread_spin_init(lock, 0);\n\tassert(!r);\n}\n\nstatic void spin_lock(spinlock_t *lock)\n{\n\tint ret = pthread_spin_lock(lock);\n\tassert(!ret);\n}\n\nstatic void spin_unlock(spinlock_t *lock)\n{\n\tint ret = pthread_spin_unlock(lock);\n\tassert(!ret);\n}\n\nstatic void spin_lock_bh(spinlock_t *lock)\n{\n\tspin_lock(lock);\n}\n\nstatic void spin_unlock_bh(spinlock_t *lock)\n{\n\tspin_unlock(lock);\n}\n\nstatic void spin_lock_irq(spinlock_t *lock)\n{\n\tspin_lock(lock);\n}\n\nstatic void spin_unlock_irq(spinlock_t *lock)\n{\n\tspin_unlock(lock);\n}\n\nstatic void spin_lock_irqsave(spinlock_t *lock, unsigned long f)\n{\n\tspin_lock(lock);\n}\n\nstatic void spin_unlock_irqrestore(spinlock_t *lock, unsigned long f)\n{\n\tspin_unlock(lock);\n}\n\n#include \"../../../include/linux/ptr_ring.h\"\n\nstatic unsigned long long headcnt, tailcnt;\nstatic struct ptr_ring array ____cacheline_aligned_in_smp;\n\n \nvoid alloc_ring(void)\n{\n\tint ret = ptr_ring_init(&array, ring_size, 0);\n\tassert(!ret);\n\t \n\tif (param)\n\t\tarray.batch = param;\n}\n\n \nint add_inbuf(unsigned len, void *buf, void *datap)\n{\n\tint ret;\n\n\tret = __ptr_ring_produce(&array, buf);\n\tif (ret >= 0) {\n\t\tret = 0;\n\t\theadcnt++;\n\t}\n\n\treturn ret;\n}\n\n \nvoid *get_buf(unsigned *lenp, void **bufp)\n{\n\tvoid *datap;\n\n\tif (tailcnt == headcnt || __ptr_ring_full(&array))\n\t\tdatap = NULL;\n\telse {\n\t\tdatap = \"Buffer\\n\";\n\t\t++tailcnt;\n\t}\n\n\treturn datap;\n}\n\nbool used_empty()\n{\n\treturn (tailcnt == headcnt || __ptr_ring_full(&array));\n}\n\nvoid disable_call()\n{\n\tassert(0);\n}\n\nbool enable_call()\n{\n\tassert(0);\n}\n\nvoid kick_available(void)\n{\n\tassert(0);\n}\n\n \nvoid disable_kick()\n{\n\tassert(0);\n}\n\nbool enable_kick()\n{\n\tassert(0);\n}\n\nbool avail_empty()\n{\n\treturn __ptr_ring_empty(&array);\n}\n\nbool use_buf(unsigned *lenp, void **bufp)\n{\n\tvoid *ptr;\n\n\tptr = __ptr_ring_consume(&array);\n\n\treturn ptr;\n}\n\nvoid call_used(void)\n{\n\tassert(0);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}