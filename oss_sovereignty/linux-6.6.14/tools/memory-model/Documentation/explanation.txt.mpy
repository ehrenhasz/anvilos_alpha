{
  "module_name": "explanation.txt",
  "hash_id": "cedf76e3487f44b4841739d2f9954c1ad9bfcf8427c5acad95aafff848602661",
  "original_prompt": "Ingested from linux-6.6.14/tools/memory-model/Documentation/explanation.txt",
  "human_readable_source": "Explanation of the Linux-Kernel Memory Consistency Model\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n:Author: Alan Stern <stern@rowland.harvard.edu>\n:Created: October 2017\n\n.. Contents\n\n  1. INTRODUCTION\n  2. BACKGROUND\n  3. A SIMPLE EXAMPLE\n  4. A SELECTION OF MEMORY MODELS\n  5. ORDERING AND CYCLES\n  6. EVENTS\n  7. THE PROGRAM ORDER RELATION: po AND po-loc\n  8. A WARNING\n  9. DEPENDENCY RELATIONS: data, addr, and ctrl\n  10. THE READS-FROM RELATION: rf, rfi, and rfe\n  11. CACHE COHERENCE AND THE COHERENCE ORDER RELATION: co, coi, and coe\n  12. THE FROM-READS RELATION: fr, fri, and fre\n  13. AN OPERATIONAL MODEL\n  14. PROPAGATION ORDER RELATION: cumul-fence\n  15. DERIVATION OF THE LKMM FROM THE OPERATIONAL MODEL\n  16. SEQUENTIAL CONSISTENCY PER VARIABLE\n  17. ATOMIC UPDATES: rmw\n  18. THE PRESERVED PROGRAM ORDER RELATION: ppo\n  19. AND THEN THERE WAS ALPHA\n  20. THE HAPPENS-BEFORE RELATION: hb\n  21. THE PROPAGATES-BEFORE RELATION: pb\n  22. RCU RELATIONS: rcu-link, rcu-gp, rcu-rscsi, rcu-order, rcu-fence, and rb\n  23. SRCU READ-SIDE CRITICAL SECTIONS\n  24. LOCKING\n  25. PLAIN ACCESSES AND DATA RACES\n  26. ODDS AND ENDS\n\n\n\nINTRODUCTION\n------------\n\nThe Linux-kernel memory consistency model (LKMM) is rather complex and\nobscure.  This is particularly evident if you read through the\nlinux-kernel.bell and linux-kernel.cat files that make up the formal\nversion of the model; they are extremely terse and their meanings are\nfar from clear.\n\nThis document describes the ideas underlying the LKMM.  It is meant\nfor people who want to understand how the model was designed.  It does\nnot go into the details of the code in the .bell and .cat files;\nrather, it explains in English what the code expresses symbolically.\n\nSections 2 (BACKGROUND) through 5 (ORDERING AND CYCLES) are aimed\ntoward beginners; they explain what memory consistency models are and\nthe basic notions shared by all such models.  People already familiar\nwith these concepts can skim or skip over them.  Sections 6 (EVENTS)\nthrough 12 (THE FROM_READS RELATION) describe the fundamental\nrelations used in many models.  Starting in Section 13 (AN OPERATIONAL\nMODEL), the workings of the LKMM itself are covered.\n\nWarning: The code examples in this document are not written in the\nproper format for litmus tests.  They don't include a header line, the\ninitializations are not enclosed in braces, the global variables are\nnot passed by pointers, and they don't have an \"exists\" clause at the\nend.  Converting them to the right format is left as an exercise for\nthe reader.\n\n\nBACKGROUND\n----------\n\nA memory consistency model (or just memory model, for short) is\nsomething which predicts, given a piece of computer code running on a\nparticular kind of system, what values may be obtained by the code's\nload instructions.  The LKMM makes these predictions for code running\nas part of the Linux kernel.\n\nIn practice, people tend to use memory models the other way around.\nThat is, given a piece of code and a collection of values specified\nfor the loads, the model will predict whether it is possible for the\ncode to run in such a way that the loads will indeed obtain the\nspecified values.  Of course, this is just another way of expressing\nthe same idea.\n\nFor code running on a uniprocessor system, the predictions are easy:\nEach load instruction must obtain the value written by the most recent\nstore instruction accessing the same location (we ignore complicating\nfactors such as DMA and mixed-size accesses.)  But on multiprocessor\nsystems, with multiple CPUs making concurrent accesses to shared\nmemory locations, things aren't so simple.\n\nDifferent architectures have differing memory models, and the Linux\nkernel supports a variety of architectures.  The LKMM has to be fairly\npermissive, in the sense that any behavior allowed by one of these\narchitectures also has to be allowed by the LKMM.\n\n\nA SIMPLE EXAMPLE\n----------------\n\nHere is a simple example to illustrate the basic concepts.  Consider\nsome code running as part of a device driver for an input device.  The\ndriver might contain an interrupt handler which collects data from the\ndevice, stores it in a buffer, and sets a flag to indicate the buffer\nis full.  Running concurrently on a different CPU might be a part of\nthe driver code being executed by a process in the midst of a read(2)\nsystem call.  This code tests the flag to see whether the buffer is\nready, and if it is, copies the data back to userspace.  The buffer\nand the flag are memory locations shared between the two CPUs.\n\nWe can abstract out the important pieces of the driver code as follows\n(the reason for using WRITE_ONCE() and READ_ONCE() instead of simple\nassignment statements is discussed later):\n\n\tint buf = 0, flag = 0;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(buf, 1);\n\t\tWRITE_ONCE(flag, 1);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\t\tint r2 = 0;\n\n\t\tr1 = READ_ONCE(flag);\n\t\tif (r1)\n\t\t\tr2 = READ_ONCE(buf);\n\t}\n\nHere the P0() function represents the interrupt handler running on one\nCPU and P1() represents the read() routine running on another.  The\nvalue 1 stored in buf represents input data collected from the device.\nThus, P0 stores the data in buf and then sets flag.  Meanwhile, P1\nreads flag into the private variable r1, and if it is set, reads the\ndata from buf into a second private variable r2 for copying to\nuserspace.  (Presumably if flag is not set then the driver will wait a\nwhile and try again.)\n\nThis pattern of memory accesses, where one CPU stores values to two\nshared memory locations and another CPU loads from those locations in\nthe opposite order, is widely known as the \"Message Passing\" or MP\npattern.  It is typical of memory access patterns in the kernel.\n\nPlease note that this example code is a simplified abstraction.  Real\nbuffers are usually larger than a single integer, real device drivers\nusually use sleep and wakeup mechanisms rather than polling for I/O\ncompletion, and real code generally doesn't bother to copy values into\nprivate variables before using them.  All that is beside the point;\nthe idea here is simply to illustrate the overall pattern of memory\naccesses by the CPUs.\n\nA memory model will predict what values P1 might obtain for its loads\nfrom flag and buf, or equivalently, what values r1 and r2 might end up\nwith after the code has finished running.\n\nSome predictions are trivial.  For instance, no sane memory model would\npredict that r1 = 42 or r2 = -7, because neither of those values ever\ngets stored in flag or buf.\n\nSome nontrivial predictions are nonetheless quite simple.  For\ninstance, P1 might run entirely before P0 begins, in which case r1 and\nr2 will both be 0 at the end.  Or P0 might run entirely before P1\nbegins, in which case r1 and r2 will both be 1.\n\nThe interesting predictions concern what might happen when the two\nroutines run concurrently.  One possibility is that P1 runs after P0's\nstore to buf but before the store to flag.  In this case, r1 and r2\nwill again both be 0.  (If P1 had been designed to read buf\nunconditionally then we would instead have r1 = 0 and r2 = 1.)\n\nHowever, the most interesting possibility is where r1 = 1 and r2 = 0.\nIf this were to occur it would mean the driver contains a bug, because\nincorrect data would get sent to the user: 0 instead of 1.  As it\nhappens, the LKMM does predict this outcome can occur, and the example\ndriver code shown above is indeed buggy.\n\n\nA SELECTION OF MEMORY MODELS\n----------------------------\n\nThe first widely cited memory model, and the simplest to understand,\nis Sequential Consistency.  According to this model, systems behave as\nif each CPU executed its instructions in order but with unspecified\ntiming.  In other words, the instructions from the various CPUs get\ninterleaved in a nondeterministic way, always according to some single\nglobal order that agrees with the order of the instructions in the\nprogram source for each CPU.  The model says that the value obtained\nby each load is simply the value written by the most recently executed\nstore to the same memory location, from any CPU.\n\nFor the MP example code shown above, Sequential Consistency predicts\nthat the undesired result r1 = 1, r2 = 0 cannot occur.  The reasoning\ngoes like this:\n\n\tSince r1 = 1, P0 must store 1 to flag before P1 loads 1 from\n\tit, as loads can obtain values only from earlier stores.\n\n\tP1 loads from flag before loading from buf, since CPUs execute\n\ttheir instructions in order.\n\n\tP1 must load 0 from buf before P0 stores 1 to it; otherwise r2\n\twould be 1 since a load obtains its value from the most recent\n\tstore to the same address.\n\n\tP0 stores 1 to buf before storing 1 to flag, since it executes\n\tits instructions in order.\n\n\tSince an instruction (in this case, P0's store to flag) cannot\n\texecute before itself, the specified outcome is impossible.\n\nHowever, real computer hardware almost never follows the Sequential\nConsistency memory model; doing so would rule out too many valuable\nperformance optimizations.  On ARM and PowerPC architectures, for\ninstance, the MP example code really does sometimes yield r1 = 1 and\nr2 = 0.\n\nx86 and SPARC follow yet a different memory model: TSO (Total Store\nOrdering).  This model predicts that the undesired outcome for the MP\npattern cannot occur, but in other respects it differs from Sequential\nConsistency.  One example is the Store Buffer (SB) pattern, in which\neach CPU stores to its own shared location and then loads from the\nother CPU's location:\n\n\tint x = 0, y = 0;\n\n\tP0()\n\t{\n\t\tint r0;\n\n\t\tWRITE_ONCE(x, 1);\n\t\tr0 = READ_ONCE(y);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\tWRITE_ONCE(y, 1);\n\t\tr1 = READ_ONCE(x);\n\t}\n\nSequential Consistency predicts that the outcome r0 = 0, r1 = 0 is\nimpossible.  (Exercise: Figure out the reasoning.)  But TSO allows\nthis outcome to occur, and in fact it does sometimes occur on x86 and\nSPARC systems.\n\nThe LKMM was inspired by the memory models followed by PowerPC, ARM,\nx86, Alpha, and other architectures.  However, it is different in\ndetail from each of them.\n\n\nORDERING AND CYCLES\n-------------------\n\nMemory models are all about ordering.  Often this is temporal ordering\n(i.e., the order in which certain events occur) but it doesn't have to\nbe; consider for example the order of instructions in a program's\nsource code.  We saw above that Sequential Consistency makes an\nimportant assumption that CPUs execute instructions in the same order\nas those instructions occur in the code, and there are many other\ninstances of ordering playing central roles in memory models.\n\nThe counterpart to ordering is a cycle.  Ordering rules out cycles:\nIt's not possible to have X ordered before Y, Y ordered before Z, and\nZ ordered before X, because this would mean that X is ordered before\nitself.  The analysis of the MP example under Sequential Consistency\ninvolved just such an impossible cycle:\n\n\tW: P0 stores 1 to flag   executes before\n\tX: P1 loads 1 from flag  executes before\n\tY: P1 loads 0 from buf   executes before\n\tZ: P0 stores 1 to buf    executes before\n\tW: P0 stores 1 to flag.\n\nIn short, if a memory model requires certain accesses to be ordered,\nand a certain outcome for the loads in a piece of code can happen only\nif those accesses would form a cycle, then the memory model predicts\nthat outcome cannot occur.\n\nThe LKMM is defined largely in terms of cycles, as we will see.\n\n\nEVENTS\n------\n\nThe LKMM does not work directly with the C statements that make up\nkernel source code.  Instead it considers the effects of those\nstatements in a more abstract form, namely, events.  The model\nincludes three types of events:\n\n\tRead events correspond to loads from shared memory, such as\n\tcalls to READ_ONCE(), smp_load_acquire(), or\n\trcu_dereference().\n\n\tWrite events correspond to stores to shared memory, such as\n\tcalls to WRITE_ONCE(), smp_store_release(), or atomic_set().\n\n\tFence events correspond to memory barriers (also known as\n\tfences), such as calls to smp_rmb() or rcu_read_lock().\n\nThese categories are not exclusive; a read or write event can also be\na fence.  This happens with functions like smp_load_acquire() or\nspin_lock().  However, no single event can be both a read and a write.\nAtomic read-modify-write accesses, such as atomic_inc() or xchg(),\ncorrespond to a pair of events: a read followed by a write.  (The\nwrite event is omitted for executions where it doesn't occur, such as\na cmpxchg() where the comparison fails.)\n\nOther parts of the code, those which do not involve interaction with\nshared memory, do not give rise to events.  Thus, arithmetic and\nlogical computations, control-flow instructions, or accesses to\nprivate memory or CPU registers are not of central interest to the\nmemory model.  They only affect the model's predictions indirectly.\nFor example, an arithmetic computation might determine the value that\ngets stored to a shared memory location (or in the case of an array\nindex, the address where the value gets stored), but the memory model\nis concerned only with the store itself -- its value and its address\n-- not the computation leading up to it.\n\nEvents in the LKMM can be linked by various relations, which we will\ndescribe in the following sections.  The memory model requires certain\nof these relations to be orderings, that is, it requires them not to\nhave any cycles.\n\n\nTHE PROGRAM ORDER RELATION: po AND po-loc\n-----------------------------------------\n\nThe most important relation between events is program order (po).  You\ncan think of it as the order in which statements occur in the source\ncode after branches are taken into account and loops have been\nunrolled.  A better description might be the order in which\ninstructions are presented to a CPU's execution unit.  Thus, we say\nthat X is po-before Y (written as \"X ->po Y\" in formulas) if X occurs\nbefore Y in the instruction stream.\n\nThis is inherently a single-CPU relation; two instructions executing\non different CPUs are never linked by po.  Also, it is by definition\nan ordering so it cannot have any cycles.\n\npo-loc is a sub-relation of po.  It links two memory accesses when the\nfirst comes before the second in program order and they access the\nsame memory location (the \"-loc\" suffix).\n\nAlthough this may seem straightforward, there is one subtle aspect to\nprogram order we need to explain.  The LKMM was inspired by low-level\narchitectural memory models which describe the behavior of machine\ncode, and it retains their outlook to a considerable extent.  The\nread, write, and fence events used by the model are close in spirit to\nindividual machine instructions.  Nevertheless, the LKMM describes\nkernel code written in C, and the mapping from C to machine code can\nbe extremely complex.\n\nOptimizing compilers have great freedom in the way they translate\nsource code to object code.  They are allowed to apply transformations\nthat add memory accesses, eliminate accesses, combine them, split them\ninto pieces, or move them around.  The use of READ_ONCE(), WRITE_ONCE(),\nor one of the other atomic or synchronization primitives prevents a\nlarge number of compiler optimizations.  In particular, it is guaranteed\nthat the compiler will not remove such accesses from the generated code\n(unless it can prove the accesses will never be executed), it will not\nchange the order in which they occur in the code (within limits imposed\nby the C standard), and it will not introduce extraneous accesses.\n\nThe MP and SB examples above used READ_ONCE() and WRITE_ONCE() rather\nthan ordinary memory accesses.  Thanks to this usage, we can be certain\nthat in the MP example, the compiler won't reorder P0's write event to\nbuf and P0's write event to flag, and similarly for the other shared\nmemory accesses in the examples.\n\nSince private variables are not shared between CPUs, they can be\naccessed normally without READ_ONCE() or WRITE_ONCE().  In fact, they\nneed not even be stored in normal memory at all -- in principle a\nprivate variable could be stored in a CPU register (hence the convention\nthat these variables have names starting with the letter 'r').\n\n\nA WARNING\n---------\n\nThe protections provided by READ_ONCE(), WRITE_ONCE(), and others are\nnot perfect; and under some circumstances it is possible for the\ncompiler to undermine the memory model.  Here is an example.  Suppose\nboth branches of an \"if\" statement store the same value to the same\nlocation:\n\n\tr1 = READ_ONCE(x);\n\tif (r1) {\n\t\tWRITE_ONCE(y, 2);\n\t\t...  /* do something */\n\t} else {\n\t\tWRITE_ONCE(y, 2);\n\t\t...  /* do something else */\n\t}\n\nFor this code, the LKMM predicts that the load from x will always be\nexecuted before either of the stores to y.  However, a compiler could\nlift the stores out of the conditional, transforming the code into\nsomething resembling:\n\n\tr1 = READ_ONCE(x);\n\tWRITE_ONCE(y, 2);\n\tif (r1) {\n\t\t...  /* do something */\n\t} else {\n\t\t...  /* do something else */\n\t}\n\nGiven this version of the code, the LKMM would predict that the load\nfrom x could be executed after the store to y.  Thus, the memory\nmodel's original prediction could be invalidated by the compiler.\n\nAnother issue arises from the fact that in C, arguments to many\noperators and function calls can be evaluated in any order.  For\nexample:\n\n\tr1 = f(5) + g(6);\n\nThe object code might call f(5) either before or after g(6); the\nmemory model cannot assume there is a fixed program order relation\nbetween them.  (In fact, if the function calls are inlined then the\ncompiler might even interleave their object code.)\n\n\nDEPENDENCY RELATIONS: data, addr, and ctrl\n------------------------------------------\n\nWe say that two events are linked by a dependency relation when the\nexecution of the second event depends in some way on a value obtained\nfrom memory by the first.  The first event must be a read, and the\nvalue it obtains must somehow affect what the second event does.\nThere are three kinds of dependencies: data, address (addr), and\ncontrol (ctrl).\n\nA read and a write event are linked by a data dependency if the value\nobtained by the read affects the value stored by the write.  As a very\nsimple example:\n\n\tint x, y;\n\n\tr1 = READ_ONCE(x);\n\tWRITE_ONCE(y, r1 + 5);\n\nThe value stored by the WRITE_ONCE obviously depends on the value\nloaded by the READ_ONCE.  Such dependencies can wind through\narbitrarily complicated computations, and a write can depend on the\nvalues of multiple reads.\n\nA read event and another memory access event are linked by an address\ndependency if the value obtained by the read affects the location\naccessed by the other event.  The second event can be either a read or\na write.  Here's another simple example:\n\n\tint a[20];\n\tint i;\n\n\tr1 = READ_ONCE(i);\n\tr2 = READ_ONCE(a[r1]);\n\nHere the location accessed by the second READ_ONCE() depends on the\nindex value loaded by the first.  Pointer indirection also gives rise\nto address dependencies, since the address of a location accessed\nthrough a pointer will depend on the value read earlier from that\npointer.\n\nFinally, a read event X and a write event Y are linked by a control\ndependency if Y syntactically lies within an arm of an if statement and\nX affects the evaluation of the if condition via a data or address\ndependency (or similarly for a switch statement).  Simple example:\n\n\tint x, y;\n\n\tr1 = READ_ONCE(x);\n\tif (r1)\n\t\tWRITE_ONCE(y, 1984);\n\nExecution of the WRITE_ONCE() is controlled by a conditional expression\nwhich depends on the value obtained by the READ_ONCE(); hence there is\na control dependency from the load to the store.\n\nIt should be pretty obvious that events can only depend on reads that\ncome earlier in program order.  Symbolically, if we have R ->data X,\nR ->addr X, or R ->ctrl X (where R is a read event), then we must also\nhave R ->po X.  It wouldn't make sense for a computation to depend\nsomehow on a value that doesn't get loaded from shared memory until\nlater in the code!\n\nHere's a trick question: When is a dependency not a dependency?  Answer:\nWhen it is purely syntactic rather than semantic.  We say a dependency\nbetween two accesses is purely syntactic if the second access doesn't\nactually depend on the result of the first.  Here is a trivial example:\n\n\tr1 = READ_ONCE(x);\n\tWRITE_ONCE(y, r1 * 0);\n\nThere appears to be a data dependency from the load of x to the store\nof y, since the value to be stored is computed from the value that was\nloaded.  But in fact, the value stored does not really depend on\nanything since it will always be 0.  Thus the data dependency is only\nsyntactic (it appears to exist in the code) but not semantic (the\nsecond access will always be the same, regardless of the value of the\nfirst access).  Given code like this, a compiler could simply discard\nthe value returned by the load from x, which would certainly destroy\nany dependency.  (The compiler is not permitted to eliminate entirely\nthe load generated for a READ_ONCE() -- that's one of the nice\nproperties of READ_ONCE() -- but it is allowed to ignore the load's\nvalue.)\n\nIt's natural to object that no one in their right mind would write\ncode like the above.  However, macro expansions can easily give rise\nto this sort of thing, in ways that often are not apparent to the\nprogrammer.\n\nAnother mechanism that can lead to purely syntactic dependencies is\nrelated to the notion of \"undefined behavior\".  Certain program\nbehaviors are called \"undefined\" in the C language specification,\nwhich means that when they occur there are no guarantees at all about\nthe outcome.  Consider the following example:\n\n\tint a[1];\n\tint i;\n\n\tr1 = READ_ONCE(i);\n\tr2 = READ_ONCE(a[r1]);\n\nAccess beyond the end or before the beginning of an array is one kind\nof undefined behavior.  Therefore the compiler doesn't have to worry\nabout what will happen if r1 is nonzero, and it can assume that r1\nwill always be zero regardless of the value actually loaded from i.\n(If the assumption turns out to be wrong the resulting behavior will\nbe undefined anyway, so the compiler doesn't care!)  Thus the value\nfrom the load can be discarded, breaking the address dependency.\n\nThe LKMM is unaware that purely syntactic dependencies are different\nfrom semantic dependencies and therefore mistakenly predicts that the\naccesses in the two examples above will be ordered.  This is another\nexample of how the compiler can undermine the memory model.  Be warned.\n\n\nTHE READS-FROM RELATION: rf, rfi, and rfe\n-----------------------------------------\n\nThe reads-from relation (rf) links a write event to a read event when\nthe value loaded by the read is the value that was stored by the\nwrite.  In colloquial terms, the load \"reads from\" the store.  We\nwrite W ->rf R to indicate that the load R reads from the store W.  We\nfurther distinguish the cases where the load and the store occur on\nthe same CPU (internal reads-from, or rfi) and where they occur on\ndifferent CPUs (external reads-from, or rfe).\n\nFor our purposes, a memory location's initial value is treated as\nthough it had been written there by an imaginary initial store that\nexecutes on a separate CPU before the main program runs.\n\nUsage of the rf relation implicitly assumes that loads will always\nread from a single store.  It doesn't apply properly in the presence\nof load-tearing, where a load obtains some of its bits from one store\nand some of them from another store.  Fortunately, use of READ_ONCE()\nand WRITE_ONCE() will prevent load-tearing; it's not possible to have:\n\n\tint x = 0;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(x, 0x1234);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\tr1 = READ_ONCE(x);\n\t}\n\nand end up with r1 = 0x1200 (partly from x's initial value and partly\nfrom the value stored by P0).\n\nOn the other hand, load-tearing is unavoidable when mixed-size\naccesses are used.  Consider this example:\n\n\tunion {\n\t\tu32\tw;\n\t\tu16\th[2];\n\t} x;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(x.h[0], 0x1234);\n\t\tWRITE_ONCE(x.h[1], 0x5678);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\tr1 = READ_ONCE(x.w);\n\t}\n\nIf r1 = 0x56781234 (little-endian!) at the end, then P1 must have read\nfrom both of P0's stores.  It is possible to handle mixed-size and\nunaligned accesses in a memory model, but the LKMM currently does not\nattempt to do so.  It requires all accesses to be properly aligned and\nof the location's actual size.\n\n\nCACHE COHERENCE AND THE COHERENCE ORDER RELATION: co, coi, and coe\n------------------------------------------------------------------\n\nCache coherence is a general principle requiring that in a\nmulti-processor system, the CPUs must share a consistent view of the\nmemory contents.  Specifically, it requires that for each location in\nshared memory, the stores to that location must form a single global\nordering which all the CPUs agree on (the coherence order), and this\nordering must be consistent with the program order for accesses to\nthat location.\n\nTo put it another way, for any variable x, the coherence order (co) of\nthe stores to x is simply the order in which the stores overwrite one\nanother.  The imaginary store which establishes x's initial value\ncomes first in the coherence order; the store which directly\noverwrites the initial value comes second; the store which overwrites\nthat value comes third, and so on.\n\nYou can think of the coherence order as being the order in which the\nstores reach x's location in memory (or if you prefer a more\nhardware-centric view, the order in which the stores get written to\nx's cache line).  We write W ->co W' if W comes before W' in the\ncoherence order, that is, if the value stored by W gets overwritten,\ndirectly or indirectly, by the value stored by W'.\n\nCoherence order is required to be consistent with program order.  This\nrequirement takes the form of four coherency rules:\n\n\tWrite-write coherence: If W ->po-loc W' (i.e., W comes before\n\tW' in program order and they access the same location), where W\n\tand W' are two stores, then W ->co W'.\n\n\tWrite-read coherence: If W ->po-loc R, where W is a store and R\n\tis a load, then R must read from W or from some other store\n\twhich comes after W in the coherence order.\n\n\tRead-write coherence: If R ->po-loc W, where R is a load and W\n\tis a store, then the store which R reads from must come before\n\tW in the coherence order.\n\n\tRead-read coherence: If R ->po-loc R', where R and R' are two\n\tloads, then either they read from the same store or else the\n\tstore read by R comes before the store read by R' in the\n\tcoherence order.\n\nThis is sometimes referred to as sequential consistency per variable,\nbecause it means that the accesses to any single memory location obey\nthe rules of the Sequential Consistency memory model.  (According to\nWikipedia, sequential consistency per variable and cache coherence\nmean the same thing except that cache coherence includes an extra\nrequirement that every store eventually becomes visible to every CPU.)\n\nAny reasonable memory model will include cache coherence.  Indeed, our\nexpectation of cache coherence is so deeply ingrained that violations\nof its requirements look more like hardware bugs than programming\nerrors:\n\n\tint x;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(x, 17);\n\t\tWRITE_ONCE(x, 23);\n\t}\n\nIf the final value stored in x after this code ran was 17, you would\nthink your computer was broken.  It would be a violation of the\nwrite-write coherence rule: Since the store of 23 comes later in\nprogram order, it must also come later in x's coherence order and\nthus must overwrite the store of 17.\n\n\tint x = 0;\n\n\tP0()\n\t{\n\t\tint r1;\n\n\t\tr1 = READ_ONCE(x);\n\t\tWRITE_ONCE(x, 666);\n\t}\n\nIf r1 = 666 at the end, this would violate the read-write coherence\nrule: The READ_ONCE() load comes before the WRITE_ONCE() store in\nprogram order, so it must not read from that store but rather from one\ncoming earlier in the coherence order (in this case, x's initial\nvalue).\n\n\tint x = 0;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(x, 5);\n\t}\n\n\tP1()\n\t{\n\t\tint r1, r2;\n\n\t\tr1 = READ_ONCE(x);\n\t\tr2 = READ_ONCE(x);\n\t}\n\nIf r1 = 5 (reading from P0's store) and r2 = 0 (reading from the\nimaginary store which establishes x's initial value) at the end, this\nwould violate the read-read coherence rule: The r1 load comes before\nthe r2 load in program order, so it must not read from a store that\ncomes later in the coherence order.\n\n(As a minor curiosity, if this code had used normal loads instead of\nREAD_ONCE() in P1, on Itanium it sometimes could end up with r1 = 5\nand r2 = 0!  This results from parallel execution of the operations\nencoded in Itanium's Very-Long-Instruction-Word format, and it is yet\nanother motivation for using READ_ONCE() when accessing shared memory\nlocations.)\n\nJust like the po relation, co is inherently an ordering -- it is not\npossible for a store to directly or indirectly overwrite itself!  And\njust like with the rf relation, we distinguish between stores that\noccur on the same CPU (internal coherence order, or coi) and stores\nthat occur on different CPUs (external coherence order, or coe).\n\nOn the other hand, stores to different memory locations are never\nrelated by co, just as instructions on different CPUs are never\nrelated by po.  Coherence order is strictly per-location, or if you\nprefer, each location has its own independent coherence order.\n\n\nTHE FROM-READS RELATION: fr, fri, and fre\n-----------------------------------------\n\nThe from-reads relation (fr) can be a little difficult for people to\ngrok.  It describes the situation where a load reads a value that gets\noverwritten by a store.  In other words, we have R ->fr W when the\nvalue that R reads is overwritten (directly or indirectly) by W, or\nequivalently, when R reads from a store which comes earlier than W in\nthe coherence order.\n\nFor example:\n\n\tint x = 0;\n\n\tP0()\n\t{\n\t\tint r1;\n\n\t\tr1 = READ_ONCE(x);\n\t\tWRITE_ONCE(x, 2);\n\t}\n\nThe value loaded from x will be 0 (assuming cache coherence!), and it\ngets overwritten by the value 2.  Thus there is an fr link from the\nREAD_ONCE() to the WRITE_ONCE().  If the code contained any later\nstores to x, there would also be fr links from the READ_ONCE() to\nthem.\n\nAs with rf, rfi, and rfe, we subdivide the fr relation into fri (when\nthe load and the store are on the same CPU) and fre (when they are on\ndifferent CPUs).\n\nNote that the fr relation is determined entirely by the rf and co\nrelations; it is not independent.  Given a read event R and a write\nevent W for the same location, we will have R ->fr W if and only if\nthe write which R reads from is co-before W.  In symbols,\n\n\t(R ->fr W) := (there exists W' with W' ->rf R and W' ->co W).\n\n\nAN OPERATIONAL MODEL\n--------------------\n\nThe LKMM is based on various operational memory models, meaning that\nthe models arise from an abstract view of how a computer system\noperates.  Here are the main ideas, as incorporated into the LKMM.\n\nThe system as a whole is divided into the CPUs and a memory subsystem.\nThe CPUs are responsible for executing instructions (not necessarily\nin program order), and they communicate with the memory subsystem.\nFor the most part, executing an instruction requires a CPU to perform\nonly internal operations.  However, loads, stores, and fences involve\nmore.\n\nWhen CPU C executes a store instruction, it tells the memory subsystem\nto store a certain value at a certain location.  The memory subsystem\npropagates the store to all the other CPUs as well as to RAM.  (As a\nspecial case, we say that the store propagates to its own CPU at the\ntime it is executed.)  The memory subsystem also determines where the\nstore falls in the location's coherence order.  In particular, it must\narrange for the store to be co-later than (i.e., to overwrite) any\nother store to the same location which has already propagated to CPU C.\n\nWhen a CPU executes a load instruction R, it first checks to see\nwhether there are any as-yet unexecuted store instructions, for the\nsame location, that come before R in program order.  If there are, it\nuses the value of the po-latest such store as the value obtained by R,\nand we say that the store's value is forwarded to R.  Otherwise, the\nCPU asks the memory subsystem for the value to load and we say that R\nis satisfied from memory.  The memory subsystem hands back the value\nof the co-latest store to the location in question which has already\npropagated to that CPU.\n\n(In fact, the picture needs to be a little more complicated than this.\nCPUs have local caches, and propagating a store to a CPU really means\npropagating it to the CPU's local cache.  A local cache can take some\ntime to process the stores that it receives, and a store can't be used\nto satisfy one of the CPU's loads until it has been processed.  On\nmost architectures, the local caches process stores in\nFirst-In-First-Out order, and consequently the processing delay\ndoesn't matter for the memory model.  But on Alpha, the local caches\nhave a partitioned design that results in non-FIFO behavior.  We will\ndiscuss this in more detail later.)\n\nNote that load instructions may be executed speculatively and may be\nrestarted under certain circumstances.  The memory model ignores these\npremature executions; we simply say that the load executes at the\nfinal time it is forwarded or satisfied.\n\nExecuting a fence (or memory barrier) instruction doesn't require a\nCPU to do anything special other than informing the memory subsystem\nabout the fence.  However, fences do constrain the way CPUs and the\nmemory subsystem handle other instructions, in two respects.\n\nFirst, a fence forces the CPU to execute various instructions in\nprogram order.  Exactly which instructions are ordered depends on the\ntype of fence:\n\n\tStrong fences, including smp_mb() and synchronize_rcu(), force\n\tthe CPU to execute all po-earlier instructions before any\n\tpo-later instructions;\n\n\tsmp_rmb() forces the CPU to execute all po-earlier loads\n\tbefore any po-later loads;\n\n\tsmp_wmb() forces the CPU to execute all po-earlier stores\n\tbefore any po-later stores;\n\n\tAcquire fences, such as smp_load_acquire(), force the CPU to\n\texecute the load associated with the fence (e.g., the load\n\tpart of an smp_load_acquire()) before any po-later\n\tinstructions;\n\n\tRelease fences, such as smp_store_release(), force the CPU to\n\texecute all po-earlier instructions before the store\n\tassociated with the fence (e.g., the store part of an\n\tsmp_store_release()).\n\nSecond, some types of fence affect the way the memory subsystem\npropagates stores.  When a fence instruction is executed on CPU C:\n\n\tFor each other CPU C', smp_wmb() forces all po-earlier stores\n\ton C to propagate to C' before any po-later stores do.\n\n\tFor each other CPU C', any store which propagates to C before\n\ta release fence is executed (including all po-earlier\n\tstores executed on C) is forced to propagate to C' before the\n\tstore associated with the release fence does.\n\n\tAny store which propagates to C before a strong fence is\n\texecuted (including all po-earlier stores on C) is forced to\n\tpropagate to all other CPUs before any instructions po-after\n\tthe strong fence are executed on C.\n\nThe propagation ordering enforced by release fences and strong fences\naffects stores from other CPUs that propagate to CPU C before the\nfence is executed, as well as stores that are executed on C before the\nfence.  We describe this property by saying that release fences and\nstrong fences are A-cumulative.  By contrast, smp_wmb() fences are not\nA-cumulative; they only affect the propagation of stores that are\nexecuted on C before the fence (i.e., those which precede the fence in\nprogram order).\n\nrcu_read_lock(), rcu_read_unlock(), and synchronize_rcu() fences have\nother properties which we discuss later.\n\n\nPROPAGATION ORDER RELATION: cumul-fence\n---------------------------------------\n\nThe fences which affect propagation order (i.e., strong, release, and\nsmp_wmb() fences) are collectively referred to as cumul-fences, even\nthough smp_wmb() isn't A-cumulative.  The cumul-fence relation is\ndefined to link memory access events E and F whenever:\n\n\tE and F are both stores on the same CPU and an smp_wmb() fence\n\tevent occurs between them in program order; or\n\n\tF is a release fence and some X comes before F in program order,\n\twhere either X = E or else E ->rf X; or\n\n\tA strong fence event occurs between some X and F in program\n\torder, where either X = E or else E ->rf X.\n\nThe operational model requires that whenever W and W' are both stores\nand W ->cumul-fence W', then W must propagate to any given CPU\nbefore W' does.  However, for different CPUs C and C', it does not\nrequire W to propagate to C before W' propagates to C'.\n\n\nDERIVATION OF THE LKMM FROM THE OPERATIONAL MODEL\n-------------------------------------------------\n\nThe LKMM is derived from the restrictions imposed by the design\noutlined above.  These restrictions involve the necessity of\nmaintaining cache coherence and the fact that a CPU can't operate on a\nvalue before it knows what that value is, among other things.\n\nThe formal version of the LKMM is defined by six requirements, or\naxioms:\n\n\tSequential consistency per variable: This requires that the\n\tsystem obey the four coherency rules.\n\n\tAtomicity: This requires that atomic read-modify-write\n\toperations really are atomic, that is, no other stores can\n\tsneak into the middle of such an update.\n\n\tHappens-before: This requires that certain instructions are\n\texecuted in a specific order.\n\n\tPropagation: This requires that certain stores propagate to\n\tCPUs and to RAM in a specific order.\n\n\tRcu: This requires that RCU read-side critical sections and\n\tgrace periods obey the rules of RCU, in particular, the\n\tGrace-Period Guarantee.\n\n\tPlain-coherence: This requires that plain memory accesses\n\t(those not using READ_ONCE(), WRITE_ONCE(), etc.) must obey\n\tthe operational model's rules regarding cache coherence.\n\nThe first and second are quite common; they can be found in many\nmemory models (such as those for C11/C++11).  The \"happens-before\" and\n\"propagation\" axioms have analogs in other memory models as well.  The\n\"rcu\" and \"plain-coherence\" axioms are specific to the LKMM.\n\nEach of these axioms is discussed below.\n\n\nSEQUENTIAL CONSISTENCY PER VARIABLE\n-----------------------------------\n\nAccording to the principle of cache coherence, the stores to any fixed\nshared location in memory form a global ordering.  We can imagine\ninserting the loads from that location into this ordering, by placing\neach load between the store that it reads from and the following\nstore.  This leaves the relative positions of loads that read from the\nsame store unspecified; let's say they are inserted in program order,\nfirst for CPU 0, then CPU 1, etc.\n\nYou can check that the four coherency rules imply that the rf, co, fr,\nand po-loc relations agree with this global ordering; in other words,\nwhenever we have X ->rf Y or X ->co Y or X ->fr Y or X ->po-loc Y, the\nX event comes before the Y event in the global ordering.  The LKMM's\n\"coherence\" axiom expresses this by requiring the union of these\nrelations not to have any cycles.  This means it must not be possible\nto find events\n\n\tX0 -> X1 -> X2 -> ... -> Xn -> X0,\n\nwhere each of the links is either rf, co, fr, or po-loc.  This has to\nhold if the accesses to the fixed memory location can be ordered as\ncache coherence demands.\n\nAlthough it is not obvious, it can be shown that the converse is also\ntrue: This LKMM axiom implies that the four coherency rules are\nobeyed.\n\n\nATOMIC UPDATES: rmw\n-------------------\n\nWhat does it mean to say that a read-modify-write (rmw) update, such\nas atomic_inc(&x), is atomic?  It means that the memory location (x in\nthis case) does not get altered between the read and the write events\nmaking up the atomic operation.  In particular, if two CPUs perform\natomic_inc(&x) concurrently, it must be guaranteed that the final\nvalue of x will be the initial value plus two.  We should never have\nthe following sequence of events:\n\n\tCPU 0 loads x obtaining 13;\n\t\t\t\t\tCPU 1 loads x obtaining 13;\n\tCPU 0 stores 14 to x;\n\t\t\t\t\tCPU 1 stores 14 to x;\n\nwhere the final value of x is wrong (14 rather than 15).\n\nIn this example, CPU 0's increment effectively gets lost because it\noccurs in between CPU 1's load and store.  To put it another way, the\nproblem is that the position of CPU 0's store in x's coherence order\nis between the store that CPU 1 reads from and the store that CPU 1\nperforms.\n\nThe same analysis applies to all atomic update operations.  Therefore,\nto enforce atomicity the LKMM requires that atomic updates follow this\nrule: Whenever R and W are the read and write events composing an\natomic read-modify-write and W' is the write event which R reads from,\nthere must not be any stores coming between W' and W in the coherence\norder.  Equivalently,\n\n\t(R ->rmw W) implies (there is no X with R ->fr X and X ->co W),\n\nwhere the rmw relation links the read and write events making up each\natomic update.  This is what the LKMM's \"atomic\" axiom says.\n\nAtomic rmw updates play one more role in the LKMM: They can form \"rmw\nsequences\".  An rmw sequence is simply a bunch of atomic updates where\neach update reads from the previous one.  Written using events, it\nlooks like this:\n\n\tZ0 ->rf Y1 ->rmw Z1 ->rf ... ->rf Yn ->rmw Zn,\n\nwhere Z0 is some store event and n can be any number (even 0, in the\ndegenerate case).  We write this relation as: Z0 ->rmw-sequence Zn.\nNote that this implies Z0 and Zn are stores to the same variable.\n\nRmw sequences have a special property in the LKMM: They can extend the\ncumul-fence relation.  That is, if we have:\n\n\tU ->cumul-fence X -> rmw-sequence Y\n\nthen also U ->cumul-fence Y.  Thinking about this in terms of the\noperational model, U ->cumul-fence X says that the store U propagates\nto each CPU before the store X does.  Then the fact that X and Y are\nlinked by an rmw sequence means that U also propagates to each CPU\nbefore Y does.  In an analogous way, rmw sequences can also extend\nthe w-post-bounded relation defined below in the PLAIN ACCESSES AND\nDATA RACES section.\n\n(The notion of rmw sequences in the LKMM is similar to, but not quite\nthe same as, that of release sequences in the C11 memory model.  They\nwere added to the LKMM to fix an obscure bug; without them, atomic\nupdates with full-barrier semantics did not always guarantee ordering\nat least as strong as atomic updates with release-barrier semantics.)\n\n\nTHE PRESERVED PROGRAM ORDER RELATION: ppo\n-----------------------------------------\n\nThere are many situations where a CPU is obliged to execute two\ninstructions in program order.  We amalgamate them into the ppo (for\n\"preserved program order\") relation, which links the po-earlier\ninstruction to the po-later instruction and is thus a sub-relation of\npo.\n\nThe operational model already includes a description of one such\nsituation: Fences are a source of ppo links.  Suppose X and Y are\nmemory accesses with X ->po Y; then the CPU must execute X before Y if\nany of the following hold:\n\n\tA strong (smp_mb() or synchronize_rcu()) fence occurs between\n\tX and Y;\n\n\tX and Y are both stores and an smp_wmb() fence occurs between\n\tthem;\n\n\tX and Y are both loads and an smp_rmb() fence occurs between\n\tthem;\n\n\tX is also an acquire fence, such as smp_load_acquire();\n\n\tY is also a release fence, such as smp_store_release().\n\nAnother possibility, not mentioned earlier but discussed in the next\nsection, is:\n\n\tX and Y are both loads, X ->addr Y (i.e., there is an address\n\tdependency from X to Y), and X is a READ_ONCE() or an atomic\n\taccess.\n\nDependencies can also cause instructions to be executed in program\norder.  This is uncontroversial when the second instruction is a\nstore; either a data, address, or control dependency from a load R to\na store W will force the CPU to execute R before W.  This is very\nsimply because the CPU cannot tell the memory subsystem about W's\nstore before it knows what value should be stored (in the case of a\ndata dependency), what location it should be stored into (in the case\nof an address dependency), or whether the store should actually take\nplace (in the case of a control dependency).\n\nDependencies to load instructions are more problematic.  To begin with,\nthere is no such thing as a data dependency to a load.  Next, a CPU\nhas no reason to respect a control dependency to a load, because it\ncan always satisfy the second load speculatively before the first, and\nthen ignore the result if it turns out that the second load shouldn't\nbe executed after all.  And lastly, the real difficulties begin when\nwe consider address dependencies to loads.\n\nTo be fair about it, all Linux-supported architectures do execute\nloads in program order if there is an address dependency between them.\nAfter all, a CPU cannot ask the memory subsystem to load a value from\na particular location before it knows what that location is.  However,\nthe split-cache design used by Alpha can cause it to behave in a way\nthat looks as if the loads were executed out of order (see the next\nsection for more details).  The kernel includes a workaround for this\nproblem when the loads come from READ_ONCE(), and therefore the LKMM\nincludes address dependencies to loads in the ppo relation.\n\nOn the other hand, dependencies can indirectly affect the ordering of\ntwo loads.  This happens when there is a dependency from a load to a\nstore and a second, po-later load reads from that store:\n\n\tR ->dep W ->rfi R',\n\nwhere the dep link can be either an address or a data dependency.  In\nthis situation we know it is possible for the CPU to execute R' before\nW, because it can forward the value that W will store to R'.  But it\ncannot execute R' before R, because it cannot forward the value before\nit knows what that value is, or that W and R' do access the same\nlocation.  However, if there is merely a control dependency between R\nand W then the CPU can speculatively forward W to R' before executing\nR; if the speculation turns out to be wrong then the CPU merely has to\nrestart or abandon R'.\n\n(In theory, a CPU might forward a store to a load when it runs across\nan address dependency like this:\n\n\tr1 = READ_ONCE(ptr);\n\tWRITE_ONCE(*r1, 17);\n\tr2 = READ_ONCE(*r1);\n\nbecause it could tell that the store and the second load access the\nsame location even before it knows what the location's address is.\nHowever, none of the architectures supported by the Linux kernel do\nthis.)\n\nTwo memory accesses of the same location must always be executed in\nprogram order if the second access is a store.  Thus, if we have\n\n\tR ->po-loc W\n\n(the po-loc link says that R comes before W in program order and they\naccess the same location), the CPU is obliged to execute W after R.\nIf it executed W first then the memory subsystem would respond to R's\nread request with the value stored by W (or an even later store), in\nviolation of the read-write coherence rule.  Similarly, if we had\n\n\tW ->po-loc W'\n\nand the CPU executed W' before W, then the memory subsystem would put\nW' before W in the coherence order.  It would effectively cause W to\noverwrite W', in violation of the write-write coherence rule.\n(Interestingly, an early ARMv8 memory model, now obsolete, proposed\nallowing out-of-order writes like this to occur.  The model avoided\nviolating the write-write coherence rule by requiring the CPU not to\nsend the W write to the memory subsystem at all!)\n\n\nAND THEN THERE WAS ALPHA\n------------------------\n\nAs mentioned above, the Alpha architecture is unique in that it does\nnot appear to respect address dependencies to loads.  This means that\ncode such as the following:\n\n\tint x = 0;\n\tint y = -1;\n\tint *ptr = &y;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(ptr, &x);\n\t}\n\n\tP1()\n\t{\n\t\tint *r1;\n\t\tint r2;\n\n\t\tr1 = ptr;\n\t\tr2 = READ_ONCE(*r1);\n\t}\n\ncan malfunction on Alpha systems (notice that P1 uses an ordinary load\nto read ptr instead of READ_ONCE()).  It is quite possible that r1 = &x\nand r2 = 0 at the end, in spite of the address dependency.\n\nAt first glance this doesn't seem to make sense.  We know that the\nsmp_wmb() forces P0's store to x to propagate to P1 before the store\nto ptr does.  And since P1 can't execute its second load\nuntil it knows what location to load from, i.e., after executing its\nfirst load, the value x = 1 must have propagated to P1 before the\nsecond load executed.  So why doesn't r2 end up equal to 1?\n\nThe answer lies in the Alpha's split local caches.  Although the two\nstores do reach P1's local cache in the proper order, it can happen\nthat the first store is processed by a busy part of the cache while\nthe second store is processed by an idle part.  As a result, the x = 1\nvalue may not become available for P1's CPU to read until after the\nptr = &x value does, leading to the undesirable result above.  The\nfinal effect is that even though the two loads really are executed in\nprogram order, it appears that they aren't.\n\nThis could not have happened if the local cache had processed the\nincoming stores in FIFO order.  By contrast, other architectures\nmaintain at least the appearance of FIFO order.\n\nIn practice, this difficulty is solved by inserting a special fence\nbetween P1's two loads when the kernel is compiled for the Alpha\narchitecture.  In fact, as of version 4.15, the kernel automatically\nadds this fence after every READ_ONCE() and atomic load on Alpha.  The\neffect of the fence is to cause the CPU not to execute any po-later\ninstructions until after the local cache has finished processing all\nthe stores it has already received.  Thus, if the code was changed to:\n\n\tP1()\n\t{\n\t\tint *r1;\n\t\tint r2;\n\n\t\tr1 = READ_ONCE(ptr);\n\t\tr2 = READ_ONCE(*r1);\n\t}\n\nthen we would never get r1 = &x and r2 = 0.  By the time P1 executed\nits second load, the x = 1 store would already be fully processed by\nthe local cache and available for satisfying the read request.  Thus\nwe have yet another reason why shared data should always be read with\nREAD_ONCE() or another synchronization primitive rather than accessed\ndirectly.\n\nThe LKMM requires that smp_rmb(), acquire fences, and strong fences\nshare this property: They do not allow the CPU to execute any po-later\ninstructions (or po-later loads in the case of smp_rmb()) until all\noutstanding stores have been processed by the local cache.  In the\ncase of a strong fence, the CPU first has to wait for all of its\npo-earlier stores to propagate to every other CPU in the system; then\nit has to wait for the local cache to process all the stores received\nas of that time -- not just the stores received when the strong fence\nbegan.\n\nAnd of course, none of this matters for any architecture other than\nAlpha.\n\n\nTHE HAPPENS-BEFORE RELATION: hb\n-------------------------------\n\nThe happens-before relation (hb) links memory accesses that have to\nexecute in a certain order.  hb includes the ppo relation and two\nothers, one of which is rfe.\n\nW ->rfe R implies that W and R are on different CPUs.  It also means\nthat W's store must have propagated to R's CPU before R executed;\notherwise R could not have read the value stored by W.  Therefore W\nmust have executed before R, and so we have W ->hb R.\n\nThe equivalent fact need not hold if W ->rfi R (i.e., W and R are on\nthe same CPU).  As we have already seen, the operational model allows\nW's value to be forwarded to R in such cases, meaning that R may well\nexecute before W does.\n\nIt's important to understand that neither coe nor fre is included in\nhb, despite their similarities to rfe.  For example, suppose we have\nW ->coe W'.  This means that W and W' are stores to the same location,\nthey execute on different CPUs, and W comes before W' in the coherence\norder (i.e., W' overwrites W).  Nevertheless, it is possible for W' to\nexecute before W, because the decision as to which store overwrites\nthe other is made later by the memory subsystem.  When the stores are\nnearly simultaneous, either one can come out on top.  Similarly,\nR ->fre W means that W overwrites the value which R reads, but it\ndoesn't mean that W has to execute after R.  All that's necessary is\nfor the memory subsystem not to propagate W to R's CPU until after R\nhas executed, which is possible if W executes shortly before R.\n\nThe third relation included in hb is like ppo, in that it only links\nevents that are on the same CPU.  However it is more difficult to\nexplain, because it arises only indirectly from the requirement of\ncache coherence.  The relation is called prop, and it links two events\non CPU C in situations where a store from some other CPU comes after\nthe first event in the coherence order and propagates to C before the\nsecond event executes.\n\nThis is best explained with some examples.  The simplest case looks\nlike this:\n\n\tint x;\n\n\tP0()\n\t{\n\t\tint r1;\n\n\t\tWRITE_ONCE(x, 1);\n\t\tr1 = READ_ONCE(x);\n\t}\n\n\tP1()\n\t{\n\t\tWRITE_ONCE(x, 8);\n\t}\n\nIf r1 = 8 at the end then P0's accesses must have executed in program\norder.  We can deduce this from the operational model; if P0's load\nhad executed before its store then the value of the store would have\nbeen forwarded to the load, so r1 would have ended up equal to 1, not\n8.  In this case there is a prop link from P0's write event to its read\nevent, because P1's store came after P0's store in x's coherence\norder, and P1's store propagated to P0 before P0's load executed.\n\nAn equally simple case involves two loads of the same location that\nread from different stores:\n\n\tint x = 0;\n\n\tP0()\n\t{\n\t\tint r1, r2;\n\n\t\tr1 = READ_ONCE(x);\n\t\tr2 = READ_ONCE(x);\n\t}\n\n\tP1()\n\t{\n\t\tWRITE_ONCE(x, 9);\n\t}\n\nIf r1 = 0 and r2 = 9 at the end then P0's accesses must have executed\nin program order.  If the second load had executed before the first\nthen the x = 9 store must have been propagated to P0 before the first\nload executed, and so r1 would have been 9 rather than 0.  In this\ncase there is a prop link from P0's first read event to its second,\nbecause P1's store overwrote the value read by P0's first load, and\nP1's store propagated to P0 before P0's second load executed.\n\nLess trivial examples of prop all involve fences.  Unlike the simple\nexamples above, they can require that some instructions are executed\nout of program order.  This next one should look familiar:\n\n\tint buf = 0, flag = 0;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(buf, 1);\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(flag, 1);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\t\tint r2;\n\n\t\tr1 = READ_ONCE(flag);\n\t\tr2 = READ_ONCE(buf);\n\t}\n\nThis is the MP pattern again, with an smp_wmb() fence between the two\nstores.  If r1 = 1 and r2 = 0 at the end then there is a prop link\nfrom P1's second load to its first (backwards!).  The reason is\nsimilar to the previous examples: The value P1 loads from buf gets\noverwritten by P0's store to buf, the fence guarantees that the store\nto buf will propagate to P1 before the store to flag does, and the\nstore to flag propagates to P1 before P1 reads flag.\n\nThe prop link says that in order to obtain the r1 = 1, r2 = 0 result,\nP1 must execute its second load before the first.  Indeed, if the load\nfrom flag were executed first, then the buf = 1 store would already\nhave propagated to P1 by the time P1's load from buf executed, so r2\nwould have been 1 at the end, not 0.  (The reasoning holds even for\nAlpha, although the details are more complicated and we will not go\ninto them.)\n\nBut what if we put an smp_rmb() fence between P1's loads?  The fence\nwould force the two loads to be executed in program order, and it\nwould generate a cycle in the hb relation: The fence would create a ppo\nlink (hence an hb link) from the first load to the second, and the\nprop relation would give an hb link from the second load to the first.\nSince an instruction can't execute before itself, we are forced to\nconclude that if an smp_rmb() fence is added, the r1 = 1, r2 = 0\noutcome is impossible -- as it should be.\n\nThe formal definition of the prop relation involves a coe or fre link,\nfollowed by an arbitrary number of cumul-fence links, ending with an\nrfe link.  You can concoct more exotic examples, containing more than\none fence, although this quickly leads to diminishing returns in terms\nof complexity.  For instance, here's an example containing a coe link\nfollowed by two cumul-fences and an rfe link, utilizing the fact that\nrelease fences are A-cumulative:\n\n\tint x, y, z;\n\n\tP0()\n\t{\n\t\tint r0;\n\n\t\tWRITE_ONCE(x, 1);\n\t\tr0 = READ_ONCE(z);\n\t}\n\n\tP1()\n\t{\n\t\tWRITE_ONCE(x, 2);\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(y, 1);\n\t}\n\n\tP2()\n\t{\n\t\tint r2;\n\n\t\tr2 = READ_ONCE(y);\n\t\tsmp_store_release(&z, 1);\n\t}\n\nIf x = 2, r0 = 1, and r2 = 1 after this code runs then there is a prop\nlink from P0's store to its load.  This is because P0's store gets\noverwritten by P1's store since x = 2 at the end (a coe link), the\nsmp_wmb() ensures that P1's store to x propagates to P2 before the\nstore to y does (the first cumul-fence), the store to y propagates to P2\nbefore P2's load and store execute, P2's smp_store_release()\nguarantees that the stores to x and y both propagate to P0 before the\nstore to z does (the second cumul-fence), and P0's load executes after the\nstore to z has propagated to P0 (an rfe link).\n\nIn summary, the fact that the hb relation links memory access events\nin the order they execute means that it must not have cycles.  This\nrequirement is the content of the LKMM's \"happens-before\" axiom.\n\nThe LKMM defines yet another relation connected to times of\ninstruction execution, but it is not included in hb.  It relies on the\nparticular properties of strong fences, which we cover in the next\nsection.\n\n\nTHE PROPAGATES-BEFORE RELATION: pb\n----------------------------------\n\nThe propagates-before (pb) relation capitalizes on the special\nfeatures of strong fences.  It links two events E and F whenever some\nstore is coherence-later than E and propagates to every CPU and to RAM\nbefore F executes.  The formal definition requires that E be linked to\nF via a coe or fre link, an arbitrary number of cumul-fences, an\noptional rfe link, a strong fence, and an arbitrary number of hb\nlinks.  Let's see how this definition works out.\n\nConsider first the case where E is a store (implying that the sequence\nof links begins with coe).  Then there are events W, X, Y, and Z such\nthat:\n\n\tE ->coe W ->cumul-fence* X ->rfe? Y ->strong-fence Z ->hb* F,\n\nwhere the * suffix indicates an arbitrary number of links of the\nspecified type, and the ? suffix indicates the link is optional (Y may\nbe equal to X).  Because of the cumul-fence links, we know that W will\npropagate to Y's CPU before X does, hence before Y executes and hence\nbefore the strong fence executes.  Because this fence is strong, we\nknow that W will propagate to every CPU and to RAM before Z executes.\nAnd because of the hb links, we know that Z will execute before F.\nThus W, which comes later than E in the coherence order, will\npropagate to every CPU and to RAM before F executes.\n\nThe case where E is a load is exactly the same, except that the first\nlink in the sequence is fre instead of coe.\n\nThe existence of a pb link from E to F implies that E must execute\nbefore F.  To see why, suppose that F executed first.  Then W would\nhave propagated to E's CPU before E executed.  If E was a store, the\nmemory subsystem would then be forced to make E come after W in the\ncoherence order, contradicting the fact that E ->coe W.  If E was a\nload, the memory subsystem would then be forced to satisfy E's read\nrequest with the value stored by W or an even later store,\ncontradicting the fact that E ->fre W.\n\nA good example illustrating how pb works is the SB pattern with strong\nfences:\n\n\tint x = 0, y = 0;\n\n\tP0()\n\t{\n\t\tint r0;\n\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_mb();\n\t\tr0 = READ_ONCE(y);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\tWRITE_ONCE(y, 1);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nIf r0 = 0 at the end then there is a pb link from P0's load to P1's\nload: an fre link from P0's load to P1's store (which overwrites the\nvalue read by P0), and a strong fence between P1's store and its load.\nIn this example, the sequences of cumul-fence and hb links are empty.\nNote that this pb link is not included in hb as an instance of prop,\nbecause it does not start and end on the same CPU.\n\nSimilarly, if r1 = 0 at the end then there is a pb link from P1's load\nto P0's.  This means that if both r1 and r2 were 0 there would be a\ncycle in pb, which is not possible since an instruction cannot execute\nbefore itself.  Thus, adding smp_mb() fences to the SB pattern\nprevents the r0 = 0, r1 = 0 outcome.\n\nIn summary, the fact that the pb relation links events in the order\nthey execute means that it cannot have cycles.  This requirement is\nthe content of the LKMM's \"propagation\" axiom.\n\n\nRCU RELATIONS: rcu-link, rcu-gp, rcu-rscsi, rcu-order, rcu-fence, and rb\n------------------------------------------------------------------------\n\nRCU (Read-Copy-Update) is a powerful synchronization mechanism.  It\nrests on two concepts: grace periods and read-side critical sections.\n\nA grace period is the span of time occupied by a call to\nsynchronize_rcu().  A read-side critical section (or just critical\nsection, for short) is a region of code delimited by rcu_read_lock()\nat the start and rcu_read_unlock() at the end.  Critical sections can\nbe nested, although we won't make use of this fact.\n\nAs far as memory models are concerned, RCU's main feature is its\nGrace-Period Guarantee, which states that a critical section can never\nspan a full grace period.  In more detail, the Guarantee says:\n\n\tFor any critical section C and any grace period G, at least\n\tone of the following statements must hold:\n\n(1)\tC ends before G does, and in addition, every store that\n\tpropagates to C's CPU before the end of C must propagate to\n\tevery CPU before G ends.\n\n(2)\tG starts before C does, and in addition, every store that\n\tpropagates to G's CPU before the start of G must propagate\n\tto every CPU before C starts.\n\nIn particular, it is not possible for a critical section to both start\nbefore and end after a grace period.\n\nHere is a simple example of RCU in action:\n\n\tint x, y;\n\n\tP0()\n\t{\n\t\trcu_read_lock();\n\t\tWRITE_ONCE(x, 1);\n\t\tWRITE_ONCE(y, 1);\n\t\trcu_read_unlock();\n\t}\n\n\tP1()\n\t{\n\t\tint r1, r2;\n\n\t\tr1 = READ_ONCE(x);\n\t\tsynchronize_rcu();\n\t\tr2 = READ_ONCE(y);\n\t}\n\nThe Grace Period Guarantee tells us that when this code runs, it will\nnever end with r1 = 1 and r2 = 0.  The reasoning is as follows.  r1 = 1\nmeans that P0's store to x propagated to P1 before P1 called\nsynchronize_rcu(), so P0's critical section must have started before\nP1's grace period, contrary to part (2) of the Guarantee.  On the\nother hand, r2 = 0 means that P0's store to y, which occurs before the\nend of the critical section, did not propagate to P1 before the end of\nthe grace period, contrary to part (1).  Together the results violate\nthe Guarantee.\n\nIn the kernel's implementations of RCU, the requirements for stores\nto propagate to every CPU are fulfilled by placing strong fences at\nsuitable places in the RCU-related code.  Thus, if a critical section\nstarts before a grace period does then the critical section's CPU will\nexecute an smp_mb() fence after the end of the critical section and\nsome time before the grace period's synchronize_rcu() call returns.\nAnd if a critical section ends after a grace period does then the\nsynchronize_rcu() routine will execute an smp_mb() fence at its start\nand some time before the critical section's opening rcu_read_lock()\nexecutes.\n\nWhat exactly do we mean by saying that a critical section \"starts\nbefore\" or \"ends after\" a grace period?  Some aspects of the meaning\nare pretty obvious, as in the example above, but the details aren't\nentirely clear.  The LKMM formalizes this notion by means of the\nrcu-link relation.  rcu-link encompasses a very general notion of\n\"before\": If E and F are RCU fence events (i.e., rcu_read_lock(),\nrcu_read_unlock(), or synchronize_rcu()) then among other things,\nE ->rcu-link F includes cases where E is po-before some memory-access\nevent X, F is po-after some memory-access event Y, and we have any of\nX ->rfe Y, X ->co Y, or X ->fr Y.\n\nThe formal definition of the rcu-link relation is more than a little\nobscure, and we won't give it here.  It is closely related to the pb\nrelation, and the details don't matter unless you want to comb through\na somewhat lengthy formal proof.  Pretty much all you need to know\nabout rcu-link is the information in the preceding paragraph.\n\nThe LKMM also defines the rcu-gp and rcu-rscsi relations.  They bring\ngrace periods and read-side critical sections into the picture, in the\nfollowing way:\n\n\tE ->rcu-gp F means that E and F are in fact the same event,\n\tand that event is a synchronize_rcu() fence (i.e., a grace\n\tperiod).\n\n\tE ->rcu-rscsi F means that E and F are the rcu_read_unlock()\n\tand rcu_read_lock() fence events delimiting some read-side\n\tcritical section.  (The 'i' at the end of the name emphasizes\n\tthat this relation is \"inverted\": It links the end of the\n\tcritical section to the start.)\n\nIf we think of the rcu-link relation as standing for an extended\n\"before\", then X ->rcu-gp Y ->rcu-link Z roughly says that X is a\ngrace period which ends before Z begins.  (In fact it covers more than\nthis, because it also includes cases where some store propagates to\nZ's CPU before Z begins but doesn't propagate to some other CPU until\nafter X ends.)  Similarly, X ->rcu-rscsi Y ->rcu-link Z says that X is\nthe end of a critical section which starts before Z begins.\n\nThe LKMM goes on to define the rcu-order relation as a sequence of\nrcu-gp and rcu-rscsi links separated by rcu-link links, in which the\nnumber of rcu-gp links is >= the number of rcu-rscsi links.  For\nexample:\n\n\tX ->rcu-gp Y ->rcu-link Z ->rcu-rscsi T ->rcu-link U ->rcu-gp V\n\nwould imply that X ->rcu-order V, because this sequence contains two\nrcu-gp links and one rcu-rscsi link.  (It also implies that\nX ->rcu-order T and Z ->rcu-order V.)  On the other hand:\n\n\tX ->rcu-rscsi Y ->rcu-link Z ->rcu-rscsi T ->rcu-link U ->rcu-gp V\n\ndoes not imply X ->rcu-order V, because the sequence contains only\none rcu-gp link but two rcu-rscsi links.\n\nThe rcu-order relation is important because the Grace Period Guarantee\nmeans that rcu-order links act kind of like strong fences.  In\nparticular, E ->rcu-order F implies not only that E begins before F\nends, but also that any write po-before E will propagate to every CPU\nbefore any instruction po-after F can execute.  (However, it does not\nimply that E must execute before F; in fact, each synchronize_rcu()\nfence event is linked to itself by rcu-order as a degenerate case.)\n\nTo prove this in full generality requires some intellectual effort.\nWe'll consider just a very simple case:\n\n\tG ->rcu-gp W ->rcu-link Z ->rcu-rscsi F.\n\nThis formula means that G and W are the same event (a grace period),\nand there are events X, Y and a read-side critical section C such that:\n\n\t1. G = W is po-before or equal to X;\n\n\t2. X comes \"before\" Y in some sense (including rfe, co and fr);\n\n\t3. Y is po-before Z;\n\n\t4. Z is the rcu_read_unlock() event marking the end of C;\n\n\t5. F is the rcu_read_lock() event marking the start of C.\n\nFrom 1 - 4 we deduce that the grace period G ends before the critical\nsection C.  Then part (2) of the Grace Period Guarantee says not only\nthat G starts before C does, but also that any write which executes on\nG's CPU before G starts must propagate to every CPU before C starts.\nIn particular, the write propagates to every CPU before F finishes\nexecuting and hence before any instruction po-after F can execute.\nThis sort of reasoning can be extended to handle all the situations\ncovered by rcu-order.\n\nThe rcu-fence relation is a simple extension of rcu-order.  While\nrcu-order only links certain fence events (calls to synchronize_rcu(),\nrcu_read_lock(), or rcu_read_unlock()), rcu-fence links any events\nthat are separated by an rcu-order link.  This is analogous to the way\nthe strong-fence relation links events that are separated by an\nsmp_mb() fence event (as mentioned above, rcu-order links act kind of\nlike strong fences).  Written symbolically, X ->rcu-fence Y means\nthere are fence events E and F such that:\n\n\tX ->po E ->rcu-order F ->po Y.\n\nFrom the discussion above, we see this implies not only that X\nexecutes before Y, but also (if X is a store) that X propagates to\nevery CPU before Y executes.  Thus rcu-fence is sort of a\n\"super-strong\" fence: Unlike the original strong fences (smp_mb() and\nsynchronize_rcu()), rcu-fence is able to link events on different\nCPUs.  (Perhaps this fact should lead us to say that rcu-fence isn't\nreally a fence at all!)\n\nFinally, the LKMM defines the RCU-before (rb) relation in terms of\nrcu-fence.  This is done in essentially the same way as the pb\nrelation was defined in terms of strong-fence.  We will omit the\ndetails; the end result is that E ->rb F implies E must execute\nbefore F, just as E ->pb F does (and for much the same reasons).\n\nPutting this all together, the LKMM expresses the Grace Period\nGuarantee by requiring that the rb relation does not contain a cycle.\nEquivalently, this \"rcu\" axiom requires that there are no events E\nand F with E ->rcu-link F ->rcu-order E.  Or to put it a third way,\nthe axiom requires that there are no cycles consisting of rcu-gp and\nrcu-rscsi alternating with rcu-link, where the number of rcu-gp links\nis >= the number of rcu-rscsi links.\n\nJustifying the axiom isn't easy, but it is in fact a valid\nformalization of the Grace Period Guarantee.  We won't attempt to go\nthrough the detailed argument, but the following analysis gives a\ntaste of what is involved.  Suppose both parts of the Guarantee are\nviolated: A critical section starts before a grace period, and some\nstore propagates to the critical section's CPU before the end of the\ncritical section but doesn't propagate to some other CPU until after\nthe end of the grace period.\n\nPutting symbols to these ideas, let L and U be the rcu_read_lock() and\nrcu_read_unlock() fence events delimiting the critical section in\nquestion, and let S be the synchronize_rcu() fence event for the grace\nperiod.  Saying that the critical section starts before S means there\nare events Q and R where Q is po-after L (which marks the start of the\ncritical section), Q is \"before\" R in the sense used by the rcu-link\nrelation, and R is po-before the grace period S.  Thus we have:\n\n\tL ->rcu-link S.\n\nLet W be the store mentioned above, let Y come before the end of the\ncritical section and witness that W propagates to the critical\nsection's CPU by reading from W, and let Z on some arbitrary CPU be a\nwitness that W has not propagated to that CPU, where Z happens after\nsome event X which is po-after S.  Symbolically, this amounts to:\n\n\tS ->po X ->hb* Z ->fr W ->rf Y ->po U.\n\nThe fr link from Z to W indicates that W has not propagated to Z's CPU\nat the time that Z executes.  From this, it can be shown (see the\ndiscussion of the rcu-link relation earlier) that S and U are related\nby rcu-link:\n\n\tS ->rcu-link U.\n\nSince S is a grace period we have S ->rcu-gp S, and since L and U are\nthe start and end of the critical section C we have U ->rcu-rscsi L.\nFrom this we obtain:\n\n\tS ->rcu-gp S ->rcu-link U ->rcu-rscsi L ->rcu-link S,\n\na forbidden cycle.  Thus the \"rcu\" axiom rules out this violation of\nthe Grace Period Guarantee.\n\nFor something a little more down-to-earth, let's see how the axiom\nworks out in practice.  Consider the RCU code example from above, this\ntime with statement labels added:\n\n\tint x, y;\n\n\tP0()\n\t{\n\t\tL: rcu_read_lock();\n\t\tX: WRITE_ONCE(x, 1);\n\t\tY: WRITE_ONCE(y, 1);\n\t\tU: rcu_read_unlock();\n\t}\n\n\tP1()\n\t{\n\t\tint r1, r2;\n\n\t\tZ: r1 = READ_ONCE(x);\n\t\tS: synchronize_rcu();\n\t\tW: r2 = READ_ONCE(y);\n\t}\n\n\nIf r2 = 0 at the end then P0's store at Y overwrites the value that\nP1's load at W reads from, so we have W ->fre Y.  Since S ->po W and\nalso Y ->po U, we get S ->rcu-link U.  In addition, S ->rcu-gp S\nbecause S is a grace period.\n\nIf r1 = 1 at the end then P1's load at Z reads from P0's store at X,\nso we have X ->rfe Z.  Together with L ->po X and Z ->po S, this\nyields L ->rcu-link S.  And since L and U are the start and end of a\ncritical section, we have U ->rcu-rscsi L.\n\nThen U ->rcu-rscsi L ->rcu-link S ->rcu-gp S ->rcu-link U is a\nforbidden cycle, violating the \"rcu\" axiom.  Hence the outcome is not\nallowed by the LKMM, as we would expect.\n\nFor contrast, let's see what can happen in a more complicated example:\n\n\tint x, y, z;\n\n\tP0()\n\t{\n\t\tint r0;\n\n\t\tL0: rcu_read_lock();\n\t\t    r0 = READ_ONCE(x);\n\t\t    WRITE_ONCE(y, 1);\n\t\tU0: rcu_read_unlock();\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\t    r1 = READ_ONCE(y);\n\t\tS1: synchronize_rcu();\n\t\t    WRITE_ONCE(z, 1);\n\t}\n\n\tP2()\n\t{\n\t\tint r2;\n\n\t\tL2: rcu_read_lock();\n\t\t    r2 = READ_ONCE(z);\n\t\t    WRITE_ONCE(x, 1);\n\t\tU2: rcu_read_unlock();\n\t}\n\nIf r0 = r1 = r2 = 1 at the end, then similar reasoning to before shows\nthat U0 ->rcu-rscsi L0 ->rcu-link S1 ->rcu-gp S1 ->rcu-link U2 ->rcu-rscsi\nL2 ->rcu-link U0.  However this cycle is not forbidden, because the\nsequence of relations contains fewer instances of rcu-gp (one) than of\nrcu-rscsi (two).  Consequently the outcome is allowed by the LKMM.\nThe following instruction timing diagram shows how it might actually\noccur:\n\nP0\t\t\tP1\t\t\tP2\n--------------------\t--------------------\t--------------------\nrcu_read_lock()\nWRITE_ONCE(y, 1)\n\t\t\tr1 = READ_ONCE(y)\n\t\t\tsynchronize_rcu() starts\n\t\t\t.\t\t\trcu_read_lock()\n\t\t\t.\t\t\tWRITE_ONCE(x, 1)\nr0 = READ_ONCE(x)\t.\nrcu_read_unlock()\t.\n\t\t\tsynchronize_rcu() ends\n\t\t\tWRITE_ONCE(z, 1)\n\t\t\t\t\t\tr2 = READ_ONCE(z)\n\t\t\t\t\t\trcu_read_unlock()\n\nThis requires P0 and P2 to execute their loads and stores out of\nprogram order, but of course they are allowed to do so.  And as you\ncan see, the Grace Period Guarantee is not violated: The critical\nsection in P0 both starts before P1's grace period does and ends\nbefore it does, and the critical section in P2 both starts after P1's\ngrace period does and ends after it does.\n\nThe LKMM supports SRCU (Sleepable Read-Copy-Update) in addition to\nnormal RCU.  The ideas involved are much the same as above, with new\nrelations srcu-gp and srcu-rscsi added to represent SRCU grace periods\nand read-side critical sections.  However, there are some significant\ndifferences between RCU read-side critical sections and their SRCU\ncounterparts, as described in the next section.\n\n\nSRCU READ-SIDE CRITICAL SECTIONS\n--------------------------------\n\nThe LKMM uses the srcu-rscsi relation to model SRCU read-side critical\nsections.  They differ from RCU read-side critical sections in the\nfollowing respects:\n\n1.\tUnlike the analogous RCU primitives, synchronize_srcu(),\n\tsrcu_read_lock(), and srcu_read_unlock() take a pointer to a\n\tstruct srcu_struct as an argument.  This structure is called\n\tan SRCU domain, and calls linked by srcu-rscsi must have the\n\tsame domain.  Read-side critical sections and grace periods\n\tassociated with different domains are independent of one\n\tanother; the SRCU version of the RCU Guarantee applies only\n\tto pairs of critical sections and grace periods having the\n\tsame domain.\n\n2.\tsrcu_read_lock() returns a value, called the index, which must\n\tbe passed to the matching srcu_read_unlock() call.  Unlike\n\trcu_read_lock() and rcu_read_unlock(), an srcu_read_lock()\n\tcall does not always have to match the next unpaired\n\tsrcu_read_unlock().  In fact, it is possible for two SRCU\n\tread-side critical sections to overlap partially, as in the\n\tfollowing example (where s is an srcu_struct and idx1 and idx2\n\tare integer variables):\n\n\t\tidx1 = srcu_read_lock(&s);\t// Start of first RSCS\n\t\tidx2 = srcu_read_lock(&s);\t// Start of second RSCS\n\t\tsrcu_read_unlock(&s, idx1);\t// End of first RSCS\n\t\tsrcu_read_unlock(&s, idx2);\t// End of second RSCS\n\n\tThe matching is determined entirely by the domain pointer and\n\tindex value.  By contrast, if the calls had been\n\trcu_read_lock() and rcu_read_unlock() then they would have\n\tcreated two nested (fully overlapping) read-side critical\n\tsections: an inner one and an outer one.\n\n3.\tThe srcu_down_read() and srcu_up_read() primitives work\n\texactly like srcu_read_lock() and srcu_read_unlock(), except\n\tthat matching calls don't have to execute on the same CPU.\n\t(The names are meant to be suggestive of operations on\n\tsemaphores.)  Since the matching is determined by the domain\n\tpointer and index value, these primitives make it possible for\n\tan SRCU read-side critical section to start on one CPU and end\n\ton another, so to speak.\n\nIn order to account for these properties of SRCU, the LKMM models\nsrcu_read_lock() as a special type of load event (which is\nappropriate, since it takes a memory location as argument and returns\na value, just as a load does) and srcu_read_unlock() as a special type\nof store event (again appropriate, since it takes as arguments a\nmemory location and a value).  These loads and stores are annotated as\nbelonging to the \"srcu-lock\" and \"srcu-unlock\" event classes\nrespectively.\n\nThis approach allows the LKMM to tell whether two events are\nassociated with the same SRCU domain, simply by checking whether they\naccess the same memory location (i.e., they are linked by the loc\nrelation).  It also gives a way to tell which unlock matches a\nparticular lock, by checking for the presence of a data dependency\nfrom the load (srcu-lock) to the store (srcu-unlock).  For example,\ngiven the situation outlined earlier (with statement labels added):\n\n\tA: idx1 = srcu_read_lock(&s);\n\tB: idx2 = srcu_read_lock(&s);\n\tC: srcu_read_unlock(&s, idx1);\n\tD: srcu_read_unlock(&s, idx2);\n\nthe LKMM will treat A and B as loads from s yielding values saved in\nidx1 and idx2 respectively.  Similarly, it will treat C and D as\nthough they stored the values from idx1 and idx2 in s.  The end result\nis much as if we had written:\n\n\tA: idx1 = READ_ONCE(s);\n\tB: idx2 = READ_ONCE(s);\n\tC: WRITE_ONCE(s, idx1);\n\tD: WRITE_ONCE(s, idx2);\n\nexcept for the presence of the special srcu-lock and srcu-unlock\nannotations.  You can see at once that we have A ->data C and\nB ->data D.  These dependencies tell the LKMM that C is the\nsrcu-unlock event matching srcu-lock event A, and D is the\nsrcu-unlock event matching srcu-lock event B.\n\nThis approach is admittedly a hack, and it has the potential to lead\nto problems.  For example, in:\n\n\tidx1 = srcu_read_lock(&s);\n\tsrcu_read_unlock(&s, idx1);\n\tidx2 = srcu_read_lock(&s);\n\tsrcu_read_unlock(&s, idx2);\n\nthe LKMM will believe that idx2 must have the same value as idx1,\nsince it reads from the immediately preceding store of idx1 in s.\nFortunately this won't matter, assuming that litmus tests never do\nanything with SRCU index values other than pass them to\nsrcu_read_unlock() or srcu_up_read() calls.\n\nHowever, sometimes it is necessary to store an index value in a\nshared variable temporarily.  In fact, this is the only way for\nsrcu_down_read() to pass the index it gets to an srcu_up_read() call\non a different CPU.  In more detail, we might have soething like:\n\n\tstruct srcu_struct s;\n\tint x;\n\n\tP0()\n\t{\n\t\tint r0;\n\n\t\tA: r0 = srcu_down_read(&s);\n\t\tB: WRITE_ONCE(x, r0);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\tC: r1 = READ_ONCE(x);\n\t\tD: srcu_up_read(&s, r1);\n\t}\n\nAssuming that P1 executes after P0 and does read the index value\nstored in x, we can write this (using brackets to represent event\nannotations) as:\n\n\tA[srcu-lock] ->data B[once] ->rf C[once] ->data D[srcu-unlock].\n\nThe LKMM defines a carry-srcu-data relation to express this pattern;\nit permits an arbitrarily long sequence of\n\n\tdata ; rf\n\npairs (that is, a data link followed by an rf link) to occur between\nan srcu-lock event and the final data dependency leading to the\nmatching srcu-unlock event.  carry-srcu-data is complicated by the\nneed to ensure that none of the intermediate store events in this\nsequence are instances of srcu-unlock.  This is necessary because in a\npattern like the one above:\n\n\tA: idx1 = srcu_read_lock(&s);\n\tB: srcu_read_unlock(&s, idx1);\n\tC: idx2 = srcu_read_lock(&s);\n\tD: srcu_read_unlock(&s, idx2);\n\nthe LKMM treats B as a store to the variable s and C as a load from\nthat variable, creating an undesirable rf link from B to C:\n\n\tA ->data B ->rf C ->data D.\n\nThis would cause carry-srcu-data to mistakenly extend a data\ndependency from A to D, giving the impression that D was the\nsrcu-unlock event matching A's srcu-lock.  To avoid such problems,\ncarry-srcu-data does not accept sequences in which the ends of any of\nthe intermediate ->data links (B above) is an srcu-unlock event.\n\n\nLOCKING\n-------\n\nThe LKMM includes locking.  In fact, there is special code for locking\nin the formal model, added in order to make tools run faster.\nHowever, this special code is intended to be more or less equivalent\nto concepts we have already covered.  A spinlock_t variable is treated\nthe same as an int, and spin_lock(&s) is treated almost the same as:\n\n\twhile (cmpxchg_acquire(&s, 0, 1) != 0)\n\t\tcpu_relax();\n\nThis waits until s is equal to 0 and then atomically sets it to 1,\nand the read part of the cmpxchg operation acts as an acquire fence.\nAn alternate way to express the same thing would be:\n\n\tr = xchg_acquire(&s, 1);\n\nalong with a requirement that at the end, r = 0.  Similarly,\nspin_trylock(&s) is treated almost the same as:\n\n\treturn !cmpxchg_acquire(&s, 0, 1);\n\nwhich atomically sets s to 1 if it is currently equal to 0 and returns\ntrue if it succeeds (the read part of the cmpxchg operation acts as an\nacquire fence only if the operation is successful).  spin_unlock(&s)\nis treated almost the same as:\n\n\tsmp_store_release(&s, 0);\n\nThe \"almost\" qualifiers above need some explanation.  In the LKMM, the\nstore-release in a spin_unlock() and the load-acquire which forms the\nfirst half of the atomic rmw update in a spin_lock() or a successful\nspin_trylock() -- we can call these things lock-releases and\nlock-acquires -- have two properties beyond those of ordinary releases\nand acquires.\n\nFirst, when a lock-acquire reads from or is po-after a lock-release,\nthe LKMM requires that every instruction po-before the lock-release\nmust execute before any instruction po-after the lock-acquire.  This\nwould naturally hold if the release and acquire operations were on\ndifferent CPUs and accessed the same lock variable, but the LKMM says\nit also holds when they are on the same CPU, even if they access\ndifferent lock variables.  For example:\n\n\tint x, y;\n\tspinlock_t s, t;\n\n\tP0()\n\t{\n\t\tint r1, r2;\n\n\t\tspin_lock(&s);\n\t\tr1 = READ_ONCE(x);\n\t\tspin_unlock(&s);\n\t\tspin_lock(&t);\n\t\tr2 = READ_ONCE(y);\n\t\tspin_unlock(&t);\n\t}\n\n\tP1()\n\t{\n\t\tWRITE_ONCE(y, 1);\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(x, 1);\n\t}\n\nHere the second spin_lock() is po-after the first spin_unlock(), and\ntherefore the load of x must execute before the load of y, even though\nthe two locking operations use different locks.  Thus we cannot have\nr1 = 1 and r2 = 0 at the end (this is an instance of the MP pattern).\n\nThis requirement does not apply to ordinary release and acquire\nfences, only to lock-related operations.  For instance, suppose P0()\nin the example had been written as:\n\n\tP0()\n\t{\n\t\tint r1, r2, r3;\n\n\t\tr1 = READ_ONCE(x);\n\t\tsmp_store_release(&s, 1);\n\t\tr3 = smp_load_acquire(&s);\n\t\tr2 = READ_ONCE(y);\n\t}\n\nThen the CPU would be allowed to forward the s = 1 value from the\nsmp_store_release() to the smp_load_acquire(), executing the\ninstructions in the following order:\n\n\t\tr3 = smp_load_acquire(&s);\t// Obtains r3 = 1\n\t\tr2 = READ_ONCE(y);\n\t\tr1 = READ_ONCE(x);\n\t\tsmp_store_release(&s, 1);\t// Value is forwarded\n\nand thus it could load y before x, obtaining r2 = 0 and r1 = 1.\n\nSecond, when a lock-acquire reads from or is po-after a lock-release,\nand some other stores W and W' occur po-before the lock-release and\npo-after the lock-acquire respectively, the LKMM requires that W must\npropagate to each CPU before W' does.  For example, consider:\n\n\tint x, y;\n\tspinlock_t s;\n\n\tP0()\n\t{\n\t\tspin_lock(&s);\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_unlock(&s);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\n\t\tspin_lock(&s);\n\t\tr1 = READ_ONCE(x);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&s);\n\t}\n\n\tP2()\n\t{\n\t\tint r2, r3;\n\n\t\tr2 = READ_ONCE(y);\n\t\tsmp_rmb();\n\t\tr3 = READ_ONCE(x);\n\t}\n\nIf r1 = 1 at the end then the spin_lock() in P1 must have read from\nthe spin_unlock() in P0.  Hence the store to x must propagate to P2\nbefore the store to y does, so we cannot have r2 = 1 and r3 = 0.  But\nif P1 had used a lock variable different from s, the writes could have\npropagated in either order.  (On the other hand, if the code in P0 and\nP1 had all executed on a single CPU, as in the example before this\none, then the writes would have propagated in order even if the two\ncritical sections used different lock variables.)\n\nThese two special requirements for lock-release and lock-acquire do\nnot arise from the operational model.  Nevertheless, kernel developers\nhave come to expect and rely on them because they do hold on all\narchitectures supported by the Linux kernel, albeit for various\ndiffering reasons.\n\n\nPLAIN ACCESSES AND DATA RACES\n-----------------------------\n\nIn the LKMM, memory accesses such as READ_ONCE(x), atomic_inc(&y),\nsmp_load_acquire(&z), and so on are collectively referred to as\n\"marked\" accesses, because they are all annotated with special\noperations of one kind or another.  Ordinary C-language memory\naccesses such as x or y = 0 are simply called \"plain\" accesses.\n\nEarly versions of the LKMM had nothing to say about plain accesses.\nThe C standard allows compilers to assume that the variables affected\nby plain accesses are not concurrently read or written by any other\nthreads or CPUs.  This leaves compilers free to implement all manner\nof transformations or optimizations of code containing plain accesses,\nmaking such code very difficult for a memory model to handle.\n\nHere is just one example of a possible pitfall:\n\n\tint a = 6;\n\tint *x = &a;\n\n\tP0()\n\t{\n\t\tint *r1;\n\t\tint r2 = 0;\n\n\t\tr1 = x;\n\t\tif (r1 != NULL)\n\t\t\tr2 = READ_ONCE(*r1);\n\t}\n\n\tP1()\n\t{\n\t\tWRITE_ONCE(x, NULL);\n\t}\n\nOn the face of it, one would expect that when this code runs, the only\npossible final values for r2 are 6 and 0, depending on whether or not\nP1's store to x propagates to P0 before P0's load from x executes.\nBut since P0's load from x is a plain access, the compiler may decide\nto carry out the load twice (for the comparison against NULL, then again\nfor the READ_ONCE()) and eliminate the temporary variable r1.  The\nobject code generated for P0 could therefore end up looking rather\nlike this:\n\n\tP0()\n\t{\n\t\tint r2 = 0;\n\n\t\tif (x != NULL)\n\t\t\tr2 = READ_ONCE(*x);\n\t}\n\nAnd now it is obvious that this code runs the risk of dereferencing a\nNULL pointer, because P1's store to x might propagate to P0 after the\ntest against NULL has been made but before the READ_ONCE() executes.\nIf the original code had said \"r1 = READ_ONCE(x)\" instead of \"r1 = x\",\nthe compiler would not have performed this optimization and there\nwould be no possibility of a NULL-pointer dereference.\n\nGiven the possibility of transformations like this one, the LKMM\ndoesn't try to predict all possible outcomes of code containing plain\naccesses.  It is instead content to determine whether the code\nviolates the compiler's assumptions, which would render the ultimate\noutcome undefined.\n\nIn technical terms, the compiler is allowed to assume that when the\nprogram executes, there will not be any data races.  A \"data race\"\noccurs when there are two memory accesses such that:\n\n1.\tthey access the same location,\n\n2.\tat least one of them is a store,\n\n3.\tat least one of them is plain,\n\n4.\tthey occur on different CPUs (or in different threads on the\n\tsame CPU), and\n\n5.\tthey execute concurrently.\n\nIn the literature, two accesses are said to \"conflict\" if they satisfy\n1 and 2 above.  We'll go a little farther and say that two accesses\nare \"race candidates\" if they satisfy 1 - 4.  Thus, whether or not two\nrace candidates actually do race in a given execution depends on\nwhether they are concurrent.\n\nThe LKMM tries to determine whether a program contains race candidates\nwhich may execute concurrently; if it does then the LKMM says there is\na potential data race and makes no predictions about the program's\noutcome.\n\nDetermining whether two accesses are race candidates is easy; you can\nsee that all the concepts involved in the definition above are already\npart of the memory model.  The hard part is telling whether they may\nexecute concurrently.  The LKMM takes a conservative attitude,\nassuming that accesses may be concurrent unless it can prove they\nare not.\n\nIf two memory accesses aren't concurrent then one must execute before\nthe other.  Therefore the LKMM decides two accesses aren't concurrent\nif they can be connected by a sequence of hb, pb, and rb links\n(together referred to as xb, for \"executes before\").  However, there\nare two complicating factors.\n\nIf X is a load and X executes before a store Y, then indeed there is\nno danger of X and Y being concurrent.  After all, Y can't have any\neffect on the value obtained by X until the memory subsystem has\npropagated Y from its own CPU to X's CPU, which won't happen until\nsome time after Y executes and thus after X executes.  But if X is a\nstore, then even if X executes before Y it is still possible that X\nwill propagate to Y's CPU just as Y is executing.  In such a case X\ncould very well interfere somehow with Y, and we would have to\nconsider X and Y to be concurrent.\n\nTherefore when X is a store, for X and Y to be non-concurrent the LKMM\nrequires not only that X must execute before Y but also that X must\npropagate to Y's CPU before Y executes.  (Or vice versa, of course, if\nY executes before X -- then Y must propagate to X's CPU before X\nexecutes if Y is a store.)  This is expressed by the visibility\nrelation (vis), where X ->vis Y is defined to hold if there is an\nintermediate event Z such that:\n\n\tX is connected to Z by a possibly empty sequence of\n\tcumul-fence links followed by an optional rfe link (if none of\n\tthese links are present, X and Z are the same event),\n\nand either:\n\n\tZ is connected to Y by a strong-fence link followed by a\n\tpossibly empty sequence of xb links,\n\nor:\n\n\tZ is on the same CPU as Y and is connected to Y by a possibly\n\tempty sequence of xb links (again, if the sequence is empty it\n\tmeans Z and Y are the same event).\n\nThe motivations behind this definition are straightforward:\n\n\tcumul-fence memory barriers force stores that are po-before\n\tthe barrier to propagate to other CPUs before stores that are\n\tpo-after the barrier.\n\n\tAn rfe link from an event W to an event R says that R reads\n\tfrom W, which certainly means that W must have propagated to\n\tR's CPU before R executed.\n\n\tstrong-fence memory barriers force stores that are po-before\n\tthe barrier, or that propagate to the barrier's CPU before the\n\tbarrier executes, to propagate to all CPUs before any events\n\tpo-after the barrier can execute.\n\nTo see how this works out in practice, consider our old friend, the MP\npattern (with fences and statement labels, but without the conditional\ntest):\n\n\tint buf = 0, flag = 0;\n\n\tP0()\n\t{\n\t\tX: WRITE_ONCE(buf, 1);\n\t\t   smp_wmb();\n\t\tW: WRITE_ONCE(flag, 1);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\t\tint r2 = 0;\n\n\t\tZ: r1 = READ_ONCE(flag);\n\t\t   smp_rmb();\n\t\tY: r2 = READ_ONCE(buf);\n\t}\n\nThe smp_wmb() memory barrier gives a cumul-fence link from X to W, and\nassuming r1 = 1 at the end, there is an rfe link from W to Z.  This\nmeans that the store to buf must propagate from P0 to P1 before Z\nexecutes.  Next, Z and Y are on the same CPU and the smp_rmb() fence\nprovides an xb link from Z to Y (i.e., it forces Z to execute before\nY).  Therefore we have X ->vis Y: X must propagate to Y's CPU before Y\nexecutes.\n\nThe second complicating factor mentioned above arises from the fact\nthat when we are considering data races, some of the memory accesses\nare plain.  Now, although we have not said so explicitly, up to this\npoint most of the relations defined by the LKMM (ppo, hb, prop,\ncumul-fence, pb, and so on -- including vis) apply only to marked\naccesses.\n\nThere are good reasons for this restriction.  The compiler is not\nallowed to apply fancy transformations to marked accesses, and\nconsequently each such access in the source code corresponds more or\nless directly to a single machine instruction in the object code.  But\nplain accesses are a different story; the compiler may combine them,\nsplit them up, duplicate them, eliminate them, invent new ones, and\nwho knows what else.  Seeing a plain access in the source code tells\nyou almost nothing about what machine instructions will end up in the\nobject code.\n\nFortunately, the compiler isn't completely free; it is subject to some\nlimitations.  For one, it is not allowed to introduce a data race into\nthe object code if the source code does not already contain a data\nrace (if it could, memory models would be useless and no multithreaded\ncode would be safe!).  For another, it cannot move a plain access past\na compiler barrier.\n\nA compiler barrier is a kind of fence, but as the name implies, it\nonly affects the compiler; it does not necessarily have any effect on\nhow instructions are executed by the CPU.  In Linux kernel source\ncode, the barrier() function is a compiler barrier.  It doesn't give\nrise directly to any machine instructions in the object code; rather,\nit affects how the compiler generates the rest of the object code.\nGiven source code like this:\n\n\t... some memory accesses ...\n\tbarrier();\n\t... some other memory accesses ...\n\nthe barrier() function ensures that the machine instructions\ncorresponding to the first group of accesses will all end po-before\nany machine instructions corresponding to the second group of accesses\n-- even if some of the accesses are plain.  (Of course, the CPU may\nthen execute some of those accesses out of program order, but we\nalready know how to deal with such issues.)  Without the barrier()\nthere would be no such guarantee; the two groups of accesses could be\nintermingled or even reversed in the object code.\n\nThe LKMM doesn't say much about the barrier() function, but it does\nrequire that all fences are also compiler barriers.  In addition, it\nrequires that the ordering properties of memory barriers such as\nsmp_rmb() or smp_store_release() apply to plain accesses as well as to\nmarked accesses.\n\nThis is the key to analyzing data races.  Consider the MP pattern\nagain, now using plain accesses for buf:\n\n\tint buf = 0, flag = 0;\n\n\tP0()\n\t{\n\t\tU: buf = 1;\n\t\t   smp_wmb();\n\t\tX: WRITE_ONCE(flag, 1);\n\t}\n\n\tP1()\n\t{\n\t\tint r1;\n\t\tint r2 = 0;\n\n\t\tY: r1 = READ_ONCE(flag);\n\t\t   if (r1) {\n\t\t\t   smp_rmb();\n\t\t\tV: r2 = buf;\n\t\t   }\n\t}\n\nThis program does not contain a data race.  Although the U and V\naccesses are race candidates, the LKMM can prove they are not\nconcurrent as follows:\n\n\tThe smp_wmb() fence in P0 is both a compiler barrier and a\n\tcumul-fence.  It guarantees that no matter what hash of\n\tmachine instructions the compiler generates for the plain\n\taccess U, all those instructions will be po-before the fence.\n\tConsequently U's store to buf, no matter how it is carried out\n\tat the machine level, must propagate to P1 before X's store to\n\tflag does.\n\n\tX and Y are both marked accesses.  Hence an rfe link from X to\n\tY is a valid indicator that X propagated to P1 before Y\n\texecuted, i.e., X ->vis Y.  (And if there is no rfe link then\n\tr1 will be 0, so V will not be executed and ipso facto won't\n\trace with U.)\n\n\tThe smp_rmb() fence in P1 is a compiler barrier as well as a\n\tfence.  It guarantees that all the machine-level instructions\n\tcorresponding to the access V will be po-after the fence, and\n\ttherefore any loads among those instructions will execute\n\tafter the fence does and hence after Y does.\n\nThus U's store to buf is forced to propagate to P1 before V's load\nexecutes (assuming V does execute), ruling out the possibility of a\ndata race between them.\n\nThis analysis illustrates how the LKMM deals with plain accesses in\ngeneral.  Suppose R is a plain load and we want to show that R\nexecutes before some marked access E.  We can do this by finding a\nmarked access X such that R and X are ordered by a suitable fence and\nX ->xb* E.  If E was also a plain access, we would also look for a\nmarked access Y such that X ->xb* Y, and Y and E are ordered by a\nfence.  We describe this arrangement by saying that R is\n\"post-bounded\" by X and E is \"pre-bounded\" by Y.\n\nIn fact, we go one step further: Since R is a read, we say that R is\n\"r-post-bounded\" by X.  Similarly, E would be \"r-pre-bounded\" or\n\"w-pre-bounded\" by Y, depending on whether E was a store or a load.\nThis distinction is needed because some fences affect only loads\n(i.e., smp_rmb()) and some affect only stores (smp_wmb()); otherwise\nthe two types of bounds are the same.  And as a degenerate case, we\nsay that a marked access pre-bounds and post-bounds itself (e.g., if R\nabove were a marked load then X could simply be taken to be R itself.)\n\nThe need to distinguish between r- and w-bounding raises yet another\nissue.  When the source code contains a plain store, the compiler is\nallowed to put plain loads of the same location into the object code.\nFor example, given the source code:\n\n\tx = 1;\n\nthe compiler is theoretically allowed to generate object code that\nlooks like:\n\n\tif (x != 1)\n\t\tx = 1;\n\nthereby adding a load (and possibly replacing the store entirely).\nFor this reason, whenever the LKMM requires a plain store to be\nw-pre-bounded or w-post-bounded by a marked access, it also requires\nthe store to be r-pre-bounded or r-post-bounded, so as to handle cases\nwhere the compiler adds a load.\n\n(This may be overly cautious.  We don't know of any examples where a\ncompiler has augmented a store with a load in this fashion, and the\nLinux kernel developers would probably fight pretty hard to change a\ncompiler if it ever did this.  Still, better safe than sorry.)\n\nIncidentally, the other tranformation -- augmenting a plain load by\nadding in a store to the same location -- is not allowed.  This is\nbecause the compiler cannot know whether any other CPUs might perform\na concurrent load from that location.  Two concurrent loads don't\nconstitute a race (they can't interfere with each other), but a store\ndoes race with a concurrent load.  Thus adding a store might create a\ndata race where one was not already present in the source code,\nsomething the compiler is forbidden to do.  Augmenting a store with a\nload, on the other hand, is acceptable because doing so won't create a\ndata race unless one already existed.\n\nThe LKMM includes a second way to pre-bound plain accesses, in\naddition to fences: an address dependency from a marked load.  That\nis, in the sequence:\n\n\tp = READ_ONCE(ptr);\n\tr = *p;\n\nthe LKMM says that the marked load of ptr pre-bounds the plain load of\n*p; the marked load must execute before any of the machine\ninstructions corresponding to the plain load.  This is a reasonable\nstipulation, since after all, the CPU can't perform the load of *p\nuntil it knows what value p will hold.  Furthermore, without some\nassumption like this one, some usages typical of RCU would count as\ndata races.  For example:\n\n\tint a = 1, b;\n\tint *ptr = &a;\n\n\tP0()\n\t{\n\t\tb = 2;\n\t\trcu_assign_pointer(ptr, &b);\n\t}\n\n\tP1()\n\t{\n\t\tint *p;\n\t\tint r;\n\n\t\trcu_read_lock();\n\t\tp = rcu_dereference(ptr);\n\t\tr = *p;\n\t\trcu_read_unlock();\n\t}\n\n(In this example the rcu_read_lock() and rcu_read_unlock() calls don't\nreally do anything, because there aren't any grace periods.  They are\nincluded merely for the sake of good form; typically P0 would call\nsynchronize_rcu() somewhere after the rcu_assign_pointer().)\n\nrcu_assign_pointer() performs a store-release, so the plain store to b\nis definitely w-post-bounded before the store to ptr, and the two\nstores will propagate to P1 in that order.  However, rcu_dereference()\nis only equivalent to READ_ONCE().  While it is a marked access, it is\nnot a fence or compiler barrier.  Hence the only guarantee we have\nthat the load of ptr in P1 is r-pre-bounded before the load of *p\n(thus avoiding a race) is the assumption about address dependencies.\n\nThis is a situation where the compiler can undermine the memory model,\nand a certain amount of care is required when programming constructs\nlike this one.  In particular, comparisons between the pointer and\nother known addresses can cause trouble.  If you have something like:\n\n\tp = rcu_dereference(ptr);\n\tif (p == &x)\n\t\tr = *p;\n\nthen the compiler just might generate object code resembling:\n\n\tp = rcu_dereference(ptr);\n\tif (p == &x)\n\t\tr = x;\n\nor even:\n\n\trtemp = x;\n\tp = rcu_dereference(ptr);\n\tif (p == &x)\n\t\tr = rtemp;\n\nwhich would invalidate the memory model's assumption, since the CPU\ncould now perform the load of x before the load of ptr (there might be\na control dependency but no address dependency at the machine level).\n\nFinally, it turns out there is a situation in which a plain write does\nnot need to be w-post-bounded: when it is separated from the other\nrace-candidate access by a fence.  At first glance this may seem\nimpossible.  After all, to be race candidates the two accesses must\nbe on different CPUs, and fences don't link events on different CPUs.\nWell, normal fences don't -- but rcu-fence can!  Here's an example:\n\n\tint x, y;\n\n\tP0()\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsynchronize_rcu();\n\t\ty = 3;\n\t}\n\n\tP1()\n\t{\n\t\trcu_read_lock();\n\t\tif (READ_ONCE(x) == 0)\n\t\t\ty = 2;\n\t\trcu_read_unlock();\n\t}\n\nDo the plain stores to y race?  Clearly not if P1 reads a non-zero\nvalue for x, so let's assume the READ_ONCE(x) does obtain 0.  This\nmeans that the read-side critical section in P1 must finish executing\nbefore the grace period in P0 does, because RCU's Grace-Period\nGuarantee says that otherwise P0's store to x would have propagated to\nP1 before the critical section started and so would have been visible\nto the READ_ONCE().  (Another way of putting it is that the fre link\nfrom the READ_ONCE() to the WRITE_ONCE() gives rise to an rcu-link\nbetween those two events.)\n\nThis means there is an rcu-fence link from P1's \"y = 2\" store to P0's\n\"y = 3\" store, and consequently the first must propagate from P1 to P0\nbefore the second can execute.  Therefore the two stores cannot be\nconcurrent and there is no race, even though P1's plain store to y\nisn't w-post-bounded by any marked accesses.\n\nPutting all this material together yields the following picture.  For\nrace-candidate stores W and W', where W ->co W', the LKMM says the\nstores don't race if W can be linked to W' by a\n\n\tw-post-bounded ; vis ; w-pre-bounded\n\nsequence.  If W is plain then they also have to be linked by an\n\n\tr-post-bounded ; xb* ; w-pre-bounded\n\nsequence, and if W' is plain then they also have to be linked by a\n\n\tw-post-bounded ; vis ; r-pre-bounded\n\nsequence.  For race-candidate load R and store W, the LKMM says the\ntwo accesses don't race if R can be linked to W by an\n\n\tr-post-bounded ; xb* ; w-pre-bounded\n\nsequence or if W can be linked to R by a\n\n\tw-post-bounded ; vis ; r-pre-bounded\n\nsequence.  For the cases involving a vis link, the LKMM also accepts\nsequences in which W is linked to W' or R by a\n\n\tstrong-fence ; xb* ; {w and/or r}-pre-bounded\n\nsequence with no post-bounding, and in every case the LKMM also allows\nthe link simply to be a fence with no bounding at all.  If no sequence\nof the appropriate sort exists, the LKMM says that the accesses race.\n\nThere is one more part of the LKMM related to plain accesses (although\nnot to data races) we should discuss.  Recall that many relations such\nas hb are limited to marked accesses only.  As a result, the\nhappens-before, propagates-before, and rcu axioms (which state that\nvarious relation must not contain a cycle) doesn't apply to plain\naccesses.  Nevertheless, we do want to rule out such cycles, because\nthey don't make sense even for plain accesses.\n\nTo this end, the LKMM imposes three extra restrictions, together\ncalled the \"plain-coherence\" axiom because of their resemblance to the\nrules used by the operational model to ensure cache coherence (that\nis, the rules governing the memory subsystem's choice of a store to\nsatisfy a load request and its determination of where a store will\nfall in the coherence order):\n\n\tIf R and W are race candidates and it is possible to link R to\n\tW by one of the xb* sequences listed above, then W ->rfe R is\n\tnot allowed (i.e., a load cannot read from a store that it\n\texecutes before, even if one or both is plain).\n\n\tIf W and R are race candidates and it is possible to link W to\n\tR by one of the vis sequences listed above, then R ->fre W is\n\tnot allowed (i.e., if a store is visible to a load then the\n\tload must read from that store or one coherence-after it).\n\n\tIf W and W' are race candidates and it is possible to link W\n\tto W' by one of the vis sequences listed above, then W' ->co W\n\tis not allowed (i.e., if one store is visible to a second then\n\tthe second must come after the first in the coherence order).\n\nThis is the extent to which the LKMM deals with plain accesses.\nPerhaps it could say more (for example, plain accesses might\ncontribute to the ppo relation), but at the moment it seems that this\nminimal, conservative approach is good enough.\n\n\nODDS AND ENDS\n-------------\n\nThis section covers material that didn't quite fit anywhere in the\nearlier sections.\n\nThe descriptions in this document don't always match the formal\nversion of the LKMM exactly.  For example, the actual formal\ndefinition of the prop relation makes the initial coe or fre part\noptional, and it doesn't require the events linked by the relation to\nbe on the same CPU.  These differences are very unimportant; indeed,\ninstances where the coe/fre part of prop is missing are of no interest\nbecause all the other parts (fences and rfe) are already included in\nhb anyway, and where the formal model adds prop into hb, it includes\nan explicit requirement that the events being linked are on the same\nCPU.\n\nAnother minor difference has to do with events that are both memory\naccesses and fences, such as those corresponding to smp_load_acquire()\ncalls.  In the formal model, these events aren't actually both reads\nand fences; rather, they are read events with an annotation marking\nthem as acquires.  (Or write events annotated as releases, in the case\nsmp_store_release().)  The final effect is the same.\n\nAlthough we didn't mention it above, the instruction execution\nordering provided by the smp_rmb() fence doesn't apply to read events\nthat are part of a non-value-returning atomic update.  For instance,\ngiven:\n\n\tatomic_inc(&x);\n\tsmp_rmb();\n\tr1 = READ_ONCE(y);\n\nit is not guaranteed that the load from y will execute after the\nupdate to x.  This is because the ARMv8 architecture allows\nnon-value-returning atomic operations effectively to be executed off\nthe CPU.  Basically, the CPU tells the memory subsystem to increment\nx, and then the increment is carried out by the memory hardware with\nno further involvement from the CPU.  Since the CPU doesn't ever read\nthe value of x, there is nothing for the smp_rmb() fence to act on.\n\nThe LKMM defines a few extra synchronization operations in terms of\nthings we have already covered.  In particular, rcu_dereference() is\ntreated as READ_ONCE() and rcu_assign_pointer() is treated as\nsmp_store_release() -- which is basically how the Linux kernel treats\nthem.\n\nAlthough we said that plain accesses are not linked by the ppo\nrelation, they do contribute to it indirectly.  Firstly, when there is\nan address dependency from a marked load R to a plain store W,\nfollowed by smp_wmb() and then a marked store W', the LKMM creates a\nppo link from R to W'.  The reasoning behind this is perhaps a little\nshaky, but essentially it says there is no way to generate object code\nfor this source code in which W' could execute before R.  Just as with\npre-bounding by address dependencies, it is possible for the compiler\nto undermine this relation if sufficient care is not taken.\n\nSecondly, plain accesses can carry dependencies: If a data dependency\nlinks a marked load R to a store W, and the store is read by a load R'\nfrom the same thread, then the data loaded by R' depends on the data\nloaded originally by R. Thus, if R' is linked to any access X by a\ndependency, R is also linked to access X by the same dependency, even\nif W' or R' (or both!) are plain.\n\nThere are a few oddball fences which need special treatment:\nsmp_mb__before_atomic(), smp_mb__after_atomic(), and\nsmp_mb__after_spinlock().  The LKMM uses fence events with special\nannotations for them; they act as strong fences just like smp_mb()\nexcept for the sets of events that they order.  Instead of ordering\nall po-earlier events against all po-later events, as smp_mb() does,\nthey behave as follows:\n\n\tsmp_mb__before_atomic() orders all po-earlier events against\n\tpo-later atomic updates and the events following them;\n\n\tsmp_mb__after_atomic() orders po-earlier atomic updates and\n\tthe events preceding them against all po-later events;\n\n\tsmp_mb__after_spinlock() orders po-earlier lock acquisition\n\tevents and the events preceding them against all po-later\n\tevents.\n\nInterestingly, RCU and locking each introduce the possibility of\ndeadlock.  When faced with code sequences such as:\n\n\tspin_lock(&s);\n\tspin_lock(&s);\n\tspin_unlock(&s);\n\tspin_unlock(&s);\n\nor:\n\n\trcu_read_lock();\n\tsynchronize_rcu();\n\trcu_read_unlock();\n\nwhat does the LKMM have to say?  Answer: It says there are no allowed\nexecutions at all, which makes sense.  But this can also lead to\nmisleading results, because if a piece of code has multiple possible\nexecutions, some of which deadlock, the model will report only on the\nnon-deadlocking executions.  For example:\n\n\tint x, y;\n\n\tP0()\n\t{\n\t\tint r0;\n\n\t\tWRITE_ONCE(x, 1);\n\t\tr0 = READ_ONCE(y);\n\t}\n\n\tP1()\n\t{\n\t\trcu_read_lock();\n\t\tif (READ_ONCE(x) > 0) {\n\t\t\tWRITE_ONCE(y, 36);\n\t\t\tsynchronize_rcu();\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\nIs it possible to end up with r0 = 36 at the end?  The LKMM will tell\nyou it is not, but the model won't mention that this is because P1\nwill self-deadlock in the executions where it stores 36 in y.\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}