{
  "module_name": "locking.txt",
  "hash_id": "08d54ad89005ecc9876891eae3f743e17e70dfb97c12433b6995d06935ff7cc7",
  "original_prompt": "Ingested from linux-6.6.14/tools/memory-model/Documentation/locking.txt",
  "human_readable_source": "Locking\n=======\n\nLocking is well-known and the common use cases are straightforward: Any\nCPU holding a given lock sees any changes previously seen or made by any\nCPU before it previously released that same lock.  This last sentence\nis the only part of this document that most developers will need to read.\n\nHowever, developers who would like to also access lock-protected shared\nvariables outside of their corresponding locks should continue reading.\n\n\nLocking and Prior Accesses\n--------------------------\n\nThe basic rule of locking is worth repeating:\n\n\tAny CPU holding a given lock sees any changes previously seen\n\tor made by any CPU before it previously released that same lock.\n\nNote that this statement is a bit stronger than \"Any CPU holding a\ngiven lock sees all changes made by any CPU during the time that CPU was\npreviously holding this same lock\".  For example, consider the following\npair of code fragments:\n\n\t/* See MP+polocks.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tr0 = READ_ONCE(y);\n\t\tspin_unlock(&mylock);\n\t\tr1 = READ_ONCE(x);\n\t}\n\nThe basic rule guarantees that if CPU0() acquires mylock before CPU1(),\nthen both r0 and r1 must be set to the value 1.  This also has the\nconsequence that if the final value of r0 is equal to 1, then the final\nvalue of r1 must also be equal to 1.  In contrast, the weaker rule would\nsay nothing about the final value of r1.\n\n\nLocking and Subsequent Accesses\n-------------------------------\n\nThe converse to the basic rule also holds:  Any CPU holding a given\nlock will not see any changes that will be made by any CPU after it\nsubsequently acquires this same lock.  This converse statement is\nillustrated by the following litmus test:\n\n\t/* See MP+porevlocks.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tr0 = READ_ONCE(y);\n\t\tspin_lock(&mylock);\n\t\tr1 = READ_ONCE(x);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_unlock(&mylock);\n\t\tWRITE_ONCE(y, 1);\n\t}\n\nThis converse to the basic rule guarantees that if CPU0() acquires\nmylock before CPU1(), then both r0 and r1 must be set to the value 0.\nThis also has the consequence that if the final value of r1 is equal\nto 0, then the final value of r0 must also be equal to 0.  In contrast,\nthe weaker rule would say nothing about the final value of r0.\n\nThese examples show only a single pair of CPUs, but the effects of the\nlocking basic rule extend across multiple acquisitions of a given lock\nacross multiple CPUs.\n\n\nDouble-Checked Locking\n----------------------\n\nIt is well known that more than just a lock is required to make\ndouble-checked locking work correctly,  This litmus test illustrates\none incorrect approach:\n\n\t/* See Documentation/litmus-tests/locking/DCL-broken.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tr0 = READ_ONCE(flag);\n\t\tif (r0 == 0) {\n\t\t\tspin_lock(&lck);\n\t\t\tr1 = READ_ONCE(flag);\n\t\t\tif (r1 == 0) {\n\t\t\t\tWRITE_ONCE(data, 1);\n\t\t\t\tWRITE_ONCE(flag, 1);\n\t\t\t}\n\t\t\tspin_unlock(&lck);\n\t\t}\n\t\tr2 = READ_ONCE(data);\n\t}\n\t/* CPU1() is the exactly the same as CPU0(). */\n\nThere are two problems.  First, there is no ordering between the first\nREAD_ONCE() of \"flag\" and the READ_ONCE() of \"data\".  Second, there is\nno ordering between the two WRITE_ONCE() calls.  It should therefore be\nno surprise that \"r2\" can be zero, and a quick herd7 run confirms this.\n\nOne way to fix this is to use smp_load_acquire() and smp_store_release()\nas shown in this corrected version:\n\n\t/* See Documentation/litmus-tests/locking/DCL-fixed.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tr0 = smp_load_acquire(&flag);\n\t\tif (r0 == 0) {\n\t\t\tspin_lock(&lck);\n\t\t\tr1 = READ_ONCE(flag);\n\t\t\tif (r1 == 0) {\n\t\t\t\tWRITE_ONCE(data, 1);\n\t\t\t\tsmp_store_release(&flag, 1);\n\t\t\t}\n\t\t\tspin_unlock(&lck);\n\t\t}\n\t\tr2 = READ_ONCE(data);\n\t}\n\t/* CPU1() is the exactly the same as CPU0(). */\n\nThe smp_load_acquire() guarantees that its load from \"flags\" will\nbe ordered before the READ_ONCE() from data, thus solving the first\nproblem.  The smp_store_release() guarantees that its store will be\nordered after the WRITE_ONCE() to \"data\", solving the second problem.\nThe smp_store_release() pairs with the smp_load_acquire(), thus ensuring\nthat the ordering provided by each actually takes effect.  Again, a\nquick herd7 run confirms this.\n\nIn short, if you access a lock-protected variable without holding the\ncorresponding lock, you will need to provide additional ordering, in\nthis case, via the smp_load_acquire() and the smp_store_release().\n\n\nOrdering Provided by a Lock to CPUs Not Holding That Lock\n---------------------------------------------------------\n\nIt is not necessarily the case that accesses ordered by locking will be\nseen as ordered by CPUs not holding that lock.  Consider this example:\n\n\t/* See Z6.0+pooncelock+pooncelock+pombonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(x, 1);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tr0 = READ_ONCE(y);\n\t\tWRITE_ONCE(z, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU2(void)\n\t{\n\t\tWRITE_ONCE(z, 2);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nCounter-intuitive though it might be, it is quite possible to have\nthe final value of r0 be 1, the final value of z be 2, and the final\nvalue of r1 be 0.  The reason for this surprising outcome is that CPU2()\nnever acquired the lock, and thus did not fully benefit from the lock's\nordering properties.\n\nOrdering can be extended to CPUs not holding the lock by careful use\nof smp_mb__after_spinlock():\n\n\t/* See Z6.0+pooncelock+poonceLock+pombonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(x, 1);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tsmp_mb__after_spinlock();\n\t\tr0 = READ_ONCE(y);\n\t\tWRITE_ONCE(z, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU2(void)\n\t{\n\t\tWRITE_ONCE(z, 2);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nThis addition of smp_mb__after_spinlock() strengthens the lock\nacquisition sufficiently to rule out the counter-intuitive outcome.\nIn other words, the addition of the smp_mb__after_spinlock() prohibits\nthe counter-intuitive result where the final value of r0 is 1, the final\nvalue of z is 2, and the final value of r1 is 0.\n\n\nNo Roach-Motel Locking!\n-----------------------\n\nThis example requires familiarity with the herd7 \"filter\" clause, so\nplease read up on that topic in litmus-tests.txt.\n\nIt is tempting to allow memory-reference instructions to be pulled\ninto a critical section, but this cannot be allowed in the general case.\nFor example, consider a spin loop preceding a lock-based critical section.\nNow, herd7 does not model spin loops, but we can emulate one with two\nloads, with a \"filter\" clause to constrain the first to return the\ninitial value and the second to return the updated value, as shown below:\n\n\t/* See Documentation/litmus-tests/locking/RM-fixed.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tspin_lock(&lck);\n\t\tr2 = atomic_inc_return(&y);\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_unlock(&lck);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tr0 = READ_ONCE(x);\n\t\tr1 = READ_ONCE(x);\n\t\tspin_lock(&lck);\n\t\tr2 = atomic_inc_return(&y);\n\t\tspin_unlock(&lck);\n\t}\n\n\tfilter (1:r0=0 /\\ 1:r1=1)\n\texists (1:r2=1)\n\nThe variable \"x\" is the control variable for the emulated spin loop.\nCPU0() sets it to \"1\" while holding the lock, and CPU1() emulates the\nspin loop by reading it twice, first into \"1:r0\" (which should get the\ninitial value \"0\") and then into \"1:r1\" (which should get the updated\nvalue \"1\").\n\nThe \"filter\" clause takes this into account, constraining \"1:r0\" to\nequal \"0\" and \"1:r1\" to equal 1.\n\nThen the \"exists\" clause checks to see if CPU1() acquired its lock first,\nwhich should not happen given the filter clause because CPU0() updates\n\"x\" while holding the lock.  And herd7 confirms this.\n\nBut suppose that the compiler was permitted to reorder the spin loop\ninto CPU1()'s critical section, like this:\n\n\t/* See Documentation/litmus-tests/locking/RM-broken.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tint r2;\n\n\t\tspin_lock(&lck);\n\t\tr2 = atomic_inc_return(&y);\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_unlock(&lck);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&lck);\n\t\tr0 = READ_ONCE(x);\n\t\tr1 = READ_ONCE(x);\n\t\tr2 = atomic_inc_return(&y);\n\t\tspin_unlock(&lck);\n\t}\n\n\tfilter (1:r0=0 /\\ 1:r1=1)\n\texists (1:r2=1)\n\nIf \"1:r0\" is equal to \"0\", \"1:r1\" can never equal \"1\" because CPU0()\ncannot update \"x\" while CPU1() holds the lock.  And herd7 confirms this,\nshowing zero executions matching the \"filter\" criteria.\n\nAnd this is why Linux-kernel lock and unlock primitives must prevent\ncode from entering critical sections.  It is not sufficient to only\nprevent code from leaving them.\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}