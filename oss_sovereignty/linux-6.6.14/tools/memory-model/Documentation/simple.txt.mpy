{
  "module_name": "simple.txt",
  "hash_id": "0ea7186ba0d8740a02021b09f4755b15e167b8c24e8d58b63e7b34d614e2b9e2",
  "original_prompt": "Ingested from linux-6.6.14/tools/memory-model/Documentation/simple.txt",
  "human_readable_source": "This document provides options for those wishing to keep their\nmemory-ordering lives simple, as is necessary for those whose domain\nis complex.  After all, there are bugs other than memory-ordering bugs,\nand the time spent gaining memory-ordering knowledge is not available\nfor gaining domain knowledge.  Furthermore Linux-kernel memory model\n(LKMM) is quite complex, with subtle differences in code often having\ndramatic effects on correctness.\n\nThe options near the beginning of this list are quite simple.  The idea\nis not that kernel hackers don't already know about them, but rather\nthat they might need the occasional reminder.\n\nPlease note that this is a generic guide, and that specific subsystems\nwill often have special requirements or idioms.  For example, developers\nof MMIO-based device drivers will often need to use mb(), rmb(), and\nwmb(), and therefore might find smp_mb(), smp_rmb(), and smp_wmb()\nto be more natural than smp_load_acquire() and smp_store_release().\nOn the other hand, those coming in from other environments will likely\nbe more familiar with these last two.\n\n\nSingle-threaded code\n====================\n\nIn single-threaded code, there is no reordering, at least assuming\nthat your toolchain and hardware are working correctly.  In addition,\nit is generally a mistake to assume your code will only run in a single\nthreaded context as the kernel can enter the same code path on multiple\nCPUs at the same time.  One important exception is a function that makes\nno external data references.\n\nIn the general case, you will need to take explicit steps to ensure that\nyour code really is executed within a single thread that does not access\nshared variables.  A simple way to achieve this is to define a global lock\nthat you acquire at the beginning of your code and release at the end,\ntaking care to ensure that all references to your code's shared data are\nalso carried out under that same lock.  Because only one thread can hold\nthis lock at a given time, your code will be executed single-threaded.\nThis approach is called \"code locking\".\n\nCode locking can severely limit both performance and scalability, so it\nshould be used with caution, and only on code paths that execute rarely.\nAfter all, a huge amount of effort was required to remove the Linux\nkernel's old \"Big Kernel Lock\", so let's please be very careful about\nadding new \"little kernel locks\".\n\nOne of the advantages of locking is that, in happy contrast with the\nyear 1981, almost all kernel developers are very familiar with locking.\nThe Linux kernel's lockdep (CONFIG_PROVE_LOCKING=y) is very helpful with\nthe formerly feared deadlock scenarios.\n\nPlease use the standard locking primitives provided by the kernel rather\nthan rolling your own.  For one thing, the standard primitives interact\nproperly with lockdep.  For another thing, these primitives have been\ntuned to deal better with high contention.  And for one final thing, it is\nsurprisingly hard to correctly code production-quality lock acquisition\nand release functions.  After all, even simple non-production-quality\nlocking functions must carefully prevent both the CPU and the compiler\nfrom moving code in either direction across the locking function.\n\nDespite the scalability limitations of single-threaded code, RCU\ntakes this approach for much of its grace-period processing and also\nfor early-boot operation.  The reason RCU is able to scale despite\nsingle-threaded grace-period processing is use of batching, where all\nupdates that accumulated during one grace period are handled by the\nnext one.  In other words, slowing down grace-period processing makes\nit more efficient.  Nor is RCU unique:  Similar batching optimizations\nare used in many I/O operations.\n\n\nPackaged code\n=============\n\nEven if performance and scalability concerns prevent your code from\nbeing completely single-threaded, it is often possible to use library\nfunctions that handle the concurrency nearly or entirely on their own.\nThis approach delegates any LKMM worries to the library maintainer.\n\nIn the kernel, what is the \"library\"?  Quite a bit.  It includes the\ncontents of the lib/ directory, much of the include/linux/ directory along\nwith a lot of other heavily used APIs.  But heavily used examples include\nthe list macros (for example, include/linux/{,rcu}list.h), workqueues,\nsmp_call_function(), and the various hash tables and search trees.\n\n\nData locking\n============\n\nWith code locking, we use single-threaded code execution to guarantee\nserialized access to the data that the code is accessing.  However,\nwe can also achieve this by instead associating the lock with specific\ninstances of the data structures.  This creates a \"critical section\"\nin the code execution that will execute as though it is single threaded.\nBy placing all the accesses and modifications to a shared data structure\ninside a critical section, we ensure that the execution context that\nholds the lock has exclusive access to the shared data.\n\nThe poster boy for this approach is the hash table, where placing a lock\nin each hash bucket allows operations on different buckets to proceed\nconcurrently.  This works because the buckets do not overlap with each\nother, so that an operation on one bucket does not interfere with any\nother bucket.\n\nAs the number of buckets increases, data locking scales naturally.\nIn particular, if the amount of data increases with the number of CPUs,\nincreasing the number of buckets as the number of CPUs increase results\nin a naturally scalable data structure.\n\n\nPer-CPU processing\n==================\n\nPartitioning processing and data over CPUs allows each CPU to take\na single-threaded approach while providing excellent performance and\nscalability.  Of course, there is no free lunch:  The dark side of this\nexcellence is substantially increased memory footprint.\n\nIn addition, it is sometimes necessary to occasionally update some global\nview of this processing and data, in which case something like locking\nmust be used to protect this global view.  This is the approach taken\nby the percpu_counter infrastructure. In many cases, there are already\ngeneric/library variants of commonly used per-cpu constructs available.\nPlease use them rather than rolling your own.\n\nRCU uses DEFINE_PER_CPU*() declaration to create a number of per-CPU\ndata sets.  For example, each CPU does private quiescent-state processing\nwithin its instance of the per-CPU rcu_data structure, and then uses data\nlocking to report quiescent states up the grace-period combining tree.\n\n\nPackaged primitives: Sequence locking\n=====================================\n\nLockless programming is considered by many to be more difficult than\nlock-based programming, but there are a few lockless design patterns that\nhave been built out into an API.  One of these APIs is sequence locking.\nAlthough this APIs can be used in extremely complex ways, there are simple\nand effective ways of using it that avoid the need to pay attention to\nmemory ordering.\n\nThe basic keep-things-simple rule for sequence locking is \"do not write\nin read-side code\".  Yes, you can do writes from within sequence-locking\nreaders, but it won't be so simple.  For example, such writes will be\nlockless and should be idempotent.\n\nFor more sophisticated use cases, LKMM can guide you, including use\ncases involving combining sequence locking with other synchronization\nprimitives.  (LKMM does not yet know about sequence locking, so it is\ncurrently necessary to open-code it in your litmus tests.)\n\nAdditional information may be found in include/linux/seqlock.h.\n\nPackaged primitives: RCU\n========================\n\nAnother lockless design pattern that has been baked into an API\nis RCU.  The Linux kernel makes sophisticated use of RCU, but the\nkeep-things-simple rules for RCU are \"do not write in read-side code\"\nand \"do not update anything that is visible to and accessed by readers\",\nand \"protect updates with locking\".\n\nThese rules are illustrated by the functions foo_update_a() and\nfoo_get_a() shown in Documentation/RCU/whatisRCU.rst.  Additional\nRCU usage patterns maybe found in Documentation/RCU and in the\nsource code.\n\n\nPackaged primitives: Atomic operations\n======================================\n\nBack in the day, the Linux kernel had three types of atomic operations:\n\n1.\tInitialization and read-out, such as atomic_set() and atomic_read().\n\n2.\tOperations that did not return a value and provided no ordering,\n\tsuch as atomic_inc() and atomic_dec().\n\n3.\tOperations that returned a value and provided full ordering, such as\n\tatomic_add_return() and atomic_dec_and_test().  Note that some\n\tvalue-returning operations provide full ordering only conditionally.\n\tFor example, cmpxchg() provides ordering only upon success.\n\nMore recent kernels have operations that return a value but do not\nprovide full ordering.  These are flagged with either a _relaxed()\nsuffix (providing no ordering), or an _acquire() or _release() suffix\n(providing limited ordering).\n\nAdditional information may be found in these files:\n\nDocumentation/atomic_t.txt\nDocumentation/atomic_bitops.txt\nDocumentation/core-api/refcount-vs-atomic.rst\n\nReading code using these primitives is often also quite helpful.\n\n\nLockless, fully ordered\n=======================\n\nWhen using locking, there often comes a time when it is necessary\nto access some variable or another without holding the data lock\nthat serializes access to that variable.\n\nIf you want to keep things simple, use the initialization and read-out\noperations from the previous section only when there are no racing\naccesses.  Otherwise, use only fully ordered operations when accessing\nor modifying the variable.  This approach guarantees that code prior\nto a given access to that variable will be seen by all CPUs has having\nhappened before any code following any later access to that same variable.\n\nPlease note that per-CPU functions are not atomic operations and\nhence they do not provide any ordering guarantees at all.\n\nIf the lockless accesses are frequently executed reads that are used\nonly for heuristics, or if they are frequently executed writes that\nare used only for statistics, please see the next section.\n\n\nLockless statistics and heuristics\n==================================\n\nUnordered primitives such as atomic_read(), atomic_set(), READ_ONCE(), and\nWRITE_ONCE() can safely be used in some cases.  These primitives provide\nno ordering, but they do prevent the compiler from carrying out a number\nof destructive optimizations (for which please see the next section).\nOne example use for these primitives is statistics, such as per-CPU\ncounters exemplified by the rt_cache_stat structure's routing-cache\nstatistics counters.  Another example use case is heuristics, such as\nthe jiffies_till_first_fqs and jiffies_till_next_fqs kernel parameters\ncontrolling how often RCU scans for idle CPUs.\n\nBut be careful.  \"Unordered\" really does mean \"unordered\".  It is all\ntoo easy to assume ordering, and this assumption must be avoided when\nusing these primitives.\n\n\nDon't let the compiler trip you up\n==================================\n\nIt can be quite tempting to use plain C-language accesses for lockless\nloads from and stores to shared variables.  Although this is both\npossible and quite common in the Linux kernel, it does require a\nsurprising amount of analysis, care, and knowledge about the compiler.\nYes, some decades ago it was not unfair to consider a C compiler to be\nan assembler with added syntax and better portability, but the advent of\nsophisticated optimizing compilers mean that those days are long gone.\nToday's optimizing compilers can profoundly rewrite your code during the\ntranslation process, and have long been ready, willing, and able to do so.\n\nTherefore, if you really need to use C-language assignments instead of\nREAD_ONCE(), WRITE_ONCE(), and so on, you will need to have a very good\nunderstanding of both the C standard and your compiler.  Here are some\nintroductory references and some tooling to start you on this noble quest:\n\nWho's afraid of a big bad optimizing compiler?\n\thttps://lwn.net/Articles/793253/\nCalibrating your fear of big bad optimizing compilers\n\thttps://lwn.net/Articles/799218/\nConcurrency bugs should fear the big bad data-race detector (part 1)\n\thttps://lwn.net/Articles/816850/\nConcurrency bugs should fear the big bad data-race detector (part 2)\n\thttps://lwn.net/Articles/816854/\n\n\nMore complex use cases\n======================\n\nIf the alternatives above do not do what you need, please look at the\nrecipes-pairs.txt file to peel off the next layer of the memory-ordering\nonion.\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}