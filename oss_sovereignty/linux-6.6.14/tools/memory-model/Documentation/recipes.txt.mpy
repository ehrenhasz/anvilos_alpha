{
  "module_name": "recipes.txt",
  "hash_id": "ce93101995b7b73886233d329ca92c0538cf90765cfe67c490a2fd8ab9e3a547",
  "original_prompt": "Ingested from linux-6.6.14/tools/memory-model/Documentation/recipes.txt",
  "human_readable_source": "This document provides \"recipes\", that is, litmus tests for commonly\noccurring situations, as well as a few that illustrate subtly broken but\nattractive nuisances.  Many of these recipes include example code from\nv5.7 of the Linux kernel.\n\nThe first section covers simple special cases, the second section\ntakes off the training wheels to cover more involved examples,\nand the third section provides a few rules of thumb.\n\n\nSimple special cases\n====================\n\nThis section presents two simple special cases, the first being where\nthere is only one CPU or only one memory location is accessed, and the\nsecond being use of that old concurrency workhorse, locking.\n\n\nSingle CPU or single memory location\n------------------------------------\n\nIf there is only one CPU on the one hand or only one variable\non the other, the code will execute in order.  There are (as\nusual) some things to be careful of:\n\n1.\tSome aspects of the C language are unordered.  For example,\n\tin the expression \"f(x) + g(y)\", the order in which f and g are\n\tcalled is not defined; the object code is allowed to use either\n\torder or even to interleave the computations.\n\n2.\tCompilers are permitted to use the \"as-if\" rule.  That is, a\n\tcompiler can emit whatever code it likes for normal accesses,\n\tas long as the results of a single-threaded execution appear\n\tjust as if the compiler had followed all the relevant rules.\n\tTo see this, compile with a high level of optimization and run\n\tthe debugger on the resulting binary.\n\n3.\tIf there is only one variable but multiple CPUs, that variable\n\tmust be properly aligned and all accesses to that variable must\n\tbe full sized.\tVariables that straddle cachelines or pages void\n\tyour full-ordering warranty, as do undersized accesses that load\n\tfrom or store to only part of the variable.\n\n4.\tIf there are multiple CPUs, accesses to shared variables should\n\tuse READ_ONCE() and WRITE_ONCE() or stronger to prevent load/store\n\ttearing, load/store fusing, and invented loads and stores.\n\tThere are exceptions to this rule, including:\n\n\ti.\tWhen there is no possibility of a given shared variable\n\t\tbeing updated by some other CPU, for example, while\n\t\tholding the update-side lock, reads from that variable\n\t\tneed not use READ_ONCE().\n\n\tii.\tWhen there is no possibility of a given shared variable\n\t\tbeing either read or updated by other CPUs, for example,\n\t\twhen running during early boot, reads from that variable\n\t\tneed not use READ_ONCE() and writes to that variable\n\t\tneed not use WRITE_ONCE().\n\n\nLocking\n-------\n\nLocking is well-known and straightforward, at least if you don't think\nabout it too hard.  And the basic rule is indeed quite simple: Any CPU that\nhas acquired a given lock sees any changes previously seen or made by any\nCPU before it released that same lock.  Note that this statement is a bit\nstronger than \"Any CPU holding a given lock sees all changes made by any\nCPU during the time that CPU was holding this same lock\".  For example,\nconsider the following pair of code fragments:\n\n\t/* See MP+polocks.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tr0 = READ_ONCE(y);\n\t\tspin_unlock(&mylock);\n\t\tr1 = READ_ONCE(x);\n\t}\n\nThe basic rule guarantees that if CPU0() acquires mylock before CPU1(),\nthen both r0 and r1 must be set to the value 1.  This also has the\nconsequence that if the final value of r0 is equal to 1, then the final\nvalue of r1 must also be equal to 1.  In contrast, the weaker rule would\nsay nothing about the final value of r1.\n\nThe converse to the basic rule also holds, as illustrated by the\nfollowing litmus test:\n\n\t/* See MP+porevlocks.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tr0 = READ_ONCE(y);\n\t\tspin_lock(&mylock);\n\t\tr1 = READ_ONCE(x);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(x, 1);\n\t\tspin_unlock(&mylock);\n\t\tWRITE_ONCE(y, 1);\n\t}\n\nThis converse to the basic rule guarantees that if CPU0() acquires\nmylock before CPU1(), then both r0 and r1 must be set to the value 0.\nThis also has the consequence that if the final value of r1 is equal\nto 0, then the final value of r0 must also be equal to 0.  In contrast,\nthe weaker rule would say nothing about the final value of r0.\n\nThese examples show only a single pair of CPUs, but the effects of the\nlocking basic rule extend across multiple acquisitions of a given lock\nacross multiple CPUs.\n\nHowever, it is not necessarily the case that accesses ordered by\nlocking will be seen as ordered by CPUs not holding that lock.\nConsider this example:\n\n\t/* See Z6.0+pooncelock+pooncelock+pombonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(x, 1);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tr0 = READ_ONCE(y);\n\t\tWRITE_ONCE(z, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU2(void)\n\t{\n\t\tWRITE_ONCE(z, 2);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nCounter-intuitive though it might be, it is quite possible to have\nthe final value of r0 be 1, the final value of z be 2, and the final\nvalue of r1 be 0.  The reason for this surprising outcome is that\nCPU2() never acquired the lock, and thus did not benefit from the\nlock's ordering properties.\n\nOrdering can be extended to CPUs not holding the lock by careful use\nof smp_mb__after_spinlock():\n\n\t/* See Z6.0+pooncelock+poonceLock+pombonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tWRITE_ONCE(x, 1);\n\t\tWRITE_ONCE(y, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tspin_lock(&mylock);\n\t\tsmp_mb__after_spinlock();\n\t\tr0 = READ_ONCE(y);\n\t\tWRITE_ONCE(z, 1);\n\t\tspin_unlock(&mylock);\n\t}\n\n\tvoid CPU2(void)\n\t{\n\t\tWRITE_ONCE(z, 2);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nThis addition of smp_mb__after_spinlock() strengthens the lock acquisition\nsufficiently to rule out the counter-intuitive outcome.\n\n\nTaking off the training wheels\n==============================\n\nThis section looks at more complex examples, including message passing,\nload buffering, release-acquire chains, store buffering.\nMany classes of litmus tests have abbreviated names, which may be found\nhere: https://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test6.pdf\n\n\nMessage passing (MP)\n--------------------\n\nThe MP pattern has one CPU execute a pair of stores to a pair of variables\nand another CPU execute a pair of loads from this same pair of variables,\nbut in the opposite order.  The goal is to avoid the counter-intuitive\noutcome in which the first load sees the value written by the second store\nbut the second load does not see the value written by the first store.\nIn the absence of any ordering, this goal may not be met, as can be seen\nin the MP+poonceonces.litmus litmus test.  This section therefore looks at\na number of ways of meeting this goal.\n\n\nRelease and acquire\n~~~~~~~~~~~~~~~~~~~\n\nUse of smp_store_release() and smp_load_acquire() is one way to force\nthe desired MP ordering.  The general approach is shown below:\n\n\t/* See MP+pooncerelease+poacquireonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_store_release(&y, 1);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tr0 = smp_load_acquire(&y);\n\t\tr1 = READ_ONCE(x);\n\t}\n\nThe smp_store_release() macro orders any prior accesses against the\nstore, while the smp_load_acquire macro orders the load against any\nsubsequent accesses.  Therefore, if the final value of r0 is the value 1,\nthe final value of r1 must also be the value 1.\n\nThe init_stack_slab() function in lib/stackdepot.c uses release-acquire\nin this way to safely initialize of a slab of the stack.  Working out\nthe mutual-exclusion design is left as an exercise for the reader.\n\n\nAssign and dereference\n~~~~~~~~~~~~~~~~~~~~~~\n\nUse of rcu_assign_pointer() and rcu_dereference() is quite similar to the\nuse of smp_store_release() and smp_load_acquire(), except that both\nrcu_assign_pointer() and rcu_dereference() operate on RCU-protected\npointers.  The general approach is shown below:\n\n\t/* See MP+onceassign+derefonce.litmus. */\n\tint z;\n\tint *y = &z;\n\tint x;\n\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\trcu_assign_pointer(y, &x);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\trcu_read_lock();\n\t\tr0 = rcu_dereference(y);\n\t\tr1 = READ_ONCE(*r0);\n\t\trcu_read_unlock();\n\t}\n\nIn this example, if the final value of r0 is &x then the final value of\nr1 must be 1.\n\nThe rcu_assign_pointer() macro has the same ordering properties as does\nsmp_store_release(), but the rcu_dereference() macro orders the load only\nagainst later accesses that depend on the value loaded.  A dependency\nis present if the value loaded determines the address of a later access\n(address dependency, as shown above), the value written by a later store\n(data dependency), or whether or not a later store is executed in the\nfirst place (control dependency).  Note that the term \"data dependency\"\nis sometimes casually used to cover both address and data dependencies.\n\nIn lib/math/prime_numbers.c, the expand_to_next_prime() function invokes\nrcu_assign_pointer(), and the next_prime_number() function invokes\nrcu_dereference().  This combination mediates access to a bit vector\nthat is expanded as additional primes are needed.\n\n\nWrite and read memory barriers\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is usually better to use smp_store_release() instead of smp_wmb()\nand to use smp_load_acquire() instead of smp_rmb().  However, the older\nsmp_wmb() and smp_rmb() APIs are still heavily used, so it is important\nto understand their use cases.  The general approach is shown below:\n\n\t/* See MP+fencewmbonceonce+fencermbonceonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(y, 1);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tr0 = READ_ONCE(y);\n\t\tsmp_rmb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nThe smp_wmb() macro orders prior stores against later stores, and the\nsmp_rmb() macro orders prior loads against later loads.  Therefore, if\nthe final value of r0 is 1, the final value of r1 must also be 1.\n\nThe xlog_state_switch_iclogs() function in fs/xfs/xfs_log.c contains\nthe following write-side code fragment:\n\n\tlog->l_curr_block -= log->l_logBBsize;\n\tASSERT(log->l_curr_block >= 0);\n\tsmp_wmb();\n\tlog->l_curr_cycle++;\n\nAnd the xlog_valid_lsn() function in fs/xfs/xfs_log_priv.h contains\nthe corresponding read-side code fragment:\n\n\tcur_cycle = READ_ONCE(log->l_curr_cycle);\n\tsmp_rmb();\n\tcur_block = READ_ONCE(log->l_curr_block);\n\nAlternatively, consider the following comment in function\nperf_output_put_handle() in kernel/events/ring_buffer.c:\n\n\t *   kernel\t\t\t\tuser\n\t *\n\t *   if (LOAD ->data_tail) {\t\tLOAD ->data_head\n\t *\t\t\t(A)\t\tsmp_rmb()\t(C)\n\t *\tSTORE $data\t\t\tLOAD $data\n\t *\tsmp_wmb()\t(B)\t\tsmp_mb()\t(D)\n\t *\tSTORE ->data_head\t\tSTORE ->data_tail\n\t *   }\n\nThe B/C pairing is an example of the MP pattern using smp_wmb() on the\nwrite side and smp_rmb() on the read side.\n\nOf course, given that smp_mb() is strictly stronger than either smp_wmb()\nor smp_rmb(), any code fragment that would work with smp_rmb() and\nsmp_wmb() would also work with smp_mb() replacing either or both of the\nweaker barriers.\n\n\nLoad buffering (LB)\n-------------------\n\nThe LB pattern has one CPU load from one variable and then store to a\nsecond, while another CPU loads from the second variable and then stores\nto the first.  The goal is to avoid the counter-intuitive situation where\neach load reads the value written by the other CPU's store.  In the\nabsence of any ordering it is quite possible that this may happen, as\ncan be seen in the LB+poonceonces.litmus litmus test.\n\nOne way of avoiding the counter-intuitive outcome is through the use of a\ncontrol dependency paired with a full memory barrier:\n\n\t/* See LB+fencembonceonce+ctrlonceonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tr0 = READ_ONCE(x);\n\t\tif (r0)\n\t\t\tWRITE_ONCE(y, 1);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tr1 = READ_ONCE(y);\n\t\tsmp_mb();\n\t\tWRITE_ONCE(x, 1);\n\t}\n\nThis pairing of a control dependency in CPU0() with a full memory\nbarrier in CPU1() prevents r0 and r1 from both ending up equal to 1.\n\nThe A/D pairing from the ring-buffer use case shown earlier also\nillustrates LB.  Here is a repeat of the comment in\nperf_output_put_handle() in kernel/events/ring_buffer.c, showing a\ncontrol dependency on the kernel side and a full memory barrier on\nthe user side:\n\n\t *   kernel\t\t\t\tuser\n\t *\n\t *   if (LOAD ->data_tail) {\t\tLOAD ->data_head\n\t *\t\t\t(A)\t\tsmp_rmb()\t(C)\n\t *\tSTORE $data\t\t\tLOAD $data\n\t *\tsmp_wmb()\t(B)\t\tsmp_mb()\t(D)\n\t *\tSTORE ->data_head\t\tSTORE ->data_tail\n\t *   }\n\t *\n\t * Where A pairs with D, and B pairs with C.\n\nThe kernel's control dependency between the load from ->data_tail\nand the store to data combined with the user's full memory barrier\nbetween the load from data and the store to ->data_tail prevents\nthe counter-intuitive outcome where the kernel overwrites the data\nbefore the user gets done loading it.\n\n\nRelease-acquire chains\n----------------------\n\nRelease-acquire chains are a low-overhead, flexible, and easy-to-use\nmethod of maintaining order.  However, they do have some limitations that\nneed to be fully understood.  Here is an example that maintains order:\n\n\t/* See ISA2+pooncerelease+poacquirerelease+poacquireonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_store_release(&y, 1);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tr0 = smp_load_acquire(y);\n\t\tsmp_store_release(&z, 1);\n\t}\n\n\tvoid CPU2(void)\n\t{\n\t\tr1 = smp_load_acquire(z);\n\t\tr2 = READ_ONCE(x);\n\t}\n\nIn this case, if r0 and r1 both have final values of 1, then r2 must\nalso have a final value of 1.\n\nThe ordering in this example is stronger than it needs to be.  For\nexample, ordering would still be preserved if CPU1()'s smp_load_acquire()\ninvocation was replaced with READ_ONCE().\n\nIt is tempting to assume that CPU0()'s store to x is globally ordered\nbefore CPU1()'s store to z, but this is not the case:\n\n\t/* See Z6.0+pooncerelease+poacquirerelease+mbonceonce.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_store_release(&y, 1);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tr0 = smp_load_acquire(y);\n\t\tsmp_store_release(&z, 1);\n\t}\n\n\tvoid CPU2(void)\n\t{\n\t\tWRITE_ONCE(z, 2);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nOne might hope that if the final value of r0 is 1 and the final value\nof z is 2, then the final value of r1 must also be 1, but it really is\npossible for r1 to have the final value of 0.  The reason, of course,\nis that in this version, CPU2() is not part of the release-acquire chain.\nThis situation is accounted for in the rules of thumb below.\n\nDespite this limitation, release-acquire chains are low-overhead as\nwell as simple and powerful, at least as memory-ordering mechanisms go.\n\n\nStore buffering\n---------------\n\nStore buffering can be thought of as upside-down load buffering, so\nthat one CPU first stores to one variable and then loads from a second,\nwhile another CPU stores to the second variable and then loads from the\nfirst.  Preserving order requires nothing less than full barriers:\n\n\t/* See SB+fencembonceonces.litmus. */\n\tvoid CPU0(void)\n\t{\n\t\tWRITE_ONCE(x, 1);\n\t\tsmp_mb();\n\t\tr0 = READ_ONCE(y);\n\t}\n\n\tvoid CPU1(void)\n\t{\n\t\tWRITE_ONCE(y, 1);\n\t\tsmp_mb();\n\t\tr1 = READ_ONCE(x);\n\t}\n\nOmitting either smp_mb() will allow both r0 and r1 to have final\nvalues of 0, but providing both full barriers as shown above prevents\nthis counter-intuitive outcome.\n\nThis pattern most famously appears as part of Dekker's locking\nalgorithm, but it has a much more practical use within the Linux kernel\nof ordering wakeups.  The following comment taken from waitqueue_active()\nin include/linux/wait.h shows the canonical pattern:\n\n *      CPU0 - waker                    CPU1 - waiter\n *\n *                                      for (;;) {\n *      @cond = true;                     prepare_to_wait(&wq_head, &wait, state);\n *      smp_mb();                         // smp_mb() from set_current_state()\n *      if (waitqueue_active(wq_head))         if (@cond)\n *        wake_up(wq_head);                      break;\n *                                        schedule();\n *                                      }\n *                                      finish_wait(&wq_head, &wait);\n\nOn CPU0, the store is to @cond and the load is in waitqueue_active().\nOn CPU1, prepare_to_wait() contains both a store to wq_head and a call\nto set_current_state(), which contains an smp_mb() barrier; the load is\n\"if (@cond)\".  The full barriers prevent the undesirable outcome where\nCPU1 puts the waiting task to sleep and CPU0 fails to wake it up.\n\nNote that use of locking can greatly simplify this pattern.\n\n\nRules of thumb\n==============\n\nThere might seem to be no pattern governing what ordering primitives are\nneeded in which situations, but this is not the case.  There is a pattern\nbased on the relation between the accesses linking successive CPUs in a\ngiven litmus test.  There are three types of linkage:\n\n1.\tWrite-to-read, where the next CPU reads the value that the\n\tprevious CPU wrote.  The LB litmus-test patterns contain only\n\tthis type of relation.\tIn formal memory-modeling texts, this\n\trelation is called \"reads-from\" and is usually abbreviated \"rf\".\n\n2.\tRead-to-write, where the next CPU overwrites the value that the\n\tprevious CPU read.  The SB litmus test contains only this type\n\tof relation.  In formal memory-modeling texts, this relation is\n\toften called \"from-reads\" and is sometimes abbreviated \"fr\".\n\n3.\tWrite-to-write, where the next CPU overwrites the value written\n\tby the previous CPU.  The Z6.0 litmus test pattern contains a\n\twrite-to-write relation between the last access of CPU1() and\n\tthe first access of CPU2().  In formal memory-modeling texts,\n\tthis relation is often called \"coherence order\" and is sometimes\n\tabbreviated \"co\".  In the C++ standard, it is instead called\n\t\"modification order\" and often abbreviated \"mo\".\n\nThe strength of memory ordering required for a given litmus test to\navoid a counter-intuitive outcome depends on the types of relations\nlinking the memory accesses for the outcome in question:\n\no\tIf all links are write-to-read links, then the weakest\n\tpossible ordering within each CPU suffices.  For example, in\n\tthe LB litmus test, a control dependency was enough to do the\n\tjob.\n\no\tIf all but one of the links are write-to-read links, then a\n\trelease-acquire chain suffices.  Both the MP and the ISA2\n\tlitmus tests illustrate this case.\n\no\tIf more than one of the links are something other than\n\twrite-to-read links, then a full memory barrier is required\n\tbetween each successive pair of non-write-to-read links.  This\n\tcase is illustrated by the Z6.0 litmus tests, both in the\n\tlocking and in the release-acquire sections.\n\nHowever, if you find yourself having to stretch these rules of thumb\nto fit your situation, you should consider creating a litmus test and\nrunning it on the model.\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}