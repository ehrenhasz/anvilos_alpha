{
  "module_name": "atomic.c",
  "hash_id": "9558daf8d76a774568630f14ee49447d081bb83d030a2c086f8cfe2697d228f2",
  "original_prompt": "Ingested from zfs-2.2.2/lib/libspl/atomic.c",
  "human_readable_source": " \n \n\n#include <atomic.h>\n\n \n#define\tATOMIC_INC(name, type) \\\n\tvoid atomic_inc_##name(volatile type *target)\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t(void) __atomic_add_fetch(target, 1, __ATOMIC_SEQ_CST);\t\\\n\t}\n\n \nATOMIC_INC(8, uint8_t)\nATOMIC_INC(16, uint16_t)\nATOMIC_INC(32, uint32_t)\nATOMIC_INC(64, uint64_t)\nATOMIC_INC(uchar, uchar_t)\nATOMIC_INC(ushort, ushort_t)\nATOMIC_INC(uint, uint_t)\nATOMIC_INC(ulong, ulong_t)\n \n\n\n#define\tATOMIC_DEC(name, type) \\\n\tvoid atomic_dec_##name(volatile type *target)\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t(void) __atomic_sub_fetch(target, 1, __ATOMIC_SEQ_CST);\t\\\n\t}\n\n \nATOMIC_DEC(8, uint8_t)\nATOMIC_DEC(16, uint16_t)\nATOMIC_DEC(32, uint32_t)\nATOMIC_DEC(64, uint64_t)\nATOMIC_DEC(uchar, uchar_t)\nATOMIC_DEC(ushort, ushort_t)\nATOMIC_DEC(uint, uint_t)\nATOMIC_DEC(ulong, ulong_t)\n \n\n\n#define\tATOMIC_ADD(name, type1, type2) \\\n\tvoid atomic_add_##name(volatile type1 *target, type2 bits)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t(void) __atomic_add_fetch(target, bits, __ATOMIC_SEQ_CST); \\\n\t}\n\nvoid\natomic_add_ptr(volatile void *target, ssize_t bits)\n{\n\t(void) __atomic_add_fetch((void **)target, bits, __ATOMIC_SEQ_CST);\n}\n\n \nATOMIC_ADD(8, uint8_t, int8_t)\nATOMIC_ADD(16, uint16_t, int16_t)\nATOMIC_ADD(32, uint32_t, int32_t)\nATOMIC_ADD(64, uint64_t, int64_t)\nATOMIC_ADD(char, uchar_t, signed char)\nATOMIC_ADD(short, ushort_t, short)\nATOMIC_ADD(int, uint_t, int)\nATOMIC_ADD(long, ulong_t, long)\n \n\n\n#define\tATOMIC_SUB(name, type1, type2) \\\n\tvoid atomic_sub_##name(volatile type1 *target, type2 bits)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t(void) __atomic_sub_fetch(target, bits, __ATOMIC_SEQ_CST); \\\n\t}\n\nvoid\natomic_sub_ptr(volatile void *target, ssize_t bits)\n{\n\t(void) __atomic_sub_fetch((void **)target, bits, __ATOMIC_SEQ_CST);\n}\n\n \nATOMIC_SUB(8, uint8_t, int8_t)\nATOMIC_SUB(16, uint16_t, int16_t)\nATOMIC_SUB(32, uint32_t, int32_t)\nATOMIC_SUB(64, uint64_t, int64_t)\nATOMIC_SUB(char, uchar_t, signed char)\nATOMIC_SUB(short, ushort_t, short)\nATOMIC_SUB(int, uint_t, int)\nATOMIC_SUB(long, ulong_t, long)\n \n\n\n#define\tATOMIC_OR(name, type) \\\n\tvoid atomic_or_##name(volatile type *target, type bits)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t(void) __atomic_or_fetch(target, bits, __ATOMIC_SEQ_CST); \\\n\t}\n\n \nATOMIC_OR(8, uint8_t)\nATOMIC_OR(16, uint16_t)\nATOMIC_OR(32, uint32_t)\nATOMIC_OR(64, uint64_t)\nATOMIC_OR(uchar, uchar_t)\nATOMIC_OR(ushort, ushort_t)\nATOMIC_OR(uint, uint_t)\nATOMIC_OR(ulong, ulong_t)\n \n\n\n#define\tATOMIC_AND(name, type) \\\n\tvoid atomic_and_##name(volatile type *target, type bits)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t(void) __atomic_and_fetch(target, bits, __ATOMIC_SEQ_CST); \\\n\t}\n\n \nATOMIC_AND(8, uint8_t)\nATOMIC_AND(16, uint16_t)\nATOMIC_AND(32, uint32_t)\nATOMIC_AND(64, uint64_t)\nATOMIC_AND(uchar, uchar_t)\nATOMIC_AND(ushort, ushort_t)\nATOMIC_AND(uint, uint_t)\nATOMIC_AND(ulong, ulong_t)\n \n\n\n \n\n#define\tATOMIC_INC_NV(name, type) \\\n\ttype atomic_inc_##name##_nv(volatile type *target)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_add_fetch(target, 1, __ATOMIC_SEQ_CST)); \\\n\t}\n\n \nATOMIC_INC_NV(8, uint8_t)\nATOMIC_INC_NV(16, uint16_t)\nATOMIC_INC_NV(32, uint32_t)\nATOMIC_INC_NV(64, uint64_t)\nATOMIC_INC_NV(uchar, uchar_t)\nATOMIC_INC_NV(ushort, ushort_t)\nATOMIC_INC_NV(uint, uint_t)\nATOMIC_INC_NV(ulong, ulong_t)\n \n\n\n#define\tATOMIC_DEC_NV(name, type) \\\n\ttype atomic_dec_##name##_nv(volatile type *target)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_sub_fetch(target, 1, __ATOMIC_SEQ_CST)); \\\n\t}\n\n \nATOMIC_DEC_NV(8, uint8_t)\nATOMIC_DEC_NV(16, uint16_t)\nATOMIC_DEC_NV(32, uint32_t)\nATOMIC_DEC_NV(64, uint64_t)\nATOMIC_DEC_NV(uchar, uchar_t)\nATOMIC_DEC_NV(ushort, ushort_t)\nATOMIC_DEC_NV(uint, uint_t)\nATOMIC_DEC_NV(ulong, ulong_t)\n \n\n\n#define\tATOMIC_ADD_NV(name, type1, type2) \\\n\ttype1 atomic_add_##name##_nv(volatile type1 *target, type2 bits) \\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_add_fetch(target, bits, __ATOMIC_SEQ_CST)); \\\n\t}\n\nvoid *\natomic_add_ptr_nv(volatile void *target, ssize_t bits)\n{\n\treturn (__atomic_add_fetch((void **)target, bits, __ATOMIC_SEQ_CST));\n}\n\n \nATOMIC_ADD_NV(8, uint8_t, int8_t)\nATOMIC_ADD_NV(16, uint16_t, int16_t)\nATOMIC_ADD_NV(32, uint32_t, int32_t)\nATOMIC_ADD_NV(64, uint64_t, int64_t)\nATOMIC_ADD_NV(char, uchar_t, signed char)\nATOMIC_ADD_NV(short, ushort_t, short)\nATOMIC_ADD_NV(int, uint_t, int)\nATOMIC_ADD_NV(long, ulong_t, long)\n \n\n\n#define\tATOMIC_SUB_NV(name, type1, type2) \\\n\ttype1 atomic_sub_##name##_nv(volatile type1 *target, type2 bits) \\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_sub_fetch(target, bits, __ATOMIC_SEQ_CST)); \\\n\t}\n\nvoid *\natomic_sub_ptr_nv(volatile void *target, ssize_t bits)\n{\n\treturn (__atomic_sub_fetch((void **)target, bits, __ATOMIC_SEQ_CST));\n}\n\n \nATOMIC_SUB_NV(8, uint8_t, int8_t)\nATOMIC_SUB_NV(char, uchar_t, signed char)\nATOMIC_SUB_NV(16, uint16_t, int16_t)\nATOMIC_SUB_NV(short, ushort_t, short)\nATOMIC_SUB_NV(32, uint32_t, int32_t)\nATOMIC_SUB_NV(int, uint_t, int)\nATOMIC_SUB_NV(long, ulong_t, long)\nATOMIC_SUB_NV(64, uint64_t, int64_t)\n \n\n\n#define\tATOMIC_OR_NV(name, type) \\\n\ttype atomic_or_##name##_nv(volatile type *target, type bits)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_or_fetch(target, bits, __ATOMIC_SEQ_CST)); \\\n\t}\n\n \nATOMIC_OR_NV(8, uint8_t)\nATOMIC_OR_NV(16, uint16_t)\nATOMIC_OR_NV(32, uint32_t)\nATOMIC_OR_NV(64, uint64_t)\nATOMIC_OR_NV(uchar, uchar_t)\nATOMIC_OR_NV(ushort, ushort_t)\nATOMIC_OR_NV(uint, uint_t)\nATOMIC_OR_NV(ulong, ulong_t)\n \n\n\n#define\tATOMIC_AND_NV(name, type) \\\n\ttype atomic_and_##name##_nv(volatile type *target, type bits)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_and_fetch(target, bits, __ATOMIC_SEQ_CST)); \\\n\t}\n\n \nATOMIC_AND_NV(8, uint8_t)\nATOMIC_AND_NV(16, uint16_t)\nATOMIC_AND_NV(32, uint32_t)\nATOMIC_AND_NV(64, uint64_t)\nATOMIC_AND_NV(uchar, uchar_t)\nATOMIC_AND_NV(ushort, ushort_t)\nATOMIC_AND_NV(uint, uint_t)\nATOMIC_AND_NV(ulong, ulong_t)\n \n\n\n \n\n#define\tATOMIC_CAS(name, type) \\\n\ttype atomic_cas_##name(volatile type *target, type exp, type des) \\\n\t{\t\t\t\t\t\t\t\t\\\n\t\t__atomic_compare_exchange_n(target, &exp, des, B_FALSE,\t\\\n\t\t    __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);\t\t\\\n\t\treturn (exp);\t\t\t\t\t\t\\\n\t}\n\nvoid *\natomic_cas_ptr(volatile void *target, void *exp, void *des)\n{\n\n\t__atomic_compare_exchange_n((void **)target, &exp, des, B_FALSE,\n\t    __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);\n\treturn (exp);\n}\n\n \nATOMIC_CAS(8, uint8_t)\nATOMIC_CAS(16, uint16_t)\nATOMIC_CAS(32, uint32_t)\nATOMIC_CAS(64, uint64_t)\nATOMIC_CAS(uchar, uchar_t)\nATOMIC_CAS(ushort, ushort_t)\nATOMIC_CAS(uint, uint_t)\nATOMIC_CAS(ulong, ulong_t)\n \n\n\n \n\n#define\tATOMIC_SWAP(name, type) \\\n\ttype atomic_swap_##name(volatile type *target, type bits)\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn (__atomic_exchange_n(target, bits, __ATOMIC_SEQ_CST)); \\\n\t}\n\n \nATOMIC_SWAP(8, uint8_t)\nATOMIC_SWAP(16, uint16_t)\nATOMIC_SWAP(32, uint32_t)\nATOMIC_SWAP(64, uint64_t)\nATOMIC_SWAP(uchar, uchar_t)\nATOMIC_SWAP(ushort, ushort_t)\nATOMIC_SWAP(uint, uint_t)\nATOMIC_SWAP(ulong, ulong_t)\n \n\nvoid *\natomic_swap_ptr(volatile void *target, void *bits)\n{\n\treturn (__atomic_exchange_n((void **)target, bits, __ATOMIC_SEQ_CST));\n}\n\n#ifndef _LP64\nuint64_t\natomic_load_64(volatile uint64_t *target)\n{\n\treturn (__atomic_load_n(target, __ATOMIC_RELAXED));\n}\n\nvoid\natomic_store_64(volatile uint64_t *target, uint64_t bits)\n{\n\treturn (__atomic_store_n(target, bits, __ATOMIC_RELAXED));\n}\n#endif\n\nint\natomic_set_long_excl(volatile ulong_t *target, uint_t value)\n{\n\tulong_t bit = 1UL << value;\n\tulong_t old = __atomic_fetch_or(target, bit, __ATOMIC_SEQ_CST);\n\treturn ((old & bit) ? -1 : 0);\n}\n\nint\natomic_clear_long_excl(volatile ulong_t *target, uint_t value)\n{\n\tulong_t bit = 1UL << value;\n\tulong_t old = __atomic_fetch_and(target, ~bit, __ATOMIC_SEQ_CST);\n\treturn ((old & bit) ? 0 : -1);\n}\n\nvoid\nmembar_enter(void)\n{\n\t__atomic_thread_fence(__ATOMIC_SEQ_CST);\n}\n\nvoid\nmembar_exit(void)\n{\n\t__atomic_thread_fence(__ATOMIC_SEQ_CST);\n}\n\nvoid\nmembar_sync(void)\n{\n\t__atomic_thread_fence(__ATOMIC_SEQ_CST);\n}\n\nvoid\nmembar_producer(void)\n{\n\t__atomic_thread_fence(__ATOMIC_RELEASE);\n}\n\nvoid\nmembar_consumer(void)\n{\n\t__atomic_thread_fence(__ATOMIC_ACQUIRE);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}