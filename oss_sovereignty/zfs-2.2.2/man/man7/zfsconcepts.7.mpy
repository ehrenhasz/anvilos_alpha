{
  "module_name": "zfsconcepts.7",
  "hash_id": "fb696afd65443b7a198de308c068436e9aa38239749b10a1fa17910b27649689",
  "original_prompt": "Ingested from zfs-2.2.2/man/man7/zfsconcepts.7",
  "human_readable_source": ".\\\"\n.\\\" CDDL HEADER START\n.\\\"\n.\\\" The contents of this file are subject to the terms of the\n.\\\" Common Development and Distribution License (the \"License\").\n.\\\" You may not use this file except in compliance with the License.\n.\\\"\n.\\\" You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE\n.\\\" or https://opensource.org/licenses/CDDL-1.0.\n.\\\" See the License for the specific language governing permissions\n.\\\" and limitations under the License.\n.\\\"\n.\\\" When distributing Covered Code, include this CDDL HEADER in each\n.\\\" file and include the License file at usr/src/OPENSOLARIS.LICENSE.\n.\\\" If applicable, add the following below this CDDL HEADER, with the\n.\\\" fields enclosed by brackets \"[]\" replaced with your own identifying\n.\\\" information: Portions Copyright [yyyy] [name of copyright owner]\n.\\\"\n.\\\" CDDL HEADER END\n.\\\"\n.\\\" Copyright (c) 2009 Sun Microsystems, Inc. All Rights Reserved.\n.\\\" Copyright 2011 Joshua M. Clulow <josh@sysmgr.org>\n.\\\" Copyright (c) 2011, 2019 by Delphix. All rights reserved.\n.\\\" Copyright (c) 2013 by Saso Kiselkov. All rights reserved.\n.\\\" Copyright (c) 2014, Joyent, Inc. All rights reserved.\n.\\\" Copyright (c) 2014 by Adam Stevko. All rights reserved.\n.\\\" Copyright (c) 2014 Integros [integros.com]\n.\\\" Copyright 2019 Richard Laager. All rights reserved.\n.\\\" Copyright 2018 Nexenta Systems, Inc.\n.\\\" Copyright 2019 Joyent, Inc.\n.\\\" Copyright 2023 Klara, Inc.\n.\\\"\n.Dd October 6, 2023\n.Dt ZFSCONCEPTS 7\n.Os\n.\n.Sh NAME\n.Nm zfsconcepts\n.Nd overview of ZFS concepts\n.\n.Sh DESCRIPTION\n.Ss ZFS File System Hierarchy\nA ZFS storage pool is a logical collection of devices that provide space for\ndatasets.\nA storage pool is also the root of the ZFS file system hierarchy.\n.Pp\nThe root of the pool can be accessed as a file system, such as mounting and\nunmounting, taking snapshots, and setting properties.\nThe physical storage characteristics, however, are managed by the\n.Xr zpool 8\ncommand.\n.Pp\nSee\n.Xr zpool 8\nfor more information on creating and administering pools.\n.Ss Snapshots\nA snapshot is a read-only copy of a file system or volume.\nSnapshots can be created extremely quickly, and initially consume no additional\nspace within the pool.\nAs data within the active dataset changes, the snapshot consumes more data than\nwould otherwise be shared with the active dataset.\n.Pp\nSnapshots can have arbitrary names.\nSnapshots of volumes can be cloned or rolled back, visibility is determined\nby the\n.Sy snapdev\nproperty of the parent volume.\n.Pp\nFile system snapshots can be accessed under the\n.Pa .zfs/snapshot\ndirectory in the root of the file system.\nSnapshots are automatically mounted on demand and may be unmounted at regular\nintervals.\nThe visibility of the\n.Pa .zfs\ndirectory can be controlled by the\n.Sy snapdir\nproperty.\n.Ss Bookmarks\nA bookmark is like a snapshot, a read-only copy of a file system or volume.\nBookmarks can be created extremely quickly, compared to snapshots, and they\nconsume no additional space within the pool.\nBookmarks can also have arbitrary names, much like snapshots.\n.Pp\nUnlike snapshots, bookmarks can not be accessed through the filesystem in any\nway.\nFrom a storage standpoint a bookmark just provides a way to reference\nwhen a snapshot was created as a distinct object.\nBookmarks are initially tied to a snapshot, not the filesystem or volume,\nand they will survive if the snapshot itself is destroyed.\nSince they are very light weight there's little incentive to destroy them.\n.Ss Clones\nA clone is a writable volume or file system whose initial contents are the same\nas another dataset.\nAs with snapshots, creating a clone is nearly instantaneous, and initially\nconsumes no additional space.\n.Pp\nClones can only be created from a snapshot.\nWhen a snapshot is cloned, it creates an implicit dependency between the parent\nand child.\nEven though the clone is created somewhere else in the dataset hierarchy, the\noriginal snapshot cannot be destroyed as long as a clone exists.\nThe\n.Sy origin\nproperty exposes this dependency, and the\n.Cm destroy\ncommand lists any such dependencies, if they exist.\n.Pp\nThe clone parent-child dependency relationship can be reversed by using the\n.Cm promote\nsubcommand.\nThis causes the\n.Qq origin\nfile system to become a clone of the specified file system, which makes it\npossible to destroy the file system that the clone was created from.\n.Ss \"Mount Points\"\nCreating a ZFS file system is a simple operation, so the number of file systems\nper system is likely to be numerous.\nTo cope with this, ZFS automatically manages mounting and unmounting file\nsystems without the need to edit the\n.Pa /etc/fstab\nfile.\nAll automatically managed file systems are mounted by ZFS at boot time.\n.Pp\nBy default, file systems are mounted under\n.Pa /path ,\nwhere\n.Ar path\nis the name of the file system in the ZFS namespace.\nDirectories are created and destroyed as needed.\n.Pp\nA file system can also have a mount point set in the\n.Sy mountpoint\nproperty.\nThis directory is created as needed, and ZFS automatically mounts the file\nsystem when the\n.Nm zfs Cm mount Fl a\ncommand is invoked\n.Po without editing\n.Pa /etc/fstab\n.Pc .\nThe\n.Sy mountpoint\nproperty can be inherited, so if\n.Em pool/home\nhas a mount point of\n.Pa /export/stuff ,\nthen\n.Em pool/home/user\nautomatically inherits a mount point of\n.Pa /export/stuff/user .\n.Pp\nA file system\n.Sy mountpoint\nproperty of\n.Sy none\nprevents the file system from being mounted.\n.Pp\nIf needed, ZFS file systems can also be managed with traditional tools\n.Po\n.Nm mount ,\n.Nm umount ,\n.Pa /etc/fstab\n.Pc .\nIf a file system's mount point is set to\n.Sy legacy ,\nZFS makes no attempt to manage the file system, and the administrator is\nresponsible for mounting and unmounting the file system.\nBecause pools must\nbe imported before a legacy mount can succeed, administrators should ensure\nthat legacy mounts are only attempted after the zpool import process\nfinishes at boot time.\nFor example, on machines using systemd, the mount option\n.Pp\n.Nm x-systemd.requires=zfs-import.target\n.Pp\nwill ensure that the zfs-import completes before systemd attempts mounting\nthe filesystem.\nSee\n.Xr systemd.mount 5\nfor details.\n.Ss Deduplication\nDeduplication is the process for removing redundant data at the block level,\nreducing the total amount of data stored.\nIf a file system has the\n.Sy dedup\nproperty enabled, duplicate data blocks are removed synchronously.\nThe result\nis that only unique data is stored and common components are shared among files.\n.Pp\nDeduplicating data is a very resource-intensive operation.\nIt is generally recommended that you have at least 1.25 GiB of RAM\nper 1 TiB of storage when you enable deduplication.\nCalculating the exact requirement depends heavily\non the type of data stored in the pool.\n.Pp\nEnabling deduplication on an improperly-designed system can result in\nperformance issues (slow I/O and administrative operations).\nIt can potentially lead to problems importing a pool due to memory exhaustion.\nDeduplication can consume significant processing power (CPU) and memory as well\nas generate additional disk I/O.\n.Pp\nBefore creating a pool with deduplication enabled, ensure that you have planned\nyour hardware requirements appropriately and implemented appropriate recovery\npractices, such as regular backups.\nConsider using the\n.Sy compression\nproperty as a less resource-intensive alternative.\n.Ss Block cloning\nBlock cloning is a facility that allows a file (or parts of a file) to be\n.Qq cloned ,\nthat is, a shallow copy made where the existing data blocks are referenced\nrather than copied.\nLater modifications to the data will cause a copy of the data block to be taken\nand that copy modified.\nThis facility is used to implement\n.Qq reflinks\nor\n.Qq file-level copy-on-write .\n.Pp\nCloned blocks are tracked in a special on-disk structure called the Block\nReference Table\n.Po BRT\n.Pc .\nUnlike deduplication, this table has minimal overhead, so can be enabled at all\ntimes.\n.Pp\nAlso unlike deduplication, cloning must be requested by a user program.\nMany common file copying programs, including newer versions of\n.Nm /bin/cp ,\nwill try to create clones automatically.\nLook for\n.Qq clone ,\n.Qq dedupe\nor\n.Qq reflink\nin the documentation for more information.\n.Pp\nThere are some limitations to block cloning.\nOnly whole blocks can be cloned, and blocks can not be cloned if they are not\nyet written to disk, or if they are encrypted, or the source and destination\n.Sy recordsize\nproperties differ.\nThe OS may add additional restrictions;\nfor example, most versions of Linux will not allow clones across datasets.\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}