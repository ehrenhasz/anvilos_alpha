{
  "module_name": "zpoolconcepts.7",
  "hash_id": "e5b1da00e06ebfd222f82fd1f49162be4b55d2c30ea181b4b38ac9a0db979170",
  "original_prompt": "Ingested from zfs-2.2.2/man/man7/zpoolconcepts.7",
  "human_readable_source": ".\\\"\n.\\\" CDDL HEADER START\n.\\\"\n.\\\" The contents of this file are subject to the terms of the\n.\\\" Common Development and Distribution License (the \"License\").\n.\\\" You may not use this file except in compliance with the License.\n.\\\"\n.\\\" You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE\n.\\\" or https://opensource.org/licenses/CDDL-1.0.\n.\\\" See the License for the specific language governing permissions\n.\\\" and limitations under the License.\n.\\\"\n.\\\" When distributing Covered Code, include this CDDL HEADER in each\n.\\\" file and include the License file at usr/src/OPENSOLARIS.LICENSE.\n.\\\" If applicable, add the following below this CDDL HEADER, with the\n.\\\" fields enclosed by brackets \"[]\" replaced with your own identifying\n.\\\" information: Portions Copyright [yyyy] [name of copyright owner]\n.\\\"\n.\\\" CDDL HEADER END\n.\\\"\n.\\\" Copyright (c) 2007, Sun Microsystems, Inc. All Rights Reserved.\n.\\\" Copyright (c) 2012, 2018 by Delphix. All rights reserved.\n.\\\" Copyright (c) 2012 Cyril Plisko. All Rights Reserved.\n.\\\" Copyright (c) 2017 Datto Inc.\n.\\\" Copyright (c) 2018 George Melikov. All Rights Reserved.\n.\\\" Copyright 2017 Nexenta Systems, Inc.\n.\\\" Copyright (c) 2017 Open-E, Inc. All Rights Reserved.\n.\\\"\n.Dd April 7, 2023\n.Dt ZPOOLCONCEPTS 7\n.Os\n.\n.Sh NAME\n.Nm zpoolconcepts\n.Nd overview of ZFS storage pools\n.\n.Sh DESCRIPTION\n.Ss Virtual Devices (vdevs)\nA \"virtual device\" describes a single device or a collection of devices,\norganized according to certain performance and fault characteristics.\nThe following virtual devices are supported:\n.Bl -tag -width \"special\"\n.It Sy disk\nA block device, typically located under\n.Pa /dev .\nZFS can use individual slices or partitions, though the recommended mode of\noperation is to use whole disks.\nA disk can be specified by a full path, or it can be a shorthand name\n.Po the relative portion of the path under\n.Pa /dev\n.Pc .\nA whole disk can be specified by omitting the slice or partition designation.\nFor example,\n.Pa sda\nis equivalent to\n.Pa /dev/sda .\nWhen given a whole disk, ZFS automatically labels the disk, if necessary.\n.It Sy file\nA regular file.\nThe use of files as a backing store is strongly discouraged.\nIt is designed primarily for experimental purposes, as the fault tolerance of a\nfile is only as good as the file system on which it resides.\nA file must be specified by a full path.\n.It Sy mirror\nA mirror of two or more devices.\nData is replicated in an identical fashion across all components of a mirror.\nA mirror with\n.Em N No disks of size Em X No can hold Em X No bytes and can withstand Em N-1\ndevices failing, without losing data.\n.It Sy raidz , raidz1 , raidz2 , raidz3\nA distributed-parity layout, similar to RAID-5/6, with improved distribution of\nparity, and which does not suffer from the RAID-5/6\n.Qq write hole ,\n.Pq in which data and parity become inconsistent after a power loss .\nData and parity is striped across all disks within a raidz group, though not\nnecessarily in a consistent stripe width.\n.Pp\nA raidz group can have single, double, or triple parity, meaning that the\nraidz group can sustain one, two, or three failures, respectively, without\nlosing any data.\nThe\n.Sy raidz1\nvdev type specifies a single-parity raidz group; the\n.Sy raidz2\nvdev type specifies a double-parity raidz group; and the\n.Sy raidz3\nvdev type specifies a triple-parity raidz group.\nThe\n.Sy raidz\nvdev type is an alias for\n.Sy raidz1 .\n.Pp\nA raidz group with\n.Em N No disks of size Em X No with Em P No parity disks can hold approximately\n.Em (N-P)*X No bytes and can withstand Em P No devices failing without losing data .\nThe minimum number of devices in a raidz group is one more than the number of\nparity disks.\nThe recommended number is between 3 and 9 to help increase performance.\n.It Sy draid , draid1 , draid2 , draid3\nA variant of raidz that provides integrated distributed hot spares, allowing\nfor faster resilvering, while retaining the benefits of raidz.\nA dRAID vdev is constructed from multiple internal raidz groups, each with\n.Em D No data devices and Em P No parity devices .\nThese groups are distributed over all of the children in order to fully\nutilize the available disk performance.\n.Pp\nUnlike raidz, dRAID uses a fixed stripe width (padding as necessary with\nzeros) to allow fully sequential resilvering.\nThis fixed stripe width significantly affects both usable capacity and IOPS.\nFor example, with the default\n.Em D=8 No and Em 4 KiB No disk sectors the minimum allocation size is Em 32 KiB .\nIf using compression, this relatively large allocation size can reduce the\neffective compression ratio.\nWhen using ZFS volumes (zvols) and dRAID, the default of the\n.Sy volblocksize\nproperty is increased to account for the allocation size.\nIf a dRAID pool will hold a significant amount of small blocks, it is\nrecommended to also add a mirrored\n.Sy special\nvdev to store those blocks.\n.Pp\nIn regards to I/O, performance is similar to raidz since, for any read, all\n.Em D No data disks must be accessed .\nDelivered random IOPS can be reasonably approximated as\n.Sy floor((N-S)/(D+P))*single_drive_IOPS .\n.Pp\nLike raidz, a dRAID can have single-, double-, or triple-parity.\nThe\n.Sy draid1 ,\n.Sy draid2 ,\nand\n.Sy draid3\ntypes can be used to specify the parity level.\nThe\n.Sy draid\nvdev type is an alias for\n.Sy draid1 .\n.Pp\nA dRAID with\n.Em N No disks of size Em X , D No data disks per redundancy group , Em P\n.No parity level, and Em S No distributed hot spares can hold approximately\n.Em (N-S)*(D/(D+P))*X No bytes and can withstand Em P\ndevices failing without losing data.\n.It Sy draid Ns Oo Ar parity Oc Ns Oo Sy \\&: Ns Ar data Ns Sy d Oc Ns Oo Sy \\&: Ns Ar children Ns Sy c Oc Ns Oo Sy \\&: Ns Ar spares Ns Sy s Oc\nA non-default dRAID configuration can be specified by appending one or more\nof the following optional arguments to the\n.Sy draid\nkeyword:\n.Bl -tag -compact -width \"children\"\n.It Ar parity\nThe parity level (1-3).\n.It Ar data\nThe number of data devices per redundancy group.\nIn general, a smaller value of\n.Em D No will increase IOPS, improve the compression ratio ,\nand speed up resilvering at the expense of total usable capacity.\nDefaults to\n.Em 8 , No unless Em N-P-S No is less than Em 8 .\n.It Ar children\nThe expected number of children.\nUseful as a cross-check when listing a large number of devices.\nAn error is returned when the provided number of children differs.\n.It Ar spares\nThe number of distributed hot spares.\nDefaults to zero.\n.El\n.It Sy spare\nA pseudo-vdev which keeps track of available hot spares for a pool.\nFor more information, see the\n.Sx Hot Spares\nsection.\n.It Sy log\nA separate intent log device.\nIf more than one log device is specified, then writes are load-balanced between\ndevices.\nLog devices can be mirrored.\nHowever, raidz vdev types are not supported for the intent log.\nFor more information, see the\n.Sx Intent Log\nsection.\n.It Sy dedup\nA device solely dedicated for deduplication tables.\nThe redundancy of this device should match the redundancy of the other normal\ndevices in the pool.\nIf more than one dedup device is specified, then\nallocations are load-balanced between those devices.\n.It Sy special\nA device dedicated solely for allocating various kinds of internal metadata,\nand optionally small file blocks.\nThe redundancy of this device should match the redundancy of the other normal\ndevices in the pool.\nIf more than one special device is specified, then\nallocations are load-balanced between those devices.\n.Pp\nFor more information on special allocations, see the\n.Sx Special Allocation Class\nsection.\n.It Sy cache\nA device used to cache storage pool data.\nA cache device cannot be configured as a mirror or raidz group.\nFor more information, see the\n.Sx Cache Devices\nsection.\n.El\n.Pp\nVirtual devices cannot be nested arbitrarily.\nA mirror, raidz or draid virtual device can only be created with files or disks.\nMirrors of mirrors or other such combinations are not allowed.\n.Pp\nA pool can have any number of virtual devices at the top of the configuration\n.Po known as\n.Qq root vdevs\n.Pc .\nData is dynamically distributed across all top-level devices to balance data\namong devices.\nAs new virtual devices are added, ZFS automatically places data on the newly\navailable devices.\n.Pp\nVirtual devices are specified one at a time on the command line,\nseparated by whitespace.\nKeywords like\n.Sy mirror No and Sy raidz\nare used to distinguish where a group ends and another begins.\nFor example, the following creates a pool with two root vdevs,\neach a mirror of two disks:\n.Dl # Nm zpool Cm create Ar mypool Sy mirror Ar sda sdb Sy mirror Ar sdc sdd\n.\n.Ss Device Failure and Recovery\nZFS supports a rich set of mechanisms for handling device failure and data\ncorruption.\nAll metadata and data is checksummed, and ZFS automatically repairs bad data\nfrom a good copy, when corruption is detected.\n.Pp\nIn order to take advantage of these features, a pool must make use of some form\nof redundancy, using either mirrored or raidz groups.\nWhile ZFS supports running in a non-redundant configuration, where each root\nvdev is simply a disk or file, this is strongly discouraged.\nA single case of bit corruption can render some or all of your data unavailable.\n.Pp\nA pool's health status is described by one of three states:\n.Sy online , degraded , No or Sy faulted .\nAn online pool has all devices operating normally.\nA degraded pool is one in which one or more devices have failed, but the data is\nstill available due to a redundant configuration.\nA faulted pool has corrupted metadata, or one or more faulted devices, and\ninsufficient replicas to continue functioning.\n.Pp\nThe health of the top-level vdev, such as a mirror or raidz device,\nis potentially impacted by the state of its associated vdevs\nor component devices.\nA top-level vdev or component device is in one of the following states:\n.Bl -tag -width \"DEGRADED\"\n.It Sy DEGRADED\nOne or more top-level vdevs is in the degraded state because one or more\ncomponent devices are offline.\nSufficient replicas exist to continue functioning.\n.Pp\nOne or more component devices is in the degraded or faulted state, but\nsufficient replicas exist to continue functioning.\nThe underlying conditions are as follows:\n.Bl -bullet -compact\n.It\nThe number of checksum errors exceeds acceptable levels and the device is\ndegraded as an indication that something may be wrong.\nZFS continues to use the device as necessary.\n.It\nThe number of I/O errors exceeds acceptable levels.\nThe device could not be marked as faulted because there are insufficient\nreplicas to continue functioning.\n.El\n.It Sy FAULTED\nOne or more top-level vdevs is in the faulted state because one or more\ncomponent devices are offline.\nInsufficient replicas exist to continue functioning.\n.Pp\nOne or more component devices is in the faulted state, and insufficient\nreplicas exist to continue functioning.\nThe underlying conditions are as follows:\n.Bl -bullet -compact\n.It\nThe device could be opened, but the contents did not match expected values.\n.It\nThe number of I/O errors exceeds acceptable levels and the device is faulted to\nprevent further use of the device.\n.El\n.It Sy OFFLINE\nThe device was explicitly taken offline by the\n.Nm zpool Cm offline\ncommand.\n.It Sy ONLINE\nThe device is online and functioning.\n.It Sy REMOVED\nThe device was physically removed while the system was running.\nDevice removal detection is hardware-dependent and may not be supported on all\nplatforms.\n.It Sy UNAVAIL\nThe device could not be opened.\nIf a pool is imported when a device was unavailable, then the device will be\nidentified by a unique identifier instead of its path since the path was never\ncorrect in the first place.\n.El\n.Pp\nChecksum errors represent events where a disk returned data that was expected\nto be correct, but was not.\nIn other words, these are instances of silent data corruption.\nThe checksum errors are reported in\n.Nm zpool Cm status\nand\n.Nm zpool Cm events .\nWhen a block is stored redundantly, a damaged block may be reconstructed\n(e.g. from raidz parity or a mirrored copy).\nIn this case, ZFS reports the checksum error against the disks that contained\ndamaged data.\nIf a block is unable to be reconstructed (e.g. due to 3 disks being damaged\nin a raidz2 group), it is not possible to determine which disks were silently\ncorrupted.\nIn this case, checksum errors are reported for all disks on which the block\nis stored.\n.Pp\nIf a device is removed and later re-attached to the system,\nZFS attempts to bring the device online automatically.\nDevice attachment detection is hardware-dependent\nand might not be supported on all platforms.\n.\n.Ss Hot Spares\nZFS allows devices to be associated with pools as\n.Qq hot spares .\nThese devices are not actively used in the pool.\nBut, when an active device\nfails, it is automatically replaced by a hot spare.\nTo create a pool with hot spares, specify a\n.Sy spare\nvdev with any number of devices.\nFor example,\n.Dl # Nm zpool Cm create Ar pool Sy mirror Ar sda sdb Sy spare Ar sdc sdd\n.Pp\nSpares can be shared across multiple pools, and can be added with the\n.Nm zpool Cm add\ncommand and removed with the\n.Nm zpool Cm remove\ncommand.\nOnce a spare replacement is initiated, a new\n.Sy spare\nvdev is created within the configuration that will remain there until the\noriginal device is replaced.\nAt this point, the hot spare becomes available again, if another device fails.\n.Pp\nIf a pool has a shared spare that is currently being used, the pool cannot be\nexported, since other pools may use this shared spare, which may lead to\npotential data corruption.\n.Pp\nShared spares add some risk.\nIf the pools are imported on different hosts,\nand both pools suffer a device failure at the same time,\nboth could attempt to use the spare at the same time.\nThis may not be detected, resulting in data corruption.\n.Pp\nAn in-progress spare replacement can be cancelled by detaching the hot spare.\nIf the original faulted device is detached, then the hot spare assumes its\nplace in the configuration, and is removed from the spare list of all active\npools.\n.Pp\nThe\n.Sy draid\nvdev type provides distributed hot spares.\nThese hot spares are named after the dRAID vdev they're a part of\n.Po Sy draid1 Ns - Ns Ar 2 Ns - Ns Ar 3 No specifies spare Ar 3 No of vdev Ar 2 ,\n.No which is a single parity dRAID Pc\nand may only be used by that dRAID vdev.\nOtherwise, they behave the same as normal hot spares.\n.Pp\nSpares cannot replace log devices.\n.\n.Ss Intent Log\nThe ZFS Intent Log (ZIL) satisfies POSIX requirements for synchronous\ntransactions.\nFor instance, databases often require their transactions to be on stable storage\ndevices when returning from a system call.\nNFS and other applications can also use\n.Xr fsync 2\nto ensure data stability.\nBy default, the intent log is allocated from blocks within the main pool.\nHowever, it might be possible to get better performance using separate intent\nlog devices such as NVRAM or a dedicated disk.\nFor example:\n.Dl # Nm zpool Cm create Ar pool sda sdb Sy log Ar sdc\n.Pp\nMultiple log devices can also be specified, and they can be mirrored.\nSee the\n.Sx EXAMPLES\nsection for an example of mirroring multiple log devices.\n.Pp\nLog devices can be added, replaced, attached, detached, and removed.\nIn addition, log devices are imported and exported as part of the pool\nthat contains them.\nMirrored devices can be removed by specifying the top-level mirror vdev.\n.\n.Ss Cache Devices\nDevices can be added to a storage pool as\n.Qq cache devices .\nThese devices provide an additional layer of caching between main memory and\ndisk.\nFor read-heavy workloads, where the working set size is much larger than what\ncan be cached in main memory, using cache devices allows much more of this\nworking set to be served from low latency media.\nUsing cache devices provides the greatest performance improvement for random\nread-workloads of mostly static content.\n.Pp\nTo create a pool with cache devices, specify a\n.Sy cache\nvdev with any number of devices.\nFor example:\n.Dl # Nm zpool Cm create Ar pool sda sdb Sy cache Ar sdc sdd\n.Pp\nCache devices cannot be mirrored or part of a raidz configuration.\nIf a read error is encountered on a cache device, that read I/O is reissued to\nthe original storage pool device, which might be part of a mirrored or raidz\nconfiguration.\n.Pp\nThe content of the cache devices is persistent across reboots and restored\nasynchronously when importing the pool in L2ARC (persistent L2ARC).\nThis can be disabled by setting\n.Sy l2arc_rebuild_enabled Ns = Ns Sy 0 .\nFor cache devices smaller than\n.Em 1 GiB ,\nZFS does not write the metadata structures\nrequired for rebuilding the L2ARC, to conserve space.\nThis can be changed with\n.Sy l2arc_rebuild_blocks_min_l2size .\nThe cache device header\n.Pq Em 512 B\nis updated even if no metadata structures are written.\nSetting\n.Sy l2arc_headroom Ns = Ns Sy 0\nwill result in scanning the full-length ARC lists for cacheable content to be\nwritten in L2ARC (persistent ARC).\nIf a cache device is added with\n.Nm zpool Cm add ,\nits label and header will be overwritten and its contents will not be\nrestored in L2ARC, even if the device was previously part of the pool.\nIf a cache device is onlined with\n.Nm zpool Cm online ,\nits contents will be restored in L2ARC.\nThis is useful in case of memory pressure,\nwhere the contents of the cache device are not fully restored in L2ARC.\nThe user can off- and online the cache device when there is less memory\npressure, to fully restore its contents to L2ARC.\n.\n.Ss Pool checkpoint\nBefore starting critical procedures that include destructive actions\n.Pq like Nm zfs Cm destroy ,\nan administrator can checkpoint the pool's state and, in the case of a\nmistake or failure, rewind the entire pool back to the checkpoint.\nOtherwise, the checkpoint can be discarded when the procedure has completed\nsuccessfully.\n.Pp\nA pool checkpoint can be thought of as a pool-wide snapshot and should be used\nwith care as it contains every part of the pool's state, from properties to vdev\nconfiguration.\nThus, certain operations are not allowed while a pool has a checkpoint.\nSpecifically, vdev removal/attach/detach, mirror splitting, and\nchanging the pool's GUID.\nAdding a new vdev is supported, but in the case of a rewind it will have to be\nadded again.\nFinally, users of this feature should keep in mind that scrubs in a pool that\nhas a checkpoint do not repair checkpointed data.\n.Pp\nTo create a checkpoint for a pool:\n.Dl # Nm zpool Cm checkpoint Ar pool\n.Pp\nTo later rewind to its checkpointed state, you need to first export it and\nthen rewind it during import:\n.Dl # Nm zpool Cm export Ar pool\n.Dl # Nm zpool Cm import Fl -rewind-to-checkpoint Ar pool\n.Pp\nTo discard the checkpoint from a pool:\n.Dl # Nm zpool Cm checkpoint Fl d Ar pool\n.Pp\nDataset reservations (controlled by the\n.Sy reservation No and Sy refreservation\nproperties) may be unenforceable while a checkpoint exists, because the\ncheckpoint is allowed to consume the dataset's reservation.\nFinally, data that is part of the checkpoint but has been freed in the\ncurrent state of the pool won't be scanned during a scrub.\n.\n.Ss Special Allocation Class\nAllocations in the special class are dedicated to specific block types.\nBy default, this includes all metadata, the indirect blocks of user data, and\nany deduplication tables.\nThe class can also be provisioned to accept small file blocks.\n.Pp\nA pool must always have at least one normal\n.Pq non- Ns Sy dedup Ns /- Ns Sy special\nvdev before\nother devices can be assigned to the special class.\nIf the\n.Sy special\nclass becomes full, then allocations intended for it\nwill spill back into the normal class.\n.Pp\nDeduplication tables can be excluded from the special class by unsetting the\n.Sy zfs_ddt_data_is_special\nZFS module parameter.\n.Pp\nInclusion of small file blocks in the special class is opt-in.\nEach dataset can control the size of small file blocks allowed\nin the special class by setting the\n.Sy special_small_blocks\nproperty to nonzero.\nSee\n.Xr zfsprops 7\nfor more info on this property.\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}