{
  "module_name": "zpool-iostat.8",
  "hash_id": "dff6c28dd3501b431a4f2f96d4eca2e26ad222ca0c330f702665ad9752ad13da",
  "original_prompt": "Ingested from zfs-2.2.2/man/man8/zpool-iostat.8",
  "human_readable_source": ".\\\"\n.\\\" CDDL HEADER START\n.\\\"\n.\\\" The contents of this file are subject to the terms of the\n.\\\" Common Development and Distribution License (the \"License\").\n.\\\" You may not use this file except in compliance with the License.\n.\\\"\n.\\\" You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE\n.\\\" or https://opensource.org/licenses/CDDL-1.0.\n.\\\" See the License for the specific language governing permissions\n.\\\" and limitations under the License.\n.\\\"\n.\\\" When distributing Covered Code, include this CDDL HEADER in each\n.\\\" file and include the License file at usr/src/OPENSOLARIS.LICENSE.\n.\\\" If applicable, add the following below this CDDL HEADER, with the\n.\\\" fields enclosed by brackets \"[]\" replaced with your own identifying\n.\\\" information: Portions Copyright [yyyy] [name of copyright owner]\n.\\\"\n.\\\" CDDL HEADER END\n.\\\"\n.\\\" Copyright (c) 2007, Sun Microsystems, Inc. All Rights Reserved.\n.\\\" Copyright (c) 2012, 2018 by Delphix. All rights reserved.\n.\\\" Copyright (c) 2012 Cyril Plisko. All Rights Reserved.\n.\\\" Copyright (c) 2017 Datto Inc.\n.\\\" Copyright (c) 2018 George Melikov. All Rights Reserved.\n.\\\" Copyright 2017 Nexenta Systems, Inc.\n.\\\" Copyright (c) 2017 Open-E, Inc. All Rights Reserved.\n.\\\"\n.Dd March 16, 2022\n.Dt ZPOOL-IOSTAT 8\n.Os\n.\n.Sh NAME\n.Nm zpool-iostat\n.Nd display logical I/O statistics for ZFS storage pools\n.Sh SYNOPSIS\n.Nm zpool\n.Cm iostat\n.Op Oo Oo Fl c Ar SCRIPT Oc Oo Fl lq Oc Oc Ns | Ns Fl rw\n.Op Fl T Sy u Ns | Ns Sy d\n.Op Fl ghHLnpPvy\n.Oo Ar pool Ns \u2026 Ns | Ns Oo Ar pool vdev Ns \u2026 Oc Ns | Ns Ar vdev Ns \u2026 Oc\n.Op Ar interval Op Ar count\n.\n.Sh DESCRIPTION\nDisplays logical I/O statistics for the given pools/vdevs.\nPhysical I/O statistics may be observed via\n.Xr iostat 1 .\nIf writes are located nearby, they may be merged into a single\nlarger operation.\nAdditional I/O may be generated depending on the level of vdev redundancy.\nTo filter output, you may pass in a list of pools, a pool and list of vdevs\nin that pool, or a list of any vdevs from any pool.\nIf no items are specified, statistics for every pool in the system are shown.\nWhen given an\n.Ar interval ,\nthe statistics are printed every\n.Ar interval\nseconds until killed.\nIf\n.Fl n\nflag is specified the headers are displayed only once, otherwise they are\ndisplayed periodically.\nIf\n.Ar count\nis specified, the command exits after\n.Ar count\nreports are printed.\nThe first report printed is always the statistics since boot regardless of\nwhether\n.Ar interval\nand\n.Ar count\nare passed.\nHowever, this behavior can be suppressed with the\n.Fl y\nflag.\nAlso note that the units of\n.Sy K ,\n.Sy M ,\n.Sy G Ns \u2026\nthat are printed in the report are in base 1024.\nTo get the raw values, use the\n.Fl p\nflag.\n.Bl -tag -width Ds\n.It Fl c Op Ar SCRIPT1 Ns Oo , Ns Ar SCRIPT2 Oc Ns \u2026\nRun a script (or scripts) on each vdev and include the output as a new column\nin the\n.Nm zpool Cm iostat\noutput.\nUsers can run any script found in their\n.Pa ~/.zpool.d\ndirectory or from the system\n.Pa /etc/zfs/zpool.d\ndirectory.\nScript names containing the slash\n.Pq Sy /\ncharacter are not allowed.\nThe default search path can be overridden by setting the\n.Sy ZPOOL_SCRIPTS_PATH\nenvironment variable.\nA privileged user can only run\n.Fl c\nif they have the\n.Sy ZPOOL_SCRIPTS_AS_ROOT\nenvironment variable set.\nIf a script requires the use of a privileged command, like\n.Xr smartctl 8 ,\nthen it's recommended you allow the user access to it in\n.Pa /etc/sudoers\nor add the user to the\n.Pa /etc/sudoers.d/zfs\nfile.\n.Pp\nIf\n.Fl c\nis passed without a script name, it prints a list of all scripts.\n.Fl c\nalso sets verbose mode\n.No \\&( Ns Fl v Ns No \\&) .\n.Pp\nScript output should be in the form of \"name=value\".\nThe column name is set to \"name\" and the value is set to \"value\".\nMultiple lines can be used to output multiple columns.\nThe first line of output not in the\n\"name=value\" format is displayed without a column title,\nand no more output after that is displayed.\nThis can be useful for printing error messages.\nBlank or NULL values are printed as a '-' to make output AWKable.\n.Pp\nThe following environment variables are set before running each script:\n.Bl -tag -compact -width \"VDEV_ENC_SYSFS_PATH\"\n.It Sy VDEV_PATH\nFull path to the vdev\n.It Sy VDEV_UPATH\nUnderlying path to the vdev\n.Pq Pa /dev/sd* .\nFor use with device mapper, multipath, or partitioned vdevs.\n.It Sy VDEV_ENC_SYSFS_PATH\nThe sysfs path to the enclosure for the vdev (if any).\n.El\n.It Fl T Sy u Ns | Ns Sy d\nDisplay a time stamp.\nSpecify\n.Sy u\nfor a printed representation of the internal representation of time.\nSee\n.Xr time 2 .\nSpecify\n.Sy d\nfor standard date format.\nSee\n.Xr date 1 .\n.It Fl g\nDisplay vdev GUIDs instead of the normal device names.\nThese GUIDs can be used in place of device names for the zpool\ndetach/offline/remove/replace commands.\n.It Fl H\nScripted mode.\nDo not display headers, and separate fields by a\nsingle tab instead of arbitrary space.\n.It Fl L\nDisplay real paths for vdevs resolving all symbolic links.\nThis can be used to look up the current block device name regardless of the\n.Pa /dev/disk/\npath used to open it.\n.It Fl n\nPrint headers only once when passed\n.It Fl p\nDisplay numbers in parsable (exact) values.\nTime values are in nanoseconds.\n.It Fl P\nDisplay full paths for vdevs instead of only the last component of the path.\nThis can be used in conjunction with the\n.Fl L\nflag.\n.It Fl r\nPrint request size histograms for the leaf vdev's I/O.\nThis includes histograms of individual I/O (ind) and aggregate I/O (agg).\nThese stats can be useful for observing how well I/O aggregation is working.\nNote that TRIM I/O may exceed 16M, but will be counted as 16M.\n.It Fl v\nVerbose statistics Reports usage statistics for individual vdevs within the\npool, in addition to the pool-wide statistics.\n.It Fl y\nNormally the first line of output reports the statistics since boot:\nsuppress it.\n.It Fl w\nDisplay latency histograms:\n.Bl -tag -compact -width \"asyncq_read/write\"\n.It Sy total_wait\nTotal I/O time (queuing + disk I/O time).\n.It Sy disk_wait\nDisk I/O time (time reading/writing the disk).\n.It Sy syncq_wait\nAmount of time I/O spent in synchronous priority queues.\nDoes not include disk time.\n.It Sy asyncq_wait\nAmount of time I/O spent in asynchronous priority queues.\nDoes not include disk time.\n.It Sy scrub\nAmount of time I/O spent in scrub queue.\nDoes not include disk time.\n.It Sy rebuild\nAmount of time I/O spent in rebuild queue.\nDoes not include disk time.\n.El\n.It Fl l\nInclude average latency statistics:\n.Bl -tag -compact -width \"asyncq_read/write\"\n.It Sy total_wait\nAverage total I/O time (queuing + disk I/O time).\n.It Sy disk_wait\nAverage disk I/O time (time reading/writing the disk).\n.It Sy syncq_wait\nAverage amount of time I/O spent in synchronous priority queues.\nDoes not include disk time.\n.It Sy asyncq_wait\nAverage amount of time I/O spent in asynchronous priority queues.\nDoes not include disk time.\n.It Sy scrub\nAverage queuing time in scrub queue.\nDoes not include disk time.\n.It Sy trim\nAverage queuing time in trim queue.\nDoes not include disk time.\n.It Sy rebuild\nAverage queuing time in rebuild queue.\nDoes not include disk time.\n.El\n.It Fl q\nInclude active queue statistics.\nEach priority queue has both pending\n.Sy ( pend )\nand active\n.Sy ( activ )\nI/O requests.\nPending requests are waiting to be issued to the disk,\nand active requests have been issued to disk and are waiting for completion.\nThese stats are broken out by priority queue:\n.Bl -tag -compact -width \"asyncq_read/write\"\n.It Sy syncq_read/write\nCurrent number of entries in synchronous priority\nqueues.\n.It Sy asyncq_read/write\nCurrent number of entries in asynchronous priority queues.\n.It Sy scrubq_read\nCurrent number of entries in scrub queue.\n.It Sy trimq_write\nCurrent number of entries in trim queue.\n.It Sy rebuildq_write\nCurrent number of entries in rebuild queue.\n.El\n.Pp\nAll queue statistics are instantaneous measurements of the number of\nentries in the queues.\nIf you specify an interval,\nthe measurements will be sampled from the end of the interval.\n.El\n.\n.Sh EXAMPLES\n.\\\" These are, respectively, examples 13, 16 from zpool.8\n.\\\" Make sure to update them bidirectionally\n.Ss Example 13 : No Adding Cache Devices to a ZFS Pool\nThe following command adds two disks for use as cache devices to a ZFS storage\npool:\n.Dl # Nm zpool Cm add Ar pool Sy cache Pa sdc sdd\n.Pp\nOnce added, the cache devices gradually fill with content from main memory.\nDepending on the size of your cache devices, it could take over an hour for\nthem to fill.\nCapacity and reads can be monitored using the\n.Cm iostat\nsubcommand as follows:\n.Dl # Nm zpool Cm iostat Fl v Ar pool 5\n.\n.Ss Example 16 : No Adding output columns\nAdditional columns can be added to the\n.Nm zpool Cm status No and Nm zpool Cm iostat No output with Fl c .\n.Bd -literal -compact -offset Ds\n.No # Nm zpool Cm status Fl c Pa vendor , Ns Pa model , Ns Pa size\n   NAME     STATE  READ WRITE CKSUM vendor  model        size\n   tank     ONLINE 0    0     0\n   mirror-0 ONLINE 0    0     0\n   U1       ONLINE 0    0     0     SEAGATE ST8000NM0075 7.3T\n   U10      ONLINE 0    0     0     SEAGATE ST8000NM0075 7.3T\n   U11      ONLINE 0    0     0     SEAGATE ST8000NM0075 7.3T\n   U12      ONLINE 0    0     0     SEAGATE ST8000NM0075 7.3T\n   U13      ONLINE 0    0     0     SEAGATE ST8000NM0075 7.3T\n   U14      ONLINE 0    0     0     SEAGATE ST8000NM0075 7.3T\n\n.No # Nm zpool Cm iostat Fl vc Pa size\n              capacity     operations     bandwidth\npool        alloc   free   read  write   read  write  size\n----------  -----  -----  -----  -----  -----  -----  ----\nrpool       14.6G  54.9G      4     55   250K  2.69M\n  sda1      14.6G  54.9G      4     55   250K  2.69M   70G\n----------  -----  -----  -----  -----  -----  -----  ----\n.Ed\n.\n.Sh SEE ALSO\n.Xr iostat 1 ,\n.Xr smartctl 8 ,\n.Xr zpool-list 8 ,\n.Xr zpool-status 8\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}