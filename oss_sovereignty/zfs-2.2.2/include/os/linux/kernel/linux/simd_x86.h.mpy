{
  "module_name": "simd_x86.h",
  "hash_id": "212ba708d6167ff5f1d44d0937a7741d4bd67ac71f2888a6ccbb628815a4cf8f",
  "original_prompt": "Ingested from zfs-2.2.2/include/os/linux/kernel/linux/simd_x86.h",
  "human_readable_source": " \n \n\n \n\n#ifndef _LINUX_SIMD_X86_H\n#define\t_LINUX_SIMD_X86_H\n\n \n#if defined(__x86)\n\n#include <sys/types.h>\n#include <asm/cpufeature.h>\n\n \n#if defined(CONFIG_X86_DEBUG_FPU) && !defined(KERNEL_EXPORTS_X86_FPU)\n#undef CONFIG_X86_DEBUG_FPU\n#endif\n\n \n#if defined(KERNEL_EXPORTS_X86_FPU)\n\n#if defined(HAVE_KERNEL_FPU_API_HEADER)\n#include <asm/fpu/api.h>\n#if defined(HAVE_KERNEL_FPU_INTERNAL_HEADER)\n#include <asm/fpu/internal.h>\n#endif\n#else\n#include <asm/i387.h>\n#endif\n\n#define\tkfpu_allowed()\t\t1\n#define\tkfpu_init()\t\t0\n#define\tkfpu_fini()\t\t((void) 0)\n\n#if defined(HAVE_UNDERSCORE_KERNEL_FPU)\n#define\tkfpu_begin()\t\t\\\n{\t\t\t\t\\\n\tpreempt_disable();\t\\\n\t__kernel_fpu_begin();\t\\\n}\n#define\tkfpu_end()\t\t\\\n{\t\t\t\t\\\n\t__kernel_fpu_end();\t\\\n\tpreempt_enable();\t\\\n}\n\n#elif defined(HAVE_KERNEL_FPU)\n#define\tkfpu_begin()\t\tkernel_fpu_begin()\n#define\tkfpu_end()\t\tkernel_fpu_end()\n\n#else\n \n#error \"Unreachable kernel configuration\"\n#endif\n\n#else  \n\n \n#if defined(HAVE_KERNEL_FPU_INTERNAL)\n\n \n#if !defined(HAVE_XSAVE)\n#error \"Toolchain needs to support the XSAVE assembler instruction\"\n#endif\n\n#ifndef XFEATURE_MASK_XTILE\n \n#define\tXFEATURE_MASK_XTILE\t0x60000\n#endif\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n\nextern uint8_t **zfs_kfpu_fpregs;\n\n \nstatic inline uint32_t\nget_xsave_area_size(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_OSXSAVE)) {\n\t\treturn (0);\n\t}\n\t \n\tuint32_t eax, ebx, ecx, edx;\n\n\teax = 13U;\n\tecx = 0U;\n\t__asm__ __volatile__(\"cpuid\"\n\t    : \"=a\" (eax), \"=b\" (ebx), \"=c\" (ecx), \"=d\" (edx)\n\t    : \"a\" (eax), \"c\" (ecx));\n\n\treturn (ecx);\n}\n\n \nstatic inline int\nget_fpuregs_save_area_order(void)\n{\n\tsize_t area_size = (size_t)get_xsave_area_size();\n\n\t \n\tif (area_size == 0) {\n\t\tarea_size = 512;\n\t}\n\treturn (get_order(area_size));\n}\n\n \nstatic inline void\nkfpu_fini(void)\n{\n\tint cpu;\n\tint order = get_fpuregs_save_area_order();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (zfs_kfpu_fpregs[cpu] != NULL) {\n\t\t\tfree_pages((unsigned long)zfs_kfpu_fpregs[cpu], order);\n\t\t}\n\t}\n\n\tkfree(zfs_kfpu_fpregs);\n}\n\nstatic inline int\nkfpu_init(void)\n{\n\tzfs_kfpu_fpregs = kzalloc(num_possible_cpus() * sizeof (uint8_t *),\n\t    GFP_KERNEL);\n\n\tif (zfs_kfpu_fpregs == NULL)\n\t\treturn (-ENOMEM);\n\n\t \n\tint cpu;\n\tint order = get_fpuregs_save_area_order();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct page *page = alloc_pages_node(cpu_to_node(cpu),\n\t\t    GFP_KERNEL | __GFP_ZERO, order);\n\t\tif (page == NULL) {\n\t\t\tkfpu_fini();\n\t\t\treturn (-ENOMEM);\n\t\t}\n\n\t\tzfs_kfpu_fpregs[cpu] = page_address(page);\n\t}\n\n\treturn (0);\n}\n\n#define\tkfpu_allowed()\t\t1\n\n \n#define\t__asm\t\t\t__asm__ __volatile__\n#define\tkfpu_fxsave(addr)\t__asm(\"fxsave %0\" : \"=m\" (*(addr)))\n#define\tkfpu_fxsaveq(addr)\t__asm(\"fxsaveq %0\" : \"=m\" (*(addr)))\n#define\tkfpu_fnsave(addr)\t__asm(\"fnsave %0; fwait\" : \"=m\" (*(addr)))\n#define\tkfpu_fxrstor(addr)\t__asm(\"fxrstor %0\" : : \"m\" (*(addr)))\n#define\tkfpu_fxrstorq(addr)\t__asm(\"fxrstorq %0\" : : \"m\" (*(addr)))\n#define\tkfpu_frstor(addr)\t__asm(\"frstor %0\" : : \"m\" (*(addr)))\n#define\tkfpu_fxsr_clean(rval)\t__asm(\"fnclex; emms; fildl %P[addr]\" \\\n\t\t\t\t    : : [addr] \"m\" (rval));\n\n#define\tkfpu_do_xsave(instruction, addr, mask)\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tuint32_t low, hi;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tlow = mask;\t\t\t\t\t\t\\\n\thi = (uint64_t)(mask) >> 32;\t\t\t\t\\\n\t__asm(instruction \" %[dst]\\n\\t\"\t\t\t\t\\\n\t    :\t\t\t\t\t\t\t\\\n\t    : [dst] \"m\" (*(addr)), \"a\" (low), \"d\" (hi)\t\t\\\n\t    : \"memory\");\t\t\t\t\t\\\n}\n\nstatic inline void\nkfpu_save_fxsr(uint8_t  *addr)\n{\n\tif (IS_ENABLED(CONFIG_X86_32))\n\t\tkfpu_fxsave(addr);\n\telse\n\t\tkfpu_fxsaveq(addr);\n}\n\nstatic inline void\nkfpu_save_fsave(uint8_t *addr)\n{\n\tkfpu_fnsave(addr);\n}\n\nstatic inline void\nkfpu_begin(void)\n{\n\t \n\tpreempt_disable();\n\tlocal_irq_disable();\n\n\t \n\tuint8_t *state = zfs_kfpu_fpregs[smp_processor_id()];\n#if defined(HAVE_XSAVES)\n\tif (static_cpu_has(X86_FEATURE_XSAVES)) {\n\t\tkfpu_do_xsave(\"xsaves\", state, ~XFEATURE_MASK_XTILE);\n\t\treturn;\n\t}\n#endif\n#if defined(HAVE_XSAVEOPT)\n\tif (static_cpu_has(X86_FEATURE_XSAVEOPT)) {\n\t\tkfpu_do_xsave(\"xsaveopt\", state, ~XFEATURE_MASK_XTILE);\n\t\treturn;\n\t}\n#endif\n\tif (static_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tkfpu_do_xsave(\"xsave\", state, ~XFEATURE_MASK_XTILE);\n\t} else if (static_cpu_has(X86_FEATURE_FXSR)) {\n\t\tkfpu_save_fxsr(state);\n\t} else {\n\t\tkfpu_save_fsave(state);\n\t}\n}\n\n#define\tkfpu_do_xrstor(instruction, addr, mask)\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tuint32_t low, hi;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tlow = mask;\t\t\t\t\t\t\\\n\thi = (uint64_t)(mask) >> 32;\t\t\t\t\\\n\t__asm(instruction \" %[src]\"\t\t\t\t\\\n\t    :\t\t\t\t\t\t\t\\\n\t    : [src] \"m\" (*(addr)), \"a\" (low), \"d\" (hi)\t\t\\\n\t    : \"memory\");\t\t\t\t\t\\\n}\n\nstatic inline void\nkfpu_restore_fxsr(uint8_t *addr)\n{\n\t \n\tif (unlikely(static_cpu_has_bug(X86_BUG_FXSAVE_LEAK)))\n\t\tkfpu_fxsr_clean(addr);\n\n\tif (IS_ENABLED(CONFIG_X86_32)) {\n\t\tkfpu_fxrstor(addr);\n\t} else {\n\t\tkfpu_fxrstorq(addr);\n\t}\n}\n\nstatic inline void\nkfpu_restore_fsave(uint8_t *addr)\n{\n\tkfpu_frstor(addr);\n}\n\nstatic inline void\nkfpu_end(void)\n{\n\tuint8_t  *state = zfs_kfpu_fpregs[smp_processor_id()];\n#if defined(HAVE_XSAVES)\n\tif (static_cpu_has(X86_FEATURE_XSAVES)) {\n\t\tkfpu_do_xrstor(\"xrstors\", state, ~XFEATURE_MASK_XTILE);\n\t\tgoto out;\n\t}\n#endif\n\tif (static_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tkfpu_do_xrstor(\"xrstor\", state, ~XFEATURE_MASK_XTILE);\n\t} else if (static_cpu_has(X86_FEATURE_FXSR)) {\n\t\tkfpu_restore_fxsr(state);\n\t} else {\n\t\tkfpu_restore_fsave(state);\n\t}\nout:\n\tlocal_irq_enable();\n\tpreempt_enable();\n\n}\n\n#else\n\n#error\t\"Exactly one of KERNEL_EXPORTS_X86_FPU or HAVE_KERNEL_FPU_INTERNAL\" \\\n\t\" must be defined\"\n\n#endif  \n#endif  \n\n \n\n \n\n \nstatic inline uint64_t\nzfs_xgetbv(uint32_t index)\n{\n\tuint32_t eax, edx;\n\t \n\t__asm__ __volatile__(\".byte 0x0f; .byte 0x01; .byte 0xd0\"\n\t    : \"=a\" (eax), \"=d\" (edx)\n\t    : \"c\" (index));\n\n\treturn ((((uint64_t)edx)<<32) | (uint64_t)eax);\n}\n\n\nstatic inline boolean_t\n__simd_state_enabled(const uint64_t state)\n{\n\tboolean_t has_osxsave;\n\tuint64_t xcr0;\n\n#if defined(X86_FEATURE_OSXSAVE)\n\thas_osxsave = !!boot_cpu_has(X86_FEATURE_OSXSAVE);\n#else\n\thas_osxsave = B_FALSE;\n#endif\n\tif (!has_osxsave)\n\t\treturn (B_FALSE);\n\n\txcr0 = zfs_xgetbv(0);\n\treturn ((xcr0 & state) == state);\n}\n\n#define\t_XSTATE_SSE_AVX\t\t(0x2 | 0x4)\n#define\t_XSTATE_AVX512\t\t(0xE0 | _XSTATE_SSE_AVX)\n\n#define\t__ymm_enabled() __simd_state_enabled(_XSTATE_SSE_AVX)\n#define\t__zmm_enabled() __simd_state_enabled(_XSTATE_AVX512)\n\n \nstatic inline boolean_t\nzfs_sse_available(void)\n{\n\treturn (!!boot_cpu_has(X86_FEATURE_XMM));\n}\n\n \nstatic inline boolean_t\nzfs_sse2_available(void)\n{\n\treturn (!!boot_cpu_has(X86_FEATURE_XMM2));\n}\n\n \nstatic inline boolean_t\nzfs_sse3_available(void)\n{\n\treturn (!!boot_cpu_has(X86_FEATURE_XMM3));\n}\n\n \nstatic inline boolean_t\nzfs_ssse3_available(void)\n{\n\treturn (!!boot_cpu_has(X86_FEATURE_SSSE3));\n}\n\n \nstatic inline boolean_t\nzfs_sse4_1_available(void)\n{\n\treturn (!!boot_cpu_has(X86_FEATURE_XMM4_1));\n}\n\n \nstatic inline boolean_t\nzfs_sse4_2_available(void)\n{\n\treturn (!!boot_cpu_has(X86_FEATURE_XMM4_2));\n}\n\n \nstatic inline boolean_t\nzfs_avx_available(void)\n{\n\treturn (boot_cpu_has(X86_FEATURE_AVX) && __ymm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx2_available(void)\n{\n\treturn (boot_cpu_has(X86_FEATURE_AVX2) && __ymm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_bmi1_available(void)\n{\n#if defined(X86_FEATURE_BMI1)\n\treturn (!!boot_cpu_has(X86_FEATURE_BMI1));\n#else\n\treturn (B_FALSE);\n#endif\n}\n\n \nstatic inline boolean_t\nzfs_bmi2_available(void)\n{\n#if defined(X86_FEATURE_BMI2)\n\treturn (!!boot_cpu_has(X86_FEATURE_BMI2));\n#else\n\treturn (B_FALSE);\n#endif\n}\n\n \nstatic inline boolean_t\nzfs_aes_available(void)\n{\n#if defined(X86_FEATURE_AES)\n\treturn (!!boot_cpu_has(X86_FEATURE_AES));\n#else\n\treturn (B_FALSE);\n#endif\n}\n\n \nstatic inline boolean_t\nzfs_pclmulqdq_available(void)\n{\n#if defined(X86_FEATURE_PCLMULQDQ)\n\treturn (!!boot_cpu_has(X86_FEATURE_PCLMULQDQ));\n#else\n\treturn (B_FALSE);\n#endif\n}\n\n \nstatic inline boolean_t\nzfs_movbe_available(void)\n{\n#if defined(X86_FEATURE_MOVBE)\n\treturn (!!boot_cpu_has(X86_FEATURE_MOVBE));\n#else\n\treturn (B_FALSE);\n#endif\n}\n\n \nstatic inline boolean_t\nzfs_shani_available(void)\n{\n#if defined(X86_FEATURE_SHA_NI)\n\treturn (!!boot_cpu_has(X86_FEATURE_SHA_NI));\n#else\n\treturn (B_FALSE);\n#endif\n}\n\n \n\n \nstatic inline boolean_t\nzfs_avx512f_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512F)\n\thas_avx512 = !!boot_cpu_has(X86_FEATURE_AVX512F);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512cd_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512CD)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512CD);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512er_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512ER)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512ER);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512pf_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512PF)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512PF);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512bw_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512BW)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512BW);\n#endif\n\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512dq_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512DQ)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512DQ);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512vl_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512VL)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512VL);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512ifma_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512IFMA)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512IFMA);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n \nstatic inline boolean_t\nzfs_avx512vbmi_available(void)\n{\n\tboolean_t has_avx512 = B_FALSE;\n\n#if defined(X86_FEATURE_AVX512VBMI)\n\thas_avx512 = boot_cpu_has(X86_FEATURE_AVX512F) &&\n\t    boot_cpu_has(X86_FEATURE_AVX512VBMI);\n#endif\n\treturn (has_avx512 && __zmm_enabled());\n}\n\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}