{
  "module_name": "blkdev_compat.h",
  "hash_id": "97c6bf0ef152a2c50433aed8baedf1298c30cc82ac0d8db115f0a89bfef1a202",
  "original_prompt": "Ingested from zfs-2.2.2/include/os/linux/kernel/linux/blkdev_compat.h",
  "human_readable_source": " \n\n \n\n#ifndef _ZFS_BLKDEV_H\n#define\t_ZFS_BLKDEV_H\n\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/hdreg.h>\n#include <linux/major.h>\n#include <linux/msdos_fs.h>\t \n#include <linux/bio.h>\n\n#ifdef HAVE_BLK_MQ\n#include <linux/blk-mq.h>\n#endif\n\n#ifndef HAVE_BLK_QUEUE_FLAG_SET\nstatic inline void\nblk_queue_flag_set(unsigned int flag, struct request_queue *q)\n{\n\tqueue_flag_set(flag, q);\n}\n#endif\n\n#ifndef HAVE_BLK_QUEUE_FLAG_CLEAR\nstatic inline void\nblk_queue_flag_clear(unsigned int flag, struct request_queue *q)\n{\n\tqueue_flag_clear(flag, q);\n}\n#endif\n\n \nstatic inline void\nblk_queue_set_write_cache(struct request_queue *q, bool wc, bool fua)\n{\n#if defined(HAVE_BLK_QUEUE_WRITE_CACHE_GPL_ONLY)\n\tif (wc)\n\t\tblk_queue_flag_set(QUEUE_FLAG_WC, q);\n\telse\n\t\tblk_queue_flag_clear(QUEUE_FLAG_WC, q);\n\tif (fua)\n\t\tblk_queue_flag_set(QUEUE_FLAG_FUA, q);\n\telse\n\t\tblk_queue_flag_clear(QUEUE_FLAG_FUA, q);\n#elif defined(HAVE_BLK_QUEUE_WRITE_CACHE)\n\tblk_queue_write_cache(q, wc, fua);\n#elif defined(HAVE_BLK_QUEUE_FLUSH_GPL_ONLY)\n\tif (wc)\n\t\tq->flush_flags |= REQ_FLUSH;\n\tif (fua)\n\t\tq->flush_flags |= REQ_FUA;\n#elif defined(HAVE_BLK_QUEUE_FLUSH)\n\tblk_queue_flush(q, (wc ? REQ_FLUSH : 0) | (fua ? REQ_FUA : 0));\n#else\n#error \"Unsupported kernel\"\n#endif\n}\n\nstatic inline void\nblk_queue_set_read_ahead(struct request_queue *q, unsigned long ra_pages)\n{\n#if !defined(HAVE_BLK_QUEUE_UPDATE_READAHEAD) && \\\n\t!defined(HAVE_DISK_UPDATE_READAHEAD)\n#ifdef HAVE_BLK_QUEUE_BDI_DYNAMIC\n\tq->backing_dev_info->ra_pages = ra_pages;\n#else\n\tq->backing_dev_info.ra_pages = ra_pages;\n#endif\n#endif\n}\n\n#ifdef HAVE_BIO_BVEC_ITER\n#define\tBIO_BI_SECTOR(bio)\t(bio)->bi_iter.bi_sector\n#define\tBIO_BI_SIZE(bio)\t(bio)->bi_iter.bi_size\n#define\tBIO_BI_IDX(bio)\t\t(bio)->bi_iter.bi_idx\n#define\tBIO_BI_SKIP(bio)\t(bio)->bi_iter.bi_bvec_done\n#define\tbio_for_each_segment4(bv, bvp, b, i)\t\\\n\tbio_for_each_segment((bv), (b), (i))\ntypedef struct bvec_iter bvec_iterator_t;\n#else\n#define\tBIO_BI_SECTOR(bio)\t(bio)->bi_sector\n#define\tBIO_BI_SIZE(bio)\t(bio)->bi_size\n#define\tBIO_BI_IDX(bio)\t\t(bio)->bi_idx\n#define\tBIO_BI_SKIP(bio)\t(0)\n#define\tbio_for_each_segment4(bv, bvp, b, i)\t\\\n\tbio_for_each_segment((bvp), (b), (i))\ntypedef int bvec_iterator_t;\n#endif\n\nstatic inline void\nbio_set_flags_failfast(struct block_device *bdev, int *flags, bool dev,\n    bool transport, bool driver)\n{\n#ifdef CONFIG_BUG\n\t \n\tif ((MAJOR(bdev->bd_dev) == LOOP_MAJOR) ||\n\t    (MAJOR(bdev->bd_dev) == MD_MAJOR))\n\t\treturn;\n\n#ifdef BLOCK_EXT_MAJOR\n\tif (MAJOR(bdev->bd_dev) == BLOCK_EXT_MAJOR)\n\t\treturn;\n#endif  \n#endif  \n\n\tif (dev)\n\t\t*flags |= REQ_FAILFAST_DEV;\n\tif (transport)\n\t\t*flags |= REQ_FAILFAST_TRANSPORT;\n\tif (driver)\n\t\t*flags |= REQ_FAILFAST_DRIVER;\n}\n\n \n#if !defined(DISK_NAME_LEN)\n#define\tDISK_NAME_LEN\t32\n#endif  \n\n#ifdef HAVE_BIO_BI_STATUS\nstatic inline int\nbi_status_to_errno(blk_status_t status)\n{\n\tswitch (status)\t{\n\tcase BLK_STS_OK:\n\t\treturn (0);\n\tcase BLK_STS_NOTSUPP:\n\t\treturn (EOPNOTSUPP);\n\tcase BLK_STS_TIMEOUT:\n\t\treturn (ETIMEDOUT);\n\tcase BLK_STS_NOSPC:\n\t\treturn (ENOSPC);\n\tcase BLK_STS_TRANSPORT:\n\t\treturn (ENOLINK);\n\tcase BLK_STS_TARGET:\n\t\treturn (EREMOTEIO);\n#ifdef HAVE_BLK_STS_RESV_CONFLICT\n\tcase BLK_STS_RESV_CONFLICT:\n#else\n\tcase BLK_STS_NEXUS:\n#endif\n\t\treturn (EBADE);\n\tcase BLK_STS_MEDIUM:\n\t\treturn (ENODATA);\n\tcase BLK_STS_PROTECTION:\n\t\treturn (EILSEQ);\n\tcase BLK_STS_RESOURCE:\n\t\treturn (ENOMEM);\n\tcase BLK_STS_AGAIN:\n\t\treturn (EAGAIN);\n\tcase BLK_STS_IOERR:\n\t\treturn (EIO);\n\tdefault:\n\t\treturn (EIO);\n\t}\n}\n\nstatic inline blk_status_t\nerrno_to_bi_status(int error)\n{\n\tswitch (error) {\n\tcase 0:\n\t\treturn (BLK_STS_OK);\n\tcase EOPNOTSUPP:\n\t\treturn (BLK_STS_NOTSUPP);\n\tcase ETIMEDOUT:\n\t\treturn (BLK_STS_TIMEOUT);\n\tcase ENOSPC:\n\t\treturn (BLK_STS_NOSPC);\n\tcase ENOLINK:\n\t\treturn (BLK_STS_TRANSPORT);\n\tcase EREMOTEIO:\n\t\treturn (BLK_STS_TARGET);\n\tcase EBADE:\n#ifdef HAVE_BLK_STS_RESV_CONFLICT\n\t\treturn (BLK_STS_RESV_CONFLICT);\n#else\n\t\treturn (BLK_STS_NEXUS);\n#endif\n\tcase ENODATA:\n\t\treturn (BLK_STS_MEDIUM);\n\tcase EILSEQ:\n\t\treturn (BLK_STS_PROTECTION);\n\tcase ENOMEM:\n\t\treturn (BLK_STS_RESOURCE);\n\tcase EAGAIN:\n\t\treturn (BLK_STS_AGAIN);\n\tcase EIO:\n\t\treturn (BLK_STS_IOERR);\n\tdefault:\n\t\treturn (BLK_STS_IOERR);\n\t}\n}\n#endif  \n\n \n#ifdef HAVE_1ARG_BIO_END_IO_T\n#ifdef HAVE_BIO_BI_STATUS\n#define\tBIO_END_IO_ERROR(bio)\t\tbi_status_to_errno(bio->bi_status)\n#define\tBIO_END_IO_PROTO(fn, x, z)\tstatic void fn(struct bio *x)\n#define\tBIO_END_IO(bio, error)\t\tbio_set_bi_status(bio, error)\nstatic inline void\nbio_set_bi_status(struct bio *bio, int error)\n{\n\tASSERT3S(error, <=, 0);\n\tbio->bi_status = errno_to_bi_status(-error);\n\tbio_endio(bio);\n}\n#else\n#define\tBIO_END_IO_ERROR(bio)\t\t(-(bio->bi_error))\n#define\tBIO_END_IO_PROTO(fn, x, z)\tstatic void fn(struct bio *x)\n#define\tBIO_END_IO(bio, error)\t\tbio_set_bi_error(bio, error)\nstatic inline void\nbio_set_bi_error(struct bio *bio, int error)\n{\n\tASSERT3S(error, <=, 0);\n\tbio->bi_error = error;\n\tbio_endio(bio);\n}\n#endif  \n\n#else\n#define\tBIO_END_IO_PROTO(fn, x, z)\tstatic void fn(struct bio *x, int z)\n#define\tBIO_END_IO(bio, error)\t\tbio_endio(bio, error);\n#endif  \n\n \nstatic inline boolean_t\nzfs_check_disk_status(struct block_device *bdev)\n{\n#if defined(GENHD_FL_UP)\n\treturn (!!(bdev->bd_disk->flags & GENHD_FL_UP));\n#elif defined(GD_DEAD)\n\treturn (!test_bit(GD_DEAD, &bdev->bd_disk->state));\n#else\n \n#error \"Unsupported kernel: no usable disk status check\"\n#endif\n}\n\n \n#ifdef HAVE_CHECK_DISK_CHANGE\n#define\tzfs_check_media_change(bdev)\tcheck_disk_change(bdev)\n#ifdef HAVE_BLKDEV_REREAD_PART\n#define\tvdev_bdev_reread_part(bdev)\tblkdev_reread_part(bdev)\n#else\n#define\tvdev_bdev_reread_part(bdev)\tcheck_disk_change(bdev)\n#endif  \n#else\n#ifdef HAVE_BDEV_CHECK_MEDIA_CHANGE\nstatic inline int\nzfs_check_media_change(struct block_device *bdev)\n{\n#ifdef HAVE_BLOCK_DEVICE_OPERATIONS_REVALIDATE_DISK\n\tstruct gendisk *gd = bdev->bd_disk;\n\tconst struct block_device_operations *bdo = gd->fops;\n#endif\n\n\tif (!bdev_check_media_change(bdev))\n\t\treturn (0);\n\n#ifdef HAVE_BLOCK_DEVICE_OPERATIONS_REVALIDATE_DISK\n\t \n\tif (bdo->revalidate_disk)\n\t\tbdo->revalidate_disk(gd);\n#endif\n\n\treturn (0);\n}\n#define\tvdev_bdev_reread_part(bdev)\tzfs_check_media_change(bdev)\n#elif defined(HAVE_DISK_CHECK_MEDIA_CHANGE)\n#define\tvdev_bdev_reread_part(bdev)\tdisk_check_media_change(bdev->bd_disk)\n#define\tzfs_check_media_change(bdev)\tdisk_check_media_change(bdev->bd_disk)\n#else\n \n#error \"Unsupported kernel: no usable disk change check\"\n#endif  \n#endif  \n\n \nstatic inline int\nvdev_lookup_bdev(const char *path, dev_t *dev)\n{\n#if defined(HAVE_DEVT_LOOKUP_BDEV)\n\treturn (lookup_bdev(path, dev));\n#elif defined(HAVE_1ARG_LOOKUP_BDEV)\n\tstruct block_device *bdev = lookup_bdev(path);\n\tif (IS_ERR(bdev))\n\t\treturn (PTR_ERR(bdev));\n\n\t*dev = bdev->bd_dev;\n\tbdput(bdev);\n\n\treturn (0);\n#elif defined(HAVE_MODE_LOOKUP_BDEV)\n\tstruct block_device *bdev = lookup_bdev(path, FMODE_READ);\n\tif (IS_ERR(bdev))\n\t\treturn (PTR_ERR(bdev));\n\n\t*dev = bdev->bd_dev;\n\tbdput(bdev);\n\n\treturn (0);\n#else\n#error \"Unsupported kernel\"\n#endif\n}\n\n#if defined(HAVE_BLK_MODE_T)\n#define\tblk_mode_is_open_write(flag)\t((flag) & BLK_OPEN_WRITE)\n#else\n#define\tblk_mode_is_open_write(flag)\t((flag) & FMODE_WRITE)\n#endif\n\n \n#if !defined(HAVE_BIO_SET_OP_ATTRS)\nstatic inline void\nbio_set_op_attrs(struct bio *bio, unsigned rw, unsigned flags)\n{\n#if defined(HAVE_BIO_BI_OPF)\n\tbio->bi_opf = rw | flags;\n#else\n\tbio->bi_rw |= rw | flags;\n#endif  \n}\n#endif\n\n \nstatic inline void\nbio_set_flush(struct bio *bio)\n{\n#if defined(HAVE_REQ_PREFLUSH)\t \n\tbio_set_op_attrs(bio, 0, REQ_PREFLUSH | REQ_OP_WRITE);\n#elif defined(WRITE_FLUSH_FUA)\t \n\tbio_set_op_attrs(bio, 0, WRITE_FLUSH_FUA);\n#else\n#error\t\"Allowing the build will cause bio_set_flush requests to be ignored.\"\n#endif\n}\n\n \nstatic inline boolean_t\nbio_is_flush(struct bio *bio)\n{\n#if defined(HAVE_REQ_OP_FLUSH) && defined(HAVE_BIO_BI_OPF)\n\treturn ((bio_op(bio) == REQ_OP_FLUSH) || (bio->bi_opf & REQ_PREFLUSH));\n#elif defined(HAVE_REQ_PREFLUSH) && defined(HAVE_BIO_BI_OPF)\n\treturn (bio->bi_opf & REQ_PREFLUSH);\n#elif defined(HAVE_REQ_PREFLUSH) && !defined(HAVE_BIO_BI_OPF)\n\treturn (bio->bi_rw & REQ_PREFLUSH);\n#elif defined(HAVE_REQ_FLUSH)\n\treturn (bio->bi_rw & REQ_FLUSH);\n#else\n#error\t\"Unsupported kernel\"\n#endif\n}\n\n \nstatic inline boolean_t\nbio_is_fua(struct bio *bio)\n{\n#if defined(HAVE_BIO_BI_OPF)\n\treturn (bio->bi_opf & REQ_FUA);\n#elif defined(REQ_FUA)\n\treturn (bio->bi_rw & REQ_FUA);\n#else\n#error\t\"Allowing the build will cause fua requests to be ignored.\"\n#endif\n}\n\n \nstatic inline boolean_t\nbio_is_discard(struct bio *bio)\n{\n#if defined(HAVE_REQ_OP_DISCARD)\n\treturn (bio_op(bio) == REQ_OP_DISCARD);\n#elif defined(HAVE_REQ_DISCARD)\n\treturn (bio->bi_rw & REQ_DISCARD);\n#else\n#error \"Unsupported kernel\"\n#endif\n}\n\n \nstatic inline boolean_t\nbio_is_secure_erase(struct bio *bio)\n{\n#if defined(HAVE_REQ_OP_SECURE_ERASE)\n\treturn (bio_op(bio) == REQ_OP_SECURE_ERASE);\n#elif defined(REQ_SECURE)\n\treturn (bio->bi_rw & REQ_SECURE);\n#else\n\treturn (0);\n#endif\n}\n\n \nstatic inline void\nblk_queue_discard_granularity(struct request_queue *q, unsigned int dg)\n{\n\tq->limits.discard_granularity = dg;\n}\n\n \nstatic inline boolean_t\nbdev_discard_supported(struct block_device *bdev)\n{\n#if defined(HAVE_BDEV_MAX_DISCARD_SECTORS)\n\treturn (!!bdev_max_discard_sectors(bdev));\n#elif defined(HAVE_BLK_QUEUE_DISCARD)\n\treturn (!!blk_queue_discard(bdev_get_queue(bdev)));\n#else\n#error \"Unsupported kernel\"\n#endif\n}\n\n \nstatic inline boolean_t\nbdev_secure_discard_supported(struct block_device *bdev)\n{\n#if defined(HAVE_BDEV_MAX_SECURE_ERASE_SECTORS)\n\treturn (!!bdev_max_secure_erase_sectors(bdev));\n#elif defined(HAVE_BLK_QUEUE_SECURE_ERASE)\n\treturn (!!blk_queue_secure_erase(bdev_get_queue(bdev)));\n#elif defined(HAVE_BLK_QUEUE_SECDISCARD)\n\treturn (!!blk_queue_secdiscard(bdev_get_queue(bdev)));\n#else\n#error \"Unsupported kernel\"\n#endif\n}\n\n \n#define\tVDEV_HOLDER\t\t\t((void *)0x2401de7)\n\nstatic inline unsigned long\nblk_generic_start_io_acct(struct request_queue *q __attribute__((unused)),\n    struct gendisk *disk __attribute__((unused)),\n    int rw __attribute__((unused)), struct bio *bio)\n{\n#if defined(HAVE_BDEV_IO_ACCT_63)\n\treturn (bdev_start_io_acct(bio->bi_bdev, bio_op(bio),\n\t    jiffies));\n#elif defined(HAVE_BDEV_IO_ACCT_OLD)\n\treturn (bdev_start_io_acct(bio->bi_bdev, bio_sectors(bio),\n\t    bio_op(bio), jiffies));\n#elif defined(HAVE_DISK_IO_ACCT)\n\treturn (disk_start_io_acct(disk, bio_sectors(bio), bio_op(bio)));\n#elif defined(HAVE_BIO_IO_ACCT)\n\treturn (bio_start_io_acct(bio));\n#elif defined(HAVE_GENERIC_IO_ACCT_3ARG)\n\tunsigned long start_time = jiffies;\n\tgeneric_start_io_acct(rw, bio_sectors(bio), &disk->part0);\n\treturn (start_time);\n#elif defined(HAVE_GENERIC_IO_ACCT_4ARG)\n\tunsigned long start_time = jiffies;\n\tgeneric_start_io_acct(q, rw, bio_sectors(bio), &disk->part0);\n\treturn (start_time);\n#else\n\t \n\treturn (0);\n#endif\n}\n\nstatic inline void\nblk_generic_end_io_acct(struct request_queue *q __attribute__((unused)),\n    struct gendisk *disk __attribute__((unused)),\n    int rw __attribute__((unused)), struct bio *bio, unsigned long start_time)\n{\n#if defined(HAVE_BDEV_IO_ACCT_63)\n\tbdev_end_io_acct(bio->bi_bdev, bio_op(bio), bio_sectors(bio),\n\t    start_time);\n#elif defined(HAVE_BDEV_IO_ACCT_OLD)\n\tbdev_end_io_acct(bio->bi_bdev, bio_op(bio), start_time);\n#elif defined(HAVE_DISK_IO_ACCT)\n\tdisk_end_io_acct(disk, bio_op(bio), start_time);\n#elif defined(HAVE_BIO_IO_ACCT)\n\tbio_end_io_acct(bio, start_time);\n#elif defined(HAVE_GENERIC_IO_ACCT_3ARG)\n\tgeneric_end_io_acct(rw, &disk->part0, start_time);\n#elif defined(HAVE_GENERIC_IO_ACCT_4ARG)\n\tgeneric_end_io_acct(q, rw, &disk->part0, start_time);\n#endif\n}\n\n#ifndef HAVE_SUBMIT_BIO_IN_BLOCK_DEVICE_OPERATIONS\nstatic inline struct request_queue *\nblk_generic_alloc_queue(make_request_fn make_request, int node_id)\n{\n#if defined(HAVE_BLK_ALLOC_QUEUE_REQUEST_FN)\n\treturn (blk_alloc_queue(make_request, node_id));\n#elif defined(HAVE_BLK_ALLOC_QUEUE_REQUEST_FN_RH)\n\treturn (blk_alloc_queue_rh(make_request, node_id));\n#else\n\tstruct request_queue *q = blk_alloc_queue(GFP_KERNEL);\n\tif (q != NULL)\n\t\tblk_queue_make_request(q, make_request);\n\n\treturn (q);\n#endif\n}\n#endif  \n\n \nstatic inline int\nio_data_dir(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL) {\n\t\tif (op_is_write(req_op(rq))) {\n\t\t\treturn (WRITE);\n\t\t} else {\n\t\t\treturn (READ);\n\t\t}\n\t}\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (bio_data_dir(bio));\n}\n\nstatic inline int\nio_is_flush(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (req_op(rq) == REQ_OP_FLUSH);\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (bio_is_flush(bio));\n}\n\nstatic inline int\nio_is_discard(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (req_op(rq) == REQ_OP_DISCARD);\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (bio_is_discard(bio));\n}\n\nstatic inline int\nio_is_secure_erase(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (req_op(rq) == REQ_OP_SECURE_ERASE);\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (bio_is_secure_erase(bio));\n}\n\nstatic inline int\nio_is_fua(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (rq->cmd_flags & REQ_FUA);\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (bio_is_fua(bio));\n}\n\n\nstatic inline uint64_t\nio_offset(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (blk_rq_pos(rq) << 9);\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (BIO_BI_SECTOR(bio) << 9);\n}\n\nstatic inline uint64_t\nio_size(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (blk_rq_bytes(rq));\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (BIO_BI_SIZE(bio));\n}\n\nstatic inline int\nio_has_data(struct bio *bio, struct request *rq)\n{\n#ifdef HAVE_BLK_MQ\n\tif (rq != NULL)\n\t\treturn (bio_has_data(rq->bio));\n#else\n\tASSERT3P(rq, ==, NULL);\n#endif\n\treturn (bio_has_data(bio));\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}