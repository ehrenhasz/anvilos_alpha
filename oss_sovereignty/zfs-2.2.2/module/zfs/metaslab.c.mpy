{
  "module_name": "metaslab.c",
  "hash_id": "2c5dcf71defbeec7b0788b424b57e99ca8f13aff966915b44d4177fe4a4661d5",
  "original_prompt": "Ingested from zfs-2.2.2/module/zfs/metaslab.c",
  "human_readable_source": " \n \n\n#include <sys/zfs_context.h>\n#include <sys/dmu.h>\n#include <sys/dmu_tx.h>\n#include <sys/space_map.h>\n#include <sys/metaslab_impl.h>\n#include <sys/vdev_impl.h>\n#include <sys/vdev_draid.h>\n#include <sys/zio.h>\n#include <sys/spa_impl.h>\n#include <sys/zfeature.h>\n#include <sys/vdev_indirect_mapping.h>\n#include <sys/zap.h>\n#include <sys/btree.h>\n\n#define\tWITH_DF_BLOCK_ALLOCATOR\n\n#define\tGANG_ALLOCATION(flags) \\\n\t((flags) & (METASLAB_GANG_CHILD | METASLAB_GANG_HEADER))\n\n \nstatic uint64_t metaslab_aliquot = 1024 * 1024;\n\n \nuint64_t metaslab_force_ganging = SPA_MAXBLOCKSIZE + 1;\n\n \nuint_t metaslab_force_ganging_pct = 3;\n\n \nint zfs_metaslab_sm_blksz_no_log = (1 << 14);\n\n \nint zfs_metaslab_sm_blksz_with_log = (1 << 17);\n\n \nuint_t zfs_condense_pct = 200;\n\n \nstatic const int zfs_metaslab_condense_block_threshold = 4;\n\n \nstatic uint_t zfs_mg_noalloc_threshold = 0;\n\n \nstatic uint_t zfs_mg_fragmentation_threshold = 95;\n\n \nstatic uint_t zfs_metaslab_fragmentation_threshold = 70;\n\n \nint metaslab_debug_load = B_FALSE;\n\n \nstatic int metaslab_debug_unload = B_FALSE;\n\n \nuint64_t metaslab_df_alloc_threshold = SPA_OLD_MAXBLOCKSIZE;\n\n \nuint_t metaslab_df_free_pct = 4;\n\n \nstatic uint_t metaslab_df_max_search = 16 * 1024 * 1024;\n\n \nstatic const uint32_t metaslab_min_search_count = 100;\n\n \nstatic int metaslab_df_use_largest_segment = B_FALSE;\n\n \nstatic uint_t metaslab_unload_delay = 32;\nstatic uint_t metaslab_unload_delay_ms = 10 * 60 * 1000;  \n\n \nuint_t metaslab_preload_limit = 10;\n\n \nstatic int metaslab_preload_enabled = B_TRUE;\n\n \nstatic int metaslab_fragmentation_factor_enabled = B_TRUE;\n\n \nstatic int metaslab_lba_weighting_enabled = B_TRUE;\n\n \nstatic int metaslab_bias_enabled = B_TRUE;\n\n \nstatic const boolean_t zfs_remap_blkptr_enable = B_TRUE;\n\n \nstatic int zfs_metaslab_segment_weight_enabled = B_TRUE;\n\n \nstatic int zfs_metaslab_switch_threshold = 2;\n\n \nstatic const boolean_t metaslab_trace_enabled = B_FALSE;\n\n \nstatic const uint64_t metaslab_trace_max_entries = 5000;\n\n \nstatic const int max_disabled_ms = 3;\n\n \nstatic uint64_t zfs_metaslab_max_size_cache_sec = 1 * 60 * 60;  \n\n \nstatic uint_t zfs_metaslab_mem_limit = 25;\n\n \nstatic const boolean_t zfs_metaslab_force_large_segs = B_FALSE;\n\n \nstatic const uint32_t metaslab_by_size_min_shift = 14;\n\n \nstatic int zfs_metaslab_try_hard_before_gang = B_FALSE;\n\n \nstatic uint_t zfs_metaslab_find_max_tries = 100;\n\nstatic uint64_t metaslab_weight(metaslab_t *, boolean_t);\nstatic void metaslab_set_fragmentation(metaslab_t *, boolean_t);\nstatic void metaslab_free_impl(vdev_t *, uint64_t, uint64_t, boolean_t);\nstatic void metaslab_check_free_impl(vdev_t *, uint64_t, uint64_t);\n\nstatic void metaslab_passivate(metaslab_t *msp, uint64_t weight);\nstatic uint64_t metaslab_weight_from_range_tree(metaslab_t *msp);\nstatic void metaslab_flush_update(metaslab_t *, dmu_tx_t *);\nstatic unsigned int metaslab_idx_func(multilist_t *, void *);\nstatic void metaslab_evict(metaslab_t *, uint64_t);\nstatic void metaslab_rt_add(range_tree_t *rt, range_seg_t *rs, void *arg);\nkmem_cache_t *metaslab_alloc_trace_cache;\n\ntypedef struct metaslab_stats {\n\tkstat_named_t metaslabstat_trace_over_limit;\n\tkstat_named_t metaslabstat_reload_tree;\n\tkstat_named_t metaslabstat_too_many_tries;\n\tkstat_named_t metaslabstat_try_hard;\n} metaslab_stats_t;\n\nstatic metaslab_stats_t metaslab_stats = {\n\t{ \"trace_over_limit\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"reload_tree\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"too_many_tries\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"try_hard\",\t\t\tKSTAT_DATA_UINT64 },\n};\n\n#define\tMETASLABSTAT_BUMP(stat) \\\n\tatomic_inc_64(&metaslab_stats.stat.value.ui64);\n\n\nstatic kstat_t *metaslab_ksp;\n\nvoid\nmetaslab_stat_init(void)\n{\n\tASSERT(metaslab_alloc_trace_cache == NULL);\n\tmetaslab_alloc_trace_cache = kmem_cache_create(\n\t    \"metaslab_alloc_trace_cache\", sizeof (metaslab_alloc_trace_t),\n\t    0, NULL, NULL, NULL, NULL, NULL, 0);\n\tmetaslab_ksp = kstat_create(\"zfs\", 0, \"metaslab_stats\",\n\t    \"misc\", KSTAT_TYPE_NAMED, sizeof (metaslab_stats) /\n\t    sizeof (kstat_named_t), KSTAT_FLAG_VIRTUAL);\n\tif (metaslab_ksp != NULL) {\n\t\tmetaslab_ksp->ks_data = &metaslab_stats;\n\t\tkstat_install(metaslab_ksp);\n\t}\n}\n\nvoid\nmetaslab_stat_fini(void)\n{\n\tif (metaslab_ksp != NULL) {\n\t\tkstat_delete(metaslab_ksp);\n\t\tmetaslab_ksp = NULL;\n\t}\n\n\tkmem_cache_destroy(metaslab_alloc_trace_cache);\n\tmetaslab_alloc_trace_cache = NULL;\n}\n\n \nmetaslab_class_t *\nmetaslab_class_create(spa_t *spa, const metaslab_ops_t *ops)\n{\n\tmetaslab_class_t *mc;\n\n\tmc = kmem_zalloc(offsetof(metaslab_class_t,\n\t    mc_allocator[spa->spa_alloc_count]), KM_SLEEP);\n\n\tmc->mc_spa = spa;\n\tmc->mc_ops = ops;\n\tmutex_init(&mc->mc_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmultilist_create(&mc->mc_metaslab_txg_list, sizeof (metaslab_t),\n\t    offsetof(metaslab_t, ms_class_txg_node), metaslab_idx_func);\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tmetaslab_class_allocator_t *mca = &mc->mc_allocator[i];\n\t\tmca->mca_rotor = NULL;\n\t\tzfs_refcount_create_tracked(&mca->mca_alloc_slots);\n\t}\n\n\treturn (mc);\n}\n\nvoid\nmetaslab_class_destroy(metaslab_class_t *mc)\n{\n\tspa_t *spa = mc->mc_spa;\n\n\tASSERT(mc->mc_alloc == 0);\n\tASSERT(mc->mc_deferred == 0);\n\tASSERT(mc->mc_space == 0);\n\tASSERT(mc->mc_dspace == 0);\n\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tmetaslab_class_allocator_t *mca = &mc->mc_allocator[i];\n\t\tASSERT(mca->mca_rotor == NULL);\n\t\tzfs_refcount_destroy(&mca->mca_alloc_slots);\n\t}\n\tmutex_destroy(&mc->mc_lock);\n\tmultilist_destroy(&mc->mc_metaslab_txg_list);\n\tkmem_free(mc, offsetof(metaslab_class_t,\n\t    mc_allocator[spa->spa_alloc_count]));\n}\n\nint\nmetaslab_class_validate(metaslab_class_t *mc)\n{\n\tmetaslab_group_t *mg;\n\tvdev_t *vd;\n\n\t \n\tASSERT(spa_config_held(mc->mc_spa, SCL_ALL, RW_READER) ||\n\t    spa_config_held(mc->mc_spa, SCL_ALL, RW_WRITER));\n\n\tif ((mg = mc->mc_allocator[0].mca_rotor) == NULL)\n\t\treturn (0);\n\n\tdo {\n\t\tvd = mg->mg_vd;\n\t\tASSERT(vd->vdev_mg != NULL);\n\t\tASSERT3P(vd->vdev_top, ==, vd);\n\t\tASSERT3P(mg->mg_class, ==, mc);\n\t\tASSERT3P(vd->vdev_ops, !=, &vdev_hole_ops);\n\t} while ((mg = mg->mg_next) != mc->mc_allocator[0].mca_rotor);\n\n\treturn (0);\n}\n\nstatic void\nmetaslab_class_space_update(metaslab_class_t *mc, int64_t alloc_delta,\n    int64_t defer_delta, int64_t space_delta, int64_t dspace_delta)\n{\n\tatomic_add_64(&mc->mc_alloc, alloc_delta);\n\tatomic_add_64(&mc->mc_deferred, defer_delta);\n\tatomic_add_64(&mc->mc_space, space_delta);\n\tatomic_add_64(&mc->mc_dspace, dspace_delta);\n}\n\nuint64_t\nmetaslab_class_get_alloc(metaslab_class_t *mc)\n{\n\treturn (mc->mc_alloc);\n}\n\nuint64_t\nmetaslab_class_get_deferred(metaslab_class_t *mc)\n{\n\treturn (mc->mc_deferred);\n}\n\nuint64_t\nmetaslab_class_get_space(metaslab_class_t *mc)\n{\n\treturn (mc->mc_space);\n}\n\nuint64_t\nmetaslab_class_get_dspace(metaslab_class_t *mc)\n{\n\treturn (spa_deflate(mc->mc_spa) ? mc->mc_dspace : mc->mc_space);\n}\n\nvoid\nmetaslab_class_histogram_verify(metaslab_class_t *mc)\n{\n\tspa_t *spa = mc->mc_spa;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tuint64_t *mc_hist;\n\tint i;\n\n\tif ((zfs_flags & ZFS_DEBUG_HISTOGRAM_VERIFY) == 0)\n\t\treturn;\n\n\tmc_hist = kmem_zalloc(sizeof (uint64_t) * RANGE_TREE_HISTOGRAM_SIZE,\n\t    KM_SLEEP);\n\n\tmutex_enter(&mc->mc_lock);\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *tvd = rvd->vdev_child[c];\n\t\tmetaslab_group_t *mg = vdev_get_mg(tvd, mc);\n\n\t\t \n\t\tif (!vdev_is_concrete(tvd) || tvd->vdev_ms_shift == 0 ||\n\t\t    mg->mg_class != mc) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tIMPLY(mg == mg->mg_vd->vdev_log_mg,\n\t\t    mc == spa_embedded_log_class(mg->mg_vd->vdev_spa));\n\n\t\tfor (i = 0; i < RANGE_TREE_HISTOGRAM_SIZE; i++)\n\t\t\tmc_hist[i] += mg->mg_histogram[i];\n\t}\n\n\tfor (i = 0; i < RANGE_TREE_HISTOGRAM_SIZE; i++) {\n\t\tVERIFY3U(mc_hist[i], ==, mc->mc_histogram[i]);\n\t}\n\n\tmutex_exit(&mc->mc_lock);\n\tkmem_free(mc_hist, sizeof (uint64_t) * RANGE_TREE_HISTOGRAM_SIZE);\n}\n\n \nuint64_t\nmetaslab_class_fragmentation(metaslab_class_t *mc)\n{\n\tvdev_t *rvd = mc->mc_spa->spa_root_vdev;\n\tuint64_t fragmentation = 0;\n\n\tspa_config_enter(mc->mc_spa, SCL_VDEV, FTAG, RW_READER);\n\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *tvd = rvd->vdev_child[c];\n\t\tmetaslab_group_t *mg = tvd->vdev_mg;\n\n\t\t \n\t\tif (!vdev_is_concrete(tvd) || tvd->vdev_ms_shift == 0 ||\n\t\t    mg->mg_class != mc) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (mg->mg_fragmentation == ZFS_FRAG_INVALID) {\n\t\t\tspa_config_exit(mc->mc_spa, SCL_VDEV, FTAG);\n\t\t\treturn (ZFS_FRAG_INVALID);\n\t\t}\n\n\t\t \n\t\tfragmentation += mg->mg_fragmentation *\n\t\t    metaslab_group_get_space(mg);\n\t}\n\tfragmentation /= metaslab_class_get_space(mc);\n\n\tASSERT3U(fragmentation, <=, 100);\n\tspa_config_exit(mc->mc_spa, SCL_VDEV, FTAG);\n\treturn (fragmentation);\n}\n\n \nuint64_t\nmetaslab_class_expandable_space(metaslab_class_t *mc)\n{\n\tvdev_t *rvd = mc->mc_spa->spa_root_vdev;\n\tuint64_t space = 0;\n\n\tspa_config_enter(mc->mc_spa, SCL_VDEV, FTAG, RW_READER);\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *tvd = rvd->vdev_child[c];\n\t\tmetaslab_group_t *mg = tvd->vdev_mg;\n\n\t\tif (!vdev_is_concrete(tvd) || tvd->vdev_ms_shift == 0 ||\n\t\t    mg->mg_class != mc) {\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tspace += P2ALIGN(tvd->vdev_max_asize - tvd->vdev_asize,\n\t\t    1ULL << tvd->vdev_ms_shift);\n\t}\n\tspa_config_exit(mc->mc_spa, SCL_VDEV, FTAG);\n\treturn (space);\n}\n\nvoid\nmetaslab_class_evict_old(metaslab_class_t *mc, uint64_t txg)\n{\n\tmultilist_t *ml = &mc->mc_metaslab_txg_list;\n\tfor (int i = 0; i < multilist_get_num_sublists(ml); i++) {\n\t\tmultilist_sublist_t *mls = multilist_sublist_lock(ml, i);\n\t\tmetaslab_t *msp = multilist_sublist_head(mls);\n\t\tmultilist_sublist_unlock(mls);\n\t\twhile (msp != NULL) {\n\t\t\tmutex_enter(&msp->ms_lock);\n\n\t\t\t \n\t\t\tif (!multilist_link_active(&msp->ms_class_txg_node)) {\n\t\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\t\ti--;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmls = multilist_sublist_lock(ml, i);\n\t\t\tmetaslab_t *next_msp = multilist_sublist_next(mls, msp);\n\t\t\tmultilist_sublist_unlock(mls);\n\t\t\tif (txg >\n\t\t\t    msp->ms_selected_txg + metaslab_unload_delay &&\n\t\t\t    gethrtime() > msp->ms_selected_time +\n\t\t\t    (uint64_t)MSEC2NSEC(metaslab_unload_delay_ms)) {\n\t\t\t\tmetaslab_evict(msp, txg);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tmsp = next_msp;\n\t\t}\n\t}\n}\n\nstatic int\nmetaslab_compare(const void *x1, const void *x2)\n{\n\tconst metaslab_t *m1 = (const metaslab_t *)x1;\n\tconst metaslab_t *m2 = (const metaslab_t *)x2;\n\n\tint sort1 = 0;\n\tint sort2 = 0;\n\tif (m1->ms_allocator != -1 && m1->ms_primary)\n\t\tsort1 = 1;\n\telse if (m1->ms_allocator != -1 && !m1->ms_primary)\n\t\tsort1 = 2;\n\tif (m2->ms_allocator != -1 && m2->ms_primary)\n\t\tsort2 = 1;\n\telse if (m2->ms_allocator != -1 && !m2->ms_primary)\n\t\tsort2 = 2;\n\n\t \n\tif (sort1 < sort2)\n\t\treturn (-1);\n\tif (sort1 > sort2)\n\t\treturn (1);\n\n\tint cmp = TREE_CMP(m2->ms_weight, m1->ms_weight);\n\tif (likely(cmp))\n\t\treturn (cmp);\n\n\tIMPLY(TREE_CMP(m1->ms_start, m2->ms_start) == 0, m1 == m2);\n\n\treturn (TREE_CMP(m1->ms_start, m2->ms_start));\n}\n\n \n \nstatic void\nmetaslab_group_alloc_update(metaslab_group_t *mg)\n{\n\tvdev_t *vd = mg->mg_vd;\n\tmetaslab_class_t *mc = mg->mg_class;\n\tvdev_stat_t *vs = &vd->vdev_stat;\n\tboolean_t was_allocatable;\n\tboolean_t was_initialized;\n\n\tASSERT(vd == vd->vdev_top);\n\tASSERT3U(spa_config_held(mc->mc_spa, SCL_ALLOC, RW_READER), ==,\n\t    SCL_ALLOC);\n\n\tmutex_enter(&mg->mg_lock);\n\twas_allocatable = mg->mg_allocatable;\n\twas_initialized = mg->mg_initialized;\n\n\tmg->mg_free_capacity = ((vs->vs_space - vs->vs_alloc) * 100) /\n\t    (vs->vs_space + 1);\n\n\tmutex_enter(&mc->mc_lock);\n\n\t \n\tmg->mg_initialized = metaslab_group_initialized(mg);\n\tif (!was_initialized && mg->mg_initialized) {\n\t\tmc->mc_groups++;\n\t} else if (was_initialized && !mg->mg_initialized) {\n\t\tASSERT3U(mc->mc_groups, >, 0);\n\t\tmc->mc_groups--;\n\t}\n\tif (mg->mg_initialized)\n\t\tmg->mg_no_free_space = B_FALSE;\n\n\t \n\tmg->mg_allocatable = (mg->mg_activation_count > 0 &&\n\t    mg->mg_free_capacity > zfs_mg_noalloc_threshold &&\n\t    (mg->mg_fragmentation == ZFS_FRAG_INVALID ||\n\t    mg->mg_fragmentation <= zfs_mg_fragmentation_threshold));\n\n\t \n\tif (was_allocatable && !mg->mg_allocatable)\n\t\tmc->mc_alloc_groups--;\n\telse if (!was_allocatable && mg->mg_allocatable)\n\t\tmc->mc_alloc_groups++;\n\tmutex_exit(&mc->mc_lock);\n\n\tmutex_exit(&mg->mg_lock);\n}\n\nint\nmetaslab_sort_by_flushed(const void *va, const void *vb)\n{\n\tconst metaslab_t *a = va;\n\tconst metaslab_t *b = vb;\n\n\tint cmp = TREE_CMP(a->ms_unflushed_txg, b->ms_unflushed_txg);\n\tif (likely(cmp))\n\t\treturn (cmp);\n\n\tuint64_t a_vdev_id = a->ms_group->mg_vd->vdev_id;\n\tuint64_t b_vdev_id = b->ms_group->mg_vd->vdev_id;\n\tcmp = TREE_CMP(a_vdev_id, b_vdev_id);\n\tif (cmp)\n\t\treturn (cmp);\n\n\treturn (TREE_CMP(a->ms_id, b->ms_id));\n}\n\nmetaslab_group_t *\nmetaslab_group_create(metaslab_class_t *mc, vdev_t *vd, int allocators)\n{\n\tmetaslab_group_t *mg;\n\n\tmg = kmem_zalloc(offsetof(metaslab_group_t,\n\t    mg_allocator[allocators]), KM_SLEEP);\n\tmutex_init(&mg->mg_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&mg->mg_ms_disabled_lock, NULL, MUTEX_DEFAULT, NULL);\n\tcv_init(&mg->mg_ms_disabled_cv, NULL, CV_DEFAULT, NULL);\n\tavl_create(&mg->mg_metaslab_tree, metaslab_compare,\n\t    sizeof (metaslab_t), offsetof(metaslab_t, ms_group_node));\n\tmg->mg_vd = vd;\n\tmg->mg_class = mc;\n\tmg->mg_activation_count = 0;\n\tmg->mg_initialized = B_FALSE;\n\tmg->mg_no_free_space = B_TRUE;\n\tmg->mg_allocators = allocators;\n\n\tfor (int i = 0; i < allocators; i++) {\n\t\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[i];\n\t\tzfs_refcount_create_tracked(&mga->mga_alloc_queue_depth);\n\t}\n\n\treturn (mg);\n}\n\nvoid\nmetaslab_group_destroy(metaslab_group_t *mg)\n{\n\tASSERT(mg->mg_prev == NULL);\n\tASSERT(mg->mg_next == NULL);\n\t \n\tASSERT(mg->mg_activation_count <= 0);\n\n\tavl_destroy(&mg->mg_metaslab_tree);\n\tmutex_destroy(&mg->mg_lock);\n\tmutex_destroy(&mg->mg_ms_disabled_lock);\n\tcv_destroy(&mg->mg_ms_disabled_cv);\n\n\tfor (int i = 0; i < mg->mg_allocators; i++) {\n\t\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[i];\n\t\tzfs_refcount_destroy(&mga->mga_alloc_queue_depth);\n\t}\n\tkmem_free(mg, offsetof(metaslab_group_t,\n\t    mg_allocator[mg->mg_allocators]));\n}\n\nvoid\nmetaslab_group_activate(metaslab_group_t *mg)\n{\n\tmetaslab_class_t *mc = mg->mg_class;\n\tspa_t *spa = mc->mc_spa;\n\tmetaslab_group_t *mgprev, *mgnext;\n\n\tASSERT3U(spa_config_held(spa, SCL_ALLOC, RW_WRITER), !=, 0);\n\n\tASSERT(mg->mg_prev == NULL);\n\tASSERT(mg->mg_next == NULL);\n\tASSERT(mg->mg_activation_count <= 0);\n\n\tif (++mg->mg_activation_count <= 0)\n\t\treturn;\n\n\tmg->mg_aliquot = metaslab_aliquot * MAX(1,\n\t    vdev_get_ndisks(mg->mg_vd) - vdev_get_nparity(mg->mg_vd));\n\tmetaslab_group_alloc_update(mg);\n\n\tif ((mgprev = mc->mc_allocator[0].mca_rotor) == NULL) {\n\t\tmg->mg_prev = mg;\n\t\tmg->mg_next = mg;\n\t} else {\n\t\tmgnext = mgprev->mg_next;\n\t\tmg->mg_prev = mgprev;\n\t\tmg->mg_next = mgnext;\n\t\tmgprev->mg_next = mg;\n\t\tmgnext->mg_prev = mg;\n\t}\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tmc->mc_allocator[i].mca_rotor = mg;\n\t\tmg = mg->mg_next;\n\t}\n}\n\n \nvoid\nmetaslab_group_passivate(metaslab_group_t *mg)\n{\n\tmetaslab_class_t *mc = mg->mg_class;\n\tspa_t *spa = mc->mc_spa;\n\tmetaslab_group_t *mgprev, *mgnext;\n\tint locks = spa_config_held(spa, SCL_ALL, RW_WRITER);\n\n\tASSERT3U(spa_config_held(spa, SCL_ALLOC | SCL_ZIO, RW_WRITER), ==,\n\t    (SCL_ALLOC | SCL_ZIO));\n\n\tif (--mg->mg_activation_count != 0) {\n\t\tfor (int i = 0; i < spa->spa_alloc_count; i++)\n\t\t\tASSERT(mc->mc_allocator[i].mca_rotor != mg);\n\t\tASSERT(mg->mg_prev == NULL);\n\t\tASSERT(mg->mg_next == NULL);\n\t\tASSERT(mg->mg_activation_count < 0);\n\t\treturn;\n\t}\n\n\t \n\tspa_config_exit(spa, locks & ~(SCL_ZIO - 1), spa);\n\ttaskq_wait_outstanding(spa->spa_metaslab_taskq, 0);\n\tspa_config_enter(spa, locks & ~(SCL_ZIO - 1), spa, RW_WRITER);\n\tmetaslab_group_alloc_update(mg);\n\tfor (int i = 0; i < mg->mg_allocators; i++) {\n\t\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[i];\n\t\tmetaslab_t *msp = mga->mga_primary;\n\t\tif (msp != NULL) {\n\t\t\tmutex_enter(&msp->ms_lock);\n\t\t\tmetaslab_passivate(msp,\n\t\t\t    metaslab_weight_from_range_tree(msp));\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t}\n\t\tmsp = mga->mga_secondary;\n\t\tif (msp != NULL) {\n\t\t\tmutex_enter(&msp->ms_lock);\n\t\t\tmetaslab_passivate(msp,\n\t\t\t    metaslab_weight_from_range_tree(msp));\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t}\n\t}\n\n\tmgprev = mg->mg_prev;\n\tmgnext = mg->mg_next;\n\n\tif (mg == mgnext) {\n\t\tmgnext = NULL;\n\t} else {\n\t\tmgprev->mg_next = mgnext;\n\t\tmgnext->mg_prev = mgprev;\n\t}\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tif (mc->mc_allocator[i].mca_rotor == mg)\n\t\t\tmc->mc_allocator[i].mca_rotor = mgnext;\n\t}\n\n\tmg->mg_prev = NULL;\n\tmg->mg_next = NULL;\n}\n\nboolean_t\nmetaslab_group_initialized(metaslab_group_t *mg)\n{\n\tvdev_t *vd = mg->mg_vd;\n\tvdev_stat_t *vs = &vd->vdev_stat;\n\n\treturn (vs->vs_space != 0 && mg->mg_activation_count > 0);\n}\n\nuint64_t\nmetaslab_group_get_space(metaslab_group_t *mg)\n{\n\t \n\tmutex_enter(&mg->mg_lock);\n\tuint64_t ms_count = avl_numnodes(&mg->mg_metaslab_tree);\n\tmutex_exit(&mg->mg_lock);\n\treturn ((1ULL << mg->mg_vd->vdev_ms_shift) * ms_count);\n}\n\nvoid\nmetaslab_group_histogram_verify(metaslab_group_t *mg)\n{\n\tuint64_t *mg_hist;\n\tavl_tree_t *t = &mg->mg_metaslab_tree;\n\tuint64_t ashift = mg->mg_vd->vdev_ashift;\n\n\tif ((zfs_flags & ZFS_DEBUG_HISTOGRAM_VERIFY) == 0)\n\t\treturn;\n\n\tmg_hist = kmem_zalloc(sizeof (uint64_t) * RANGE_TREE_HISTOGRAM_SIZE,\n\t    KM_SLEEP);\n\n\tASSERT3U(RANGE_TREE_HISTOGRAM_SIZE, >=,\n\t    SPACE_MAP_HISTOGRAM_SIZE + ashift);\n\n\tmutex_enter(&mg->mg_lock);\n\tfor (metaslab_t *msp = avl_first(t);\n\t    msp != NULL; msp = AVL_NEXT(t, msp)) {\n\t\tVERIFY3P(msp->ms_group, ==, mg);\n\t\t \n\t\tif (msp->ms_sm == NULL)\n\t\t\tcontinue;\n\n\t\tfor (int i = 0; i < SPACE_MAP_HISTOGRAM_SIZE; i++) {\n\t\t\tmg_hist[i + ashift] +=\n\t\t\t    msp->ms_sm->sm_phys->smp_histogram[i];\n\t\t}\n\t}\n\n\tfor (int i = 0; i < RANGE_TREE_HISTOGRAM_SIZE; i ++)\n\t\tVERIFY3U(mg_hist[i], ==, mg->mg_histogram[i]);\n\n\tmutex_exit(&mg->mg_lock);\n\n\tkmem_free(mg_hist, sizeof (uint64_t) * RANGE_TREE_HISTOGRAM_SIZE);\n}\n\nstatic void\nmetaslab_group_histogram_add(metaslab_group_t *mg, metaslab_t *msp)\n{\n\tmetaslab_class_t *mc = mg->mg_class;\n\tuint64_t ashift = mg->mg_vd->vdev_ashift;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tif (msp->ms_sm == NULL)\n\t\treturn;\n\n\tmutex_enter(&mg->mg_lock);\n\tmutex_enter(&mc->mc_lock);\n\tfor (int i = 0; i < SPACE_MAP_HISTOGRAM_SIZE; i++) {\n\t\tIMPLY(mg == mg->mg_vd->vdev_log_mg,\n\t\t    mc == spa_embedded_log_class(mg->mg_vd->vdev_spa));\n\t\tmg->mg_histogram[i + ashift] +=\n\t\t    msp->ms_sm->sm_phys->smp_histogram[i];\n\t\tmc->mc_histogram[i + ashift] +=\n\t\t    msp->ms_sm->sm_phys->smp_histogram[i];\n\t}\n\tmutex_exit(&mc->mc_lock);\n\tmutex_exit(&mg->mg_lock);\n}\n\nvoid\nmetaslab_group_histogram_remove(metaslab_group_t *mg, metaslab_t *msp)\n{\n\tmetaslab_class_t *mc = mg->mg_class;\n\tuint64_t ashift = mg->mg_vd->vdev_ashift;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tif (msp->ms_sm == NULL)\n\t\treturn;\n\n\tmutex_enter(&mg->mg_lock);\n\tmutex_enter(&mc->mc_lock);\n\tfor (int i = 0; i < SPACE_MAP_HISTOGRAM_SIZE; i++) {\n\t\tASSERT3U(mg->mg_histogram[i + ashift], >=,\n\t\t    msp->ms_sm->sm_phys->smp_histogram[i]);\n\t\tASSERT3U(mc->mc_histogram[i + ashift], >=,\n\t\t    msp->ms_sm->sm_phys->smp_histogram[i]);\n\t\tIMPLY(mg == mg->mg_vd->vdev_log_mg,\n\t\t    mc == spa_embedded_log_class(mg->mg_vd->vdev_spa));\n\n\t\tmg->mg_histogram[i + ashift] -=\n\t\t    msp->ms_sm->sm_phys->smp_histogram[i];\n\t\tmc->mc_histogram[i + ashift] -=\n\t\t    msp->ms_sm->sm_phys->smp_histogram[i];\n\t}\n\tmutex_exit(&mc->mc_lock);\n\tmutex_exit(&mg->mg_lock);\n}\n\nstatic void\nmetaslab_group_add(metaslab_group_t *mg, metaslab_t *msp)\n{\n\tASSERT(msp->ms_group == NULL);\n\tmutex_enter(&mg->mg_lock);\n\tmsp->ms_group = mg;\n\tmsp->ms_weight = 0;\n\tavl_add(&mg->mg_metaslab_tree, msp);\n\tmutex_exit(&mg->mg_lock);\n\n\tmutex_enter(&msp->ms_lock);\n\tmetaslab_group_histogram_add(mg, msp);\n\tmutex_exit(&msp->ms_lock);\n}\n\nstatic void\nmetaslab_group_remove(metaslab_group_t *mg, metaslab_t *msp)\n{\n\tmutex_enter(&msp->ms_lock);\n\tmetaslab_group_histogram_remove(mg, msp);\n\tmutex_exit(&msp->ms_lock);\n\n\tmutex_enter(&mg->mg_lock);\n\tASSERT(msp->ms_group == mg);\n\tavl_remove(&mg->mg_metaslab_tree, msp);\n\n\tmetaslab_class_t *mc = msp->ms_group->mg_class;\n\tmultilist_sublist_t *mls =\n\t    multilist_sublist_lock_obj(&mc->mc_metaslab_txg_list, msp);\n\tif (multilist_link_active(&msp->ms_class_txg_node))\n\t\tmultilist_sublist_remove(mls, msp);\n\tmultilist_sublist_unlock(mls);\n\n\tmsp->ms_group = NULL;\n\tmutex_exit(&mg->mg_lock);\n}\n\nstatic void\nmetaslab_group_sort_impl(metaslab_group_t *mg, metaslab_t *msp, uint64_t weight)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT(MUTEX_HELD(&mg->mg_lock));\n\tASSERT(msp->ms_group == mg);\n\n\tavl_remove(&mg->mg_metaslab_tree, msp);\n\tmsp->ms_weight = weight;\n\tavl_add(&mg->mg_metaslab_tree, msp);\n\n}\n\nstatic void\nmetaslab_group_sort(metaslab_group_t *mg, metaslab_t *msp, uint64_t weight)\n{\n\t \n\tASSERT(weight >= SPA_MINBLOCKSIZE || weight == 0);\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tmutex_enter(&mg->mg_lock);\n\tmetaslab_group_sort_impl(mg, msp, weight);\n\tmutex_exit(&mg->mg_lock);\n}\n\n \nuint64_t\nmetaslab_group_fragmentation(metaslab_group_t *mg)\n{\n\tvdev_t *vd = mg->mg_vd;\n\tuint64_t fragmentation = 0;\n\tuint64_t valid_ms = 0;\n\n\tfor (int m = 0; m < vd->vdev_ms_count; m++) {\n\t\tmetaslab_t *msp = vd->vdev_ms[m];\n\n\t\tif (msp->ms_fragmentation == ZFS_FRAG_INVALID)\n\t\t\tcontinue;\n\t\tif (msp->ms_group != mg)\n\t\t\tcontinue;\n\n\t\tvalid_ms++;\n\t\tfragmentation += msp->ms_fragmentation;\n\t}\n\n\tif (valid_ms <= mg->mg_vd->vdev_ms_count / 2)\n\t\treturn (ZFS_FRAG_INVALID);\n\n\tfragmentation /= valid_ms;\n\tASSERT3U(fragmentation, <=, 100);\n\treturn (fragmentation);\n}\n\n \nstatic boolean_t\nmetaslab_group_allocatable(metaslab_group_t *mg, metaslab_group_t *rotor,\n    int flags, uint64_t psize, int allocator, int d)\n{\n\tspa_t *spa = mg->mg_vd->vdev_spa;\n\tmetaslab_class_t *mc = mg->mg_class;\n\n\t \n\tif ((mc != spa_normal_class(spa) &&\n\t    mc != spa_special_class(spa) &&\n\t    mc != spa_dedup_class(spa)) ||\n\t    mc->mc_groups <= 1)\n\t\treturn (B_TRUE);\n\n\t \n\tif (mg->mg_allocatable) {\n\t\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\t\tint64_t qdepth;\n\t\tuint64_t qmax = mga->mga_cur_max_alloc_queue_depth;\n\n\t\tif (!mc->mc_alloc_throttle_enabled)\n\t\t\treturn (B_TRUE);\n\n\t\t \n\t\tif (mg->mg_no_free_space)\n\t\t\treturn (B_FALSE);\n\n\t\t \n\t\tif (flags & METASLAB_DONT_THROTTLE)\n\t\t\treturn (B_TRUE);\n\n\t\t \n\t\tqmax = qmax * (4 + d) / 4;\n\n\t\tqdepth = zfs_refcount_count(&mga->mga_alloc_queue_depth);\n\n\t\t \n\t\tif (qdepth < qmax || mc->mc_alloc_groups == 1)\n\t\t\treturn (B_TRUE);\n\t\tASSERT3U(mc->mc_alloc_groups, >, 1);\n\n\t\t \n\t\tfor (metaslab_group_t *mgp = mg->mg_next;\n\t\t    mgp != rotor; mgp = mgp->mg_next) {\n\t\t\tmetaslab_group_allocator_t *mgap =\n\t\t\t    &mgp->mg_allocator[allocator];\n\t\t\tqmax = mgap->mga_cur_max_alloc_queue_depth;\n\t\t\tqmax = qmax * (4 + d) / 4;\n\t\t\tqdepth =\n\t\t\t    zfs_refcount_count(&mgap->mga_alloc_queue_depth);\n\n\t\t\t \n\t\t\tif (qdepth < qmax && !mgp->mg_no_free_space)\n\t\t\t\treturn (B_FALSE);\n\t\t}\n\n\t\t \n\t\treturn (B_TRUE);\n\n\t} else if (mc->mc_alloc_groups == 0 || psize == SPA_MINBLOCKSIZE) {\n\t\treturn (B_TRUE);\n\t}\n\treturn (B_FALSE);\n}\n\n \n\n \n__attribute__((always_inline)) inline\nstatic int\nmetaslab_rangesize32_compare(const void *x1, const void *x2)\n{\n\tconst range_seg32_t *r1 = x1;\n\tconst range_seg32_t *r2 = x2;\n\n\tuint64_t rs_size1 = r1->rs_end - r1->rs_start;\n\tuint64_t rs_size2 = r2->rs_end - r2->rs_start;\n\n\tint cmp = TREE_CMP(rs_size1, rs_size2);\n\n\treturn (cmp + !cmp * TREE_CMP(r1->rs_start, r2->rs_start));\n}\n\n \n__attribute__((always_inline)) inline\nstatic int\nmetaslab_rangesize64_compare(const void *x1, const void *x2)\n{\n\tconst range_seg64_t *r1 = x1;\n\tconst range_seg64_t *r2 = x2;\n\n\tuint64_t rs_size1 = r1->rs_end - r1->rs_start;\n\tuint64_t rs_size2 = r2->rs_end - r2->rs_start;\n\n\tint cmp = TREE_CMP(rs_size1, rs_size2);\n\n\treturn (cmp + !cmp * TREE_CMP(r1->rs_start, r2->rs_start));\n}\n\ntypedef struct metaslab_rt_arg {\n\tzfs_btree_t *mra_bt;\n\tuint32_t mra_floor_shift;\n} metaslab_rt_arg_t;\n\nstruct mssa_arg {\n\trange_tree_t *rt;\n\tmetaslab_rt_arg_t *mra;\n};\n\nstatic void\nmetaslab_size_sorted_add(void *arg, uint64_t start, uint64_t size)\n{\n\tstruct mssa_arg *mssap = arg;\n\trange_tree_t *rt = mssap->rt;\n\tmetaslab_rt_arg_t *mrap = mssap->mra;\n\trange_seg_max_t seg = {0};\n\trs_set_start(&seg, rt, start);\n\trs_set_end(&seg, rt, start + size);\n\tmetaslab_rt_add(rt, &seg, mrap);\n}\n\nstatic void\nmetaslab_size_tree_full_load(range_tree_t *rt)\n{\n\tmetaslab_rt_arg_t *mrap = rt->rt_arg;\n\tMETASLABSTAT_BUMP(metaslabstat_reload_tree);\n\tASSERT0(zfs_btree_numnodes(mrap->mra_bt));\n\tmrap->mra_floor_shift = 0;\n\tstruct mssa_arg arg = {0};\n\targ.rt = rt;\n\targ.mra = mrap;\n\trange_tree_walk(rt, metaslab_size_sorted_add, &arg);\n}\n\n\nZFS_BTREE_FIND_IN_BUF_FUNC(metaslab_rt_find_rangesize32_in_buf,\n    range_seg32_t, metaslab_rangesize32_compare)\n\nZFS_BTREE_FIND_IN_BUF_FUNC(metaslab_rt_find_rangesize64_in_buf,\n    range_seg64_t, metaslab_rangesize64_compare)\n\n \nstatic void\nmetaslab_rt_create(range_tree_t *rt, void *arg)\n{\n\tmetaslab_rt_arg_t *mrap = arg;\n\tzfs_btree_t *size_tree = mrap->mra_bt;\n\n\tsize_t size;\n\tint (*compare) (const void *, const void *);\n\tbt_find_in_buf_f bt_find;\n\tswitch (rt->rt_type) {\n\tcase RANGE_SEG32:\n\t\tsize = sizeof (range_seg32_t);\n\t\tcompare = metaslab_rangesize32_compare;\n\t\tbt_find = metaslab_rt_find_rangesize32_in_buf;\n\t\tbreak;\n\tcase RANGE_SEG64:\n\t\tsize = sizeof (range_seg64_t);\n\t\tcompare = metaslab_rangesize64_compare;\n\t\tbt_find = metaslab_rt_find_rangesize64_in_buf;\n\t\tbreak;\n\tdefault:\n\t\tpanic(\"Invalid range seg type %d\", rt->rt_type);\n\t}\n\tzfs_btree_create(size_tree, compare, bt_find, size);\n\tmrap->mra_floor_shift = metaslab_by_size_min_shift;\n}\n\nstatic void\nmetaslab_rt_destroy(range_tree_t *rt, void *arg)\n{\n\t(void) rt;\n\tmetaslab_rt_arg_t *mrap = arg;\n\tzfs_btree_t *size_tree = mrap->mra_bt;\n\n\tzfs_btree_destroy(size_tree);\n\tkmem_free(mrap, sizeof (*mrap));\n}\n\nstatic void\nmetaslab_rt_add(range_tree_t *rt, range_seg_t *rs, void *arg)\n{\n\tmetaslab_rt_arg_t *mrap = arg;\n\tzfs_btree_t *size_tree = mrap->mra_bt;\n\n\tif (rs_get_end(rs, rt) - rs_get_start(rs, rt) <\n\t    (1ULL << mrap->mra_floor_shift))\n\t\treturn;\n\n\tzfs_btree_add(size_tree, rs);\n}\n\nstatic void\nmetaslab_rt_remove(range_tree_t *rt, range_seg_t *rs, void *arg)\n{\n\tmetaslab_rt_arg_t *mrap = arg;\n\tzfs_btree_t *size_tree = mrap->mra_bt;\n\n\tif (rs_get_end(rs, rt) - rs_get_start(rs, rt) < (1ULL <<\n\t    mrap->mra_floor_shift))\n\t\treturn;\n\n\tzfs_btree_remove(size_tree, rs);\n}\n\nstatic void\nmetaslab_rt_vacate(range_tree_t *rt, void *arg)\n{\n\tmetaslab_rt_arg_t *mrap = arg;\n\tzfs_btree_t *size_tree = mrap->mra_bt;\n\tzfs_btree_clear(size_tree);\n\tzfs_btree_destroy(size_tree);\n\n\tmetaslab_rt_create(rt, arg);\n}\n\nstatic const range_tree_ops_t metaslab_rt_ops = {\n\t.rtop_create = metaslab_rt_create,\n\t.rtop_destroy = metaslab_rt_destroy,\n\t.rtop_add = metaslab_rt_add,\n\t.rtop_remove = metaslab_rt_remove,\n\t.rtop_vacate = metaslab_rt_vacate\n};\n\n \n\n \nuint64_t\nmetaslab_largest_allocatable(metaslab_t *msp)\n{\n\tzfs_btree_t *t = &msp->ms_allocatable_by_size;\n\trange_seg_t *rs;\n\n\tif (t == NULL)\n\t\treturn (0);\n\tif (zfs_btree_numnodes(t) == 0)\n\t\tmetaslab_size_tree_full_load(msp->ms_allocatable);\n\n\trs = zfs_btree_last(t, NULL);\n\tif (rs == NULL)\n\t\treturn (0);\n\n\treturn (rs_get_end(rs, msp->ms_allocatable) - rs_get_start(rs,\n\t    msp->ms_allocatable));\n}\n\n \nstatic uint64_t\nmetaslab_largest_unflushed_free(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tif (msp->ms_unflushed_frees == NULL)\n\t\treturn (0);\n\n\tif (zfs_btree_numnodes(&msp->ms_unflushed_frees_by_size) == 0)\n\t\tmetaslab_size_tree_full_load(msp->ms_unflushed_frees);\n\trange_seg_t *rs = zfs_btree_last(&msp->ms_unflushed_frees_by_size,\n\t    NULL);\n\tif (rs == NULL)\n\t\treturn (0);\n\n\t \n\tuint64_t rstart = rs_get_start(rs, msp->ms_unflushed_frees);\n\tuint64_t rsize = rs_get_end(rs, msp->ms_unflushed_frees) - rstart;\n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\tuint64_t start = 0;\n\t\tuint64_t size = 0;\n\t\tboolean_t found = range_tree_find_in(msp->ms_defer[t], rstart,\n\t\t    rsize, &start, &size);\n\t\tif (found) {\n\t\t\tif (rstart == start)\n\t\t\t\treturn (0);\n\t\t\trsize = start - rstart;\n\t\t}\n\t}\n\n\tuint64_t start = 0;\n\tuint64_t size = 0;\n\tboolean_t found = range_tree_find_in(msp->ms_freed, rstart,\n\t    rsize, &start, &size);\n\tif (found)\n\t\trsize = start - rstart;\n\n\treturn (rsize);\n}\n\nstatic range_seg_t *\nmetaslab_block_find(zfs_btree_t *t, range_tree_t *rt, uint64_t start,\n    uint64_t size, zfs_btree_index_t *where)\n{\n\trange_seg_t *rs;\n\trange_seg_max_t rsearch;\n\n\trs_set_start(&rsearch, rt, start);\n\trs_set_end(&rsearch, rt, start + size);\n\n\trs = zfs_btree_find(t, &rsearch, where);\n\tif (rs == NULL) {\n\t\trs = zfs_btree_next(t, where, where);\n\t}\n\n\treturn (rs);\n}\n\n#if defined(WITH_DF_BLOCK_ALLOCATOR) || \\\n    defined(WITH_CF_BLOCK_ALLOCATOR)\n\n \nstatic uint64_t\nmetaslab_block_picker(range_tree_t *rt, uint64_t *cursor, uint64_t size,\n    uint64_t max_search)\n{\n\tif (*cursor == 0)\n\t\t*cursor = rt->rt_start;\n\tzfs_btree_t *bt = &rt->rt_root;\n\tzfs_btree_index_t where;\n\trange_seg_t *rs = metaslab_block_find(bt, rt, *cursor, size, &where);\n\tuint64_t first_found;\n\tint count_searched = 0;\n\n\tif (rs != NULL)\n\t\tfirst_found = rs_get_start(rs, rt);\n\n\twhile (rs != NULL && (rs_get_start(rs, rt) - first_found <=\n\t    max_search || count_searched < metaslab_min_search_count)) {\n\t\tuint64_t offset = rs_get_start(rs, rt);\n\t\tif (offset + size <= rs_get_end(rs, rt)) {\n\t\t\t*cursor = offset + size;\n\t\t\treturn (offset);\n\t\t}\n\t\trs = zfs_btree_next(bt, &where, &where);\n\t\tcount_searched++;\n\t}\n\n\t*cursor = 0;\n\treturn (-1ULL);\n}\n#endif  \n\n#if defined(WITH_DF_BLOCK_ALLOCATOR)\n \nstatic uint64_t\nmetaslab_df_alloc(metaslab_t *msp, uint64_t size)\n{\n\t \n\tuint64_t align = size & -size;\n\tuint64_t *cursor = &msp->ms_lbas[highbit64(align) - 1];\n\trange_tree_t *rt = msp->ms_allocatable;\n\tuint_t free_pct = range_tree_space(rt) * 100 / msp->ms_size;\n\tuint64_t offset;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tif (metaslab_largest_allocatable(msp) < metaslab_df_alloc_threshold ||\n\t    free_pct < metaslab_df_free_pct) {\n\t\toffset = -1;\n\t} else {\n\t\toffset = metaslab_block_picker(rt,\n\t\t    cursor, size, metaslab_df_max_search);\n\t}\n\n\tif (offset == -1) {\n\t\trange_seg_t *rs;\n\t\tif (zfs_btree_numnodes(&msp->ms_allocatable_by_size) == 0)\n\t\t\tmetaslab_size_tree_full_load(msp->ms_allocatable);\n\n\t\tif (metaslab_df_use_largest_segment) {\n\t\t\t \n\t\t\trs = zfs_btree_last(&msp->ms_allocatable_by_size, NULL);\n\t\t} else {\n\t\t\tzfs_btree_index_t where;\n\t\t\t \n\t\t\trs = metaslab_block_find(&msp->ms_allocatable_by_size,\n\t\t\t    rt, msp->ms_start, size, &where);\n\t\t}\n\t\tif (rs != NULL && rs_get_start(rs, rt) + size <= rs_get_end(rs,\n\t\t    rt)) {\n\t\t\toffset = rs_get_start(rs, rt);\n\t\t\t*cursor = offset + size;\n\t\t}\n\t}\n\n\treturn (offset);\n}\n\nconst metaslab_ops_t zfs_metaslab_ops = {\n\tmetaslab_df_alloc\n};\n#endif  \n\n#if defined(WITH_CF_BLOCK_ALLOCATOR)\n \nstatic uint64_t\nmetaslab_cf_alloc(metaslab_t *msp, uint64_t size)\n{\n\trange_tree_t *rt = msp->ms_allocatable;\n\tzfs_btree_t *t = &msp->ms_allocatable_by_size;\n\tuint64_t *cursor = &msp->ms_lbas[0];\n\tuint64_t *cursor_end = &msp->ms_lbas[1];\n\tuint64_t offset = 0;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tASSERT3U(*cursor_end, >=, *cursor);\n\n\tif ((*cursor + size) > *cursor_end) {\n\t\trange_seg_t *rs;\n\n\t\tif (zfs_btree_numnodes(t) == 0)\n\t\t\tmetaslab_size_tree_full_load(msp->ms_allocatable);\n\t\trs = zfs_btree_last(t, NULL);\n\t\tif (rs == NULL || (rs_get_end(rs, rt) - rs_get_start(rs, rt)) <\n\t\t    size)\n\t\t\treturn (-1ULL);\n\n\t\t*cursor = rs_get_start(rs, rt);\n\t\t*cursor_end = rs_get_end(rs, rt);\n\t}\n\n\toffset = *cursor;\n\t*cursor += size;\n\n\treturn (offset);\n}\n\nconst metaslab_ops_t zfs_metaslab_ops = {\n\tmetaslab_cf_alloc\n};\n#endif  \n\n#if defined(WITH_NDF_BLOCK_ALLOCATOR)\n \n\n \nuint64_t metaslab_ndf_clump_shift = 4;\n\nstatic uint64_t\nmetaslab_ndf_alloc(metaslab_t *msp, uint64_t size)\n{\n\tzfs_btree_t *t = &msp->ms_allocatable->rt_root;\n\trange_tree_t *rt = msp->ms_allocatable;\n\tzfs_btree_index_t where;\n\trange_seg_t *rs;\n\trange_seg_max_t rsearch;\n\tuint64_t hbit = highbit64(size);\n\tuint64_t *cursor = &msp->ms_lbas[hbit - 1];\n\tuint64_t max_size = metaslab_largest_allocatable(msp);\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tif (max_size < size)\n\t\treturn (-1ULL);\n\n\trs_set_start(&rsearch, rt, *cursor);\n\trs_set_end(&rsearch, rt, *cursor + size);\n\n\trs = zfs_btree_find(t, &rsearch, &where);\n\tif (rs == NULL || (rs_get_end(rs, rt) - rs_get_start(rs, rt)) < size) {\n\t\tt = &msp->ms_allocatable_by_size;\n\n\t\trs_set_start(&rsearch, rt, 0);\n\t\trs_set_end(&rsearch, rt, MIN(max_size, 1ULL << (hbit +\n\t\t    metaslab_ndf_clump_shift)));\n\n\t\trs = zfs_btree_find(t, &rsearch, &where);\n\t\tif (rs == NULL)\n\t\t\trs = zfs_btree_next(t, &where, &where);\n\t\tASSERT(rs != NULL);\n\t}\n\n\tif ((rs_get_end(rs, rt) - rs_get_start(rs, rt)) >= size) {\n\t\t*cursor = rs_get_start(rs, rt) + size;\n\t\treturn (rs_get_start(rs, rt));\n\t}\n\treturn (-1ULL);\n}\n\nconst metaslab_ops_t zfs_metaslab_ops = {\n\tmetaslab_ndf_alloc\n};\n#endif  \n\n\n \n\n \nstatic void\nmetaslab_load_wait(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\twhile (msp->ms_loading) {\n\t\tASSERT(!msp->ms_loaded);\n\t\tcv_wait(&msp->ms_load_cv, &msp->ms_lock);\n\t}\n}\n\n \nstatic void\nmetaslab_flush_wait(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\twhile (msp->ms_flushing)\n\t\tcv_wait(&msp->ms_flush_cv, &msp->ms_lock);\n}\n\nstatic unsigned int\nmetaslab_idx_func(multilist_t *ml, void *arg)\n{\n\tmetaslab_t *msp = arg;\n\n\t \n\treturn ((unsigned int)msp->ms_id % multilist_get_num_sublists(ml));\n}\n\nuint64_t\nmetaslab_allocated_space(metaslab_t *msp)\n{\n\treturn (msp->ms_allocated_space);\n}\n\n \nstatic void\nmetaslab_verify_space(metaslab_t *msp, uint64_t txg)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\tuint64_t allocating = 0;\n\tuint64_t sm_free_space, msp_free_space;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT(!msp->ms_condensing);\n\n\tif ((zfs_flags & ZFS_DEBUG_METASLAB_VERIFY) == 0)\n\t\treturn;\n\n\t \n\tif (txg != spa_syncing_txg(spa) || msp->ms_sm == NULL ||\n\t    !msp->ms_loaded)\n\t\treturn;\n\n\t \n\tASSERT3S(space_map_allocated(msp->ms_sm), >=, 0);\n\n\tASSERT3U(space_map_allocated(msp->ms_sm), >=,\n\t    range_tree_space(msp->ms_unflushed_frees));\n\n\tASSERT3U(metaslab_allocated_space(msp), ==,\n\t    space_map_allocated(msp->ms_sm) +\n\t    range_tree_space(msp->ms_unflushed_allocs) -\n\t    range_tree_space(msp->ms_unflushed_frees));\n\n\tsm_free_space = msp->ms_size - metaslab_allocated_space(msp);\n\n\t \n\tfor (int t = 0; t < TXG_CONCURRENT_STATES; t++) {\n\t\tallocating +=\n\t\t    range_tree_space(msp->ms_allocating[(txg + t) & TXG_MASK]);\n\t}\n\tASSERT3U(allocating + msp->ms_allocated_this_txg, ==,\n\t    msp->ms_allocating_total);\n\n\tASSERT3U(msp->ms_deferspace, ==,\n\t    range_tree_space(msp->ms_defer[0]) +\n\t    range_tree_space(msp->ms_defer[1]));\n\n\tmsp_free_space = range_tree_space(msp->ms_allocatable) + allocating +\n\t    msp->ms_deferspace + range_tree_space(msp->ms_freed);\n\n\tVERIFY3U(sm_free_space, ==, msp_free_space);\n}\n\nstatic void\nmetaslab_aux_histograms_clear(metaslab_t *msp)\n{\n\t \n\tASSERT(msp->ms_loaded);\n\n\tmemset(msp->ms_synchist, 0, sizeof (msp->ms_synchist));\n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++)\n\t\tmemset(msp->ms_deferhist[t], 0, sizeof (msp->ms_deferhist[t]));\n}\n\nstatic void\nmetaslab_aux_histogram_add(uint64_t *histogram, uint64_t shift,\n    range_tree_t *rt)\n{\n\t \n\tint idx = 0;\n\tfor (int i = shift; i < RANGE_TREE_HISTOGRAM_SIZE; i++) {\n\t\tASSERT3U(i, >=, idx + shift);\n\t\thistogram[idx] += rt->rt_histogram[i] << (i - idx - shift);\n\n\t\tif (idx < SPACE_MAP_HISTOGRAM_SIZE - 1) {\n\t\t\tASSERT3U(idx + shift, ==, i);\n\t\t\tidx++;\n\t\t\tASSERT3U(idx, <, SPACE_MAP_HISTOGRAM_SIZE);\n\t\t}\n\t}\n}\n\n \nstatic void\nmetaslab_aux_histograms_update(metaslab_t *msp)\n{\n\tspace_map_t *sm = msp->ms_sm;\n\tASSERT(sm != NULL);\n\n\t \n\tif (msp->ms_loaded) {\n\t\tmetaslab_aux_histograms_clear(msp);\n\n\t\tmetaslab_aux_histogram_add(msp->ms_synchist,\n\t\t    sm->sm_shift, msp->ms_freed);\n\n\t\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\t\tmetaslab_aux_histogram_add(msp->ms_deferhist[t],\n\t\t\t    sm->sm_shift, msp->ms_defer[t]);\n\t\t}\n\t}\n\n\tmetaslab_aux_histogram_add(msp->ms_synchist,\n\t    sm->sm_shift, msp->ms_freeing);\n}\n\n \nstatic void\nmetaslab_aux_histograms_update_done(metaslab_t *msp, boolean_t defer_allowed)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\tspace_map_t *sm = msp->ms_sm;\n\n\tif (sm == NULL) {\n\t\t \n\t\treturn;\n\t}\n\n\t \n\tuint64_t hist_index = spa_syncing_txg(spa) % TXG_DEFER_SIZE;\n\tif (defer_allowed) {\n\t\tmemcpy(msp->ms_deferhist[hist_index], msp->ms_synchist,\n\t\t    sizeof (msp->ms_synchist));\n\t} else {\n\t\tmemset(msp->ms_deferhist[hist_index], 0,\n\t\t    sizeof (msp->ms_deferhist[hist_index]));\n\t}\n\tmemset(msp->ms_synchist, 0, sizeof (msp->ms_synchist));\n}\n\n \nstatic void\nmetaslab_verify_weight_and_frag(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tif ((zfs_flags & ZFS_DEBUG_METASLAB_VERIFY) == 0)\n\t\treturn;\n\n\t \n\tif (msp->ms_group == NULL)\n\t\treturn;\n\n\t \n\tvdev_t *vd = msp->ms_group->mg_vd;\n\tif (vd->vdev_removing)\n\t\treturn;\n\n\t \n\tfor (int t = 0; t < TXG_SIZE; t++) {\n\t\tif (txg_list_member(&vd->vdev_ms_list, msp, t))\n\t\t\treturn;\n\t}\n\n\t \n\tif (!spa_writeable(msp->ms_group->mg_vd->vdev_spa))\n\t\treturn;\n\n\t \n\tif (msp->ms_loaded) {\n\t\trange_tree_stat_verify(msp->ms_allocatable);\n\t\tVERIFY(space_map_histogram_verify(msp->ms_sm,\n\t\t    msp->ms_allocatable));\n\t}\n\n\tuint64_t weight = msp->ms_weight;\n\tuint64_t was_active = msp->ms_weight & METASLAB_ACTIVE_MASK;\n\tboolean_t space_based = WEIGHT_IS_SPACEBASED(msp->ms_weight);\n\tuint64_t frag = msp->ms_fragmentation;\n\tuint64_t max_segsize = msp->ms_max_size;\n\n\tmsp->ms_weight = 0;\n\tmsp->ms_fragmentation = 0;\n\n\t \n\tmsp->ms_weight = metaslab_weight(msp, B_TRUE) | was_active;\n\n\tVERIFY3U(max_segsize, ==, msp->ms_max_size);\n\n\t \n\tif ((space_based && !WEIGHT_IS_SPACEBASED(msp->ms_weight)) ||\n\t    (!space_based && WEIGHT_IS_SPACEBASED(msp->ms_weight))) {\n\t\tmsp->ms_fragmentation = frag;\n\t\tmsp->ms_weight = weight;\n\t\treturn;\n\t}\n\n\tVERIFY3U(msp->ms_fragmentation, ==, frag);\n\tVERIFY3U(msp->ms_weight, ==, weight);\n}\n\n \nstatic void\nmetaslab_potentially_evict(metaslab_class_t *mc)\n{\n#ifdef _KERNEL\n\tuint64_t allmem = arc_all_memory();\n\tuint64_t inuse = spl_kmem_cache_inuse(zfs_btree_leaf_cache);\n\tuint64_t size =\tspl_kmem_cache_entry_size(zfs_btree_leaf_cache);\n\tuint_t tries = 0;\n\tfor (; allmem * zfs_metaslab_mem_limit / 100 < inuse * size &&\n\t    tries < multilist_get_num_sublists(&mc->mc_metaslab_txg_list) * 2;\n\t    tries++) {\n\t\tunsigned int idx = multilist_get_random_index(\n\t\t    &mc->mc_metaslab_txg_list);\n\t\tmultilist_sublist_t *mls =\n\t\t    multilist_sublist_lock(&mc->mc_metaslab_txg_list, idx);\n\t\tmetaslab_t *msp = multilist_sublist_head(mls);\n\t\tmultilist_sublist_unlock(mls);\n\t\twhile (msp != NULL && allmem * zfs_metaslab_mem_limit / 100 <\n\t\t    inuse * size) {\n\t\t\tVERIFY3P(mls, ==, multilist_sublist_lock(\n\t\t\t    &mc->mc_metaslab_txg_list, idx));\n\t\t\tASSERT3U(idx, ==,\n\t\t\t    metaslab_idx_func(&mc->mc_metaslab_txg_list, msp));\n\n\t\t\tif (!multilist_link_active(&msp->ms_class_txg_node)) {\n\t\t\t\tmultilist_sublist_unlock(mls);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmetaslab_t *next_msp = multilist_sublist_next(mls, msp);\n\t\t\tmultilist_sublist_unlock(mls);\n\t\t\t \n\t\t\tif (msp->ms_loading) {\n\t\t\t\tmsp = next_msp;\n\t\t\t\tinuse =\n\t\t\t\t    spl_kmem_cache_inuse(zfs_btree_leaf_cache);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tmutex_enter(&msp->ms_lock);\n\t\t\tif (msp->ms_allocator == -1 && msp->ms_sm != NULL &&\n\t\t\t    msp->ms_allocating_total == 0) {\n\t\t\t\tmetaslab_unload(msp);\n\t\t\t}\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tmsp = next_msp;\n\t\t\tinuse = spl_kmem_cache_inuse(zfs_btree_leaf_cache);\n\t\t}\n\t}\n#else\n\t(void) mc, (void) zfs_metaslab_mem_limit;\n#endif\n}\n\nstatic int\nmetaslab_load_impl(metaslab_t *msp)\n{\n\tint error = 0;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT(msp->ms_loading);\n\tASSERT(!msp->ms_condensing);\n\n\t \n\tuint64_t length = msp->ms_synced_length;\n\tmutex_exit(&msp->ms_lock);\n\n\thrtime_t load_start = gethrtime();\n\tmetaslab_rt_arg_t *mrap;\n\tif (msp->ms_allocatable->rt_arg == NULL) {\n\t\tmrap = kmem_zalloc(sizeof (*mrap), KM_SLEEP);\n\t} else {\n\t\tmrap = msp->ms_allocatable->rt_arg;\n\t\tmsp->ms_allocatable->rt_ops = NULL;\n\t\tmsp->ms_allocatable->rt_arg = NULL;\n\t}\n\tmrap->mra_bt = &msp->ms_allocatable_by_size;\n\tmrap->mra_floor_shift = metaslab_by_size_min_shift;\n\n\tif (msp->ms_sm != NULL) {\n\t\terror = space_map_load_length(msp->ms_sm, msp->ms_allocatable,\n\t\t    SM_FREE, length);\n\n\t\t \n\t\tmetaslab_rt_create(msp->ms_allocatable, mrap);\n\t\tmsp->ms_allocatable->rt_ops = &metaslab_rt_ops;\n\t\tmsp->ms_allocatable->rt_arg = mrap;\n\n\t\tstruct mssa_arg arg = {0};\n\t\targ.rt = msp->ms_allocatable;\n\t\targ.mra = mrap;\n\t\trange_tree_walk(msp->ms_allocatable, metaslab_size_sorted_add,\n\t\t    &arg);\n\t} else {\n\t\t \n\t\tmetaslab_rt_create(msp->ms_allocatable, mrap);\n\t\tmsp->ms_allocatable->rt_ops = &metaslab_rt_ops;\n\t\tmsp->ms_allocatable->rt_arg = mrap;\n\t\t \n\t\trange_tree_add(msp->ms_allocatable,\n\t\t    msp->ms_start, msp->ms_size);\n\n\t\tif (msp->ms_new) {\n\t\t\t \n\t\t\tASSERT(range_tree_is_empty(msp->ms_unflushed_allocs));\n\t\t\tASSERT(range_tree_is_empty(msp->ms_unflushed_frees));\n\t\t}\n\t}\n\n\t \n\tmutex_enter(&msp->ms_sync_lock);\n\tmutex_enter(&msp->ms_lock);\n\n\tASSERT(!msp->ms_condensing);\n\tASSERT(!msp->ms_flushing);\n\n\tif (error != 0) {\n\t\tmutex_exit(&msp->ms_sync_lock);\n\t\treturn (error);\n\t}\n\n\tASSERT3P(msp->ms_group, !=, NULL);\n\tmsp->ms_loaded = B_TRUE;\n\n\t \n\trange_tree_walk(msp->ms_unflushed_allocs,\n\t    range_tree_remove, msp->ms_allocatable);\n\trange_tree_walk(msp->ms_unflushed_frees,\n\t    range_tree_add, msp->ms_allocatable);\n\n\tASSERT3P(msp->ms_group, !=, NULL);\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\tif (spa_syncing_log_sm(spa) != NULL) {\n\t\tASSERT(spa_feature_is_enabled(spa,\n\t\t    SPA_FEATURE_LOG_SPACEMAP));\n\n\t\t \n\t\trange_tree_walk(msp->ms_freed,\n\t\t    range_tree_remove, msp->ms_allocatable);\n\t}\n\n\t \n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\trange_tree_walk(msp->ms_defer[t],\n\t\t    range_tree_remove, msp->ms_allocatable);\n\t}\n\n\t \n\tuint64_t weight = msp->ms_weight;\n\tuint64_t max_size = msp->ms_max_size;\n\tmetaslab_recalculate_weight_and_sort(msp);\n\tif (!WEIGHT_IS_SPACEBASED(weight))\n\t\tASSERT3U(weight, <=, msp->ms_weight);\n\tmsp->ms_max_size = metaslab_largest_allocatable(msp);\n\tASSERT3U(max_size, <=, msp->ms_max_size);\n\thrtime_t load_end = gethrtime();\n\tmsp->ms_load_time = load_end;\n\tzfs_dbgmsg(\"metaslab_load: txg %llu, spa %s, vdev_id %llu, \"\n\t    \"ms_id %llu, smp_length %llu, \"\n\t    \"unflushed_allocs %llu, unflushed_frees %llu, \"\n\t    \"freed %llu, defer %llu + %llu, unloaded time %llu ms, \"\n\t    \"loading_time %lld ms, ms_max_size %llu, \"\n\t    \"max size error %lld, \"\n\t    \"old_weight %llx, new_weight %llx\",\n\t    (u_longlong_t)spa_syncing_txg(spa), spa_name(spa),\n\t    (u_longlong_t)msp->ms_group->mg_vd->vdev_id,\n\t    (u_longlong_t)msp->ms_id,\n\t    (u_longlong_t)space_map_length(msp->ms_sm),\n\t    (u_longlong_t)range_tree_space(msp->ms_unflushed_allocs),\n\t    (u_longlong_t)range_tree_space(msp->ms_unflushed_frees),\n\t    (u_longlong_t)range_tree_space(msp->ms_freed),\n\t    (u_longlong_t)range_tree_space(msp->ms_defer[0]),\n\t    (u_longlong_t)range_tree_space(msp->ms_defer[1]),\n\t    (longlong_t)((load_start - msp->ms_unload_time) / 1000000),\n\t    (longlong_t)((load_end - load_start) / 1000000),\n\t    (u_longlong_t)msp->ms_max_size,\n\t    (u_longlong_t)msp->ms_max_size - max_size,\n\t    (u_longlong_t)weight, (u_longlong_t)msp->ms_weight);\n\n\tmetaslab_verify_space(msp, spa_syncing_txg(spa));\n\tmutex_exit(&msp->ms_sync_lock);\n\treturn (0);\n}\n\nint\nmetaslab_load(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tmetaslab_load_wait(msp);\n\tif (msp->ms_loaded)\n\t\treturn (0);\n\tVERIFY(!msp->ms_loading);\n\tASSERT(!msp->ms_condensing);\n\n\t \n\tmsp->ms_loading = B_TRUE;\n\n\t \n\tif (msp->ms_flushing)\n\t\tmetaslab_flush_wait(msp);\n\n\t \n\tASSERT(!msp->ms_loaded);\n\n\t \n\tif (spa_normal_class(msp->ms_group->mg_class->mc_spa) ==\n\t    msp->ms_group->mg_class) {\n\t\tmetaslab_potentially_evict(msp->ms_group->mg_class);\n\t}\n\n\tint error = metaslab_load_impl(msp);\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tmsp->ms_loading = B_FALSE;\n\tcv_broadcast(&msp->ms_load_cv);\n\n\treturn (error);\n}\n\nvoid\nmetaslab_unload(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tif (!msp->ms_loaded)\n\t\treturn;\n\n\trange_tree_vacate(msp->ms_allocatable, NULL, NULL);\n\tmsp->ms_loaded = B_FALSE;\n\tmsp->ms_unload_time = gethrtime();\n\n\tmsp->ms_activation_weight = 0;\n\tmsp->ms_weight &= ~METASLAB_ACTIVE_MASK;\n\n\tif (msp->ms_group != NULL) {\n\t\tmetaslab_class_t *mc = msp->ms_group->mg_class;\n\t\tmultilist_sublist_t *mls =\n\t\t    multilist_sublist_lock_obj(&mc->mc_metaslab_txg_list, msp);\n\t\tif (multilist_link_active(&msp->ms_class_txg_node))\n\t\t\tmultilist_sublist_remove(mls, msp);\n\t\tmultilist_sublist_unlock(mls);\n\n\t\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\t\tzfs_dbgmsg(\"metaslab_unload: txg %llu, spa %s, vdev_id %llu, \"\n\t\t    \"ms_id %llu, weight %llx, \"\n\t\t    \"selected txg %llu (%llu ms ago), alloc_txg %llu, \"\n\t\t    \"loaded %llu ms ago, max_size %llu\",\n\t\t    (u_longlong_t)spa_syncing_txg(spa), spa_name(spa),\n\t\t    (u_longlong_t)msp->ms_group->mg_vd->vdev_id,\n\t\t    (u_longlong_t)msp->ms_id,\n\t\t    (u_longlong_t)msp->ms_weight,\n\t\t    (u_longlong_t)msp->ms_selected_txg,\n\t\t    (u_longlong_t)(msp->ms_unload_time -\n\t\t    msp->ms_selected_time) / 1000 / 1000,\n\t\t    (u_longlong_t)msp->ms_alloc_txg,\n\t\t    (u_longlong_t)(msp->ms_unload_time -\n\t\t    msp->ms_load_time) / 1000 / 1000,\n\t\t    (u_longlong_t)msp->ms_max_size);\n\t}\n\n\t \n\tif (msp->ms_group != NULL)\n\t\tmetaslab_recalculate_weight_and_sort(msp);\n}\n\n \nrange_seg_type_t\nmetaslab_calculate_range_tree_type(vdev_t *vdev, metaslab_t *msp,\n    uint64_t *start, uint64_t *shift)\n{\n\tif (vdev->vdev_ms_shift - vdev->vdev_ashift < 32 &&\n\t    !zfs_metaslab_force_large_segs) {\n\t\t*shift = vdev->vdev_ashift;\n\t\t*start = msp->ms_start;\n\t\treturn (RANGE_SEG32);\n\t} else {\n\t\t*shift = 0;\n\t\t*start = 0;\n\t\treturn (RANGE_SEG64);\n\t}\n}\n\nvoid\nmetaslab_set_selected_txg(metaslab_t *msp, uint64_t txg)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tmetaslab_class_t *mc = msp->ms_group->mg_class;\n\tmultilist_sublist_t *mls =\n\t    multilist_sublist_lock_obj(&mc->mc_metaslab_txg_list, msp);\n\tif (multilist_link_active(&msp->ms_class_txg_node))\n\t\tmultilist_sublist_remove(mls, msp);\n\tmsp->ms_selected_txg = txg;\n\tmsp->ms_selected_time = gethrtime();\n\tmultilist_sublist_insert_tail(mls, msp);\n\tmultilist_sublist_unlock(mls);\n}\n\nvoid\nmetaslab_space_update(vdev_t *vd, metaslab_class_t *mc, int64_t alloc_delta,\n    int64_t defer_delta, int64_t space_delta)\n{\n\tvdev_space_update(vd, alloc_delta, defer_delta, space_delta);\n\n\tASSERT3P(vd->vdev_spa->spa_root_vdev, ==, vd->vdev_parent);\n\tASSERT(vd->vdev_ms_count != 0);\n\n\tmetaslab_class_space_update(mc, alloc_delta, defer_delta, space_delta,\n\t    vdev_deflated_space(vd, space_delta));\n}\n\nint\nmetaslab_init(metaslab_group_t *mg, uint64_t id, uint64_t object,\n    uint64_t txg, metaslab_t **msp)\n{\n\tvdev_t *vd = mg->mg_vd;\n\tspa_t *spa = vd->vdev_spa;\n\tobjset_t *mos = spa->spa_meta_objset;\n\tmetaslab_t *ms;\n\tint error;\n\n\tms = kmem_zalloc(sizeof (metaslab_t), KM_SLEEP);\n\tmutex_init(&ms->ms_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&ms->ms_sync_lock, NULL, MUTEX_DEFAULT, NULL);\n\tcv_init(&ms->ms_load_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&ms->ms_flush_cv, NULL, CV_DEFAULT, NULL);\n\tmultilist_link_init(&ms->ms_class_txg_node);\n\n\tms->ms_id = id;\n\tms->ms_start = id << vd->vdev_ms_shift;\n\tms->ms_size = 1ULL << vd->vdev_ms_shift;\n\tms->ms_allocator = -1;\n\tms->ms_new = B_TRUE;\n\n\tvdev_ops_t *ops = vd->vdev_ops;\n\tif (ops->vdev_op_metaslab_init != NULL)\n\t\tops->vdev_op_metaslab_init(vd, &ms->ms_start, &ms->ms_size);\n\n\t \n\tif (object != 0 && !(spa->spa_mode == SPA_MODE_READ &&\n\t    !spa->spa_read_spacemaps)) {\n\t\terror = space_map_open(&ms->ms_sm, mos, object, ms->ms_start,\n\t\t    ms->ms_size, vd->vdev_ashift);\n\n\t\tif (error != 0) {\n\t\t\tkmem_free(ms, sizeof (metaslab_t));\n\t\t\treturn (error);\n\t\t}\n\n\t\tASSERT(ms->ms_sm != NULL);\n\t\tms->ms_allocated_space = space_map_allocated(ms->ms_sm);\n\t}\n\n\tuint64_t shift, start;\n\trange_seg_type_t type =\n\t    metaslab_calculate_range_tree_type(vd, ms, &start, &shift);\n\n\tms->ms_allocatable = range_tree_create(NULL, type, NULL, start, shift);\n\tfor (int t = 0; t < TXG_SIZE; t++) {\n\t\tms->ms_allocating[t] = range_tree_create(NULL, type,\n\t\t    NULL, start, shift);\n\t}\n\tms->ms_freeing = range_tree_create(NULL, type, NULL, start, shift);\n\tms->ms_freed = range_tree_create(NULL, type, NULL, start, shift);\n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\tms->ms_defer[t] = range_tree_create(NULL, type, NULL,\n\t\t    start, shift);\n\t}\n\tms->ms_checkpointing =\n\t    range_tree_create(NULL, type, NULL, start, shift);\n\tms->ms_unflushed_allocs =\n\t    range_tree_create(NULL, type, NULL, start, shift);\n\n\tmetaslab_rt_arg_t *mrap = kmem_zalloc(sizeof (*mrap), KM_SLEEP);\n\tmrap->mra_bt = &ms->ms_unflushed_frees_by_size;\n\tmrap->mra_floor_shift = metaslab_by_size_min_shift;\n\tms->ms_unflushed_frees = range_tree_create(&metaslab_rt_ops,\n\t    type, mrap, start, shift);\n\n\tms->ms_trim = range_tree_create(NULL, type, NULL, start, shift);\n\n\tmetaslab_group_add(mg, ms);\n\tmetaslab_set_fragmentation(ms, B_FALSE);\n\n\t \n\tif (txg <= TXG_INITIAL) {\n\t\tmetaslab_sync_done(ms, 0);\n\t\tmetaslab_space_update(vd, mg->mg_class,\n\t\t    metaslab_allocated_space(ms), 0, 0);\n\t}\n\n\tif (txg != 0) {\n\t\tvdev_dirty(vd, 0, NULL, txg);\n\t\tvdev_dirty(vd, VDD_METASLAB, ms, txg);\n\t}\n\n\t*msp = ms;\n\n\treturn (0);\n}\n\nstatic void\nmetaslab_fini_flush_data(metaslab_t *msp)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\n\tif (metaslab_unflushed_txg(msp) == 0) {\n\t\tASSERT3P(avl_find(&spa->spa_metaslabs_by_flushed, msp, NULL),\n\t\t    ==, NULL);\n\t\treturn;\n\t}\n\tASSERT(spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP));\n\n\tmutex_enter(&spa->spa_flushed_ms_lock);\n\tavl_remove(&spa->spa_metaslabs_by_flushed, msp);\n\tmutex_exit(&spa->spa_flushed_ms_lock);\n\n\tspa_log_sm_decrement_mscount(spa, metaslab_unflushed_txg(msp));\n\tspa_log_summary_decrement_mscount(spa, metaslab_unflushed_txg(msp),\n\t    metaslab_unflushed_dirty(msp));\n}\n\nuint64_t\nmetaslab_unflushed_changes_memused(metaslab_t *ms)\n{\n\treturn ((range_tree_numsegs(ms->ms_unflushed_allocs) +\n\t    range_tree_numsegs(ms->ms_unflushed_frees)) *\n\t    ms->ms_unflushed_allocs->rt_root.bt_elem_size);\n}\n\nvoid\nmetaslab_fini(metaslab_t *msp)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tvdev_t *vd = mg->mg_vd;\n\tspa_t *spa = vd->vdev_spa;\n\n\tmetaslab_fini_flush_data(msp);\n\n\tmetaslab_group_remove(mg, msp);\n\n\tmutex_enter(&msp->ms_lock);\n\tVERIFY(msp->ms_group == NULL);\n\n\t \n\tif (!msp->ms_new) {\n\t\tmetaslab_space_update(vd, mg->mg_class,\n\t\t    -metaslab_allocated_space(msp), 0, -msp->ms_size);\n\n\t}\n\tspace_map_close(msp->ms_sm);\n\tmsp->ms_sm = NULL;\n\n\tmetaslab_unload(msp);\n\n\trange_tree_destroy(msp->ms_allocatable);\n\trange_tree_destroy(msp->ms_freeing);\n\trange_tree_destroy(msp->ms_freed);\n\n\tASSERT3U(spa->spa_unflushed_stats.sus_memused, >=,\n\t    metaslab_unflushed_changes_memused(msp));\n\tspa->spa_unflushed_stats.sus_memused -=\n\t    metaslab_unflushed_changes_memused(msp);\n\trange_tree_vacate(msp->ms_unflushed_allocs, NULL, NULL);\n\trange_tree_destroy(msp->ms_unflushed_allocs);\n\trange_tree_destroy(msp->ms_checkpointing);\n\trange_tree_vacate(msp->ms_unflushed_frees, NULL, NULL);\n\trange_tree_destroy(msp->ms_unflushed_frees);\n\n\tfor (int t = 0; t < TXG_SIZE; t++) {\n\t\trange_tree_destroy(msp->ms_allocating[t]);\n\t}\n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\trange_tree_destroy(msp->ms_defer[t]);\n\t}\n\tASSERT0(msp->ms_deferspace);\n\n\tfor (int t = 0; t < TXG_SIZE; t++)\n\t\tASSERT(!txg_list_member(&vd->vdev_ms_list, msp, t));\n\n\trange_tree_vacate(msp->ms_trim, NULL, NULL);\n\trange_tree_destroy(msp->ms_trim);\n\n\tmutex_exit(&msp->ms_lock);\n\tcv_destroy(&msp->ms_load_cv);\n\tcv_destroy(&msp->ms_flush_cv);\n\tmutex_destroy(&msp->ms_lock);\n\tmutex_destroy(&msp->ms_sync_lock);\n\tASSERT3U(msp->ms_allocator, ==, -1);\n\n\tkmem_free(msp, sizeof (metaslab_t));\n}\n\n#define\tFRAGMENTATION_TABLE_SIZE\t17\n\n \nstatic const int zfs_frag_table[FRAGMENTATION_TABLE_SIZE] = {\n\t100,\t \n\t100,\t \n\t98,\t \n\t95,\t \n\t90,\t \n\t80,\t \n\t70,\t \n\t60,\t \n\t50,\t \n\t40,\t \n\t30,\t \n\t20,\t \n\t15,\t \n\t10,\t \n\t5,\t \n\t0\t \n};\n\n \nstatic void\nmetaslab_set_fragmentation(metaslab_t *msp, boolean_t nodirty)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\tuint64_t fragmentation = 0;\n\tuint64_t total = 0;\n\tboolean_t feature_enabled = spa_feature_is_enabled(spa,\n\t    SPA_FEATURE_SPACEMAP_HISTOGRAM);\n\n\tif (!feature_enabled) {\n\t\tmsp->ms_fragmentation = ZFS_FRAG_INVALID;\n\t\treturn;\n\t}\n\n\t \n\tif (msp->ms_sm == NULL) {\n\t\tmsp->ms_fragmentation = 0;\n\t\treturn;\n\t}\n\n\t \n\tif (msp->ms_sm->sm_dbuf->db_size != sizeof (space_map_phys_t)) {\n\t\tuint64_t txg = spa_syncing_txg(spa);\n\t\tvdev_t *vd = msp->ms_group->mg_vd;\n\n\t\t \n\t\tif (!nodirty &&\n\t\t    spa_writeable(spa) && txg < spa_final_dirty_txg(spa)) {\n\t\t\tmsp->ms_condense_wanted = B_TRUE;\n\t\t\tvdev_dirty(vd, VDD_METASLAB, msp, txg + 1);\n\t\t\tzfs_dbgmsg(\"txg %llu, requesting force condense: \"\n\t\t\t    \"ms_id %llu, vdev_id %llu\", (u_longlong_t)txg,\n\t\t\t    (u_longlong_t)msp->ms_id,\n\t\t\t    (u_longlong_t)vd->vdev_id);\n\t\t}\n\t\tmsp->ms_fragmentation = ZFS_FRAG_INVALID;\n\t\treturn;\n\t}\n\n\tfor (int i = 0; i < SPACE_MAP_HISTOGRAM_SIZE; i++) {\n\t\tuint64_t space = 0;\n\t\tuint8_t shift = msp->ms_sm->sm_shift;\n\n\t\tint idx = MIN(shift - SPA_MINBLOCKSHIFT + i,\n\t\t    FRAGMENTATION_TABLE_SIZE - 1);\n\n\t\tif (msp->ms_sm->sm_phys->smp_histogram[i] == 0)\n\t\t\tcontinue;\n\n\t\tspace = msp->ms_sm->sm_phys->smp_histogram[i] << (i + shift);\n\t\ttotal += space;\n\n\t\tASSERT3U(idx, <, FRAGMENTATION_TABLE_SIZE);\n\t\tfragmentation += space * zfs_frag_table[idx];\n\t}\n\n\tif (total > 0)\n\t\tfragmentation /= total;\n\tASSERT3U(fragmentation, <=, 100);\n\n\tmsp->ms_fragmentation = fragmentation;\n}\n\n \nstatic uint64_t\nmetaslab_space_weight(metaslab_t *msp)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tvdev_t *vd = mg->mg_vd;\n\tuint64_t weight, space;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tspace = msp->ms_size - metaslab_allocated_space(msp);\n\n\tif (metaslab_fragmentation_factor_enabled &&\n\t    msp->ms_fragmentation != ZFS_FRAG_INVALID) {\n\t\t \n\t\tspace = (space * (100 - (msp->ms_fragmentation - 1))) / 100;\n\n\t\t \n\t\tif (space > 0 && space < SPA_MINBLOCKSIZE)\n\t\t\tspace = SPA_MINBLOCKSIZE;\n\t}\n\tweight = space;\n\n\t \n\tif (!vd->vdev_nonrot && metaslab_lba_weighting_enabled) {\n\t\tweight = 2 * weight - (msp->ms_id * weight) / vd->vdev_ms_count;\n\t\tASSERT(weight >= space && weight <= 2 * space);\n\t}\n\n\t \n\tif (msp->ms_loaded && msp->ms_fragmentation != ZFS_FRAG_INVALID &&\n\t    msp->ms_fragmentation <= zfs_metaslab_fragmentation_threshold) {\n\t\tweight |= (msp->ms_weight & METASLAB_ACTIVE_MASK);\n\t}\n\n\tWEIGHT_SET_SPACEBASED(weight);\n\treturn (weight);\n}\n\n \nstatic uint64_t\nmetaslab_weight_from_range_tree(metaslab_t *msp)\n{\n\tuint64_t weight = 0;\n\tuint32_t segments = 0;\n\n\tASSERT(msp->ms_loaded);\n\n\tfor (int i = RANGE_TREE_HISTOGRAM_SIZE - 1; i >= SPA_MINBLOCKSHIFT;\n\t    i--) {\n\t\tuint8_t shift = msp->ms_group->mg_vd->vdev_ashift;\n\t\tint max_idx = SPACE_MAP_HISTOGRAM_SIZE + shift - 1;\n\n\t\tsegments <<= 1;\n\t\tsegments += msp->ms_allocatable->rt_histogram[i];\n\n\t\t \n\t\tif (i > max_idx)\n\t\t\tcontinue;\n\n\t\tif (segments != 0) {\n\t\t\tWEIGHT_SET_COUNT(weight, segments);\n\t\t\tWEIGHT_SET_INDEX(weight, i);\n\t\t\tWEIGHT_SET_ACTIVE(weight, 0);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn (weight);\n}\n\n \nstatic uint64_t\nmetaslab_weight_from_spacemap(metaslab_t *msp)\n{\n\tspace_map_t *sm = msp->ms_sm;\n\tASSERT(!msp->ms_loaded);\n\tASSERT(sm != NULL);\n\tASSERT3U(space_map_object(sm), !=, 0);\n\tASSERT3U(sm->sm_dbuf->db_size, ==, sizeof (space_map_phys_t));\n\n\t \n\tuint64_t deferspace_histogram[SPACE_MAP_HISTOGRAM_SIZE] = {0};\n\tfor (int i = 0; i < SPACE_MAP_HISTOGRAM_SIZE; i++)\n\t\tdeferspace_histogram[i] += msp->ms_synchist[i];\n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\tfor (int i = 0; i < SPACE_MAP_HISTOGRAM_SIZE; i++) {\n\t\t\tdeferspace_histogram[i] += msp->ms_deferhist[t][i];\n\t\t}\n\t}\n\n\tuint64_t weight = 0;\n\tfor (int i = SPACE_MAP_HISTOGRAM_SIZE - 1; i >= 0; i--) {\n\t\tASSERT3U(sm->sm_phys->smp_histogram[i], >=,\n\t\t    deferspace_histogram[i]);\n\t\tuint64_t count =\n\t\t    sm->sm_phys->smp_histogram[i] - deferspace_histogram[i];\n\t\tif (count != 0) {\n\t\t\tWEIGHT_SET_COUNT(weight, count);\n\t\t\tWEIGHT_SET_INDEX(weight, i + sm->sm_shift);\n\t\t\tWEIGHT_SET_ACTIVE(weight, 0);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn (weight);\n}\n\n \nstatic uint64_t\nmetaslab_segment_weight(metaslab_t *msp)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tuint64_t weight = 0;\n\tuint8_t shift = mg->mg_vd->vdev_ashift;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tif (metaslab_allocated_space(msp) == 0) {\n\t\tint idx = highbit64(msp->ms_size) - 1;\n\t\tint max_idx = SPACE_MAP_HISTOGRAM_SIZE + shift - 1;\n\n\t\tif (idx < max_idx) {\n\t\t\tWEIGHT_SET_COUNT(weight, 1ULL);\n\t\t\tWEIGHT_SET_INDEX(weight, idx);\n\t\t} else {\n\t\t\tWEIGHT_SET_COUNT(weight, 1ULL << (idx - max_idx));\n\t\t\tWEIGHT_SET_INDEX(weight, max_idx);\n\t\t}\n\t\tWEIGHT_SET_ACTIVE(weight, 0);\n\t\tASSERT(!WEIGHT_IS_SPACEBASED(weight));\n\t\treturn (weight);\n\t}\n\n\tASSERT3U(msp->ms_sm->sm_dbuf->db_size, ==, sizeof (space_map_phys_t));\n\n\t \n\tif (metaslab_allocated_space(msp) == msp->ms_size)\n\t\treturn (0);\n\t \n\tif (msp->ms_loaded) {\n\t\tweight = metaslab_weight_from_range_tree(msp);\n\t} else {\n\t\tweight = metaslab_weight_from_spacemap(msp);\n\t}\n\n\t \n\tif (msp->ms_activation_weight != 0 && weight != 0)\n\t\tWEIGHT_SET_ACTIVE(weight, WEIGHT_GET_ACTIVE(msp->ms_weight));\n\treturn (weight);\n}\n\n \nstatic boolean_t\nmetaslab_should_allocate(metaslab_t *msp, uint64_t asize, boolean_t try_hard)\n{\n\t \n\tif (unlikely(msp->ms_new))\n\t\treturn (B_FALSE);\n\n\t \n\tif (msp->ms_loaded ||\n\t    (msp->ms_max_size != 0 && !try_hard && gethrtime() <\n\t    msp->ms_unload_time + SEC2NSEC(zfs_metaslab_max_size_cache_sec)))\n\t\treturn (msp->ms_max_size >= asize);\n\n\tboolean_t should_allocate;\n\tif (!WEIGHT_IS_SPACEBASED(msp->ms_weight)) {\n\t\t \n\t\tshould_allocate = (asize <\n\t\t    1ULL << (WEIGHT_GET_INDEX(msp->ms_weight) + 1));\n\t} else {\n\t\tshould_allocate = (asize <=\n\t\t    (msp->ms_weight & ~METASLAB_WEIGHT_TYPE));\n\t}\n\n\treturn (should_allocate);\n}\n\nstatic uint64_t\nmetaslab_weight(metaslab_t *msp, boolean_t nodirty)\n{\n\tvdev_t *vd = msp->ms_group->mg_vd;\n\tspa_t *spa = vd->vdev_spa;\n\tuint64_t weight;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tmetaslab_set_fragmentation(msp, nodirty);\n\n\t \n\tif (msp->ms_loaded) {\n\t\tmsp->ms_max_size = metaslab_largest_allocatable(msp);\n\t} else {\n\t\tmsp->ms_max_size = MAX(msp->ms_max_size,\n\t\t    metaslab_largest_unflushed_free(msp));\n\t}\n\n\t \n\tif (zfs_metaslab_segment_weight_enabled &&\n\t    spa_feature_is_enabled(spa, SPA_FEATURE_SPACEMAP_HISTOGRAM) &&\n\t    (msp->ms_sm == NULL || msp->ms_sm->sm_dbuf->db_size ==\n\t    sizeof (space_map_phys_t))) {\n\t\tweight = metaslab_segment_weight(msp);\n\t} else {\n\t\tweight = metaslab_space_weight(msp);\n\t}\n\treturn (weight);\n}\n\nvoid\nmetaslab_recalculate_weight_and_sort(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tuint64_t was_active = msp->ms_weight & METASLAB_ACTIVE_MASK;\n\tmetaslab_group_sort(msp->ms_group, msp,\n\t    metaslab_weight(msp, B_FALSE) | was_active);\n}\n\nstatic int\nmetaslab_activate_allocator(metaslab_group_t *mg, metaslab_t *msp,\n    int allocator, uint64_t activation_weight)\n{\n\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tif (activation_weight == METASLAB_WEIGHT_CLAIM) {\n\t\tASSERT0(msp->ms_activation_weight);\n\t\tmsp->ms_activation_weight = msp->ms_weight;\n\t\tmetaslab_group_sort(mg, msp, msp->ms_weight |\n\t\t    activation_weight);\n\t\treturn (0);\n\t}\n\n\tmetaslab_t **mspp = (activation_weight == METASLAB_WEIGHT_PRIMARY ?\n\t    &mga->mga_primary : &mga->mga_secondary);\n\n\tmutex_enter(&mg->mg_lock);\n\tif (*mspp != NULL) {\n\t\tmutex_exit(&mg->mg_lock);\n\t\treturn (EEXIST);\n\t}\n\n\t*mspp = msp;\n\tASSERT3S(msp->ms_allocator, ==, -1);\n\tmsp->ms_allocator = allocator;\n\tmsp->ms_primary = (activation_weight == METASLAB_WEIGHT_PRIMARY);\n\n\tASSERT0(msp->ms_activation_weight);\n\tmsp->ms_activation_weight = msp->ms_weight;\n\tmetaslab_group_sort_impl(mg, msp,\n\t    msp->ms_weight | activation_weight);\n\tmutex_exit(&mg->mg_lock);\n\n\treturn (0);\n}\n\nstatic int\nmetaslab_activate(metaslab_t *msp, int allocator, uint64_t activation_weight)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\t \n\tif ((msp->ms_weight & METASLAB_ACTIVE_MASK) != 0) {\n\t\tASSERT(msp->ms_loaded);\n\t\treturn (0);\n\t}\n\n\tint error = metaslab_load(msp);\n\tif (error != 0) {\n\t\tmetaslab_group_sort(msp->ms_group, msp, 0);\n\t\treturn (error);\n\t}\n\n\t \n\tif ((msp->ms_weight & METASLAB_ACTIVE_MASK) != 0) {\n\t\tif (msp->ms_allocator != allocator)\n\t\t\treturn (EBUSY);\n\n\t\tif ((msp->ms_weight & activation_weight) == 0)\n\t\t\treturn (SET_ERROR(EBUSY));\n\n\t\tEQUIV((activation_weight == METASLAB_WEIGHT_PRIMARY),\n\t\t    msp->ms_primary);\n\t\treturn (0);\n\t}\n\n\t \n\tif (msp->ms_weight == 0) {\n\t\tASSERT0(range_tree_space(msp->ms_allocatable));\n\t\treturn (SET_ERROR(ENOSPC));\n\t}\n\n\tif ((error = metaslab_activate_allocator(msp->ms_group, msp,\n\t    allocator, activation_weight)) != 0) {\n\t\treturn (error);\n\t}\n\n\tASSERT(msp->ms_loaded);\n\tASSERT(msp->ms_weight & METASLAB_ACTIVE_MASK);\n\n\treturn (0);\n}\n\nstatic void\nmetaslab_passivate_allocator(metaslab_group_t *mg, metaslab_t *msp,\n    uint64_t weight)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT(msp->ms_loaded);\n\n\tif (msp->ms_weight & METASLAB_WEIGHT_CLAIM) {\n\t\tmetaslab_group_sort(mg, msp, weight);\n\t\treturn;\n\t}\n\n\tmutex_enter(&mg->mg_lock);\n\tASSERT3P(msp->ms_group, ==, mg);\n\tASSERT3S(0, <=, msp->ms_allocator);\n\tASSERT3U(msp->ms_allocator, <, mg->mg_allocators);\n\n\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[msp->ms_allocator];\n\tif (msp->ms_primary) {\n\t\tASSERT3P(mga->mga_primary, ==, msp);\n\t\tASSERT(msp->ms_weight & METASLAB_WEIGHT_PRIMARY);\n\t\tmga->mga_primary = NULL;\n\t} else {\n\t\tASSERT3P(mga->mga_secondary, ==, msp);\n\t\tASSERT(msp->ms_weight & METASLAB_WEIGHT_SECONDARY);\n\t\tmga->mga_secondary = NULL;\n\t}\n\tmsp->ms_allocator = -1;\n\tmetaslab_group_sort_impl(mg, msp, weight);\n\tmutex_exit(&mg->mg_lock);\n}\n\nstatic void\nmetaslab_passivate(metaslab_t *msp, uint64_t weight)\n{\n\tuint64_t size __maybe_unused = weight & ~METASLAB_WEIGHT_TYPE;\n\n\t \n\tASSERT(!WEIGHT_IS_SPACEBASED(msp->ms_weight) ||\n\t    size >= SPA_MINBLOCKSIZE ||\n\t    range_tree_space(msp->ms_allocatable) == 0);\n\tASSERT0(weight & METASLAB_ACTIVE_MASK);\n\n\tASSERT(msp->ms_activation_weight != 0);\n\tmsp->ms_activation_weight = 0;\n\tmetaslab_passivate_allocator(msp->ms_group, msp, weight);\n\tASSERT0(msp->ms_weight & METASLAB_ACTIVE_MASK);\n}\n\n \nstatic void\nmetaslab_segment_may_passivate(metaslab_t *msp)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\n\tif (WEIGHT_IS_SPACEBASED(msp->ms_weight) || spa_sync_pass(spa) > 1)\n\t\treturn;\n\n\t \n\tuint64_t weight = metaslab_weight_from_range_tree(msp);\n\tint activation_idx = WEIGHT_GET_INDEX(msp->ms_activation_weight);\n\tint current_idx = WEIGHT_GET_INDEX(weight);\n\n\tif (current_idx <= activation_idx - zfs_metaslab_switch_threshold)\n\t\tmetaslab_passivate(msp, weight);\n}\n\nstatic void\nmetaslab_preload(void *arg)\n{\n\tmetaslab_t *msp = arg;\n\tmetaslab_class_t *mc = msp->ms_group->mg_class;\n\tspa_t *spa = mc->mc_spa;\n\tfstrans_cookie_t cookie = spl_fstrans_mark();\n\n\tASSERT(!MUTEX_HELD(&msp->ms_group->mg_lock));\n\n\tmutex_enter(&msp->ms_lock);\n\t(void) metaslab_load(msp);\n\tmetaslab_set_selected_txg(msp, spa_syncing_txg(spa));\n\tmutex_exit(&msp->ms_lock);\n\tspl_fstrans_unmark(cookie);\n}\n\nstatic void\nmetaslab_group_preload(metaslab_group_t *mg)\n{\n\tspa_t *spa = mg->mg_vd->vdev_spa;\n\tmetaslab_t *msp;\n\tavl_tree_t *t = &mg->mg_metaslab_tree;\n\tint m = 0;\n\n\tif (spa_shutting_down(spa) || !metaslab_preload_enabled)\n\t\treturn;\n\n\tmutex_enter(&mg->mg_lock);\n\n\t \n\tfor (msp = avl_first(t); msp != NULL; msp = AVL_NEXT(t, msp)) {\n\t\tASSERT3P(msp->ms_group, ==, mg);\n\n\t\t \n\t\tif (++m > metaslab_preload_limit && !msp->ms_condense_wanted) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tVERIFY(taskq_dispatch(spa->spa_metaslab_taskq, metaslab_preload,\n\t\t    msp, TQ_SLEEP | (m <= mg->mg_allocators ? TQ_FRONT : 0))\n\t\t    != TASKQID_INVALID);\n\t}\n\tmutex_exit(&mg->mg_lock);\n}\n\n \nstatic boolean_t\nmetaslab_should_condense(metaslab_t *msp)\n{\n\tspace_map_t *sm = msp->ms_sm;\n\tvdev_t *vd = msp->ms_group->mg_vd;\n\tuint64_t vdev_blocksize = 1ULL << vd->vdev_ashift;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT(msp->ms_loaded);\n\tASSERT(sm != NULL);\n\tASSERT3U(spa_sync_pass(vd->vdev_spa), ==, 1);\n\n\t \n\tif (range_tree_numsegs(msp->ms_allocatable) == 0 ||\n\t    msp->ms_condense_wanted)\n\t\treturn (B_TRUE);\n\n\tuint64_t record_size = MAX(sm->sm_blksz, vdev_blocksize);\n\tuint64_t object_size = space_map_length(sm);\n\tuint64_t optimal_size = space_map_estimate_optimal_size(sm,\n\t    msp->ms_allocatable, SM_NO_VDEVID);\n\n\treturn (object_size >= (optimal_size * zfs_condense_pct / 100) &&\n\t    object_size > zfs_metaslab_condense_block_threshold * record_size);\n}\n\n \nstatic void\nmetaslab_condense(metaslab_t *msp, dmu_tx_t *tx)\n{\n\trange_tree_t *condense_tree;\n\tspace_map_t *sm = msp->ms_sm;\n\tuint64_t txg = dmu_tx_get_txg(tx);\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT(msp->ms_loaded);\n\tASSERT(msp->ms_sm != NULL);\n\n\t \n\tASSERT3U(spa_sync_pass(spa), ==, 1);\n\tASSERT(range_tree_is_empty(msp->ms_freed));  \n\n\tzfs_dbgmsg(\"condensing: txg %llu, msp[%llu] %px, vdev id %llu, \"\n\t    \"spa %s, smp size %llu, segments %llu, forcing condense=%s\",\n\t    (u_longlong_t)txg, (u_longlong_t)msp->ms_id, msp,\n\t    (u_longlong_t)msp->ms_group->mg_vd->vdev_id,\n\t    spa->spa_name, (u_longlong_t)space_map_length(msp->ms_sm),\n\t    (u_longlong_t)range_tree_numsegs(msp->ms_allocatable),\n\t    msp->ms_condense_wanted ? \"TRUE\" : \"FALSE\");\n\n\tmsp->ms_condense_wanted = B_FALSE;\n\n\trange_seg_type_t type;\n\tuint64_t shift, start;\n\ttype = metaslab_calculate_range_tree_type(msp->ms_group->mg_vd, msp,\n\t    &start, &shift);\n\n\tcondense_tree = range_tree_create(NULL, type, NULL, start, shift);\n\n\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\trange_tree_walk(msp->ms_defer[t],\n\t\t    range_tree_add, condense_tree);\n\t}\n\n\tfor (int t = 0; t < TXG_CONCURRENT_STATES; t++) {\n\t\trange_tree_walk(msp->ms_allocating[(txg + t) & TXG_MASK],\n\t\t    range_tree_add, condense_tree);\n\t}\n\n\tASSERT3U(spa->spa_unflushed_stats.sus_memused, >=,\n\t    metaslab_unflushed_changes_memused(msp));\n\tspa->spa_unflushed_stats.sus_memused -=\n\t    metaslab_unflushed_changes_memused(msp);\n\trange_tree_vacate(msp->ms_unflushed_allocs, NULL, NULL);\n\trange_tree_vacate(msp->ms_unflushed_frees, NULL, NULL);\n\n\t \n\tmsp->ms_condensing = B_TRUE;\n\n\tmutex_exit(&msp->ms_lock);\n\tuint64_t object = space_map_object(msp->ms_sm);\n\tspace_map_truncate(sm,\n\t    spa_feature_is_enabled(spa, SPA_FEATURE_LOG_SPACEMAP) ?\n\t    zfs_metaslab_sm_blksz_with_log : zfs_metaslab_sm_blksz_no_log, tx);\n\n\t \n\tif (space_map_object(msp->ms_sm) != object) {\n\t\tobject = space_map_object(msp->ms_sm);\n\t\tdmu_write(spa->spa_meta_objset,\n\t\t    msp->ms_group->mg_vd->vdev_ms_array, sizeof (uint64_t) *\n\t\t    msp->ms_id, sizeof (uint64_t), &object, tx);\n\t}\n\n\t \n\trange_tree_t *tmp_tree = range_tree_create(NULL, type, NULL, start,\n\t    shift);\n\trange_tree_add(tmp_tree, msp->ms_start, msp->ms_size);\n\tspace_map_write(sm, tmp_tree, SM_ALLOC, SM_NO_VDEVID, tx);\n\tspace_map_write(sm, msp->ms_allocatable, SM_FREE, SM_NO_VDEVID, tx);\n\tspace_map_write(sm, condense_tree, SM_FREE, SM_NO_VDEVID, tx);\n\n\trange_tree_vacate(condense_tree, NULL, NULL);\n\trange_tree_destroy(condense_tree);\n\trange_tree_vacate(tmp_tree, NULL, NULL);\n\trange_tree_destroy(tmp_tree);\n\tmutex_enter(&msp->ms_lock);\n\n\tmsp->ms_condensing = B_FALSE;\n\tmetaslab_flush_update(msp, tx);\n}\n\nstatic void\nmetaslab_unflushed_add(metaslab_t *msp, dmu_tx_t *tx)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\tASSERT(spa_syncing_log_sm(spa) != NULL);\n\tASSERT(msp->ms_sm != NULL);\n\tASSERT(range_tree_is_empty(msp->ms_unflushed_allocs));\n\tASSERT(range_tree_is_empty(msp->ms_unflushed_frees));\n\n\tmutex_enter(&spa->spa_flushed_ms_lock);\n\tmetaslab_set_unflushed_txg(msp, spa_syncing_txg(spa), tx);\n\tmetaslab_set_unflushed_dirty(msp, B_TRUE);\n\tavl_add(&spa->spa_metaslabs_by_flushed, msp);\n\tmutex_exit(&spa->spa_flushed_ms_lock);\n\n\tspa_log_sm_increment_current_mscount(spa);\n\tspa_log_summary_add_flushed_metaslab(spa, B_TRUE);\n}\n\nvoid\nmetaslab_unflushed_bump(metaslab_t *msp, dmu_tx_t *tx, boolean_t dirty)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\tASSERT(spa_syncing_log_sm(spa) != NULL);\n\tASSERT(msp->ms_sm != NULL);\n\tASSERT(metaslab_unflushed_txg(msp) != 0);\n\tASSERT3P(avl_find(&spa->spa_metaslabs_by_flushed, msp, NULL), ==, msp);\n\tASSERT(range_tree_is_empty(msp->ms_unflushed_allocs));\n\tASSERT(range_tree_is_empty(msp->ms_unflushed_frees));\n\n\tVERIFY3U(tx->tx_txg, <=, spa_final_dirty_txg(spa));\n\n\t \n\tuint64_t ms_prev_flushed_txg = metaslab_unflushed_txg(msp);\n\tboolean_t ms_prev_flushed_dirty = metaslab_unflushed_dirty(msp);\n\tmutex_enter(&spa->spa_flushed_ms_lock);\n\tavl_remove(&spa->spa_metaslabs_by_flushed, msp);\n\tmetaslab_set_unflushed_txg(msp, spa_syncing_txg(spa), tx);\n\tmetaslab_set_unflushed_dirty(msp, dirty);\n\tavl_add(&spa->spa_metaslabs_by_flushed, msp);\n\tmutex_exit(&spa->spa_flushed_ms_lock);\n\n\t \n\tspa_log_sm_decrement_mscount(spa, ms_prev_flushed_txg);\n\tspa_log_sm_increment_current_mscount(spa);\n\n\t \n\tspa_log_summary_decrement_mscount(spa, ms_prev_flushed_txg,\n\t    ms_prev_flushed_dirty);\n\tspa_log_summary_add_flushed_metaslab(spa, dirty);\n\n\t \n\tspa_cleanup_old_sm_logs(spa, tx);\n}\n\n \nstatic void\nmetaslab_flush_update(metaslab_t *msp, dmu_tx_t *tx)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tspa_t *spa = mg->mg_vd->vdev_spa;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tASSERT3U(spa_sync_pass(spa), ==, 1);\n\n\t \n\tmsp->ms_synced_length = space_map_length(msp->ms_sm);\n\n\t \n\tif (!spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP) ||\n\t    metaslab_unflushed_txg(msp) == 0)\n\t\treturn;\n\n\tmetaslab_unflushed_bump(msp, tx, B_FALSE);\n}\n\nboolean_t\nmetaslab_flush(metaslab_t *msp, dmu_tx_t *tx)\n{\n\tspa_t *spa = msp->ms_group->mg_vd->vdev_spa;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tASSERT3U(spa_sync_pass(spa), ==, 1);\n\tASSERT(spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP));\n\n\tASSERT(msp->ms_sm != NULL);\n\tASSERT(metaslab_unflushed_txg(msp) != 0);\n\tASSERT(avl_find(&spa->spa_metaslabs_by_flushed, msp, NULL) != NULL);\n\n\t \n\tASSERT3U(metaslab_unflushed_txg(msp), <, dmu_tx_get_txg(tx));\n\n\t \n\tif (msp->ms_loading)\n\t\treturn (B_FALSE);\n\n\tmetaslab_verify_space(msp, dmu_tx_get_txg(tx));\n\tmetaslab_verify_weight_and_frag(msp);\n\n\t \n\tif (msp->ms_loaded && metaslab_should_condense(msp)) {\n\t\tmetaslab_group_t *mg = msp->ms_group;\n\n\t\t \n\t\tmetaslab_group_histogram_verify(mg);\n\t\tmetaslab_class_histogram_verify(mg->mg_class);\n\t\tmetaslab_group_histogram_remove(mg, msp);\n\n\t\tmetaslab_condense(msp, tx);\n\n\t\tspace_map_histogram_clear(msp->ms_sm);\n\t\tspace_map_histogram_add(msp->ms_sm, msp->ms_allocatable, tx);\n\t\tASSERT(range_tree_is_empty(msp->ms_freed));\n\t\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\t\tspace_map_histogram_add(msp->ms_sm,\n\t\t\t    msp->ms_defer[t], tx);\n\t\t}\n\t\tmetaslab_aux_histograms_update(msp);\n\n\t\tmetaslab_group_histogram_add(mg, msp);\n\t\tmetaslab_group_histogram_verify(mg);\n\t\tmetaslab_class_histogram_verify(mg->mg_class);\n\n\t\tmetaslab_verify_space(msp, dmu_tx_get_txg(tx));\n\n\t\t \n\t\tmetaslab_recalculate_weight_and_sort(msp);\n\t\treturn (B_TRUE);\n\t}\n\n\tmsp->ms_flushing = B_TRUE;\n\tuint64_t sm_len_before = space_map_length(msp->ms_sm);\n\n\tmutex_exit(&msp->ms_lock);\n\tspace_map_write(msp->ms_sm, msp->ms_unflushed_allocs, SM_ALLOC,\n\t    SM_NO_VDEVID, tx);\n\tspace_map_write(msp->ms_sm, msp->ms_unflushed_frees, SM_FREE,\n\t    SM_NO_VDEVID, tx);\n\tmutex_enter(&msp->ms_lock);\n\n\tuint64_t sm_len_after = space_map_length(msp->ms_sm);\n\tif (zfs_flags & ZFS_DEBUG_LOG_SPACEMAP) {\n\t\tzfs_dbgmsg(\"flushing: txg %llu, spa %s, vdev_id %llu, \"\n\t\t    \"ms_id %llu, unflushed_allocs %llu, unflushed_frees %llu, \"\n\t\t    \"appended %llu bytes\", (u_longlong_t)dmu_tx_get_txg(tx),\n\t\t    spa_name(spa),\n\t\t    (u_longlong_t)msp->ms_group->mg_vd->vdev_id,\n\t\t    (u_longlong_t)msp->ms_id,\n\t\t    (u_longlong_t)range_tree_space(msp->ms_unflushed_allocs),\n\t\t    (u_longlong_t)range_tree_space(msp->ms_unflushed_frees),\n\t\t    (u_longlong_t)(sm_len_after - sm_len_before));\n\t}\n\n\tASSERT3U(spa->spa_unflushed_stats.sus_memused, >=,\n\t    metaslab_unflushed_changes_memused(msp));\n\tspa->spa_unflushed_stats.sus_memused -=\n\t    metaslab_unflushed_changes_memused(msp);\n\trange_tree_vacate(msp->ms_unflushed_allocs, NULL, NULL);\n\trange_tree_vacate(msp->ms_unflushed_frees, NULL, NULL);\n\n\tmetaslab_verify_space(msp, dmu_tx_get_txg(tx));\n\tmetaslab_verify_weight_and_frag(msp);\n\n\tmetaslab_flush_update(msp, tx);\n\n\tmetaslab_verify_space(msp, dmu_tx_get_txg(tx));\n\tmetaslab_verify_weight_and_frag(msp);\n\n\tmsp->ms_flushing = B_FALSE;\n\tcv_broadcast(&msp->ms_flush_cv);\n\treturn (B_TRUE);\n}\n\n \nvoid\nmetaslab_sync(metaslab_t *msp, uint64_t txg)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tvdev_t *vd = mg->mg_vd;\n\tspa_t *spa = vd->vdev_spa;\n\tobjset_t *mos = spa_meta_objset(spa);\n\trange_tree_t *alloctree = msp->ms_allocating[txg & TXG_MASK];\n\tdmu_tx_t *tx;\n\n\tASSERT(!vd->vdev_ishole);\n\n\t \n\tif (msp->ms_new) {\n\t\tASSERT0(range_tree_space(alloctree));\n\t\tASSERT0(range_tree_space(msp->ms_freeing));\n\t\tASSERT0(range_tree_space(msp->ms_freed));\n\t\tASSERT0(range_tree_space(msp->ms_checkpointing));\n\t\tASSERT0(range_tree_space(msp->ms_trim));\n\t\treturn;\n\t}\n\n\t \n\tif (range_tree_is_empty(alloctree) &&\n\t    range_tree_is_empty(msp->ms_freeing) &&\n\t    range_tree_is_empty(msp->ms_checkpointing) &&\n\t    !(msp->ms_loaded && msp->ms_condense_wanted &&\n\t    txg <= spa_final_dirty_txg(spa)))\n\t\treturn;\n\n\n\tVERIFY3U(txg, <=, spa_final_dirty_txg(spa));\n\n\t \n\ttx = dmu_tx_create_assigned(spa_get_dsl(spa), txg);\n\n\t \n\tspa_generate_syncing_log_sm(spa, tx);\n\n\tif (msp->ms_sm == NULL) {\n\t\tuint64_t new_object = space_map_alloc(mos,\n\t\t    spa_feature_is_enabled(spa, SPA_FEATURE_LOG_SPACEMAP) ?\n\t\t    zfs_metaslab_sm_blksz_with_log :\n\t\t    zfs_metaslab_sm_blksz_no_log, tx);\n\t\tVERIFY3U(new_object, !=, 0);\n\n\t\tdmu_write(mos, vd->vdev_ms_array, sizeof (uint64_t) *\n\t\t    msp->ms_id, sizeof (uint64_t), &new_object, tx);\n\n\t\tVERIFY0(space_map_open(&msp->ms_sm, mos, new_object,\n\t\t    msp->ms_start, msp->ms_size, vd->vdev_ashift));\n\t\tASSERT(msp->ms_sm != NULL);\n\n\t\tASSERT(range_tree_is_empty(msp->ms_unflushed_allocs));\n\t\tASSERT(range_tree_is_empty(msp->ms_unflushed_frees));\n\t\tASSERT0(metaslab_allocated_space(msp));\n\t}\n\n\tif (!range_tree_is_empty(msp->ms_checkpointing) &&\n\t    vd->vdev_checkpoint_sm == NULL) {\n\t\tASSERT(spa_has_checkpoint(spa));\n\n\t\tuint64_t new_object = space_map_alloc(mos,\n\t\t    zfs_vdev_standard_sm_blksz, tx);\n\t\tVERIFY3U(new_object, !=, 0);\n\n\t\tVERIFY0(space_map_open(&vd->vdev_checkpoint_sm,\n\t\t    mos, new_object, 0, vd->vdev_asize, vd->vdev_ashift));\n\t\tASSERT3P(vd->vdev_checkpoint_sm, !=, NULL);\n\n\t\t \n\t\tVERIFY0(zap_add(vd->vdev_spa->spa_meta_objset,\n\t\t    vd->vdev_top_zap, VDEV_TOP_ZAP_POOL_CHECKPOINT_SM,\n\t\t    sizeof (new_object), 1, &new_object, tx));\n\t}\n\n\tmutex_enter(&msp->ms_sync_lock);\n\tmutex_enter(&msp->ms_lock);\n\n\t \n\tmetaslab_group_histogram_verify(mg);\n\tmetaslab_class_histogram_verify(mg->mg_class);\n\tmetaslab_group_histogram_remove(mg, msp);\n\n\tif (spa->spa_sync_pass == 1 && msp->ms_loaded &&\n\t    metaslab_should_condense(msp))\n\t\tmetaslab_condense(msp, tx);\n\n\t \n\tmutex_exit(&msp->ms_lock);\n\tspace_map_t *log_sm = spa_syncing_log_sm(spa);\n\tif (log_sm != NULL) {\n\t\tASSERT(spa_feature_is_enabled(spa, SPA_FEATURE_LOG_SPACEMAP));\n\t\tif (metaslab_unflushed_txg(msp) == 0)\n\t\t\tmetaslab_unflushed_add(msp, tx);\n\t\telse if (!metaslab_unflushed_dirty(msp))\n\t\t\tmetaslab_unflushed_bump(msp, tx, B_TRUE);\n\n\t\tspace_map_write(log_sm, alloctree, SM_ALLOC,\n\t\t    vd->vdev_id, tx);\n\t\tspace_map_write(log_sm, msp->ms_freeing, SM_FREE,\n\t\t    vd->vdev_id, tx);\n\t\tmutex_enter(&msp->ms_lock);\n\n\t\tASSERT3U(spa->spa_unflushed_stats.sus_memused, >=,\n\t\t    metaslab_unflushed_changes_memused(msp));\n\t\tspa->spa_unflushed_stats.sus_memused -=\n\t\t    metaslab_unflushed_changes_memused(msp);\n\t\trange_tree_remove_xor_add(alloctree,\n\t\t    msp->ms_unflushed_frees, msp->ms_unflushed_allocs);\n\t\trange_tree_remove_xor_add(msp->ms_freeing,\n\t\t    msp->ms_unflushed_allocs, msp->ms_unflushed_frees);\n\t\tspa->spa_unflushed_stats.sus_memused +=\n\t\t    metaslab_unflushed_changes_memused(msp);\n\t} else {\n\t\tASSERT(!spa_feature_is_enabled(spa, SPA_FEATURE_LOG_SPACEMAP));\n\n\t\tspace_map_write(msp->ms_sm, alloctree, SM_ALLOC,\n\t\t    SM_NO_VDEVID, tx);\n\t\tspace_map_write(msp->ms_sm, msp->ms_freeing, SM_FREE,\n\t\t    SM_NO_VDEVID, tx);\n\t\tmutex_enter(&msp->ms_lock);\n\t}\n\n\tmsp->ms_allocated_space += range_tree_space(alloctree);\n\tASSERT3U(msp->ms_allocated_space, >=,\n\t    range_tree_space(msp->ms_freeing));\n\tmsp->ms_allocated_space -= range_tree_space(msp->ms_freeing);\n\n\tif (!range_tree_is_empty(msp->ms_checkpointing)) {\n\t\tASSERT(spa_has_checkpoint(spa));\n\t\tASSERT3P(vd->vdev_checkpoint_sm, !=, NULL);\n\n\t\t \n\t\tmutex_exit(&msp->ms_lock);\n\t\tspace_map_write(vd->vdev_checkpoint_sm,\n\t\t    msp->ms_checkpointing, SM_FREE, SM_NO_VDEVID, tx);\n\t\tmutex_enter(&msp->ms_lock);\n\n\t\tspa->spa_checkpoint_info.sci_dspace +=\n\t\t    range_tree_space(msp->ms_checkpointing);\n\t\tvd->vdev_stat.vs_checkpoint_space +=\n\t\t    range_tree_space(msp->ms_checkpointing);\n\t\tASSERT3U(vd->vdev_stat.vs_checkpoint_space, ==,\n\t\t    -space_map_allocated(vd->vdev_checkpoint_sm));\n\n\t\trange_tree_vacate(msp->ms_checkpointing, NULL, NULL);\n\t}\n\n\tif (msp->ms_loaded) {\n\t\t \n\t\tspace_map_histogram_clear(msp->ms_sm);\n\t\tspace_map_histogram_add(msp->ms_sm, msp->ms_allocatable, tx);\n\n\t\t \n\t\tspace_map_histogram_add(msp->ms_sm, msp->ms_freed, tx);\n\n\t\t \n\t\tfor (int t = 0; t < TXG_DEFER_SIZE; t++) {\n\t\t\tspace_map_histogram_add(msp->ms_sm,\n\t\t\t    msp->ms_defer[t], tx);\n\t\t}\n\t}\n\n\t \n\tspace_map_histogram_add(msp->ms_sm, msp->ms_freeing, tx);\n\tmetaslab_aux_histograms_update(msp);\n\n\tmetaslab_group_histogram_add(mg, msp);\n\tmetaslab_group_histogram_verify(mg);\n\tmetaslab_class_histogram_verify(mg->mg_class);\n\n\t \n\tif (spa_sync_pass(spa) == 1) {\n\t\trange_tree_swap(&msp->ms_freeing, &msp->ms_freed);\n\t\tASSERT0(msp->ms_allocated_this_txg);\n\t} else {\n\t\trange_tree_vacate(msp->ms_freeing,\n\t\t    range_tree_add, msp->ms_freed);\n\t}\n\tmsp->ms_allocated_this_txg += range_tree_space(alloctree);\n\trange_tree_vacate(alloctree, NULL, NULL);\n\n\tASSERT0(range_tree_space(msp->ms_allocating[txg & TXG_MASK]));\n\tASSERT0(range_tree_space(msp->ms_allocating[TXG_CLEAN(txg)\n\t    & TXG_MASK]));\n\tASSERT0(range_tree_space(msp->ms_freeing));\n\tASSERT0(range_tree_space(msp->ms_checkpointing));\n\n\tmutex_exit(&msp->ms_lock);\n\n\t \n\tuint64_t object;\n\tVERIFY0(dmu_read(mos, vd->vdev_ms_array,\n\t    msp->ms_id * sizeof (uint64_t), sizeof (uint64_t), &object, 0));\n\tVERIFY3U(object, ==, space_map_object(msp->ms_sm));\n\n\tmutex_exit(&msp->ms_sync_lock);\n\tdmu_tx_commit(tx);\n}\n\nstatic void\nmetaslab_evict(metaslab_t *msp, uint64_t txg)\n{\n\tif (!msp->ms_loaded || msp->ms_disabled != 0)\n\t\treturn;\n\n\tfor (int t = 1; t < TXG_CONCURRENT_STATES; t++) {\n\t\tVERIFY0(range_tree_space(\n\t\t    msp->ms_allocating[(txg + t) & TXG_MASK]));\n\t}\n\tif (msp->ms_allocator != -1)\n\t\tmetaslab_passivate(msp, msp->ms_weight & ~METASLAB_ACTIVE_MASK);\n\n\tif (!metaslab_debug_unload)\n\t\tmetaslab_unload(msp);\n}\n\n \nvoid\nmetaslab_sync_done(metaslab_t *msp, uint64_t txg)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tvdev_t *vd = mg->mg_vd;\n\tspa_t *spa = vd->vdev_spa;\n\trange_tree_t **defer_tree;\n\tint64_t alloc_delta, defer_delta;\n\tboolean_t defer_allowed = B_TRUE;\n\n\tASSERT(!vd->vdev_ishole);\n\n\tmutex_enter(&msp->ms_lock);\n\n\tif (msp->ms_new) {\n\t\t \n\t\tmetaslab_space_update(vd, mg->mg_class, 0, 0, msp->ms_size);\n\n\t\t \n\t\tVERIFY0(msp->ms_allocated_this_txg);\n\t\tVERIFY0(range_tree_space(msp->ms_freed));\n\t}\n\n\tASSERT0(range_tree_space(msp->ms_freeing));\n\tASSERT0(range_tree_space(msp->ms_checkpointing));\n\n\tdefer_tree = &msp->ms_defer[txg % TXG_DEFER_SIZE];\n\n\tuint64_t free_space = metaslab_class_get_space(spa_normal_class(spa)) -\n\t    metaslab_class_get_alloc(spa_normal_class(spa));\n\tif (free_space <= spa_get_slop_space(spa) || vd->vdev_removing) {\n\t\tdefer_allowed = B_FALSE;\n\t}\n\n\tdefer_delta = 0;\n\talloc_delta = msp->ms_allocated_this_txg -\n\t    range_tree_space(msp->ms_freed);\n\n\tif (defer_allowed) {\n\t\tdefer_delta = range_tree_space(msp->ms_freed) -\n\t\t    range_tree_space(*defer_tree);\n\t} else {\n\t\tdefer_delta -= range_tree_space(*defer_tree);\n\t}\n\tmetaslab_space_update(vd, mg->mg_class, alloc_delta + defer_delta,\n\t    defer_delta, 0);\n\n\tif (spa_syncing_log_sm(spa) == NULL) {\n\t\t \n\t\tmetaslab_load_wait(msp);\n\t} else {\n\t\tASSERT(spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP));\n\t}\n\n\t \n\tif (spa_get_autotrim(spa) == SPA_AUTOTRIM_ON) {\n\t\trange_tree_walk(*defer_tree, range_tree_add, msp->ms_trim);\n\t\tif (!defer_allowed) {\n\t\t\trange_tree_walk(msp->ms_freed, range_tree_add,\n\t\t\t    msp->ms_trim);\n\t\t}\n\t} else {\n\t\trange_tree_vacate(msp->ms_trim, NULL, NULL);\n\t}\n\n\t \n\trange_tree_vacate(*defer_tree,\n\t    msp->ms_loaded ? range_tree_add : NULL, msp->ms_allocatable);\n\tif (defer_allowed) {\n\t\trange_tree_swap(&msp->ms_freed, defer_tree);\n\t} else {\n\t\trange_tree_vacate(msp->ms_freed,\n\t\t    msp->ms_loaded ? range_tree_add : NULL,\n\t\t    msp->ms_allocatable);\n\t}\n\n\tmsp->ms_synced_length = space_map_length(msp->ms_sm);\n\n\tmsp->ms_deferspace += defer_delta;\n\tASSERT3S(msp->ms_deferspace, >=, 0);\n\tASSERT3S(msp->ms_deferspace, <=, msp->ms_size);\n\tif (msp->ms_deferspace != 0) {\n\t\t \n\t\tvdev_dirty(vd, VDD_METASLAB, msp, txg + 1);\n\t}\n\tmetaslab_aux_histograms_update_done(msp, defer_allowed);\n\n\tif (msp->ms_new) {\n\t\tmsp->ms_new = B_FALSE;\n\t\tmutex_enter(&mg->mg_lock);\n\t\tmg->mg_ms_ready++;\n\t\tmutex_exit(&mg->mg_lock);\n\t}\n\n\t \n\tmetaslab_recalculate_weight_and_sort(msp);\n\n\tASSERT0(range_tree_space(msp->ms_allocating[txg & TXG_MASK]));\n\tASSERT0(range_tree_space(msp->ms_freeing));\n\tASSERT0(range_tree_space(msp->ms_freed));\n\tASSERT0(range_tree_space(msp->ms_checkpointing));\n\tmsp->ms_allocating_total -= msp->ms_allocated_this_txg;\n\tmsp->ms_allocated_this_txg = 0;\n\tmutex_exit(&msp->ms_lock);\n}\n\nvoid\nmetaslab_sync_reassess(metaslab_group_t *mg)\n{\n\tspa_t *spa = mg->mg_class->mc_spa;\n\n\tspa_config_enter(spa, SCL_ALLOC, FTAG, RW_READER);\n\tmetaslab_group_alloc_update(mg);\n\tmg->mg_fragmentation = metaslab_group_fragmentation(mg);\n\n\t \n\tif (mg->mg_activation_count > 0) {\n\t\tmetaslab_group_preload(mg);\n\t}\n\tspa_config_exit(spa, SCL_ALLOC, FTAG);\n}\n\n \nstatic boolean_t\nmetaslab_is_unique(metaslab_t *msp, dva_t *dva)\n{\n\tuint64_t dva_ms_id;\n\n\tif (DVA_GET_ASIZE(dva) == 0)\n\t\treturn (B_TRUE);\n\n\tif (msp->ms_group->mg_vd->vdev_id != DVA_GET_VDEV(dva))\n\t\treturn (B_TRUE);\n\n\tdva_ms_id = DVA_GET_OFFSET(dva) >> msp->ms_group->mg_vd->vdev_ms_shift;\n\n\treturn (msp->ms_id != dva_ms_id);\n}\n\n \n\n \nstatic void\nmetaslab_trace_add(zio_alloc_list_t *zal, metaslab_group_t *mg,\n    metaslab_t *msp, uint64_t psize, uint32_t dva_id, uint64_t offset,\n    int allocator)\n{\n\tmetaslab_alloc_trace_t *mat;\n\n\tif (!metaslab_trace_enabled)\n\t\treturn;\n\n\t \n\tif (zal->zal_size == metaslab_trace_max_entries) {\n\t\tmetaslab_alloc_trace_t *mat_next;\n#ifdef ZFS_DEBUG\n\t\tpanic(\"too many entries in allocation list\");\n#endif\n\t\tMETASLABSTAT_BUMP(metaslabstat_trace_over_limit);\n\t\tzal->zal_size--;\n\t\tmat_next = list_next(&zal->zal_list, list_head(&zal->zal_list));\n\t\tlist_remove(&zal->zal_list, mat_next);\n\t\tkmem_cache_free(metaslab_alloc_trace_cache, mat_next);\n\t}\n\n\tmat = kmem_cache_alloc(metaslab_alloc_trace_cache, KM_SLEEP);\n\tlist_link_init(&mat->mat_list_node);\n\tmat->mat_mg = mg;\n\tmat->mat_msp = msp;\n\tmat->mat_size = psize;\n\tmat->mat_dva_id = dva_id;\n\tmat->mat_offset = offset;\n\tmat->mat_weight = 0;\n\tmat->mat_allocator = allocator;\n\n\tif (msp != NULL)\n\t\tmat->mat_weight = msp->ms_weight;\n\n\t \n\tlist_insert_tail(&zal->zal_list, mat);\n\tzal->zal_size++;\n\n\tASSERT3U(zal->zal_size, <=, metaslab_trace_max_entries);\n}\n\nvoid\nmetaslab_trace_init(zio_alloc_list_t *zal)\n{\n\tlist_create(&zal->zal_list, sizeof (metaslab_alloc_trace_t),\n\t    offsetof(metaslab_alloc_trace_t, mat_list_node));\n\tzal->zal_size = 0;\n}\n\nvoid\nmetaslab_trace_fini(zio_alloc_list_t *zal)\n{\n\tmetaslab_alloc_trace_t *mat;\n\n\twhile ((mat = list_remove_head(&zal->zal_list)) != NULL)\n\t\tkmem_cache_free(metaslab_alloc_trace_cache, mat);\n\tlist_destroy(&zal->zal_list);\n\tzal->zal_size = 0;\n}\n\n \n\nstatic void\nmetaslab_group_alloc_increment(spa_t *spa, uint64_t vdev, const void *tag,\n    int flags, int allocator)\n{\n\tif (!(flags & METASLAB_ASYNC_ALLOC) ||\n\t    (flags & METASLAB_DONT_THROTTLE))\n\t\treturn;\n\n\tmetaslab_group_t *mg = vdev_lookup_top(spa, vdev)->vdev_mg;\n\tif (!mg->mg_class->mc_alloc_throttle_enabled)\n\t\treturn;\n\n\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\t(void) zfs_refcount_add(&mga->mga_alloc_queue_depth, tag);\n}\n\nstatic void\nmetaslab_group_increment_qdepth(metaslab_group_t *mg, int allocator)\n{\n\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\tmetaslab_class_allocator_t *mca =\n\t    &mg->mg_class->mc_allocator[allocator];\n\tuint64_t max = mg->mg_max_alloc_queue_depth;\n\tuint64_t cur = mga->mga_cur_max_alloc_queue_depth;\n\twhile (cur < max) {\n\t\tif (atomic_cas_64(&mga->mga_cur_max_alloc_queue_depth,\n\t\t    cur, cur + 1) == cur) {\n\t\t\tatomic_inc_64(&mca->mca_alloc_max_slots);\n\t\t\treturn;\n\t\t}\n\t\tcur = mga->mga_cur_max_alloc_queue_depth;\n\t}\n}\n\nvoid\nmetaslab_group_alloc_decrement(spa_t *spa, uint64_t vdev, const void *tag,\n    int flags, int allocator, boolean_t io_complete)\n{\n\tif (!(flags & METASLAB_ASYNC_ALLOC) ||\n\t    (flags & METASLAB_DONT_THROTTLE))\n\t\treturn;\n\n\tmetaslab_group_t *mg = vdev_lookup_top(spa, vdev)->vdev_mg;\n\tif (!mg->mg_class->mc_alloc_throttle_enabled)\n\t\treturn;\n\n\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\t(void) zfs_refcount_remove(&mga->mga_alloc_queue_depth, tag);\n\tif (io_complete)\n\t\tmetaslab_group_increment_qdepth(mg, allocator);\n}\n\nvoid\nmetaslab_group_alloc_verify(spa_t *spa, const blkptr_t *bp, const void *tag,\n    int allocator)\n{\n#ifdef ZFS_DEBUG\n\tconst dva_t *dva = bp->blk_dva;\n\tint ndvas = BP_GET_NDVAS(bp);\n\n\tfor (int d = 0; d < ndvas; d++) {\n\t\tuint64_t vdev = DVA_GET_VDEV(&dva[d]);\n\t\tmetaslab_group_t *mg = vdev_lookup_top(spa, vdev)->vdev_mg;\n\t\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\t\tVERIFY(zfs_refcount_not_held(&mga->mga_alloc_queue_depth, tag));\n\t}\n#endif\n}\n\nstatic uint64_t\nmetaslab_block_alloc(metaslab_t *msp, uint64_t size, uint64_t txg)\n{\n\tuint64_t start;\n\trange_tree_t *rt = msp->ms_allocatable;\n\tmetaslab_class_t *mc = msp->ms_group->mg_class;\n\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\tVERIFY(!msp->ms_condensing);\n\tVERIFY0(msp->ms_disabled);\n\n\tstart = mc->mc_ops->msop_alloc(msp, size);\n\tif (start != -1ULL) {\n\t\tmetaslab_group_t *mg = msp->ms_group;\n\t\tvdev_t *vd = mg->mg_vd;\n\n\t\tVERIFY0(P2PHASE(start, 1ULL << vd->vdev_ashift));\n\t\tVERIFY0(P2PHASE(size, 1ULL << vd->vdev_ashift));\n\t\tVERIFY3U(range_tree_space(rt) - size, <=, msp->ms_size);\n\t\trange_tree_remove(rt, start, size);\n\t\trange_tree_clear(msp->ms_trim, start, size);\n\n\t\tif (range_tree_is_empty(msp->ms_allocating[txg & TXG_MASK]))\n\t\t\tvdev_dirty(mg->mg_vd, VDD_METASLAB, msp, txg);\n\n\t\trange_tree_add(msp->ms_allocating[txg & TXG_MASK], start, size);\n\t\tmsp->ms_allocating_total += size;\n\n\t\t \n\t\tmsp->ms_alloc_txg = txg;\n\t\tmetaslab_verify_space(msp, txg);\n\t}\n\n\t \n\tmsp->ms_max_size = metaslab_largest_allocatable(msp);\n\treturn (start);\n}\n\n \nstatic metaslab_t *\nfind_valid_metaslab(metaslab_group_t *mg, uint64_t activation_weight,\n    dva_t *dva, int d, boolean_t want_unique, uint64_t asize, int allocator,\n    boolean_t try_hard, zio_alloc_list_t *zal, metaslab_t *search,\n    boolean_t *was_active)\n{\n\tavl_index_t idx;\n\tavl_tree_t *t = &mg->mg_metaslab_tree;\n\tmetaslab_t *msp = avl_find(t, search, &idx);\n\tif (msp == NULL)\n\t\tmsp = avl_nearest(t, idx, AVL_AFTER);\n\n\tuint_t tries = 0;\n\tfor (; msp != NULL; msp = AVL_NEXT(t, msp)) {\n\t\tint i;\n\n\t\tif (!try_hard && tries > zfs_metaslab_find_max_tries) {\n\t\t\tMETASLABSTAT_BUMP(metaslabstat_too_many_tries);\n\t\t\treturn (NULL);\n\t\t}\n\t\ttries++;\n\n\t\tif (!metaslab_should_allocate(msp, asize, try_hard)) {\n\t\t\tmetaslab_trace_add(zal, mg, msp, asize, d,\n\t\t\t    TRACE_TOO_SMALL, allocator);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (msp->ms_condensing || msp->ms_disabled > 0)\n\t\t\tcontinue;\n\n\t\t*was_active = msp->ms_allocator != -1;\n\t\t \n\t\tif (activation_weight == METASLAB_WEIGHT_PRIMARY || *was_active)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < d; i++) {\n\t\t\tif (want_unique &&\n\t\t\t    !metaslab_is_unique(msp, &dva[i]))\n\t\t\t\tbreak;   \n\t\t}\n\t\tif (i == d)\n\t\t\tbreak;\n\t}\n\n\tif (msp != NULL) {\n\t\tsearch->ms_weight = msp->ms_weight;\n\t\tsearch->ms_start = msp->ms_start + 1;\n\t\tsearch->ms_allocator = msp->ms_allocator;\n\t\tsearch->ms_primary = msp->ms_primary;\n\t}\n\treturn (msp);\n}\n\nstatic void\nmetaslab_active_mask_verify(metaslab_t *msp)\n{\n\tASSERT(MUTEX_HELD(&msp->ms_lock));\n\n\tif ((zfs_flags & ZFS_DEBUG_METASLAB_VERIFY) == 0)\n\t\treturn;\n\n\tif ((msp->ms_weight & METASLAB_ACTIVE_MASK) == 0)\n\t\treturn;\n\n\tif (msp->ms_weight & METASLAB_WEIGHT_PRIMARY) {\n\t\tVERIFY0(msp->ms_weight & METASLAB_WEIGHT_SECONDARY);\n\t\tVERIFY0(msp->ms_weight & METASLAB_WEIGHT_CLAIM);\n\t\tVERIFY3S(msp->ms_allocator, !=, -1);\n\t\tVERIFY(msp->ms_primary);\n\t\treturn;\n\t}\n\n\tif (msp->ms_weight & METASLAB_WEIGHT_SECONDARY) {\n\t\tVERIFY0(msp->ms_weight & METASLAB_WEIGHT_PRIMARY);\n\t\tVERIFY0(msp->ms_weight & METASLAB_WEIGHT_CLAIM);\n\t\tVERIFY3S(msp->ms_allocator, !=, -1);\n\t\tVERIFY(!msp->ms_primary);\n\t\treturn;\n\t}\n\n\tif (msp->ms_weight & METASLAB_WEIGHT_CLAIM) {\n\t\tVERIFY0(msp->ms_weight & METASLAB_WEIGHT_PRIMARY);\n\t\tVERIFY0(msp->ms_weight & METASLAB_WEIGHT_SECONDARY);\n\t\tVERIFY3S(msp->ms_allocator, ==, -1);\n\t\treturn;\n\t}\n}\n\nstatic uint64_t\nmetaslab_group_alloc_normal(metaslab_group_t *mg, zio_alloc_list_t *zal,\n    uint64_t asize, uint64_t txg, boolean_t want_unique, dva_t *dva, int d,\n    int allocator, boolean_t try_hard)\n{\n\tmetaslab_t *msp = NULL;\n\tuint64_t offset = -1ULL;\n\n\tuint64_t activation_weight = METASLAB_WEIGHT_PRIMARY;\n\tfor (int i = 0; i < d; i++) {\n\t\tif (activation_weight == METASLAB_WEIGHT_PRIMARY &&\n\t\t    DVA_GET_VDEV(&dva[i]) == mg->mg_vd->vdev_id) {\n\t\t\tactivation_weight = METASLAB_WEIGHT_SECONDARY;\n\t\t} else if (activation_weight == METASLAB_WEIGHT_SECONDARY &&\n\t\t    DVA_GET_VDEV(&dva[i]) == mg->mg_vd->vdev_id) {\n\t\t\tactivation_weight = METASLAB_WEIGHT_CLAIM;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (mg->mg_ms_ready < mg->mg_allocators * 3)\n\t\tallocator = 0;\n\tmetaslab_group_allocator_t *mga = &mg->mg_allocator[allocator];\n\n\tASSERT3U(mg->mg_vd->vdev_ms_count, >=, 2);\n\n\tmetaslab_t *search = kmem_alloc(sizeof (*search), KM_SLEEP);\n\tsearch->ms_weight = UINT64_MAX;\n\tsearch->ms_start = 0;\n\t \n\tsearch->ms_allocator = -1;\n\tsearch->ms_primary = B_TRUE;\n\tfor (;;) {\n\t\tboolean_t was_active = B_FALSE;\n\n\t\tmutex_enter(&mg->mg_lock);\n\n\t\tif (activation_weight == METASLAB_WEIGHT_PRIMARY &&\n\t\t    mga->mga_primary != NULL) {\n\t\t\tmsp = mga->mga_primary;\n\n\t\t\t \n\t\t\tASSERT(msp->ms_primary);\n\t\t\tASSERT3S(msp->ms_allocator, ==, allocator);\n\t\t\tASSERT(msp->ms_loaded);\n\n\t\t\twas_active = B_TRUE;\n\t\t\tASSERT(msp->ms_weight & METASLAB_ACTIVE_MASK);\n\t\t} else if (activation_weight == METASLAB_WEIGHT_SECONDARY &&\n\t\t    mga->mga_secondary != NULL) {\n\t\t\tmsp = mga->mga_secondary;\n\n\t\t\t \n\t\t\tASSERT(!msp->ms_primary);\n\t\t\tASSERT3S(msp->ms_allocator, ==, allocator);\n\t\t\tASSERT(msp->ms_loaded);\n\n\t\t\twas_active = B_TRUE;\n\t\t\tASSERT(msp->ms_weight & METASLAB_ACTIVE_MASK);\n\t\t} else {\n\t\t\tmsp = find_valid_metaslab(mg, activation_weight, dva, d,\n\t\t\t    want_unique, asize, allocator, try_hard, zal,\n\t\t\t    search, &was_active);\n\t\t}\n\n\t\tmutex_exit(&mg->mg_lock);\n\t\tif (msp == NULL) {\n\t\t\tkmem_free(search, sizeof (*search));\n\t\t\treturn (-1ULL);\n\t\t}\n\t\tmutex_enter(&msp->ms_lock);\n\n\t\tmetaslab_active_mask_verify(msp);\n\n\t\t \n#if 0\n\t\tDTRACE_PROBE3(ms__activation__attempt,\n\t\t    metaslab_t *, msp, uint64_t, activation_weight,\n\t\t    boolean_t, was_active);\n#endif\n\n\t\t \n\t\tif (was_active && !(msp->ms_weight & METASLAB_ACTIVE_MASK)) {\n\t\t\tASSERT3S(msp->ms_allocator, ==, -1);\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!was_active && (msp->ms_weight & METASLAB_ACTIVE_MASK) &&\n\t\t    (msp->ms_allocator != -1) &&\n\t\t    (msp->ms_allocator != allocator || ((activation_weight ==\n\t\t    METASLAB_WEIGHT_PRIMARY) != msp->ms_primary))) {\n\t\t\tASSERT(msp->ms_loaded);\n\t\t\tASSERT((msp->ms_weight & METASLAB_WEIGHT_CLAIM) ||\n\t\t\t    msp->ms_allocator != -1);\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (msp->ms_weight & METASLAB_WEIGHT_CLAIM &&\n\t\t    activation_weight != METASLAB_WEIGHT_CLAIM) {\n\t\t\tASSERT(msp->ms_loaded);\n\t\t\tASSERT3S(msp->ms_allocator, ==, -1);\n\t\t\tmetaslab_passivate(msp, msp->ms_weight &\n\t\t\t    ~METASLAB_WEIGHT_CLAIM);\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmetaslab_set_selected_txg(msp, txg);\n\n\t\tint activation_error =\n\t\t    metaslab_activate(msp, allocator, activation_weight);\n\t\tmetaslab_active_mask_verify(msp);\n\n\t\t \n\t\tboolean_t activated;\n\t\tif (activation_error == 0) {\n\t\t\tactivated = B_TRUE;\n\t\t} else if (activation_error == EBUSY ||\n\t\t    activation_error == EEXIST) {\n\t\t\tactivated = B_FALSE;\n\t\t} else {\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tASSERT(msp->ms_loaded);\n\n\t\t \n\t\tif (!metaslab_should_allocate(msp, asize, try_hard)) {\n\t\t\t \n\t\t\tmetaslab_trace_add(zal, mg, msp, asize, d,\n\t\t\t    TRACE_TOO_SMALL, allocator);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (msp->ms_condensing) {\n\t\t\tmetaslab_trace_add(zal, mg, msp, asize, d,\n\t\t\t    TRACE_CONDENSING, allocator);\n\t\t\tif (activated) {\n\t\t\t\tmetaslab_passivate(msp, msp->ms_weight &\n\t\t\t\t    ~METASLAB_ACTIVE_MASK);\n\t\t\t}\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t} else if (msp->ms_disabled > 0) {\n\t\t\tmetaslab_trace_add(zal, mg, msp, asize, d,\n\t\t\t    TRACE_DISABLED, allocator);\n\t\t\tif (activated) {\n\t\t\t\tmetaslab_passivate(msp, msp->ms_weight &\n\t\t\t\t    ~METASLAB_ACTIVE_MASK);\n\t\t\t}\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\toffset = metaslab_block_alloc(msp, asize, txg);\n\t\tmetaslab_trace_add(zal, mg, msp, asize, d, offset, allocator);\n\n\t\tif (offset != -1ULL) {\n\t\t\t \n\t\t\tif (activated)\n\t\t\t\tmetaslab_segment_may_passivate(msp);\n\t\t\tbreak;\n\t\t}\nnext:\n\t\tASSERT(msp->ms_loaded);\n\n\t\t \n#if 0\n\t\tDTRACE_PROBE2(ms__alloc__failure, metaslab_t *, msp,\n\t\t    uint64_t, asize);\n#endif\n\n\t\t \n\t\tuint64_t weight;\n\t\tif (WEIGHT_IS_SPACEBASED(msp->ms_weight)) {\n\t\t\tweight = metaslab_largest_allocatable(msp);\n\t\t\tWEIGHT_SET_SPACEBASED(weight);\n\t\t} else {\n\t\t\tweight = metaslab_weight_from_range_tree(msp);\n\t\t}\n\n\t\tif (activated) {\n\t\t\tmetaslab_passivate(msp, weight);\n\t\t} else {\n\t\t\t \n\t\t\tweight |= msp->ms_weight & METASLAB_ACTIVE_MASK;\n\t\t\tmetaslab_group_sort(mg, msp, weight);\n\t\t}\n\t\tmetaslab_active_mask_verify(msp);\n\n\t\t \n\t\tASSERT(!metaslab_should_allocate(msp, asize, try_hard));\n\n\t\tmutex_exit(&msp->ms_lock);\n\t}\n\tmutex_exit(&msp->ms_lock);\n\tkmem_free(search, sizeof (*search));\n\treturn (offset);\n}\n\nstatic uint64_t\nmetaslab_group_alloc(metaslab_group_t *mg, zio_alloc_list_t *zal,\n    uint64_t asize, uint64_t txg, boolean_t want_unique, dva_t *dva, int d,\n    int allocator, boolean_t try_hard)\n{\n\tuint64_t offset;\n\tASSERT(mg->mg_initialized);\n\n\toffset = metaslab_group_alloc_normal(mg, zal, asize, txg, want_unique,\n\t    dva, d, allocator, try_hard);\n\n\tmutex_enter(&mg->mg_lock);\n\tif (offset == -1ULL) {\n\t\tmg->mg_failed_allocations++;\n\t\tmetaslab_trace_add(zal, mg, NULL, asize, d,\n\t\t    TRACE_GROUP_FAILURE, allocator);\n\t\tif (asize == SPA_GANGBLOCKSIZE) {\n\t\t\t \n\t\t\tmg->mg_no_free_space = B_TRUE;\n\t\t}\n\t}\n\tmg->mg_allocations++;\n\tmutex_exit(&mg->mg_lock);\n\treturn (offset);\n}\n\n \nint\nmetaslab_alloc_dva(spa_t *spa, metaslab_class_t *mc, uint64_t psize,\n    dva_t *dva, int d, dva_t *hintdva, uint64_t txg, int flags,\n    zio_alloc_list_t *zal, int allocator)\n{\n\tmetaslab_class_allocator_t *mca = &mc->mc_allocator[allocator];\n\tmetaslab_group_t *mg, *rotor;\n\tvdev_t *vd;\n\tboolean_t try_hard = B_FALSE;\n\n\tASSERT(!DVA_IS_VALID(&dva[d]));\n\n\t \n\tif (psize >= metaslab_force_ganging &&\n\t    metaslab_force_ganging_pct > 0 &&\n\t    (random_in_range(100) < MIN(metaslab_force_ganging_pct, 100))) {\n\t\tmetaslab_trace_add(zal, NULL, NULL, psize, d, TRACE_FORCE_GANG,\n\t\t    allocator);\n\t\treturn (SET_ERROR(ENOSPC));\n\t}\n\n\t \n\tif (hintdva) {\n\t\tvd = vdev_lookup_top(spa, DVA_GET_VDEV(&hintdva[d]));\n\n\t\t \n\t\tif (vd != NULL && vd->vdev_mg != NULL) {\n\t\t\tmg = vdev_get_mg(vd, mc);\n\n\t\t\tif (flags & METASLAB_HINTBP_AVOID)\n\t\t\t\tmg = mg->mg_next;\n\t\t} else {\n\t\t\tmg = mca->mca_rotor;\n\t\t}\n\t} else if (d != 0) {\n\t\tvd = vdev_lookup_top(spa, DVA_GET_VDEV(&dva[d - 1]));\n\t\tmg = vd->vdev_mg->mg_next;\n\t} else {\n\t\tASSERT(mca->mca_rotor != NULL);\n\t\tmg = mca->mca_rotor;\n\t}\n\n\t \n\tif (mg->mg_class != mc || mg->mg_activation_count <= 0)\n\t\tmg = mca->mca_rotor;\n\n\trotor = mg;\ntop:\n\tdo {\n\t\tboolean_t allocatable;\n\n\t\tASSERT(mg->mg_activation_count == 1);\n\t\tvd = mg->mg_vd;\n\n\t\t \n\t\tif (try_hard) {\n\t\t\tspa_config_enter(spa, SCL_ZIO, FTAG, RW_READER);\n\t\t\tallocatable = vdev_allocatable(vd);\n\t\t\tspa_config_exit(spa, SCL_ZIO, FTAG);\n\t\t} else {\n\t\t\tallocatable = vdev_allocatable(vd);\n\t\t}\n\n\t\t \n\t\tif (allocatable && !GANG_ALLOCATION(flags) && !try_hard) {\n\t\t\tallocatable = metaslab_group_allocatable(mg, rotor,\n\t\t\t    flags, psize, allocator, d);\n\t\t}\n\n\t\tif (!allocatable) {\n\t\t\tmetaslab_trace_add(zal, mg, NULL, psize, d,\n\t\t\t    TRACE_NOT_ALLOCATABLE, allocator);\n\t\t\tgoto next;\n\t\t}\n\n\t\tASSERT(mg->mg_initialized);\n\n\t\t \n\t\tif (vd->vdev_state < VDEV_STATE_HEALTHY &&\n\t\t    d == 0 && !try_hard && vd->vdev_children == 0) {\n\t\t\tmetaslab_trace_add(zal, mg, NULL, psize, d,\n\t\t\t    TRACE_VDEV_ERROR, allocator);\n\t\t\tgoto next;\n\t\t}\n\n\t\tASSERT(mg->mg_class == mc);\n\n\t\tuint64_t asize = vdev_psize_to_asize(vd, psize);\n\t\tASSERT(P2PHASE(asize, 1ULL << vd->vdev_ashift) == 0);\n\n\t\t \n\t\tuint64_t offset = metaslab_group_alloc(mg, zal, asize, txg,\n\t\t    !try_hard, dva, d, allocator, try_hard);\n\n\t\tif (offset != -1ULL) {\n\t\t\t \n\t\t\tif (mca->mca_aliquot == 0 && metaslab_bias_enabled) {\n\t\t\t\tvdev_stat_t *vs = &vd->vdev_stat;\n\t\t\t\tint64_t vs_free = vs->vs_space - vs->vs_alloc;\n\t\t\t\tint64_t mc_free = mc->mc_space - mc->mc_alloc;\n\t\t\t\tint64_t ratio;\n\n\t\t\t\t \n\t\t\t\tratio = (vs_free * mc->mc_alloc_groups * 100) /\n\t\t\t\t    (mc_free + 1);\n\t\t\t\tmg->mg_bias = ((ratio - 100) *\n\t\t\t\t    (int64_t)mg->mg_aliquot) / 100;\n\t\t\t} else if (!metaslab_bias_enabled) {\n\t\t\t\tmg->mg_bias = 0;\n\t\t\t}\n\n\t\t\tif ((flags & METASLAB_ZIL) ||\n\t\t\t    atomic_add_64_nv(&mca->mca_aliquot, asize) >=\n\t\t\t    mg->mg_aliquot + mg->mg_bias) {\n\t\t\t\tmca->mca_rotor = mg->mg_next;\n\t\t\t\tmca->mca_aliquot = 0;\n\t\t\t}\n\n\t\t\tDVA_SET_VDEV(&dva[d], vd->vdev_id);\n\t\t\tDVA_SET_OFFSET(&dva[d], offset);\n\t\t\tDVA_SET_GANG(&dva[d],\n\t\t\t    ((flags & METASLAB_GANG_HEADER) ? 1 : 0));\n\t\t\tDVA_SET_ASIZE(&dva[d], asize);\n\n\t\t\treturn (0);\n\t\t}\nnext:\n\t\tmca->mca_rotor = mg->mg_next;\n\t\tmca->mca_aliquot = 0;\n\t} while ((mg = mg->mg_next) != rotor);\n\n\t \n\tif (!try_hard && (zfs_metaslab_try_hard_before_gang ||\n\t    GANG_ALLOCATION(flags) || (flags & METASLAB_ZIL) != 0 ||\n\t    psize <= 1 << spa->spa_min_ashift)) {\n\t\tMETASLABSTAT_BUMP(metaslabstat_try_hard);\n\t\ttry_hard = B_TRUE;\n\t\tgoto top;\n\t}\n\n\tmemset(&dva[d], 0, sizeof (dva_t));\n\n\tmetaslab_trace_add(zal, rotor, NULL, psize, d, TRACE_ENOSPC, allocator);\n\treturn (SET_ERROR(ENOSPC));\n}\n\nvoid\nmetaslab_free_concrete(vdev_t *vd, uint64_t offset, uint64_t asize,\n    boolean_t checkpoint)\n{\n\tmetaslab_t *msp;\n\tspa_t *spa = vd->vdev_spa;\n\n\tASSERT(vdev_is_concrete(vd));\n\tASSERT3U(spa_config_held(spa, SCL_ALL, RW_READER), !=, 0);\n\tASSERT3U(offset >> vd->vdev_ms_shift, <, vd->vdev_ms_count);\n\n\tmsp = vd->vdev_ms[offset >> vd->vdev_ms_shift];\n\n\tVERIFY(!msp->ms_condensing);\n\tVERIFY3U(offset, >=, msp->ms_start);\n\tVERIFY3U(offset + asize, <=, msp->ms_start + msp->ms_size);\n\tVERIFY0(P2PHASE(offset, 1ULL << vd->vdev_ashift));\n\tVERIFY0(P2PHASE(asize, 1ULL << vd->vdev_ashift));\n\n\tmetaslab_check_free_impl(vd, offset, asize);\n\n\tmutex_enter(&msp->ms_lock);\n\tif (range_tree_is_empty(msp->ms_freeing) &&\n\t    range_tree_is_empty(msp->ms_checkpointing)) {\n\t\tvdev_dirty(vd, VDD_METASLAB, msp, spa_syncing_txg(spa));\n\t}\n\n\tif (checkpoint) {\n\t\tASSERT(spa_has_checkpoint(spa));\n\t\trange_tree_add(msp->ms_checkpointing, offset, asize);\n\t} else {\n\t\trange_tree_add(msp->ms_freeing, offset, asize);\n\t}\n\tmutex_exit(&msp->ms_lock);\n}\n\nvoid\nmetaslab_free_impl_cb(uint64_t inner_offset, vdev_t *vd, uint64_t offset,\n    uint64_t size, void *arg)\n{\n\t(void) inner_offset;\n\tboolean_t *checkpoint = arg;\n\n\tASSERT3P(checkpoint, !=, NULL);\n\n\tif (vd->vdev_ops->vdev_op_remap != NULL)\n\t\tvdev_indirect_mark_obsolete(vd, offset, size);\n\telse\n\t\tmetaslab_free_impl(vd, offset, size, *checkpoint);\n}\n\nstatic void\nmetaslab_free_impl(vdev_t *vd, uint64_t offset, uint64_t size,\n    boolean_t checkpoint)\n{\n\tspa_t *spa = vd->vdev_spa;\n\n\tASSERT3U(spa_config_held(spa, SCL_ALL, RW_READER), !=, 0);\n\n\tif (spa_syncing_txg(spa) > spa_freeze_txg(spa))\n\t\treturn;\n\n\tif (spa->spa_vdev_removal != NULL &&\n\t    spa->spa_vdev_removal->svr_vdev_id == vd->vdev_id &&\n\t    vdev_is_concrete(vd)) {\n\t\t \n\t\tfree_from_removing_vdev(vd, offset, size);\n\t} else if (vd->vdev_ops->vdev_op_remap != NULL) {\n\t\tvdev_indirect_mark_obsolete(vd, offset, size);\n\t\tvd->vdev_ops->vdev_op_remap(vd, offset, size,\n\t\t    metaslab_free_impl_cb, &checkpoint);\n\t} else {\n\t\tmetaslab_free_concrete(vd, offset, size, checkpoint);\n\t}\n}\n\ntypedef struct remap_blkptr_cb_arg {\n\tblkptr_t *rbca_bp;\n\tspa_remap_cb_t rbca_cb;\n\tvdev_t *rbca_remap_vd;\n\tuint64_t rbca_remap_offset;\n\tvoid *rbca_cb_arg;\n} remap_blkptr_cb_arg_t;\n\nstatic void\nremap_blkptr_cb(uint64_t inner_offset, vdev_t *vd, uint64_t offset,\n    uint64_t size, void *arg)\n{\n\tremap_blkptr_cb_arg_t *rbca = arg;\n\tblkptr_t *bp = rbca->rbca_bp;\n\n\t \n\tif (size != DVA_GET_ASIZE(&bp->blk_dva[0]))\n\t\treturn;\n\tASSERT0(inner_offset);\n\n\tif (rbca->rbca_cb != NULL) {\n\t\t \n\t\tASSERT3P(rbca->rbca_remap_vd->vdev_ops, ==, &vdev_indirect_ops);\n\n\t\trbca->rbca_cb(rbca->rbca_remap_vd->vdev_id,\n\t\t    rbca->rbca_remap_offset, size, rbca->rbca_cb_arg);\n\n\t\t \n\t\trbca->rbca_remap_vd = vd;\n\t\trbca->rbca_remap_offset = offset;\n\t}\n\n\t \n\tvdev_t *oldvd = vdev_lookup_top(vd->vdev_spa,\n\t    DVA_GET_VDEV(&bp->blk_dva[0]));\n\tvdev_indirect_births_t *vib = oldvd->vdev_indirect_births;\n\tbp->blk_phys_birth = vdev_indirect_births_physbirth(vib,\n\t    DVA_GET_OFFSET(&bp->blk_dva[0]), DVA_GET_ASIZE(&bp->blk_dva[0]));\n\n\tDVA_SET_VDEV(&bp->blk_dva[0], vd->vdev_id);\n\tDVA_SET_OFFSET(&bp->blk_dva[0], offset);\n}\n\n \nboolean_t\nspa_remap_blkptr(spa_t *spa, blkptr_t *bp, spa_remap_cb_t callback, void *arg)\n{\n\tremap_blkptr_cb_arg_t rbca;\n\n\tif (!zfs_remap_blkptr_enable)\n\t\treturn (B_FALSE);\n\n\tif (!spa_feature_is_enabled(spa, SPA_FEATURE_OBSOLETE_COUNTS))\n\t\treturn (B_FALSE);\n\n\t \n\tif (BP_GET_DEDUP(bp))\n\t\treturn (B_FALSE);\n\n\t \n\tif (BP_IS_GANG(bp))\n\t\treturn (B_FALSE);\n\n\t \n\tif (BP_GET_NDVAS(bp) < 1)\n\t\treturn (B_FALSE);\n\n\t \n\tdva_t *dva = &bp->blk_dva[0];\n\n\tuint64_t offset = DVA_GET_OFFSET(dva);\n\tuint64_t size = DVA_GET_ASIZE(dva);\n\tvdev_t *vd = vdev_lookup_top(spa, DVA_GET_VDEV(dva));\n\n\tif (vd->vdev_ops->vdev_op_remap == NULL)\n\t\treturn (B_FALSE);\n\n\trbca.rbca_bp = bp;\n\trbca.rbca_cb = callback;\n\trbca.rbca_remap_vd = vd;\n\trbca.rbca_remap_offset = offset;\n\trbca.rbca_cb_arg = arg;\n\n\t \n\tvd->vdev_ops->vdev_op_remap(vd, offset, size, remap_blkptr_cb, &rbca);\n\n\t \n\tif (DVA_GET_VDEV(&rbca.rbca_bp->blk_dva[0]) == vd->vdev_id)\n\t\treturn (B_FALSE);\n\n\treturn (B_TRUE);\n}\n\n \nvoid\nmetaslab_unalloc_dva(spa_t *spa, const dva_t *dva, uint64_t txg)\n{\n\tmetaslab_t *msp;\n\tvdev_t *vd;\n\tuint64_t vdev = DVA_GET_VDEV(dva);\n\tuint64_t offset = DVA_GET_OFFSET(dva);\n\tuint64_t size = DVA_GET_ASIZE(dva);\n\n\tASSERT(DVA_IS_VALID(dva));\n\tASSERT3U(spa_config_held(spa, SCL_ALL, RW_READER), !=, 0);\n\n\tif (txg > spa_freeze_txg(spa))\n\t\treturn;\n\n\tif ((vd = vdev_lookup_top(spa, vdev)) == NULL || !DVA_IS_VALID(dva) ||\n\t    (offset >> vd->vdev_ms_shift) >= vd->vdev_ms_count) {\n\t\tzfs_panic_recover(\"metaslab_free_dva(): bad DVA %llu:%llu:%llu\",\n\t\t    (u_longlong_t)vdev, (u_longlong_t)offset,\n\t\t    (u_longlong_t)size);\n\t\treturn;\n\t}\n\n\tASSERT(!vd->vdev_removing);\n\tASSERT(vdev_is_concrete(vd));\n\tASSERT0(vd->vdev_indirect_config.vic_mapping_object);\n\tASSERT3P(vd->vdev_indirect_mapping, ==, NULL);\n\n\tif (DVA_GET_GANG(dva))\n\t\tsize = vdev_gang_header_asize(vd);\n\n\tmsp = vd->vdev_ms[offset >> vd->vdev_ms_shift];\n\n\tmutex_enter(&msp->ms_lock);\n\trange_tree_remove(msp->ms_allocating[txg & TXG_MASK],\n\t    offset, size);\n\tmsp->ms_allocating_total -= size;\n\n\tVERIFY(!msp->ms_condensing);\n\tVERIFY3U(offset, >=, msp->ms_start);\n\tVERIFY3U(offset + size, <=, msp->ms_start + msp->ms_size);\n\tVERIFY3U(range_tree_space(msp->ms_allocatable) + size, <=,\n\t    msp->ms_size);\n\tVERIFY0(P2PHASE(offset, 1ULL << vd->vdev_ashift));\n\tVERIFY0(P2PHASE(size, 1ULL << vd->vdev_ashift));\n\trange_tree_add(msp->ms_allocatable, offset, size);\n\tmutex_exit(&msp->ms_lock);\n}\n\n \nvoid\nmetaslab_free_dva(spa_t *spa, const dva_t *dva, boolean_t checkpoint)\n{\n\tuint64_t vdev = DVA_GET_VDEV(dva);\n\tuint64_t offset = DVA_GET_OFFSET(dva);\n\tuint64_t size = DVA_GET_ASIZE(dva);\n\tvdev_t *vd = vdev_lookup_top(spa, vdev);\n\n\tASSERT(DVA_IS_VALID(dva));\n\tASSERT3U(spa_config_held(spa, SCL_ALL, RW_READER), !=, 0);\n\n\tif (DVA_GET_GANG(dva)) {\n\t\tsize = vdev_gang_header_asize(vd);\n\t}\n\n\tmetaslab_free_impl(vd, offset, size, checkpoint);\n}\n\n \nboolean_t\nmetaslab_class_throttle_reserve(metaslab_class_t *mc, int slots, int allocator,\n    zio_t *zio, int flags)\n{\n\tmetaslab_class_allocator_t *mca = &mc->mc_allocator[allocator];\n\tuint64_t max = mca->mca_alloc_max_slots;\n\n\tASSERT(mc->mc_alloc_throttle_enabled);\n\tif (GANG_ALLOCATION(flags) || (flags & METASLAB_MUST_RESERVE) ||\n\t    zfs_refcount_count(&mca->mca_alloc_slots) + slots <= max) {\n\t\t \n\t\tzfs_refcount_add_few(&mca->mca_alloc_slots, slots, zio);\n\t\tzio->io_flags |= ZIO_FLAG_IO_ALLOCATING;\n\t\treturn (B_TRUE);\n\t}\n\treturn (B_FALSE);\n}\n\nvoid\nmetaslab_class_throttle_unreserve(metaslab_class_t *mc, int slots,\n    int allocator, zio_t *zio)\n{\n\tmetaslab_class_allocator_t *mca = &mc->mc_allocator[allocator];\n\n\tASSERT(mc->mc_alloc_throttle_enabled);\n\tzfs_refcount_remove_few(&mca->mca_alloc_slots, slots, zio);\n}\n\nstatic int\nmetaslab_claim_concrete(vdev_t *vd, uint64_t offset, uint64_t size,\n    uint64_t txg)\n{\n\tmetaslab_t *msp;\n\tspa_t *spa = vd->vdev_spa;\n\tint error = 0;\n\n\tif (offset >> vd->vdev_ms_shift >= vd->vdev_ms_count)\n\t\treturn (SET_ERROR(ENXIO));\n\n\tASSERT3P(vd->vdev_ms, !=, NULL);\n\tmsp = vd->vdev_ms[offset >> vd->vdev_ms_shift];\n\n\tmutex_enter(&msp->ms_lock);\n\n\tif ((txg != 0 && spa_writeable(spa)) || !msp->ms_loaded) {\n\t\terror = metaslab_activate(msp, 0, METASLAB_WEIGHT_CLAIM);\n\t\tif (error == EBUSY) {\n\t\t\tASSERT(msp->ms_loaded);\n\t\t\tASSERT(msp->ms_weight & METASLAB_ACTIVE_MASK);\n\t\t\terror = 0;\n\t\t}\n\t}\n\n\tif (error == 0 &&\n\t    !range_tree_contains(msp->ms_allocatable, offset, size))\n\t\terror = SET_ERROR(ENOENT);\n\n\tif (error || txg == 0) {\t \n\t\tmutex_exit(&msp->ms_lock);\n\t\treturn (error);\n\t}\n\n\tVERIFY(!msp->ms_condensing);\n\tVERIFY0(P2PHASE(offset, 1ULL << vd->vdev_ashift));\n\tVERIFY0(P2PHASE(size, 1ULL << vd->vdev_ashift));\n\tVERIFY3U(range_tree_space(msp->ms_allocatable) - size, <=,\n\t    msp->ms_size);\n\trange_tree_remove(msp->ms_allocatable, offset, size);\n\trange_tree_clear(msp->ms_trim, offset, size);\n\n\tif (spa_writeable(spa)) {\t \n\t\tmetaslab_class_t *mc = msp->ms_group->mg_class;\n\t\tmultilist_sublist_t *mls =\n\t\t    multilist_sublist_lock_obj(&mc->mc_metaslab_txg_list, msp);\n\t\tif (!multilist_link_active(&msp->ms_class_txg_node)) {\n\t\t\tmsp->ms_selected_txg = txg;\n\t\t\tmultilist_sublist_insert_head(mls, msp);\n\t\t}\n\t\tmultilist_sublist_unlock(mls);\n\n\t\tif (range_tree_is_empty(msp->ms_allocating[txg & TXG_MASK]))\n\t\t\tvdev_dirty(vd, VDD_METASLAB, msp, txg);\n\t\trange_tree_add(msp->ms_allocating[txg & TXG_MASK],\n\t\t    offset, size);\n\t\tmsp->ms_allocating_total += size;\n\t}\n\n\tmutex_exit(&msp->ms_lock);\n\n\treturn (0);\n}\n\ntypedef struct metaslab_claim_cb_arg_t {\n\tuint64_t\tmcca_txg;\n\tint\t\tmcca_error;\n} metaslab_claim_cb_arg_t;\n\nstatic void\nmetaslab_claim_impl_cb(uint64_t inner_offset, vdev_t *vd, uint64_t offset,\n    uint64_t size, void *arg)\n{\n\t(void) inner_offset;\n\tmetaslab_claim_cb_arg_t *mcca_arg = arg;\n\n\tif (mcca_arg->mcca_error == 0) {\n\t\tmcca_arg->mcca_error = metaslab_claim_concrete(vd, offset,\n\t\t    size, mcca_arg->mcca_txg);\n\t}\n}\n\nint\nmetaslab_claim_impl(vdev_t *vd, uint64_t offset, uint64_t size, uint64_t txg)\n{\n\tif (vd->vdev_ops->vdev_op_remap != NULL) {\n\t\tmetaslab_claim_cb_arg_t arg;\n\n\t\t \n\t\tASSERT(!spa_writeable(vd->vdev_spa));\n\t\targ.mcca_error = 0;\n\t\targ.mcca_txg = txg;\n\n\t\tvd->vdev_ops->vdev_op_remap(vd, offset, size,\n\t\t    metaslab_claim_impl_cb, &arg);\n\n\t\tif (arg.mcca_error == 0) {\n\t\t\targ.mcca_error = metaslab_claim_concrete(vd,\n\t\t\t    offset, size, txg);\n\t\t}\n\t\treturn (arg.mcca_error);\n\t} else {\n\t\treturn (metaslab_claim_concrete(vd, offset, size, txg));\n\t}\n}\n\n \nstatic int\nmetaslab_claim_dva(spa_t *spa, const dva_t *dva, uint64_t txg)\n{\n\tuint64_t vdev = DVA_GET_VDEV(dva);\n\tuint64_t offset = DVA_GET_OFFSET(dva);\n\tuint64_t size = DVA_GET_ASIZE(dva);\n\tvdev_t *vd;\n\n\tif ((vd = vdev_lookup_top(spa, vdev)) == NULL) {\n\t\treturn (SET_ERROR(ENXIO));\n\t}\n\n\tASSERT(DVA_IS_VALID(dva));\n\n\tif (DVA_GET_GANG(dva))\n\t\tsize = vdev_gang_header_asize(vd);\n\n\treturn (metaslab_claim_impl(vd, offset, size, txg));\n}\n\nint\nmetaslab_alloc(spa_t *spa, metaslab_class_t *mc, uint64_t psize, blkptr_t *bp,\n    int ndvas, uint64_t txg, blkptr_t *hintbp, int flags,\n    zio_alloc_list_t *zal, zio_t *zio, int allocator)\n{\n\tdva_t *dva = bp->blk_dva;\n\tdva_t *hintdva = (hintbp != NULL) ? hintbp->blk_dva : NULL;\n\tint error = 0;\n\n\tASSERT(bp->blk_birth == 0);\n\tASSERT(BP_PHYSICAL_BIRTH(bp) == 0);\n\n\tspa_config_enter(spa, SCL_ALLOC, FTAG, RW_READER);\n\n\tif (mc->mc_allocator[allocator].mca_rotor == NULL) {\n\t\t \n\t\tspa_config_exit(spa, SCL_ALLOC, FTAG);\n\t\treturn (SET_ERROR(ENOSPC));\n\t}\n\n\tASSERT(ndvas > 0 && ndvas <= spa_max_replication(spa));\n\tASSERT(BP_GET_NDVAS(bp) == 0);\n\tASSERT(hintbp == NULL || ndvas <= BP_GET_NDVAS(hintbp));\n\tASSERT3P(zal, !=, NULL);\n\n\tfor (int d = 0; d < ndvas; d++) {\n\t\terror = metaslab_alloc_dva(spa, mc, psize, dva, d, hintdva,\n\t\t    txg, flags, zal, allocator);\n\t\tif (error != 0) {\n\t\t\tfor (d--; d >= 0; d--) {\n\t\t\t\tmetaslab_unalloc_dva(spa, &dva[d], txg);\n\t\t\t\tmetaslab_group_alloc_decrement(spa,\n\t\t\t\t    DVA_GET_VDEV(&dva[d]), zio, flags,\n\t\t\t\t    allocator, B_FALSE);\n\t\t\t\tmemset(&dva[d], 0, sizeof (dva_t));\n\t\t\t}\n\t\t\tspa_config_exit(spa, SCL_ALLOC, FTAG);\n\t\t\treturn (error);\n\t\t} else {\n\t\t\t \n\t\t\tmetaslab_group_alloc_increment(spa,\n\t\t\t    DVA_GET_VDEV(&dva[d]), zio, flags, allocator);\n\t\t}\n\t}\n\tASSERT(error == 0);\n\tASSERT(BP_GET_NDVAS(bp) == ndvas);\n\n\tspa_config_exit(spa, SCL_ALLOC, FTAG);\n\n\tBP_SET_BIRTH(bp, txg, 0);\n\n\treturn (0);\n}\n\nvoid\nmetaslab_free(spa_t *spa, const blkptr_t *bp, uint64_t txg, boolean_t now)\n{\n\tconst dva_t *dva = bp->blk_dva;\n\tint ndvas = BP_GET_NDVAS(bp);\n\n\tASSERT(!BP_IS_HOLE(bp));\n\tASSERT(!now || bp->blk_birth >= spa_syncing_txg(spa));\n\n\t \n\tboolean_t checkpoint = B_FALSE;\n\tif (bp->blk_birth <= spa->spa_checkpoint_txg &&\n\t    spa_syncing_txg(spa) > spa->spa_checkpoint_txg) {\n\t\t \n\t\tASSERT(!now);\n\t\tASSERT3U(spa_syncing_txg(spa), ==, txg);\n\t\tcheckpoint = B_TRUE;\n\t}\n\n\tspa_config_enter(spa, SCL_FREE, FTAG, RW_READER);\n\n\tfor (int d = 0; d < ndvas; d++) {\n\t\tif (now) {\n\t\t\tmetaslab_unalloc_dva(spa, &dva[d], txg);\n\t\t} else {\n\t\t\tASSERT3U(txg, ==, spa_syncing_txg(spa));\n\t\t\tmetaslab_free_dva(spa, &dva[d], checkpoint);\n\t\t}\n\t}\n\n\tspa_config_exit(spa, SCL_FREE, FTAG);\n}\n\nint\nmetaslab_claim(spa_t *spa, const blkptr_t *bp, uint64_t txg)\n{\n\tconst dva_t *dva = bp->blk_dva;\n\tint ndvas = BP_GET_NDVAS(bp);\n\tint error = 0;\n\n\tASSERT(!BP_IS_HOLE(bp));\n\n\tif (txg != 0) {\n\t\t \n\t\tif ((error = metaslab_claim(spa, bp, 0)) != 0)\n\t\t\treturn (error);\n\t}\n\n\tspa_config_enter(spa, SCL_ALLOC, FTAG, RW_READER);\n\n\tfor (int d = 0; d < ndvas; d++) {\n\t\terror = metaslab_claim_dva(spa, &dva[d], txg);\n\t\tif (error != 0)\n\t\t\tbreak;\n\t}\n\n\tspa_config_exit(spa, SCL_ALLOC, FTAG);\n\n\tASSERT(error == 0 || txg == 0);\n\n\treturn (error);\n}\n\nstatic void\nmetaslab_check_free_impl_cb(uint64_t inner, vdev_t *vd, uint64_t offset,\n    uint64_t size, void *arg)\n{\n\t(void) inner, (void) arg;\n\n\tif (vd->vdev_ops == &vdev_indirect_ops)\n\t\treturn;\n\n\tmetaslab_check_free_impl(vd, offset, size);\n}\n\nstatic void\nmetaslab_check_free_impl(vdev_t *vd, uint64_t offset, uint64_t size)\n{\n\tmetaslab_t *msp;\n\tspa_t *spa __maybe_unused = vd->vdev_spa;\n\n\tif ((zfs_flags & ZFS_DEBUG_ZIO_FREE) == 0)\n\t\treturn;\n\n\tif (vd->vdev_ops->vdev_op_remap != NULL) {\n\t\tvd->vdev_ops->vdev_op_remap(vd, offset, size,\n\t\t    metaslab_check_free_impl_cb, NULL);\n\t\treturn;\n\t}\n\n\tASSERT(vdev_is_concrete(vd));\n\tASSERT3U(offset >> vd->vdev_ms_shift, <, vd->vdev_ms_count);\n\tASSERT3U(spa_config_held(spa, SCL_ALL, RW_READER), !=, 0);\n\n\tmsp = vd->vdev_ms[offset >> vd->vdev_ms_shift];\n\n\tmutex_enter(&msp->ms_lock);\n\tif (msp->ms_loaded) {\n\t\trange_tree_verify_not_present(msp->ms_allocatable,\n\t\t    offset, size);\n\t}\n\n\t \n\trange_tree_verify_not_present(msp->ms_freeing, offset, size);\n\trange_tree_verify_not_present(msp->ms_checkpointing, offset, size);\n\trange_tree_verify_not_present(msp->ms_freed, offset, size);\n\tfor (int j = 0; j < TXG_DEFER_SIZE; j++)\n\t\trange_tree_verify_not_present(msp->ms_defer[j], offset, size);\n\trange_tree_verify_not_present(msp->ms_trim, offset, size);\n\tmutex_exit(&msp->ms_lock);\n}\n\nvoid\nmetaslab_check_free(spa_t *spa, const blkptr_t *bp)\n{\n\tif ((zfs_flags & ZFS_DEBUG_ZIO_FREE) == 0)\n\t\treturn;\n\n\tspa_config_enter(spa, SCL_VDEV, FTAG, RW_READER);\n\tfor (int i = 0; i < BP_GET_NDVAS(bp); i++) {\n\t\tuint64_t vdev = DVA_GET_VDEV(&bp->blk_dva[i]);\n\t\tvdev_t *vd = vdev_lookup_top(spa, vdev);\n\t\tuint64_t offset = DVA_GET_OFFSET(&bp->blk_dva[i]);\n\t\tuint64_t size = DVA_GET_ASIZE(&bp->blk_dva[i]);\n\n\t\tif (DVA_GET_GANG(&bp->blk_dva[i]))\n\t\t\tsize = vdev_gang_header_asize(vd);\n\n\t\tASSERT3P(vd, !=, NULL);\n\n\t\tmetaslab_check_free_impl(vd, offset, size);\n\t}\n\tspa_config_exit(spa, SCL_VDEV, FTAG);\n}\n\nstatic void\nmetaslab_group_disable_wait(metaslab_group_t *mg)\n{\n\tASSERT(MUTEX_HELD(&mg->mg_ms_disabled_lock));\n\twhile (mg->mg_disabled_updating) {\n\t\tcv_wait(&mg->mg_ms_disabled_cv, &mg->mg_ms_disabled_lock);\n\t}\n}\n\nstatic void\nmetaslab_group_disabled_increment(metaslab_group_t *mg)\n{\n\tASSERT(MUTEX_HELD(&mg->mg_ms_disabled_lock));\n\tASSERT(mg->mg_disabled_updating);\n\n\twhile (mg->mg_ms_disabled >= max_disabled_ms) {\n\t\tcv_wait(&mg->mg_ms_disabled_cv, &mg->mg_ms_disabled_lock);\n\t}\n\tmg->mg_ms_disabled++;\n\tASSERT3U(mg->mg_ms_disabled, <=, max_disabled_ms);\n}\n\n \nvoid\nmetaslab_disable(metaslab_t *msp)\n{\n\tASSERT(!MUTEX_HELD(&msp->ms_lock));\n\tmetaslab_group_t *mg = msp->ms_group;\n\n\tmutex_enter(&mg->mg_ms_disabled_lock);\n\n\t \n\tmetaslab_group_disable_wait(mg);\n\tmg->mg_disabled_updating = B_TRUE;\n\tif (msp->ms_disabled == 0) {\n\t\tmetaslab_group_disabled_increment(mg);\n\t}\n\tmutex_enter(&msp->ms_lock);\n\tmsp->ms_disabled++;\n\tmutex_exit(&msp->ms_lock);\n\n\tmg->mg_disabled_updating = B_FALSE;\n\tcv_broadcast(&mg->mg_ms_disabled_cv);\n\tmutex_exit(&mg->mg_ms_disabled_lock);\n}\n\nvoid\nmetaslab_enable(metaslab_t *msp, boolean_t sync, boolean_t unload)\n{\n\tmetaslab_group_t *mg = msp->ms_group;\n\tspa_t *spa = mg->mg_vd->vdev_spa;\n\n\t \n\tif (sync)\n\t\ttxg_wait_synced(spa_get_dsl(spa), 0);\n\n\tmutex_enter(&mg->mg_ms_disabled_lock);\n\tmutex_enter(&msp->ms_lock);\n\tif (--msp->ms_disabled == 0) {\n\t\tmg->mg_ms_disabled--;\n\t\tcv_broadcast(&mg->mg_ms_disabled_cv);\n\t\tif (unload)\n\t\t\tmetaslab_unload(msp);\n\t}\n\tmutex_exit(&msp->ms_lock);\n\tmutex_exit(&mg->mg_ms_disabled_lock);\n}\n\nvoid\nmetaslab_set_unflushed_dirty(metaslab_t *ms, boolean_t dirty)\n{\n\tms->ms_unflushed_dirty = dirty;\n}\n\nstatic void\nmetaslab_update_ondisk_flush_data(metaslab_t *ms, dmu_tx_t *tx)\n{\n\tvdev_t *vd = ms->ms_group->mg_vd;\n\tspa_t *spa = vd->vdev_spa;\n\tobjset_t *mos = spa_meta_objset(spa);\n\n\tASSERT(spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP));\n\n\tmetaslab_unflushed_phys_t entry = {\n\t\t.msp_unflushed_txg = metaslab_unflushed_txg(ms),\n\t};\n\tuint64_t entry_size = sizeof (entry);\n\tuint64_t entry_offset = ms->ms_id * entry_size;\n\n\tuint64_t object = 0;\n\tint err = zap_lookup(mos, vd->vdev_top_zap,\n\t    VDEV_TOP_ZAP_MS_UNFLUSHED_PHYS_TXGS, sizeof (uint64_t), 1,\n\t    &object);\n\tif (err == ENOENT) {\n\t\tobject = dmu_object_alloc(mos, DMU_OTN_UINT64_METADATA,\n\t\t    SPA_OLD_MAXBLOCKSIZE, DMU_OT_NONE, 0, tx);\n\t\tVERIFY0(zap_add(mos, vd->vdev_top_zap,\n\t\t    VDEV_TOP_ZAP_MS_UNFLUSHED_PHYS_TXGS, sizeof (uint64_t), 1,\n\t\t    &object, tx));\n\t} else {\n\t\tVERIFY0(err);\n\t}\n\n\tdmu_write(spa_meta_objset(spa), object, entry_offset, entry_size,\n\t    &entry, tx);\n}\n\nvoid\nmetaslab_set_unflushed_txg(metaslab_t *ms, uint64_t txg, dmu_tx_t *tx)\n{\n\tms->ms_unflushed_txg = txg;\n\tmetaslab_update_ondisk_flush_data(ms, tx);\n}\n\nboolean_t\nmetaslab_unflushed_dirty(metaslab_t *ms)\n{\n\treturn (ms->ms_unflushed_dirty);\n}\n\nuint64_t\nmetaslab_unflushed_txg(metaslab_t *ms)\n{\n\treturn (ms->ms_unflushed_txg);\n}\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, aliquot, U64, ZMOD_RW,\n\t\"Allocation granularity (a.k.a. stripe size)\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, debug_load, INT, ZMOD_RW,\n\t\"Load all metaslabs when pool is first opened\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, debug_unload, INT, ZMOD_RW,\n\t\"Prevent metaslabs from being unloaded\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, preload_enabled, INT, ZMOD_RW,\n\t\"Preload potential metaslabs during reassessment\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, preload_limit, UINT, ZMOD_RW,\n\t\"Max number of metaslabs per group to preload\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, unload_delay, UINT, ZMOD_RW,\n\t\"Delay in txgs after metaslab was last used before unloading\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, unload_delay_ms, UINT, ZMOD_RW,\n\t\"Delay in milliseconds after metaslab was last used before unloading\");\n\n \nZFS_MODULE_PARAM(zfs_mg, zfs_mg_, noalloc_threshold, UINT, ZMOD_RW,\n\t\"Percentage of metaslab group size that should be free to make it \"\n\t\"eligible for allocation\");\n\nZFS_MODULE_PARAM(zfs_mg, zfs_mg_, fragmentation_threshold, UINT, ZMOD_RW,\n\t\"Percentage of metaslab group size that should be considered eligible \"\n\t\"for allocations unless all metaslab groups within the metaslab class \"\n\t\"have also crossed this threshold\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, fragmentation_factor_enabled, INT,\n\tZMOD_RW,\n\t\"Use the fragmentation metric to prefer less fragmented metaslabs\");\n \n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, fragmentation_threshold, UINT,\n\tZMOD_RW, \"Fragmentation for metaslab to allow allocation\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, lba_weighting_enabled, INT, ZMOD_RW,\n\t\"Prefer metaslabs with lower LBAs\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, bias_enabled, INT, ZMOD_RW,\n\t\"Enable metaslab group biasing\");\n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, segment_weight_enabled, INT,\n\tZMOD_RW, \"Enable segment-based metaslab selection\");\n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, switch_threshold, INT, ZMOD_RW,\n\t\"Segment-based metaslab selection maximum buckets before switching\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, force_ganging, U64, ZMOD_RW,\n\t\"Blocks larger than this size are sometimes forced to be gang blocks\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, force_ganging_pct, UINT, ZMOD_RW,\n\t\"Percentage of large blocks that will be forced to be gang blocks\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, df_max_search, UINT, ZMOD_RW,\n\t\"Max distance (bytes) to search forward before using size tree\");\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, df_use_largest_segment, INT, ZMOD_RW,\n\t\"When looking in size tree, use largest segment instead of exact fit\");\n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, max_size_cache_sec, U64,\n\tZMOD_RW, \"How long to trust the cached max chunk size of a metaslab\");\n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, mem_limit, UINT, ZMOD_RW,\n\t\"Percentage of memory that can be used to store metaslab range trees\");\n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, try_hard_before_gang, INT,\n\tZMOD_RW, \"Try hard to allocate before ganging\");\n\nZFS_MODULE_PARAM(zfs_metaslab, zfs_metaslab_, find_max_tries, UINT, ZMOD_RW,\n\t\"Normally only consider this many of the best metaslabs in each vdev\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}