{
  "module_name": "spa_misc.c",
  "hash_id": "f0df4ab22a86fc613814c75e9fe7ef10ac4bc784f020133d8bbd3eef9de22c22",
  "original_prompt": "Ingested from zfs-2.2.2/module/zfs/spa_misc.c",
  "human_readable_source": " \n \n\n#include <sys/zfs_context.h>\n#include <sys/zfs_chksum.h>\n#include <sys/spa_impl.h>\n#include <sys/zio.h>\n#include <sys/zio_checksum.h>\n#include <sys/zio_compress.h>\n#include <sys/dmu.h>\n#include <sys/dmu_tx.h>\n#include <sys/zap.h>\n#include <sys/zil.h>\n#include <sys/vdev_impl.h>\n#include <sys/vdev_initialize.h>\n#include <sys/vdev_trim.h>\n#include <sys/vdev_file.h>\n#include <sys/vdev_raidz.h>\n#include <sys/metaslab.h>\n#include <sys/uberblock_impl.h>\n#include <sys/txg.h>\n#include <sys/avl.h>\n#include <sys/unique.h>\n#include <sys/dsl_pool.h>\n#include <sys/dsl_dir.h>\n#include <sys/dsl_prop.h>\n#include <sys/fm/util.h>\n#include <sys/dsl_scan.h>\n#include <sys/fs/zfs.h>\n#include <sys/metaslab_impl.h>\n#include <sys/arc.h>\n#include <sys/brt.h>\n#include <sys/ddt.h>\n#include <sys/kstat.h>\n#include \"zfs_prop.h\"\n#include <sys/btree.h>\n#include <sys/zfeature.h>\n#include <sys/qat.h>\n#include <sys/zstd/zstd.h>\n\n \n\nstatic avl_tree_t spa_namespace_avl;\nkmutex_t spa_namespace_lock;\nstatic kcondvar_t spa_namespace_cv;\nstatic const int spa_max_replication_override = SPA_DVAS_PER_BP;\n\nstatic kmutex_t spa_spare_lock;\nstatic avl_tree_t spa_spare_avl;\nstatic kmutex_t spa_l2cache_lock;\nstatic avl_tree_t spa_l2cache_avl;\n\nspa_mode_t spa_mode_global = SPA_MODE_UNINIT;\n\n#ifdef ZFS_DEBUG\n \nint zfs_flags = ~(ZFS_DEBUG_DPRINTF | ZFS_DEBUG_SET_ERROR |\n    ZFS_DEBUG_INDIRECT_REMAP);\n#else\nint zfs_flags = 0;\n#endif\n\n \nint zfs_recover = B_FALSE;\n\n \nint zfs_free_leak_on_eio = B_FALSE;\n\n \nuint64_t zfs_deadman_synctime_ms = 600000UL;   \n\n \nuint64_t zfs_deadman_ziotime_ms = 300000UL;   \n\n \nuint64_t zfs_deadman_checktime_ms = 60000UL;   \n\n \nint zfs_deadman_enabled = B_TRUE;\n\n \nconst char *zfs_deadman_failmode = \"wait\";\n\n \nuint_t spa_asize_inflation = 24;\n\n \nuint_t spa_slop_shift = 5;\nstatic const uint64_t spa_min_slop = 128ULL * 1024 * 1024;\nstatic const uint64_t spa_max_slop = 128ULL * 1024 * 1024 * 1024;\nstatic const int spa_allocators = 4;\n\n\nvoid\nspa_load_failed(spa_t *spa, const char *fmt, ...)\n{\n\tva_list adx;\n\tchar buf[256];\n\n\tva_start(adx, fmt);\n\t(void) vsnprintf(buf, sizeof (buf), fmt, adx);\n\tva_end(adx);\n\n\tzfs_dbgmsg(\"spa_load(%s, config %s): FAILED: %s\", spa->spa_name,\n\t    spa->spa_trust_config ? \"trusted\" : \"untrusted\", buf);\n}\n\nvoid\nspa_load_note(spa_t *spa, const char *fmt, ...)\n{\n\tva_list adx;\n\tchar buf[256];\n\n\tva_start(adx, fmt);\n\t(void) vsnprintf(buf, sizeof (buf), fmt, adx);\n\tva_end(adx);\n\n\tzfs_dbgmsg(\"spa_load(%s, config %s): %s\", spa->spa_name,\n\t    spa->spa_trust_config ? \"trusted\" : \"untrusted\", buf);\n}\n\n \nstatic int zfs_ddt_data_is_special = B_TRUE;\nstatic int zfs_user_indirect_is_special = B_TRUE;\n\n \nstatic uint_t zfs_special_class_metadata_reserve_pct = 25;\n\n \nstatic void\nspa_config_lock_init(spa_t *spa)\n{\n\tfor (int i = 0; i < SCL_LOCKS; i++) {\n\t\tspa_config_lock_t *scl = &spa->spa_config_lock[i];\n\t\tmutex_init(&scl->scl_lock, NULL, MUTEX_DEFAULT, NULL);\n\t\tcv_init(&scl->scl_cv, NULL, CV_DEFAULT, NULL);\n\t\tscl->scl_writer = NULL;\n\t\tscl->scl_write_wanted = 0;\n\t\tscl->scl_count = 0;\n\t}\n}\n\nstatic void\nspa_config_lock_destroy(spa_t *spa)\n{\n\tfor (int i = 0; i < SCL_LOCKS; i++) {\n\t\tspa_config_lock_t *scl = &spa->spa_config_lock[i];\n\t\tmutex_destroy(&scl->scl_lock);\n\t\tcv_destroy(&scl->scl_cv);\n\t\tASSERT(scl->scl_writer == NULL);\n\t\tASSERT(scl->scl_write_wanted == 0);\n\t\tASSERT(scl->scl_count == 0);\n\t}\n}\n\nint\nspa_config_tryenter(spa_t *spa, int locks, const void *tag, krw_t rw)\n{\n\tfor (int i = 0; i < SCL_LOCKS; i++) {\n\t\tspa_config_lock_t *scl = &spa->spa_config_lock[i];\n\t\tif (!(locks & (1 << i)))\n\t\t\tcontinue;\n\t\tmutex_enter(&scl->scl_lock);\n\t\tif (rw == RW_READER) {\n\t\t\tif (scl->scl_writer || scl->scl_write_wanted) {\n\t\t\t\tmutex_exit(&scl->scl_lock);\n\t\t\t\tspa_config_exit(spa, locks & ((1 << i) - 1),\n\t\t\t\t    tag);\n\t\t\t\treturn (0);\n\t\t\t}\n\t\t} else {\n\t\t\tASSERT(scl->scl_writer != curthread);\n\t\t\tif (scl->scl_count != 0) {\n\t\t\t\tmutex_exit(&scl->scl_lock);\n\t\t\t\tspa_config_exit(spa, locks & ((1 << i) - 1),\n\t\t\t\t    tag);\n\t\t\t\treturn (0);\n\t\t\t}\n\t\t\tscl->scl_writer = curthread;\n\t\t}\n\t\tscl->scl_count++;\n\t\tmutex_exit(&scl->scl_lock);\n\t}\n\treturn (1);\n}\n\nstatic void\nspa_config_enter_impl(spa_t *spa, int locks, const void *tag, krw_t rw,\n    int mmp_flag)\n{\n\t(void) tag;\n\tint wlocks_held = 0;\n\n\tASSERT3U(SCL_LOCKS, <, sizeof (wlocks_held) * NBBY);\n\n\tfor (int i = 0; i < SCL_LOCKS; i++) {\n\t\tspa_config_lock_t *scl = &spa->spa_config_lock[i];\n\t\tif (scl->scl_writer == curthread)\n\t\t\twlocks_held |= (1 << i);\n\t\tif (!(locks & (1 << i)))\n\t\t\tcontinue;\n\t\tmutex_enter(&scl->scl_lock);\n\t\tif (rw == RW_READER) {\n\t\t\twhile (scl->scl_writer ||\n\t\t\t    (!mmp_flag && scl->scl_write_wanted)) {\n\t\t\t\tcv_wait(&scl->scl_cv, &scl->scl_lock);\n\t\t\t}\n\t\t} else {\n\t\t\tASSERT(scl->scl_writer != curthread);\n\t\t\twhile (scl->scl_count != 0) {\n\t\t\t\tscl->scl_write_wanted++;\n\t\t\t\tcv_wait(&scl->scl_cv, &scl->scl_lock);\n\t\t\t\tscl->scl_write_wanted--;\n\t\t\t}\n\t\t\tscl->scl_writer = curthread;\n\t\t}\n\t\tscl->scl_count++;\n\t\tmutex_exit(&scl->scl_lock);\n\t}\n\tASSERT3U(wlocks_held, <=, locks);\n}\n\nvoid\nspa_config_enter(spa_t *spa, int locks, const void *tag, krw_t rw)\n{\n\tspa_config_enter_impl(spa, locks, tag, rw, 0);\n}\n\n \n\nvoid\nspa_config_enter_mmp(spa_t *spa, int locks, const void *tag, krw_t rw)\n{\n\tspa_config_enter_impl(spa, locks, tag, rw, 1);\n}\n\nvoid\nspa_config_exit(spa_t *spa, int locks, const void *tag)\n{\n\t(void) tag;\n\tfor (int i = SCL_LOCKS - 1; i >= 0; i--) {\n\t\tspa_config_lock_t *scl = &spa->spa_config_lock[i];\n\t\tif (!(locks & (1 << i)))\n\t\t\tcontinue;\n\t\tmutex_enter(&scl->scl_lock);\n\t\tASSERT(scl->scl_count > 0);\n\t\tif (--scl->scl_count == 0) {\n\t\t\tASSERT(scl->scl_writer == NULL ||\n\t\t\t    scl->scl_writer == curthread);\n\t\t\tscl->scl_writer = NULL;\t \n\t\t\tcv_broadcast(&scl->scl_cv);\n\t\t}\n\t\tmutex_exit(&scl->scl_lock);\n\t}\n}\n\nint\nspa_config_held(spa_t *spa, int locks, krw_t rw)\n{\n\tint locks_held = 0;\n\n\tfor (int i = 0; i < SCL_LOCKS; i++) {\n\t\tspa_config_lock_t *scl = &spa->spa_config_lock[i];\n\t\tif (!(locks & (1 << i)))\n\t\t\tcontinue;\n\t\tif ((rw == RW_READER && scl->scl_count != 0) ||\n\t\t    (rw == RW_WRITER && scl->scl_writer == curthread))\n\t\t\tlocks_held |= 1 << i;\n\t}\n\n\treturn (locks_held);\n}\n\n \n\n \nspa_t *\nspa_lookup(const char *name)\n{\n\tstatic spa_t search;\t \n\tspa_t *spa;\n\tavl_index_t where;\n\tchar *cp;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\t(void) strlcpy(search.spa_name, name, sizeof (search.spa_name));\n\n\t \n\tcp = strpbrk(search.spa_name, \"/@#\");\n\tif (cp != NULL)\n\t\t*cp = '\\0';\n\n\tspa = avl_find(&spa_namespace_avl, &search, &where);\n\n\treturn (spa);\n}\n\n \nvoid\nspa_deadman(void *arg)\n{\n\tspa_t *spa = arg;\n\n\t \n\tif (spa_suspended(spa))\n\t\treturn;\n\n\tzfs_dbgmsg(\"slow spa_sync: started %llu seconds ago, calls %llu\",\n\t    (gethrtime() - spa->spa_sync_starttime) / NANOSEC,\n\t    (u_longlong_t)++spa->spa_deadman_calls);\n\tif (zfs_deadman_enabled)\n\t\tvdev_deadman(spa->spa_root_vdev, FTAG);\n\n\tspa->spa_deadman_tqid = taskq_dispatch_delay(system_delay_taskq,\n\t    spa_deadman, spa, TQ_SLEEP, ddi_get_lbolt() +\n\t    MSEC_TO_TICK(zfs_deadman_checktime_ms));\n}\n\nstatic int\nspa_log_sm_sort_by_txg(const void *va, const void *vb)\n{\n\tconst spa_log_sm_t *a = va;\n\tconst spa_log_sm_t *b = vb;\n\n\treturn (TREE_CMP(a->sls_txg, b->sls_txg));\n}\n\n \nspa_t *\nspa_add(const char *name, nvlist_t *config, const char *altroot)\n{\n\tspa_t *spa;\n\tspa_config_dirent_t *dp;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tspa = kmem_zalloc(sizeof (spa_t), KM_SLEEP);\n\n\tmutex_init(&spa->spa_async_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_errlist_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_errlog_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_evicting_os_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_history_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_proc_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_props_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_cksum_tmpls_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_scrub_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_suspend_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_vdev_top_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_feat_stats_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_flushed_ms_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa->spa_activities_lock, NULL, MUTEX_DEFAULT, NULL);\n\n\tcv_init(&spa->spa_async_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&spa->spa_evicting_os_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&spa->spa_proc_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&spa->spa_scrub_io_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&spa->spa_suspend_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&spa->spa_activities_cv, NULL, CV_DEFAULT, NULL);\n\tcv_init(&spa->spa_waiters_cv, NULL, CV_DEFAULT, NULL);\n\n\tfor (int t = 0; t < TXG_SIZE; t++)\n\t\tbplist_create(&spa->spa_free_bplist[t]);\n\n\t(void) strlcpy(spa->spa_name, name, sizeof (spa->spa_name));\n\tspa->spa_state = POOL_STATE_UNINITIALIZED;\n\tspa->spa_freeze_txg = UINT64_MAX;\n\tspa->spa_final_txg = UINT64_MAX;\n\tspa->spa_load_max_txg = UINT64_MAX;\n\tspa->spa_proc = &p0;\n\tspa->spa_proc_state = SPA_PROC_NONE;\n\tspa->spa_trust_config = B_TRUE;\n\tspa->spa_hostid = zone_get_hostid(NULL);\n\n\tspa->spa_deadman_synctime = MSEC2NSEC(zfs_deadman_synctime_ms);\n\tspa->spa_deadman_ziotime = MSEC2NSEC(zfs_deadman_ziotime_ms);\n\tspa_set_deadman_failmode(spa, zfs_deadman_failmode);\n\n\tzfs_refcount_create(&spa->spa_refcount);\n\tspa_config_lock_init(spa);\n\tspa_stats_init(spa);\n\n\tavl_add(&spa_namespace_avl, spa);\n\n\t \n\tif (altroot)\n\t\tspa->spa_root = spa_strdup(altroot);\n\n\tspa->spa_alloc_count = spa_allocators;\n\tspa->spa_allocs = kmem_zalloc(spa->spa_alloc_count *\n\t    sizeof (spa_alloc_t), KM_SLEEP);\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tmutex_init(&spa->spa_allocs[i].spaa_lock, NULL, MUTEX_DEFAULT,\n\t\t    NULL);\n\t\tavl_create(&spa->spa_allocs[i].spaa_tree, zio_bookmark_compare,\n\t\t    sizeof (zio_t), offsetof(zio_t, io_queue_node.a));\n\t}\n\tavl_create(&spa->spa_metaslabs_by_flushed, metaslab_sort_by_flushed,\n\t    sizeof (metaslab_t), offsetof(metaslab_t, ms_spa_txg_node));\n\tavl_create(&spa->spa_sm_logs_by_txg, spa_log_sm_sort_by_txg,\n\t    sizeof (spa_log_sm_t), offsetof(spa_log_sm_t, sls_node));\n\tlist_create(&spa->spa_log_summary, sizeof (log_summary_entry_t),\n\t    offsetof(log_summary_entry_t, lse_node));\n\n\t \n\tlist_create(&spa->spa_config_list, sizeof (spa_config_dirent_t),\n\t    offsetof(spa_config_dirent_t, scd_link));\n\n\tdp = kmem_zalloc(sizeof (spa_config_dirent_t), KM_SLEEP);\n\tdp->scd_path = altroot ? NULL : spa_strdup(spa_config_path);\n\tlist_insert_head(&spa->spa_config_list, dp);\n\n\tVERIFY(nvlist_alloc(&spa->spa_load_info, NV_UNIQUE_NAME,\n\t    KM_SLEEP) == 0);\n\n\tif (config != NULL) {\n\t\tnvlist_t *features;\n\n\t\tif (nvlist_lookup_nvlist(config, ZPOOL_CONFIG_FEATURES_FOR_READ,\n\t\t    &features) == 0) {\n\t\t\tVERIFY(nvlist_dup(features, &spa->spa_label_features,\n\t\t\t    0) == 0);\n\t\t}\n\n\t\tVERIFY(nvlist_dup(config, &spa->spa_config, 0) == 0);\n\t}\n\n\tif (spa->spa_label_features == NULL) {\n\t\tVERIFY(nvlist_alloc(&spa->spa_label_features, NV_UNIQUE_NAME,\n\t\t    KM_SLEEP) == 0);\n\t}\n\n\tspa->spa_min_ashift = INT_MAX;\n\tspa->spa_max_ashift = 0;\n\tspa->spa_min_alloc = INT_MAX;\n\tspa->spa_gcd_alloc = INT_MAX;\n\n\t \n\tspa->spa_dedup_dspace = ~0ULL;\n\n\t \n\tfor (int i = 0; i < SPA_FEATURES; i++) {\n\t\tspa->spa_feat_refcount_cache[i] = SPA_FEATURE_DISABLED;\n\t}\n\n\tlist_create(&spa->spa_leaf_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_leaf_node));\n\n\treturn (spa);\n}\n\n \nvoid\nspa_remove(spa_t *spa)\n{\n\tspa_config_dirent_t *dp;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tASSERT(spa_state(spa) == POOL_STATE_UNINITIALIZED);\n\tASSERT3U(zfs_refcount_count(&spa->spa_refcount), ==, 0);\n\tASSERT0(spa->spa_waiters);\n\n\tnvlist_free(spa->spa_config_splitting);\n\n\tavl_remove(&spa_namespace_avl, spa);\n\tcv_broadcast(&spa_namespace_cv);\n\n\tif (spa->spa_root)\n\t\tspa_strfree(spa->spa_root);\n\n\twhile ((dp = list_remove_head(&spa->spa_config_list)) != NULL) {\n\t\tif (dp->scd_path != NULL)\n\t\t\tspa_strfree(dp->scd_path);\n\t\tkmem_free(dp, sizeof (spa_config_dirent_t));\n\t}\n\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tavl_destroy(&spa->spa_allocs[i].spaa_tree);\n\t\tmutex_destroy(&spa->spa_allocs[i].spaa_lock);\n\t}\n\tkmem_free(spa->spa_allocs, spa->spa_alloc_count *\n\t    sizeof (spa_alloc_t));\n\n\tavl_destroy(&spa->spa_metaslabs_by_flushed);\n\tavl_destroy(&spa->spa_sm_logs_by_txg);\n\tlist_destroy(&spa->spa_log_summary);\n\tlist_destroy(&spa->spa_config_list);\n\tlist_destroy(&spa->spa_leaf_list);\n\n\tnvlist_free(spa->spa_label_features);\n\tnvlist_free(spa->spa_load_info);\n\tnvlist_free(spa->spa_feat_stats);\n\tspa_config_set(spa, NULL);\n\n\tzfs_refcount_destroy(&spa->spa_refcount);\n\n\tspa_stats_destroy(spa);\n\tspa_config_lock_destroy(spa);\n\n\tfor (int t = 0; t < TXG_SIZE; t++)\n\t\tbplist_destroy(&spa->spa_free_bplist[t]);\n\n\tzio_checksum_templates_free(spa);\n\n\tcv_destroy(&spa->spa_async_cv);\n\tcv_destroy(&spa->spa_evicting_os_cv);\n\tcv_destroy(&spa->spa_proc_cv);\n\tcv_destroy(&spa->spa_scrub_io_cv);\n\tcv_destroy(&spa->spa_suspend_cv);\n\tcv_destroy(&spa->spa_activities_cv);\n\tcv_destroy(&spa->spa_waiters_cv);\n\n\tmutex_destroy(&spa->spa_flushed_ms_lock);\n\tmutex_destroy(&spa->spa_async_lock);\n\tmutex_destroy(&spa->spa_errlist_lock);\n\tmutex_destroy(&spa->spa_errlog_lock);\n\tmutex_destroy(&spa->spa_evicting_os_lock);\n\tmutex_destroy(&spa->spa_history_lock);\n\tmutex_destroy(&spa->spa_proc_lock);\n\tmutex_destroy(&spa->spa_props_lock);\n\tmutex_destroy(&spa->spa_cksum_tmpls_lock);\n\tmutex_destroy(&spa->spa_scrub_lock);\n\tmutex_destroy(&spa->spa_suspend_lock);\n\tmutex_destroy(&spa->spa_vdev_top_lock);\n\tmutex_destroy(&spa->spa_feat_stats_lock);\n\tmutex_destroy(&spa->spa_activities_lock);\n\n\tkmem_free(spa, sizeof (spa_t));\n}\n\n \nspa_t *\nspa_next(spa_t *prev)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tif (prev)\n\t\treturn (AVL_NEXT(&spa_namespace_avl, prev));\n\telse\n\t\treturn (avl_first(&spa_namespace_avl));\n}\n\n \n\n \nvoid\nspa_open_ref(spa_t *spa, const void *tag)\n{\n\tASSERT(zfs_refcount_count(&spa->spa_refcount) >= spa->spa_minref ||\n\t    MUTEX_HELD(&spa_namespace_lock));\n\t(void) zfs_refcount_add(&spa->spa_refcount, tag);\n}\n\n \nvoid\nspa_close(spa_t *spa, const void *tag)\n{\n\tASSERT(zfs_refcount_count(&spa->spa_refcount) > spa->spa_minref ||\n\t    MUTEX_HELD(&spa_namespace_lock));\n\t(void) zfs_refcount_remove(&spa->spa_refcount, tag);\n}\n\n \nvoid\nspa_async_close(spa_t *spa, const void *tag)\n{\n\t(void) zfs_refcount_remove(&spa->spa_refcount, tag);\n}\n\n \nboolean_t\nspa_refcount_zero(spa_t *spa)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\treturn (zfs_refcount_count(&spa->spa_refcount) == spa->spa_minref);\n}\n\n \n\n \n\ntypedef struct spa_aux {\n\tuint64_t\taux_guid;\n\tuint64_t\taux_pool;\n\tavl_node_t\taux_avl;\n\tint\t\taux_count;\n} spa_aux_t;\n\nstatic inline int\nspa_aux_compare(const void *a, const void *b)\n{\n\tconst spa_aux_t *sa = (const spa_aux_t *)a;\n\tconst spa_aux_t *sb = (const spa_aux_t *)b;\n\n\treturn (TREE_CMP(sa->aux_guid, sb->aux_guid));\n}\n\nstatic void\nspa_aux_add(vdev_t *vd, avl_tree_t *avl)\n{\n\tavl_index_t where;\n\tspa_aux_t search;\n\tspa_aux_t *aux;\n\n\tsearch.aux_guid = vd->vdev_guid;\n\tif ((aux = avl_find(avl, &search, &where)) != NULL) {\n\t\taux->aux_count++;\n\t} else {\n\t\taux = kmem_zalloc(sizeof (spa_aux_t), KM_SLEEP);\n\t\taux->aux_guid = vd->vdev_guid;\n\t\taux->aux_count = 1;\n\t\tavl_insert(avl, aux, where);\n\t}\n}\n\nstatic void\nspa_aux_remove(vdev_t *vd, avl_tree_t *avl)\n{\n\tspa_aux_t search;\n\tspa_aux_t *aux;\n\tavl_index_t where;\n\n\tsearch.aux_guid = vd->vdev_guid;\n\taux = avl_find(avl, &search, &where);\n\n\tASSERT(aux != NULL);\n\n\tif (--aux->aux_count == 0) {\n\t\tavl_remove(avl, aux);\n\t\tkmem_free(aux, sizeof (spa_aux_t));\n\t} else if (aux->aux_pool == spa_guid(vd->vdev_spa)) {\n\t\taux->aux_pool = 0ULL;\n\t}\n}\n\nstatic boolean_t\nspa_aux_exists(uint64_t guid, uint64_t *pool, int *refcnt, avl_tree_t *avl)\n{\n\tspa_aux_t search, *found;\n\n\tsearch.aux_guid = guid;\n\tfound = avl_find(avl, &search, NULL);\n\n\tif (pool) {\n\t\tif (found)\n\t\t\t*pool = found->aux_pool;\n\t\telse\n\t\t\t*pool = 0ULL;\n\t}\n\n\tif (refcnt) {\n\t\tif (found)\n\t\t\t*refcnt = found->aux_count;\n\t\telse\n\t\t\t*refcnt = 0;\n\t}\n\n\treturn (found != NULL);\n}\n\nstatic void\nspa_aux_activate(vdev_t *vd, avl_tree_t *avl)\n{\n\tspa_aux_t search, *found;\n\tavl_index_t where;\n\n\tsearch.aux_guid = vd->vdev_guid;\n\tfound = avl_find(avl, &search, &where);\n\tASSERT(found != NULL);\n\tASSERT(found->aux_pool == 0ULL);\n\n\tfound->aux_pool = spa_guid(vd->vdev_spa);\n}\n\n \n\nstatic int\nspa_spare_compare(const void *a, const void *b)\n{\n\treturn (spa_aux_compare(a, b));\n}\n\nvoid\nspa_spare_add(vdev_t *vd)\n{\n\tmutex_enter(&spa_spare_lock);\n\tASSERT(!vd->vdev_isspare);\n\tspa_aux_add(vd, &spa_spare_avl);\n\tvd->vdev_isspare = B_TRUE;\n\tmutex_exit(&spa_spare_lock);\n}\n\nvoid\nspa_spare_remove(vdev_t *vd)\n{\n\tmutex_enter(&spa_spare_lock);\n\tASSERT(vd->vdev_isspare);\n\tspa_aux_remove(vd, &spa_spare_avl);\n\tvd->vdev_isspare = B_FALSE;\n\tmutex_exit(&spa_spare_lock);\n}\n\nboolean_t\nspa_spare_exists(uint64_t guid, uint64_t *pool, int *refcnt)\n{\n\tboolean_t found;\n\n\tmutex_enter(&spa_spare_lock);\n\tfound = spa_aux_exists(guid, pool, refcnt, &spa_spare_avl);\n\tmutex_exit(&spa_spare_lock);\n\n\treturn (found);\n}\n\nvoid\nspa_spare_activate(vdev_t *vd)\n{\n\tmutex_enter(&spa_spare_lock);\n\tASSERT(vd->vdev_isspare);\n\tspa_aux_activate(vd, &spa_spare_avl);\n\tmutex_exit(&spa_spare_lock);\n}\n\n \n\nstatic int\nspa_l2cache_compare(const void *a, const void *b)\n{\n\treturn (spa_aux_compare(a, b));\n}\n\nvoid\nspa_l2cache_add(vdev_t *vd)\n{\n\tmutex_enter(&spa_l2cache_lock);\n\tASSERT(!vd->vdev_isl2cache);\n\tspa_aux_add(vd, &spa_l2cache_avl);\n\tvd->vdev_isl2cache = B_TRUE;\n\tmutex_exit(&spa_l2cache_lock);\n}\n\nvoid\nspa_l2cache_remove(vdev_t *vd)\n{\n\tmutex_enter(&spa_l2cache_lock);\n\tASSERT(vd->vdev_isl2cache);\n\tspa_aux_remove(vd, &spa_l2cache_avl);\n\tvd->vdev_isl2cache = B_FALSE;\n\tmutex_exit(&spa_l2cache_lock);\n}\n\nboolean_t\nspa_l2cache_exists(uint64_t guid, uint64_t *pool)\n{\n\tboolean_t found;\n\n\tmutex_enter(&spa_l2cache_lock);\n\tfound = spa_aux_exists(guid, pool, NULL, &spa_l2cache_avl);\n\tmutex_exit(&spa_l2cache_lock);\n\n\treturn (found);\n}\n\nvoid\nspa_l2cache_activate(vdev_t *vd)\n{\n\tmutex_enter(&spa_l2cache_lock);\n\tASSERT(vd->vdev_isl2cache);\n\tspa_aux_activate(vd, &spa_l2cache_avl);\n\tmutex_exit(&spa_l2cache_lock);\n}\n\n \n\n \nuint64_t\nspa_vdev_enter(spa_t *spa)\n{\n\tmutex_enter(&spa->spa_vdev_top_lock);\n\tmutex_enter(&spa_namespace_lock);\n\n\tvdev_autotrim_stop_all(spa);\n\n\treturn (spa_vdev_config_enter(spa));\n}\n\n \nuint64_t\nspa_vdev_detach_enter(spa_t *spa, uint64_t guid)\n{\n\tmutex_enter(&spa->spa_vdev_top_lock);\n\tmutex_enter(&spa_namespace_lock);\n\n\tvdev_autotrim_stop_all(spa);\n\n\tif (guid != 0) {\n\t\tvdev_t *vd = spa_lookup_by_guid(spa, guid, B_FALSE);\n\t\tif (vd) {\n\t\t\tvdev_rebuild_stop_wait(vd->vdev_top);\n\t\t}\n\t}\n\n\treturn (spa_vdev_config_enter(spa));\n}\n\n \nuint64_t\nspa_vdev_config_enter(spa_t *spa)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tspa_config_enter(spa, SCL_ALL, spa, RW_WRITER);\n\n\treturn (spa_last_synced_txg(spa) + 1);\n}\n\n \nvoid\nspa_vdev_config_exit(spa_t *spa, vdev_t *vd, uint64_t txg, int error,\n    const char *tag)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tint config_changed = B_FALSE;\n\n\tASSERT(txg > spa_last_synced_txg(spa));\n\n\tspa->spa_pending_vdev = NULL;\n\n\t \n\tvdev_dtl_reassess(spa->spa_root_vdev, 0, 0, B_FALSE, B_FALSE);\n\n\tif (error == 0 && !list_is_empty(&spa->spa_config_dirty_list)) {\n\t\tconfig_changed = B_TRUE;\n\t\tspa->spa_config_generation++;\n\t}\n\n\t \n\tASSERT(metaslab_class_validate(spa_normal_class(spa)) == 0);\n\tASSERT(metaslab_class_validate(spa_log_class(spa)) == 0);\n\tASSERT(metaslab_class_validate(spa_embedded_log_class(spa)) == 0);\n\tASSERT(metaslab_class_validate(spa_special_class(spa)) == 0);\n\tASSERT(metaslab_class_validate(spa_dedup_class(spa)) == 0);\n\n\tspa_config_exit(spa, SCL_ALL, spa);\n\n\t \n\tif (zio_injection_enabled)\n\t\tzio_handle_panic_injection(spa, tag, 0);\n\n\t \n\tif (error == 0)\n\t\ttxg_wait_synced(spa->spa_dsl_pool, txg);\n\n\tif (vd != NULL) {\n\t\tASSERT(!vd->vdev_detached || vd->vdev_dtl_sm == NULL);\n\t\tif (vd->vdev_ops->vdev_op_leaf) {\n\t\t\tmutex_enter(&vd->vdev_initialize_lock);\n\t\t\tvdev_initialize_stop(vd, VDEV_INITIALIZE_CANCELED,\n\t\t\t    NULL);\n\t\t\tmutex_exit(&vd->vdev_initialize_lock);\n\n\t\t\tmutex_enter(&vd->vdev_trim_lock);\n\t\t\tvdev_trim_stop(vd, VDEV_TRIM_CANCELED, NULL);\n\t\t\tmutex_exit(&vd->vdev_trim_lock);\n\t\t}\n\n\t\t \n\t\tvdev_autotrim_stop_wait(vd);\n\n\t\tspa_config_enter(spa, SCL_STATE_ALL, spa, RW_WRITER);\n\t\tvdev_free(vd);\n\t\tspa_config_exit(spa, SCL_STATE_ALL, spa);\n\t}\n\n\t \n\tif (config_changed)\n\t\tspa_write_cachefile(spa, B_FALSE, B_TRUE, B_TRUE);\n}\n\n \nint\nspa_vdev_exit(spa_t *spa, vdev_t *vd, uint64_t txg, int error)\n{\n\tvdev_autotrim_restart(spa);\n\tvdev_rebuild_restart(spa);\n\n\tspa_vdev_config_exit(spa, vd, txg, error, FTAG);\n\tmutex_exit(&spa_namespace_lock);\n\tmutex_exit(&spa->spa_vdev_top_lock);\n\n\treturn (error);\n}\n\n \nvoid\nspa_vdev_state_enter(spa_t *spa, int oplocks)\n{\n\tint locks = SCL_STATE_ALL | oplocks;\n\n\t \n\tif (spa_is_root(spa)) {\n\t\tint low = locks & ~(SCL_ZIO - 1);\n\t\tint high = locks & ~low;\n\n\t\tspa_config_enter(spa, high, spa, RW_WRITER);\n\t\tvdev_hold(spa->spa_root_vdev);\n\t\tspa_config_enter(spa, low, spa, RW_WRITER);\n\t} else {\n\t\tspa_config_enter(spa, locks, spa, RW_WRITER);\n\t}\n\tspa->spa_vdev_locks = locks;\n}\n\nint\nspa_vdev_state_exit(spa_t *spa, vdev_t *vd, int error)\n{\n\tboolean_t config_changed = B_FALSE;\n\tvdev_t *vdev_top;\n\n\tif (vd == NULL || vd == spa->spa_root_vdev) {\n\t\tvdev_top = spa->spa_root_vdev;\n\t} else {\n\t\tvdev_top = vd->vdev_top;\n\t}\n\n\tif (vd != NULL || error == 0)\n\t\tvdev_dtl_reassess(vdev_top, 0, 0, B_FALSE, B_FALSE);\n\n\tif (vd != NULL) {\n\t\tif (vd != spa->spa_root_vdev)\n\t\t\tvdev_state_dirty(vdev_top);\n\n\t\tconfig_changed = B_TRUE;\n\t\tspa->spa_config_generation++;\n\t}\n\n\tif (spa_is_root(spa))\n\t\tvdev_rele(spa->spa_root_vdev);\n\n\tASSERT3U(spa->spa_vdev_locks, >=, SCL_STATE_ALL);\n\tspa_config_exit(spa, spa->spa_vdev_locks, spa);\n\n\t \n\tif (vd != NULL)\n\t\ttxg_wait_synced(spa->spa_dsl_pool, 0);\n\n\t \n\tif (config_changed) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_write_cachefile(spa, B_FALSE, B_TRUE, B_FALSE);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\treturn (error);\n}\n\n \n\nvoid\nspa_activate_mos_feature(spa_t *spa, const char *feature, dmu_tx_t *tx)\n{\n\tif (!nvlist_exists(spa->spa_label_features, feature)) {\n\t\tfnvlist_add_boolean(spa->spa_label_features, feature);\n\t\t \n\t\tif (tx->tx_txg != TXG_INITIAL)\n\t\t\tvdev_config_dirty(spa->spa_root_vdev);\n\t}\n}\n\nvoid\nspa_deactivate_mos_feature(spa_t *spa, const char *feature)\n{\n\tif (nvlist_remove_all(spa->spa_label_features, feature) == 0)\n\t\tvdev_config_dirty(spa->spa_root_vdev);\n}\n\n \nspa_t *\nspa_by_guid(uint64_t pool_guid, uint64_t device_guid)\n{\n\tspa_t *spa;\n\tavl_tree_t *t = &spa_namespace_avl;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tfor (spa = avl_first(t); spa != NULL; spa = AVL_NEXT(t, spa)) {\n\t\tif (spa->spa_state == POOL_STATE_UNINITIALIZED)\n\t\t\tcontinue;\n\t\tif (spa->spa_root_vdev == NULL)\n\t\t\tcontinue;\n\t\tif (spa_guid(spa) == pool_guid) {\n\t\t\tif (device_guid == 0)\n\t\t\t\tbreak;\n\n\t\t\tif (vdev_lookup_by_guid(spa->spa_root_vdev,\n\t\t\t    device_guid) != NULL)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (spa->spa_pending_vdev) {\n\t\t\t\tif (vdev_lookup_by_guid(spa->spa_pending_vdev,\n\t\t\t\t    device_guid) != NULL)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn (spa);\n}\n\n \nboolean_t\nspa_guid_exists(uint64_t pool_guid, uint64_t device_guid)\n{\n\treturn (spa_by_guid(pool_guid, device_guid) != NULL);\n}\n\nchar *\nspa_strdup(const char *s)\n{\n\tsize_t len;\n\tchar *new;\n\n\tlen = strlen(s);\n\tnew = kmem_alloc(len + 1, KM_SLEEP);\n\tmemcpy(new, s, len + 1);\n\n\treturn (new);\n}\n\nvoid\nspa_strfree(char *s)\n{\n\tkmem_free(s, strlen(s) + 1);\n}\n\nuint64_t\nspa_generate_guid(spa_t *spa)\n{\n\tuint64_t guid;\n\n\tif (spa != NULL) {\n\t\tdo {\n\t\t\t(void) random_get_pseudo_bytes((void *)&guid,\n\t\t\t    sizeof (guid));\n\t\t} while (guid == 0 || spa_guid_exists(spa_guid(spa), guid));\n\t} else {\n\t\tdo {\n\t\t\t(void) random_get_pseudo_bytes((void *)&guid,\n\t\t\t    sizeof (guid));\n\t\t} while (guid == 0 || spa_guid_exists(guid, 0));\n\t}\n\n\treturn (guid);\n}\n\nvoid\nsnprintf_blkptr(char *buf, size_t buflen, const blkptr_t *bp)\n{\n\tchar type[256];\n\tconst char *checksum = NULL;\n\tconst char *compress = NULL;\n\n\tif (bp != NULL) {\n\t\tif (BP_GET_TYPE(bp) & DMU_OT_NEWTYPE) {\n\t\t\tdmu_object_byteswap_t bswap =\n\t\t\t    DMU_OT_BYTESWAP(BP_GET_TYPE(bp));\n\t\t\t(void) snprintf(type, sizeof (type), \"bswap %s %s\",\n\t\t\t    DMU_OT_IS_METADATA(BP_GET_TYPE(bp)) ?\n\t\t\t    \"metadata\" : \"data\",\n\t\t\t    dmu_ot_byteswap[bswap].ob_name);\n\t\t} else {\n\t\t\t(void) strlcpy(type, dmu_ot[BP_GET_TYPE(bp)].ot_name,\n\t\t\t    sizeof (type));\n\t\t}\n\t\tif (!BP_IS_EMBEDDED(bp)) {\n\t\t\tchecksum =\n\t\t\t    zio_checksum_table[BP_GET_CHECKSUM(bp)].ci_name;\n\t\t}\n\t\tcompress = zio_compress_table[BP_GET_COMPRESS(bp)].ci_name;\n\t}\n\n\tSNPRINTF_BLKPTR(kmem_scnprintf, ' ', buf, buflen, bp, type, checksum,\n\t    compress);\n}\n\nvoid\nspa_freeze(spa_t *spa)\n{\n\tuint64_t freeze_txg = 0;\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\tif (spa->spa_freeze_txg == UINT64_MAX) {\n\t\tfreeze_txg = spa_last_synced_txg(spa) + TXG_SIZE;\n\t\tspa->spa_freeze_txg = freeze_txg;\n\t}\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\tif (freeze_txg != 0)\n\t\ttxg_wait_synced(spa_get_dsl(spa), freeze_txg);\n}\n\nvoid\nzfs_panic_recover(const char *fmt, ...)\n{\n\tva_list adx;\n\n\tva_start(adx, fmt);\n\tvcmn_err(zfs_recover ? CE_WARN : CE_PANIC, fmt, adx);\n\tva_end(adx);\n}\n\n \nuint64_t\nzfs_strtonum(const char *str, char **nptr)\n{\n\tuint64_t val = 0;\n\tchar c;\n\tint digit;\n\n\twhile ((c = *str) != '\\0') {\n\t\tif (c >= '0' && c <= '9')\n\t\t\tdigit = c - '0';\n\t\telse if (c >= 'a' && c <= 'f')\n\t\t\tdigit = 10 + c - 'a';\n\t\telse\n\t\t\tbreak;\n\n\t\tval *= 16;\n\t\tval += digit;\n\n\t\tstr++;\n\t}\n\n\tif (nptr)\n\t\t*nptr = (char *)str;\n\n\treturn (val);\n}\n\nvoid\nspa_activate_allocation_classes(spa_t *spa, dmu_tx_t *tx)\n{\n\t \n\tASSERT(spa_feature_is_enabled(spa, SPA_FEATURE_ALLOCATION_CLASSES));\n\tspa_feature_incr(spa, SPA_FEATURE_ALLOCATION_CLASSES, tx);\n}\n\n \n\nboolean_t\nspa_shutting_down(spa_t *spa)\n{\n\treturn (spa->spa_async_suspended);\n}\n\ndsl_pool_t *\nspa_get_dsl(spa_t *spa)\n{\n\treturn (spa->spa_dsl_pool);\n}\n\nboolean_t\nspa_is_initializing(spa_t *spa)\n{\n\treturn (spa->spa_is_initializing);\n}\n\nboolean_t\nspa_indirect_vdevs_loaded(spa_t *spa)\n{\n\treturn (spa->spa_indirect_vdevs_loaded);\n}\n\nblkptr_t *\nspa_get_rootblkptr(spa_t *spa)\n{\n\treturn (&spa->spa_ubsync.ub_rootbp);\n}\n\nvoid\nspa_set_rootblkptr(spa_t *spa, const blkptr_t *bp)\n{\n\tspa->spa_uberblock.ub_rootbp = *bp;\n}\n\nvoid\nspa_altroot(spa_t *spa, char *buf, size_t buflen)\n{\n\tif (spa->spa_root == NULL)\n\t\tbuf[0] = '\\0';\n\telse\n\t\t(void) strlcpy(buf, spa->spa_root, buflen);\n}\n\nuint32_t\nspa_sync_pass(spa_t *spa)\n{\n\treturn (spa->spa_sync_pass);\n}\n\nchar *\nspa_name(spa_t *spa)\n{\n\treturn (spa->spa_name);\n}\n\nuint64_t\nspa_guid(spa_t *spa)\n{\n\tdsl_pool_t *dp = spa_get_dsl(spa);\n\tuint64_t guid;\n\n\t \n\tif (spa->spa_root_vdev == NULL)\n\t\treturn (spa->spa_config_guid);\n\n\tguid = spa->spa_last_synced_guid != 0 ?\n\t    spa->spa_last_synced_guid : spa->spa_root_vdev->vdev_guid;\n\n\t \n\tif (dp && dsl_pool_sync_context(dp))\n\t\treturn (spa->spa_root_vdev->vdev_guid);\n\telse\n\t\treturn (guid);\n}\n\nuint64_t\nspa_load_guid(spa_t *spa)\n{\n\t \n\treturn (spa->spa_load_guid);\n}\n\nuint64_t\nspa_last_synced_txg(spa_t *spa)\n{\n\treturn (spa->spa_ubsync.ub_txg);\n}\n\nuint64_t\nspa_first_txg(spa_t *spa)\n{\n\treturn (spa->spa_first_txg);\n}\n\nuint64_t\nspa_syncing_txg(spa_t *spa)\n{\n\treturn (spa->spa_syncing_txg);\n}\n\n \nuint64_t\nspa_final_dirty_txg(spa_t *spa)\n{\n\treturn (spa->spa_final_txg - TXG_DEFER_SIZE);\n}\n\npool_state_t\nspa_state(spa_t *spa)\n{\n\treturn (spa->spa_state);\n}\n\nspa_load_state_t\nspa_load_state(spa_t *spa)\n{\n\treturn (spa->spa_load_state);\n}\n\nuint64_t\nspa_freeze_txg(spa_t *spa)\n{\n\treturn (spa->spa_freeze_txg);\n}\n\n \nuint64_t\nspa_get_worst_case_asize(spa_t *spa, uint64_t lsize)\n{\n\tif (lsize == 0)\n\t\treturn (0);\t \n\treturn (MAX(lsize, 1 << spa->spa_max_ashift) * spa_asize_inflation);\n}\n\n \nuint64_t\nspa_get_slop_space(spa_t *spa)\n{\n\tuint64_t space = 0;\n\tuint64_t slop = 0;\n\n\t \n\tif (spa->spa_dedup_dspace == ~0ULL)\n\t\tspa_update_dspace(spa);\n\n\t \n\tspace = spa_get_dspace(spa) - spa->spa_dedup_dspace;\n\tslop = MIN(space >> spa_slop_shift, spa_max_slop);\n\n\t \n\tuint64_t embedded_log =\n\t    metaslab_class_get_dspace(spa_embedded_log_class(spa));\n\tslop -= MIN(embedded_log, slop >> 1);\n\n\t \n\tslop = MAX(slop, MIN(space >> 1, spa_min_slop));\n\treturn (slop);\n}\n\nuint64_t\nspa_get_dspace(spa_t *spa)\n{\n\treturn (spa->spa_dspace);\n}\n\nuint64_t\nspa_get_checkpoint_space(spa_t *spa)\n{\n\treturn (spa->spa_checkpoint_info.sci_dspace);\n}\n\nvoid\nspa_update_dspace(spa_t *spa)\n{\n\tspa->spa_dspace = metaslab_class_get_dspace(spa_normal_class(spa)) +\n\t    ddt_get_dedup_dspace(spa) + brt_get_dspace(spa);\n\tif (spa->spa_nonallocating_dspace > 0) {\n\t\t \n\t\tASSERT3U(spa->spa_dspace, >=, spa->spa_nonallocating_dspace);\n\t\tspa->spa_dspace -= spa->spa_nonallocating_dspace;\n\t}\n}\n\n \nuint64_t\nspa_get_failmode(spa_t *spa)\n{\n\treturn (spa->spa_failmode);\n}\n\nboolean_t\nspa_suspended(spa_t *spa)\n{\n\treturn (spa->spa_suspended != ZIO_SUSPEND_NONE);\n}\n\nuint64_t\nspa_version(spa_t *spa)\n{\n\treturn (spa->spa_ubsync.ub_version);\n}\n\nboolean_t\nspa_deflate(spa_t *spa)\n{\n\treturn (spa->spa_deflate);\n}\n\nmetaslab_class_t *\nspa_normal_class(spa_t *spa)\n{\n\treturn (spa->spa_normal_class);\n}\n\nmetaslab_class_t *\nspa_log_class(spa_t *spa)\n{\n\treturn (spa->spa_log_class);\n}\n\nmetaslab_class_t *\nspa_embedded_log_class(spa_t *spa)\n{\n\treturn (spa->spa_embedded_log_class);\n}\n\nmetaslab_class_t *\nspa_special_class(spa_t *spa)\n{\n\treturn (spa->spa_special_class);\n}\n\nmetaslab_class_t *\nspa_dedup_class(spa_t *spa)\n{\n\treturn (spa->spa_dedup_class);\n}\n\n \nmetaslab_class_t *\nspa_preferred_class(spa_t *spa, uint64_t size, dmu_object_type_t objtype,\n    uint_t level, uint_t special_smallblk)\n{\n\t \n\tASSERT(objtype != DMU_OT_INTENT_LOG);\n\n\tboolean_t has_special_class = spa->spa_special_class->mc_groups != 0;\n\n\tif (DMU_OT_IS_DDT(objtype)) {\n\t\tif (spa->spa_dedup_class->mc_groups != 0)\n\t\t\treturn (spa_dedup_class(spa));\n\t\telse if (has_special_class && zfs_ddt_data_is_special)\n\t\t\treturn (spa_special_class(spa));\n\t\telse\n\t\t\treturn (spa_normal_class(spa));\n\t}\n\n\t \n\tif (level > 0 && (DMU_OT_IS_FILE(objtype) || objtype == DMU_OT_ZVOL)) {\n\t\tif (has_special_class && zfs_user_indirect_is_special)\n\t\t\treturn (spa_special_class(spa));\n\t\telse\n\t\t\treturn (spa_normal_class(spa));\n\t}\n\n\tif (DMU_OT_IS_METADATA(objtype) || level > 0) {\n\t\tif (has_special_class)\n\t\t\treturn (spa_special_class(spa));\n\t\telse\n\t\t\treturn (spa_normal_class(spa));\n\t}\n\n\t \n\tif (DMU_OT_IS_FILE(objtype) &&\n\t    has_special_class && size <= special_smallblk) {\n\t\tmetaslab_class_t *special = spa_special_class(spa);\n\t\tuint64_t alloc = metaslab_class_get_alloc(special);\n\t\tuint64_t space = metaslab_class_get_space(special);\n\t\tuint64_t limit =\n\t\t    (space * (100 - zfs_special_class_metadata_reserve_pct))\n\t\t    / 100;\n\n\t\tif (alloc < limit)\n\t\t\treturn (special);\n\t}\n\n\treturn (spa_normal_class(spa));\n}\n\nvoid\nspa_evicting_os_register(spa_t *spa, objset_t *os)\n{\n\tmutex_enter(&spa->spa_evicting_os_lock);\n\tlist_insert_head(&spa->spa_evicting_os_list, os);\n\tmutex_exit(&spa->spa_evicting_os_lock);\n}\n\nvoid\nspa_evicting_os_deregister(spa_t *spa, objset_t *os)\n{\n\tmutex_enter(&spa->spa_evicting_os_lock);\n\tlist_remove(&spa->spa_evicting_os_list, os);\n\tcv_broadcast(&spa->spa_evicting_os_cv);\n\tmutex_exit(&spa->spa_evicting_os_lock);\n}\n\nvoid\nspa_evicting_os_wait(spa_t *spa)\n{\n\tmutex_enter(&spa->spa_evicting_os_lock);\n\twhile (!list_is_empty(&spa->spa_evicting_os_list))\n\t\tcv_wait(&spa->spa_evicting_os_cv, &spa->spa_evicting_os_lock);\n\tmutex_exit(&spa->spa_evicting_os_lock);\n\n\tdmu_buf_user_evict_wait();\n}\n\nint\nspa_max_replication(spa_t *spa)\n{\n\t \n\tif (spa_version(spa) < SPA_VERSION_DITTO_BLOCKS)\n\t\treturn (1);\n\treturn (MIN(SPA_DVAS_PER_BP, spa_max_replication_override));\n}\n\nint\nspa_prev_software_version(spa_t *spa)\n{\n\treturn (spa->spa_prev_software_version);\n}\n\nuint64_t\nspa_deadman_synctime(spa_t *spa)\n{\n\treturn (spa->spa_deadman_synctime);\n}\n\nspa_autotrim_t\nspa_get_autotrim(spa_t *spa)\n{\n\treturn (spa->spa_autotrim);\n}\n\nuint64_t\nspa_deadman_ziotime(spa_t *spa)\n{\n\treturn (spa->spa_deadman_ziotime);\n}\n\nuint64_t\nspa_get_deadman_failmode(spa_t *spa)\n{\n\treturn (spa->spa_deadman_failmode);\n}\n\nvoid\nspa_set_deadman_failmode(spa_t *spa, const char *failmode)\n{\n\tif (strcmp(failmode, \"wait\") == 0)\n\t\tspa->spa_deadman_failmode = ZIO_FAILURE_MODE_WAIT;\n\telse if (strcmp(failmode, \"continue\") == 0)\n\t\tspa->spa_deadman_failmode = ZIO_FAILURE_MODE_CONTINUE;\n\telse if (strcmp(failmode, \"panic\") == 0)\n\t\tspa->spa_deadman_failmode = ZIO_FAILURE_MODE_PANIC;\n\telse\n\t\tspa->spa_deadman_failmode = ZIO_FAILURE_MODE_WAIT;\n}\n\nvoid\nspa_set_deadman_ziotime(hrtime_t ns)\n{\n\tspa_t *spa = NULL;\n\n\tif (spa_mode_global != SPA_MODE_UNINIT) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\twhile ((spa = spa_next(spa)) != NULL)\n\t\t\tspa->spa_deadman_ziotime = ns;\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n}\n\nvoid\nspa_set_deadman_synctime(hrtime_t ns)\n{\n\tspa_t *spa = NULL;\n\n\tif (spa_mode_global != SPA_MODE_UNINIT) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\twhile ((spa = spa_next(spa)) != NULL)\n\t\t\tspa->spa_deadman_synctime = ns;\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n}\n\nuint64_t\ndva_get_dsize_sync(spa_t *spa, const dva_t *dva)\n{\n\tuint64_t asize = DVA_GET_ASIZE(dva);\n\tuint64_t dsize = asize;\n\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_READER) != 0);\n\n\tif (asize != 0 && spa->spa_deflate) {\n\t\tvdev_t *vd = vdev_lookup_top(spa, DVA_GET_VDEV(dva));\n\t\tif (vd != NULL)\n\t\t\tdsize = (asize >> SPA_MINBLOCKSHIFT) *\n\t\t\t    vd->vdev_deflate_ratio;\n\t}\n\n\treturn (dsize);\n}\n\nuint64_t\nbp_get_dsize_sync(spa_t *spa, const blkptr_t *bp)\n{\n\tuint64_t dsize = 0;\n\n\tfor (int d = 0; d < BP_GET_NDVAS(bp); d++)\n\t\tdsize += dva_get_dsize_sync(spa, &bp->blk_dva[d]);\n\n\treturn (dsize);\n}\n\nuint64_t\nbp_get_dsize(spa_t *spa, const blkptr_t *bp)\n{\n\tuint64_t dsize = 0;\n\n\tspa_config_enter(spa, SCL_VDEV, FTAG, RW_READER);\n\n\tfor (int d = 0; d < BP_GET_NDVAS(bp); d++)\n\t\tdsize += dva_get_dsize_sync(spa, &bp->blk_dva[d]);\n\n\tspa_config_exit(spa, SCL_VDEV, FTAG);\n\n\treturn (dsize);\n}\n\nuint64_t\nspa_dirty_data(spa_t *spa)\n{\n\treturn (spa->spa_dsl_pool->dp_dirty_total);\n}\n\n \n\ntypedef struct spa_import_progress {\n\tuint64_t\t\tpool_guid;\t \n\tchar\t\t\t*pool_name;\n\tspa_load_state_t\tspa_load_state;\n\tuint64_t\t\tmmp_sec_remaining;\t \n\tuint64_t\t\tspa_load_max_txg;\t \n\tprocfs_list_node_t\tsmh_node;\n} spa_import_progress_t;\n\nspa_history_list_t *spa_import_progress_list = NULL;\n\nstatic int\nspa_import_progress_show_header(struct seq_file *f)\n{\n\tseq_printf(f, \"%-20s %-14s %-14s %-12s %s\\n\", \"pool_guid\",\n\t    \"load_state\", \"multihost_secs\", \"max_txg\",\n\t    \"pool_name\");\n\treturn (0);\n}\n\nstatic int\nspa_import_progress_show(struct seq_file *f, void *data)\n{\n\tspa_import_progress_t *sip = (spa_import_progress_t *)data;\n\n\tseq_printf(f, \"%-20llu %-14llu %-14llu %-12llu %s\\n\",\n\t    (u_longlong_t)sip->pool_guid, (u_longlong_t)sip->spa_load_state,\n\t    (u_longlong_t)sip->mmp_sec_remaining,\n\t    (u_longlong_t)sip->spa_load_max_txg,\n\t    (sip->pool_name ? sip->pool_name : \"-\"));\n\n\treturn (0);\n}\n\n \nstatic void\nspa_import_progress_truncate(spa_history_list_t *shl, unsigned int size)\n{\n\tspa_import_progress_t *sip;\n\twhile (shl->size > size) {\n\t\tsip = list_remove_head(&shl->procfs_list.pl_list);\n\t\tif (sip->pool_name)\n\t\t\tspa_strfree(sip->pool_name);\n\t\tkmem_free(sip, sizeof (spa_import_progress_t));\n\t\tshl->size--;\n\t}\n\n\tIMPLY(size == 0, list_is_empty(&shl->procfs_list.pl_list));\n}\n\nstatic void\nspa_import_progress_init(void)\n{\n\tspa_import_progress_list = kmem_zalloc(sizeof (spa_history_list_t),\n\t    KM_SLEEP);\n\n\tspa_import_progress_list->size = 0;\n\n\tspa_import_progress_list->procfs_list.pl_private =\n\t    spa_import_progress_list;\n\n\tprocfs_list_install(\"zfs\",\n\t    NULL,\n\t    \"import_progress\",\n\t    0644,\n\t    &spa_import_progress_list->procfs_list,\n\t    spa_import_progress_show,\n\t    spa_import_progress_show_header,\n\t    NULL,\n\t    offsetof(spa_import_progress_t, smh_node));\n}\n\nstatic void\nspa_import_progress_destroy(void)\n{\n\tspa_history_list_t *shl = spa_import_progress_list;\n\tprocfs_list_uninstall(&shl->procfs_list);\n\tspa_import_progress_truncate(shl, 0);\n\tprocfs_list_destroy(&shl->procfs_list);\n\tkmem_free(shl, sizeof (spa_history_list_t));\n}\n\nint\nspa_import_progress_set_state(uint64_t pool_guid,\n    spa_load_state_t load_state)\n{\n\tspa_history_list_t *shl = spa_import_progress_list;\n\tspa_import_progress_t *sip;\n\tint error = ENOENT;\n\n\tif (shl->size == 0)\n\t\treturn (0);\n\n\tmutex_enter(&shl->procfs_list.pl_lock);\n\tfor (sip = list_tail(&shl->procfs_list.pl_list); sip != NULL;\n\t    sip = list_prev(&shl->procfs_list.pl_list, sip)) {\n\t\tif (sip->pool_guid == pool_guid) {\n\t\t\tsip->spa_load_state = load_state;\n\t\t\terror = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_exit(&shl->procfs_list.pl_lock);\n\n\treturn (error);\n}\n\nint\nspa_import_progress_set_max_txg(uint64_t pool_guid, uint64_t load_max_txg)\n{\n\tspa_history_list_t *shl = spa_import_progress_list;\n\tspa_import_progress_t *sip;\n\tint error = ENOENT;\n\n\tif (shl->size == 0)\n\t\treturn (0);\n\n\tmutex_enter(&shl->procfs_list.pl_lock);\n\tfor (sip = list_tail(&shl->procfs_list.pl_list); sip != NULL;\n\t    sip = list_prev(&shl->procfs_list.pl_list, sip)) {\n\t\tif (sip->pool_guid == pool_guid) {\n\t\t\tsip->spa_load_max_txg = load_max_txg;\n\t\t\terror = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_exit(&shl->procfs_list.pl_lock);\n\n\treturn (error);\n}\n\nint\nspa_import_progress_set_mmp_check(uint64_t pool_guid,\n    uint64_t mmp_sec_remaining)\n{\n\tspa_history_list_t *shl = spa_import_progress_list;\n\tspa_import_progress_t *sip;\n\tint error = ENOENT;\n\n\tif (shl->size == 0)\n\t\treturn (0);\n\n\tmutex_enter(&shl->procfs_list.pl_lock);\n\tfor (sip = list_tail(&shl->procfs_list.pl_list); sip != NULL;\n\t    sip = list_prev(&shl->procfs_list.pl_list, sip)) {\n\t\tif (sip->pool_guid == pool_guid) {\n\t\t\tsip->mmp_sec_remaining = mmp_sec_remaining;\n\t\t\terror = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_exit(&shl->procfs_list.pl_lock);\n\n\treturn (error);\n}\n\n \nvoid\nspa_import_progress_add(spa_t *spa)\n{\n\tspa_history_list_t *shl = spa_import_progress_list;\n\tspa_import_progress_t *sip;\n\tconst char *poolname = NULL;\n\n\tsip = kmem_zalloc(sizeof (spa_import_progress_t), KM_SLEEP);\n\tsip->pool_guid = spa_guid(spa);\n\n\t(void) nvlist_lookup_string(spa->spa_config, ZPOOL_CONFIG_POOL_NAME,\n\t    &poolname);\n\tif (poolname == NULL)\n\t\tpoolname = spa_name(spa);\n\tsip->pool_name = spa_strdup(poolname);\n\tsip->spa_load_state = spa_load_state(spa);\n\n\tmutex_enter(&shl->procfs_list.pl_lock);\n\tprocfs_list_add(&shl->procfs_list, sip);\n\tshl->size++;\n\tmutex_exit(&shl->procfs_list.pl_lock);\n}\n\nvoid\nspa_import_progress_remove(uint64_t pool_guid)\n{\n\tspa_history_list_t *shl = spa_import_progress_list;\n\tspa_import_progress_t *sip;\n\n\tmutex_enter(&shl->procfs_list.pl_lock);\n\tfor (sip = list_tail(&shl->procfs_list.pl_list); sip != NULL;\n\t    sip = list_prev(&shl->procfs_list.pl_list, sip)) {\n\t\tif (sip->pool_guid == pool_guid) {\n\t\t\tif (sip->pool_name)\n\t\t\t\tspa_strfree(sip->pool_name);\n\t\t\tlist_remove(&shl->procfs_list.pl_list, sip);\n\t\t\tshl->size--;\n\t\t\tkmem_free(sip, sizeof (spa_import_progress_t));\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_exit(&shl->procfs_list.pl_lock);\n}\n\n \n\nstatic int\nspa_name_compare(const void *a1, const void *a2)\n{\n\tconst spa_t *s1 = a1;\n\tconst spa_t *s2 = a2;\n\tint s;\n\n\ts = strcmp(s1->spa_name, s2->spa_name);\n\n\treturn (TREE_ISIGN(s));\n}\n\nvoid\nspa_boot_init(void)\n{\n\tspa_config_load();\n}\n\nvoid\nspa_init(spa_mode_t mode)\n{\n\tmutex_init(&spa_namespace_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa_spare_lock, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&spa_l2cache_lock, NULL, MUTEX_DEFAULT, NULL);\n\tcv_init(&spa_namespace_cv, NULL, CV_DEFAULT, NULL);\n\n\tavl_create(&spa_namespace_avl, spa_name_compare, sizeof (spa_t),\n\t    offsetof(spa_t, spa_avl));\n\n\tavl_create(&spa_spare_avl, spa_spare_compare, sizeof (spa_aux_t),\n\t    offsetof(spa_aux_t, aux_avl));\n\n\tavl_create(&spa_l2cache_avl, spa_l2cache_compare, sizeof (spa_aux_t),\n\t    offsetof(spa_aux_t, aux_avl));\n\n\tspa_mode_global = mode;\n\n#ifndef _KERNEL\n\tif (spa_mode_global != SPA_MODE_READ && dprintf_find_string(\"watch\")) {\n\t\tstruct sigaction sa;\n\n\t\tsa.sa_flags = SA_SIGINFO;\n\t\tsigemptyset(&sa.sa_mask);\n\t\tsa.sa_sigaction = arc_buf_sigsegv;\n\n\t\tif (sigaction(SIGSEGV, &sa, NULL) == -1) {\n\t\t\tperror(\"could not enable watchpoints: \"\n\t\t\t    \"sigaction(SIGSEGV, ...) = \");\n\t\t} else {\n\t\t\tarc_watch = B_TRUE;\n\t\t}\n\t}\n#endif\n\n\tfm_init();\n\tzfs_refcount_init();\n\tunique_init();\n\tzfs_btree_init();\n\tmetaslab_stat_init();\n\tbrt_init();\n\tddt_init();\n\tzio_init();\n\tdmu_init();\n\tzil_init();\n\tvdev_mirror_stat_init();\n\tvdev_raidz_math_init();\n\tvdev_file_init();\n\tzfs_prop_init();\n\tchksum_init();\n\tzpool_prop_init();\n\tzpool_feature_init();\n\tspa_config_load();\n\tvdev_prop_init();\n\tl2arc_start();\n\tscan_init();\n\tqat_init();\n\tspa_import_progress_init();\n}\n\nvoid\nspa_fini(void)\n{\n\tl2arc_stop();\n\n\tspa_evict_all();\n\n\tvdev_file_fini();\n\tvdev_mirror_stat_fini();\n\tvdev_raidz_math_fini();\n\tchksum_fini();\n\tzil_fini();\n\tdmu_fini();\n\tzio_fini();\n\tddt_fini();\n\tbrt_fini();\n\tmetaslab_stat_fini();\n\tzfs_btree_fini();\n\tunique_fini();\n\tzfs_refcount_fini();\n\tfm_fini();\n\tscan_fini();\n\tqat_fini();\n\tspa_import_progress_destroy();\n\n\tavl_destroy(&spa_namespace_avl);\n\tavl_destroy(&spa_spare_avl);\n\tavl_destroy(&spa_l2cache_avl);\n\n\tcv_destroy(&spa_namespace_cv);\n\tmutex_destroy(&spa_namespace_lock);\n\tmutex_destroy(&spa_spare_lock);\n\tmutex_destroy(&spa_l2cache_lock);\n}\n\n \nboolean_t\nspa_has_slogs(spa_t *spa)\n{\n\treturn (spa->spa_log_class->mc_groups != 0);\n}\n\nspa_log_state_t\nspa_get_log_state(spa_t *spa)\n{\n\treturn (spa->spa_log_state);\n}\n\nvoid\nspa_set_log_state(spa_t *spa, spa_log_state_t state)\n{\n\tspa->spa_log_state = state;\n}\n\nboolean_t\nspa_is_root(spa_t *spa)\n{\n\treturn (spa->spa_is_root);\n}\n\nboolean_t\nspa_writeable(spa_t *spa)\n{\n\treturn (!!(spa->spa_mode & SPA_MODE_WRITE) && spa->spa_trust_config);\n}\n\n \nboolean_t\nspa_has_pending_synctask(spa_t *spa)\n{\n\treturn (!txg_all_lists_empty(&spa->spa_dsl_pool->dp_sync_tasks) ||\n\t    !txg_all_lists_empty(&spa->spa_dsl_pool->dp_early_sync_tasks));\n}\n\nspa_mode_t\nspa_mode(spa_t *spa)\n{\n\treturn (spa->spa_mode);\n}\n\nuint64_t\nspa_bootfs(spa_t *spa)\n{\n\treturn (spa->spa_bootfs);\n}\n\nuint64_t\nspa_delegation(spa_t *spa)\n{\n\treturn (spa->spa_delegation);\n}\n\nobjset_t *\nspa_meta_objset(spa_t *spa)\n{\n\treturn (spa->spa_meta_objset);\n}\n\nenum zio_checksum\nspa_dedup_checksum(spa_t *spa)\n{\n\treturn (spa->spa_dedup_checksum);\n}\n\n \nvoid\nspa_scan_stat_init(spa_t *spa)\n{\n\t \n\tspa->spa_scan_pass_start = gethrestime_sec();\n\tif (dsl_scan_is_paused_scrub(spa->spa_dsl_pool->dp_scan))\n\t\tspa->spa_scan_pass_scrub_pause = spa->spa_scan_pass_start;\n\telse\n\t\tspa->spa_scan_pass_scrub_pause = 0;\n\n\tif (dsl_errorscrub_is_paused(spa->spa_dsl_pool->dp_scan))\n\t\tspa->spa_scan_pass_errorscrub_pause = spa->spa_scan_pass_start;\n\telse\n\t\tspa->spa_scan_pass_errorscrub_pause = 0;\n\n\tspa->spa_scan_pass_scrub_spent_paused = 0;\n\tspa->spa_scan_pass_exam = 0;\n\tspa->spa_scan_pass_issued = 0;\n\n\t\n\tspa->spa_scan_pass_errorscrub_spent_paused = 0;\n}\n\n \nint\nspa_scan_get_stats(spa_t *spa, pool_scan_stat_t *ps)\n{\n\tdsl_scan_t *scn = spa->spa_dsl_pool ? spa->spa_dsl_pool->dp_scan : NULL;\n\n\tif (scn == NULL || (scn->scn_phys.scn_func == POOL_SCAN_NONE &&\n\t    scn->errorscrub_phys.dep_func == POOL_SCAN_NONE))\n\t\treturn (SET_ERROR(ENOENT));\n\n\tmemset(ps, 0, sizeof (pool_scan_stat_t));\n\n\t \n\tps->pss_func = scn->scn_phys.scn_func;\n\tps->pss_state = scn->scn_phys.scn_state;\n\tps->pss_start_time = scn->scn_phys.scn_start_time;\n\tps->pss_end_time = scn->scn_phys.scn_end_time;\n\tps->pss_to_examine = scn->scn_phys.scn_to_examine;\n\tps->pss_examined = scn->scn_phys.scn_examined;\n\tps->pss_skipped = scn->scn_phys.scn_skipped;\n\tps->pss_processed = scn->scn_phys.scn_processed;\n\tps->pss_errors = scn->scn_phys.scn_errors;\n\n\t \n\tps->pss_pass_exam = spa->spa_scan_pass_exam;\n\tps->pss_pass_start = spa->spa_scan_pass_start;\n\tps->pss_pass_scrub_pause = spa->spa_scan_pass_scrub_pause;\n\tps->pss_pass_scrub_spent_paused = spa->spa_scan_pass_scrub_spent_paused;\n\tps->pss_pass_issued = spa->spa_scan_pass_issued;\n\tps->pss_issued =\n\t    scn->scn_issued_before_pass + spa->spa_scan_pass_issued;\n\n\t \n\tps->pss_error_scrub_func = scn->errorscrub_phys.dep_func;\n\tps->pss_error_scrub_state = scn->errorscrub_phys.dep_state;\n\tps->pss_error_scrub_start = scn->errorscrub_phys.dep_start_time;\n\tps->pss_error_scrub_end = scn->errorscrub_phys.dep_end_time;\n\tps->pss_error_scrub_examined = scn->errorscrub_phys.dep_examined;\n\tps->pss_error_scrub_to_be_examined =\n\t    scn->errorscrub_phys.dep_to_examine;\n\n\t \n\tps->pss_pass_error_scrub_pause = spa->spa_scan_pass_errorscrub_pause;\n\n\treturn (0);\n}\n\nint\nspa_maxblocksize(spa_t *spa)\n{\n\tif (spa_feature_is_enabled(spa, SPA_FEATURE_LARGE_BLOCKS))\n\t\treturn (SPA_MAXBLOCKSIZE);\n\telse\n\t\treturn (SPA_OLD_MAXBLOCKSIZE);\n}\n\n\n \nuint64_t\nspa_get_last_removal_txg(spa_t *spa)\n{\n\tuint64_t vdevid;\n\tuint64_t ret = -1ULL;\n\n\tspa_config_enter(spa, SCL_VDEV, FTAG, RW_READER);\n\t \n\tvdevid = spa->spa_removing_phys.sr_prev_indirect_vdev;\n\n\twhile (vdevid != -1ULL) {\n\t\tvdev_t *vd = vdev_lookup_top(spa, vdevid);\n\t\tvdev_indirect_births_t *vib = vd->vdev_indirect_births;\n\n\t\tASSERT3P(vd->vdev_ops, ==, &vdev_indirect_ops);\n\n\t\t \n\t\tif (vdev_indirect_births_count(vib) != 0) {\n\t\t\tret = vdev_indirect_births_last_entry_txg(vib);\n\t\t\tbreak;\n\t\t}\n\n\t\tvdevid = vd->vdev_indirect_config.vic_prev_indirect_vdev;\n\t}\n\tspa_config_exit(spa, SCL_VDEV, FTAG);\n\n\tIMPLY(ret != -1ULL,\n\t    spa_feature_is_active(spa, SPA_FEATURE_DEVICE_REMOVAL));\n\n\treturn (ret);\n}\n\nint\nspa_maxdnodesize(spa_t *spa)\n{\n\tif (spa_feature_is_enabled(spa, SPA_FEATURE_LARGE_DNODE))\n\t\treturn (DNODE_MAX_SIZE);\n\telse\n\t\treturn (DNODE_MIN_SIZE);\n}\n\nboolean_t\nspa_multihost(spa_t *spa)\n{\n\treturn (spa->spa_multihost ? B_TRUE : B_FALSE);\n}\n\nuint32_t\nspa_get_hostid(spa_t *spa)\n{\n\treturn (spa->spa_hostid);\n}\n\nboolean_t\nspa_trust_config(spa_t *spa)\n{\n\treturn (spa->spa_trust_config);\n}\n\nuint64_t\nspa_missing_tvds_allowed(spa_t *spa)\n{\n\treturn (spa->spa_missing_tvds_allowed);\n}\n\nspace_map_t *\nspa_syncing_log_sm(spa_t *spa)\n{\n\treturn (spa->spa_syncing_log_sm);\n}\n\nvoid\nspa_set_missing_tvds(spa_t *spa, uint64_t missing)\n{\n\tspa->spa_missing_tvds = missing;\n}\n\n \nconst char *\nspa_state_to_name(spa_t *spa)\n{\n\tASSERT3P(spa, !=, NULL);\n\n\t \n\tvdev_t *rvd = spa->spa_root_vdev;\n\tif (rvd == NULL) {\n\t\treturn (\"TRANSITIONING\");\n\t}\n\tvdev_state_t state = rvd->vdev_state;\n\tvdev_aux_t aux = rvd->vdev_stat.vs_aux;\n\n\tif (spa_suspended(spa))\n\t\treturn (\"SUSPENDED\");\n\n\tswitch (state) {\n\tcase VDEV_STATE_CLOSED:\n\tcase VDEV_STATE_OFFLINE:\n\t\treturn (\"OFFLINE\");\n\tcase VDEV_STATE_REMOVED:\n\t\treturn (\"REMOVED\");\n\tcase VDEV_STATE_CANT_OPEN:\n\t\tif (aux == VDEV_AUX_CORRUPT_DATA || aux == VDEV_AUX_BAD_LOG)\n\t\t\treturn (\"FAULTED\");\n\t\telse if (aux == VDEV_AUX_SPLIT_POOL)\n\t\t\treturn (\"SPLIT\");\n\t\telse\n\t\t\treturn (\"UNAVAIL\");\n\tcase VDEV_STATE_FAULTED:\n\t\treturn (\"FAULTED\");\n\tcase VDEV_STATE_DEGRADED:\n\t\treturn (\"DEGRADED\");\n\tcase VDEV_STATE_HEALTHY:\n\t\treturn (\"ONLINE\");\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn (\"UNKNOWN\");\n}\n\nboolean_t\nspa_top_vdevs_spacemap_addressable(spa_t *spa)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tfor (uint64_t c = 0; c < rvd->vdev_children; c++) {\n\t\tif (!vdev_is_spacemap_addressable(rvd->vdev_child[c]))\n\t\t\treturn (B_FALSE);\n\t}\n\treturn (B_TRUE);\n}\n\nboolean_t\nspa_has_checkpoint(spa_t *spa)\n{\n\treturn (spa->spa_checkpoint_txg != 0);\n}\n\nboolean_t\nspa_importing_readonly_checkpoint(spa_t *spa)\n{\n\treturn ((spa->spa_import_flags & ZFS_IMPORT_CHECKPOINT) &&\n\t    spa->spa_mode == SPA_MODE_READ);\n}\n\nuint64_t\nspa_min_claim_txg(spa_t *spa)\n{\n\tuint64_t checkpoint_txg = spa->spa_uberblock.ub_checkpoint_txg;\n\n\tif (checkpoint_txg != 0)\n\t\treturn (checkpoint_txg + 1);\n\n\treturn (spa->spa_first_txg);\n}\n\n \nboolean_t\nspa_suspend_async_destroy(spa_t *spa)\n{\n\tdsl_pool_t *dp = spa_get_dsl(spa);\n\n\tuint64_t unreserved = dsl_pool_unreserved_space(dp,\n\t    ZFS_SPACE_CHECK_EXTRA_RESERVED);\n\tuint64_t used = dsl_dir_phys(dp->dp_root_dir)->dd_used_bytes;\n\tuint64_t avail = (unreserved > used) ? (unreserved - used) : 0;\n\n\tif (spa_has_checkpoint(spa) && avail == 0)\n\t\treturn (B_TRUE);\n\n\treturn (B_FALSE);\n}\n\n#if defined(_KERNEL)\n\nint\nparam_set_deadman_failmode_common(const char *val)\n{\n\tspa_t *spa = NULL;\n\tchar *p;\n\n\tif (val == NULL)\n\t\treturn (SET_ERROR(EINVAL));\n\n\tif ((p = strchr(val, '\\n')) != NULL)\n\t\t*p = '\\0';\n\n\tif (strcmp(val, \"wait\") != 0 && strcmp(val, \"continue\") != 0 &&\n\t    strcmp(val, \"panic\"))\n\t\treturn (SET_ERROR(EINVAL));\n\n\tif (spa_mode_global != SPA_MODE_UNINIT) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\twhile ((spa = spa_next(spa)) != NULL)\n\t\t\tspa_set_deadman_failmode(spa, val);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\treturn (0);\n}\n#endif\n\n \nEXPORT_SYMBOL(spa_lookup);\nEXPORT_SYMBOL(spa_add);\nEXPORT_SYMBOL(spa_remove);\nEXPORT_SYMBOL(spa_next);\n\n \nEXPORT_SYMBOL(spa_open_ref);\nEXPORT_SYMBOL(spa_close);\nEXPORT_SYMBOL(spa_refcount_zero);\n\n \nEXPORT_SYMBOL(spa_config_tryenter);\nEXPORT_SYMBOL(spa_config_enter);\nEXPORT_SYMBOL(spa_config_exit);\nEXPORT_SYMBOL(spa_config_held);\n\n \nEXPORT_SYMBOL(spa_vdev_enter);\nEXPORT_SYMBOL(spa_vdev_exit);\n\n \nEXPORT_SYMBOL(spa_vdev_state_enter);\nEXPORT_SYMBOL(spa_vdev_state_exit);\n\n \nEXPORT_SYMBOL(spa_shutting_down);\nEXPORT_SYMBOL(spa_get_dsl);\nEXPORT_SYMBOL(spa_get_rootblkptr);\nEXPORT_SYMBOL(spa_set_rootblkptr);\nEXPORT_SYMBOL(spa_altroot);\nEXPORT_SYMBOL(spa_sync_pass);\nEXPORT_SYMBOL(spa_name);\nEXPORT_SYMBOL(spa_guid);\nEXPORT_SYMBOL(spa_last_synced_txg);\nEXPORT_SYMBOL(spa_first_txg);\nEXPORT_SYMBOL(spa_syncing_txg);\nEXPORT_SYMBOL(spa_version);\nEXPORT_SYMBOL(spa_state);\nEXPORT_SYMBOL(spa_load_state);\nEXPORT_SYMBOL(spa_freeze_txg);\nEXPORT_SYMBOL(spa_get_dspace);\nEXPORT_SYMBOL(spa_update_dspace);\nEXPORT_SYMBOL(spa_deflate);\nEXPORT_SYMBOL(spa_normal_class);\nEXPORT_SYMBOL(spa_log_class);\nEXPORT_SYMBOL(spa_special_class);\nEXPORT_SYMBOL(spa_preferred_class);\nEXPORT_SYMBOL(spa_max_replication);\nEXPORT_SYMBOL(spa_prev_software_version);\nEXPORT_SYMBOL(spa_get_failmode);\nEXPORT_SYMBOL(spa_suspended);\nEXPORT_SYMBOL(spa_bootfs);\nEXPORT_SYMBOL(spa_delegation);\nEXPORT_SYMBOL(spa_meta_objset);\nEXPORT_SYMBOL(spa_maxblocksize);\nEXPORT_SYMBOL(spa_maxdnodesize);\n\n \nEXPORT_SYMBOL(spa_guid_exists);\nEXPORT_SYMBOL(spa_strdup);\nEXPORT_SYMBOL(spa_strfree);\nEXPORT_SYMBOL(spa_generate_guid);\nEXPORT_SYMBOL(snprintf_blkptr);\nEXPORT_SYMBOL(spa_freeze);\nEXPORT_SYMBOL(spa_upgrade);\nEXPORT_SYMBOL(spa_evict_all);\nEXPORT_SYMBOL(spa_lookup_by_guid);\nEXPORT_SYMBOL(spa_has_spare);\nEXPORT_SYMBOL(dva_get_dsize_sync);\nEXPORT_SYMBOL(bp_get_dsize_sync);\nEXPORT_SYMBOL(bp_get_dsize);\nEXPORT_SYMBOL(spa_has_slogs);\nEXPORT_SYMBOL(spa_is_root);\nEXPORT_SYMBOL(spa_writeable);\nEXPORT_SYMBOL(spa_mode);\nEXPORT_SYMBOL(spa_namespace_lock);\nEXPORT_SYMBOL(spa_trust_config);\nEXPORT_SYMBOL(spa_missing_tvds_allowed);\nEXPORT_SYMBOL(spa_set_missing_tvds);\nEXPORT_SYMBOL(spa_state_to_name);\nEXPORT_SYMBOL(spa_importing_readonly_checkpoint);\nEXPORT_SYMBOL(spa_min_claim_txg);\nEXPORT_SYMBOL(spa_suspend_async_destroy);\nEXPORT_SYMBOL(spa_has_checkpoint);\nEXPORT_SYMBOL(spa_top_vdevs_spacemap_addressable);\n\nZFS_MODULE_PARAM(zfs, zfs_, flags, UINT, ZMOD_RW,\n\t\"Set additional debugging flags\");\n\nZFS_MODULE_PARAM(zfs, zfs_, recover, INT, ZMOD_RW,\n\t\"Set to attempt to recover from fatal errors\");\n\nZFS_MODULE_PARAM(zfs, zfs_, free_leak_on_eio, INT, ZMOD_RW,\n\t\"Set to ignore IO errors during free and permanently leak the space\");\n\nZFS_MODULE_PARAM(zfs_deadman, zfs_deadman_, checktime_ms, U64, ZMOD_RW,\n\t\"Dead I/O check interval in milliseconds\");\n\nZFS_MODULE_PARAM(zfs_deadman, zfs_deadman_, enabled, INT, ZMOD_RW,\n\t\"Enable deadman timer\");\n\nZFS_MODULE_PARAM(zfs_spa, spa_, asize_inflation, UINT, ZMOD_RW,\n\t\"SPA size estimate multiplication factor\");\n\nZFS_MODULE_PARAM(zfs, zfs_, ddt_data_is_special, INT, ZMOD_RW,\n\t\"Place DDT data into the special class\");\n\nZFS_MODULE_PARAM(zfs, zfs_, user_indirect_is_special, INT, ZMOD_RW,\n\t\"Place user data indirect blocks into the special class\");\n\n \nZFS_MODULE_PARAM_CALL(zfs_deadman, zfs_deadman_, failmode,\n\tparam_set_deadman_failmode, param_get_charp, ZMOD_RW,\n\t\"Failmode for deadman timer\");\n\nZFS_MODULE_PARAM_CALL(zfs_deadman, zfs_deadman_, synctime_ms,\n\tparam_set_deadman_synctime, spl_param_get_u64, ZMOD_RW,\n\t\"Pool sync expiration time in milliseconds\");\n\nZFS_MODULE_PARAM_CALL(zfs_deadman, zfs_deadman_, ziotime_ms,\n\tparam_set_deadman_ziotime, spl_param_get_u64, ZMOD_RW,\n\t\"IO expiration time in milliseconds\");\n\nZFS_MODULE_PARAM(zfs, zfs_, special_class_metadata_reserve_pct, UINT, ZMOD_RW,\n\t\"Small file blocks in special vdevs depends on this much \"\n\t\"free space available\");\n \n\nZFS_MODULE_PARAM_CALL(zfs_spa, spa_, slop_shift, param_set_slop_shift,\n\tparam_get_uint, ZMOD_RW, \"Reserved free space in pool\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}