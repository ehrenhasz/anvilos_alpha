{
  "module_name": "arc.c",
  "hash_id": "a7368ce3b14d5637e979eca9faed53b559835ceac5b00ccd8e4c1c056c8bacf8",
  "original_prompt": "Ingested from zfs-2.2.2/module/zfs/arc.c",
  "human_readable_source": " \n \n\n \n\n \n\n \n\n#include <sys/spa.h>\n#include <sys/zio.h>\n#include <sys/spa_impl.h>\n#include <sys/zio_compress.h>\n#include <sys/zio_checksum.h>\n#include <sys/zfs_context.h>\n#include <sys/arc.h>\n#include <sys/zfs_refcount.h>\n#include <sys/vdev.h>\n#include <sys/vdev_impl.h>\n#include <sys/dsl_pool.h>\n#include <sys/multilist.h>\n#include <sys/abd.h>\n#include <sys/zil.h>\n#include <sys/fm/fs/zfs.h>\n#include <sys/callb.h>\n#include <sys/kstat.h>\n#include <sys/zthr.h>\n#include <zfs_fletcher.h>\n#include <sys/arc_impl.h>\n#include <sys/trace_zfs.h>\n#include <sys/aggsum.h>\n#include <sys/wmsum.h>\n#include <cityhash.h>\n#include <sys/vdev_trim.h>\n#include <sys/zfs_racct.h>\n#include <sys/zstd/zstd.h>\n\n#ifndef _KERNEL\n \nboolean_t arc_watch = B_FALSE;\n#endif\n\n \nstatic zthr_t *arc_reap_zthr;\n\n \nstatic zthr_t *arc_evict_zthr;\nstatic arc_buf_hdr_t **arc_state_evict_markers;\nstatic int arc_state_evict_marker_count;\n\nstatic kmutex_t arc_evict_lock;\nstatic boolean_t arc_evict_needed = B_FALSE;\nstatic clock_t arc_last_uncached_flush;\n\n \nstatic uint64_t arc_evict_count;\n\n \nstatic list_t arc_evict_waiters;\n\n \nstatic uint_t zfs_arc_eviction_pct = 200;\n\n \nstatic uint_t zfs_arc_evict_batch_limit = 10;\n\n \nuint_t arc_grow_retry = 5;\n\n \nstatic const int arc_kmem_cache_reap_retry_ms = 1000;\n\n \nstatic int zfs_arc_overflow_shift = 8;\n\n \nuint_t arc_shrink_shift = 7;\n\n \n#ifdef _KERNEL\nuint_t zfs_arc_pc_percent = 0;\n#endif\n\n \nuint_t\t\tarc_no_grow_shift = 5;\n\n\n \nstatic uint_t\t\tarc_min_prefetch_ms;\nstatic uint_t\t\tarc_min_prescient_prefetch_ms;\n\n \nuint_t arc_lotsfree_percent = 10;\n\n \nboolean_t arc_warm;\n\n \nuint64_t zfs_arc_max = 0;\nuint64_t zfs_arc_min = 0;\nstatic uint64_t zfs_arc_dnode_limit = 0;\nstatic uint_t zfs_arc_dnode_reduce_percent = 10;\nstatic uint_t zfs_arc_grow_retry = 0;\nstatic uint_t zfs_arc_shrink_shift = 0;\nuint_t zfs_arc_average_blocksize = 8 * 1024;  \n\n \nstatic const unsigned long zfs_arc_dirty_limit_percent = 50;\nstatic const unsigned long zfs_arc_anon_limit_percent = 25;\nstatic const unsigned long zfs_arc_pool_dirty_percent = 20;\n\n \nint zfs_compressed_arc_enabled = B_TRUE;\n\n \nstatic uint_t zfs_arc_meta_balance = 500;\n\n \nstatic uint_t zfs_arc_dnode_limit_percent = 10;\n\n \nstatic uint64_t zfs_arc_sys_free = 0;\nstatic uint_t zfs_arc_min_prefetch_ms = 0;\nstatic uint_t zfs_arc_min_prescient_prefetch_ms = 0;\nstatic uint_t zfs_arc_lotsfree_percent = 10;\n\n \nstatic int zfs_arc_prune_task_threads = 1;\n\n \narc_state_t ARC_anon;\narc_state_t ARC_mru;\narc_state_t ARC_mru_ghost;\narc_state_t ARC_mfu;\narc_state_t ARC_mfu_ghost;\narc_state_t ARC_l2c_only;\narc_state_t ARC_uncached;\n\narc_stats_t arc_stats = {\n\t{ \"hits\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"iohits\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"misses\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"demand_data_hits\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"demand_data_iohits\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"demand_data_misses\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"demand_metadata_hits\",\tKSTAT_DATA_UINT64 },\n\t{ \"demand_metadata_iohits\",\tKSTAT_DATA_UINT64 },\n\t{ \"demand_metadata_misses\",\tKSTAT_DATA_UINT64 },\n\t{ \"prefetch_data_hits\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"prefetch_data_iohits\",\tKSTAT_DATA_UINT64 },\n\t{ \"prefetch_data_misses\",\tKSTAT_DATA_UINT64 },\n\t{ \"prefetch_metadata_hits\",\tKSTAT_DATA_UINT64 },\n\t{ \"prefetch_metadata_iohits\",\tKSTAT_DATA_UINT64 },\n\t{ \"prefetch_metadata_misses\",\tKSTAT_DATA_UINT64 },\n\t{ \"mru_hits\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_ghost_hits\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_hits\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_ghost_hits\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"uncached_hits\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"deleted\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mutex_miss\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"access_skip\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"evict_skip\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"evict_not_enough\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"evict_l2_cached\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"evict_l2_eligible\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"evict_l2_eligible_mfu\",\tKSTAT_DATA_UINT64 },\n\t{ \"evict_l2_eligible_mru\",\tKSTAT_DATA_UINT64 },\n\t{ \"evict_l2_ineligible\",\tKSTAT_DATA_UINT64 },\n\t{ \"evict_l2_skip\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"hash_elements\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"hash_elements_max\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"hash_collisions\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"hash_chains\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"hash_chain_max\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"meta\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"pd\",\t\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"pm\",\t\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"c\",\t\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"c_min\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"c_max\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"compressed_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"uncompressed_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"overhead_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"hdr_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"data_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"metadata_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"dbuf_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"dnode_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"bonus_size\",\t\t\tKSTAT_DATA_UINT64 },\n#if defined(COMPAT_FREEBSD11)\n\t{ \"other_size\",\t\t\tKSTAT_DATA_UINT64 },\n#endif\n\t{ \"anon_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"anon_data\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"anon_metadata\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"anon_evictable_data\",\tKSTAT_DATA_UINT64 },\n\t{ \"anon_evictable_metadata\",\tKSTAT_DATA_UINT64 },\n\t{ \"mru_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_data\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_metadata\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_evictable_data\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_evictable_metadata\",\tKSTAT_DATA_UINT64 },\n\t{ \"mru_ghost_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_ghost_data\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_ghost_metadata\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mru_ghost_evictable_data\",\tKSTAT_DATA_UINT64 },\n\t{ \"mru_ghost_evictable_metadata\", KSTAT_DATA_UINT64 },\n\t{ \"mfu_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_data\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_metadata\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_evictable_data\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_evictable_metadata\",\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_ghost_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_ghost_data\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_ghost_metadata\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_ghost_evictable_data\",\tKSTAT_DATA_UINT64 },\n\t{ \"mfu_ghost_evictable_metadata\", KSTAT_DATA_UINT64 },\n\t{ \"uncached_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"uncached_data\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"uncached_metadata\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"uncached_evictable_data\",\tKSTAT_DATA_UINT64 },\n\t{ \"uncached_evictable_metadata\", KSTAT_DATA_UINT64 },\n\t{ \"l2_hits\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_misses\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_prefetch_asize\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_mru_asize\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_mfu_asize\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_bufc_data_asize\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_bufc_metadata_asize\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_feeds\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rw_clash\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_read_bytes\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_write_bytes\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_writes_sent\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_writes_done\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_writes_error\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_writes_lock_retry\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_evict_lock_retry\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_evict_reading\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_evict_l1cached\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_free_on_write\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_abort_lowmem\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_cksum_bad\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_io_error\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_size\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_asize\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_hdr_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_log_blk_writes\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_log_blk_avg_asize\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_log_blk_asize\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_log_blk_count\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_data_to_meta_ratio\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_success\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_unsupported\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_io_errors\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_dh_errors\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_cksum_lb_errors\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_lowmem\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_asize\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_bufs\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_bufs_precached\",\tKSTAT_DATA_UINT64 },\n\t{ \"l2_rebuild_log_blks\",\tKSTAT_DATA_UINT64 },\n\t{ \"memory_throttle_count\",\tKSTAT_DATA_UINT64 },\n\t{ \"memory_direct_count\",\tKSTAT_DATA_UINT64 },\n\t{ \"memory_indirect_count\",\tKSTAT_DATA_UINT64 },\n\t{ \"memory_all_bytes\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"memory_free_bytes\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"memory_available_bytes\",\tKSTAT_DATA_INT64 },\n\t{ \"arc_no_grow\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_tempreserve\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_loaned_bytes\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_prune\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_meta_used\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_dnode_limit\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"async_upgrade_sync\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"predictive_prefetch\", KSTAT_DATA_UINT64 },\n\t{ \"demand_hit_predictive_prefetch\", KSTAT_DATA_UINT64 },\n\t{ \"demand_iohit_predictive_prefetch\", KSTAT_DATA_UINT64 },\n\t{ \"prescient_prefetch\", KSTAT_DATA_UINT64 },\n\t{ \"demand_hit_prescient_prefetch\", KSTAT_DATA_UINT64 },\n\t{ \"demand_iohit_prescient_prefetch\", KSTAT_DATA_UINT64 },\n\t{ \"arc_need_free\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_sys_free\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"arc_raw_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"cached_only_in_progress\",\tKSTAT_DATA_UINT64 },\n\t{ \"abd_chunk_waste_size\",\tKSTAT_DATA_UINT64 },\n};\n\narc_sums_t arc_sums;\n\n#define\tARCSTAT_MAX(stat, val) {\t\t\t\t\t\\\n\tuint64_t m;\t\t\t\t\t\t\t\\\n\twhile ((val) > (m = arc_stats.stat.value.ui64) &&\t\t\\\n\t    (m != atomic_cas_64(&arc_stats.stat.value.ui64, m, (val))))\t\\\n\t\tcontinue;\t\t\t\t\t\t\\\n}\n\n \n#define\tARCSTAT_CONDSTAT(cond1, stat1, notstat1, cond2, stat2, notstat2, stat) \\\n\tif (cond1) {\t\t\t\t\t\t\t\\\n\t\tif (cond2) {\t\t\t\t\t\t\\\n\t\t\tARCSTAT_BUMP(arcstat_##stat1##_##stat2##_##stat); \\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tARCSTAT_BUMP(arcstat_##stat1##_##notstat2##_##stat); \\\n\t\t}\t\t\t\t\t\t\t\\\n\t} else {\t\t\t\t\t\t\t\\\n\t\tif (cond2) {\t\t\t\t\t\t\\\n\t\t\tARCSTAT_BUMP(arcstat_##notstat1##_##stat2##_##stat); \\\n\t\t} else {\t\t\t\t\t\t\\\n\t\t\tARCSTAT_BUMP(arcstat_##notstat1##_##notstat2##_##stat);\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\n\n \n#define\tARCSTAT_F_AVG_FACTOR\t3\n#define\tARCSTAT_F_AVG(stat, value) \\\n\tdo { \\\n\t\tuint64_t x = ARCSTAT(stat); \\\n\t\tx = x - x / ARCSTAT_F_AVG_FACTOR + \\\n\t\t    (value) / ARCSTAT_F_AVG_FACTOR; \\\n\t\tARCSTAT(stat) = x; \\\n\t} while (0)\n\nstatic kstat_t\t\t\t*arc_ksp;\n\n \n#define\tarc_tempreserve\tARCSTAT(arcstat_tempreserve)\n#define\tarc_loaned_bytes\tARCSTAT(arcstat_loaned_bytes)\n#define\tarc_dnode_limit\tARCSTAT(arcstat_dnode_limit)  \n#define\tarc_need_free\tARCSTAT(arcstat_need_free)  \n\nhrtime_t arc_growtime;\nlist_t arc_prune_list;\nkmutex_t arc_prune_mtx;\ntaskq_t *arc_prune_taskq;\n\n#define\tGHOST_STATE(state)\t\\\n\t((state) == arc_mru_ghost || (state) == arc_mfu_ghost ||\t\\\n\t(state) == arc_l2c_only)\n\n#define\tHDR_IN_HASH_TABLE(hdr)\t((hdr)->b_flags & ARC_FLAG_IN_HASH_TABLE)\n#define\tHDR_IO_IN_PROGRESS(hdr)\t((hdr)->b_flags & ARC_FLAG_IO_IN_PROGRESS)\n#define\tHDR_IO_ERROR(hdr)\t((hdr)->b_flags & ARC_FLAG_IO_ERROR)\n#define\tHDR_PREFETCH(hdr)\t((hdr)->b_flags & ARC_FLAG_PREFETCH)\n#define\tHDR_PRESCIENT_PREFETCH(hdr)\t\\\n\t((hdr)->b_flags & ARC_FLAG_PRESCIENT_PREFETCH)\n#define\tHDR_COMPRESSION_ENABLED(hdr)\t\\\n\t((hdr)->b_flags & ARC_FLAG_COMPRESSED_ARC)\n\n#define\tHDR_L2CACHE(hdr)\t((hdr)->b_flags & ARC_FLAG_L2CACHE)\n#define\tHDR_UNCACHED(hdr)\t((hdr)->b_flags & ARC_FLAG_UNCACHED)\n#define\tHDR_L2_READING(hdr)\t\\\n\t(((hdr)->b_flags & ARC_FLAG_IO_IN_PROGRESS) &&\t\\\n\t((hdr)->b_flags & ARC_FLAG_HAS_L2HDR))\n#define\tHDR_L2_WRITING(hdr)\t((hdr)->b_flags & ARC_FLAG_L2_WRITING)\n#define\tHDR_L2_EVICTED(hdr)\t((hdr)->b_flags & ARC_FLAG_L2_EVICTED)\n#define\tHDR_L2_WRITE_HEAD(hdr)\t((hdr)->b_flags & ARC_FLAG_L2_WRITE_HEAD)\n#define\tHDR_PROTECTED(hdr)\t((hdr)->b_flags & ARC_FLAG_PROTECTED)\n#define\tHDR_NOAUTH(hdr)\t\t((hdr)->b_flags & ARC_FLAG_NOAUTH)\n#define\tHDR_SHARED_DATA(hdr)\t((hdr)->b_flags & ARC_FLAG_SHARED_DATA)\n\n#define\tHDR_ISTYPE_METADATA(hdr)\t\\\n\t((hdr)->b_flags & ARC_FLAG_BUFC_METADATA)\n#define\tHDR_ISTYPE_DATA(hdr)\t(!HDR_ISTYPE_METADATA(hdr))\n\n#define\tHDR_HAS_L1HDR(hdr)\t((hdr)->b_flags & ARC_FLAG_HAS_L1HDR)\n#define\tHDR_HAS_L2HDR(hdr)\t((hdr)->b_flags & ARC_FLAG_HAS_L2HDR)\n#define\tHDR_HAS_RABD(hdr)\t\\\n\t(HDR_HAS_L1HDR(hdr) && HDR_PROTECTED(hdr) &&\t\\\n\t(hdr)->b_crypt_hdr.b_rabd != NULL)\n#define\tHDR_ENCRYPTED(hdr)\t\\\n\t(HDR_PROTECTED(hdr) && DMU_OT_IS_ENCRYPTED((hdr)->b_crypt_hdr.b_ot))\n#define\tHDR_AUTHENTICATED(hdr)\t\\\n\t(HDR_PROTECTED(hdr) && !DMU_OT_IS_ENCRYPTED((hdr)->b_crypt_hdr.b_ot))\n\n \n#define\tHDR_COMPRESS_OFFSET\t(highbit64(ARC_FLAG_COMPRESS_0) - 1)\n\n#define\tHDR_GET_COMPRESS(hdr)\t((enum zio_compress)BF32_GET((hdr)->b_flags, \\\n\tHDR_COMPRESS_OFFSET, SPA_COMPRESSBITS))\n#define\tHDR_SET_COMPRESS(hdr, cmp) BF32_SET((hdr)->b_flags, \\\n\tHDR_COMPRESS_OFFSET, SPA_COMPRESSBITS, (cmp));\n\n#define\tARC_BUF_LAST(buf)\t((buf)->b_next == NULL)\n#define\tARC_BUF_SHARED(buf)\t((buf)->b_flags & ARC_BUF_FLAG_SHARED)\n#define\tARC_BUF_COMPRESSED(buf)\t((buf)->b_flags & ARC_BUF_FLAG_COMPRESSED)\n#define\tARC_BUF_ENCRYPTED(buf)\t((buf)->b_flags & ARC_BUF_FLAG_ENCRYPTED)\n\n \n\n#define\tHDR_FULL_SIZE ((int64_t)sizeof (arc_buf_hdr_t))\n#define\tHDR_L2ONLY_SIZE ((int64_t)offsetof(arc_buf_hdr_t, b_l1hdr))\n\n \n\n#define\tBUF_LOCKS 2048\ntypedef struct buf_hash_table {\n\tuint64_t ht_mask;\n\tarc_buf_hdr_t **ht_table;\n\tkmutex_t ht_locks[BUF_LOCKS] ____cacheline_aligned;\n} buf_hash_table_t;\n\nstatic buf_hash_table_t buf_hash_table;\n\n#define\tBUF_HASH_INDEX(spa, dva, birth) \\\n\t(buf_hash(spa, dva, birth) & buf_hash_table.ht_mask)\n#define\tBUF_HASH_LOCK(idx)\t(&buf_hash_table.ht_locks[idx & (BUF_LOCKS-1)])\n#define\tHDR_LOCK(hdr) \\\n\t(BUF_HASH_LOCK(BUF_HASH_INDEX(hdr->b_spa, &hdr->b_dva, hdr->b_birth)))\n\nuint64_t zfs_crc64_table[256];\n\n \n\n#define\tL2ARC_WRITE_SIZE\t(8 * 1024 * 1024)\t \n#define\tL2ARC_HEADROOM\t\t2\t\t\t \n\n \n#define\tL2ARC_HEADROOM_BOOST\t200\n#define\tL2ARC_FEED_SECS\t\t1\t\t \n#define\tL2ARC_FEED_MIN_MS\t200\t\t \n\n \n#define\tL2ARC_FEED_TYPES\t4\n\n \nuint64_t l2arc_write_max = L2ARC_WRITE_SIZE;\t \nuint64_t l2arc_write_boost = L2ARC_WRITE_SIZE;\t \nuint64_t l2arc_headroom = L2ARC_HEADROOM;\t \nuint64_t l2arc_headroom_boost = L2ARC_HEADROOM_BOOST;\nuint64_t l2arc_feed_secs = L2ARC_FEED_SECS;\t \nuint64_t l2arc_feed_min_ms = L2ARC_FEED_MIN_MS;\t \nint l2arc_noprefetch = B_TRUE;\t\t\t \nint l2arc_feed_again = B_TRUE;\t\t\t \nint l2arc_norw = B_FALSE;\t\t\t \nstatic uint_t l2arc_meta_percent = 33;\t \n\n \nstatic list_t L2ARC_dev_list;\t\t\t \nstatic list_t *l2arc_dev_list;\t\t\t \nstatic kmutex_t l2arc_dev_mtx;\t\t\t \nstatic l2arc_dev_t *l2arc_dev_last;\t\t \nstatic list_t L2ARC_free_on_write;\t\t \nstatic list_t *l2arc_free_on_write;\t\t \nstatic kmutex_t l2arc_free_on_write_mtx;\t \nstatic uint64_t l2arc_ndev;\t\t\t \n\ntypedef struct l2arc_read_callback {\n\tarc_buf_hdr_t\t\t*l2rcb_hdr;\t\t \n\tblkptr_t\t\tl2rcb_bp;\t\t \n\tzbookmark_phys_t\tl2rcb_zb;\t\t \n\tint\t\t\tl2rcb_flags;\t\t \n\tabd_t\t\t\t*l2rcb_abd;\t\t \n} l2arc_read_callback_t;\n\ntypedef struct l2arc_data_free {\n\t \n\tabd_t\t\t*l2df_abd;\n\tsize_t\t\tl2df_size;\n\tarc_buf_contents_t l2df_type;\n\tlist_node_t\tl2df_list_node;\n} l2arc_data_free_t;\n\ntypedef enum arc_fill_flags {\n\tARC_FILL_LOCKED\t\t= 1 << 0,  \n\tARC_FILL_COMPRESSED\t= 1 << 1,  \n\tARC_FILL_ENCRYPTED\t= 1 << 2,  \n\tARC_FILL_NOAUTH\t\t= 1 << 3,  \n\tARC_FILL_IN_PLACE\t= 1 << 4   \n} arc_fill_flags_t;\n\ntypedef enum arc_ovf_level {\n\tARC_OVF_NONE,\t\t\t \n\tARC_OVF_SOME,\t\t\t \n\tARC_OVF_SEVERE\t\t\t \n} arc_ovf_level_t;\n\nstatic kmutex_t l2arc_feed_thr_lock;\nstatic kcondvar_t l2arc_feed_thr_cv;\nstatic uint8_t l2arc_thread_exit;\n\nstatic kmutex_t l2arc_rebuild_thr_lock;\nstatic kcondvar_t l2arc_rebuild_thr_cv;\n\nenum arc_hdr_alloc_flags {\n\tARC_HDR_ALLOC_RDATA = 0x1,\n\tARC_HDR_USE_RESERVE = 0x4,\n\tARC_HDR_ALLOC_LINEAR = 0x8,\n};\n\n\nstatic abd_t *arc_get_data_abd(arc_buf_hdr_t *, uint64_t, const void *, int);\nstatic void *arc_get_data_buf(arc_buf_hdr_t *, uint64_t, const void *);\nstatic void arc_get_data_impl(arc_buf_hdr_t *, uint64_t, const void *, int);\nstatic void arc_free_data_abd(arc_buf_hdr_t *, abd_t *, uint64_t, const void *);\nstatic void arc_free_data_buf(arc_buf_hdr_t *, void *, uint64_t, const void *);\nstatic void arc_free_data_impl(arc_buf_hdr_t *hdr, uint64_t size,\n    const void *tag);\nstatic void arc_hdr_free_abd(arc_buf_hdr_t *, boolean_t);\nstatic void arc_hdr_alloc_abd(arc_buf_hdr_t *, int);\nstatic void arc_hdr_destroy(arc_buf_hdr_t *);\nstatic void arc_access(arc_buf_hdr_t *, arc_flags_t, boolean_t);\nstatic void arc_buf_watch(arc_buf_t *);\nstatic void arc_change_state(arc_state_t *, arc_buf_hdr_t *);\n\nstatic arc_buf_contents_t arc_buf_type(arc_buf_hdr_t *);\nstatic uint32_t arc_bufc_to_flags(arc_buf_contents_t);\nstatic inline void arc_hdr_set_flags(arc_buf_hdr_t *hdr, arc_flags_t flags);\nstatic inline void arc_hdr_clear_flags(arc_buf_hdr_t *hdr, arc_flags_t flags);\n\nstatic boolean_t l2arc_write_eligible(uint64_t, arc_buf_hdr_t *);\nstatic void l2arc_read_done(zio_t *);\nstatic void l2arc_do_free_on_write(void);\nstatic void l2arc_hdr_arcstats_update(arc_buf_hdr_t *hdr, boolean_t incr,\n    boolean_t state_only);\n\nstatic void arc_prune_async(uint64_t adjust);\n\n#define\tl2arc_hdr_arcstats_increment(hdr) \\\n\tl2arc_hdr_arcstats_update((hdr), B_TRUE, B_FALSE)\n#define\tl2arc_hdr_arcstats_decrement(hdr) \\\n\tl2arc_hdr_arcstats_update((hdr), B_FALSE, B_FALSE)\n#define\tl2arc_hdr_arcstats_increment_state(hdr) \\\n\tl2arc_hdr_arcstats_update((hdr), B_TRUE, B_TRUE)\n#define\tl2arc_hdr_arcstats_decrement_state(hdr) \\\n\tl2arc_hdr_arcstats_update((hdr), B_FALSE, B_TRUE)\n\n \nint l2arc_exclude_special = 0;\n\n \nstatic int l2arc_mfuonly = 0;\n\n \nstatic uint64_t l2arc_trim_ahead = 0;\n\n \nstatic int l2arc_rebuild_enabled = B_TRUE;\nstatic uint64_t l2arc_rebuild_blocks_min_l2size = 1024 * 1024 * 1024;\n\n \nvoid l2arc_rebuild_vdev(vdev_t *vd, boolean_t reopen);\nstatic __attribute__((noreturn)) void l2arc_dev_rebuild_thread(void *arg);\nstatic int l2arc_rebuild(l2arc_dev_t *dev);\n\n \nstatic int l2arc_dev_hdr_read(l2arc_dev_t *dev);\nstatic int l2arc_log_blk_read(l2arc_dev_t *dev,\n    const l2arc_log_blkptr_t *this_lp, const l2arc_log_blkptr_t *next_lp,\n    l2arc_log_blk_phys_t *this_lb, l2arc_log_blk_phys_t *next_lb,\n    zio_t *this_io, zio_t **next_io);\nstatic zio_t *l2arc_log_blk_fetch(vdev_t *vd,\n    const l2arc_log_blkptr_t *lp, l2arc_log_blk_phys_t *lb);\nstatic void l2arc_log_blk_fetch_abort(zio_t *zio);\n\n \nstatic void l2arc_log_blk_restore(l2arc_dev_t *dev,\n    const l2arc_log_blk_phys_t *lb, uint64_t lb_asize);\nstatic void l2arc_hdr_restore(const l2arc_log_ent_phys_t *le,\n    l2arc_dev_t *dev);\n\n \nstatic uint64_t l2arc_log_blk_commit(l2arc_dev_t *dev, zio_t *pio,\n    l2arc_write_callback_t *cb);\n\n \nboolean_t l2arc_log_blkptr_valid(l2arc_dev_t *dev,\n    const l2arc_log_blkptr_t *lbp);\nstatic boolean_t l2arc_log_blk_insert(l2arc_dev_t *dev,\n    const arc_buf_hdr_t *ab);\nboolean_t l2arc_range_check_overlap(uint64_t bottom,\n    uint64_t top, uint64_t check);\nstatic void l2arc_blk_fetch_done(zio_t *zio);\nstatic inline uint64_t\n    l2arc_log_blk_overhead(uint64_t write_sz, l2arc_dev_t *dev);\n\n \nstatic uint64_t\nbuf_hash(uint64_t spa, const dva_t *dva, uint64_t birth)\n{\n\treturn (cityhash4(spa, dva->dva_word[0], dva->dva_word[1], birth));\n}\n\n#define\tHDR_EMPTY(hdr)\t\t\t\t\t\t\\\n\t((hdr)->b_dva.dva_word[0] == 0 &&\t\t\t\\\n\t(hdr)->b_dva.dva_word[1] == 0)\n\n#define\tHDR_EMPTY_OR_LOCKED(hdr)\t\t\t\t\\\n\t(HDR_EMPTY(hdr) || MUTEX_HELD(HDR_LOCK(hdr)))\n\n#define\tHDR_EQUAL(spa, dva, birth, hdr)\t\t\t\t\\\n\t((hdr)->b_dva.dva_word[0] == (dva)->dva_word[0]) &&\t\\\n\t((hdr)->b_dva.dva_word[1] == (dva)->dva_word[1]) &&\t\\\n\t((hdr)->b_birth == birth) && ((hdr)->b_spa == spa)\n\nstatic void\nbuf_discard_identity(arc_buf_hdr_t *hdr)\n{\n\thdr->b_dva.dva_word[0] = 0;\n\thdr->b_dva.dva_word[1] = 0;\n\thdr->b_birth = 0;\n}\n\nstatic arc_buf_hdr_t *\nbuf_hash_find(uint64_t spa, const blkptr_t *bp, kmutex_t **lockp)\n{\n\tconst dva_t *dva = BP_IDENTITY(bp);\n\tuint64_t birth = BP_PHYSICAL_BIRTH(bp);\n\tuint64_t idx = BUF_HASH_INDEX(spa, dva, birth);\n\tkmutex_t *hash_lock = BUF_HASH_LOCK(idx);\n\tarc_buf_hdr_t *hdr;\n\n\tmutex_enter(hash_lock);\n\tfor (hdr = buf_hash_table.ht_table[idx]; hdr != NULL;\n\t    hdr = hdr->b_hash_next) {\n\t\tif (HDR_EQUAL(spa, dva, birth, hdr)) {\n\t\t\t*lockp = hash_lock;\n\t\t\treturn (hdr);\n\t\t}\n\t}\n\tmutex_exit(hash_lock);\n\t*lockp = NULL;\n\treturn (NULL);\n}\n\n \nstatic arc_buf_hdr_t *\nbuf_hash_insert(arc_buf_hdr_t *hdr, kmutex_t **lockp)\n{\n\tuint64_t idx = BUF_HASH_INDEX(hdr->b_spa, &hdr->b_dva, hdr->b_birth);\n\tkmutex_t *hash_lock = BUF_HASH_LOCK(idx);\n\tarc_buf_hdr_t *fhdr;\n\tuint32_t i;\n\n\tASSERT(!DVA_IS_EMPTY(&hdr->b_dva));\n\tASSERT(hdr->b_birth != 0);\n\tASSERT(!HDR_IN_HASH_TABLE(hdr));\n\n\tif (lockp != NULL) {\n\t\t*lockp = hash_lock;\n\t\tmutex_enter(hash_lock);\n\t} else {\n\t\tASSERT(MUTEX_HELD(hash_lock));\n\t}\n\n\tfor (fhdr = buf_hash_table.ht_table[idx], i = 0; fhdr != NULL;\n\t    fhdr = fhdr->b_hash_next, i++) {\n\t\tif (HDR_EQUAL(hdr->b_spa, &hdr->b_dva, hdr->b_birth, fhdr))\n\t\t\treturn (fhdr);\n\t}\n\n\thdr->b_hash_next = buf_hash_table.ht_table[idx];\n\tbuf_hash_table.ht_table[idx] = hdr;\n\tarc_hdr_set_flags(hdr, ARC_FLAG_IN_HASH_TABLE);\n\n\t \n\tif (i > 0) {\n\t\tARCSTAT_BUMP(arcstat_hash_collisions);\n\t\tif (i == 1)\n\t\t\tARCSTAT_BUMP(arcstat_hash_chains);\n\n\t\tARCSTAT_MAX(arcstat_hash_chain_max, i);\n\t}\n\tuint64_t he = atomic_inc_64_nv(\n\t    &arc_stats.arcstat_hash_elements.value.ui64);\n\tARCSTAT_MAX(arcstat_hash_elements_max, he);\n\n\treturn (NULL);\n}\n\nstatic void\nbuf_hash_remove(arc_buf_hdr_t *hdr)\n{\n\tarc_buf_hdr_t *fhdr, **hdrp;\n\tuint64_t idx = BUF_HASH_INDEX(hdr->b_spa, &hdr->b_dva, hdr->b_birth);\n\n\tASSERT(MUTEX_HELD(BUF_HASH_LOCK(idx)));\n\tASSERT(HDR_IN_HASH_TABLE(hdr));\n\n\thdrp = &buf_hash_table.ht_table[idx];\n\twhile ((fhdr = *hdrp) != hdr) {\n\t\tASSERT3P(fhdr, !=, NULL);\n\t\thdrp = &fhdr->b_hash_next;\n\t}\n\t*hdrp = hdr->b_hash_next;\n\thdr->b_hash_next = NULL;\n\tarc_hdr_clear_flags(hdr, ARC_FLAG_IN_HASH_TABLE);\n\n\t \n\tatomic_dec_64(&arc_stats.arcstat_hash_elements.value.ui64);\n\n\tif (buf_hash_table.ht_table[idx] &&\n\t    buf_hash_table.ht_table[idx]->b_hash_next == NULL)\n\t\tARCSTAT_BUMPDOWN(arcstat_hash_chains);\n}\n\n \n\nstatic kmem_cache_t *hdr_full_cache;\nstatic kmem_cache_t *hdr_l2only_cache;\nstatic kmem_cache_t *buf_cache;\n\nstatic void\nbuf_fini(void)\n{\n#if defined(_KERNEL)\n\t \n\tvmem_free(buf_hash_table.ht_table,\n\t    (buf_hash_table.ht_mask + 1) * sizeof (void *));\n#else\n\tkmem_free(buf_hash_table.ht_table,\n\t    (buf_hash_table.ht_mask + 1) * sizeof (void *));\n#endif\n\tfor (int i = 0; i < BUF_LOCKS; i++)\n\t\tmutex_destroy(BUF_HASH_LOCK(i));\n\tkmem_cache_destroy(hdr_full_cache);\n\tkmem_cache_destroy(hdr_l2only_cache);\n\tkmem_cache_destroy(buf_cache);\n}\n\n \nstatic int\nhdr_full_cons(void *vbuf, void *unused, int kmflag)\n{\n\t(void) unused, (void) kmflag;\n\tarc_buf_hdr_t *hdr = vbuf;\n\n\tmemset(hdr, 0, HDR_FULL_SIZE);\n\thdr->b_l1hdr.b_byteswap = DMU_BSWAP_NUMFUNCS;\n\tzfs_refcount_create(&hdr->b_l1hdr.b_refcnt);\n#ifdef ZFS_DEBUG\n\tmutex_init(&hdr->b_l1hdr.b_freeze_lock, NULL, MUTEX_DEFAULT, NULL);\n#endif\n\tmultilist_link_init(&hdr->b_l1hdr.b_arc_node);\n\tlist_link_init(&hdr->b_l2hdr.b_l2node);\n\tarc_space_consume(HDR_FULL_SIZE, ARC_SPACE_HDRS);\n\n\treturn (0);\n}\n\nstatic int\nhdr_l2only_cons(void *vbuf, void *unused, int kmflag)\n{\n\t(void) unused, (void) kmflag;\n\tarc_buf_hdr_t *hdr = vbuf;\n\n\tmemset(hdr, 0, HDR_L2ONLY_SIZE);\n\tarc_space_consume(HDR_L2ONLY_SIZE, ARC_SPACE_L2HDRS);\n\n\treturn (0);\n}\n\nstatic int\nbuf_cons(void *vbuf, void *unused, int kmflag)\n{\n\t(void) unused, (void) kmflag;\n\tarc_buf_t *buf = vbuf;\n\n\tmemset(buf, 0, sizeof (arc_buf_t));\n\tarc_space_consume(sizeof (arc_buf_t), ARC_SPACE_HDRS);\n\n\treturn (0);\n}\n\n \nstatic void\nhdr_full_dest(void *vbuf, void *unused)\n{\n\t(void) unused;\n\tarc_buf_hdr_t *hdr = vbuf;\n\n\tASSERT(HDR_EMPTY(hdr));\n\tzfs_refcount_destroy(&hdr->b_l1hdr.b_refcnt);\n#ifdef ZFS_DEBUG\n\tmutex_destroy(&hdr->b_l1hdr.b_freeze_lock);\n#endif\n\tASSERT(!multilist_link_active(&hdr->b_l1hdr.b_arc_node));\n\tarc_space_return(HDR_FULL_SIZE, ARC_SPACE_HDRS);\n}\n\nstatic void\nhdr_l2only_dest(void *vbuf, void *unused)\n{\n\t(void) unused;\n\tarc_buf_hdr_t *hdr = vbuf;\n\n\tASSERT(HDR_EMPTY(hdr));\n\tarc_space_return(HDR_L2ONLY_SIZE, ARC_SPACE_L2HDRS);\n}\n\nstatic void\nbuf_dest(void *vbuf, void *unused)\n{\n\t(void) unused;\n\t(void) vbuf;\n\n\tarc_space_return(sizeof (arc_buf_t), ARC_SPACE_HDRS);\n}\n\nstatic void\nbuf_init(void)\n{\n\tuint64_t *ct = NULL;\n\tuint64_t hsize = 1ULL << 12;\n\tint i, j;\n\n\t \n\twhile (hsize * zfs_arc_average_blocksize < arc_all_memory())\n\t\thsize <<= 1;\nretry:\n\tbuf_hash_table.ht_mask = hsize - 1;\n#if defined(_KERNEL)\n\t \n\tbuf_hash_table.ht_table =\n\t    vmem_zalloc(hsize * sizeof (void*), KM_SLEEP);\n#else\n\tbuf_hash_table.ht_table =\n\t    kmem_zalloc(hsize * sizeof (void*), KM_NOSLEEP);\n#endif\n\tif (buf_hash_table.ht_table == NULL) {\n\t\tASSERT(hsize > (1ULL << 8));\n\t\thsize >>= 1;\n\t\tgoto retry;\n\t}\n\n\thdr_full_cache = kmem_cache_create(\"arc_buf_hdr_t_full\", HDR_FULL_SIZE,\n\t    0, hdr_full_cons, hdr_full_dest, NULL, NULL, NULL, 0);\n\thdr_l2only_cache = kmem_cache_create(\"arc_buf_hdr_t_l2only\",\n\t    HDR_L2ONLY_SIZE, 0, hdr_l2only_cons, hdr_l2only_dest, NULL,\n\t    NULL, NULL, 0);\n\tbuf_cache = kmem_cache_create(\"arc_buf_t\", sizeof (arc_buf_t),\n\t    0, buf_cons, buf_dest, NULL, NULL, NULL, 0);\n\n\tfor (i = 0; i < 256; i++)\n\t\tfor (ct = zfs_crc64_table + i, *ct = i, j = 8; j > 0; j--)\n\t\t\t*ct = (*ct >> 1) ^ (-(*ct & 1) & ZFS_CRC64_POLY);\n\n\tfor (i = 0; i < BUF_LOCKS; i++)\n\t\tmutex_init(BUF_HASH_LOCK(i), NULL, MUTEX_DEFAULT, NULL);\n}\n\n#define\tARC_MINTIME\t(hz>>4)  \n\n \nuint64_t\narc_buf_size(arc_buf_t *buf)\n{\n\treturn (ARC_BUF_COMPRESSED(buf) ?\n\t    HDR_GET_PSIZE(buf->b_hdr) : HDR_GET_LSIZE(buf->b_hdr));\n}\n\nuint64_t\narc_buf_lsize(arc_buf_t *buf)\n{\n\treturn (HDR_GET_LSIZE(buf->b_hdr));\n}\n\n \nboolean_t\narc_is_encrypted(arc_buf_t *buf)\n{\n\treturn (ARC_BUF_ENCRYPTED(buf) != 0);\n}\n\n \nboolean_t\narc_is_unauthenticated(arc_buf_t *buf)\n{\n\treturn (HDR_NOAUTH(buf->b_hdr) != 0);\n}\n\nvoid\narc_get_raw_params(arc_buf_t *buf, boolean_t *byteorder, uint8_t *salt,\n    uint8_t *iv, uint8_t *mac)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT(HDR_PROTECTED(hdr));\n\n\tmemcpy(salt, hdr->b_crypt_hdr.b_salt, ZIO_DATA_SALT_LEN);\n\tmemcpy(iv, hdr->b_crypt_hdr.b_iv, ZIO_DATA_IV_LEN);\n\tmemcpy(mac, hdr->b_crypt_hdr.b_mac, ZIO_DATA_MAC_LEN);\n\t*byteorder = (hdr->b_l1hdr.b_byteswap == DMU_BSWAP_NUMFUNCS) ?\n\t    ZFS_HOST_BYTEORDER : !ZFS_HOST_BYTEORDER;\n}\n\n \nenum zio_compress\narc_get_compression(arc_buf_t *buf)\n{\n\treturn (ARC_BUF_COMPRESSED(buf) ?\n\t    HDR_GET_COMPRESS(buf->b_hdr) : ZIO_COMPRESS_OFF);\n}\n\n \nstatic inline enum zio_compress\narc_hdr_get_compress(arc_buf_hdr_t *hdr)\n{\n\treturn (HDR_COMPRESSION_ENABLED(hdr) ?\n\t    HDR_GET_COMPRESS(hdr) : ZIO_COMPRESS_OFF);\n}\n\nuint8_t\narc_get_complevel(arc_buf_t *buf)\n{\n\treturn (buf->b_hdr->b_complevel);\n}\n\nstatic inline boolean_t\narc_buf_is_shared(arc_buf_t *buf)\n{\n\tboolean_t shared = (buf->b_data != NULL &&\n\t    buf->b_hdr->b_l1hdr.b_pabd != NULL &&\n\t    abd_is_linear(buf->b_hdr->b_l1hdr.b_pabd) &&\n\t    buf->b_data == abd_to_buf(buf->b_hdr->b_l1hdr.b_pabd));\n\tIMPLY(shared, HDR_SHARED_DATA(buf->b_hdr));\n\tEQUIV(shared, ARC_BUF_SHARED(buf));\n\tIMPLY(shared, ARC_BUF_COMPRESSED(buf) || ARC_BUF_LAST(buf));\n\n\t \n\n\treturn (shared);\n}\n\n \nstatic inline void\narc_cksum_free(arc_buf_hdr_t *hdr)\n{\n#ifdef ZFS_DEBUG\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\tmutex_enter(&hdr->b_l1hdr.b_freeze_lock);\n\tif (hdr->b_l1hdr.b_freeze_cksum != NULL) {\n\t\tkmem_free(hdr->b_l1hdr.b_freeze_cksum, sizeof (zio_cksum_t));\n\t\thdr->b_l1hdr.b_freeze_cksum = NULL;\n\t}\n\tmutex_exit(&hdr->b_l1hdr.b_freeze_lock);\n#endif\n}\n\n \nstatic boolean_t\narc_hdr_has_uncompressed_buf(arc_buf_hdr_t *hdr)\n{\n\tASSERT(hdr->b_l1hdr.b_state == arc_anon || HDR_EMPTY_OR_LOCKED(hdr));\n\n\tfor (arc_buf_t *b = hdr->b_l1hdr.b_buf; b != NULL; b = b->b_next) {\n\t\tif (!ARC_BUF_COMPRESSED(b)) {\n\t\t\treturn (B_TRUE);\n\t\t}\n\t}\n\treturn (B_FALSE);\n}\n\n\n \nstatic void\narc_cksum_verify(arc_buf_t *buf)\n{\n#ifdef ZFS_DEBUG\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\tzio_cksum_t zc;\n\n\tif (!(zfs_flags & ZFS_DEBUG_MODIFY))\n\t\treturn;\n\n\tif (ARC_BUF_COMPRESSED(buf))\n\t\treturn;\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\tmutex_enter(&hdr->b_l1hdr.b_freeze_lock);\n\n\tif (hdr->b_l1hdr.b_freeze_cksum == NULL || HDR_IO_ERROR(hdr)) {\n\t\tmutex_exit(&hdr->b_l1hdr.b_freeze_lock);\n\t\treturn;\n\t}\n\n\tfletcher_2_native(buf->b_data, arc_buf_size(buf), NULL, &zc);\n\tif (!ZIO_CHECKSUM_EQUAL(*hdr->b_l1hdr.b_freeze_cksum, zc))\n\t\tpanic(\"buffer modified while frozen!\");\n\tmutex_exit(&hdr->b_l1hdr.b_freeze_lock);\n#endif\n}\n\n \nstatic boolean_t\narc_cksum_is_equal(arc_buf_hdr_t *hdr, zio_t *zio)\n{\n\tASSERT(!BP_IS_EMBEDDED(zio->io_bp));\n\tVERIFY3U(BP_GET_PSIZE(zio->io_bp), ==, HDR_GET_PSIZE(hdr));\n\n\t \n\treturn (zio_checksum_error_impl(zio->io_spa, zio->io_bp,\n\t    BP_GET_CHECKSUM(zio->io_bp), zio->io_abd, zio->io_size,\n\t    zio->io_offset, NULL) == 0);\n}\n\n \nstatic void\narc_cksum_compute(arc_buf_t *buf)\n{\n\tif (!(zfs_flags & ZFS_DEBUG_MODIFY))\n\t\treturn;\n\n#ifdef ZFS_DEBUG\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tmutex_enter(&hdr->b_l1hdr.b_freeze_lock);\n\tif (hdr->b_l1hdr.b_freeze_cksum != NULL || ARC_BUF_COMPRESSED(buf)) {\n\t\tmutex_exit(&hdr->b_l1hdr.b_freeze_lock);\n\t\treturn;\n\t}\n\n\tASSERT(!ARC_BUF_ENCRYPTED(buf));\n\tASSERT(!ARC_BUF_COMPRESSED(buf));\n\thdr->b_l1hdr.b_freeze_cksum = kmem_alloc(sizeof (zio_cksum_t),\n\t    KM_SLEEP);\n\tfletcher_2_native(buf->b_data, arc_buf_size(buf), NULL,\n\t    hdr->b_l1hdr.b_freeze_cksum);\n\tmutex_exit(&hdr->b_l1hdr.b_freeze_lock);\n#endif\n\tarc_buf_watch(buf);\n}\n\n#ifndef _KERNEL\nvoid\narc_buf_sigsegv(int sig, siginfo_t *si, void *unused)\n{\n\t(void) sig, (void) unused;\n\tpanic(\"Got SIGSEGV at address: 0x%lx\\n\", (long)si->si_addr);\n}\n#endif\n\nstatic void\narc_buf_unwatch(arc_buf_t *buf)\n{\n#ifndef _KERNEL\n\tif (arc_watch) {\n\t\tASSERT0(mprotect(buf->b_data, arc_buf_size(buf),\n\t\t    PROT_READ | PROT_WRITE));\n\t}\n#else\n\t(void) buf;\n#endif\n}\n\nstatic void\narc_buf_watch(arc_buf_t *buf)\n{\n#ifndef _KERNEL\n\tif (arc_watch)\n\t\tASSERT0(mprotect(buf->b_data, arc_buf_size(buf),\n\t\t    PROT_READ));\n#else\n\t(void) buf;\n#endif\n}\n\nstatic arc_buf_contents_t\narc_buf_type(arc_buf_hdr_t *hdr)\n{\n\tarc_buf_contents_t type;\n\tif (HDR_ISTYPE_METADATA(hdr)) {\n\t\ttype = ARC_BUFC_METADATA;\n\t} else {\n\t\ttype = ARC_BUFC_DATA;\n\t}\n\tVERIFY3U(hdr->b_type, ==, type);\n\treturn (type);\n}\n\nboolean_t\narc_is_metadata(arc_buf_t *buf)\n{\n\treturn (HDR_ISTYPE_METADATA(buf->b_hdr) != 0);\n}\n\nstatic uint32_t\narc_bufc_to_flags(arc_buf_contents_t type)\n{\n\tswitch (type) {\n\tcase ARC_BUFC_DATA:\n\t\t \n\t\treturn (0);\n\tcase ARC_BUFC_METADATA:\n\t\treturn (ARC_FLAG_BUFC_METADATA);\n\tdefault:\n\t\tbreak;\n\t}\n\tpanic(\"undefined ARC buffer type!\");\n\treturn ((uint32_t)-1);\n}\n\nvoid\narc_buf_thaw(arc_buf_t *buf)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT3P(hdr->b_l1hdr.b_state, ==, arc_anon);\n\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\n\tarc_cksum_verify(buf);\n\n\t \n\tif (ARC_BUF_COMPRESSED(buf))\n\t\treturn;\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tarc_cksum_free(hdr);\n\tarc_buf_unwatch(buf);\n}\n\nvoid\narc_buf_freeze(arc_buf_t *buf)\n{\n\tif (!(zfs_flags & ZFS_DEBUG_MODIFY))\n\t\treturn;\n\n\tif (ARC_BUF_COMPRESSED(buf))\n\t\treturn;\n\n\tASSERT(HDR_HAS_L1HDR(buf->b_hdr));\n\tarc_cksum_compute(buf);\n}\n\n \nstatic inline void\narc_hdr_set_flags(arc_buf_hdr_t *hdr, arc_flags_t flags)\n{\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\thdr->b_flags |= flags;\n}\n\nstatic inline void\narc_hdr_clear_flags(arc_buf_hdr_t *hdr, arc_flags_t flags)\n{\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\thdr->b_flags &= ~flags;\n}\n\n \nstatic void\narc_hdr_set_compress(arc_buf_hdr_t *hdr, enum zio_compress cmp)\n{\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\n\t \n\tif (!zfs_compressed_arc_enabled || HDR_GET_PSIZE(hdr) == 0) {\n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_COMPRESSED_ARC);\n\t\tASSERT(!HDR_COMPRESSION_ENABLED(hdr));\n\t} else {\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_COMPRESSED_ARC);\n\t\tASSERT(HDR_COMPRESSION_ENABLED(hdr));\n\t}\n\n\tHDR_SET_COMPRESS(hdr, cmp);\n\tASSERT3U(HDR_GET_COMPRESS(hdr), ==, cmp);\n}\n\n \nstatic boolean_t\narc_buf_try_copy_decompressed_data(arc_buf_t *buf)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\tboolean_t copied = B_FALSE;\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT3P(buf->b_data, !=, NULL);\n\tASSERT(!ARC_BUF_COMPRESSED(buf));\n\n\tfor (arc_buf_t *from = hdr->b_l1hdr.b_buf; from != NULL;\n\t    from = from->b_next) {\n\t\t \n\t\tif (from == buf) {\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!ARC_BUF_COMPRESSED(from)) {\n\t\t\tmemcpy(buf->b_data, from->b_data, arc_buf_size(buf));\n\t\t\tcopied = B_TRUE;\n\t\t\tbreak;\n\t\t}\n\t}\n\n#ifdef ZFS_DEBUG\n\t \n\tif (zfs_flags & ZFS_DEBUG_MODIFY)\n\t\tEQUIV(!copied, hdr->b_l1hdr.b_freeze_cksum == NULL);\n#endif\n\n\treturn (copied);\n}\n\n \nstatic arc_buf_hdr_t *\narc_buf_alloc_l2only(size_t size, arc_buf_contents_t type, l2arc_dev_t *dev,\n    dva_t dva, uint64_t daddr, int32_t psize, uint64_t birth,\n    enum zio_compress compress, uint8_t complevel, boolean_t protected,\n    boolean_t prefetch, arc_state_type_t arcs_state)\n{\n\tarc_buf_hdr_t\t*hdr;\n\n\tASSERT(size != 0);\n\thdr = kmem_cache_alloc(hdr_l2only_cache, KM_SLEEP);\n\thdr->b_birth = birth;\n\thdr->b_type = type;\n\thdr->b_flags = 0;\n\tarc_hdr_set_flags(hdr, arc_bufc_to_flags(type) | ARC_FLAG_HAS_L2HDR);\n\tHDR_SET_LSIZE(hdr, size);\n\tHDR_SET_PSIZE(hdr, psize);\n\tarc_hdr_set_compress(hdr, compress);\n\thdr->b_complevel = complevel;\n\tif (protected)\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PROTECTED);\n\tif (prefetch)\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PREFETCH);\n\thdr->b_spa = spa_load_guid(dev->l2ad_vdev->vdev_spa);\n\n\thdr->b_dva = dva;\n\n\thdr->b_l2hdr.b_dev = dev;\n\thdr->b_l2hdr.b_daddr = daddr;\n\thdr->b_l2hdr.b_arcs_state = arcs_state;\n\n\treturn (hdr);\n}\n\n \nstatic uint64_t\narc_hdr_size(arc_buf_hdr_t *hdr)\n{\n\tuint64_t size;\n\n\tif (arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF &&\n\t    HDR_GET_PSIZE(hdr) > 0) {\n\t\tsize = HDR_GET_PSIZE(hdr);\n\t} else {\n\t\tASSERT3U(HDR_GET_LSIZE(hdr), !=, 0);\n\t\tsize = HDR_GET_LSIZE(hdr);\n\t}\n\treturn (size);\n}\n\nstatic int\narc_hdr_authenticate(arc_buf_hdr_t *hdr, spa_t *spa, uint64_t dsobj)\n{\n\tint ret;\n\tuint64_t csize;\n\tuint64_t lsize = HDR_GET_LSIZE(hdr);\n\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\tvoid *tmpbuf = NULL;\n\tabd_t *abd = hdr->b_l1hdr.b_pabd;\n\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\tASSERT(HDR_AUTHENTICATED(hdr));\n\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\n\t \n\tif (HDR_GET_COMPRESS(hdr) != ZIO_COMPRESS_OFF &&\n\t    !HDR_COMPRESSION_ENABLED(hdr)) {\n\n\t\tcsize = zio_compress_data(HDR_GET_COMPRESS(hdr),\n\t\t    hdr->b_l1hdr.b_pabd, &tmpbuf, lsize, hdr->b_complevel);\n\t\tASSERT3P(tmpbuf, !=, NULL);\n\t\tASSERT3U(csize, <=, psize);\n\t\tabd = abd_get_from_buf(tmpbuf, lsize);\n\t\tabd_take_ownership_of_buf(abd, B_TRUE);\n\t\tabd_zero_off(abd, csize, psize - csize);\n\t}\n\n\t \n\tif (hdr->b_crypt_hdr.b_ot == DMU_OT_OBJSET) {\n\t\tASSERT3U(HDR_GET_COMPRESS(hdr), ==, ZIO_COMPRESS_OFF);\n\t\tASSERT3U(lsize, ==, psize);\n\t\tret = spa_do_crypt_objset_mac_abd(B_FALSE, spa, dsobj, abd,\n\t\t    psize, hdr->b_l1hdr.b_byteswap != DMU_BSWAP_NUMFUNCS);\n\t} else {\n\t\tret = spa_do_crypt_mac_abd(B_FALSE, spa, dsobj, abd, psize,\n\t\t    hdr->b_crypt_hdr.b_mac);\n\t}\n\n\tif (ret == 0)\n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_NOAUTH);\n\telse if (ret != ENOENT)\n\t\tgoto error;\n\n\tif (tmpbuf != NULL)\n\t\tabd_free(abd);\n\n\treturn (0);\n\nerror:\n\tif (tmpbuf != NULL)\n\t\tabd_free(abd);\n\n\treturn (ret);\n}\n\n \nstatic int\narc_hdr_decrypt(arc_buf_hdr_t *hdr, spa_t *spa, const zbookmark_phys_t *zb)\n{\n\tint ret;\n\tabd_t *cabd = NULL;\n\tvoid *tmp = NULL;\n\tboolean_t no_crypt = B_FALSE;\n\tboolean_t bswap = (hdr->b_l1hdr.b_byteswap != DMU_BSWAP_NUMFUNCS);\n\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\tASSERT(HDR_ENCRYPTED(hdr));\n\n\tarc_hdr_alloc_abd(hdr, 0);\n\n\tret = spa_do_crypt_abd(B_FALSE, spa, zb, hdr->b_crypt_hdr.b_ot,\n\t    B_FALSE, bswap, hdr->b_crypt_hdr.b_salt, hdr->b_crypt_hdr.b_iv,\n\t    hdr->b_crypt_hdr.b_mac, HDR_GET_PSIZE(hdr), hdr->b_l1hdr.b_pabd,\n\t    hdr->b_crypt_hdr.b_rabd, &no_crypt);\n\tif (ret != 0)\n\t\tgoto error;\n\n\tif (no_crypt) {\n\t\tabd_copy(hdr->b_l1hdr.b_pabd, hdr->b_crypt_hdr.b_rabd,\n\t\t    HDR_GET_PSIZE(hdr));\n\t}\n\n\t \n\tif (HDR_GET_COMPRESS(hdr) != ZIO_COMPRESS_OFF &&\n\t    !HDR_COMPRESSION_ENABLED(hdr)) {\n\t\t \n\t\tcabd = arc_get_data_abd(hdr, arc_hdr_size(hdr), hdr, 0);\n\t\ttmp = abd_borrow_buf(cabd, arc_hdr_size(hdr));\n\n\t\tret = zio_decompress_data(HDR_GET_COMPRESS(hdr),\n\t\t    hdr->b_l1hdr.b_pabd, tmp, HDR_GET_PSIZE(hdr),\n\t\t    HDR_GET_LSIZE(hdr), &hdr->b_complevel);\n\t\tif (ret != 0) {\n\t\t\tabd_return_buf(cabd, tmp, arc_hdr_size(hdr));\n\t\t\tgoto error;\n\t\t}\n\n\t\tabd_return_buf_copy(cabd, tmp, arc_hdr_size(hdr));\n\t\tarc_free_data_abd(hdr, hdr->b_l1hdr.b_pabd,\n\t\t    arc_hdr_size(hdr), hdr);\n\t\thdr->b_l1hdr.b_pabd = cabd;\n\t}\n\n\treturn (0);\n\nerror:\n\tarc_hdr_free_abd(hdr, B_FALSE);\n\tif (cabd != NULL)\n\t\tarc_free_data_buf(hdr, cabd, arc_hdr_size(hdr), hdr);\n\n\treturn (ret);\n}\n\n \nstatic int\narc_fill_hdr_crypt(arc_buf_hdr_t *hdr, kmutex_t *hash_lock, spa_t *spa,\n    const zbookmark_phys_t *zb, boolean_t noauth)\n{\n\tint ret;\n\n\tASSERT(HDR_PROTECTED(hdr));\n\n\tif (hash_lock != NULL)\n\t\tmutex_enter(hash_lock);\n\n\tif (HDR_NOAUTH(hdr) && !noauth) {\n\t\t \n\t\tret = arc_hdr_authenticate(hdr, spa, zb->zb_objset);\n\t\tif (ret != 0)\n\t\t\tgoto error;\n\t} else if (HDR_HAS_RABD(hdr) && hdr->b_l1hdr.b_pabd == NULL) {\n\t\t \n\t\tret = arc_hdr_decrypt(hdr, spa, zb);\n\t\tif (ret != 0)\n\t\t\tgoto error;\n\t}\n\n\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\n\tif (hash_lock != NULL)\n\t\tmutex_exit(hash_lock);\n\n\treturn (0);\n\nerror:\n\tif (hash_lock != NULL)\n\t\tmutex_exit(hash_lock);\n\n\treturn (ret);\n}\n\n \nstatic void\narc_buf_untransform_in_place(arc_buf_t *buf)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT(HDR_ENCRYPTED(hdr));\n\tASSERT3U(hdr->b_crypt_hdr.b_ot, ==, DMU_OT_DNODE);\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\n\tzio_crypt_copy_dnode_bonus(hdr->b_l1hdr.b_pabd, buf->b_data,\n\t    arc_buf_size(buf));\n\tbuf->b_flags &= ~ARC_BUF_FLAG_ENCRYPTED;\n\tbuf->b_flags &= ~ARC_BUF_FLAG_COMPRESSED;\n}\n\n \nstatic int\narc_buf_fill(arc_buf_t *buf, spa_t *spa, const zbookmark_phys_t *zb,\n    arc_fill_flags_t flags)\n{\n\tint error = 0;\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\tboolean_t hdr_compressed =\n\t    (arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF);\n\tboolean_t compressed = (flags & ARC_FILL_COMPRESSED) != 0;\n\tboolean_t encrypted = (flags & ARC_FILL_ENCRYPTED) != 0;\n\tdmu_object_byteswap_t bswap = hdr->b_l1hdr.b_byteswap;\n\tkmutex_t *hash_lock = (flags & ARC_FILL_LOCKED) ? NULL : HDR_LOCK(hdr);\n\n\tASSERT3P(buf->b_data, !=, NULL);\n\tIMPLY(compressed, hdr_compressed || ARC_BUF_ENCRYPTED(buf));\n\tIMPLY(compressed, ARC_BUF_COMPRESSED(buf));\n\tIMPLY(encrypted, HDR_ENCRYPTED(hdr));\n\tIMPLY(encrypted, ARC_BUF_ENCRYPTED(buf));\n\tIMPLY(encrypted, ARC_BUF_COMPRESSED(buf));\n\tIMPLY(encrypted, !arc_buf_is_shared(buf));\n\n\t \n\tif (encrypted) {\n\t\tASSERT(HDR_HAS_RABD(hdr));\n\t\tabd_copy_to_buf(buf->b_data, hdr->b_crypt_hdr.b_rabd,\n\t\t    HDR_GET_PSIZE(hdr));\n\t\tgoto byteswap;\n\t}\n\n\t \n\tif (HDR_PROTECTED(hdr)) {\n\t\terror = arc_fill_hdr_crypt(hdr, hash_lock, spa,\n\t\t    zb, !!(flags & ARC_FILL_NOAUTH));\n\t\tif (error == EACCES && (flags & ARC_FILL_IN_PLACE) != 0) {\n\t\t\treturn (error);\n\t\t} else if (error != 0) {\n\t\t\tif (hash_lock != NULL)\n\t\t\t\tmutex_enter(hash_lock);\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_IO_ERROR);\n\t\t\tif (hash_lock != NULL)\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\treturn (error);\n\t\t}\n\t}\n\n\t \n\tif ((flags & ARC_FILL_IN_PLACE) != 0) {\n\t\tASSERT(!hdr_compressed);\n\t\tASSERT(!compressed);\n\t\tASSERT(!encrypted);\n\n\t\tif (HDR_ENCRYPTED(hdr) && ARC_BUF_ENCRYPTED(buf)) {\n\t\t\tASSERT3U(hdr->b_crypt_hdr.b_ot, ==, DMU_OT_DNODE);\n\n\t\t\tif (hash_lock != NULL)\n\t\t\t\tmutex_enter(hash_lock);\n\t\t\tarc_buf_untransform_in_place(buf);\n\t\t\tif (hash_lock != NULL)\n\t\t\t\tmutex_exit(hash_lock);\n\n\t\t\t \n\t\t\tarc_cksum_compute(buf);\n\t\t}\n\n\t\treturn (0);\n\t}\n\n\tif (hdr_compressed == compressed) {\n\t\tif (ARC_BUF_SHARED(buf)) {\n\t\t\tASSERT(arc_buf_is_shared(buf));\n\t\t} else {\n\t\t\tabd_copy_to_buf(buf->b_data, hdr->b_l1hdr.b_pabd,\n\t\t\t    arc_buf_size(buf));\n\t\t}\n\t} else {\n\t\tASSERT(hdr_compressed);\n\t\tASSERT(!compressed);\n\n\t\t \n\t\tif (ARC_BUF_SHARED(buf)) {\n\t\t\tASSERT(ARC_BUF_COMPRESSED(buf));\n\n\t\t\t \n\t\t\tbuf->b_flags &= ~ARC_BUF_FLAG_SHARED;\n\t\t\tbuf->b_data =\n\t\t\t    arc_get_data_buf(hdr, HDR_GET_LSIZE(hdr), buf);\n\t\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_SHARED_DATA);\n\n\t\t\t \n\t\t\tARCSTAT_INCR(arcstat_overhead_size, HDR_GET_LSIZE(hdr));\n\t\t} else if (ARC_BUF_COMPRESSED(buf)) {\n\t\t\tASSERT(!arc_buf_is_shared(buf));\n\n\t\t\t \n\t\t\tarc_free_data_buf(hdr, buf->b_data, HDR_GET_PSIZE(hdr),\n\t\t\t    buf);\n\t\t\tbuf->b_data =\n\t\t\t    arc_get_data_buf(hdr, HDR_GET_LSIZE(hdr), buf);\n\n\t\t\t \n\t\t\tARCSTAT_INCR(arcstat_overhead_size,\n\t\t\t    HDR_GET_LSIZE(hdr) - HDR_GET_PSIZE(hdr));\n\t\t}\n\n\t\t \n\t\tbuf->b_flags &= ~ARC_BUF_FLAG_COMPRESSED;\n\n\t\t \n\t\tif (arc_buf_try_copy_decompressed_data(buf)) {\n\t\t\t \n\t\t\treturn (0);\n\t\t} else {\n\t\t\terror = zio_decompress_data(HDR_GET_COMPRESS(hdr),\n\t\t\t    hdr->b_l1hdr.b_pabd, buf->b_data,\n\t\t\t    HDR_GET_PSIZE(hdr), HDR_GET_LSIZE(hdr),\n\t\t\t    &hdr->b_complevel);\n\n\t\t\t \n\t\t\tif (error != 0) {\n\t\t\t\tzfs_dbgmsg(\n\t\t\t\t    \"hdr %px, compress %d, psize %d, lsize %d\",\n\t\t\t\t    hdr, arc_hdr_get_compress(hdr),\n\t\t\t\t    HDR_GET_PSIZE(hdr), HDR_GET_LSIZE(hdr));\n\t\t\t\tif (hash_lock != NULL)\n\t\t\t\t\tmutex_enter(hash_lock);\n\t\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_IO_ERROR);\n\t\t\t\tif (hash_lock != NULL)\n\t\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\treturn (SET_ERROR(EIO));\n\t\t\t}\n\t\t}\n\t}\n\nbyteswap:\n\t \n\tif (bswap != DMU_BSWAP_NUMFUNCS) {\n\t\tASSERT(!HDR_SHARED_DATA(hdr));\n\t\tASSERT3U(bswap, <, DMU_BSWAP_NUMFUNCS);\n\t\tdmu_ot_byteswap[bswap].ob_func(buf->b_data, HDR_GET_LSIZE(hdr));\n\t}\n\n\t \n\tarc_cksum_compute(buf);\n\n\treturn (0);\n}\n\n \nint\narc_untransform(arc_buf_t *buf, spa_t *spa, const zbookmark_phys_t *zb,\n    boolean_t in_place)\n{\n\tint ret;\n\tarc_fill_flags_t flags = 0;\n\n\tif (in_place)\n\t\tflags |= ARC_FILL_IN_PLACE;\n\n\tret = arc_buf_fill(buf, spa, zb, flags);\n\tif (ret == ECKSUM) {\n\t\t \n\t\tret = SET_ERROR(EIO);\n\t\tspa_log_error(spa, zb, &buf->b_hdr->b_birth);\n\t\t(void) zfs_ereport_post(FM_EREPORT_ZFS_AUTHENTICATION,\n\t\t    spa, NULL, zb, NULL, 0);\n\t}\n\n\treturn (ret);\n}\n\n \nstatic void\narc_evictable_space_increment(arc_buf_hdr_t *hdr, arc_state_t *state)\n{\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\tif (GHOST_STATE(state)) {\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, NULL);\n\t\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\tASSERT(!HDR_HAS_RABD(hdr));\n\t\t(void) zfs_refcount_add_many(&state->arcs_esize[type],\n\t\t    HDR_GET_LSIZE(hdr), hdr);\n\t\treturn;\n\t}\n\n\tif (hdr->b_l1hdr.b_pabd != NULL) {\n\t\t(void) zfs_refcount_add_many(&state->arcs_esize[type],\n\t\t    arc_hdr_size(hdr), hdr);\n\t}\n\tif (HDR_HAS_RABD(hdr)) {\n\t\t(void) zfs_refcount_add_many(&state->arcs_esize[type],\n\t\t    HDR_GET_PSIZE(hdr), hdr);\n\t}\n\n\tfor (arc_buf_t *buf = hdr->b_l1hdr.b_buf; buf != NULL;\n\t    buf = buf->b_next) {\n\t\tif (ARC_BUF_SHARED(buf))\n\t\t\tcontinue;\n\t\t(void) zfs_refcount_add_many(&state->arcs_esize[type],\n\t\t    arc_buf_size(buf), buf);\n\t}\n}\n\n \nstatic void\narc_evictable_space_decrement(arc_buf_hdr_t *hdr, arc_state_t *state)\n{\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\tif (GHOST_STATE(state)) {\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, NULL);\n\t\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\tASSERT(!HDR_HAS_RABD(hdr));\n\t\t(void) zfs_refcount_remove_many(&state->arcs_esize[type],\n\t\t    HDR_GET_LSIZE(hdr), hdr);\n\t\treturn;\n\t}\n\n\tif (hdr->b_l1hdr.b_pabd != NULL) {\n\t\t(void) zfs_refcount_remove_many(&state->arcs_esize[type],\n\t\t    arc_hdr_size(hdr), hdr);\n\t}\n\tif (HDR_HAS_RABD(hdr)) {\n\t\t(void) zfs_refcount_remove_many(&state->arcs_esize[type],\n\t\t    HDR_GET_PSIZE(hdr), hdr);\n\t}\n\n\tfor (arc_buf_t *buf = hdr->b_l1hdr.b_buf; buf != NULL;\n\t    buf = buf->b_next) {\n\t\tif (ARC_BUF_SHARED(buf))\n\t\t\tcontinue;\n\t\t(void) zfs_refcount_remove_many(&state->arcs_esize[type],\n\t\t    arc_buf_size(buf), buf);\n\t}\n}\n\n \nstatic void\nadd_reference(arc_buf_hdr_t *hdr, const void *tag)\n{\n\tarc_state_t *state = hdr->b_l1hdr.b_state;\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tif (!HDR_EMPTY(hdr) && !MUTEX_HELD(HDR_LOCK(hdr))) {\n\t\tASSERT(state == arc_anon);\n\t\tASSERT(zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt));\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, NULL);\n\t}\n\n\tif ((zfs_refcount_add(&hdr->b_l1hdr.b_refcnt, tag) == 1) &&\n\t    state != arc_anon && state != arc_l2c_only) {\n\t\t \n\t\tmultilist_remove(&state->arcs_list[arc_buf_type(hdr)], hdr);\n\t\tarc_evictable_space_decrement(hdr, state);\n\t}\n}\n\n \nstatic int\nremove_reference(arc_buf_hdr_t *hdr, const void *tag)\n{\n\tint cnt;\n\tarc_state_t *state = hdr->b_l1hdr.b_state;\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT(state == arc_anon || MUTEX_HELD(HDR_LOCK(hdr)));\n\tASSERT(!GHOST_STATE(state));\t \n\n\tif ((cnt = zfs_refcount_remove(&hdr->b_l1hdr.b_refcnt, tag)) != 0)\n\t\treturn (cnt);\n\n\tif (state == arc_anon) {\n\t\tarc_hdr_destroy(hdr);\n\t\treturn (0);\n\t}\n\tif (state == arc_uncached && !HDR_PREFETCH(hdr)) {\n\t\tarc_change_state(arc_anon, hdr);\n\t\tarc_hdr_destroy(hdr);\n\t\treturn (0);\n\t}\n\tmultilist_insert(&state->arcs_list[arc_buf_type(hdr)], hdr);\n\tarc_evictable_space_increment(hdr, state);\n\treturn (0);\n}\n\n \nvoid\narc_buf_info(arc_buf_t *ab, arc_buf_info_t *abi, int state_index)\n{\n\t(void) state_index;\n\tarc_buf_hdr_t *hdr = ab->b_hdr;\n\tl1arc_buf_hdr_t *l1hdr = NULL;\n\tl2arc_buf_hdr_t *l2hdr = NULL;\n\tarc_state_t *state = NULL;\n\n\tmemset(abi, 0, sizeof (arc_buf_info_t));\n\n\tif (hdr == NULL)\n\t\treturn;\n\n\tabi->abi_flags = hdr->b_flags;\n\n\tif (HDR_HAS_L1HDR(hdr)) {\n\t\tl1hdr = &hdr->b_l1hdr;\n\t\tstate = l1hdr->b_state;\n\t}\n\tif (HDR_HAS_L2HDR(hdr))\n\t\tl2hdr = &hdr->b_l2hdr;\n\n\tif (l1hdr) {\n\t\tabi->abi_bufcnt = 0;\n\t\tfor (arc_buf_t *buf = l1hdr->b_buf; buf; buf = buf->b_next)\n\t\t\tabi->abi_bufcnt++;\n\t\tabi->abi_access = l1hdr->b_arc_access;\n\t\tabi->abi_mru_hits = l1hdr->b_mru_hits;\n\t\tabi->abi_mru_ghost_hits = l1hdr->b_mru_ghost_hits;\n\t\tabi->abi_mfu_hits = l1hdr->b_mfu_hits;\n\t\tabi->abi_mfu_ghost_hits = l1hdr->b_mfu_ghost_hits;\n\t\tabi->abi_holds = zfs_refcount_count(&l1hdr->b_refcnt);\n\t}\n\n\tif (l2hdr) {\n\t\tabi->abi_l2arc_dattr = l2hdr->b_daddr;\n\t\tabi->abi_l2arc_hits = l2hdr->b_hits;\n\t}\n\n\tabi->abi_state_type = state ? state->arcs_state : ARC_STATE_ANON;\n\tabi->abi_state_contents = arc_buf_type(hdr);\n\tabi->abi_size = arc_hdr_size(hdr);\n}\n\n \nstatic void\narc_change_state(arc_state_t *new_state, arc_buf_hdr_t *hdr)\n{\n\tarc_state_t *old_state;\n\tint64_t refcnt;\n\tboolean_t update_old, update_new;\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\t \n\tif (HDR_HAS_L1HDR(hdr)) {\n\t\told_state = hdr->b_l1hdr.b_state;\n\t\trefcnt = zfs_refcount_count(&hdr->b_l1hdr.b_refcnt);\n\t\tupdate_old = (hdr->b_l1hdr.b_buf != NULL ||\n\t\t    hdr->b_l1hdr.b_pabd != NULL || HDR_HAS_RABD(hdr));\n\n\t\tIMPLY(GHOST_STATE(old_state), hdr->b_l1hdr.b_buf == NULL);\n\t\tIMPLY(GHOST_STATE(new_state), hdr->b_l1hdr.b_buf == NULL);\n\t\tIMPLY(old_state == arc_anon, hdr->b_l1hdr.b_buf == NULL ||\n\t\t    ARC_BUF_LAST(hdr->b_l1hdr.b_buf));\n\t} else {\n\t\told_state = arc_l2c_only;\n\t\trefcnt = 0;\n\t\tupdate_old = B_FALSE;\n\t}\n\tupdate_new = update_old;\n\tif (GHOST_STATE(old_state))\n\t\tupdate_old = B_TRUE;\n\tif (GHOST_STATE(new_state))\n\t\tupdate_new = B_TRUE;\n\n\tASSERT(MUTEX_HELD(HDR_LOCK(hdr)));\n\tASSERT3P(new_state, !=, old_state);\n\n\t \n\tif (refcnt == 0) {\n\t\tif (old_state != arc_anon && old_state != arc_l2c_only) {\n\t\t\tASSERT(HDR_HAS_L1HDR(hdr));\n\t\t\t \n\t\t\tif (multilist_link_active(&hdr->b_l1hdr.b_arc_node)) {\n\t\t\t\tmultilist_remove(&old_state->arcs_list[type],\n\t\t\t\t    hdr);\n\t\t\t\tarc_evictable_space_decrement(hdr, old_state);\n\t\t\t}\n\t\t}\n\t\tif (new_state != arc_anon && new_state != arc_l2c_only) {\n\t\t\t \n\t\t\tASSERT(HDR_HAS_L1HDR(hdr));\n\t\t\tmultilist_insert(&new_state->arcs_list[type], hdr);\n\t\t\tarc_evictable_space_increment(hdr, new_state);\n\t\t}\n\t}\n\n\tASSERT(!HDR_EMPTY(hdr));\n\tif (new_state == arc_anon && HDR_IN_HASH_TABLE(hdr))\n\t\tbuf_hash_remove(hdr);\n\n\t \n\n\tif (update_new && new_state != arc_l2c_only) {\n\t\tASSERT(HDR_HAS_L1HDR(hdr));\n\t\tif (GHOST_STATE(new_state)) {\n\n\t\t\t \n\t\t\t(void) zfs_refcount_add_many(\n\t\t\t    &new_state->arcs_size[type],\n\t\t\t    HDR_GET_LSIZE(hdr), hdr);\n\t\t\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\t\tASSERT(!HDR_HAS_RABD(hdr));\n\t\t} else {\n\n\t\t\t \n\t\t\tfor (arc_buf_t *buf = hdr->b_l1hdr.b_buf; buf != NULL;\n\t\t\t    buf = buf->b_next) {\n\n\t\t\t\t \n\t\t\t\tif (ARC_BUF_SHARED(buf))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t(void) zfs_refcount_add_many(\n\t\t\t\t    &new_state->arcs_size[type],\n\t\t\t\t    arc_buf_size(buf), buf);\n\t\t\t}\n\n\t\t\tif (hdr->b_l1hdr.b_pabd != NULL) {\n\t\t\t\t(void) zfs_refcount_add_many(\n\t\t\t\t    &new_state->arcs_size[type],\n\t\t\t\t    arc_hdr_size(hdr), hdr);\n\t\t\t}\n\n\t\t\tif (HDR_HAS_RABD(hdr)) {\n\t\t\t\t(void) zfs_refcount_add_many(\n\t\t\t\t    &new_state->arcs_size[type],\n\t\t\t\t    HDR_GET_PSIZE(hdr), hdr);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (update_old && old_state != arc_l2c_only) {\n\t\tASSERT(HDR_HAS_L1HDR(hdr));\n\t\tif (GHOST_STATE(old_state)) {\n\t\t\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\t\tASSERT(!HDR_HAS_RABD(hdr));\n\n\t\t\t \n\n\t\t\t(void) zfs_refcount_remove_many(\n\t\t\t    &old_state->arcs_size[type],\n\t\t\t    HDR_GET_LSIZE(hdr), hdr);\n\t\t} else {\n\n\t\t\t \n\t\t\tfor (arc_buf_t *buf = hdr->b_l1hdr.b_buf; buf != NULL;\n\t\t\t    buf = buf->b_next) {\n\n\t\t\t\t \n\t\t\t\tif (ARC_BUF_SHARED(buf))\n\t\t\t\t\tcontinue;\n\n\t\t\t\t(void) zfs_refcount_remove_many(\n\t\t\t\t    &old_state->arcs_size[type],\n\t\t\t\t    arc_buf_size(buf), buf);\n\t\t\t}\n\t\t\tASSERT(hdr->b_l1hdr.b_pabd != NULL ||\n\t\t\t    HDR_HAS_RABD(hdr));\n\n\t\t\tif (hdr->b_l1hdr.b_pabd != NULL) {\n\t\t\t\t(void) zfs_refcount_remove_many(\n\t\t\t\t    &old_state->arcs_size[type],\n\t\t\t\t    arc_hdr_size(hdr), hdr);\n\t\t\t}\n\n\t\t\tif (HDR_HAS_RABD(hdr)) {\n\t\t\t\t(void) zfs_refcount_remove_many(\n\t\t\t\t    &old_state->arcs_size[type],\n\t\t\t\t    HDR_GET_PSIZE(hdr), hdr);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (HDR_HAS_L1HDR(hdr)) {\n\t\thdr->b_l1hdr.b_state = new_state;\n\n\t\tif (HDR_HAS_L2HDR(hdr) && new_state != arc_l2c_only) {\n\t\t\tl2arc_hdr_arcstats_decrement_state(hdr);\n\t\t\thdr->b_l2hdr.b_arcs_state = new_state->arcs_state;\n\t\t\tl2arc_hdr_arcstats_increment_state(hdr);\n\t\t}\n\t}\n}\n\nvoid\narc_space_consume(uint64_t space, arc_space_type_t type)\n{\n\tASSERT(type >= 0 && type < ARC_SPACE_NUMTYPES);\n\n\tswitch (type) {\n\tdefault:\n\t\tbreak;\n\tcase ARC_SPACE_DATA:\n\t\tARCSTAT_INCR(arcstat_data_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_META:\n\t\tARCSTAT_INCR(arcstat_metadata_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_BONUS:\n\t\tARCSTAT_INCR(arcstat_bonus_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_DNODE:\n\t\tARCSTAT_INCR(arcstat_dnode_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_DBUF:\n\t\tARCSTAT_INCR(arcstat_dbuf_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_HDRS:\n\t\tARCSTAT_INCR(arcstat_hdr_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_L2HDRS:\n\t\taggsum_add(&arc_sums.arcstat_l2_hdr_size, space);\n\t\tbreak;\n\tcase ARC_SPACE_ABD_CHUNK_WASTE:\n\t\t \n\t\tARCSTAT_INCR(arcstat_abd_chunk_waste_size, space);\n\t\tbreak;\n\t}\n\n\tif (type != ARC_SPACE_DATA && type != ARC_SPACE_ABD_CHUNK_WASTE)\n\t\tARCSTAT_INCR(arcstat_meta_used, space);\n\n\taggsum_add(&arc_sums.arcstat_size, space);\n}\n\nvoid\narc_space_return(uint64_t space, arc_space_type_t type)\n{\n\tASSERT(type >= 0 && type < ARC_SPACE_NUMTYPES);\n\n\tswitch (type) {\n\tdefault:\n\t\tbreak;\n\tcase ARC_SPACE_DATA:\n\t\tARCSTAT_INCR(arcstat_data_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_META:\n\t\tARCSTAT_INCR(arcstat_metadata_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_BONUS:\n\t\tARCSTAT_INCR(arcstat_bonus_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_DNODE:\n\t\tARCSTAT_INCR(arcstat_dnode_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_DBUF:\n\t\tARCSTAT_INCR(arcstat_dbuf_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_HDRS:\n\t\tARCSTAT_INCR(arcstat_hdr_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_L2HDRS:\n\t\taggsum_add(&arc_sums.arcstat_l2_hdr_size, -space);\n\t\tbreak;\n\tcase ARC_SPACE_ABD_CHUNK_WASTE:\n\t\tARCSTAT_INCR(arcstat_abd_chunk_waste_size, -space);\n\t\tbreak;\n\t}\n\n\tif (type != ARC_SPACE_DATA && type != ARC_SPACE_ABD_CHUNK_WASTE)\n\t\tARCSTAT_INCR(arcstat_meta_used, -space);\n\n\tASSERT(aggsum_compare(&arc_sums.arcstat_size, space) >= 0);\n\taggsum_add(&arc_sums.arcstat_size, -space);\n}\n\n \nstatic boolean_t\narc_can_share(arc_buf_hdr_t *hdr, arc_buf_t *buf)\n{\n\t \n\tASSERT3P(buf->b_hdr, ==, hdr);\n\tboolean_t hdr_compressed =\n\t    arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF;\n\tboolean_t buf_compressed = ARC_BUF_COMPRESSED(buf) != 0;\n\treturn (!ARC_BUF_ENCRYPTED(buf) &&\n\t    buf_compressed == hdr_compressed &&\n\t    hdr->b_l1hdr.b_byteswap == DMU_BSWAP_NUMFUNCS &&\n\t    !HDR_SHARED_DATA(hdr) &&\n\t    (ARC_BUF_LAST(buf) || ARC_BUF_COMPRESSED(buf)));\n}\n\n \nstatic int\narc_buf_alloc_impl(arc_buf_hdr_t *hdr, spa_t *spa, const zbookmark_phys_t *zb,\n    const void *tag, boolean_t encrypted, boolean_t compressed,\n    boolean_t noauth, boolean_t fill, arc_buf_t **ret)\n{\n\tarc_buf_t *buf;\n\tarc_fill_flags_t flags = ARC_FILL_LOCKED;\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT3U(HDR_GET_LSIZE(hdr), >, 0);\n\tVERIFY(hdr->b_type == ARC_BUFC_DATA ||\n\t    hdr->b_type == ARC_BUFC_METADATA);\n\tASSERT3P(ret, !=, NULL);\n\tASSERT3P(*ret, ==, NULL);\n\tIMPLY(encrypted, compressed);\n\n\tbuf = *ret = kmem_cache_alloc(buf_cache, KM_PUSHPAGE);\n\tbuf->b_hdr = hdr;\n\tbuf->b_data = NULL;\n\tbuf->b_next = hdr->b_l1hdr.b_buf;\n\tbuf->b_flags = 0;\n\n\tadd_reference(hdr, tag);\n\n\t \n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\n\t \n\tif (encrypted) {\n\t\tbuf->b_flags |= ARC_BUF_FLAG_COMPRESSED;\n\t\tbuf->b_flags |= ARC_BUF_FLAG_ENCRYPTED;\n\t\tflags |= ARC_FILL_COMPRESSED | ARC_FILL_ENCRYPTED;\n\t} else if (compressed &&\n\t    arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF) {\n\t\tbuf->b_flags |= ARC_BUF_FLAG_COMPRESSED;\n\t\tflags |= ARC_FILL_COMPRESSED;\n\t}\n\n\tif (noauth) {\n\t\tASSERT0(encrypted);\n\t\tflags |= ARC_FILL_NOAUTH;\n\t}\n\n\t \n\tboolean_t can_share = arc_can_share(hdr, buf) &&\n\t    !HDR_L2_WRITING(hdr) &&\n\t    hdr->b_l1hdr.b_pabd != NULL &&\n\t    abd_is_linear(hdr->b_l1hdr.b_pabd) &&\n\t    !abd_is_linear_page(hdr->b_l1hdr.b_pabd);\n\n\t \n\tif (can_share) {\n\t\tbuf->b_data = abd_to_buf(hdr->b_l1hdr.b_pabd);\n\t\tbuf->b_flags |= ARC_BUF_FLAG_SHARED;\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_SHARED_DATA);\n\t} else {\n\t\tbuf->b_data =\n\t\t    arc_get_data_buf(hdr, arc_buf_size(buf), buf);\n\t\tARCSTAT_INCR(arcstat_overhead_size, arc_buf_size(buf));\n\t}\n\tVERIFY3P(buf->b_data, !=, NULL);\n\n\thdr->b_l1hdr.b_buf = buf;\n\n\t \n\tif (fill) {\n\t\tASSERT3P(zb, !=, NULL);\n\t\treturn (arc_buf_fill(buf, spa, zb, flags));\n\t}\n\n\treturn (0);\n}\n\nstatic const char *arc_onloan_tag = \"onloan\";\n\nstatic inline void\narc_loaned_bytes_update(int64_t delta)\n{\n\tatomic_add_64(&arc_loaned_bytes, delta);\n\n\t \n\tASSERT3S(atomic_add_64_nv(&arc_loaned_bytes, 0), >=, 0);\n}\n\n \narc_buf_t *\narc_loan_buf(spa_t *spa, boolean_t is_metadata, int size)\n{\n\tarc_buf_t *buf = arc_alloc_buf(spa, arc_onloan_tag,\n\t    is_metadata ? ARC_BUFC_METADATA : ARC_BUFC_DATA, size);\n\n\tarc_loaned_bytes_update(arc_buf_size(buf));\n\n\treturn (buf);\n}\n\narc_buf_t *\narc_loan_compressed_buf(spa_t *spa, uint64_t psize, uint64_t lsize,\n    enum zio_compress compression_type, uint8_t complevel)\n{\n\tarc_buf_t *buf = arc_alloc_compressed_buf(spa, arc_onloan_tag,\n\t    psize, lsize, compression_type, complevel);\n\n\tarc_loaned_bytes_update(arc_buf_size(buf));\n\n\treturn (buf);\n}\n\narc_buf_t *\narc_loan_raw_buf(spa_t *spa, uint64_t dsobj, boolean_t byteorder,\n    const uint8_t *salt, const uint8_t *iv, const uint8_t *mac,\n    dmu_object_type_t ot, uint64_t psize, uint64_t lsize,\n    enum zio_compress compression_type, uint8_t complevel)\n{\n\tarc_buf_t *buf = arc_alloc_raw_buf(spa, arc_onloan_tag, dsobj,\n\t    byteorder, salt, iv, mac, ot, psize, lsize, compression_type,\n\t    complevel);\n\n\tatomic_add_64(&arc_loaned_bytes, psize);\n\treturn (buf);\n}\n\n\n \nvoid\narc_return_buf(arc_buf_t *buf, const void *tag)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT3P(buf->b_data, !=, NULL);\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\t(void) zfs_refcount_add(&hdr->b_l1hdr.b_refcnt, tag);\n\t(void) zfs_refcount_remove(&hdr->b_l1hdr.b_refcnt, arc_onloan_tag);\n\n\tarc_loaned_bytes_update(-arc_buf_size(buf));\n}\n\n \nvoid\narc_loan_inuse_buf(arc_buf_t *buf, const void *tag)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT3P(buf->b_data, !=, NULL);\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\t(void) zfs_refcount_add(&hdr->b_l1hdr.b_refcnt, arc_onloan_tag);\n\t(void) zfs_refcount_remove(&hdr->b_l1hdr.b_refcnt, tag);\n\n\tarc_loaned_bytes_update(arc_buf_size(buf));\n}\n\nstatic void\nl2arc_free_abd_on_write(abd_t *abd, size_t size, arc_buf_contents_t type)\n{\n\tl2arc_data_free_t *df = kmem_alloc(sizeof (*df), KM_SLEEP);\n\n\tdf->l2df_abd = abd;\n\tdf->l2df_size = size;\n\tdf->l2df_type = type;\n\tmutex_enter(&l2arc_free_on_write_mtx);\n\tlist_insert_head(l2arc_free_on_write, df);\n\tmutex_exit(&l2arc_free_on_write_mtx);\n}\n\nstatic void\narc_hdr_free_on_write(arc_buf_hdr_t *hdr, boolean_t free_rdata)\n{\n\tarc_state_t *state = hdr->b_l1hdr.b_state;\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\tuint64_t size = (free_rdata) ? HDR_GET_PSIZE(hdr) : arc_hdr_size(hdr);\n\n\t \n\tif (multilist_link_active(&hdr->b_l1hdr.b_arc_node)) {\n\t\tASSERT(zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt));\n\t\tASSERT(state != arc_anon && state != arc_l2c_only);\n\n\t\t(void) zfs_refcount_remove_many(&state->arcs_esize[type],\n\t\t    size, hdr);\n\t}\n\t(void) zfs_refcount_remove_many(&state->arcs_size[type], size, hdr);\n\tif (type == ARC_BUFC_METADATA) {\n\t\tarc_space_return(size, ARC_SPACE_META);\n\t} else {\n\t\tASSERT(type == ARC_BUFC_DATA);\n\t\tarc_space_return(size, ARC_SPACE_DATA);\n\t}\n\n\tif (free_rdata) {\n\t\tl2arc_free_abd_on_write(hdr->b_crypt_hdr.b_rabd, size, type);\n\t} else {\n\t\tl2arc_free_abd_on_write(hdr->b_l1hdr.b_pabd, size, type);\n\t}\n}\n\n \nstatic void\narc_share_buf(arc_buf_hdr_t *hdr, arc_buf_t *buf)\n{\n\tASSERT(arc_can_share(hdr, buf));\n\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\tASSERT(!ARC_BUF_ENCRYPTED(buf));\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\n\t \n\tzfs_refcount_transfer_ownership_many(\n\t    &hdr->b_l1hdr.b_state->arcs_size[arc_buf_type(hdr)],\n\t    arc_hdr_size(hdr), buf, hdr);\n\thdr->b_l1hdr.b_pabd = abd_get_from_buf(buf->b_data, arc_buf_size(buf));\n\tabd_take_ownership_of_buf(hdr->b_l1hdr.b_pabd,\n\t    HDR_ISTYPE_METADATA(hdr));\n\tarc_hdr_set_flags(hdr, ARC_FLAG_SHARED_DATA);\n\tbuf->b_flags |= ARC_BUF_FLAG_SHARED;\n\n\t \n\tARCSTAT_INCR(arcstat_compressed_size, arc_hdr_size(hdr));\n\tARCSTAT_INCR(arcstat_uncompressed_size, HDR_GET_LSIZE(hdr));\n\tARCSTAT_INCR(arcstat_overhead_size, -arc_buf_size(buf));\n}\n\nstatic void\narc_unshare_buf(arc_buf_hdr_t *hdr, arc_buf_t *buf)\n{\n\tASSERT(arc_buf_is_shared(buf));\n\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\n\t \n\tzfs_refcount_transfer_ownership_many(\n\t    &hdr->b_l1hdr.b_state->arcs_size[arc_buf_type(hdr)],\n\t    arc_hdr_size(hdr), hdr, buf);\n\tarc_hdr_clear_flags(hdr, ARC_FLAG_SHARED_DATA);\n\tabd_release_ownership_of_buf(hdr->b_l1hdr.b_pabd);\n\tabd_free(hdr->b_l1hdr.b_pabd);\n\thdr->b_l1hdr.b_pabd = NULL;\n\tbuf->b_flags &= ~ARC_BUF_FLAG_SHARED;\n\n\t \n\tARCSTAT_INCR(arcstat_compressed_size, -arc_hdr_size(hdr));\n\tARCSTAT_INCR(arcstat_uncompressed_size, -HDR_GET_LSIZE(hdr));\n\tARCSTAT_INCR(arcstat_overhead_size, arc_buf_size(buf));\n}\n\n \nstatic arc_buf_t *\narc_buf_remove(arc_buf_hdr_t *hdr, arc_buf_t *buf)\n{\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\n\tarc_buf_t **bufp = &hdr->b_l1hdr.b_buf;\n\tarc_buf_t *lastbuf = NULL;\n\n\t \n\twhile (*bufp != NULL) {\n\t\tif (*bufp == buf)\n\t\t\t*bufp = buf->b_next;\n\n\t\t \n\t\tif (*bufp != NULL) {\n\t\t\tlastbuf = *bufp;\n\t\t\tbufp = &(*bufp)->b_next;\n\t\t}\n\t}\n\tbuf->b_next = NULL;\n\tASSERT3P(lastbuf, !=, buf);\n\tIMPLY(lastbuf != NULL, ARC_BUF_LAST(lastbuf));\n\n\treturn (lastbuf);\n}\n\n \nstatic void\narc_buf_destroy_impl(arc_buf_t *buf)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\t \n\tif (buf->b_data != NULL) {\n\t\t \n\t\tASSERT(HDR_EMPTY_OR_LOCKED(hdr));\n\n\t\tarc_cksum_verify(buf);\n\t\tarc_buf_unwatch(buf);\n\n\t\tif (ARC_BUF_SHARED(buf)) {\n\t\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_SHARED_DATA);\n\t\t} else {\n\t\t\tASSERT(!arc_buf_is_shared(buf));\n\t\t\tuint64_t size = arc_buf_size(buf);\n\t\t\tarc_free_data_buf(hdr, buf->b_data, size, buf);\n\t\t\tARCSTAT_INCR(arcstat_overhead_size, -size);\n\t\t}\n\t\tbuf->b_data = NULL;\n\n\t\t \n\t\tif (ARC_BUF_ENCRYPTED(buf) && HDR_HAS_RABD(hdr) &&\n\t\t    hdr->b_l1hdr.b_pabd != NULL && !HDR_IO_IN_PROGRESS(hdr)) {\n\t\t\tarc_buf_t *b;\n\t\t\tfor (b = hdr->b_l1hdr.b_buf; b; b = b->b_next) {\n\t\t\t\tif (b != buf && ARC_BUF_ENCRYPTED(b))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (b == NULL)\n\t\t\t\tarc_hdr_free_abd(hdr, B_TRUE);\n\t\t}\n\t}\n\n\tarc_buf_t *lastbuf = arc_buf_remove(hdr, buf);\n\n\tif (ARC_BUF_SHARED(buf) && !ARC_BUF_COMPRESSED(buf)) {\n\t\t \n\t\tif (lastbuf != NULL && !ARC_BUF_ENCRYPTED(lastbuf)) {\n\t\t\t \n\t\t\tASSERT(!arc_buf_is_shared(lastbuf));\n\t\t\t \n\t\t\tASSERT(!ARC_BUF_COMPRESSED(lastbuf));\n\n\t\t\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\t\t\tarc_hdr_free_abd(hdr, B_FALSE);\n\n\t\t\t \n\t\t\tarc_share_buf(hdr, lastbuf);\n\t\t}\n\t} else if (HDR_SHARED_DATA(hdr)) {\n\t\t \n\t\tASSERT3P(lastbuf, !=, NULL);\n\t\tASSERT(arc_buf_is_shared(lastbuf) ||\n\t\t    arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF);\n\t}\n\n\t \n\tif (!arc_hdr_has_uncompressed_buf(hdr)) {\n\t\tarc_cksum_free(hdr);\n\t}\n\n\t \n\tbuf->b_hdr = NULL;\n\tkmem_cache_free(buf_cache, buf);\n}\n\nstatic void\narc_hdr_alloc_abd(arc_buf_hdr_t *hdr, int alloc_flags)\n{\n\tuint64_t size;\n\tboolean_t alloc_rdata = ((alloc_flags & ARC_HDR_ALLOC_RDATA) != 0);\n\n\tASSERT3U(HDR_GET_LSIZE(hdr), >, 0);\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT(!HDR_SHARED_DATA(hdr) || alloc_rdata);\n\tIMPLY(alloc_rdata, HDR_PROTECTED(hdr));\n\n\tif (alloc_rdata) {\n\t\tsize = HDR_GET_PSIZE(hdr);\n\t\tASSERT3P(hdr->b_crypt_hdr.b_rabd, ==, NULL);\n\t\thdr->b_crypt_hdr.b_rabd = arc_get_data_abd(hdr, size, hdr,\n\t\t    alloc_flags);\n\t\tASSERT3P(hdr->b_crypt_hdr.b_rabd, !=, NULL);\n\t\tARCSTAT_INCR(arcstat_raw_size, size);\n\t} else {\n\t\tsize = arc_hdr_size(hdr);\n\t\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\thdr->b_l1hdr.b_pabd = arc_get_data_abd(hdr, size, hdr,\n\t\t    alloc_flags);\n\t\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\t}\n\n\tARCSTAT_INCR(arcstat_compressed_size, size);\n\tARCSTAT_INCR(arcstat_uncompressed_size, HDR_GET_LSIZE(hdr));\n}\n\nstatic void\narc_hdr_free_abd(arc_buf_hdr_t *hdr, boolean_t free_rdata)\n{\n\tuint64_t size = (free_rdata) ? HDR_GET_PSIZE(hdr) : arc_hdr_size(hdr);\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT(hdr->b_l1hdr.b_pabd != NULL || HDR_HAS_RABD(hdr));\n\tIMPLY(free_rdata, HDR_HAS_RABD(hdr));\n\n\t \n\tif (HDR_L2_WRITING(hdr)) {\n\t\tarc_hdr_free_on_write(hdr, free_rdata);\n\t\tARCSTAT_BUMP(arcstat_l2_free_on_write);\n\t} else if (free_rdata) {\n\t\tarc_free_data_abd(hdr, hdr->b_crypt_hdr.b_rabd, size, hdr);\n\t} else {\n\t\tarc_free_data_abd(hdr, hdr->b_l1hdr.b_pabd, size, hdr);\n\t}\n\n\tif (free_rdata) {\n\t\thdr->b_crypt_hdr.b_rabd = NULL;\n\t\tARCSTAT_INCR(arcstat_raw_size, -size);\n\t} else {\n\t\thdr->b_l1hdr.b_pabd = NULL;\n\t}\n\n\tif (hdr->b_l1hdr.b_pabd == NULL && !HDR_HAS_RABD(hdr))\n\t\thdr->b_l1hdr.b_byteswap = DMU_BSWAP_NUMFUNCS;\n\n\tARCSTAT_INCR(arcstat_compressed_size, -size);\n\tARCSTAT_INCR(arcstat_uncompressed_size, -HDR_GET_LSIZE(hdr));\n}\n\n \nstatic arc_buf_hdr_t *\narc_hdr_alloc(uint64_t spa, int32_t psize, int32_t lsize,\n    boolean_t protected, enum zio_compress compression_type, uint8_t complevel,\n    arc_buf_contents_t type)\n{\n\tarc_buf_hdr_t *hdr;\n\n\tVERIFY(type == ARC_BUFC_DATA || type == ARC_BUFC_METADATA);\n\thdr = kmem_cache_alloc(hdr_full_cache, KM_PUSHPAGE);\n\n\tASSERT(HDR_EMPTY(hdr));\n#ifdef ZFS_DEBUG\n\tASSERT3P(hdr->b_l1hdr.b_freeze_cksum, ==, NULL);\n#endif\n\tHDR_SET_PSIZE(hdr, psize);\n\tHDR_SET_LSIZE(hdr, lsize);\n\thdr->b_spa = spa;\n\thdr->b_type = type;\n\thdr->b_flags = 0;\n\tarc_hdr_set_flags(hdr, arc_bufc_to_flags(type) | ARC_FLAG_HAS_L1HDR);\n\tarc_hdr_set_compress(hdr, compression_type);\n\thdr->b_complevel = complevel;\n\tif (protected)\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PROTECTED);\n\n\thdr->b_l1hdr.b_state = arc_anon;\n\thdr->b_l1hdr.b_arc_access = 0;\n\thdr->b_l1hdr.b_mru_hits = 0;\n\thdr->b_l1hdr.b_mru_ghost_hits = 0;\n\thdr->b_l1hdr.b_mfu_hits = 0;\n\thdr->b_l1hdr.b_mfu_ghost_hits = 0;\n\thdr->b_l1hdr.b_buf = NULL;\n\n\tASSERT(zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt));\n\n\treturn (hdr);\n}\n\n \nstatic arc_buf_hdr_t *\narc_hdr_realloc(arc_buf_hdr_t *hdr, kmem_cache_t *old, kmem_cache_t *new)\n{\n\tASSERT(HDR_HAS_L2HDR(hdr));\n\n\tarc_buf_hdr_t *nhdr;\n\tl2arc_dev_t *dev = hdr->b_l2hdr.b_dev;\n\n\tASSERT((old == hdr_full_cache && new == hdr_l2only_cache) ||\n\t    (old == hdr_l2only_cache && new == hdr_full_cache));\n\n\tnhdr = kmem_cache_alloc(new, KM_PUSHPAGE);\n\n\tASSERT(MUTEX_HELD(HDR_LOCK(hdr)));\n\tbuf_hash_remove(hdr);\n\n\tmemcpy(nhdr, hdr, HDR_L2ONLY_SIZE);\n\n\tif (new == hdr_full_cache) {\n\t\tarc_hdr_set_flags(nhdr, ARC_FLAG_HAS_L1HDR);\n\t\t \n\t\tnhdr->b_l1hdr.b_state = arc_l2c_only;\n\n\t\t \n\t\tASSERT3P(nhdr->b_l1hdr.b_pabd, ==, NULL);\n\t\tASSERT(!HDR_HAS_RABD(hdr));\n\t} else {\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, NULL);\n#ifdef ZFS_DEBUG\n\t\tASSERT3P(hdr->b_l1hdr.b_freeze_cksum, ==, NULL);\n#endif\n\n\t\t \n\t\tASSERT(!multilist_link_active(&hdr->b_l1hdr.b_arc_node));\n\n\t\t \n\t\tVERIFY(!HDR_L2_WRITING(hdr));\n\t\tVERIFY3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\tASSERT(!HDR_HAS_RABD(hdr));\n\n\t\tarc_hdr_clear_flags(nhdr, ARC_FLAG_HAS_L1HDR);\n\t}\n\t \n\t(void) buf_hash_insert(nhdr, NULL);\n\n\tASSERT(list_link_active(&hdr->b_l2hdr.b_l2node));\n\n\tmutex_enter(&dev->l2ad_mtx);\n\n\t \n\tlist_insert_after(&dev->l2ad_buflist, hdr, nhdr);\n\tlist_remove(&dev->l2ad_buflist, hdr);\n\n\tmutex_exit(&dev->l2ad_mtx);\n\n\t \n\n\t(void) zfs_refcount_remove_many(&dev->l2ad_alloc,\n\t    arc_hdr_size(hdr), hdr);\n\t(void) zfs_refcount_add_many(&dev->l2ad_alloc,\n\t    arc_hdr_size(nhdr), nhdr);\n\n\tbuf_discard_identity(hdr);\n\tkmem_cache_free(old, hdr);\n\n\treturn (nhdr);\n}\n\n \nvoid\narc_convert_to_raw(arc_buf_t *buf, uint64_t dsobj, boolean_t byteorder,\n    dmu_object_type_t ot, const uint8_t *salt, const uint8_t *iv,\n    const uint8_t *mac)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT(ot == DMU_OT_DNODE || ot == DMU_OT_OBJSET);\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT3P(hdr->b_l1hdr.b_state, ==, arc_anon);\n\n\tbuf->b_flags |= (ARC_BUF_FLAG_COMPRESSED | ARC_BUF_FLAG_ENCRYPTED);\n\tarc_hdr_set_flags(hdr, ARC_FLAG_PROTECTED);\n\thdr->b_crypt_hdr.b_dsobj = dsobj;\n\thdr->b_crypt_hdr.b_ot = ot;\n\thdr->b_l1hdr.b_byteswap = (byteorder == ZFS_HOST_BYTEORDER) ?\n\t    DMU_BSWAP_NUMFUNCS : DMU_OT_BYTESWAP(ot);\n\tif (!arc_hdr_has_uncompressed_buf(hdr))\n\t\tarc_cksum_free(hdr);\n\n\tif (salt != NULL)\n\t\tmemcpy(hdr->b_crypt_hdr.b_salt, salt, ZIO_DATA_SALT_LEN);\n\tif (iv != NULL)\n\t\tmemcpy(hdr->b_crypt_hdr.b_iv, iv, ZIO_DATA_IV_LEN);\n\tif (mac != NULL)\n\t\tmemcpy(hdr->b_crypt_hdr.b_mac, mac, ZIO_DATA_MAC_LEN);\n}\n\n \narc_buf_t *\narc_alloc_buf(spa_t *spa, const void *tag, arc_buf_contents_t type,\n    int32_t size)\n{\n\tarc_buf_hdr_t *hdr = arc_hdr_alloc(spa_load_guid(spa), size, size,\n\t    B_FALSE, ZIO_COMPRESS_OFF, 0, type);\n\n\tarc_buf_t *buf = NULL;\n\tVERIFY0(arc_buf_alloc_impl(hdr, spa, NULL, tag, B_FALSE, B_FALSE,\n\t    B_FALSE, B_FALSE, &buf));\n\tarc_buf_thaw(buf);\n\n\treturn (buf);\n}\n\n \narc_buf_t *\narc_alloc_compressed_buf(spa_t *spa, const void *tag, uint64_t psize,\n    uint64_t lsize, enum zio_compress compression_type, uint8_t complevel)\n{\n\tASSERT3U(lsize, >, 0);\n\tASSERT3U(lsize, >=, psize);\n\tASSERT3U(compression_type, >, ZIO_COMPRESS_OFF);\n\tASSERT3U(compression_type, <, ZIO_COMPRESS_FUNCTIONS);\n\n\tarc_buf_hdr_t *hdr = arc_hdr_alloc(spa_load_guid(spa), psize, lsize,\n\t    B_FALSE, compression_type, complevel, ARC_BUFC_DATA);\n\n\tarc_buf_t *buf = NULL;\n\tVERIFY0(arc_buf_alloc_impl(hdr, spa, NULL, tag, B_FALSE,\n\t    B_TRUE, B_FALSE, B_FALSE, &buf));\n\tarc_buf_thaw(buf);\n\n\t \n\tarc_share_buf(hdr, buf);\n\n\treturn (buf);\n}\n\narc_buf_t *\narc_alloc_raw_buf(spa_t *spa, const void *tag, uint64_t dsobj,\n    boolean_t byteorder, const uint8_t *salt, const uint8_t *iv,\n    const uint8_t *mac, dmu_object_type_t ot, uint64_t psize, uint64_t lsize,\n    enum zio_compress compression_type, uint8_t complevel)\n{\n\tarc_buf_hdr_t *hdr;\n\tarc_buf_t *buf;\n\tarc_buf_contents_t type = DMU_OT_IS_METADATA(ot) ?\n\t    ARC_BUFC_METADATA : ARC_BUFC_DATA;\n\n\tASSERT3U(lsize, >, 0);\n\tASSERT3U(lsize, >=, psize);\n\tASSERT3U(compression_type, >=, ZIO_COMPRESS_OFF);\n\tASSERT3U(compression_type, <, ZIO_COMPRESS_FUNCTIONS);\n\n\thdr = arc_hdr_alloc(spa_load_guid(spa), psize, lsize, B_TRUE,\n\t    compression_type, complevel, type);\n\n\thdr->b_crypt_hdr.b_dsobj = dsobj;\n\thdr->b_crypt_hdr.b_ot = ot;\n\thdr->b_l1hdr.b_byteswap = (byteorder == ZFS_HOST_BYTEORDER) ?\n\t    DMU_BSWAP_NUMFUNCS : DMU_OT_BYTESWAP(ot);\n\tmemcpy(hdr->b_crypt_hdr.b_salt, salt, ZIO_DATA_SALT_LEN);\n\tmemcpy(hdr->b_crypt_hdr.b_iv, iv, ZIO_DATA_IV_LEN);\n\tmemcpy(hdr->b_crypt_hdr.b_mac, mac, ZIO_DATA_MAC_LEN);\n\n\t \n\tbuf = NULL;\n\tVERIFY0(arc_buf_alloc_impl(hdr, spa, NULL, tag, B_TRUE, B_TRUE,\n\t    B_FALSE, B_FALSE, &buf));\n\tarc_buf_thaw(buf);\n\n\treturn (buf);\n}\n\nstatic void\nl2arc_hdr_arcstats_update(arc_buf_hdr_t *hdr, boolean_t incr,\n    boolean_t state_only)\n{\n\tl2arc_buf_hdr_t *l2hdr = &hdr->b_l2hdr;\n\tl2arc_dev_t *dev = l2hdr->b_dev;\n\tuint64_t lsize = HDR_GET_LSIZE(hdr);\n\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\tuint64_t asize = vdev_psize_to_asize(dev->l2ad_vdev, psize);\n\tarc_buf_contents_t type = hdr->b_type;\n\tint64_t lsize_s;\n\tint64_t psize_s;\n\tint64_t asize_s;\n\n\tif (incr) {\n\t\tlsize_s = lsize;\n\t\tpsize_s = psize;\n\t\tasize_s = asize;\n\t} else {\n\t\tlsize_s = -lsize;\n\t\tpsize_s = -psize;\n\t\tasize_s = -asize;\n\t}\n\n\t \n\tif (HDR_PREFETCH(hdr)) {\n\t\tARCSTAT_INCR(arcstat_l2_prefetch_asize, asize_s);\n\t} else {\n\t\t \n\t\tswitch (hdr->b_l2hdr.b_arcs_state) {\n\t\t\tcase ARC_STATE_MRU_GHOST:\n\t\t\tcase ARC_STATE_MRU:\n\t\t\t\tARCSTAT_INCR(arcstat_l2_mru_asize, asize_s);\n\t\t\t\tbreak;\n\t\t\tcase ARC_STATE_MFU_GHOST:\n\t\t\tcase ARC_STATE_MFU:\n\t\t\t\tARCSTAT_INCR(arcstat_l2_mfu_asize, asize_s);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (state_only)\n\t\treturn;\n\n\tARCSTAT_INCR(arcstat_l2_psize, psize_s);\n\tARCSTAT_INCR(arcstat_l2_lsize, lsize_s);\n\n\tswitch (type) {\n\t\tcase ARC_BUFC_DATA:\n\t\t\tARCSTAT_INCR(arcstat_l2_bufc_data_asize, asize_s);\n\t\t\tbreak;\n\t\tcase ARC_BUFC_METADATA:\n\t\t\tARCSTAT_INCR(arcstat_l2_bufc_metadata_asize, asize_s);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t}\n}\n\n\nstatic void\narc_hdr_l2hdr_destroy(arc_buf_hdr_t *hdr)\n{\n\tl2arc_buf_hdr_t *l2hdr = &hdr->b_l2hdr;\n\tl2arc_dev_t *dev = l2hdr->b_dev;\n\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\tuint64_t asize = vdev_psize_to_asize(dev->l2ad_vdev, psize);\n\n\tASSERT(MUTEX_HELD(&dev->l2ad_mtx));\n\tASSERT(HDR_HAS_L2HDR(hdr));\n\n\tlist_remove(&dev->l2ad_buflist, hdr);\n\n\tl2arc_hdr_arcstats_decrement(hdr);\n\tvdev_space_update(dev->l2ad_vdev, -asize, 0, 0);\n\n\t(void) zfs_refcount_remove_many(&dev->l2ad_alloc, arc_hdr_size(hdr),\n\t    hdr);\n\tarc_hdr_clear_flags(hdr, ARC_FLAG_HAS_L2HDR);\n}\n\nstatic void\narc_hdr_destroy(arc_buf_hdr_t *hdr)\n{\n\tif (HDR_HAS_L1HDR(hdr)) {\n\t\tASSERT(zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt));\n\t\tASSERT3P(hdr->b_l1hdr.b_state, ==, arc_anon);\n\t}\n\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\tASSERT(!HDR_IN_HASH_TABLE(hdr));\n\n\tif (HDR_HAS_L2HDR(hdr)) {\n\t\tl2arc_dev_t *dev = hdr->b_l2hdr.b_dev;\n\t\tboolean_t buflist_held = MUTEX_HELD(&dev->l2ad_mtx);\n\n\t\tif (!buflist_held)\n\t\t\tmutex_enter(&dev->l2ad_mtx);\n\n\t\t \n\t\tif (HDR_HAS_L2HDR(hdr)) {\n\n\t\t\tif (!HDR_EMPTY(hdr))\n\t\t\t\tbuf_discard_identity(hdr);\n\n\t\t\tarc_hdr_l2hdr_destroy(hdr);\n\t\t}\n\n\t\tif (!buflist_held)\n\t\t\tmutex_exit(&dev->l2ad_mtx);\n\t}\n\n\t \n\tif (!HDR_EMPTY(hdr))\n\t\tbuf_discard_identity(hdr);\n\n\tif (HDR_HAS_L1HDR(hdr)) {\n\t\tarc_cksum_free(hdr);\n\n\t\twhile (hdr->b_l1hdr.b_buf != NULL)\n\t\t\tarc_buf_destroy_impl(hdr->b_l1hdr.b_buf);\n\n\t\tif (hdr->b_l1hdr.b_pabd != NULL)\n\t\t\tarc_hdr_free_abd(hdr, B_FALSE);\n\n\t\tif (HDR_HAS_RABD(hdr))\n\t\t\tarc_hdr_free_abd(hdr, B_TRUE);\n\t}\n\n\tASSERT3P(hdr->b_hash_next, ==, NULL);\n\tif (HDR_HAS_L1HDR(hdr)) {\n\t\tASSERT(!multilist_link_active(&hdr->b_l1hdr.b_arc_node));\n\t\tASSERT3P(hdr->b_l1hdr.b_acb, ==, NULL);\n#ifdef ZFS_DEBUG\n\t\tASSERT3P(hdr->b_l1hdr.b_freeze_cksum, ==, NULL);\n#endif\n\t\tkmem_cache_free(hdr_full_cache, hdr);\n\t} else {\n\t\tkmem_cache_free(hdr_l2only_cache, hdr);\n\t}\n}\n\nvoid\narc_buf_destroy(arc_buf_t *buf, const void *tag)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tif (hdr->b_l1hdr.b_state == arc_anon) {\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, buf);\n\t\tASSERT(ARC_BUF_LAST(buf));\n\t\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\t\tVERIFY0(remove_reference(hdr, tag));\n\t\treturn;\n\t}\n\n\tkmutex_t *hash_lock = HDR_LOCK(hdr);\n\tmutex_enter(hash_lock);\n\n\tASSERT3P(hdr, ==, buf->b_hdr);\n\tASSERT3P(hdr->b_l1hdr.b_buf, !=, NULL);\n\tASSERT3P(hash_lock, ==, HDR_LOCK(hdr));\n\tASSERT3P(hdr->b_l1hdr.b_state, !=, arc_anon);\n\tASSERT3P(buf->b_data, !=, NULL);\n\n\tarc_buf_destroy_impl(buf);\n\t(void) remove_reference(hdr, tag);\n\tmutex_exit(hash_lock);\n}\n\n \nstatic int64_t\narc_evict_hdr(arc_buf_hdr_t *hdr, uint64_t *real_evicted)\n{\n\tarc_state_t *evicted_state, *state;\n\tint64_t bytes_evicted = 0;\n\tuint_t min_lifetime = HDR_PRESCIENT_PREFETCH(hdr) ?\n\t    arc_min_prescient_prefetch_ms : arc_min_prefetch_ms;\n\n\tASSERT(MUTEX_HELD(HDR_LOCK(hdr)));\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\tASSERT3P(hdr->b_l1hdr.b_buf, ==, NULL);\n\tASSERT0(zfs_refcount_count(&hdr->b_l1hdr.b_refcnt));\n\n\t*real_evicted = 0;\n\tstate = hdr->b_l1hdr.b_state;\n\tif (GHOST_STATE(state)) {\n\n\t\t \n\t\tif (HDR_HAS_L2HDR(hdr) && HDR_L2_WRITING(hdr)) {\n\t\t\tARCSTAT_BUMP(arcstat_evict_l2_skip);\n\t\t\treturn (bytes_evicted);\n\t\t}\n\n\t\tARCSTAT_BUMP(arcstat_deleted);\n\t\tbytes_evicted += HDR_GET_LSIZE(hdr);\n\n\t\tDTRACE_PROBE1(arc__delete, arc_buf_hdr_t *, hdr);\n\n\t\tif (HDR_HAS_L2HDR(hdr)) {\n\t\t\tASSERT(hdr->b_l1hdr.b_pabd == NULL);\n\t\t\tASSERT(!HDR_HAS_RABD(hdr));\n\t\t\t \n\t\t\tarc_change_state(arc_l2c_only, hdr);\n\t\t\t \n\t\t\t(void) arc_hdr_realloc(hdr, hdr_full_cache,\n\t\t\t    hdr_l2only_cache);\n\t\t\t*real_evicted += HDR_FULL_SIZE - HDR_L2ONLY_SIZE;\n\t\t} else {\n\t\t\tarc_change_state(arc_anon, hdr);\n\t\t\tarc_hdr_destroy(hdr);\n\t\t\t*real_evicted += HDR_FULL_SIZE;\n\t\t}\n\t\treturn (bytes_evicted);\n\t}\n\n\tASSERT(state == arc_mru || state == arc_mfu || state == arc_uncached);\n\tevicted_state = (state == arc_uncached) ? arc_anon :\n\t    ((state == arc_mru) ? arc_mru_ghost : arc_mfu_ghost);\n\n\t \n\tif ((hdr->b_flags & (ARC_FLAG_PREFETCH | ARC_FLAG_INDIRECT)) &&\n\t    ddi_get_lbolt() - hdr->b_l1hdr.b_arc_access <\n\t    MSEC_TO_TICK(min_lifetime)) {\n\t\tARCSTAT_BUMP(arcstat_evict_skip);\n\t\treturn (bytes_evicted);\n\t}\n\n\tif (HDR_HAS_L2HDR(hdr)) {\n\t\tARCSTAT_INCR(arcstat_evict_l2_cached, HDR_GET_LSIZE(hdr));\n\t} else {\n\t\tif (l2arc_write_eligible(hdr->b_spa, hdr)) {\n\t\t\tARCSTAT_INCR(arcstat_evict_l2_eligible,\n\t\t\t    HDR_GET_LSIZE(hdr));\n\n\t\t\tswitch (state->arcs_state) {\n\t\t\t\tcase ARC_STATE_MRU:\n\t\t\t\t\tARCSTAT_INCR(\n\t\t\t\t\t    arcstat_evict_l2_eligible_mru,\n\t\t\t\t\t    HDR_GET_LSIZE(hdr));\n\t\t\t\t\tbreak;\n\t\t\t\tcase ARC_STATE_MFU:\n\t\t\t\t\tARCSTAT_INCR(\n\t\t\t\t\t    arcstat_evict_l2_eligible_mfu,\n\t\t\t\t\t    HDR_GET_LSIZE(hdr));\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tARCSTAT_INCR(arcstat_evict_l2_ineligible,\n\t\t\t    HDR_GET_LSIZE(hdr));\n\t\t}\n\t}\n\n\tbytes_evicted += arc_hdr_size(hdr);\n\t*real_evicted += arc_hdr_size(hdr);\n\n\t \n\tif (hdr->b_l1hdr.b_pabd != NULL)\n\t\tarc_hdr_free_abd(hdr, B_FALSE);\n\n\tif (HDR_HAS_RABD(hdr))\n\t\tarc_hdr_free_abd(hdr, B_TRUE);\n\n\tarc_change_state(evicted_state, hdr);\n\tDTRACE_PROBE1(arc__evict, arc_buf_hdr_t *, hdr);\n\tif (evicted_state == arc_anon) {\n\t\tarc_hdr_destroy(hdr);\n\t\t*real_evicted += HDR_FULL_SIZE;\n\t} else {\n\t\tASSERT(HDR_IN_HASH_TABLE(hdr));\n\t}\n\n\treturn (bytes_evicted);\n}\n\nstatic void\narc_set_need_free(void)\n{\n\tASSERT(MUTEX_HELD(&arc_evict_lock));\n\tint64_t remaining = arc_free_memory() - arc_sys_free / 2;\n\tarc_evict_waiter_t *aw = list_tail(&arc_evict_waiters);\n\tif (aw == NULL) {\n\t\tarc_need_free = MAX(-remaining, 0);\n\t} else {\n\t\tarc_need_free =\n\t\t    MAX(-remaining, (int64_t)(aw->aew_count - arc_evict_count));\n\t}\n}\n\nstatic uint64_t\narc_evict_state_impl(multilist_t *ml, int idx, arc_buf_hdr_t *marker,\n    uint64_t spa, uint64_t bytes)\n{\n\tmultilist_sublist_t *mls;\n\tuint64_t bytes_evicted = 0, real_evicted = 0;\n\tarc_buf_hdr_t *hdr;\n\tkmutex_t *hash_lock;\n\tuint_t evict_count = zfs_arc_evict_batch_limit;\n\n\tASSERT3P(marker, !=, NULL);\n\n\tmls = multilist_sublist_lock(ml, idx);\n\n\tfor (hdr = multilist_sublist_prev(mls, marker); likely(hdr != NULL);\n\t    hdr = multilist_sublist_prev(mls, marker)) {\n\t\tif ((evict_count == 0) || (bytes_evicted >= bytes))\n\t\t\tbreak;\n\n\t\t \n\t\tmultilist_sublist_move_forward(mls, marker);\n\n\t\t \n\t\tif (hdr->b_spa == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (spa != 0 && hdr->b_spa != spa) {\n\t\t\tARCSTAT_BUMP(arcstat_evict_skip);\n\t\t\tcontinue;\n\t\t}\n\n\t\thash_lock = HDR_LOCK(hdr);\n\n\t\t \n\t\tASSERT(!MUTEX_HELD(hash_lock));\n\n\t\tif (mutex_tryenter(hash_lock)) {\n\t\t\tuint64_t revicted;\n\t\t\tuint64_t evicted = arc_evict_hdr(hdr, &revicted);\n\t\t\tmutex_exit(hash_lock);\n\n\t\t\tbytes_evicted += evicted;\n\t\t\treal_evicted += revicted;\n\n\t\t\t \n\t\t\tif (evicted != 0)\n\t\t\t\tevict_count--;\n\n\t\t} else {\n\t\t\tARCSTAT_BUMP(arcstat_mutex_miss);\n\t\t}\n\t}\n\n\tmultilist_sublist_unlock(mls);\n\n\t \n\tmutex_enter(&arc_evict_lock);\n\tarc_evict_count += real_evicted;\n\n\tif (arc_free_memory() > arc_sys_free / 2) {\n\t\tarc_evict_waiter_t *aw;\n\t\twhile ((aw = list_head(&arc_evict_waiters)) != NULL &&\n\t\t    aw->aew_count <= arc_evict_count) {\n\t\t\tlist_remove(&arc_evict_waiters, aw);\n\t\t\tcv_broadcast(&aw->aew_cv);\n\t\t}\n\t}\n\tarc_set_need_free();\n\tmutex_exit(&arc_evict_lock);\n\n\t \n\tkpreempt(KPREEMPT_SYNC);\n\n\treturn (bytes_evicted);\n}\n\n \nstatic arc_buf_hdr_t **\narc_state_alloc_markers(int count)\n{\n\tarc_buf_hdr_t **markers;\n\n\tmarkers = kmem_zalloc(sizeof (*markers) * count, KM_SLEEP);\n\tfor (int i = 0; i < count; i++) {\n\t\tmarkers[i] = kmem_cache_alloc(hdr_full_cache, KM_SLEEP);\n\n\t\t \n\t\tmarkers[i]->b_spa = 0;\n\n\t}\n\treturn (markers);\n}\n\nstatic void\narc_state_free_markers(arc_buf_hdr_t **markers, int count)\n{\n\tfor (int i = 0; i < count; i++)\n\t\tkmem_cache_free(hdr_full_cache, markers[i]);\n\tkmem_free(markers, sizeof (*markers) * count);\n}\n\n \nstatic uint64_t\narc_evict_state(arc_state_t *state, arc_buf_contents_t type, uint64_t spa,\n    uint64_t bytes)\n{\n\tuint64_t total_evicted = 0;\n\tmultilist_t *ml = &state->arcs_list[type];\n\tint num_sublists;\n\tarc_buf_hdr_t **markers;\n\n\tnum_sublists = multilist_get_num_sublists(ml);\n\n\t \n\tif (zthr_iscurthread(arc_evict_zthr)) {\n\t\tmarkers = arc_state_evict_markers;\n\t\tASSERT3S(num_sublists, <=, arc_state_evict_marker_count);\n\t} else {\n\t\tmarkers = arc_state_alloc_markers(num_sublists);\n\t}\n\tfor (int i = 0; i < num_sublists; i++) {\n\t\tmultilist_sublist_t *mls;\n\n\t\tmls = multilist_sublist_lock(ml, i);\n\t\tmultilist_sublist_insert_tail(mls, markers[i]);\n\t\tmultilist_sublist_unlock(mls);\n\t}\n\n\t \n\twhile (total_evicted < bytes) {\n\t\tint sublist_idx = multilist_get_random_index(ml);\n\t\tuint64_t scan_evicted = 0;\n\n\t\t \n\t\tfor (int i = 0; i < num_sublists; i++) {\n\t\t\tuint64_t bytes_remaining;\n\t\t\tuint64_t bytes_evicted;\n\n\t\t\tif (total_evicted < bytes)\n\t\t\t\tbytes_remaining = bytes - total_evicted;\n\t\t\telse\n\t\t\t\tbreak;\n\n\t\t\tbytes_evicted = arc_evict_state_impl(ml, sublist_idx,\n\t\t\t    markers[sublist_idx], spa, bytes_remaining);\n\n\t\t\tscan_evicted += bytes_evicted;\n\t\t\ttotal_evicted += bytes_evicted;\n\n\t\t\t \n\t\t\tif (++sublist_idx >= num_sublists)\n\t\t\t\tsublist_idx = 0;\n\t\t}\n\n\t\t \n\t\tif (scan_evicted == 0) {\n\t\t\t \n\t\t\tASSERT3S(bytes, !=, 0);\n\n\t\t\t \n\t\t\tif (bytes != ARC_EVICT_ALL) {\n\t\t\t\tASSERT3S(total_evicted, <, bytes);\n\t\t\t\tARCSTAT_BUMP(arcstat_evict_not_enough);\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (int i = 0; i < num_sublists; i++) {\n\t\tmultilist_sublist_t *mls = multilist_sublist_lock(ml, i);\n\t\tmultilist_sublist_remove(mls, markers[i]);\n\t\tmultilist_sublist_unlock(mls);\n\t}\n\tif (markers != arc_state_evict_markers)\n\t\tarc_state_free_markers(markers, num_sublists);\n\n\treturn (total_evicted);\n}\n\n \nstatic uint64_t\narc_flush_state(arc_state_t *state, uint64_t spa, arc_buf_contents_t type,\n    boolean_t retry)\n{\n\tuint64_t evicted = 0;\n\n\twhile (zfs_refcount_count(&state->arcs_esize[type]) != 0) {\n\t\tevicted += arc_evict_state(state, type, spa, ARC_EVICT_ALL);\n\n\t\tif (!retry)\n\t\t\tbreak;\n\t}\n\n\treturn (evicted);\n}\n\n \nstatic uint64_t\narc_evict_impl(arc_state_t *state, arc_buf_contents_t type, int64_t bytes)\n{\n\tuint64_t delta;\n\n\tif (bytes > 0 && zfs_refcount_count(&state->arcs_esize[type]) > 0) {\n\t\tdelta = MIN(zfs_refcount_count(&state->arcs_esize[type]),\n\t\t    bytes);\n\t\treturn (arc_evict_state(state, type, 0, delta));\n\t}\n\n\treturn (0);\n}\n\n \nstatic uint64_t\narc_evict_adj(uint64_t frac, uint64_t total, uint64_t up, uint64_t down,\n    uint_t balance)\n{\n\tif (total < 8 || up + down == 0)\n\t\treturn (frac);\n\n\t \n\tif (up + down >= total / 4) {\n\t\tuint64_t scale = (up + down) / (total / 8);\n\t\tup /= scale;\n\t\tdown /= scale;\n\t}\n\n\t \n\tint s = highbit64(total);\n\ts = MIN(64 - s, 32);\n\n\tuint64_t ofrac = (1ULL << 32) - frac;\n\n\tif (frac >= 4 * ofrac)\n\t\tup /= frac / (2 * ofrac + 1);\n\tup = (up << s) / (total >> (32 - s));\n\tif (ofrac >= 4 * frac)\n\t\tdown /= ofrac / (2 * frac + 1);\n\tdown = (down << s) / (total >> (32 - s));\n\tdown = down * 100 / balance;\n\n\treturn (frac + up - down);\n}\n\n \nstatic uint64_t\narc_evict(void)\n{\n\tuint64_t asize, bytes, total_evicted = 0;\n\tint64_t e, mrud, mrum, mfud, mfum, w;\n\tstatic uint64_t ogrd, ogrm, ogfd, ogfm;\n\tstatic uint64_t gsrd, gsrm, gsfd, gsfm;\n\tuint64_t ngrd, ngrm, ngfd, ngfm;\n\n\t \n\tmrud = zfs_refcount_count(&arc_mru->arcs_size[ARC_BUFC_DATA]) +\n\t    zfs_refcount_count(&arc_anon->arcs_size[ARC_BUFC_DATA]);\n\tmrum = zfs_refcount_count(&arc_mru->arcs_size[ARC_BUFC_METADATA]) +\n\t    zfs_refcount_count(&arc_anon->arcs_size[ARC_BUFC_METADATA]);\n\tmfud = zfs_refcount_count(&arc_mfu->arcs_size[ARC_BUFC_DATA]);\n\tmfum = zfs_refcount_count(&arc_mfu->arcs_size[ARC_BUFC_METADATA]);\n\tuint64_t d = mrud + mfud;\n\tuint64_t m = mrum + mfum;\n\tuint64_t t = d + m;\n\n\t \n\tngrd = wmsum_value(&arc_mru_ghost->arcs_hits[ARC_BUFC_DATA]);\n\tuint64_t grd = ngrd - ogrd;\n\togrd = ngrd;\n\tngrm = wmsum_value(&arc_mru_ghost->arcs_hits[ARC_BUFC_METADATA]);\n\tuint64_t grm = ngrm - ogrm;\n\togrm = ngrm;\n\tngfd = wmsum_value(&arc_mfu_ghost->arcs_hits[ARC_BUFC_DATA]);\n\tuint64_t gfd = ngfd - ogfd;\n\togfd = ngfd;\n\tngfm = wmsum_value(&arc_mfu_ghost->arcs_hits[ARC_BUFC_METADATA]);\n\tuint64_t gfm = ngfm - ogfm;\n\togfm = ngfm;\n\n\t \n\tarc_meta = arc_evict_adj(arc_meta, gsrd + gsrm + gsfd + gsfm,\n\t    grm + gfm, grd + gfd, zfs_arc_meta_balance);\n\tarc_pd = arc_evict_adj(arc_pd, gsrd + gsfd, grd, gfd, 100);\n\tarc_pm = arc_evict_adj(arc_pm, gsrm + gsfm, grm, gfm, 100);\n\n\tasize = aggsum_value(&arc_sums.arcstat_size);\n\tint64_t wt = t - (asize - arc_c);\n\n\t \n\tint64_t prune = 0;\n\tint64_t dn = wmsum_value(&arc_sums.arcstat_dnode_size);\n\tw = wt * (int64_t)(arc_meta >> 16) >> 16;\n\tif (zfs_refcount_count(&arc_mru->arcs_size[ARC_BUFC_METADATA]) +\n\t    zfs_refcount_count(&arc_mfu->arcs_size[ARC_BUFC_METADATA]) -\n\t    zfs_refcount_count(&arc_mru->arcs_esize[ARC_BUFC_METADATA]) -\n\t    zfs_refcount_count(&arc_mfu->arcs_esize[ARC_BUFC_METADATA]) >\n\t    w * 3 / 4) {\n\t\tprune = dn / sizeof (dnode_t) *\n\t\t    zfs_arc_dnode_reduce_percent / 100;\n\t} else if (dn > arc_dnode_limit) {\n\t\tprune = (dn - arc_dnode_limit) / sizeof (dnode_t) *\n\t\t    zfs_arc_dnode_reduce_percent / 100;\n\t}\n\tif (prune > 0)\n\t\tarc_prune_async(prune);\n\n\t \n\tw = wt * (int64_t)(arc_meta * arc_pm >> 48) >> 16;\n\te = MIN((int64_t)(asize - arc_c), (int64_t)(mrum - w));\n\tbytes = arc_evict_impl(arc_mru, ARC_BUFC_METADATA, e);\n\ttotal_evicted += bytes;\n\tmrum -= bytes;\n\tasize -= bytes;\n\n\t \n\tw = wt * (int64_t)(arc_meta >> 16) >> 16;\n\te = MIN((int64_t)(asize - arc_c), (int64_t)(m - w));\n\tbytes = arc_evict_impl(arc_mfu, ARC_BUFC_METADATA, e);\n\ttotal_evicted += bytes;\n\tmfum -= bytes;\n\tasize -= bytes;\n\n\t \n\twt -= m - total_evicted;\n\tw = wt * (int64_t)(arc_pd >> 16) >> 16;\n\te = MIN((int64_t)(asize - arc_c), (int64_t)(mrud - w));\n\tbytes = arc_evict_impl(arc_mru, ARC_BUFC_DATA, e);\n\ttotal_evicted += bytes;\n\tmrud -= bytes;\n\tasize -= bytes;\n\n\t \n\te = asize - arc_c;\n\tbytes = arc_evict_impl(arc_mfu, ARC_BUFC_DATA, e);\n\tmfud -= bytes;\n\ttotal_evicted += bytes;\n\n\t \n\tgsrd = (mrum + mfud + mfum) / 2;\n\te = zfs_refcount_count(&arc_mru_ghost->arcs_size[ARC_BUFC_DATA]) -\n\t    gsrd;\n\t(void) arc_evict_impl(arc_mru_ghost, ARC_BUFC_DATA, e);\n\n\tgsrm = (mrud + mfud + mfum) / 2;\n\te = zfs_refcount_count(&arc_mru_ghost->arcs_size[ARC_BUFC_METADATA]) -\n\t    gsrm;\n\t(void) arc_evict_impl(arc_mru_ghost, ARC_BUFC_METADATA, e);\n\n\tgsfd = (mrud + mrum + mfum) / 2;\n\te = zfs_refcount_count(&arc_mfu_ghost->arcs_size[ARC_BUFC_DATA]) -\n\t    gsfd;\n\t(void) arc_evict_impl(arc_mfu_ghost, ARC_BUFC_DATA, e);\n\n\tgsfm = (mrud + mrum + mfud) / 2;\n\te = zfs_refcount_count(&arc_mfu_ghost->arcs_size[ARC_BUFC_METADATA]) -\n\t    gsfm;\n\t(void) arc_evict_impl(arc_mfu_ghost, ARC_BUFC_METADATA, e);\n\n\treturn (total_evicted);\n}\n\nvoid\narc_flush(spa_t *spa, boolean_t retry)\n{\n\tuint64_t guid = 0;\n\n\t \n\tASSERT(!retry || spa == NULL);\n\n\tif (spa != NULL)\n\t\tguid = spa_load_guid(spa);\n\n\t(void) arc_flush_state(arc_mru, guid, ARC_BUFC_DATA, retry);\n\t(void) arc_flush_state(arc_mru, guid, ARC_BUFC_METADATA, retry);\n\n\t(void) arc_flush_state(arc_mfu, guid, ARC_BUFC_DATA, retry);\n\t(void) arc_flush_state(arc_mfu, guid, ARC_BUFC_METADATA, retry);\n\n\t(void) arc_flush_state(arc_mru_ghost, guid, ARC_BUFC_DATA, retry);\n\t(void) arc_flush_state(arc_mru_ghost, guid, ARC_BUFC_METADATA, retry);\n\n\t(void) arc_flush_state(arc_mfu_ghost, guid, ARC_BUFC_DATA, retry);\n\t(void) arc_flush_state(arc_mfu_ghost, guid, ARC_BUFC_METADATA, retry);\n\n\t(void) arc_flush_state(arc_uncached, guid, ARC_BUFC_DATA, retry);\n\t(void) arc_flush_state(arc_uncached, guid, ARC_BUFC_METADATA, retry);\n}\n\nvoid\narc_reduce_target_size(int64_t to_free)\n{\n\tuint64_t c = arc_c;\n\n\tif (c <= arc_c_min)\n\t\treturn;\n\n\t \n\tuint64_t asize = aggsum_value(&arc_sums.arcstat_size);\n\tif (asize < c)\n\t\tto_free += c - asize;\n\tarc_c = MAX((int64_t)c - to_free, (int64_t)arc_c_min);\n\n\t \n\tmutex_enter(&arc_evict_lock);\n\tarc_evict_needed = B_TRUE;\n\tmutex_exit(&arc_evict_lock);\n\tzthr_wakeup(arc_evict_zthr);\n}\n\n \nboolean_t\narc_reclaim_needed(void)\n{\n\treturn (arc_available_memory() < 0);\n}\n\nvoid\narc_kmem_reap_soon(void)\n{\n\tsize_t\t\t\ti;\n\tkmem_cache_t\t\t*prev_cache = NULL;\n\tkmem_cache_t\t\t*prev_data_cache = NULL;\n\n#ifdef _KERNEL\n#if defined(_ILP32)\n\t \n\tkmem_reap();\n#endif\n#endif\n\n\tfor (i = 0; i < SPA_MAXBLOCKSIZE >> SPA_MINBLOCKSHIFT; i++) {\n#if defined(_ILP32)\n\t\t \n\t\tif (zio_buf_cache[i] == NULL)\n\t\t\tbreak;\n#endif\n\t\tif (zio_buf_cache[i] != prev_cache) {\n\t\t\tprev_cache = zio_buf_cache[i];\n\t\t\tkmem_cache_reap_now(zio_buf_cache[i]);\n\t\t}\n\t\tif (zio_data_buf_cache[i] != prev_data_cache) {\n\t\t\tprev_data_cache = zio_data_buf_cache[i];\n\t\t\tkmem_cache_reap_now(zio_data_buf_cache[i]);\n\t\t}\n\t}\n\tkmem_cache_reap_now(buf_cache);\n\tkmem_cache_reap_now(hdr_full_cache);\n\tkmem_cache_reap_now(hdr_l2only_cache);\n\tkmem_cache_reap_now(zfs_btree_leaf_cache);\n\tabd_cache_reap_now();\n}\n\nstatic boolean_t\narc_evict_cb_check(void *arg, zthr_t *zthr)\n{\n\t(void) arg, (void) zthr;\n\n#ifdef ZFS_DEBUG\n\t \n\tif (arc_ksp != NULL)\n\t\tarc_ksp->ks_update(arc_ksp, KSTAT_READ);\n#endif\n\n\t \n\tif (arc_evict_needed)\n\t\treturn (B_TRUE);\n\n\t \n\treturn ((zfs_refcount_count(&arc_uncached->arcs_esize[ARC_BUFC_DATA]) +\n\t    zfs_refcount_count(&arc_uncached->arcs_esize[ARC_BUFC_METADATA]) &&\n\t    ddi_get_lbolt() - arc_last_uncached_flush >\n\t    MSEC_TO_TICK(arc_min_prefetch_ms / 2)));\n}\n\n \nstatic void\narc_evict_cb(void *arg, zthr_t *zthr)\n{\n\t(void) arg, (void) zthr;\n\n\tuint64_t evicted = 0;\n\tfstrans_cookie_t cookie = spl_fstrans_mark();\n\n\t \n\tarc_last_uncached_flush = ddi_get_lbolt();\n\tevicted += arc_flush_state(arc_uncached, 0, ARC_BUFC_DATA, B_FALSE);\n\tevicted += arc_flush_state(arc_uncached, 0, ARC_BUFC_METADATA, B_FALSE);\n\n\t \n\tif (arc_evict_needed)\n\t\tevicted += arc_evict();\n\n\t \n\tmutex_enter(&arc_evict_lock);\n\tarc_evict_needed = !zthr_iscancelled(arc_evict_zthr) &&\n\t    evicted > 0 && aggsum_compare(&arc_sums.arcstat_size, arc_c) > 0;\n\tif (!arc_evict_needed) {\n\t\t \n\t\tarc_evict_waiter_t *aw;\n\t\twhile ((aw = list_remove_head(&arc_evict_waiters)) != NULL) {\n\t\t\tcv_broadcast(&aw->aew_cv);\n\t\t}\n\t\tarc_set_need_free();\n\t}\n\tmutex_exit(&arc_evict_lock);\n\tspl_fstrans_unmark(cookie);\n}\n\nstatic boolean_t\narc_reap_cb_check(void *arg, zthr_t *zthr)\n{\n\t(void) arg, (void) zthr;\n\n\tint64_t free_memory = arc_available_memory();\n\tstatic int reap_cb_check_counter = 0;\n\n\t \n\tif (!kmem_cache_reap_active() && free_memory < 0) {\n\n\t\tarc_no_grow = B_TRUE;\n\t\tarc_warm = B_TRUE;\n\t\t \n\t\tarc_growtime = gethrtime() + SEC2NSEC(arc_grow_retry);\n\t\treturn (B_TRUE);\n\t} else if (free_memory < arc_c >> arc_no_grow_shift) {\n\t\tarc_no_grow = B_TRUE;\n\t} else if (gethrtime() >= arc_growtime) {\n\t\tarc_no_grow = B_FALSE;\n\t}\n\n\t \n\tif (!((reap_cb_check_counter++) % 60))\n\t\tzfs_zstd_cache_reap_now();\n\n\treturn (B_FALSE);\n}\n\n \nstatic void\narc_reap_cb(void *arg, zthr_t *zthr)\n{\n\t(void) arg, (void) zthr;\n\n\tint64_t free_memory;\n\tfstrans_cookie_t cookie = spl_fstrans_mark();\n\n\t \n\tarc_kmem_reap_soon();\n\n\t \n\tdelay((hz * arc_kmem_cache_reap_retry_ms + 999) / 1000);\n\n\t \n\tfree_memory = arc_available_memory();\n\n\tint64_t can_free = arc_c - arc_c_min;\n\tif (can_free > 0) {\n\t\tint64_t to_free = (can_free >> arc_shrink_shift) - free_memory;\n\t\tif (to_free > 0)\n\t\t\tarc_reduce_target_size(to_free);\n\t}\n\tspl_fstrans_unmark(cookie);\n}\n\n#ifdef _KERNEL\n \n\n#endif  \n\n \nstatic void\narc_adapt(uint64_t bytes)\n{\n\t \n\tif (arc_reclaim_needed()) {\n\t\tzthr_wakeup(arc_reap_zthr);\n\t\treturn;\n\t}\n\n\tif (arc_no_grow)\n\t\treturn;\n\n\tif (arc_c >= arc_c_max)\n\t\treturn;\n\n\t \n\tif (aggsum_upper_bound(&arc_sums.arcstat_size) +\n\t    2 * SPA_MAXBLOCKSIZE >= arc_c) {\n\t\tuint64_t dc = MAX(bytes, SPA_OLD_MAXBLOCKSIZE);\n\t\tif (atomic_add_64_nv(&arc_c, dc) > arc_c_max)\n\t\t\tarc_c = arc_c_max;\n\t}\n}\n\n \nstatic arc_ovf_level_t\narc_is_overflowing(boolean_t use_reserve)\n{\n\t \n\tint64_t overflow = MAX(SPA_MAXBLOCKSIZE,\n\t    arc_c >> zfs_arc_overflow_shift);\n\n\t \n\tint64_t over = aggsum_lower_bound(&arc_sums.arcstat_size) -\n\t    arc_c - overflow / 2;\n\tif (!use_reserve)\n\t\toverflow /= 2;\n\treturn (over < 0 ? ARC_OVF_NONE :\n\t    over < overflow ? ARC_OVF_SOME : ARC_OVF_SEVERE);\n}\n\nstatic abd_t *\narc_get_data_abd(arc_buf_hdr_t *hdr, uint64_t size, const void *tag,\n    int alloc_flags)\n{\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\tarc_get_data_impl(hdr, size, tag, alloc_flags);\n\tif (alloc_flags & ARC_HDR_ALLOC_LINEAR)\n\t\treturn (abd_alloc_linear(size, type == ARC_BUFC_METADATA));\n\telse\n\t\treturn (abd_alloc(size, type == ARC_BUFC_METADATA));\n}\n\nstatic void *\narc_get_data_buf(arc_buf_hdr_t *hdr, uint64_t size, const void *tag)\n{\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\tarc_get_data_impl(hdr, size, tag, 0);\n\tif (type == ARC_BUFC_METADATA) {\n\t\treturn (zio_buf_alloc(size));\n\t} else {\n\t\tASSERT(type == ARC_BUFC_DATA);\n\t\treturn (zio_data_buf_alloc(size));\n\t}\n}\n\n \nvoid\narc_wait_for_eviction(uint64_t amount, boolean_t use_reserve)\n{\n\tswitch (arc_is_overflowing(use_reserve)) {\n\tcase ARC_OVF_NONE:\n\t\treturn;\n\tcase ARC_OVF_SOME:\n\t\t \n\t\tif (!arc_evict_needed) {\n\t\t\tarc_evict_needed = B_TRUE;\n\t\t\tzthr_wakeup(arc_evict_zthr);\n\t\t}\n\t\treturn;\n\tcase ARC_OVF_SEVERE:\n\tdefault:\n\t{\n\t\tarc_evict_waiter_t aw;\n\t\tlist_link_init(&aw.aew_node);\n\t\tcv_init(&aw.aew_cv, NULL, CV_DEFAULT, NULL);\n\n\t\tuint64_t last_count = 0;\n\t\tmutex_enter(&arc_evict_lock);\n\t\tif (!list_is_empty(&arc_evict_waiters)) {\n\t\t\tarc_evict_waiter_t *last =\n\t\t\t    list_tail(&arc_evict_waiters);\n\t\t\tlast_count = last->aew_count;\n\t\t} else if (!arc_evict_needed) {\n\t\t\tarc_evict_needed = B_TRUE;\n\t\t\tzthr_wakeup(arc_evict_zthr);\n\t\t}\n\t\t \n\t\taw.aew_count = MAX(last_count, arc_evict_count) + amount;\n\n\t\tlist_insert_tail(&arc_evict_waiters, &aw);\n\n\t\tarc_set_need_free();\n\n\t\tDTRACE_PROBE3(arc__wait__for__eviction,\n\t\t    uint64_t, amount,\n\t\t    uint64_t, arc_evict_count,\n\t\t    uint64_t, aw.aew_count);\n\n\t\t \n\t\tdo {\n\t\t\tcv_wait(&aw.aew_cv, &arc_evict_lock);\n\t\t} while (list_link_active(&aw.aew_node));\n\t\tmutex_exit(&arc_evict_lock);\n\n\t\tcv_destroy(&aw.aew_cv);\n\t}\n\t}\n}\n\n \nstatic void\narc_get_data_impl(arc_buf_hdr_t *hdr, uint64_t size, const void *tag,\n    int alloc_flags)\n{\n\tarc_adapt(size);\n\n\t \n\tarc_wait_for_eviction(size * zfs_arc_eviction_pct / 100,\n\t    alloc_flags & ARC_HDR_USE_RESERVE);\n\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\tif (type == ARC_BUFC_METADATA) {\n\t\tarc_space_consume(size, ARC_SPACE_META);\n\t} else {\n\t\tarc_space_consume(size, ARC_SPACE_DATA);\n\t}\n\n\t \n\tarc_state_t *state = hdr->b_l1hdr.b_state;\n\tif (!GHOST_STATE(state)) {\n\n\t\t(void) zfs_refcount_add_many(&state->arcs_size[type], size,\n\t\t    tag);\n\n\t\t \n\t\tif (multilist_link_active(&hdr->b_l1hdr.b_arc_node)) {\n\t\t\tASSERT(zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt));\n\t\t\t(void) zfs_refcount_add_many(&state->arcs_esize[type],\n\t\t\t    size, tag);\n\t\t}\n\t}\n}\n\nstatic void\narc_free_data_abd(arc_buf_hdr_t *hdr, abd_t *abd, uint64_t size,\n    const void *tag)\n{\n\tarc_free_data_impl(hdr, size, tag);\n\tabd_free(abd);\n}\n\nstatic void\narc_free_data_buf(arc_buf_hdr_t *hdr, void *buf, uint64_t size, const void *tag)\n{\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\tarc_free_data_impl(hdr, size, tag);\n\tif (type == ARC_BUFC_METADATA) {\n\t\tzio_buf_free(buf, size);\n\t} else {\n\t\tASSERT(type == ARC_BUFC_DATA);\n\t\tzio_data_buf_free(buf, size);\n\t}\n}\n\n \nstatic void\narc_free_data_impl(arc_buf_hdr_t *hdr, uint64_t size, const void *tag)\n{\n\tarc_state_t *state = hdr->b_l1hdr.b_state;\n\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\t \n\tif (multilist_link_active(&hdr->b_l1hdr.b_arc_node)) {\n\t\tASSERT(zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt));\n\t\tASSERT(state != arc_anon && state != arc_l2c_only);\n\n\t\t(void) zfs_refcount_remove_many(&state->arcs_esize[type],\n\t\t    size, tag);\n\t}\n\t(void) zfs_refcount_remove_many(&state->arcs_size[type], size, tag);\n\n\tVERIFY3U(hdr->b_type, ==, type);\n\tif (type == ARC_BUFC_METADATA) {\n\t\tarc_space_return(size, ARC_SPACE_META);\n\t} else {\n\t\tASSERT(type == ARC_BUFC_DATA);\n\t\tarc_space_return(size, ARC_SPACE_DATA);\n\t}\n}\n\n \nstatic void\narc_access(arc_buf_hdr_t *hdr, arc_flags_t arc_flags, boolean_t hit)\n{\n\tASSERT(MUTEX_HELD(HDR_LOCK(hdr)));\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\t \n\tboolean_t was_prefetch = HDR_PREFETCH(hdr);\n\tboolean_t now_prefetch = arc_flags & ARC_FLAG_PREFETCH;\n\tif (was_prefetch != now_prefetch) {\n\t\tif (was_prefetch) {\n\t\t\tARCSTAT_CONDSTAT(hit, demand_hit, demand_iohit,\n\t\t\t    HDR_PRESCIENT_PREFETCH(hdr), prescient, predictive,\n\t\t\t    prefetch);\n\t\t}\n\t\tif (HDR_HAS_L2HDR(hdr))\n\t\t\tl2arc_hdr_arcstats_decrement_state(hdr);\n\t\tif (was_prefetch) {\n\t\t\tarc_hdr_clear_flags(hdr,\n\t\t\t    ARC_FLAG_PREFETCH | ARC_FLAG_PRESCIENT_PREFETCH);\n\t\t} else {\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PREFETCH);\n\t\t}\n\t\tif (HDR_HAS_L2HDR(hdr))\n\t\t\tl2arc_hdr_arcstats_increment_state(hdr);\n\t}\n\tif (now_prefetch) {\n\t\tif (arc_flags & ARC_FLAG_PRESCIENT_PREFETCH) {\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PRESCIENT_PREFETCH);\n\t\t\tARCSTAT_BUMP(arcstat_prescient_prefetch);\n\t\t} else {\n\t\t\tARCSTAT_BUMP(arcstat_predictive_prefetch);\n\t\t}\n\t}\n\tif (arc_flags & ARC_FLAG_L2CACHE)\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_L2CACHE);\n\n\tclock_t now = ddi_get_lbolt();\n\tif (hdr->b_l1hdr.b_state == arc_anon) {\n\t\tarc_state_t\t*new_state;\n\t\t \n\t\tASSERT0(hdr->b_l1hdr.b_arc_access);\n\t\thdr->b_l1hdr.b_arc_access = now;\n\t\tif (HDR_UNCACHED(hdr)) {\n\t\t\tnew_state = arc_uncached;\n\t\t\tDTRACE_PROBE1(new_state__uncached, arc_buf_hdr_t *,\n\t\t\t    hdr);\n\t\t} else {\n\t\t\tnew_state = arc_mru;\n\t\t\tDTRACE_PROBE1(new_state__mru, arc_buf_hdr_t *, hdr);\n\t\t}\n\t\tarc_change_state(new_state, hdr);\n\t} else if (hdr->b_l1hdr.b_state == arc_mru) {\n\t\t \n\t\tif (HDR_IO_IN_PROGRESS(hdr)) {\n\t\t\thdr->b_l1hdr.b_arc_access = now;\n\t\t\treturn;\n\t\t}\n\t\thdr->b_l1hdr.b_mru_hits++;\n\t\tARCSTAT_BUMP(arcstat_mru_hits);\n\n\t\t \n\t\tif (was_prefetch) {\n\t\t\thdr->b_l1hdr.b_arc_access = now;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (ddi_time_after(now, hdr->b_l1hdr.b_arc_access +\n\t\t    ARC_MINTIME)) {\n\t\t\thdr->b_l1hdr.b_arc_access = now;\n\t\t\tDTRACE_PROBE1(new_state__mfu, arc_buf_hdr_t *, hdr);\n\t\t\tarc_change_state(arc_mfu, hdr);\n\t\t}\n\t} else if (hdr->b_l1hdr.b_state == arc_mru_ghost) {\n\t\tarc_state_t\t*new_state;\n\t\t \n\t\thdr->b_l1hdr.b_mru_ghost_hits++;\n\t\tARCSTAT_BUMP(arcstat_mru_ghost_hits);\n\t\thdr->b_l1hdr.b_arc_access = now;\n\t\twmsum_add(&arc_mru_ghost->arcs_hits[arc_buf_type(hdr)],\n\t\t    arc_hdr_size(hdr));\n\t\tif (was_prefetch) {\n\t\t\tnew_state = arc_mru;\n\t\t\tDTRACE_PROBE1(new_state__mru, arc_buf_hdr_t *, hdr);\n\t\t} else {\n\t\t\tnew_state = arc_mfu;\n\t\t\tDTRACE_PROBE1(new_state__mfu, arc_buf_hdr_t *, hdr);\n\t\t}\n\t\tarc_change_state(new_state, hdr);\n\t} else if (hdr->b_l1hdr.b_state == arc_mfu) {\n\t\t \n\t\tif (!HDR_IO_IN_PROGRESS(hdr)) {\n\t\t\thdr->b_l1hdr.b_mfu_hits++;\n\t\t\tARCSTAT_BUMP(arcstat_mfu_hits);\n\t\t}\n\t\thdr->b_l1hdr.b_arc_access = now;\n\t} else if (hdr->b_l1hdr.b_state == arc_mfu_ghost) {\n\t\t \n\t\thdr->b_l1hdr.b_mfu_ghost_hits++;\n\t\tARCSTAT_BUMP(arcstat_mfu_ghost_hits);\n\t\thdr->b_l1hdr.b_arc_access = now;\n\t\twmsum_add(&arc_mfu_ghost->arcs_hits[arc_buf_type(hdr)],\n\t\t    arc_hdr_size(hdr));\n\t\tDTRACE_PROBE1(new_state__mfu, arc_buf_hdr_t *, hdr);\n\t\tarc_change_state(arc_mfu, hdr);\n\t} else if (hdr->b_l1hdr.b_state == arc_uncached) {\n\t\t \n\t\tif (!HDR_IO_IN_PROGRESS(hdr))\n\t\t\tARCSTAT_BUMP(arcstat_uncached_hits);\n\t\thdr->b_l1hdr.b_arc_access = now;\n\t} else if (hdr->b_l1hdr.b_state == arc_l2c_only) {\n\t\t \n\t\thdr->b_l1hdr.b_arc_access = now;\n\t\tDTRACE_PROBE1(new_state__mru, arc_buf_hdr_t *, hdr);\n\t\tarc_change_state(arc_mru, hdr);\n\t} else {\n\t\tcmn_err(CE_PANIC, \"invalid arc state 0x%p\",\n\t\t    hdr->b_l1hdr.b_state);\n\t}\n}\n\n \nvoid\narc_buf_access(arc_buf_t *buf)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\t \n\tif (hdr->b_l1hdr.b_state == arc_anon || HDR_EMPTY(hdr))\n\t\treturn;\n\n\tkmutex_t *hash_lock = HDR_LOCK(hdr);\n\tmutex_enter(hash_lock);\n\n\tif (hdr->b_l1hdr.b_state == arc_anon || HDR_EMPTY(hdr)) {\n\t\tmutex_exit(hash_lock);\n\t\tARCSTAT_BUMP(arcstat_access_skip);\n\t\treturn;\n\t}\n\n\tASSERT(hdr->b_l1hdr.b_state == arc_mru ||\n\t    hdr->b_l1hdr.b_state == arc_mfu ||\n\t    hdr->b_l1hdr.b_state == arc_uncached);\n\n\tDTRACE_PROBE1(arc__hit, arc_buf_hdr_t *, hdr);\n\tarc_access(hdr, 0, B_TRUE);\n\tmutex_exit(hash_lock);\n\n\tARCSTAT_BUMP(arcstat_hits);\n\tARCSTAT_CONDSTAT(B_TRUE  , demand, prefetch,\n\t    !HDR_ISTYPE_METADATA(hdr), data, metadata, hits);\n}\n\n \nvoid\narc_bcopy_func(zio_t *zio, const zbookmark_phys_t *zb, const blkptr_t *bp,\n    arc_buf_t *buf, void *arg)\n{\n\t(void) zio, (void) zb, (void) bp;\n\n\tif (buf == NULL)\n\t\treturn;\n\n\tmemcpy(arg, buf->b_data, arc_buf_size(buf));\n\tarc_buf_destroy(buf, arg);\n}\n\n \nvoid\narc_getbuf_func(zio_t *zio, const zbookmark_phys_t *zb, const blkptr_t *bp,\n    arc_buf_t *buf, void *arg)\n{\n\t(void) zb, (void) bp;\n\tarc_buf_t **bufp = arg;\n\n\tif (buf == NULL) {\n\t\tASSERT(zio == NULL || zio->io_error != 0);\n\t\t*bufp = NULL;\n\t} else {\n\t\tASSERT(zio == NULL || zio->io_error == 0);\n\t\t*bufp = buf;\n\t\tASSERT(buf->b_data != NULL);\n\t}\n}\n\nstatic void\narc_hdr_verify(arc_buf_hdr_t *hdr, blkptr_t *bp)\n{\n\tif (BP_IS_HOLE(bp) || BP_IS_EMBEDDED(bp)) {\n\t\tASSERT3U(HDR_GET_PSIZE(hdr), ==, 0);\n\t\tASSERT3U(arc_hdr_get_compress(hdr), ==, ZIO_COMPRESS_OFF);\n\t} else {\n\t\tif (HDR_COMPRESSION_ENABLED(hdr)) {\n\t\t\tASSERT3U(arc_hdr_get_compress(hdr), ==,\n\t\t\t    BP_GET_COMPRESS(bp));\n\t\t}\n\t\tASSERT3U(HDR_GET_LSIZE(hdr), ==, BP_GET_LSIZE(bp));\n\t\tASSERT3U(HDR_GET_PSIZE(hdr), ==, BP_GET_PSIZE(bp));\n\t\tASSERT3U(!!HDR_PROTECTED(hdr), ==, BP_IS_PROTECTED(bp));\n\t}\n}\n\nstatic void\narc_read_done(zio_t *zio)\n{\n\tblkptr_t \t*bp = zio->io_bp;\n\tarc_buf_hdr_t\t*hdr = zio->io_private;\n\tkmutex_t\t*hash_lock = NULL;\n\tarc_callback_t\t*callback_list;\n\tarc_callback_t\t*acb;\n\n\t \n\tif (HDR_IN_HASH_TABLE(hdr)) {\n\t\tarc_buf_hdr_t *found;\n\n\t\tASSERT3U(hdr->b_birth, ==, BP_PHYSICAL_BIRTH(zio->io_bp));\n\t\tASSERT3U(hdr->b_dva.dva_word[0], ==,\n\t\t    BP_IDENTITY(zio->io_bp)->dva_word[0]);\n\t\tASSERT3U(hdr->b_dva.dva_word[1], ==,\n\t\t    BP_IDENTITY(zio->io_bp)->dva_word[1]);\n\n\t\tfound = buf_hash_find(hdr->b_spa, zio->io_bp, &hash_lock);\n\n\t\tASSERT((found == hdr &&\n\t\t    DVA_EQUAL(&hdr->b_dva, BP_IDENTITY(zio->io_bp))) ||\n\t\t    (found == hdr && HDR_L2_READING(hdr)));\n\t\tASSERT3P(hash_lock, !=, NULL);\n\t}\n\n\tif (BP_IS_PROTECTED(bp)) {\n\t\thdr->b_crypt_hdr.b_ot = BP_GET_TYPE(bp);\n\t\thdr->b_crypt_hdr.b_dsobj = zio->io_bookmark.zb_objset;\n\t\tzio_crypt_decode_params_bp(bp, hdr->b_crypt_hdr.b_salt,\n\t\t    hdr->b_crypt_hdr.b_iv);\n\n\t\tif (zio->io_error == 0) {\n\t\t\tif (BP_GET_TYPE(bp) == DMU_OT_INTENT_LOG) {\n\t\t\t\tvoid *tmpbuf;\n\n\t\t\t\ttmpbuf = abd_borrow_buf_copy(zio->io_abd,\n\t\t\t\t    sizeof (zil_chain_t));\n\t\t\t\tzio_crypt_decode_mac_zil(tmpbuf,\n\t\t\t\t    hdr->b_crypt_hdr.b_mac);\n\t\t\t\tabd_return_buf(zio->io_abd, tmpbuf,\n\t\t\t\t    sizeof (zil_chain_t));\n\t\t\t} else {\n\t\t\t\tzio_crypt_decode_mac_bp(bp,\n\t\t\t\t    hdr->b_crypt_hdr.b_mac);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (zio->io_error == 0) {\n\t\t \n\t\tif (BP_SHOULD_BYTESWAP(zio->io_bp)) {\n\t\t\tif (BP_GET_LEVEL(zio->io_bp) > 0) {\n\t\t\t\thdr->b_l1hdr.b_byteswap = DMU_BSWAP_UINT64;\n\t\t\t} else {\n\t\t\t\thdr->b_l1hdr.b_byteswap =\n\t\t\t\t    DMU_OT_BYTESWAP(BP_GET_TYPE(zio->io_bp));\n\t\t\t}\n\t\t} else {\n\t\t\thdr->b_l1hdr.b_byteswap = DMU_BSWAP_NUMFUNCS;\n\t\t}\n\t\tif (!HDR_L2_READING(hdr)) {\n\t\t\thdr->b_complevel = zio->io_prop.zp_complevel;\n\t\t}\n\t}\n\n\tarc_hdr_clear_flags(hdr, ARC_FLAG_L2_EVICTED);\n\tif (l2arc_noprefetch && HDR_PREFETCH(hdr))\n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_L2CACHE);\n\n\tcallback_list = hdr->b_l1hdr.b_acb;\n\tASSERT3P(callback_list, !=, NULL);\n\thdr->b_l1hdr.b_acb = NULL;\n\n\t \n\tint callback_cnt = 0;\n\tfor (acb = callback_list; acb != NULL; acb = acb->acb_next) {\n\n\t\t \n\t\tcallback_list = acb;\n\n\t\tif (!acb->acb_done || acb->acb_nobuf)\n\t\t\tcontinue;\n\n\t\tcallback_cnt++;\n\n\t\tif (zio->io_error != 0)\n\t\t\tcontinue;\n\n\t\tint error = arc_buf_alloc_impl(hdr, zio->io_spa,\n\t\t    &acb->acb_zb, acb->acb_private, acb->acb_encrypted,\n\t\t    acb->acb_compressed, acb->acb_noauth, B_TRUE,\n\t\t    &acb->acb_buf);\n\n\t\t \n\t\tASSERT((zio->io_flags & ZIO_FLAG_SPECULATIVE) ||\n\t\t    error != EACCES);\n\n\t\t \n\t\tif (error == ECKSUM) {\n\t\t\tASSERT(BP_IS_PROTECTED(bp));\n\t\t\terror = SET_ERROR(EIO);\n\t\t\tif ((zio->io_flags & ZIO_FLAG_SPECULATIVE) == 0) {\n\t\t\t\tspa_log_error(zio->io_spa, &acb->acb_zb,\n\t\t\t\t    &zio->io_bp->blk_birth);\n\t\t\t\t(void) zfs_ereport_post(\n\t\t\t\t    FM_EREPORT_ZFS_AUTHENTICATION,\n\t\t\t\t    zio->io_spa, NULL, &acb->acb_zb, zio, 0);\n\t\t\t}\n\t\t}\n\n\t\tif (error != 0) {\n\t\t\t \n\t\t\tzio->io_error = error;\n\t\t}\n\t}\n\n\t \n\tASSERT(callback_cnt < 2 || hash_lock != NULL);\n\n\tif (zio->io_error == 0) {\n\t\tarc_hdr_verify(hdr, zio->io_bp);\n\t} else {\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_IO_ERROR);\n\t\tif (hdr->b_l1hdr.b_state != arc_anon)\n\t\t\tarc_change_state(arc_anon, hdr);\n\t\tif (HDR_IN_HASH_TABLE(hdr))\n\t\t\tbuf_hash_remove(hdr);\n\t}\n\n\tarc_hdr_clear_flags(hdr, ARC_FLAG_IO_IN_PROGRESS);\n\t(void) remove_reference(hdr, hdr);\n\n\tif (hash_lock != NULL)\n\t\tmutex_exit(hash_lock);\n\n\t \n\twhile ((acb = callback_list) != NULL) {\n\t\tif (acb->acb_done != NULL) {\n\t\t\tif (zio->io_error != 0 && acb->acb_buf != NULL) {\n\t\t\t\t \n\t\t\t\tarc_buf_destroy(acb->acb_buf,\n\t\t\t\t    acb->acb_private);\n\t\t\t\tacb->acb_buf = NULL;\n\t\t\t}\n\t\t\tacb->acb_done(zio, &zio->io_bookmark, zio->io_bp,\n\t\t\t    acb->acb_buf, acb->acb_private);\n\t\t}\n\n\t\tif (acb->acb_zio_dummy != NULL) {\n\t\t\tacb->acb_zio_dummy->io_error = zio->io_error;\n\t\t\tzio_nowait(acb->acb_zio_dummy);\n\t\t}\n\n\t\tcallback_list = acb->acb_prev;\n\t\tif (acb->acb_wait) {\n\t\t\tmutex_enter(&acb->acb_wait_lock);\n\t\t\tacb->acb_wait_error = zio->io_error;\n\t\t\tacb->acb_wait = B_FALSE;\n\t\t\tcv_signal(&acb->acb_wait_cv);\n\t\t\tmutex_exit(&acb->acb_wait_lock);\n\t\t\t \n\t\t} else {\n\t\t\tkmem_free(acb, sizeof (arc_callback_t));\n\t\t}\n\t}\n}\n\n \nint\narc_read(zio_t *pio, spa_t *spa, const blkptr_t *bp,\n    arc_read_done_func_t *done, void *private, zio_priority_t priority,\n    int zio_flags, arc_flags_t *arc_flags, const zbookmark_phys_t *zb)\n{\n\tarc_buf_hdr_t *hdr = NULL;\n\tkmutex_t *hash_lock = NULL;\n\tzio_t *rzio;\n\tuint64_t guid = spa_load_guid(spa);\n\tboolean_t compressed_read = (zio_flags & ZIO_FLAG_RAW_COMPRESS) != 0;\n\tboolean_t encrypted_read = BP_IS_ENCRYPTED(bp) &&\n\t    (zio_flags & ZIO_FLAG_RAW_ENCRYPT) != 0;\n\tboolean_t noauth_read = BP_IS_AUTHENTICATED(bp) &&\n\t    (zio_flags & ZIO_FLAG_RAW_ENCRYPT) != 0;\n\tboolean_t embedded_bp = !!BP_IS_EMBEDDED(bp);\n\tboolean_t no_buf = *arc_flags & ARC_FLAG_NO_BUF;\n\tarc_buf_t *buf = NULL;\n\tint rc = 0;\n\n\tASSERT(!embedded_bp ||\n\t    BPE_GET_ETYPE(bp) == BP_EMBEDDED_TYPE_DATA);\n\tASSERT(!BP_IS_HOLE(bp));\n\tASSERT(!BP_IS_REDACTED(bp));\n\n\t \n\tfstrans_cookie_t cookie = spl_fstrans_mark();\ntop:\n\t \n\tif (!zfs_blkptr_verify(spa, bp, (zio_flags & ZIO_FLAG_CONFIG_WRITER) ?\n\t    BLK_CONFIG_HELD : BLK_CONFIG_NEEDED, BLK_VERIFY_LOG)) {\n\t\trc = SET_ERROR(ECKSUM);\n\t\tgoto done;\n\t}\n\n\tif (!embedded_bp) {\n\t\t \n\t\thdr = buf_hash_find(guid, bp, &hash_lock);\n\t}\n\n\t \n\tif (hdr != NULL && HDR_HAS_L1HDR(hdr) && (HDR_HAS_RABD(hdr) ||\n\t    (hdr->b_l1hdr.b_pabd != NULL && !encrypted_read))) {\n\t\tboolean_t is_data = !HDR_ISTYPE_METADATA(hdr);\n\n\t\tif (HDR_IO_IN_PROGRESS(hdr)) {\n\t\t\tif (*arc_flags & ARC_FLAG_CACHED_ONLY) {\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\tARCSTAT_BUMP(arcstat_cached_only_in_progress);\n\t\t\t\trc = SET_ERROR(ENOENT);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tzio_t *head_zio = hdr->b_l1hdr.b_acb->acb_zio_head;\n\t\t\tASSERT3P(head_zio, !=, NULL);\n\t\t\tif ((hdr->b_flags & ARC_FLAG_PRIO_ASYNC_READ) &&\n\t\t\t    priority == ZIO_PRIORITY_SYNC_READ) {\n\t\t\t\t \n\t\t\t\tzio_change_priority(head_zio, priority);\n\t\t\t\tDTRACE_PROBE1(arc__async__upgrade__sync,\n\t\t\t\t    arc_buf_hdr_t *, hdr);\n\t\t\t\tARCSTAT_BUMP(arcstat_async_upgrade_sync);\n\t\t\t}\n\n\t\t\tDTRACE_PROBE1(arc__iohit, arc_buf_hdr_t *, hdr);\n\t\t\tarc_access(hdr, *arc_flags, B_FALSE);\n\n\t\t\t \n\t\t\tarc_callback_t *acb = NULL;\n\t\t\tif (done || pio || *arc_flags & ARC_FLAG_WAIT) {\n\t\t\t\tacb = kmem_zalloc(sizeof (arc_callback_t),\n\t\t\t\t    KM_SLEEP);\n\t\t\t\tacb->acb_done = done;\n\t\t\t\tacb->acb_private = private;\n\t\t\t\tacb->acb_compressed = compressed_read;\n\t\t\t\tacb->acb_encrypted = encrypted_read;\n\t\t\t\tacb->acb_noauth = noauth_read;\n\t\t\t\tacb->acb_nobuf = no_buf;\n\t\t\t\tif (*arc_flags & ARC_FLAG_WAIT) {\n\t\t\t\t\tacb->acb_wait = B_TRUE;\n\t\t\t\t\tmutex_init(&acb->acb_wait_lock, NULL,\n\t\t\t\t\t    MUTEX_DEFAULT, NULL);\n\t\t\t\t\tcv_init(&acb->acb_wait_cv, NULL,\n\t\t\t\t\t    CV_DEFAULT, NULL);\n\t\t\t\t}\n\t\t\t\tacb->acb_zb = *zb;\n\t\t\t\tif (pio != NULL) {\n\t\t\t\t\tacb->acb_zio_dummy = zio_null(pio,\n\t\t\t\t\t    spa, NULL, NULL, NULL, zio_flags);\n\t\t\t\t}\n\t\t\t\tacb->acb_zio_head = head_zio;\n\t\t\t\tacb->acb_next = hdr->b_l1hdr.b_acb;\n\t\t\t\thdr->b_l1hdr.b_acb->acb_prev = acb;\n\t\t\t\thdr->b_l1hdr.b_acb = acb;\n\t\t\t}\n\t\t\tmutex_exit(hash_lock);\n\n\t\t\tARCSTAT_BUMP(arcstat_iohits);\n\t\t\tARCSTAT_CONDSTAT(!(*arc_flags & ARC_FLAG_PREFETCH),\n\t\t\t    demand, prefetch, is_data, data, metadata, iohits);\n\n\t\t\tif (*arc_flags & ARC_FLAG_WAIT) {\n\t\t\t\tmutex_enter(&acb->acb_wait_lock);\n\t\t\t\twhile (acb->acb_wait) {\n\t\t\t\t\tcv_wait(&acb->acb_wait_cv,\n\t\t\t\t\t    &acb->acb_wait_lock);\n\t\t\t\t}\n\t\t\t\trc = acb->acb_wait_error;\n\t\t\t\tmutex_exit(&acb->acb_wait_lock);\n\t\t\t\tmutex_destroy(&acb->acb_wait_lock);\n\t\t\t\tcv_destroy(&acb->acb_wait_cv);\n\t\t\t\tkmem_free(acb, sizeof (arc_callback_t));\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\n\t\tASSERT(hdr->b_l1hdr.b_state == arc_mru ||\n\t\t    hdr->b_l1hdr.b_state == arc_mfu ||\n\t\t    hdr->b_l1hdr.b_state == arc_uncached);\n\n\t\tDTRACE_PROBE1(arc__hit, arc_buf_hdr_t *, hdr);\n\t\tarc_access(hdr, *arc_flags, B_TRUE);\n\n\t\tif (done && !no_buf) {\n\t\t\tASSERT(!embedded_bp || !BP_IS_HOLE(bp));\n\n\t\t\t \n\t\t\trc = arc_buf_alloc_impl(hdr, spa, zb, private,\n\t\t\t    encrypted_read, compressed_read, noauth_read,\n\t\t\t    B_TRUE, &buf);\n\t\t\tif (rc == ECKSUM) {\n\t\t\t\t \n\t\t\t\trc = SET_ERROR(EIO);\n\t\t\t\tif ((zio_flags & ZIO_FLAG_SPECULATIVE) == 0) {\n\t\t\t\t\tspa_log_error(spa, zb, &hdr->b_birth);\n\t\t\t\t\t(void) zfs_ereport_post(\n\t\t\t\t\t    FM_EREPORT_ZFS_AUTHENTICATION,\n\t\t\t\t\t    spa, NULL, zb, NULL, 0);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (rc != 0) {\n\t\t\t\tarc_buf_destroy_impl(buf);\n\t\t\t\tbuf = NULL;\n\t\t\t\t(void) remove_reference(hdr, private);\n\t\t\t}\n\n\t\t\t \n\t\t\tASSERT((zio_flags & ZIO_FLAG_SPECULATIVE) ||\n\t\t\t    rc != EACCES);\n\t\t}\n\t\tmutex_exit(hash_lock);\n\t\tARCSTAT_BUMP(arcstat_hits);\n\t\tARCSTAT_CONDSTAT(!(*arc_flags & ARC_FLAG_PREFETCH),\n\t\t    demand, prefetch, is_data, data, metadata, hits);\n\t\t*arc_flags |= ARC_FLAG_CACHED;\n\t\tgoto done;\n\t} else {\n\t\tuint64_t lsize = BP_GET_LSIZE(bp);\n\t\tuint64_t psize = BP_GET_PSIZE(bp);\n\t\tarc_callback_t *acb;\n\t\tvdev_t *vd = NULL;\n\t\tuint64_t addr = 0;\n\t\tboolean_t devw = B_FALSE;\n\t\tuint64_t size;\n\t\tabd_t *hdr_abd;\n\t\tint alloc_flags = encrypted_read ? ARC_HDR_ALLOC_RDATA : 0;\n\t\tarc_buf_contents_t type = BP_GET_BUFC_TYPE(bp);\n\n\t\tif (*arc_flags & ARC_FLAG_CACHED_ONLY) {\n\t\t\tif (hash_lock != NULL)\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\trc = SET_ERROR(ENOENT);\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (hdr == NULL) {\n\t\t\t \n\t\t\tarc_buf_hdr_t *exists = NULL;\n\t\t\thdr = arc_hdr_alloc(spa_load_guid(spa), psize, lsize,\n\t\t\t    BP_IS_PROTECTED(bp), BP_GET_COMPRESS(bp), 0, type);\n\n\t\t\tif (!embedded_bp) {\n\t\t\t\thdr->b_dva = *BP_IDENTITY(bp);\n\t\t\t\thdr->b_birth = BP_PHYSICAL_BIRTH(bp);\n\t\t\t\texists = buf_hash_insert(hdr, &hash_lock);\n\t\t\t}\n\t\t\tif (exists != NULL) {\n\t\t\t\t \n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\tbuf_discard_identity(hdr);\n\t\t\t\tarc_hdr_destroy(hdr);\n\t\t\t\tgoto top;  \n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (!HDR_HAS_L1HDR(hdr)) {\n\t\t\t\thdr = arc_hdr_realloc(hdr, hdr_l2only_cache,\n\t\t\t\t    hdr_full_cache);\n\t\t\t}\n\n\t\t\tif (GHOST_STATE(hdr->b_l1hdr.b_state)) {\n\t\t\t\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\t\t\t\tASSERT(!HDR_HAS_RABD(hdr));\n\t\t\t\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\t\t\t\tASSERT0(zfs_refcount_count(\n\t\t\t\t    &hdr->b_l1hdr.b_refcnt));\n\t\t\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, NULL);\n#ifdef ZFS_DEBUG\n\t\t\t\tASSERT3P(hdr->b_l1hdr.b_freeze_cksum, ==, NULL);\n#endif\n\t\t\t} else if (HDR_IO_IN_PROGRESS(hdr)) {\n\t\t\t\t \n\t\t\t\tarc_callback_t *acb = kmem_zalloc(\n\t\t\t\t    sizeof (arc_callback_t), KM_SLEEP);\n\t\t\t\tacb->acb_wait = B_TRUE;\n\t\t\t\tmutex_init(&acb->acb_wait_lock, NULL,\n\t\t\t\t    MUTEX_DEFAULT, NULL);\n\t\t\t\tcv_init(&acb->acb_wait_cv, NULL, CV_DEFAULT,\n\t\t\t\t    NULL);\n\t\t\t\tacb->acb_zio_head =\n\t\t\t\t    hdr->b_l1hdr.b_acb->acb_zio_head;\n\t\t\t\tacb->acb_next = hdr->b_l1hdr.b_acb;\n\t\t\t\thdr->b_l1hdr.b_acb->acb_prev = acb;\n\t\t\t\thdr->b_l1hdr.b_acb = acb;\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\tmutex_enter(&acb->acb_wait_lock);\n\t\t\t\twhile (acb->acb_wait) {\n\t\t\t\t\tcv_wait(&acb->acb_wait_cv,\n\t\t\t\t\t    &acb->acb_wait_lock);\n\t\t\t\t}\n\t\t\t\tmutex_exit(&acb->acb_wait_lock);\n\t\t\t\tmutex_destroy(&acb->acb_wait_lock);\n\t\t\t\tcv_destroy(&acb->acb_wait_cv);\n\t\t\t\tkmem_free(acb, sizeof (arc_callback_t));\n\t\t\t\tgoto top;\n\t\t\t}\n\t\t}\n\t\tif (*arc_flags & ARC_FLAG_UNCACHED) {\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_UNCACHED);\n\t\t\tif (!encrypted_read)\n\t\t\t\talloc_flags |= ARC_HDR_ALLOC_LINEAR;\n\t\t}\n\n\t\t \n\t\tadd_reference(hdr, hdr);\n\t\tif (!embedded_bp)\n\t\t\tarc_access(hdr, *arc_flags, B_FALSE);\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_IO_IN_PROGRESS);\n\t\tarc_hdr_alloc_abd(hdr, alloc_flags);\n\t\tif (encrypted_read) {\n\t\t\tASSERT(HDR_HAS_RABD(hdr));\n\t\t\tsize = HDR_GET_PSIZE(hdr);\n\t\t\thdr_abd = hdr->b_crypt_hdr.b_rabd;\n\t\t\tzio_flags |= ZIO_FLAG_RAW;\n\t\t} else {\n\t\t\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\t\t\tsize = arc_hdr_size(hdr);\n\t\t\thdr_abd = hdr->b_l1hdr.b_pabd;\n\n\t\t\tif (arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF) {\n\t\t\t\tzio_flags |= ZIO_FLAG_RAW_COMPRESS;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (BP_IS_AUTHENTICATED(bp))\n\t\t\t\tzio_flags |= ZIO_FLAG_RAW_ENCRYPT;\n\t\t}\n\n\t\tif (BP_IS_AUTHENTICATED(bp))\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_NOAUTH);\n\t\tif (BP_GET_LEVEL(bp) > 0)\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_INDIRECT);\n\t\tASSERT(!GHOST_STATE(hdr->b_l1hdr.b_state));\n\n\t\tacb = kmem_zalloc(sizeof (arc_callback_t), KM_SLEEP);\n\t\tacb->acb_done = done;\n\t\tacb->acb_private = private;\n\t\tacb->acb_compressed = compressed_read;\n\t\tacb->acb_encrypted = encrypted_read;\n\t\tacb->acb_noauth = noauth_read;\n\t\tacb->acb_zb = *zb;\n\n\t\tASSERT3P(hdr->b_l1hdr.b_acb, ==, NULL);\n\t\thdr->b_l1hdr.b_acb = acb;\n\n\t\tif (HDR_HAS_L2HDR(hdr) &&\n\t\t    (vd = hdr->b_l2hdr.b_dev->l2ad_vdev) != NULL) {\n\t\t\tdevw = hdr->b_l2hdr.b_dev->l2ad_writing;\n\t\t\taddr = hdr->b_l2hdr.b_daddr;\n\t\t\t \n\t\t\tif (vdev_is_dead(vd) ||\n\t\t\t    !spa_config_tryenter(spa, SCL_L2ARC, vd, RW_READER))\n\t\t\t\tvd = NULL;\n\t\t}\n\n\t\t \n\t\tif (priority == ZIO_PRIORITY_ASYNC_READ ||\n\t\t    priority == ZIO_PRIORITY_SCRUB)\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PRIO_ASYNC_READ);\n\t\telse\n\t\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_PRIO_ASYNC_READ);\n\n\t\t \n\t\tASSERT3U(HDR_GET_LSIZE(hdr), ==, lsize);\n\n\t\t \n\t\tif (!embedded_bp) {\n\t\t\tDTRACE_PROBE4(arc__miss, arc_buf_hdr_t *, hdr,\n\t\t\t    blkptr_t *, bp, uint64_t, lsize,\n\t\t\t    zbookmark_phys_t *, zb);\n\t\t\tARCSTAT_BUMP(arcstat_misses);\n\t\t\tARCSTAT_CONDSTAT(!(*arc_flags & ARC_FLAG_PREFETCH),\n\t\t\t    demand, prefetch, !HDR_ISTYPE_METADATA(hdr), data,\n\t\t\t    metadata, misses);\n\t\t\tzfs_racct_read(size, 1);\n\t\t}\n\n\t\t \n\t\tconst boolean_t spa_has_l2 = l2arc_ndev != 0 &&\n\t\t    spa->spa_l2cache.sav_count > 0;\n\n\t\tif (vd != NULL && spa_has_l2 && !(l2arc_norw && devw)) {\n\t\t\t \n\t\t\tif (HDR_HAS_L2HDR(hdr) &&\n\t\t\t    !HDR_L2_WRITING(hdr) && !HDR_L2_EVICTED(hdr)) {\n\t\t\t\tl2arc_read_callback_t *cb;\n\t\t\t\tabd_t *abd;\n\t\t\t\tuint64_t asize;\n\n\t\t\t\tDTRACE_PROBE1(l2arc__hit, arc_buf_hdr_t *, hdr);\n\t\t\t\tARCSTAT_BUMP(arcstat_l2_hits);\n\t\t\t\thdr->b_l2hdr.b_hits++;\n\n\t\t\t\tcb = kmem_zalloc(sizeof (l2arc_read_callback_t),\n\t\t\t\t    KM_SLEEP);\n\t\t\t\tcb->l2rcb_hdr = hdr;\n\t\t\t\tcb->l2rcb_bp = *bp;\n\t\t\t\tcb->l2rcb_zb = *zb;\n\t\t\t\tcb->l2rcb_flags = zio_flags;\n\n\t\t\t\t \n\t\t\t\tif (HDR_GET_COMPRESS(hdr) != ZIO_COMPRESS_OFF &&\n\t\t\t\t    !HDR_COMPRESSION_ENABLED(hdr) &&\n\t\t\t\t    HDR_GET_PSIZE(hdr) != 0) {\n\t\t\t\t\tsize = HDR_GET_PSIZE(hdr);\n\t\t\t\t}\n\n\t\t\t\tasize = vdev_psize_to_asize(vd, size);\n\t\t\t\tif (asize != size) {\n\t\t\t\t\tabd = abd_alloc_for_io(asize,\n\t\t\t\t\t    HDR_ISTYPE_METADATA(hdr));\n\t\t\t\t\tcb->l2rcb_abd = abd;\n\t\t\t\t} else {\n\t\t\t\t\tabd = hdr_abd;\n\t\t\t\t}\n\n\t\t\t\tASSERT(addr >= VDEV_LABEL_START_SIZE &&\n\t\t\t\t    addr + asize <= vd->vdev_psize -\n\t\t\t\t    VDEV_LABEL_END_SIZE);\n\n\t\t\t\t \n\t\t\t\tASSERT3U(arc_hdr_get_compress(hdr), !=,\n\t\t\t\t    ZIO_COMPRESS_EMPTY);\n\t\t\t\trzio = zio_read_phys(pio, vd, addr,\n\t\t\t\t    asize, abd,\n\t\t\t\t    ZIO_CHECKSUM_OFF,\n\t\t\t\t    l2arc_read_done, cb, priority,\n\t\t\t\t    zio_flags | ZIO_FLAG_CANFAIL |\n\t\t\t\t    ZIO_FLAG_DONT_PROPAGATE |\n\t\t\t\t    ZIO_FLAG_DONT_RETRY, B_FALSE);\n\t\t\t\tacb->acb_zio_head = rzio;\n\n\t\t\t\tif (hash_lock != NULL)\n\t\t\t\t\tmutex_exit(hash_lock);\n\n\t\t\t\tDTRACE_PROBE2(l2arc__read, vdev_t *, vd,\n\t\t\t\t    zio_t *, rzio);\n\t\t\t\tARCSTAT_INCR(arcstat_l2_read_bytes,\n\t\t\t\t    HDR_GET_PSIZE(hdr));\n\n\t\t\t\tif (*arc_flags & ARC_FLAG_NOWAIT) {\n\t\t\t\t\tzio_nowait(rzio);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tASSERT(*arc_flags & ARC_FLAG_WAIT);\n\t\t\t\tif (zio_wait(rzio) == 0)\n\t\t\t\t\tgoto out;\n\n\t\t\t\t \n\t\t\t\tif (hash_lock != NULL)\n\t\t\t\t\tmutex_enter(hash_lock);\n\t\t\t} else {\n\t\t\t\tDTRACE_PROBE1(l2arc__miss,\n\t\t\t\t    arc_buf_hdr_t *, hdr);\n\t\t\t\tARCSTAT_BUMP(arcstat_l2_misses);\n\t\t\t\tif (HDR_L2_WRITING(hdr))\n\t\t\t\t\tARCSTAT_BUMP(arcstat_l2_rw_clash);\n\t\t\t\tspa_config_exit(spa, SCL_L2ARC, vd);\n\t\t\t}\n\t\t} else {\n\t\t\tif (vd != NULL)\n\t\t\t\tspa_config_exit(spa, SCL_L2ARC, vd);\n\n\t\t\t \n\t\t\tif (spa_has_l2) {\n\t\t\t\t \n\t\t\t\tif (!embedded_bp) {\n\t\t\t\t\tDTRACE_PROBE1(l2arc__miss,\n\t\t\t\t\t    arc_buf_hdr_t *, hdr);\n\t\t\t\t\tARCSTAT_BUMP(arcstat_l2_misses);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\trzio = zio_read(pio, spa, bp, hdr_abd, size,\n\t\t    arc_read_done, hdr, priority, zio_flags, zb);\n\t\tacb->acb_zio_head = rzio;\n\n\t\tif (hash_lock != NULL)\n\t\t\tmutex_exit(hash_lock);\n\n\t\tif (*arc_flags & ARC_FLAG_WAIT) {\n\t\t\trc = zio_wait(rzio);\n\t\t\tgoto out;\n\t\t}\n\n\t\tASSERT(*arc_flags & ARC_FLAG_NOWAIT);\n\t\tzio_nowait(rzio);\n\t}\n\nout:\n\t \n\tif (!embedded_bp)\n\t\tspa_read_history_add(spa, zb, *arc_flags);\n\tspl_fstrans_unmark(cookie);\n\treturn (rc);\n\ndone:\n\tif (done)\n\t\tdone(NULL, zb, bp, buf, private);\n\tif (pio && rc != 0) {\n\t\tzio_t *zio = zio_null(pio, spa, NULL, NULL, NULL, zio_flags);\n\t\tzio->io_error = rc;\n\t\tzio_nowait(zio);\n\t}\n\tgoto out;\n}\n\narc_prune_t *\narc_add_prune_callback(arc_prune_func_t *func, void *private)\n{\n\tarc_prune_t *p;\n\n\tp = kmem_alloc(sizeof (*p), KM_SLEEP);\n\tp->p_pfunc = func;\n\tp->p_private = private;\n\tlist_link_init(&p->p_node);\n\tzfs_refcount_create(&p->p_refcnt);\n\n\tmutex_enter(&arc_prune_mtx);\n\tzfs_refcount_add(&p->p_refcnt, &arc_prune_list);\n\tlist_insert_head(&arc_prune_list, p);\n\tmutex_exit(&arc_prune_mtx);\n\n\treturn (p);\n}\n\nvoid\narc_remove_prune_callback(arc_prune_t *p)\n{\n\tboolean_t wait = B_FALSE;\n\tmutex_enter(&arc_prune_mtx);\n\tlist_remove(&arc_prune_list, p);\n\tif (zfs_refcount_remove(&p->p_refcnt, &arc_prune_list) > 0)\n\t\twait = B_TRUE;\n\tmutex_exit(&arc_prune_mtx);\n\n\t \n\tif (wait)\n\t\ttaskq_wait_outstanding(arc_prune_taskq, 0);\n\tASSERT0(zfs_refcount_count(&p->p_refcnt));\n\tzfs_refcount_destroy(&p->p_refcnt);\n\tkmem_free(p, sizeof (*p));\n}\n\n \nstatic void\narc_prune_task(void *ptr)\n{\n\tarc_prune_t *ap = (arc_prune_t *)ptr;\n\tarc_prune_func_t *func = ap->p_pfunc;\n\n\tif (func != NULL)\n\t\tfunc(ap->p_adjust, ap->p_private);\n\n\tzfs_refcount_remove(&ap->p_refcnt, func);\n}\n\n \nstatic void\narc_prune_async(uint64_t adjust)\n{\n\tarc_prune_t *ap;\n\n\tmutex_enter(&arc_prune_mtx);\n\tfor (ap = list_head(&arc_prune_list); ap != NULL;\n\t    ap = list_next(&arc_prune_list, ap)) {\n\n\t\tif (zfs_refcount_count(&ap->p_refcnt) >= 2)\n\t\t\tcontinue;\n\n\t\tzfs_refcount_add(&ap->p_refcnt, ap->p_pfunc);\n\t\tap->p_adjust = adjust;\n\t\tif (taskq_dispatch(arc_prune_taskq, arc_prune_task,\n\t\t    ap, TQ_SLEEP) == TASKQID_INVALID) {\n\t\t\tzfs_refcount_remove(&ap->p_refcnt, ap->p_pfunc);\n\t\t\tcontinue;\n\t\t}\n\t\tARCSTAT_BUMP(arcstat_prune);\n\t}\n\tmutex_exit(&arc_prune_mtx);\n}\n\n \nvoid\narc_freed(spa_t *spa, const blkptr_t *bp)\n{\n\tarc_buf_hdr_t *hdr;\n\tkmutex_t *hash_lock;\n\tuint64_t guid = spa_load_guid(spa);\n\n\tASSERT(!BP_IS_EMBEDDED(bp));\n\n\thdr = buf_hash_find(guid, bp, &hash_lock);\n\tif (hdr == NULL)\n\t\treturn;\n\n\t \n\tif (!HDR_HAS_L1HDR(hdr) ||\n\t    zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt)) {\n\t\tarc_change_state(arc_anon, hdr);\n\t\tarc_hdr_destroy(hdr);\n\t\tmutex_exit(hash_lock);\n\t} else {\n\t\tmutex_exit(hash_lock);\n\t}\n\n}\n\n \nvoid\narc_release(arc_buf_t *buf, const void *tag)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\t \n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\t \n\tif (hdr->b_l1hdr.b_state == arc_anon) {\n\t\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\t\tASSERT(!HDR_IN_HASH_TABLE(hdr));\n\t\tASSERT(!HDR_HAS_L2HDR(hdr));\n\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, buf);\n\t\tASSERT(ARC_BUF_LAST(buf));\n\t\tASSERT3S(zfs_refcount_count(&hdr->b_l1hdr.b_refcnt), ==, 1);\n\t\tASSERT(!multilist_link_active(&hdr->b_l1hdr.b_arc_node));\n\n\t\thdr->b_l1hdr.b_arc_access = 0;\n\n\t\t \n\t\tbuf_discard_identity(hdr);\n\t\tarc_buf_thaw(buf);\n\n\t\treturn;\n\t}\n\n\tkmutex_t *hash_lock = HDR_LOCK(hdr);\n\tmutex_enter(hash_lock);\n\n\t \n\tarc_state_t *state = hdr->b_l1hdr.b_state;\n\tASSERT3P(hash_lock, ==, HDR_LOCK(hdr));\n\tASSERT3P(state, !=, arc_anon);\n\n\t \n\tASSERT3S(zfs_refcount_count(&hdr->b_l1hdr.b_refcnt), >, 0);\n\n\tif (HDR_HAS_L2HDR(hdr)) {\n\t\tmutex_enter(&hdr->b_l2hdr.b_dev->l2ad_mtx);\n\n\t\t \n\t\tif (HDR_HAS_L2HDR(hdr))\n\t\t\tarc_hdr_l2hdr_destroy(hdr);\n\n\t\tmutex_exit(&hdr->b_l2hdr.b_dev->l2ad_mtx);\n\t}\n\n\t \n\tif (hdr->b_l1hdr.b_buf != buf || !ARC_BUF_LAST(buf)) {\n\t\tarc_buf_hdr_t *nhdr;\n\t\tuint64_t spa = hdr->b_spa;\n\t\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\t\tuint64_t lsize = HDR_GET_LSIZE(hdr);\n\t\tboolean_t protected = HDR_PROTECTED(hdr);\n\t\tenum zio_compress compress = arc_hdr_get_compress(hdr);\n\t\tarc_buf_contents_t type = arc_buf_type(hdr);\n\t\tVERIFY3U(hdr->b_type, ==, type);\n\n\t\tASSERT(hdr->b_l1hdr.b_buf != buf || buf->b_next != NULL);\n\t\tVERIFY3S(remove_reference(hdr, tag), >, 0);\n\n\t\tif (ARC_BUF_SHARED(buf) && !ARC_BUF_COMPRESSED(buf)) {\n\t\t\tASSERT3P(hdr->b_l1hdr.b_buf, !=, buf);\n\t\t\tASSERT(ARC_BUF_LAST(buf));\n\t\t}\n\n\t\t \n\t\tarc_buf_t *lastbuf = arc_buf_remove(hdr, buf);\n\t\tASSERT3P(lastbuf, !=, NULL);\n\n\t\t \n\t\tif (ARC_BUF_SHARED(buf)) {\n\t\t\tASSERT3P(hdr->b_l1hdr.b_buf, !=, buf);\n\t\t\tASSERT(!arc_buf_is_shared(lastbuf));\n\n\t\t\t \n\t\t\tarc_unshare_buf(hdr, buf);\n\n\t\t\t \n\t\t\tif (arc_can_share(hdr, lastbuf)) {\n\t\t\t\tarc_share_buf(hdr, lastbuf);\n\t\t\t} else {\n\t\t\t\tarc_hdr_alloc_abd(hdr, 0);\n\t\t\t\tabd_copy_from_buf(hdr->b_l1hdr.b_pabd,\n\t\t\t\t    buf->b_data, psize);\n\t\t\t}\n\t\t\tVERIFY3P(lastbuf->b_data, !=, NULL);\n\t\t} else if (HDR_SHARED_DATA(hdr)) {\n\t\t\t \n\t\t\tASSERT(arc_buf_is_shared(lastbuf) ||\n\t\t\t    arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF);\n\t\t\tASSERT(!arc_buf_is_shared(buf));\n\t\t}\n\n\t\tASSERT(hdr->b_l1hdr.b_pabd != NULL || HDR_HAS_RABD(hdr));\n\t\tASSERT3P(state, !=, arc_l2c_only);\n\n\t\t(void) zfs_refcount_remove_many(&state->arcs_size[type],\n\t\t    arc_buf_size(buf), buf);\n\n\t\tif (zfs_refcount_is_zero(&hdr->b_l1hdr.b_refcnt)) {\n\t\t\tASSERT3P(state, !=, arc_l2c_only);\n\t\t\t(void) zfs_refcount_remove_many(\n\t\t\t    &state->arcs_esize[type],\n\t\t\t    arc_buf_size(buf), buf);\n\t\t}\n\n\t\tarc_cksum_verify(buf);\n\t\tarc_buf_unwatch(buf);\n\n\t\t \n\t\tif (!arc_hdr_has_uncompressed_buf(hdr))\n\t\t\tarc_cksum_free(hdr);\n\n\t\tmutex_exit(hash_lock);\n\n\t\tnhdr = arc_hdr_alloc(spa, psize, lsize, protected,\n\t\t    compress, hdr->b_complevel, type);\n\t\tASSERT3P(nhdr->b_l1hdr.b_buf, ==, NULL);\n\t\tASSERT0(zfs_refcount_count(&nhdr->b_l1hdr.b_refcnt));\n\t\tVERIFY3U(nhdr->b_type, ==, type);\n\t\tASSERT(!HDR_SHARED_DATA(nhdr));\n\n\t\tnhdr->b_l1hdr.b_buf = buf;\n\t\t(void) zfs_refcount_add(&nhdr->b_l1hdr.b_refcnt, tag);\n\t\tbuf->b_hdr = nhdr;\n\n\t\t(void) zfs_refcount_add_many(&arc_anon->arcs_size[type],\n\t\t    arc_buf_size(buf), buf);\n\t} else {\n\t\tASSERT(zfs_refcount_count(&hdr->b_l1hdr.b_refcnt) == 1);\n\t\t \n\t\tASSERT(!multilist_link_active(&hdr->b_l1hdr.b_arc_node));\n\t\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\t\thdr->b_l1hdr.b_mru_hits = 0;\n\t\thdr->b_l1hdr.b_mru_ghost_hits = 0;\n\t\thdr->b_l1hdr.b_mfu_hits = 0;\n\t\thdr->b_l1hdr.b_mfu_ghost_hits = 0;\n\t\tarc_change_state(arc_anon, hdr);\n\t\thdr->b_l1hdr.b_arc_access = 0;\n\n\t\tmutex_exit(hash_lock);\n\t\tbuf_discard_identity(hdr);\n\t\tarc_buf_thaw(buf);\n\t}\n}\n\nint\narc_released(arc_buf_t *buf)\n{\n\treturn (buf->b_data != NULL &&\n\t    buf->b_hdr->b_l1hdr.b_state == arc_anon);\n}\n\n#ifdef ZFS_DEBUG\nint\narc_referenced(arc_buf_t *buf)\n{\n\treturn (zfs_refcount_count(&buf->b_hdr->b_l1hdr.b_refcnt));\n}\n#endif\n\nstatic void\narc_write_ready(zio_t *zio)\n{\n\tarc_write_callback_t *callback = zio->io_private;\n\tarc_buf_t *buf = callback->awcb_buf;\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\tblkptr_t *bp = zio->io_bp;\n\tuint64_t psize = BP_IS_HOLE(bp) ? 0 : BP_GET_PSIZE(bp);\n\tfstrans_cookie_t cookie = spl_fstrans_mark();\n\n\tASSERT(HDR_HAS_L1HDR(hdr));\n\tASSERT(!zfs_refcount_is_zero(&buf->b_hdr->b_l1hdr.b_refcnt));\n\tASSERT3P(hdr->b_l1hdr.b_buf, !=, NULL);\n\n\t \n\tif (zio->io_flags & ZIO_FLAG_REEXECUTED) {\n\t\tarc_cksum_free(hdr);\n\t\tarc_buf_unwatch(buf);\n\t\tif (hdr->b_l1hdr.b_pabd != NULL) {\n\t\t\tif (ARC_BUF_SHARED(buf)) {\n\t\t\t\tarc_unshare_buf(hdr, buf);\n\t\t\t} else {\n\t\t\t\tASSERT(!arc_buf_is_shared(buf));\n\t\t\t\tarc_hdr_free_abd(hdr, B_FALSE);\n\t\t\t}\n\t\t}\n\n\t\tif (HDR_HAS_RABD(hdr))\n\t\t\tarc_hdr_free_abd(hdr, B_TRUE);\n\t}\n\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\tASSERT(!HDR_HAS_RABD(hdr));\n\tASSERT(!HDR_SHARED_DATA(hdr));\n\tASSERT(!arc_buf_is_shared(buf));\n\n\tcallback->awcb_ready(zio, buf, callback->awcb_private);\n\n\tif (HDR_IO_IN_PROGRESS(hdr)) {\n\t\tASSERT(zio->io_flags & ZIO_FLAG_REEXECUTED);\n\t} else {\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_IO_IN_PROGRESS);\n\t\tadd_reference(hdr, hdr);  \n\t}\n\n\tif (BP_IS_PROTECTED(bp)) {\n\t\t \n\t\tASSERT3U(BP_GET_TYPE(bp), !=, DMU_OT_INTENT_LOG);\n\n\t\tif (BP_SHOULD_BYTESWAP(bp)) {\n\t\t\tif (BP_GET_LEVEL(bp) > 0) {\n\t\t\t\thdr->b_l1hdr.b_byteswap = DMU_BSWAP_UINT64;\n\t\t\t} else {\n\t\t\t\thdr->b_l1hdr.b_byteswap =\n\t\t\t\t    DMU_OT_BYTESWAP(BP_GET_TYPE(bp));\n\t\t\t}\n\t\t} else {\n\t\t\thdr->b_l1hdr.b_byteswap = DMU_BSWAP_NUMFUNCS;\n\t\t}\n\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_PROTECTED);\n\t\thdr->b_crypt_hdr.b_ot = BP_GET_TYPE(bp);\n\t\thdr->b_crypt_hdr.b_dsobj = zio->io_bookmark.zb_objset;\n\t\tzio_crypt_decode_params_bp(bp, hdr->b_crypt_hdr.b_salt,\n\t\t    hdr->b_crypt_hdr.b_iv);\n\t\tzio_crypt_decode_mac_bp(bp, hdr->b_crypt_hdr.b_mac);\n\t} else {\n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_PROTECTED);\n\t}\n\n\t \n\tif (BP_IS_AUTHENTICATED(bp) && ARC_BUF_ENCRYPTED(buf)) {\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_NOAUTH);\n\t\tbuf->b_flags &= ~ARC_BUF_FLAG_ENCRYPTED;\n\t\tif (BP_GET_COMPRESS(bp) == ZIO_COMPRESS_OFF)\n\t\t\tbuf->b_flags &= ~ARC_BUF_FLAG_COMPRESSED;\n\t} else if (BP_IS_HOLE(bp) && ARC_BUF_ENCRYPTED(buf)) {\n\t\tbuf->b_flags &= ~ARC_BUF_FLAG_ENCRYPTED;\n\t\tbuf->b_flags &= ~ARC_BUF_FLAG_COMPRESSED;\n\t}\n\n\t \n\tarc_cksum_compute(buf);\n\n\tenum zio_compress compress;\n\tif (BP_IS_HOLE(bp) || BP_IS_EMBEDDED(bp)) {\n\t\tcompress = ZIO_COMPRESS_OFF;\n\t} else {\n\t\tASSERT3U(HDR_GET_LSIZE(hdr), ==, BP_GET_LSIZE(bp));\n\t\tcompress = BP_GET_COMPRESS(bp);\n\t}\n\tHDR_SET_PSIZE(hdr, psize);\n\tarc_hdr_set_compress(hdr, compress);\n\thdr->b_complevel = zio->io_prop.zp_complevel;\n\n\tif (zio->io_error != 0 || psize == 0)\n\t\tgoto out;\n\n\t \n\tif (ARC_BUF_ENCRYPTED(buf)) {\n\t\tASSERT3U(psize, >, 0);\n\t\tASSERT(ARC_BUF_COMPRESSED(buf));\n\t\tarc_hdr_alloc_abd(hdr, ARC_HDR_ALLOC_RDATA |\n\t\t    ARC_HDR_USE_RESERVE);\n\t\tabd_copy(hdr->b_crypt_hdr.b_rabd, zio->io_abd, psize);\n\t} else if (!(HDR_UNCACHED(hdr) ||\n\t    abd_size_alloc_linear(arc_buf_size(buf))) ||\n\t    !arc_can_share(hdr, buf)) {\n\t\t \n\t\tif (BP_IS_ENCRYPTED(bp)) {\n\t\t\tASSERT3U(psize, >, 0);\n\t\t\tarc_hdr_alloc_abd(hdr, ARC_HDR_ALLOC_RDATA |\n\t\t\t    ARC_HDR_USE_RESERVE);\n\t\t\tabd_copy(hdr->b_crypt_hdr.b_rabd, zio->io_abd, psize);\n\t\t} else if (arc_hdr_get_compress(hdr) != ZIO_COMPRESS_OFF &&\n\t\t    !ARC_BUF_COMPRESSED(buf)) {\n\t\t\tASSERT3U(psize, >, 0);\n\t\t\tarc_hdr_alloc_abd(hdr, ARC_HDR_USE_RESERVE);\n\t\t\tabd_copy(hdr->b_l1hdr.b_pabd, zio->io_abd, psize);\n\t\t} else {\n\t\t\tASSERT3U(zio->io_orig_size, ==, arc_hdr_size(hdr));\n\t\t\tarc_hdr_alloc_abd(hdr, ARC_HDR_USE_RESERVE);\n\t\t\tabd_copy_from_buf(hdr->b_l1hdr.b_pabd, buf->b_data,\n\t\t\t    arc_buf_size(buf));\n\t\t}\n\t} else {\n\t\tASSERT3P(buf->b_data, ==, abd_to_buf(zio->io_orig_abd));\n\t\tASSERT3U(zio->io_orig_size, ==, arc_buf_size(buf));\n\t\tASSERT3P(hdr->b_l1hdr.b_buf, ==, buf);\n\t\tASSERT(ARC_BUF_LAST(buf));\n\n\t\tarc_share_buf(hdr, buf);\n\t}\n\nout:\n\tarc_hdr_verify(hdr, bp);\n\tspl_fstrans_unmark(cookie);\n}\n\nstatic void\narc_write_children_ready(zio_t *zio)\n{\n\tarc_write_callback_t *callback = zio->io_private;\n\tarc_buf_t *buf = callback->awcb_buf;\n\n\tcallback->awcb_children_ready(zio, buf, callback->awcb_private);\n}\n\nstatic void\narc_write_done(zio_t *zio)\n{\n\tarc_write_callback_t *callback = zio->io_private;\n\tarc_buf_t *buf = callback->awcb_buf;\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\n\tASSERT3P(hdr->b_l1hdr.b_acb, ==, NULL);\n\n\tif (zio->io_error == 0) {\n\t\tarc_hdr_verify(hdr, zio->io_bp);\n\n\t\tif (BP_IS_HOLE(zio->io_bp) || BP_IS_EMBEDDED(zio->io_bp)) {\n\t\t\tbuf_discard_identity(hdr);\n\t\t} else {\n\t\t\thdr->b_dva = *BP_IDENTITY(zio->io_bp);\n\t\t\thdr->b_birth = BP_PHYSICAL_BIRTH(zio->io_bp);\n\t\t}\n\t} else {\n\t\tASSERT(HDR_EMPTY(hdr));\n\t}\n\n\t \n\tif (!HDR_EMPTY(hdr)) {\n\t\tarc_buf_hdr_t *exists;\n\t\tkmutex_t *hash_lock;\n\n\t\tASSERT3U(zio->io_error, ==, 0);\n\n\t\tarc_cksum_verify(buf);\n\n\t\texists = buf_hash_insert(hdr, &hash_lock);\n\t\tif (exists != NULL) {\n\t\t\t \n\t\t\tif (zio->io_flags & ZIO_FLAG_IO_REWRITE) {\n\t\t\t\tif (!BP_EQUAL(&zio->io_bp_orig, zio->io_bp))\n\t\t\t\t\tpanic(\"bad overwrite, hdr=%p exists=%p\",\n\t\t\t\t\t    (void *)hdr, (void *)exists);\n\t\t\t\tASSERT(zfs_refcount_is_zero(\n\t\t\t\t    &exists->b_l1hdr.b_refcnt));\n\t\t\t\tarc_change_state(arc_anon, exists);\n\t\t\t\tarc_hdr_destroy(exists);\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\texists = buf_hash_insert(hdr, &hash_lock);\n\t\t\t\tASSERT3P(exists, ==, NULL);\n\t\t\t} else if (zio->io_flags & ZIO_FLAG_NOPWRITE) {\n\t\t\t\t \n\t\t\t\tASSERT(zio->io_prop.zp_nopwrite);\n\t\t\t\tif (!BP_EQUAL(&zio->io_bp_orig, zio->io_bp))\n\t\t\t\t\tpanic(\"bad nopwrite, hdr=%p exists=%p\",\n\t\t\t\t\t    (void *)hdr, (void *)exists);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tASSERT3P(hdr->b_l1hdr.b_buf, !=, NULL);\n\t\t\t\tASSERT(ARC_BUF_LAST(hdr->b_l1hdr.b_buf));\n\t\t\t\tASSERT(hdr->b_l1hdr.b_state == arc_anon);\n\t\t\t\tASSERT(BP_GET_DEDUP(zio->io_bp));\n\t\t\t\tASSERT(BP_GET_LEVEL(zio->io_bp) == 0);\n\t\t\t}\n\t\t}\n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_IO_IN_PROGRESS);\n\t\tVERIFY3S(remove_reference(hdr, hdr), >, 0);\n\t\t \n\t\tif (exists == NULL && hdr->b_l1hdr.b_state == arc_anon)\n\t\t\tarc_access(hdr, 0, B_FALSE);\n\t\tmutex_exit(hash_lock);\n\t} else {\n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_IO_IN_PROGRESS);\n\t\tVERIFY3S(remove_reference(hdr, hdr), >, 0);\n\t}\n\n\tcallback->awcb_done(zio, buf, callback->awcb_private);\n\n\tabd_free(zio->io_abd);\n\tkmem_free(callback, sizeof (arc_write_callback_t));\n}\n\nzio_t *\narc_write(zio_t *pio, spa_t *spa, uint64_t txg,\n    blkptr_t *bp, arc_buf_t *buf, boolean_t uncached, boolean_t l2arc,\n    const zio_prop_t *zp, arc_write_done_func_t *ready,\n    arc_write_done_func_t *children_ready, arc_write_done_func_t *done,\n    void *private, zio_priority_t priority, int zio_flags,\n    const zbookmark_phys_t *zb)\n{\n\tarc_buf_hdr_t *hdr = buf->b_hdr;\n\tarc_write_callback_t *callback;\n\tzio_t *zio;\n\tzio_prop_t localprop = *zp;\n\n\tASSERT3P(ready, !=, NULL);\n\tASSERT3P(done, !=, NULL);\n\tASSERT(!HDR_IO_ERROR(hdr));\n\tASSERT(!HDR_IO_IN_PROGRESS(hdr));\n\tASSERT3P(hdr->b_l1hdr.b_acb, ==, NULL);\n\tASSERT3P(hdr->b_l1hdr.b_buf, !=, NULL);\n\tif (uncached)\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_UNCACHED);\n\telse if (l2arc)\n\t\tarc_hdr_set_flags(hdr, ARC_FLAG_L2CACHE);\n\n\tif (ARC_BUF_ENCRYPTED(buf)) {\n\t\tASSERT(ARC_BUF_COMPRESSED(buf));\n\t\tlocalprop.zp_encrypt = B_TRUE;\n\t\tlocalprop.zp_compress = HDR_GET_COMPRESS(hdr);\n\t\tlocalprop.zp_complevel = hdr->b_complevel;\n\t\tlocalprop.zp_byteorder =\n\t\t    (hdr->b_l1hdr.b_byteswap == DMU_BSWAP_NUMFUNCS) ?\n\t\t    ZFS_HOST_BYTEORDER : !ZFS_HOST_BYTEORDER;\n\t\tmemcpy(localprop.zp_salt, hdr->b_crypt_hdr.b_salt,\n\t\t    ZIO_DATA_SALT_LEN);\n\t\tmemcpy(localprop.zp_iv, hdr->b_crypt_hdr.b_iv,\n\t\t    ZIO_DATA_IV_LEN);\n\t\tmemcpy(localprop.zp_mac, hdr->b_crypt_hdr.b_mac,\n\t\t    ZIO_DATA_MAC_LEN);\n\t\tif (DMU_OT_IS_ENCRYPTED(localprop.zp_type)) {\n\t\t\tlocalprop.zp_nopwrite = B_FALSE;\n\t\t\tlocalprop.zp_copies =\n\t\t\t    MIN(localprop.zp_copies, SPA_DVAS_PER_BP - 1);\n\t\t}\n\t\tzio_flags |= ZIO_FLAG_RAW;\n\t} else if (ARC_BUF_COMPRESSED(buf)) {\n\t\tASSERT3U(HDR_GET_LSIZE(hdr), !=, arc_buf_size(buf));\n\t\tlocalprop.zp_compress = HDR_GET_COMPRESS(hdr);\n\t\tlocalprop.zp_complevel = hdr->b_complevel;\n\t\tzio_flags |= ZIO_FLAG_RAW_COMPRESS;\n\t}\n\tcallback = kmem_zalloc(sizeof (arc_write_callback_t), KM_SLEEP);\n\tcallback->awcb_ready = ready;\n\tcallback->awcb_children_ready = children_ready;\n\tcallback->awcb_done = done;\n\tcallback->awcb_private = private;\n\tcallback->awcb_buf = buf;\n\n\t \n\tif (hdr->b_l1hdr.b_pabd != NULL) {\n\t\t \n\t\tif (ARC_BUF_SHARED(buf)) {\n\t\t\tarc_unshare_buf(hdr, buf);\n\t\t} else {\n\t\t\tASSERT(!arc_buf_is_shared(buf));\n\t\t\tarc_hdr_free_abd(hdr, B_FALSE);\n\t\t}\n\t\tVERIFY3P(buf->b_data, !=, NULL);\n\t}\n\n\tif (HDR_HAS_RABD(hdr))\n\t\tarc_hdr_free_abd(hdr, B_TRUE);\n\n\tif (!(zio_flags & ZIO_FLAG_RAW))\n\t\tarc_hdr_set_compress(hdr, ZIO_COMPRESS_OFF);\n\n\tASSERT(!arc_buf_is_shared(buf));\n\tASSERT3P(hdr->b_l1hdr.b_pabd, ==, NULL);\n\n\tzio = zio_write(pio, spa, txg, bp,\n\t    abd_get_from_buf(buf->b_data, HDR_GET_LSIZE(hdr)),\n\t    HDR_GET_LSIZE(hdr), arc_buf_size(buf), &localprop, arc_write_ready,\n\t    (children_ready != NULL) ? arc_write_children_ready : NULL,\n\t    arc_write_done, callback, priority, zio_flags, zb);\n\n\treturn (zio);\n}\n\nvoid\narc_tempreserve_clear(uint64_t reserve)\n{\n\tatomic_add_64(&arc_tempreserve, -reserve);\n\tASSERT((int64_t)arc_tempreserve >= 0);\n}\n\nint\narc_tempreserve_space(spa_t *spa, uint64_t reserve, uint64_t txg)\n{\n\tint error;\n\tuint64_t anon_size;\n\n\tif (!arc_no_grow &&\n\t    reserve > arc_c/4 &&\n\t    reserve * 4 > (2ULL << SPA_MAXBLOCKSHIFT))\n\t\tarc_c = MIN(arc_c_max, reserve * 4);\n\n\t \n\tif (reserve > arc_c) {\n\t\tDMU_TX_STAT_BUMP(dmu_tx_memory_reserve);\n\t\treturn (SET_ERROR(ERESTART));\n\t}\n\n\t \n\n\t \n\tASSERT3S(atomic_add_64_nv(&arc_loaned_bytes, 0), >=, 0);\n\n\tanon_size = MAX((int64_t)\n\t    (zfs_refcount_count(&arc_anon->arcs_size[ARC_BUFC_DATA]) +\n\t    zfs_refcount_count(&arc_anon->arcs_size[ARC_BUFC_METADATA]) -\n\t    arc_loaned_bytes), 0);\n\n\t \n\terror = arc_memory_throttle(spa, reserve, txg);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\tuint64_t total_dirty = reserve + arc_tempreserve + anon_size;\n\tuint64_t spa_dirty_anon = spa_dirty_data(spa);\n\tuint64_t rarc_c = arc_warm ? arc_c : arc_c_max;\n\tif (total_dirty > rarc_c * zfs_arc_dirty_limit_percent / 100 &&\n\t    anon_size > rarc_c * zfs_arc_anon_limit_percent / 100 &&\n\t    spa_dirty_anon > anon_size * zfs_arc_pool_dirty_percent / 100) {\n#ifdef ZFS_DEBUG\n\t\tuint64_t meta_esize = zfs_refcount_count(\n\t\t    &arc_anon->arcs_esize[ARC_BUFC_METADATA]);\n\t\tuint64_t data_esize =\n\t\t    zfs_refcount_count(&arc_anon->arcs_esize[ARC_BUFC_DATA]);\n\t\tdprintf(\"failing, arc_tempreserve=%lluK anon_meta=%lluK \"\n\t\t    \"anon_data=%lluK tempreserve=%lluK rarc_c=%lluK\\n\",\n\t\t    (u_longlong_t)arc_tempreserve >> 10,\n\t\t    (u_longlong_t)meta_esize >> 10,\n\t\t    (u_longlong_t)data_esize >> 10,\n\t\t    (u_longlong_t)reserve >> 10,\n\t\t    (u_longlong_t)rarc_c >> 10);\n#endif\n\t\tDMU_TX_STAT_BUMP(dmu_tx_dirty_throttle);\n\t\treturn (SET_ERROR(ERESTART));\n\t}\n\tatomic_add_64(&arc_tempreserve, reserve);\n\treturn (0);\n}\n\nstatic void\narc_kstat_update_state(arc_state_t *state, kstat_named_t *size,\n    kstat_named_t *data, kstat_named_t *metadata,\n    kstat_named_t *evict_data, kstat_named_t *evict_metadata)\n{\n\tdata->value.ui64 =\n\t    zfs_refcount_count(&state->arcs_size[ARC_BUFC_DATA]);\n\tmetadata->value.ui64 =\n\t    zfs_refcount_count(&state->arcs_size[ARC_BUFC_METADATA]);\n\tsize->value.ui64 = data->value.ui64 + metadata->value.ui64;\n\tevict_data->value.ui64 =\n\t    zfs_refcount_count(&state->arcs_esize[ARC_BUFC_DATA]);\n\tevict_metadata->value.ui64 =\n\t    zfs_refcount_count(&state->arcs_esize[ARC_BUFC_METADATA]);\n}\n\nstatic int\narc_kstat_update(kstat_t *ksp, int rw)\n{\n\tarc_stats_t *as = ksp->ks_data;\n\n\tif (rw == KSTAT_WRITE)\n\t\treturn (SET_ERROR(EACCES));\n\n\tas->arcstat_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_hits);\n\tas->arcstat_iohits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_iohits);\n\tas->arcstat_misses.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_misses);\n\tas->arcstat_demand_data_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_data_hits);\n\tas->arcstat_demand_data_iohits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_data_iohits);\n\tas->arcstat_demand_data_misses.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_data_misses);\n\tas->arcstat_demand_metadata_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_metadata_hits);\n\tas->arcstat_demand_metadata_iohits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_metadata_iohits);\n\tas->arcstat_demand_metadata_misses.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_metadata_misses);\n\tas->arcstat_prefetch_data_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prefetch_data_hits);\n\tas->arcstat_prefetch_data_iohits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prefetch_data_iohits);\n\tas->arcstat_prefetch_data_misses.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prefetch_data_misses);\n\tas->arcstat_prefetch_metadata_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prefetch_metadata_hits);\n\tas->arcstat_prefetch_metadata_iohits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prefetch_metadata_iohits);\n\tas->arcstat_prefetch_metadata_misses.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prefetch_metadata_misses);\n\tas->arcstat_mru_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_mru_hits);\n\tas->arcstat_mru_ghost_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_mru_ghost_hits);\n\tas->arcstat_mfu_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_mfu_hits);\n\tas->arcstat_mfu_ghost_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_mfu_ghost_hits);\n\tas->arcstat_uncached_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_uncached_hits);\n\tas->arcstat_deleted.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_deleted);\n\tas->arcstat_mutex_miss.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_mutex_miss);\n\tas->arcstat_access_skip.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_access_skip);\n\tas->arcstat_evict_skip.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_skip);\n\tas->arcstat_evict_not_enough.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_not_enough);\n\tas->arcstat_evict_l2_cached.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_l2_cached);\n\tas->arcstat_evict_l2_eligible.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_l2_eligible);\n\tas->arcstat_evict_l2_eligible_mfu.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_l2_eligible_mfu);\n\tas->arcstat_evict_l2_eligible_mru.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_l2_eligible_mru);\n\tas->arcstat_evict_l2_ineligible.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_l2_ineligible);\n\tas->arcstat_evict_l2_skip.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_evict_l2_skip);\n\tas->arcstat_hash_collisions.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_hash_collisions);\n\tas->arcstat_hash_chains.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_hash_chains);\n\tas->arcstat_size.value.ui64 =\n\t    aggsum_value(&arc_sums.arcstat_size);\n\tas->arcstat_compressed_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_compressed_size);\n\tas->arcstat_uncompressed_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_uncompressed_size);\n\tas->arcstat_overhead_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_overhead_size);\n\tas->arcstat_hdr_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_hdr_size);\n\tas->arcstat_data_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_data_size);\n\tas->arcstat_metadata_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_metadata_size);\n\tas->arcstat_dbuf_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_dbuf_size);\n#if defined(COMPAT_FREEBSD11)\n\tas->arcstat_other_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_bonus_size) +\n\t    wmsum_value(&arc_sums.arcstat_dnode_size) +\n\t    wmsum_value(&arc_sums.arcstat_dbuf_size);\n#endif\n\n\tarc_kstat_update_state(arc_anon,\n\t    &as->arcstat_anon_size,\n\t    &as->arcstat_anon_data,\n\t    &as->arcstat_anon_metadata,\n\t    &as->arcstat_anon_evictable_data,\n\t    &as->arcstat_anon_evictable_metadata);\n\tarc_kstat_update_state(arc_mru,\n\t    &as->arcstat_mru_size,\n\t    &as->arcstat_mru_data,\n\t    &as->arcstat_mru_metadata,\n\t    &as->arcstat_mru_evictable_data,\n\t    &as->arcstat_mru_evictable_metadata);\n\tarc_kstat_update_state(arc_mru_ghost,\n\t    &as->arcstat_mru_ghost_size,\n\t    &as->arcstat_mru_ghost_data,\n\t    &as->arcstat_mru_ghost_metadata,\n\t    &as->arcstat_mru_ghost_evictable_data,\n\t    &as->arcstat_mru_ghost_evictable_metadata);\n\tarc_kstat_update_state(arc_mfu,\n\t    &as->arcstat_mfu_size,\n\t    &as->arcstat_mfu_data,\n\t    &as->arcstat_mfu_metadata,\n\t    &as->arcstat_mfu_evictable_data,\n\t    &as->arcstat_mfu_evictable_metadata);\n\tarc_kstat_update_state(arc_mfu_ghost,\n\t    &as->arcstat_mfu_ghost_size,\n\t    &as->arcstat_mfu_ghost_data,\n\t    &as->arcstat_mfu_ghost_metadata,\n\t    &as->arcstat_mfu_ghost_evictable_data,\n\t    &as->arcstat_mfu_ghost_evictable_metadata);\n\tarc_kstat_update_state(arc_uncached,\n\t    &as->arcstat_uncached_size,\n\t    &as->arcstat_uncached_data,\n\t    &as->arcstat_uncached_metadata,\n\t    &as->arcstat_uncached_evictable_data,\n\t    &as->arcstat_uncached_evictable_metadata);\n\n\tas->arcstat_dnode_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_dnode_size);\n\tas->arcstat_bonus_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_bonus_size);\n\tas->arcstat_l2_hits.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_hits);\n\tas->arcstat_l2_misses.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_misses);\n\tas->arcstat_l2_prefetch_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_prefetch_asize);\n\tas->arcstat_l2_mru_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_mru_asize);\n\tas->arcstat_l2_mfu_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_mfu_asize);\n\tas->arcstat_l2_bufc_data_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_bufc_data_asize);\n\tas->arcstat_l2_bufc_metadata_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_bufc_metadata_asize);\n\tas->arcstat_l2_feeds.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_feeds);\n\tas->arcstat_l2_rw_clash.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rw_clash);\n\tas->arcstat_l2_read_bytes.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_read_bytes);\n\tas->arcstat_l2_write_bytes.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_write_bytes);\n\tas->arcstat_l2_writes_sent.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_writes_sent);\n\tas->arcstat_l2_writes_done.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_writes_done);\n\tas->arcstat_l2_writes_error.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_writes_error);\n\tas->arcstat_l2_writes_lock_retry.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_writes_lock_retry);\n\tas->arcstat_l2_evict_lock_retry.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_evict_lock_retry);\n\tas->arcstat_l2_evict_reading.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_evict_reading);\n\tas->arcstat_l2_evict_l1cached.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_evict_l1cached);\n\tas->arcstat_l2_free_on_write.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_free_on_write);\n\tas->arcstat_l2_abort_lowmem.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_abort_lowmem);\n\tas->arcstat_l2_cksum_bad.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_cksum_bad);\n\tas->arcstat_l2_io_error.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_io_error);\n\tas->arcstat_l2_lsize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_lsize);\n\tas->arcstat_l2_psize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_psize);\n\tas->arcstat_l2_hdr_size.value.ui64 =\n\t    aggsum_value(&arc_sums.arcstat_l2_hdr_size);\n\tas->arcstat_l2_log_blk_writes.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_log_blk_writes);\n\tas->arcstat_l2_log_blk_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_log_blk_asize);\n\tas->arcstat_l2_log_blk_count.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_log_blk_count);\n\tas->arcstat_l2_rebuild_success.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_success);\n\tas->arcstat_l2_rebuild_abort_unsupported.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_abort_unsupported);\n\tas->arcstat_l2_rebuild_abort_io_errors.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_abort_io_errors);\n\tas->arcstat_l2_rebuild_abort_dh_errors.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_abort_dh_errors);\n\tas->arcstat_l2_rebuild_abort_cksum_lb_errors.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_abort_cksum_lb_errors);\n\tas->arcstat_l2_rebuild_abort_lowmem.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_abort_lowmem);\n\tas->arcstat_l2_rebuild_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_size);\n\tas->arcstat_l2_rebuild_asize.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_asize);\n\tas->arcstat_l2_rebuild_bufs.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_bufs);\n\tas->arcstat_l2_rebuild_bufs_precached.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_bufs_precached);\n\tas->arcstat_l2_rebuild_log_blks.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_l2_rebuild_log_blks);\n\tas->arcstat_memory_throttle_count.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_memory_throttle_count);\n\tas->arcstat_memory_direct_count.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_memory_direct_count);\n\tas->arcstat_memory_indirect_count.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_memory_indirect_count);\n\n\tas->arcstat_memory_all_bytes.value.ui64 =\n\t    arc_all_memory();\n\tas->arcstat_memory_free_bytes.value.ui64 =\n\t    arc_free_memory();\n\tas->arcstat_memory_available_bytes.value.i64 =\n\t    arc_available_memory();\n\n\tas->arcstat_prune.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prune);\n\tas->arcstat_meta_used.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_meta_used);\n\tas->arcstat_async_upgrade_sync.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_async_upgrade_sync);\n\tas->arcstat_predictive_prefetch.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_predictive_prefetch);\n\tas->arcstat_demand_hit_predictive_prefetch.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_hit_predictive_prefetch);\n\tas->arcstat_demand_iohit_predictive_prefetch.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_iohit_predictive_prefetch);\n\tas->arcstat_prescient_prefetch.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_prescient_prefetch);\n\tas->arcstat_demand_hit_prescient_prefetch.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_hit_prescient_prefetch);\n\tas->arcstat_demand_iohit_prescient_prefetch.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_demand_iohit_prescient_prefetch);\n\tas->arcstat_raw_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_raw_size);\n\tas->arcstat_cached_only_in_progress.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_cached_only_in_progress);\n\tas->arcstat_abd_chunk_waste_size.value.ui64 =\n\t    wmsum_value(&arc_sums.arcstat_abd_chunk_waste_size);\n\n\treturn (0);\n}\n\n \nstatic unsigned int\narc_state_multilist_index_func(multilist_t *ml, void *obj)\n{\n\tarc_buf_hdr_t *hdr = obj;\n\n\t \n\tASSERT(!HDR_EMPTY(hdr));\n\n\t \n\treturn ((unsigned int)buf_hash(hdr->b_spa, &hdr->b_dva, hdr->b_birth) %\n\t    multilist_get_num_sublists(ml));\n}\n\nstatic unsigned int\narc_state_l2c_multilist_index_func(multilist_t *ml, void *obj)\n{\n\tpanic(\"Header %p insert into arc_l2c_only %p\", obj, ml);\n}\n\n#define\tWARN_IF_TUNING_IGNORED(tuning, value, do_warn) do {\t\\\n\tif ((do_warn) && (tuning) && ((tuning) != (value))) {\t\\\n\t\tcmn_err(CE_WARN,\t\t\t\t\\\n\t\t    \"ignoring tunable %s (using %llu instead)\",\t\\\n\t\t    (#tuning), (u_longlong_t)(value));\t\\\n\t}\t\t\t\t\t\t\t\\\n} while (0)\n\n \nvoid\narc_tuning_update(boolean_t verbose)\n{\n\tuint64_t allmem = arc_all_memory();\n\n\t \n\tif ((zfs_arc_min) && (zfs_arc_min != arc_c_min) &&\n\t    (zfs_arc_min >= 2ULL << SPA_MAXBLOCKSHIFT) &&\n\t    (zfs_arc_min <= arc_c_max)) {\n\t\tarc_c_min = zfs_arc_min;\n\t\tarc_c = MAX(arc_c, arc_c_min);\n\t}\n\tWARN_IF_TUNING_IGNORED(zfs_arc_min, arc_c_min, verbose);\n\n\t \n\tif ((zfs_arc_max) && (zfs_arc_max != arc_c_max) &&\n\t    (zfs_arc_max >= MIN_ARC_MAX) && (zfs_arc_max < allmem) &&\n\t    (zfs_arc_max > arc_c_min)) {\n\t\tarc_c_max = zfs_arc_max;\n\t\tarc_c = MIN(arc_c, arc_c_max);\n\t\tif (arc_dnode_limit > arc_c_max)\n\t\t\tarc_dnode_limit = arc_c_max;\n\t}\n\tWARN_IF_TUNING_IGNORED(zfs_arc_max, arc_c_max, verbose);\n\n\t \n\tarc_dnode_limit = zfs_arc_dnode_limit ? zfs_arc_dnode_limit :\n\t    MIN(zfs_arc_dnode_limit_percent, 100) * arc_c_max / 100;\n\tWARN_IF_TUNING_IGNORED(zfs_arc_dnode_limit, arc_dnode_limit, verbose);\n\n\t \n\tif (zfs_arc_grow_retry)\n\t\tarc_grow_retry = zfs_arc_grow_retry;\n\n\t \n\tif (zfs_arc_shrink_shift) {\n\t\tarc_shrink_shift = zfs_arc_shrink_shift;\n\t\tarc_no_grow_shift = MIN(arc_no_grow_shift, arc_shrink_shift -1);\n\t}\n\n\t \n\tif (zfs_arc_min_prefetch_ms)\n\t\tarc_min_prefetch_ms = zfs_arc_min_prefetch_ms;\n\n\t \n\tif (zfs_arc_min_prescient_prefetch_ms) {\n\t\tarc_min_prescient_prefetch_ms =\n\t\t    zfs_arc_min_prescient_prefetch_ms;\n\t}\n\n\t \n\tif (zfs_arc_lotsfree_percent <= 100)\n\t\tarc_lotsfree_percent = zfs_arc_lotsfree_percent;\n\tWARN_IF_TUNING_IGNORED(zfs_arc_lotsfree_percent, arc_lotsfree_percent,\n\t    verbose);\n\n\t \n\tif ((zfs_arc_sys_free) && (zfs_arc_sys_free != arc_sys_free))\n\t\tarc_sys_free = MIN(zfs_arc_sys_free, allmem);\n\tWARN_IF_TUNING_IGNORED(zfs_arc_sys_free, arc_sys_free, verbose);\n}\n\nstatic void\narc_state_multilist_init(multilist_t *ml,\n    multilist_sublist_index_func_t *index_func, int *maxcountp)\n{\n\tmultilist_create(ml, sizeof (arc_buf_hdr_t),\n\t    offsetof(arc_buf_hdr_t, b_l1hdr.b_arc_node), index_func);\n\t*maxcountp = MAX(*maxcountp, multilist_get_num_sublists(ml));\n}\n\nstatic void\narc_state_init(void)\n{\n\tint num_sublists = 0;\n\n\tarc_state_multilist_init(&arc_mru->arcs_list[ARC_BUFC_METADATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mru->arcs_list[ARC_BUFC_DATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mru_ghost->arcs_list[ARC_BUFC_METADATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mru_ghost->arcs_list[ARC_BUFC_DATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mfu->arcs_list[ARC_BUFC_METADATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mfu->arcs_list[ARC_BUFC_DATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mfu_ghost->arcs_list[ARC_BUFC_METADATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_mfu_ghost->arcs_list[ARC_BUFC_DATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_uncached->arcs_list[ARC_BUFC_METADATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_uncached->arcs_list[ARC_BUFC_DATA],\n\t    arc_state_multilist_index_func, &num_sublists);\n\n\t \n\tarc_state_multilist_init(&arc_l2c_only->arcs_list[ARC_BUFC_METADATA],\n\t    arc_state_l2c_multilist_index_func, &num_sublists);\n\tarc_state_multilist_init(&arc_l2c_only->arcs_list[ARC_BUFC_DATA],\n\t    arc_state_l2c_multilist_index_func, &num_sublists);\n\n\t \n\tarc_state_evict_marker_count = num_sublists;\n\n\tzfs_refcount_create(&arc_anon->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_anon->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mru->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mru->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mru_ghost->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mru_ghost->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mfu->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mfu->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mfu_ghost->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mfu_ghost->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_l2c_only->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_l2c_only->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_uncached->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_uncached->arcs_esize[ARC_BUFC_DATA]);\n\n\tzfs_refcount_create(&arc_anon->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_anon->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mru->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mru->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mru_ghost->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mru_ghost->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mfu->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mfu->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_mfu_ghost->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_mfu_ghost->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_l2c_only->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_l2c_only->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_create(&arc_uncached->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_create(&arc_uncached->arcs_size[ARC_BUFC_METADATA]);\n\n\twmsum_init(&arc_mru_ghost->arcs_hits[ARC_BUFC_DATA], 0);\n\twmsum_init(&arc_mru_ghost->arcs_hits[ARC_BUFC_METADATA], 0);\n\twmsum_init(&arc_mfu_ghost->arcs_hits[ARC_BUFC_DATA], 0);\n\twmsum_init(&arc_mfu_ghost->arcs_hits[ARC_BUFC_METADATA], 0);\n\n\twmsum_init(&arc_sums.arcstat_hits, 0);\n\twmsum_init(&arc_sums.arcstat_iohits, 0);\n\twmsum_init(&arc_sums.arcstat_misses, 0);\n\twmsum_init(&arc_sums.arcstat_demand_data_hits, 0);\n\twmsum_init(&arc_sums.arcstat_demand_data_iohits, 0);\n\twmsum_init(&arc_sums.arcstat_demand_data_misses, 0);\n\twmsum_init(&arc_sums.arcstat_demand_metadata_hits, 0);\n\twmsum_init(&arc_sums.arcstat_demand_metadata_iohits, 0);\n\twmsum_init(&arc_sums.arcstat_demand_metadata_misses, 0);\n\twmsum_init(&arc_sums.arcstat_prefetch_data_hits, 0);\n\twmsum_init(&arc_sums.arcstat_prefetch_data_iohits, 0);\n\twmsum_init(&arc_sums.arcstat_prefetch_data_misses, 0);\n\twmsum_init(&arc_sums.arcstat_prefetch_metadata_hits, 0);\n\twmsum_init(&arc_sums.arcstat_prefetch_metadata_iohits, 0);\n\twmsum_init(&arc_sums.arcstat_prefetch_metadata_misses, 0);\n\twmsum_init(&arc_sums.arcstat_mru_hits, 0);\n\twmsum_init(&arc_sums.arcstat_mru_ghost_hits, 0);\n\twmsum_init(&arc_sums.arcstat_mfu_hits, 0);\n\twmsum_init(&arc_sums.arcstat_mfu_ghost_hits, 0);\n\twmsum_init(&arc_sums.arcstat_uncached_hits, 0);\n\twmsum_init(&arc_sums.arcstat_deleted, 0);\n\twmsum_init(&arc_sums.arcstat_mutex_miss, 0);\n\twmsum_init(&arc_sums.arcstat_access_skip, 0);\n\twmsum_init(&arc_sums.arcstat_evict_skip, 0);\n\twmsum_init(&arc_sums.arcstat_evict_not_enough, 0);\n\twmsum_init(&arc_sums.arcstat_evict_l2_cached, 0);\n\twmsum_init(&arc_sums.arcstat_evict_l2_eligible, 0);\n\twmsum_init(&arc_sums.arcstat_evict_l2_eligible_mfu, 0);\n\twmsum_init(&arc_sums.arcstat_evict_l2_eligible_mru, 0);\n\twmsum_init(&arc_sums.arcstat_evict_l2_ineligible, 0);\n\twmsum_init(&arc_sums.arcstat_evict_l2_skip, 0);\n\twmsum_init(&arc_sums.arcstat_hash_collisions, 0);\n\twmsum_init(&arc_sums.arcstat_hash_chains, 0);\n\taggsum_init(&arc_sums.arcstat_size, 0);\n\twmsum_init(&arc_sums.arcstat_compressed_size, 0);\n\twmsum_init(&arc_sums.arcstat_uncompressed_size, 0);\n\twmsum_init(&arc_sums.arcstat_overhead_size, 0);\n\twmsum_init(&arc_sums.arcstat_hdr_size, 0);\n\twmsum_init(&arc_sums.arcstat_data_size, 0);\n\twmsum_init(&arc_sums.arcstat_metadata_size, 0);\n\twmsum_init(&arc_sums.arcstat_dbuf_size, 0);\n\twmsum_init(&arc_sums.arcstat_dnode_size, 0);\n\twmsum_init(&arc_sums.arcstat_bonus_size, 0);\n\twmsum_init(&arc_sums.arcstat_l2_hits, 0);\n\twmsum_init(&arc_sums.arcstat_l2_misses, 0);\n\twmsum_init(&arc_sums.arcstat_l2_prefetch_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_mru_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_mfu_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_bufc_data_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_bufc_metadata_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_feeds, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rw_clash, 0);\n\twmsum_init(&arc_sums.arcstat_l2_read_bytes, 0);\n\twmsum_init(&arc_sums.arcstat_l2_write_bytes, 0);\n\twmsum_init(&arc_sums.arcstat_l2_writes_sent, 0);\n\twmsum_init(&arc_sums.arcstat_l2_writes_done, 0);\n\twmsum_init(&arc_sums.arcstat_l2_writes_error, 0);\n\twmsum_init(&arc_sums.arcstat_l2_writes_lock_retry, 0);\n\twmsum_init(&arc_sums.arcstat_l2_evict_lock_retry, 0);\n\twmsum_init(&arc_sums.arcstat_l2_evict_reading, 0);\n\twmsum_init(&arc_sums.arcstat_l2_evict_l1cached, 0);\n\twmsum_init(&arc_sums.arcstat_l2_free_on_write, 0);\n\twmsum_init(&arc_sums.arcstat_l2_abort_lowmem, 0);\n\twmsum_init(&arc_sums.arcstat_l2_cksum_bad, 0);\n\twmsum_init(&arc_sums.arcstat_l2_io_error, 0);\n\twmsum_init(&arc_sums.arcstat_l2_lsize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_psize, 0);\n\taggsum_init(&arc_sums.arcstat_l2_hdr_size, 0);\n\twmsum_init(&arc_sums.arcstat_l2_log_blk_writes, 0);\n\twmsum_init(&arc_sums.arcstat_l2_log_blk_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_log_blk_count, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_success, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_abort_unsupported, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_abort_io_errors, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_abort_dh_errors, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_abort_cksum_lb_errors, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_abort_lowmem, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_size, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_asize, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_bufs, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_bufs_precached, 0);\n\twmsum_init(&arc_sums.arcstat_l2_rebuild_log_blks, 0);\n\twmsum_init(&arc_sums.arcstat_memory_throttle_count, 0);\n\twmsum_init(&arc_sums.arcstat_memory_direct_count, 0);\n\twmsum_init(&arc_sums.arcstat_memory_indirect_count, 0);\n\twmsum_init(&arc_sums.arcstat_prune, 0);\n\twmsum_init(&arc_sums.arcstat_meta_used, 0);\n\twmsum_init(&arc_sums.arcstat_async_upgrade_sync, 0);\n\twmsum_init(&arc_sums.arcstat_predictive_prefetch, 0);\n\twmsum_init(&arc_sums.arcstat_demand_hit_predictive_prefetch, 0);\n\twmsum_init(&arc_sums.arcstat_demand_iohit_predictive_prefetch, 0);\n\twmsum_init(&arc_sums.arcstat_prescient_prefetch, 0);\n\twmsum_init(&arc_sums.arcstat_demand_hit_prescient_prefetch, 0);\n\twmsum_init(&arc_sums.arcstat_demand_iohit_prescient_prefetch, 0);\n\twmsum_init(&arc_sums.arcstat_raw_size, 0);\n\twmsum_init(&arc_sums.arcstat_cached_only_in_progress, 0);\n\twmsum_init(&arc_sums.arcstat_abd_chunk_waste_size, 0);\n\n\tarc_anon->arcs_state = ARC_STATE_ANON;\n\tarc_mru->arcs_state = ARC_STATE_MRU;\n\tarc_mru_ghost->arcs_state = ARC_STATE_MRU_GHOST;\n\tarc_mfu->arcs_state = ARC_STATE_MFU;\n\tarc_mfu_ghost->arcs_state = ARC_STATE_MFU_GHOST;\n\tarc_l2c_only->arcs_state = ARC_STATE_L2C_ONLY;\n\tarc_uncached->arcs_state = ARC_STATE_UNCACHED;\n}\n\nstatic void\narc_state_fini(void)\n{\n\tzfs_refcount_destroy(&arc_anon->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_anon->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mru->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mru->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mru_ghost->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mru_ghost->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mfu->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mfu->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mfu_ghost->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mfu_ghost->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_l2c_only->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_l2c_only->arcs_esize[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_uncached->arcs_esize[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_uncached->arcs_esize[ARC_BUFC_DATA]);\n\n\tzfs_refcount_destroy(&arc_anon->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_anon->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mru->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mru->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mru_ghost->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mru_ghost->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mfu->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mfu->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_mfu_ghost->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_mfu_ghost->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_l2c_only->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_l2c_only->arcs_size[ARC_BUFC_METADATA]);\n\tzfs_refcount_destroy(&arc_uncached->arcs_size[ARC_BUFC_DATA]);\n\tzfs_refcount_destroy(&arc_uncached->arcs_size[ARC_BUFC_METADATA]);\n\n\tmultilist_destroy(&arc_mru->arcs_list[ARC_BUFC_METADATA]);\n\tmultilist_destroy(&arc_mru_ghost->arcs_list[ARC_BUFC_METADATA]);\n\tmultilist_destroy(&arc_mfu->arcs_list[ARC_BUFC_METADATA]);\n\tmultilist_destroy(&arc_mfu_ghost->arcs_list[ARC_BUFC_METADATA]);\n\tmultilist_destroy(&arc_mru->arcs_list[ARC_BUFC_DATA]);\n\tmultilist_destroy(&arc_mru_ghost->arcs_list[ARC_BUFC_DATA]);\n\tmultilist_destroy(&arc_mfu->arcs_list[ARC_BUFC_DATA]);\n\tmultilist_destroy(&arc_mfu_ghost->arcs_list[ARC_BUFC_DATA]);\n\tmultilist_destroy(&arc_l2c_only->arcs_list[ARC_BUFC_METADATA]);\n\tmultilist_destroy(&arc_l2c_only->arcs_list[ARC_BUFC_DATA]);\n\tmultilist_destroy(&arc_uncached->arcs_list[ARC_BUFC_METADATA]);\n\tmultilist_destroy(&arc_uncached->arcs_list[ARC_BUFC_DATA]);\n\n\twmsum_fini(&arc_mru_ghost->arcs_hits[ARC_BUFC_DATA]);\n\twmsum_fini(&arc_mru_ghost->arcs_hits[ARC_BUFC_METADATA]);\n\twmsum_fini(&arc_mfu_ghost->arcs_hits[ARC_BUFC_DATA]);\n\twmsum_fini(&arc_mfu_ghost->arcs_hits[ARC_BUFC_METADATA]);\n\n\twmsum_fini(&arc_sums.arcstat_hits);\n\twmsum_fini(&arc_sums.arcstat_iohits);\n\twmsum_fini(&arc_sums.arcstat_misses);\n\twmsum_fini(&arc_sums.arcstat_demand_data_hits);\n\twmsum_fini(&arc_sums.arcstat_demand_data_iohits);\n\twmsum_fini(&arc_sums.arcstat_demand_data_misses);\n\twmsum_fini(&arc_sums.arcstat_demand_metadata_hits);\n\twmsum_fini(&arc_sums.arcstat_demand_metadata_iohits);\n\twmsum_fini(&arc_sums.arcstat_demand_metadata_misses);\n\twmsum_fini(&arc_sums.arcstat_prefetch_data_hits);\n\twmsum_fini(&arc_sums.arcstat_prefetch_data_iohits);\n\twmsum_fini(&arc_sums.arcstat_prefetch_data_misses);\n\twmsum_fini(&arc_sums.arcstat_prefetch_metadata_hits);\n\twmsum_fini(&arc_sums.arcstat_prefetch_metadata_iohits);\n\twmsum_fini(&arc_sums.arcstat_prefetch_metadata_misses);\n\twmsum_fini(&arc_sums.arcstat_mru_hits);\n\twmsum_fini(&arc_sums.arcstat_mru_ghost_hits);\n\twmsum_fini(&arc_sums.arcstat_mfu_hits);\n\twmsum_fini(&arc_sums.arcstat_mfu_ghost_hits);\n\twmsum_fini(&arc_sums.arcstat_uncached_hits);\n\twmsum_fini(&arc_sums.arcstat_deleted);\n\twmsum_fini(&arc_sums.arcstat_mutex_miss);\n\twmsum_fini(&arc_sums.arcstat_access_skip);\n\twmsum_fini(&arc_sums.arcstat_evict_skip);\n\twmsum_fini(&arc_sums.arcstat_evict_not_enough);\n\twmsum_fini(&arc_sums.arcstat_evict_l2_cached);\n\twmsum_fini(&arc_sums.arcstat_evict_l2_eligible);\n\twmsum_fini(&arc_sums.arcstat_evict_l2_eligible_mfu);\n\twmsum_fini(&arc_sums.arcstat_evict_l2_eligible_mru);\n\twmsum_fini(&arc_sums.arcstat_evict_l2_ineligible);\n\twmsum_fini(&arc_sums.arcstat_evict_l2_skip);\n\twmsum_fini(&arc_sums.arcstat_hash_collisions);\n\twmsum_fini(&arc_sums.arcstat_hash_chains);\n\taggsum_fini(&arc_sums.arcstat_size);\n\twmsum_fini(&arc_sums.arcstat_compressed_size);\n\twmsum_fini(&arc_sums.arcstat_uncompressed_size);\n\twmsum_fini(&arc_sums.arcstat_overhead_size);\n\twmsum_fini(&arc_sums.arcstat_hdr_size);\n\twmsum_fini(&arc_sums.arcstat_data_size);\n\twmsum_fini(&arc_sums.arcstat_metadata_size);\n\twmsum_fini(&arc_sums.arcstat_dbuf_size);\n\twmsum_fini(&arc_sums.arcstat_dnode_size);\n\twmsum_fini(&arc_sums.arcstat_bonus_size);\n\twmsum_fini(&arc_sums.arcstat_l2_hits);\n\twmsum_fini(&arc_sums.arcstat_l2_misses);\n\twmsum_fini(&arc_sums.arcstat_l2_prefetch_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_mru_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_mfu_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_bufc_data_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_bufc_metadata_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_feeds);\n\twmsum_fini(&arc_sums.arcstat_l2_rw_clash);\n\twmsum_fini(&arc_sums.arcstat_l2_read_bytes);\n\twmsum_fini(&arc_sums.arcstat_l2_write_bytes);\n\twmsum_fini(&arc_sums.arcstat_l2_writes_sent);\n\twmsum_fini(&arc_sums.arcstat_l2_writes_done);\n\twmsum_fini(&arc_sums.arcstat_l2_writes_error);\n\twmsum_fini(&arc_sums.arcstat_l2_writes_lock_retry);\n\twmsum_fini(&arc_sums.arcstat_l2_evict_lock_retry);\n\twmsum_fini(&arc_sums.arcstat_l2_evict_reading);\n\twmsum_fini(&arc_sums.arcstat_l2_evict_l1cached);\n\twmsum_fini(&arc_sums.arcstat_l2_free_on_write);\n\twmsum_fini(&arc_sums.arcstat_l2_abort_lowmem);\n\twmsum_fini(&arc_sums.arcstat_l2_cksum_bad);\n\twmsum_fini(&arc_sums.arcstat_l2_io_error);\n\twmsum_fini(&arc_sums.arcstat_l2_lsize);\n\twmsum_fini(&arc_sums.arcstat_l2_psize);\n\taggsum_fini(&arc_sums.arcstat_l2_hdr_size);\n\twmsum_fini(&arc_sums.arcstat_l2_log_blk_writes);\n\twmsum_fini(&arc_sums.arcstat_l2_log_blk_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_log_blk_count);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_success);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_abort_unsupported);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_abort_io_errors);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_abort_dh_errors);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_abort_cksum_lb_errors);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_abort_lowmem);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_size);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_asize);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_bufs);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_bufs_precached);\n\twmsum_fini(&arc_sums.arcstat_l2_rebuild_log_blks);\n\twmsum_fini(&arc_sums.arcstat_memory_throttle_count);\n\twmsum_fini(&arc_sums.arcstat_memory_direct_count);\n\twmsum_fini(&arc_sums.arcstat_memory_indirect_count);\n\twmsum_fini(&arc_sums.arcstat_prune);\n\twmsum_fini(&arc_sums.arcstat_meta_used);\n\twmsum_fini(&arc_sums.arcstat_async_upgrade_sync);\n\twmsum_fini(&arc_sums.arcstat_predictive_prefetch);\n\twmsum_fini(&arc_sums.arcstat_demand_hit_predictive_prefetch);\n\twmsum_fini(&arc_sums.arcstat_demand_iohit_predictive_prefetch);\n\twmsum_fini(&arc_sums.arcstat_prescient_prefetch);\n\twmsum_fini(&arc_sums.arcstat_demand_hit_prescient_prefetch);\n\twmsum_fini(&arc_sums.arcstat_demand_iohit_prescient_prefetch);\n\twmsum_fini(&arc_sums.arcstat_raw_size);\n\twmsum_fini(&arc_sums.arcstat_cached_only_in_progress);\n\twmsum_fini(&arc_sums.arcstat_abd_chunk_waste_size);\n}\n\nuint64_t\narc_target_bytes(void)\n{\n\treturn (arc_c);\n}\n\nvoid\narc_set_limits(uint64_t allmem)\n{\n\t \n\tarc_c_min = MAX(allmem / 32, 2ULL << SPA_MAXBLOCKSHIFT);\n\n\t \n\tarc_c_max = arc_default_max(arc_c_min, allmem);\n}\nvoid\narc_init(void)\n{\n\tuint64_t percent, allmem = arc_all_memory();\n\tmutex_init(&arc_evict_lock, NULL, MUTEX_DEFAULT, NULL);\n\tlist_create(&arc_evict_waiters, sizeof (arc_evict_waiter_t),\n\t    offsetof(arc_evict_waiter_t, aew_node));\n\n\tarc_min_prefetch_ms = 1000;\n\tarc_min_prescient_prefetch_ms = 6000;\n\n#if defined(_KERNEL)\n\tarc_lowmem_init();\n#endif\n\n\tarc_set_limits(allmem);\n\n#ifdef _KERNEL\n\t \n\tif (zfs_arc_max != 0 && zfs_arc_max >= MIN_ARC_MAX &&\n\t    zfs_arc_max < allmem) {\n\t\tarc_c_max = zfs_arc_max;\n\t\tif (arc_c_min >= arc_c_max) {\n\t\t\tarc_c_min = MAX(zfs_arc_max / 2,\n\t\t\t    2ULL << SPA_MAXBLOCKSHIFT);\n\t\t}\n\t}\n#else\n\t \n\tarc_c_min = MAX(arc_c_max / 2, 2ULL << SPA_MAXBLOCKSHIFT);\n#endif\n\n\tarc_c = arc_c_min;\n\t \n\tarc_meta = (1ULL << 32) / 4;\t \n\tarc_pd = (1ULL << 32) / 2;\t \n\tarc_pm = (1ULL << 32) / 2;\t \n\n\tpercent = MIN(zfs_arc_dnode_limit_percent, 100);\n\tarc_dnode_limit = arc_c_max * percent / 100;\n\n\t \n\tarc_tuning_update(B_TRUE);\n\n\t \n\tif (kmem_debugging())\n\t\tarc_c = arc_c / 2;\n\tif (arc_c < arc_c_min)\n\t\tarc_c = arc_c_min;\n\n\tarc_register_hotplug();\n\n\tarc_state_init();\n\n\tbuf_init();\n\n\tlist_create(&arc_prune_list, sizeof (arc_prune_t),\n\t    offsetof(arc_prune_t, p_node));\n\tmutex_init(&arc_prune_mtx, NULL, MUTEX_DEFAULT, NULL);\n\n\tarc_prune_taskq = taskq_create(\"arc_prune\", zfs_arc_prune_task_threads,\n\t    defclsyspri, 100, INT_MAX, TASKQ_PREPOPULATE | TASKQ_DYNAMIC);\n\n\tarc_ksp = kstat_create(\"zfs\", 0, \"arcstats\", \"misc\", KSTAT_TYPE_NAMED,\n\t    sizeof (arc_stats) / sizeof (kstat_named_t), KSTAT_FLAG_VIRTUAL);\n\n\tif (arc_ksp != NULL) {\n\t\tarc_ksp->ks_data = &arc_stats;\n\t\tarc_ksp->ks_update = arc_kstat_update;\n\t\tkstat_install(arc_ksp);\n\t}\n\n\tarc_state_evict_markers =\n\t    arc_state_alloc_markers(arc_state_evict_marker_count);\n\tarc_evict_zthr = zthr_create_timer(\"arc_evict\",\n\t    arc_evict_cb_check, arc_evict_cb, NULL, SEC2NSEC(1), defclsyspri);\n\tarc_reap_zthr = zthr_create_timer(\"arc_reap\",\n\t    arc_reap_cb_check, arc_reap_cb, NULL, SEC2NSEC(1), minclsyspri);\n\n\tarc_warm = B_FALSE;\n\n\t \n#ifdef __LP64__\n\tif (zfs_dirty_data_max_max == 0)\n\t\tzfs_dirty_data_max_max = MIN(4ULL * 1024 * 1024 * 1024,\n\t\t    allmem * zfs_dirty_data_max_max_percent / 100);\n#else\n\tif (zfs_dirty_data_max_max == 0)\n\t\tzfs_dirty_data_max_max = MIN(1ULL * 1024 * 1024 * 1024,\n\t\t    allmem * zfs_dirty_data_max_max_percent / 100);\n#endif\n\n\tif (zfs_dirty_data_max == 0) {\n\t\tzfs_dirty_data_max = allmem *\n\t\t    zfs_dirty_data_max_percent / 100;\n\t\tzfs_dirty_data_max = MIN(zfs_dirty_data_max,\n\t\t    zfs_dirty_data_max_max);\n\t}\n\n\tif (zfs_wrlog_data_max == 0) {\n\n\t\t \n\t\tzfs_wrlog_data_max = zfs_dirty_data_max * 2;\n\t}\n}\n\nvoid\narc_fini(void)\n{\n\tarc_prune_t *p;\n\n#ifdef _KERNEL\n\tarc_lowmem_fini();\n#endif  \n\n\t \n\tarc_flush(NULL, B_TRUE);\n\n\tif (arc_ksp != NULL) {\n\t\tkstat_delete(arc_ksp);\n\t\tarc_ksp = NULL;\n\t}\n\n\ttaskq_wait(arc_prune_taskq);\n\ttaskq_destroy(arc_prune_taskq);\n\n\tmutex_enter(&arc_prune_mtx);\n\twhile ((p = list_remove_head(&arc_prune_list)) != NULL) {\n\t\tzfs_refcount_remove(&p->p_refcnt, &arc_prune_list);\n\t\tzfs_refcount_destroy(&p->p_refcnt);\n\t\tkmem_free(p, sizeof (*p));\n\t}\n\tmutex_exit(&arc_prune_mtx);\n\n\tlist_destroy(&arc_prune_list);\n\tmutex_destroy(&arc_prune_mtx);\n\n\t(void) zthr_cancel(arc_evict_zthr);\n\t(void) zthr_cancel(arc_reap_zthr);\n\tarc_state_free_markers(arc_state_evict_markers,\n\t    arc_state_evict_marker_count);\n\n\tmutex_destroy(&arc_evict_lock);\n\tlist_destroy(&arc_evict_waiters);\n\n\t \n\tl2arc_do_free_on_write();\n\n\t \n\tbuf_fini();\n\tarc_state_fini();\n\n\tarc_unregister_hotplug();\n\n\t \n\tzthr_destroy(arc_evict_zthr);\n\tzthr_destroy(arc_reap_zthr);\n\n\tASSERT0(arc_loaned_bytes);\n}\n\n \n\nstatic boolean_t\nl2arc_write_eligible(uint64_t spa_guid, arc_buf_hdr_t *hdr)\n{\n\t \n\tif (hdr->b_spa != spa_guid || HDR_HAS_L2HDR(hdr) ||\n\t    HDR_IO_IN_PROGRESS(hdr) || !HDR_L2CACHE(hdr))\n\t\treturn (B_FALSE);\n\n\treturn (B_TRUE);\n}\n\nstatic uint64_t\nl2arc_write_size(l2arc_dev_t *dev)\n{\n\tuint64_t size;\n\n\t \n\tsize = l2arc_write_max;\n\tif (size == 0) {\n\t\tcmn_err(CE_NOTE, \"Bad value for l2arc_write_max, value must \"\n\t\t    \"be greater than zero, resetting it to the default (%d)\",\n\t\t    L2ARC_WRITE_SIZE);\n\t\tsize = l2arc_write_max = L2ARC_WRITE_SIZE;\n\t}\n\n\tif (arc_warm == B_FALSE)\n\t\tsize += l2arc_write_boost;\n\n\t \n\tsize += l2arc_log_blk_overhead(size, dev);\n\tif (dev->l2ad_vdev->vdev_has_trim && l2arc_trim_ahead > 0) {\n\t\t \n\t\tsize += MAX(64 * 1024 * 1024,\n\t\t    (size * l2arc_trim_ahead) / 100);\n\t}\n\n\t \n\tif (size > dev->l2ad_end - dev->l2ad_start) {\n\t\tcmn_err(CE_NOTE, \"l2arc_write_max or l2arc_write_boost \"\n\t\t    \"plus the overhead of log blocks (persistent L2ARC, \"\n\t\t    \"%llu bytes) exceeds the size of the cache device \"\n\t\t    \"(guid %llu), resetting them to the default (%d)\",\n\t\t    (u_longlong_t)l2arc_log_blk_overhead(size, dev),\n\t\t    (u_longlong_t)dev->l2ad_vdev->vdev_guid, L2ARC_WRITE_SIZE);\n\n\t\tsize = l2arc_write_max = l2arc_write_boost = L2ARC_WRITE_SIZE;\n\n\t\tif (l2arc_trim_ahead > 1) {\n\t\t\tcmn_err(CE_NOTE, \"l2arc_trim_ahead set to 1\");\n\t\t\tl2arc_trim_ahead = 1;\n\t\t}\n\n\t\tif (arc_warm == B_FALSE)\n\t\t\tsize += l2arc_write_boost;\n\n\t\tsize += l2arc_log_blk_overhead(size, dev);\n\t\tif (dev->l2ad_vdev->vdev_has_trim && l2arc_trim_ahead > 0) {\n\t\t\tsize += MAX(64 * 1024 * 1024,\n\t\t\t    (size * l2arc_trim_ahead) / 100);\n\t\t}\n\t}\n\n\treturn (size);\n\n}\n\nstatic clock_t\nl2arc_write_interval(clock_t began, uint64_t wanted, uint64_t wrote)\n{\n\tclock_t interval, next, now;\n\n\t \n\tif (l2arc_feed_again && wrote > (wanted / 2))\n\t\tinterval = (hz * l2arc_feed_min_ms) / 1000;\n\telse\n\t\tinterval = hz * l2arc_feed_secs;\n\n\tnow = ddi_get_lbolt();\n\tnext = MAX(now, MIN(now + interval, began + interval));\n\n\treturn (next);\n}\n\n \nstatic l2arc_dev_t *\nl2arc_dev_get_next(void)\n{\n\tl2arc_dev_t *first, *next = NULL;\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\tmutex_enter(&l2arc_dev_mtx);\n\n\t \n\tif (l2arc_ndev == 0)\n\t\tgoto out;\n\n\tfirst = NULL;\n\tnext = l2arc_dev_last;\n\tdo {\n\t\t \n\t\tif (next == NULL) {\n\t\t\tnext = list_head(l2arc_dev_list);\n\t\t} else {\n\t\t\tnext = list_next(l2arc_dev_list, next);\n\t\t\tif (next == NULL)\n\t\t\t\tnext = list_head(l2arc_dev_list);\n\t\t}\n\n\t\t \n\t\tif (first == NULL)\n\t\t\tfirst = next;\n\t\telse if (next == first)\n\t\t\tbreak;\n\n\t\tASSERT3P(next, !=, NULL);\n\t} while (vdev_is_dead(next->l2ad_vdev) || next->l2ad_rebuild ||\n\t    next->l2ad_trim_all);\n\n\t \n\tif (vdev_is_dead(next->l2ad_vdev) || next->l2ad_rebuild ||\n\t    next->l2ad_trim_all)\n\t\tnext = NULL;\n\n\tl2arc_dev_last = next;\n\nout:\n\tmutex_exit(&l2arc_dev_mtx);\n\n\t \n\tif (next != NULL)\n\t\tspa_config_enter(next->l2ad_spa, SCL_L2ARC, next, RW_READER);\n\tmutex_exit(&spa_namespace_lock);\n\n\treturn (next);\n}\n\n \nstatic void\nl2arc_do_free_on_write(void)\n{\n\tl2arc_data_free_t *df;\n\n\tmutex_enter(&l2arc_free_on_write_mtx);\n\twhile ((df = list_remove_head(l2arc_free_on_write)) != NULL) {\n\t\tASSERT3P(df->l2df_abd, !=, NULL);\n\t\tabd_free(df->l2df_abd);\n\t\tkmem_free(df, sizeof (l2arc_data_free_t));\n\t}\n\tmutex_exit(&l2arc_free_on_write_mtx);\n}\n\n \nstatic void\nl2arc_write_done(zio_t *zio)\n{\n\tl2arc_write_callback_t\t*cb;\n\tl2arc_lb_abd_buf_t\t*abd_buf;\n\tl2arc_lb_ptr_buf_t\t*lb_ptr_buf;\n\tl2arc_dev_t\t\t*dev;\n\tl2arc_dev_hdr_phys_t\t*l2dhdr;\n\tlist_t\t\t\t*buflist;\n\tarc_buf_hdr_t\t\t*head, *hdr, *hdr_prev;\n\tkmutex_t\t\t*hash_lock;\n\tint64_t\t\t\tbytes_dropped = 0;\n\n\tcb = zio->io_private;\n\tASSERT3P(cb, !=, NULL);\n\tdev = cb->l2wcb_dev;\n\tl2dhdr = dev->l2ad_dev_hdr;\n\tASSERT3P(dev, !=, NULL);\n\thead = cb->l2wcb_head;\n\tASSERT3P(head, !=, NULL);\n\tbuflist = &dev->l2ad_buflist;\n\tASSERT3P(buflist, !=, NULL);\n\tDTRACE_PROBE2(l2arc__iodone, zio_t *, zio,\n\t    l2arc_write_callback_t *, cb);\n\n\t \ntop:\n\tmutex_enter(&dev->l2ad_mtx);\n\tfor (hdr = list_prev(buflist, head); hdr; hdr = hdr_prev) {\n\t\thdr_prev = list_prev(buflist, hdr);\n\n\t\thash_lock = HDR_LOCK(hdr);\n\n\t\t \n\t\tif (!mutex_tryenter(hash_lock)) {\n\t\t\t \n\t\t\tARCSTAT_BUMP(arcstat_l2_writes_lock_retry);\n\n\t\t\t \n\t\t\tlist_remove(buflist, head);\n\t\t\tlist_insert_after(buflist, hdr, head);\n\n\t\t\tmutex_exit(&dev->l2ad_mtx);\n\n\t\t\t \n\t\t\tmutex_enter(hash_lock);\n\t\t\tmutex_exit(hash_lock);\n\t\t\tgoto top;\n\t\t}\n\n\t\t \n\t\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\t\t \n\t\tif (zio->io_error != 0) {\n\t\t\t \n\t\t\tlist_remove(buflist, hdr);\n\t\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_HAS_L2HDR);\n\n\t\t\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\t\t\tl2arc_hdr_arcstats_decrement(hdr);\n\n\t\t\tbytes_dropped +=\n\t\t\t    vdev_psize_to_asize(dev->l2ad_vdev, psize);\n\t\t\t(void) zfs_refcount_remove_many(&dev->l2ad_alloc,\n\t\t\t    arc_hdr_size(hdr), hdr);\n\t\t}\n\n\t\t \n\t\tarc_hdr_clear_flags(hdr, ARC_FLAG_L2_WRITING);\n\n\t\tmutex_exit(hash_lock);\n\t}\n\n\t \n\twhile ((abd_buf = list_remove_tail(&cb->l2wcb_abd_list)) != NULL) {\n\t\tabd_free(abd_buf->abd);\n\t\tzio_buf_free(abd_buf, sizeof (*abd_buf));\n\t\tif (zio->io_error != 0) {\n\t\t\tlb_ptr_buf = list_remove_head(&dev->l2ad_lbptr_list);\n\t\t\t \n\t\t\tuint64_t asize =\n\t\t\t    L2BLK_GET_PSIZE((lb_ptr_buf->lb_ptr)->lbp_prop);\n\t\t\tbytes_dropped += asize;\n\t\t\tARCSTAT_INCR(arcstat_l2_log_blk_asize, -asize);\n\t\t\tARCSTAT_BUMPDOWN(arcstat_l2_log_blk_count);\n\t\t\tzfs_refcount_remove_many(&dev->l2ad_lb_asize, asize,\n\t\t\t    lb_ptr_buf);\n\t\t\tzfs_refcount_remove(&dev->l2ad_lb_count, lb_ptr_buf);\n\t\t\tkmem_free(lb_ptr_buf->lb_ptr,\n\t\t\t    sizeof (l2arc_log_blkptr_t));\n\t\t\tkmem_free(lb_ptr_buf, sizeof (l2arc_lb_ptr_buf_t));\n\t\t}\n\t}\n\tlist_destroy(&cb->l2wcb_abd_list);\n\n\tif (zio->io_error != 0) {\n\t\tARCSTAT_BUMP(arcstat_l2_writes_error);\n\n\t\t \n\t\tlb_ptr_buf = list_head(&dev->l2ad_lbptr_list);\n\t\tfor (int i = 0; i < 2; i++) {\n\t\t\tif (lb_ptr_buf == NULL) {\n\t\t\t\t \n\t\t\t\tif (i == 0) {\n\t\t\t\t\tmemset(l2dhdr, 0,\n\t\t\t\t\t    dev->l2ad_dev_hdr_asize);\n\t\t\t\t} else {\n\t\t\t\t\tmemset(&l2dhdr->dh_start_lbps[i], 0,\n\t\t\t\t\t    sizeof (l2arc_log_blkptr_t));\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmemcpy(&l2dhdr->dh_start_lbps[i], lb_ptr_buf->lb_ptr,\n\t\t\t    sizeof (l2arc_log_blkptr_t));\n\t\t\tlb_ptr_buf = list_next(&dev->l2ad_lbptr_list,\n\t\t\t    lb_ptr_buf);\n\t\t}\n\t}\n\n\tARCSTAT_BUMP(arcstat_l2_writes_done);\n\tlist_remove(buflist, head);\n\tASSERT(!HDR_HAS_L1HDR(head));\n\tkmem_cache_free(hdr_l2only_cache, head);\n\tmutex_exit(&dev->l2ad_mtx);\n\n\tASSERT(dev->l2ad_vdev != NULL);\n\tvdev_space_update(dev->l2ad_vdev, -bytes_dropped, 0, 0);\n\n\tl2arc_do_free_on_write();\n\n\tkmem_free(cb, sizeof (l2arc_write_callback_t));\n}\n\nstatic int\nl2arc_untransform(zio_t *zio, l2arc_read_callback_t *cb)\n{\n\tint ret;\n\tspa_t *spa = zio->io_spa;\n\tarc_buf_hdr_t *hdr = cb->l2rcb_hdr;\n\tblkptr_t *bp = zio->io_bp;\n\tuint8_t salt[ZIO_DATA_SALT_LEN];\n\tuint8_t iv[ZIO_DATA_IV_LEN];\n\tuint8_t mac[ZIO_DATA_MAC_LEN];\n\tboolean_t no_crypt = B_FALSE;\n\n\t \n\tASSERT3U(BP_GET_TYPE(bp), !=, DMU_OT_INTENT_LOG);\n\tASSERT(MUTEX_HELD(HDR_LOCK(hdr)));\n\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\n\t \n\tif (BP_IS_ENCRYPTED(bp)) {\n\t\tabd_t *eabd = arc_get_data_abd(hdr, arc_hdr_size(hdr), hdr,\n\t\t    ARC_HDR_USE_RESERVE);\n\n\t\tzio_crypt_decode_params_bp(bp, salt, iv);\n\t\tzio_crypt_decode_mac_bp(bp, mac);\n\n\t\tret = spa_do_crypt_abd(B_FALSE, spa, &cb->l2rcb_zb,\n\t\t    BP_GET_TYPE(bp), BP_GET_DEDUP(bp), BP_SHOULD_BYTESWAP(bp),\n\t\t    salt, iv, mac, HDR_GET_PSIZE(hdr), eabd,\n\t\t    hdr->b_l1hdr.b_pabd, &no_crypt);\n\t\tif (ret != 0) {\n\t\t\tarc_free_data_abd(hdr, eabd, arc_hdr_size(hdr), hdr);\n\t\t\tgoto error;\n\t\t}\n\n\t\t \n\t\tif (!no_crypt) {\n\t\t\tarc_free_data_abd(hdr, hdr->b_l1hdr.b_pabd,\n\t\t\t    arc_hdr_size(hdr), hdr);\n\t\t\thdr->b_l1hdr.b_pabd = eabd;\n\t\t\tzio->io_abd = eabd;\n\t\t} else {\n\t\t\tarc_free_data_abd(hdr, eabd, arc_hdr_size(hdr), hdr);\n\t\t}\n\t}\n\n\t \n\tif (HDR_GET_COMPRESS(hdr) != ZIO_COMPRESS_OFF &&\n\t    !HDR_COMPRESSION_ENABLED(hdr)) {\n\t\tabd_t *cabd = arc_get_data_abd(hdr, arc_hdr_size(hdr), hdr,\n\t\t    ARC_HDR_USE_RESERVE);\n\t\tvoid *tmp = abd_borrow_buf(cabd, arc_hdr_size(hdr));\n\n\t\tret = zio_decompress_data(HDR_GET_COMPRESS(hdr),\n\t\t    hdr->b_l1hdr.b_pabd, tmp, HDR_GET_PSIZE(hdr),\n\t\t    HDR_GET_LSIZE(hdr), &hdr->b_complevel);\n\t\tif (ret != 0) {\n\t\t\tabd_return_buf_copy(cabd, tmp, arc_hdr_size(hdr));\n\t\t\tarc_free_data_abd(hdr, cabd, arc_hdr_size(hdr), hdr);\n\t\t\tgoto error;\n\t\t}\n\n\t\tabd_return_buf_copy(cabd, tmp, arc_hdr_size(hdr));\n\t\tarc_free_data_abd(hdr, hdr->b_l1hdr.b_pabd,\n\t\t    arc_hdr_size(hdr), hdr);\n\t\thdr->b_l1hdr.b_pabd = cabd;\n\t\tzio->io_abd = cabd;\n\t\tzio->io_size = HDR_GET_LSIZE(hdr);\n\t}\n\n\treturn (0);\n\nerror:\n\treturn (ret);\n}\n\n\n \nstatic void\nl2arc_read_done(zio_t *zio)\n{\n\tint tfm_error = 0;\n\tl2arc_read_callback_t *cb = zio->io_private;\n\tarc_buf_hdr_t *hdr;\n\tkmutex_t *hash_lock;\n\tboolean_t valid_cksum;\n\tboolean_t using_rdata = (BP_IS_ENCRYPTED(&cb->l2rcb_bp) &&\n\t    (cb->l2rcb_flags & ZIO_FLAG_RAW_ENCRYPT));\n\n\tASSERT3P(zio->io_vd, !=, NULL);\n\tASSERT(zio->io_flags & ZIO_FLAG_DONT_PROPAGATE);\n\n\tspa_config_exit(zio->io_spa, SCL_L2ARC, zio->io_vd);\n\n\tASSERT3P(cb, !=, NULL);\n\thdr = cb->l2rcb_hdr;\n\tASSERT3P(hdr, !=, NULL);\n\n\thash_lock = HDR_LOCK(hdr);\n\tmutex_enter(hash_lock);\n\tASSERT3P(hash_lock, ==, HDR_LOCK(hdr));\n\n\t \n\tif (cb->l2rcb_abd != NULL) {\n\t\tASSERT3U(arc_hdr_size(hdr), <, zio->io_size);\n\t\tif (zio->io_error == 0) {\n\t\t\tif (using_rdata) {\n\t\t\t\tabd_copy(hdr->b_crypt_hdr.b_rabd,\n\t\t\t\t    cb->l2rcb_abd, arc_hdr_size(hdr));\n\t\t\t} else {\n\t\t\t\tabd_copy(hdr->b_l1hdr.b_pabd,\n\t\t\t\t    cb->l2rcb_abd, arc_hdr_size(hdr));\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tabd_free(cb->l2rcb_abd);\n\t\tzio->io_size = zio->io_orig_size = arc_hdr_size(hdr);\n\n\t\tif (using_rdata) {\n\t\t\tASSERT(HDR_HAS_RABD(hdr));\n\t\t\tzio->io_abd = zio->io_orig_abd =\n\t\t\t    hdr->b_crypt_hdr.b_rabd;\n\t\t} else {\n\t\t\tASSERT3P(hdr->b_l1hdr.b_pabd, !=, NULL);\n\t\t\tzio->io_abd = zio->io_orig_abd = hdr->b_l1hdr.b_pabd;\n\t\t}\n\t}\n\n\tASSERT3P(zio->io_abd, !=, NULL);\n\n\t \n\tASSERT(zio->io_abd == hdr->b_l1hdr.b_pabd ||\n\t    (HDR_HAS_RABD(hdr) && zio->io_abd == hdr->b_crypt_hdr.b_rabd));\n\tzio->io_bp_copy = cb->l2rcb_bp;\t \n\tzio->io_bp = &zio->io_bp_copy;\t \n\tzio->io_prop.zp_complevel = hdr->b_complevel;\n\n\tvalid_cksum = arc_cksum_is_equal(hdr, zio);\n\n\t \n\tif (valid_cksum && !using_rdata)\n\t\ttfm_error = l2arc_untransform(zio, cb);\n\n\tif (valid_cksum && tfm_error == 0 && zio->io_error == 0 &&\n\t    !HDR_L2_EVICTED(hdr)) {\n\t\tmutex_exit(hash_lock);\n\t\tzio->io_private = hdr;\n\t\tarc_read_done(zio);\n\t} else {\n\t\t \n\t\tif (zio->io_error != 0) {\n\t\t\tARCSTAT_BUMP(arcstat_l2_io_error);\n\t\t} else {\n\t\t\tzio->io_error = SET_ERROR(EIO);\n\t\t}\n\t\tif (!valid_cksum || tfm_error != 0)\n\t\t\tARCSTAT_BUMP(arcstat_l2_cksum_bad);\n\n\t\t \n\t\tif (zio->io_waiter == NULL) {\n\t\t\tzio_t *pio = zio_unique_parent(zio);\n\t\t\tvoid *abd = (using_rdata) ?\n\t\t\t    hdr->b_crypt_hdr.b_rabd : hdr->b_l1hdr.b_pabd;\n\n\t\t\tASSERT(!pio || pio->io_child_type == ZIO_CHILD_LOGICAL);\n\n\t\t\tzio = zio_read(pio, zio->io_spa, zio->io_bp,\n\t\t\t    abd, zio->io_size, arc_read_done,\n\t\t\t    hdr, zio->io_priority, cb->l2rcb_flags,\n\t\t\t    &cb->l2rcb_zb);\n\n\t\t\t \n\t\t\tfor (struct arc_callback *acb = hdr->b_l1hdr.b_acb;\n\t\t\t    acb != NULL; acb = acb->acb_next)\n\t\t\t\tacb->acb_zio_head = zio;\n\n\t\t\tmutex_exit(hash_lock);\n\t\t\tzio_nowait(zio);\n\t\t} else {\n\t\t\tmutex_exit(hash_lock);\n\t\t}\n\t}\n\n\tkmem_free(cb, sizeof (l2arc_read_callback_t));\n}\n\n \nstatic multilist_sublist_t *\nl2arc_sublist_lock(int list_num)\n{\n\tmultilist_t *ml = NULL;\n\tunsigned int idx;\n\n\tASSERT(list_num >= 0 && list_num < L2ARC_FEED_TYPES);\n\n\tswitch (list_num) {\n\tcase 0:\n\t\tml = &arc_mfu->arcs_list[ARC_BUFC_METADATA];\n\t\tbreak;\n\tcase 1:\n\t\tml = &arc_mru->arcs_list[ARC_BUFC_METADATA];\n\t\tbreak;\n\tcase 2:\n\t\tml = &arc_mfu->arcs_list[ARC_BUFC_DATA];\n\t\tbreak;\n\tcase 3:\n\t\tml = &arc_mru->arcs_list[ARC_BUFC_DATA];\n\t\tbreak;\n\tdefault:\n\t\treturn (NULL);\n\t}\n\n\t \n\tidx = multilist_get_random_index(ml);\n\treturn (multilist_sublist_lock(ml, idx));\n}\n\n \nstatic inline uint64_t\nl2arc_log_blk_overhead(uint64_t write_sz, l2arc_dev_t *dev)\n{\n\tif (dev->l2ad_log_entries == 0) {\n\t\treturn (0);\n\t} else {\n\t\tuint64_t log_entries = write_sz >> SPA_MINBLOCKSHIFT;\n\n\t\tuint64_t log_blocks = (log_entries +\n\t\t    dev->l2ad_log_entries - 1) /\n\t\t    dev->l2ad_log_entries;\n\n\t\treturn (vdev_psize_to_asize(dev->l2ad_vdev,\n\t\t    sizeof (l2arc_log_blk_phys_t)) * log_blocks);\n\t}\n}\n\n \nstatic void\nl2arc_evict(l2arc_dev_t *dev, uint64_t distance, boolean_t all)\n{\n\tlist_t *buflist;\n\tarc_buf_hdr_t *hdr, *hdr_prev;\n\tkmutex_t *hash_lock;\n\tuint64_t taddr;\n\tl2arc_lb_ptr_buf_t *lb_ptr_buf, *lb_ptr_buf_prev;\n\tvdev_t *vd = dev->l2ad_vdev;\n\tboolean_t rerun;\n\n\tbuflist = &dev->l2ad_buflist;\n\ntop:\n\trerun = B_FALSE;\n\tif (dev->l2ad_hand + distance > dev->l2ad_end) {\n\t\t \n\t\trerun = B_TRUE;\n\t\ttaddr = dev->l2ad_end;\n\t} else {\n\t\ttaddr = dev->l2ad_hand + distance;\n\t}\n\tDTRACE_PROBE4(l2arc__evict, l2arc_dev_t *, dev, list_t *, buflist,\n\t    uint64_t, taddr, boolean_t, all);\n\n\tif (!all) {\n\t\t \n\t\tif (dev->l2ad_first) {\n\t\t\t \n\t\t\tgoto out;\n\t\t} else {\n\t\t\t \n\t\t\tif (vd->vdev_has_trim && dev->l2ad_evict < taddr &&\n\t\t\t    l2arc_trim_ahead > 0) {\n\t\t\t\t \n\t\t\t\tspa_config_exit(dev->l2ad_spa, SCL_L2ARC, dev);\n\t\t\t\tvdev_trim_simple(vd,\n\t\t\t\t    dev->l2ad_evict - VDEV_LABEL_START_SIZE,\n\t\t\t\t    taddr - dev->l2ad_evict);\n\t\t\t\tspa_config_enter(dev->l2ad_spa, SCL_L2ARC, dev,\n\t\t\t\t    RW_READER);\n\t\t\t}\n\n\t\t\t \n\t\t\tdev->l2ad_evict = MAX(dev->l2ad_evict, taddr);\n\t\t}\n\t}\n\nretry:\n\tmutex_enter(&dev->l2ad_mtx);\n\t \n\tfor (lb_ptr_buf = list_tail(&dev->l2ad_lbptr_list); lb_ptr_buf;\n\t    lb_ptr_buf = lb_ptr_buf_prev) {\n\n\t\tlb_ptr_buf_prev = list_prev(&dev->l2ad_lbptr_list, lb_ptr_buf);\n\n\t\t \n\t\tuint64_t asize = L2BLK_GET_PSIZE(\n\t\t    (lb_ptr_buf->lb_ptr)->lbp_prop);\n\n\t\t \n\t\tif (!all && l2arc_log_blkptr_valid(dev, lb_ptr_buf->lb_ptr)) {\n\t\t\tbreak;\n\t\t} else {\n\t\t\tvdev_space_update(vd, -asize, 0, 0);\n\t\t\tARCSTAT_INCR(arcstat_l2_log_blk_asize, -asize);\n\t\t\tARCSTAT_BUMPDOWN(arcstat_l2_log_blk_count);\n\t\t\tzfs_refcount_remove_many(&dev->l2ad_lb_asize, asize,\n\t\t\t    lb_ptr_buf);\n\t\t\tzfs_refcount_remove(&dev->l2ad_lb_count, lb_ptr_buf);\n\t\t\tlist_remove(&dev->l2ad_lbptr_list, lb_ptr_buf);\n\t\t\tkmem_free(lb_ptr_buf->lb_ptr,\n\t\t\t    sizeof (l2arc_log_blkptr_t));\n\t\t\tkmem_free(lb_ptr_buf, sizeof (l2arc_lb_ptr_buf_t));\n\t\t}\n\t}\n\n\tfor (hdr = list_tail(buflist); hdr; hdr = hdr_prev) {\n\t\thdr_prev = list_prev(buflist, hdr);\n\n\t\tASSERT(!HDR_EMPTY(hdr));\n\t\thash_lock = HDR_LOCK(hdr);\n\n\t\t \n\t\tif (!mutex_tryenter(hash_lock)) {\n\t\t\t \n\t\t\tARCSTAT_BUMP(arcstat_l2_evict_lock_retry);\n\t\t\tmutex_exit(&dev->l2ad_mtx);\n\t\t\tmutex_enter(hash_lock);\n\t\t\tmutex_exit(hash_lock);\n\t\t\tgoto retry;\n\t\t}\n\n\t\t \n\t\tASSERT(HDR_HAS_L2HDR(hdr));\n\n\t\t \n\t\tASSERT(!HDR_L2_WRITING(hdr));\n\t\tASSERT(!HDR_L2_WRITE_HEAD(hdr));\n\n\t\tif (!all && (hdr->b_l2hdr.b_daddr >= dev->l2ad_evict ||\n\t\t    hdr->b_l2hdr.b_daddr < dev->l2ad_hand)) {\n\t\t\t \n\t\t\tmutex_exit(hash_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!HDR_HAS_L1HDR(hdr)) {\n\t\t\tASSERT(!HDR_L2_READING(hdr));\n\t\t\t \n\t\t\tarc_change_state(arc_anon, hdr);\n\t\t\tarc_hdr_destroy(hdr);\n\t\t} else {\n\t\t\tASSERT(hdr->b_l1hdr.b_state != arc_l2c_only);\n\t\t\tARCSTAT_BUMP(arcstat_l2_evict_l1cached);\n\t\t\t \n\t\t\tif (HDR_L2_READING(hdr)) {\n\t\t\t\tARCSTAT_BUMP(arcstat_l2_evict_reading);\n\t\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_L2_EVICTED);\n\t\t\t}\n\n\t\t\tarc_hdr_l2hdr_destroy(hdr);\n\t\t}\n\t\tmutex_exit(hash_lock);\n\t}\n\tmutex_exit(&dev->l2ad_mtx);\n\nout:\n\t \n\tif (!all && rerun) {\n\t\t \n\t\tdev->l2ad_hand = dev->l2ad_start;\n\t\tdev->l2ad_evict = dev->l2ad_start;\n\t\tdev->l2ad_first = B_FALSE;\n\t\tgoto top;\n\t}\n\n\tif (!all) {\n\t\t \n\t\tASSERT3U(dev->l2ad_hand + distance, <, dev->l2ad_end);\n\t\tif (!dev->l2ad_first)\n\t\t\tASSERT3U(dev->l2ad_hand, <=, dev->l2ad_evict);\n\t}\n}\n\n \nstatic int\nl2arc_apply_transforms(spa_t *spa, arc_buf_hdr_t *hdr, uint64_t asize,\n    abd_t **abd_out)\n{\n\tint ret;\n\tvoid *tmp = NULL;\n\tabd_t *cabd = NULL, *eabd = NULL, *to_write = hdr->b_l1hdr.b_pabd;\n\tenum zio_compress compress = HDR_GET_COMPRESS(hdr);\n\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\tuint64_t size = arc_hdr_size(hdr);\n\tboolean_t ismd = HDR_ISTYPE_METADATA(hdr);\n\tboolean_t bswap = (hdr->b_l1hdr.b_byteswap != DMU_BSWAP_NUMFUNCS);\n\tdsl_crypto_key_t *dck = NULL;\n\tuint8_t mac[ZIO_DATA_MAC_LEN] = { 0 };\n\tboolean_t no_crypt = B_FALSE;\n\n\tASSERT((HDR_GET_COMPRESS(hdr) != ZIO_COMPRESS_OFF &&\n\t    !HDR_COMPRESSION_ENABLED(hdr)) ||\n\t    HDR_ENCRYPTED(hdr) || HDR_SHARED_DATA(hdr) || psize != asize);\n\tASSERT3U(psize, <=, asize);\n\n\t \n\tif (HDR_HAS_RABD(hdr) && asize != psize) {\n\t\tASSERT3U(asize, >=, psize);\n\t\tto_write = abd_alloc_for_io(asize, ismd);\n\t\tabd_copy(to_write, hdr->b_crypt_hdr.b_rabd, psize);\n\t\tif (psize != asize)\n\t\t\tabd_zero_off(to_write, psize, asize - psize);\n\t\tgoto out;\n\t}\n\n\tif ((compress == ZIO_COMPRESS_OFF || HDR_COMPRESSION_ENABLED(hdr)) &&\n\t    !HDR_ENCRYPTED(hdr)) {\n\t\tASSERT3U(size, ==, psize);\n\t\tto_write = abd_alloc_for_io(asize, ismd);\n\t\tabd_copy(to_write, hdr->b_l1hdr.b_pabd, size);\n\t\tif (size != asize)\n\t\t\tabd_zero_off(to_write, size, asize - size);\n\t\tgoto out;\n\t}\n\n\tif (compress != ZIO_COMPRESS_OFF && !HDR_COMPRESSION_ENABLED(hdr)) {\n\t\t \n\t\tuint64_t bufsize = MAX(size, asize);\n\t\tcabd = abd_alloc_for_io(bufsize, ismd);\n\t\ttmp = abd_borrow_buf(cabd, bufsize);\n\n\t\tpsize = zio_compress_data(compress, to_write, &tmp, size,\n\t\t    hdr->b_complevel);\n\n\t\tif (psize >= asize) {\n\t\t\tpsize = HDR_GET_PSIZE(hdr);\n\t\t\tabd_return_buf_copy(cabd, tmp, bufsize);\n\t\t\tHDR_SET_COMPRESS(hdr, ZIO_COMPRESS_OFF);\n\t\t\tto_write = cabd;\n\t\t\tabd_copy(to_write, hdr->b_l1hdr.b_pabd, psize);\n\t\t\tif (psize != asize)\n\t\t\t\tabd_zero_off(to_write, psize, asize - psize);\n\t\t\tgoto encrypt;\n\t\t}\n\t\tASSERT3U(psize, <=, HDR_GET_PSIZE(hdr));\n\t\tif (psize < asize)\n\t\t\tmemset((char *)tmp + psize, 0, bufsize - psize);\n\t\tpsize = HDR_GET_PSIZE(hdr);\n\t\tabd_return_buf_copy(cabd, tmp, bufsize);\n\t\tto_write = cabd;\n\t}\n\nencrypt:\n\tif (HDR_ENCRYPTED(hdr)) {\n\t\teabd = abd_alloc_for_io(asize, ismd);\n\n\t\t \n\t\tret = spa_keystore_lookup_key(spa, hdr->b_crypt_hdr.b_dsobj,\n\t\t    FTAG, &dck);\n\t\tif (ret != 0)\n\t\t\tgoto error;\n\n\t\tret = zio_do_crypt_abd(B_TRUE, &dck->dck_key,\n\t\t    hdr->b_crypt_hdr.b_ot, bswap, hdr->b_crypt_hdr.b_salt,\n\t\t    hdr->b_crypt_hdr.b_iv, mac, psize, to_write, eabd,\n\t\t    &no_crypt);\n\t\tif (ret != 0)\n\t\t\tgoto error;\n\n\t\tif (no_crypt)\n\t\t\tabd_copy(eabd, to_write, psize);\n\n\t\tif (psize != asize)\n\t\t\tabd_zero_off(eabd, psize, asize - psize);\n\n\t\t \n\t\tASSERT0(memcmp(mac, hdr->b_crypt_hdr.b_mac, ZIO_DATA_MAC_LEN));\n\t\tspa_keystore_dsl_key_rele(spa, dck, FTAG);\n\n\t\tif (to_write == cabd)\n\t\t\tabd_free(cabd);\n\n\t\tto_write = eabd;\n\t}\n\nout:\n\tASSERT3P(to_write, !=, hdr->b_l1hdr.b_pabd);\n\t*abd_out = to_write;\n\treturn (0);\n\nerror:\n\tif (dck != NULL)\n\t\tspa_keystore_dsl_key_rele(spa, dck, FTAG);\n\tif (cabd != NULL)\n\t\tabd_free(cabd);\n\tif (eabd != NULL)\n\t\tabd_free(eabd);\n\n\t*abd_out = NULL;\n\treturn (ret);\n}\n\nstatic void\nl2arc_blk_fetch_done(zio_t *zio)\n{\n\tl2arc_read_callback_t *cb;\n\n\tcb = zio->io_private;\n\tif (cb->l2rcb_abd != NULL)\n\t\tabd_free(cb->l2rcb_abd);\n\tkmem_free(cb, sizeof (l2arc_read_callback_t));\n}\n\n \nstatic uint64_t\nl2arc_write_buffers(spa_t *spa, l2arc_dev_t *dev, uint64_t target_sz)\n{\n\tarc_buf_hdr_t \t\t*hdr, *hdr_prev, *head;\n\tuint64_t \t\twrite_asize, write_psize, write_lsize, headroom;\n\tboolean_t\t\tfull;\n\tl2arc_write_callback_t\t*cb = NULL;\n\tzio_t \t\t\t*pio, *wzio;\n\tuint64_t \t\tguid = spa_load_guid(spa);\n\tl2arc_dev_hdr_phys_t\t*l2dhdr = dev->l2ad_dev_hdr;\n\n\tASSERT3P(dev->l2ad_vdev, !=, NULL);\n\n\tpio = NULL;\n\twrite_lsize = write_asize = write_psize = 0;\n\tfull = B_FALSE;\n\thead = kmem_cache_alloc(hdr_l2only_cache, KM_PUSHPAGE);\n\tarc_hdr_set_flags(head, ARC_FLAG_L2_WRITE_HEAD | ARC_FLAG_HAS_L2HDR);\n\n\t \n\tfor (int pass = 0; pass < L2ARC_FEED_TYPES; pass++) {\n\t\t \n\t\tif (l2arc_mfuonly) {\n\t\t\tif (pass == 1 || pass == 3)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tmultilist_sublist_t *mls = l2arc_sublist_lock(pass);\n\t\tuint64_t passed_sz = 0;\n\n\t\tVERIFY3P(mls, !=, NULL);\n\n\t\t \n\t\tif (arc_warm == B_FALSE)\n\t\t\thdr = multilist_sublist_head(mls);\n\t\telse\n\t\t\thdr = multilist_sublist_tail(mls);\n\n\t\theadroom = target_sz * l2arc_headroom;\n\t\tif (zfs_compressed_arc_enabled)\n\t\t\theadroom = (headroom * l2arc_headroom_boost) / 100;\n\n\t\tfor (; hdr; hdr = hdr_prev) {\n\t\t\tkmutex_t *hash_lock;\n\t\t\tabd_t *to_write = NULL;\n\n\t\t\tif (arc_warm == B_FALSE)\n\t\t\t\thdr_prev = multilist_sublist_next(mls, hdr);\n\t\t\telse\n\t\t\t\thdr_prev = multilist_sublist_prev(mls, hdr);\n\n\t\t\thash_lock = HDR_LOCK(hdr);\n\t\t\tif (!mutex_tryenter(hash_lock)) {\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tpassed_sz += HDR_GET_LSIZE(hdr);\n\t\t\tif (l2arc_headroom != 0 && passed_sz > headroom) {\n\t\t\t\t \n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!l2arc_write_eligible(guid, hdr)) {\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tASSERT(HDR_HAS_L1HDR(hdr));\n\n\t\t\tASSERT3U(HDR_GET_PSIZE(hdr), >, 0);\n\t\t\tASSERT3U(arc_hdr_size(hdr), >, 0);\n\t\t\tASSERT(hdr->b_l1hdr.b_pabd != NULL ||\n\t\t\t    HDR_HAS_RABD(hdr));\n\t\t\tuint64_t psize = HDR_GET_PSIZE(hdr);\n\t\t\tuint64_t asize = vdev_psize_to_asize(dev->l2ad_vdev,\n\t\t\t    psize);\n\n\t\t\t \n\t\t\tif (write_asize + asize +\n\t\t\t    sizeof (l2arc_log_blk_phys_t) > target_sz) {\n\t\t\t\tfull = B_TRUE;\n\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_L2_WRITING);\n\n\t\t\t \n\t\t\tif (HDR_HAS_RABD(hdr) && psize == asize) {\n\t\t\t\tto_write = hdr->b_crypt_hdr.b_rabd;\n\t\t\t} else if ((HDR_COMPRESSION_ENABLED(hdr) ||\n\t\t\t    HDR_GET_COMPRESS(hdr) == ZIO_COMPRESS_OFF) &&\n\t\t\t    !HDR_ENCRYPTED(hdr) && !HDR_SHARED_DATA(hdr) &&\n\t\t\t    psize == asize) {\n\t\t\t\tto_write = hdr->b_l1hdr.b_pabd;\n\t\t\t} else {\n\t\t\t\tint ret;\n\t\t\t\tarc_buf_contents_t type = arc_buf_type(hdr);\n\n\t\t\t\tret = l2arc_apply_transforms(spa, hdr, asize,\n\t\t\t\t    &to_write);\n\t\t\t\tif (ret != 0) {\n\t\t\t\t\tarc_hdr_clear_flags(hdr,\n\t\t\t\t\t    ARC_FLAG_L2_WRITING);\n\t\t\t\t\tmutex_exit(hash_lock);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tl2arc_free_abd_on_write(to_write, asize, type);\n\t\t\t}\n\n\t\t\tif (pio == NULL) {\n\t\t\t\t \n\t\t\t\tmutex_enter(&dev->l2ad_mtx);\n\t\t\t\tlist_insert_head(&dev->l2ad_buflist, head);\n\t\t\t\tmutex_exit(&dev->l2ad_mtx);\n\n\t\t\t\tcb = kmem_alloc(\n\t\t\t\t    sizeof (l2arc_write_callback_t), KM_SLEEP);\n\t\t\t\tcb->l2wcb_dev = dev;\n\t\t\t\tcb->l2wcb_head = head;\n\t\t\t\t \n\t\t\t\tlist_create(&cb->l2wcb_abd_list,\n\t\t\t\t    sizeof (l2arc_lb_abd_buf_t),\n\t\t\t\t    offsetof(l2arc_lb_abd_buf_t, node));\n\t\t\t\tpio = zio_root(spa, l2arc_write_done, cb,\n\t\t\t\t    ZIO_FLAG_CANFAIL);\n\t\t\t}\n\n\t\t\thdr->b_l2hdr.b_dev = dev;\n\t\t\thdr->b_l2hdr.b_hits = 0;\n\n\t\t\thdr->b_l2hdr.b_daddr = dev->l2ad_hand;\n\t\t\thdr->b_l2hdr.b_arcs_state =\n\t\t\t    hdr->b_l1hdr.b_state->arcs_state;\n\t\t\tarc_hdr_set_flags(hdr, ARC_FLAG_HAS_L2HDR);\n\n\t\t\tmutex_enter(&dev->l2ad_mtx);\n\t\t\tlist_insert_head(&dev->l2ad_buflist, hdr);\n\t\t\tmutex_exit(&dev->l2ad_mtx);\n\n\t\t\t(void) zfs_refcount_add_many(&dev->l2ad_alloc,\n\t\t\t    arc_hdr_size(hdr), hdr);\n\n\t\t\twzio = zio_write_phys(pio, dev->l2ad_vdev,\n\t\t\t    hdr->b_l2hdr.b_daddr, asize, to_write,\n\t\t\t    ZIO_CHECKSUM_OFF, NULL, hdr,\n\t\t\t    ZIO_PRIORITY_ASYNC_WRITE,\n\t\t\t    ZIO_FLAG_CANFAIL, B_FALSE);\n\n\t\t\twrite_lsize += HDR_GET_LSIZE(hdr);\n\t\t\tDTRACE_PROBE2(l2arc__write, vdev_t *, dev->l2ad_vdev,\n\t\t\t    zio_t *, wzio);\n\n\t\t\twrite_psize += psize;\n\t\t\twrite_asize += asize;\n\t\t\tdev->l2ad_hand += asize;\n\t\t\tl2arc_hdr_arcstats_increment(hdr);\n\t\t\tvdev_space_update(dev->l2ad_vdev, asize, 0, 0);\n\n\t\t\tmutex_exit(hash_lock);\n\n\t\t\t \n\t\t\tif (l2arc_log_blk_insert(dev, hdr)) {\n\t\t\t\t \n\t\t\t\twrite_asize +=\n\t\t\t\t    l2arc_log_blk_commit(dev, pio, cb);\n\t\t\t}\n\n\t\t\tzio_nowait(wzio);\n\t\t}\n\n\t\tmultilist_sublist_unlock(mls);\n\n\t\tif (full == B_TRUE)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (pio == NULL) {\n\t\tASSERT0(write_lsize);\n\t\tASSERT(!HDR_HAS_L1HDR(head));\n\t\tkmem_cache_free(hdr_l2only_cache, head);\n\n\t\t \n\t\tif (dev->l2ad_evict != l2dhdr->dh_evict)\n\t\t\tl2arc_dev_hdr_update(dev);\n\n\t\treturn (0);\n\t}\n\n\tif (!dev->l2ad_first)\n\t\tASSERT3U(dev->l2ad_hand, <=, dev->l2ad_evict);\n\n\tASSERT3U(write_asize, <=, target_sz);\n\tARCSTAT_BUMP(arcstat_l2_writes_sent);\n\tARCSTAT_INCR(arcstat_l2_write_bytes, write_psize);\n\n\tdev->l2ad_writing = B_TRUE;\n\t(void) zio_wait(pio);\n\tdev->l2ad_writing = B_FALSE;\n\n\t \n\tl2arc_dev_hdr_update(dev);\n\n\treturn (write_asize);\n}\n\nstatic boolean_t\nl2arc_hdr_limit_reached(void)\n{\n\tint64_t s = aggsum_upper_bound(&arc_sums.arcstat_l2_hdr_size);\n\n\treturn (arc_reclaim_needed() ||\n\t    (s > (arc_warm ? arc_c : arc_c_max) * l2arc_meta_percent / 100));\n}\n\n \nstatic  __attribute__((noreturn)) void\nl2arc_feed_thread(void *unused)\n{\n\t(void) unused;\n\tcallb_cpr_t cpr;\n\tl2arc_dev_t *dev;\n\tspa_t *spa;\n\tuint64_t size, wrote;\n\tclock_t begin, next = ddi_get_lbolt();\n\tfstrans_cookie_t cookie;\n\n\tCALLB_CPR_INIT(&cpr, &l2arc_feed_thr_lock, callb_generic_cpr, FTAG);\n\n\tmutex_enter(&l2arc_feed_thr_lock);\n\n\tcookie = spl_fstrans_mark();\n\twhile (l2arc_thread_exit == 0) {\n\t\tCALLB_CPR_SAFE_BEGIN(&cpr);\n\t\t(void) cv_timedwait_idle(&l2arc_feed_thr_cv,\n\t\t    &l2arc_feed_thr_lock, next);\n\t\tCALLB_CPR_SAFE_END(&cpr, &l2arc_feed_thr_lock);\n\t\tnext = ddi_get_lbolt() + hz;\n\n\t\t \n\t\tmutex_enter(&l2arc_dev_mtx);\n\t\tif (l2arc_ndev == 0) {\n\t\t\tmutex_exit(&l2arc_dev_mtx);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_exit(&l2arc_dev_mtx);\n\t\tbegin = ddi_get_lbolt();\n\n\t\t \n\t\tif ((dev = l2arc_dev_get_next()) == NULL)\n\t\t\tcontinue;\n\n\t\tspa = dev->l2ad_spa;\n\t\tASSERT3P(spa, !=, NULL);\n\n\t\t \n\t\tif (!spa_writeable(spa)) {\n\t\t\tnext = ddi_get_lbolt() + 5 * l2arc_feed_secs * hz;\n\t\t\tspa_config_exit(spa, SCL_L2ARC, dev);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (l2arc_hdr_limit_reached()) {\n\t\t\tARCSTAT_BUMP(arcstat_l2_abort_lowmem);\n\t\t\tspa_config_exit(spa, SCL_L2ARC, dev);\n\t\t\tcontinue;\n\t\t}\n\n\t\tARCSTAT_BUMP(arcstat_l2_feeds);\n\n\t\tsize = l2arc_write_size(dev);\n\n\t\t \n\t\tl2arc_evict(dev, size, B_FALSE);\n\n\t\t \n\t\twrote = l2arc_write_buffers(spa, dev, size);\n\n\t\t \n\t\tnext = l2arc_write_interval(begin, size, wrote);\n\t\tspa_config_exit(spa, SCL_L2ARC, dev);\n\t}\n\tspl_fstrans_unmark(cookie);\n\n\tl2arc_thread_exit = 0;\n\tcv_broadcast(&l2arc_feed_thr_cv);\n\tCALLB_CPR_EXIT(&cpr);\t\t \n\tthread_exit();\n}\n\nboolean_t\nl2arc_vdev_present(vdev_t *vd)\n{\n\treturn (l2arc_vdev_get(vd) != NULL);\n}\n\n \nl2arc_dev_t *\nl2arc_vdev_get(vdev_t *vd)\n{\n\tl2arc_dev_t\t*dev;\n\n\tmutex_enter(&l2arc_dev_mtx);\n\tfor (dev = list_head(l2arc_dev_list); dev != NULL;\n\t    dev = list_next(l2arc_dev_list, dev)) {\n\t\tif (dev->l2ad_vdev == vd)\n\t\t\tbreak;\n\t}\n\tmutex_exit(&l2arc_dev_mtx);\n\n\treturn (dev);\n}\n\nstatic void\nl2arc_rebuild_dev(l2arc_dev_t *dev, boolean_t reopen)\n{\n\tl2arc_dev_hdr_phys_t *l2dhdr = dev->l2ad_dev_hdr;\n\tuint64_t l2dhdr_asize = dev->l2ad_dev_hdr_asize;\n\tspa_t *spa = dev->l2ad_spa;\n\n\t \n\tif (dev->l2ad_end < l2arc_rebuild_blocks_min_l2size) {\n\t\tdev->l2ad_log_entries = 0;\n\t} else {\n\t\tdev->l2ad_log_entries = MIN((dev->l2ad_end -\n\t\t    dev->l2ad_start) >> SPA_MAXBLOCKSHIFT,\n\t\t    L2ARC_LOG_BLK_MAX_ENTRIES);\n\t}\n\n\t \n\tif (l2arc_dev_hdr_read(dev) == 0 && dev->l2ad_log_entries > 0) {\n\t\t \n\t\tif (reopen) {\n\t\t\tif (!l2arc_rebuild_enabled) {\n\t\t\t\treturn;\n\t\t\t} else {\n\t\t\t\tl2arc_evict(dev, 0, B_TRUE);\n\t\t\t\t \n\t\t\t\tdev->l2ad_log_ent_idx = 0;\n\t\t\t\tdev->l2ad_log_blk_payload_asize = 0;\n\t\t\t\tdev->l2ad_log_blk_payload_start = 0;\n\t\t\t}\n\t\t}\n\t\t \n\t\tdev->l2ad_rebuild = B_TRUE;\n\t} else if (spa_writeable(spa)) {\n\t\t \n\t\tif (l2arc_trim_ahead > 0) {\n\t\t\tdev->l2ad_trim_all = B_TRUE;\n\t\t} else {\n\t\t\tmemset(l2dhdr, 0, l2dhdr_asize);\n\t\t\tl2arc_dev_hdr_update(dev);\n\t\t}\n\t}\n}\n\n \nvoid\nl2arc_add_vdev(spa_t *spa, vdev_t *vd)\n{\n\tl2arc_dev_t\t\t*adddev;\n\tuint64_t\t\tl2dhdr_asize;\n\n\tASSERT(!l2arc_vdev_present(vd));\n\n\t \n\tadddev = vmem_zalloc(sizeof (l2arc_dev_t), KM_SLEEP);\n\tadddev->l2ad_spa = spa;\n\tadddev->l2ad_vdev = vd;\n\t \n\tl2dhdr_asize = adddev->l2ad_dev_hdr_asize =\n\t    MAX(sizeof (*adddev->l2ad_dev_hdr), 1 << vd->vdev_ashift);\n\tadddev->l2ad_start = VDEV_LABEL_START_SIZE + l2dhdr_asize;\n\tadddev->l2ad_end = VDEV_LABEL_START_SIZE + vdev_get_min_asize(vd);\n\tASSERT3U(adddev->l2ad_start, <, adddev->l2ad_end);\n\tadddev->l2ad_hand = adddev->l2ad_start;\n\tadddev->l2ad_evict = adddev->l2ad_start;\n\tadddev->l2ad_first = B_TRUE;\n\tadddev->l2ad_writing = B_FALSE;\n\tadddev->l2ad_trim_all = B_FALSE;\n\tlist_link_init(&adddev->l2ad_node);\n\tadddev->l2ad_dev_hdr = kmem_zalloc(l2dhdr_asize, KM_SLEEP);\n\n\tmutex_init(&adddev->l2ad_mtx, NULL, MUTEX_DEFAULT, NULL);\n\t \n\tlist_create(&adddev->l2ad_buflist, sizeof (arc_buf_hdr_t),\n\t    offsetof(arc_buf_hdr_t, b_l2hdr.b_l2node));\n\n\t \n\tlist_create(&adddev->l2ad_lbptr_list, sizeof (l2arc_lb_ptr_buf_t),\n\t    offsetof(l2arc_lb_ptr_buf_t, node));\n\n\tvdev_space_update(vd, 0, 0, adddev->l2ad_end - adddev->l2ad_hand);\n\tzfs_refcount_create(&adddev->l2ad_alloc);\n\tzfs_refcount_create(&adddev->l2ad_lb_asize);\n\tzfs_refcount_create(&adddev->l2ad_lb_count);\n\n\t \n\tl2arc_rebuild_dev(adddev, B_FALSE);\n\n\t \n\tmutex_enter(&l2arc_dev_mtx);\n\tlist_insert_head(l2arc_dev_list, adddev);\n\tatomic_inc_64(&l2arc_ndev);\n\tmutex_exit(&l2arc_dev_mtx);\n}\n\n \nvoid\nl2arc_rebuild_vdev(vdev_t *vd, boolean_t reopen)\n{\n\tl2arc_dev_t\t\t*dev = NULL;\n\n\tdev = l2arc_vdev_get(vd);\n\tASSERT3P(dev, !=, NULL);\n\n\t \n\tl2arc_rebuild_dev(dev, reopen);\n}\n\n \nvoid\nl2arc_remove_vdev(vdev_t *vd)\n{\n\tl2arc_dev_t *remdev = NULL;\n\n\t \n\tremdev = l2arc_vdev_get(vd);\n\tASSERT3P(remdev, !=, NULL);\n\n\t \n\tmutex_enter(&l2arc_rebuild_thr_lock);\n\tif (remdev->l2ad_rebuild_began == B_TRUE) {\n\t\tremdev->l2ad_rebuild_cancel = B_TRUE;\n\t\twhile (remdev->l2ad_rebuild == B_TRUE)\n\t\t\tcv_wait(&l2arc_rebuild_thr_cv, &l2arc_rebuild_thr_lock);\n\t}\n\tmutex_exit(&l2arc_rebuild_thr_lock);\n\n\t \n\tmutex_enter(&l2arc_dev_mtx);\n\tlist_remove(l2arc_dev_list, remdev);\n\tl2arc_dev_last = NULL;\t\t \n\tatomic_dec_64(&l2arc_ndev);\n\tmutex_exit(&l2arc_dev_mtx);\n\n\t \n\tl2arc_evict(remdev, 0, B_TRUE);\n\tlist_destroy(&remdev->l2ad_buflist);\n\tASSERT(list_is_empty(&remdev->l2ad_lbptr_list));\n\tlist_destroy(&remdev->l2ad_lbptr_list);\n\tmutex_destroy(&remdev->l2ad_mtx);\n\tzfs_refcount_destroy(&remdev->l2ad_alloc);\n\tzfs_refcount_destroy(&remdev->l2ad_lb_asize);\n\tzfs_refcount_destroy(&remdev->l2ad_lb_count);\n\tkmem_free(remdev->l2ad_dev_hdr, remdev->l2ad_dev_hdr_asize);\n\tvmem_free(remdev, sizeof (l2arc_dev_t));\n}\n\nvoid\nl2arc_init(void)\n{\n\tl2arc_thread_exit = 0;\n\tl2arc_ndev = 0;\n\n\tmutex_init(&l2arc_feed_thr_lock, NULL, MUTEX_DEFAULT, NULL);\n\tcv_init(&l2arc_feed_thr_cv, NULL, CV_DEFAULT, NULL);\n\tmutex_init(&l2arc_rebuild_thr_lock, NULL, MUTEX_DEFAULT, NULL);\n\tcv_init(&l2arc_rebuild_thr_cv, NULL, CV_DEFAULT, NULL);\n\tmutex_init(&l2arc_dev_mtx, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_init(&l2arc_free_on_write_mtx, NULL, MUTEX_DEFAULT, NULL);\n\n\tl2arc_dev_list = &L2ARC_dev_list;\n\tl2arc_free_on_write = &L2ARC_free_on_write;\n\tlist_create(l2arc_dev_list, sizeof (l2arc_dev_t),\n\t    offsetof(l2arc_dev_t, l2ad_node));\n\tlist_create(l2arc_free_on_write, sizeof (l2arc_data_free_t),\n\t    offsetof(l2arc_data_free_t, l2df_list_node));\n}\n\nvoid\nl2arc_fini(void)\n{\n\tmutex_destroy(&l2arc_feed_thr_lock);\n\tcv_destroy(&l2arc_feed_thr_cv);\n\tmutex_destroy(&l2arc_rebuild_thr_lock);\n\tcv_destroy(&l2arc_rebuild_thr_cv);\n\tmutex_destroy(&l2arc_dev_mtx);\n\tmutex_destroy(&l2arc_free_on_write_mtx);\n\n\tlist_destroy(l2arc_dev_list);\n\tlist_destroy(l2arc_free_on_write);\n}\n\nvoid\nl2arc_start(void)\n{\n\tif (!(spa_mode_global & SPA_MODE_WRITE))\n\t\treturn;\n\n\t(void) thread_create(NULL, 0, l2arc_feed_thread, NULL, 0, &p0,\n\t    TS_RUN, defclsyspri);\n}\n\nvoid\nl2arc_stop(void)\n{\n\tif (!(spa_mode_global & SPA_MODE_WRITE))\n\t\treturn;\n\n\tmutex_enter(&l2arc_feed_thr_lock);\n\tcv_signal(&l2arc_feed_thr_cv);\t \n\tl2arc_thread_exit = 1;\n\twhile (l2arc_thread_exit != 0)\n\t\tcv_wait(&l2arc_feed_thr_cv, &l2arc_feed_thr_lock);\n\tmutex_exit(&l2arc_feed_thr_lock);\n}\n\n \nvoid\nl2arc_spa_rebuild_start(spa_t *spa)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\t \n\tfor (int i = 0; i < spa->spa_l2cache.sav_count; i++) {\n\t\tl2arc_dev_t *dev =\n\t\t    l2arc_vdev_get(spa->spa_l2cache.sav_vdevs[i]);\n\t\tif (dev == NULL) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\tmutex_enter(&l2arc_rebuild_thr_lock);\n\t\tif (dev->l2ad_rebuild && !dev->l2ad_rebuild_cancel) {\n\t\t\tdev->l2ad_rebuild_began = B_TRUE;\n\t\t\t(void) thread_create(NULL, 0, l2arc_dev_rebuild_thread,\n\t\t\t    dev, 0, &p0, TS_RUN, minclsyspri);\n\t\t}\n\t\tmutex_exit(&l2arc_rebuild_thr_lock);\n\t}\n}\n\n \nstatic __attribute__((noreturn)) void\nl2arc_dev_rebuild_thread(void *arg)\n{\n\tl2arc_dev_t *dev = arg;\n\n\tVERIFY(!dev->l2ad_rebuild_cancel);\n\tVERIFY(dev->l2ad_rebuild);\n\t(void) l2arc_rebuild(dev);\n\tmutex_enter(&l2arc_rebuild_thr_lock);\n\tdev->l2ad_rebuild_began = B_FALSE;\n\tdev->l2ad_rebuild = B_FALSE;\n\tmutex_exit(&l2arc_rebuild_thr_lock);\n\n\tthread_exit();\n}\n\n \nstatic int\nl2arc_rebuild(l2arc_dev_t *dev)\n{\n\tvdev_t\t\t\t*vd = dev->l2ad_vdev;\n\tspa_t\t\t\t*spa = vd->vdev_spa;\n\tint\t\t\terr = 0;\n\tl2arc_dev_hdr_phys_t\t*l2dhdr = dev->l2ad_dev_hdr;\n\tl2arc_log_blk_phys_t\t*this_lb, *next_lb;\n\tzio_t\t\t\t*this_io = NULL, *next_io = NULL;\n\tl2arc_log_blkptr_t\tlbps[2];\n\tl2arc_lb_ptr_buf_t\t*lb_ptr_buf;\n\tboolean_t\t\tlock_held;\n\n\tthis_lb = vmem_zalloc(sizeof (*this_lb), KM_SLEEP);\n\tnext_lb = vmem_zalloc(sizeof (*next_lb), KM_SLEEP);\n\n\t \n\tspa_config_enter(spa, SCL_L2ARC, vd, RW_READER);\n\tlock_held = B_TRUE;\n\n\t \n\tdev->l2ad_evict = MAX(l2dhdr->dh_evict, dev->l2ad_start);\n\tdev->l2ad_hand = MAX(l2dhdr->dh_start_lbps[0].lbp_daddr +\n\t    L2BLK_GET_PSIZE((&l2dhdr->dh_start_lbps[0])->lbp_prop),\n\t    dev->l2ad_start);\n\tdev->l2ad_first = !!(l2dhdr->dh_flags & L2ARC_DEV_HDR_EVICT_FIRST);\n\n\tvd->vdev_trim_action_time = l2dhdr->dh_trim_action_time;\n\tvd->vdev_trim_state = l2dhdr->dh_trim_state;\n\n\t \n\tif (!l2arc_rebuild_enabled)\n\t\tgoto out;\n\n\t \n\tmemcpy(lbps, l2dhdr->dh_start_lbps, sizeof (lbps));\n\n\t \n\tfor (;;) {\n\t\tif (!l2arc_log_blkptr_valid(dev, &lbps[0]))\n\t\t\tbreak;\n\n\t\tif ((err = l2arc_log_blk_read(dev, &lbps[0], &lbps[1],\n\t\t    this_lb, next_lb, this_io, &next_io)) != 0)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (l2arc_hdr_limit_reached()) {\n\t\t\tARCSTAT_BUMP(arcstat_l2_rebuild_abort_lowmem);\n\t\t\tcmn_err(CE_NOTE, \"System running low on memory, \"\n\t\t\t    \"aborting L2ARC rebuild.\");\n\t\t\terr = SET_ERROR(ENOMEM);\n\t\t\tgoto out;\n\t\t}\n\n\t\tspa_config_exit(spa, SCL_L2ARC, vd);\n\t\tlock_held = B_FALSE;\n\n\t\t \n\t\tuint64_t asize = L2BLK_GET_PSIZE((&lbps[0])->lbp_prop);\n\t\tl2arc_log_blk_restore(dev, this_lb, asize);\n\n\t\t \n\t\tlb_ptr_buf = kmem_zalloc(sizeof (l2arc_lb_ptr_buf_t), KM_SLEEP);\n\t\tlb_ptr_buf->lb_ptr = kmem_zalloc(sizeof (l2arc_log_blkptr_t),\n\t\t    KM_SLEEP);\n\t\tmemcpy(lb_ptr_buf->lb_ptr, &lbps[0],\n\t\t    sizeof (l2arc_log_blkptr_t));\n\t\tmutex_enter(&dev->l2ad_mtx);\n\t\tlist_insert_tail(&dev->l2ad_lbptr_list, lb_ptr_buf);\n\t\tARCSTAT_INCR(arcstat_l2_log_blk_asize, asize);\n\t\tARCSTAT_BUMP(arcstat_l2_log_blk_count);\n\t\tzfs_refcount_add_many(&dev->l2ad_lb_asize, asize, lb_ptr_buf);\n\t\tzfs_refcount_add(&dev->l2ad_lb_count, lb_ptr_buf);\n\t\tmutex_exit(&dev->l2ad_mtx);\n\t\tvdev_space_update(vd, asize, 0, 0);\n\n\t\t \n\t\tif (l2arc_range_check_overlap(lbps[1].lbp_payload_start,\n\t\t    lbps[0].lbp_payload_start, dev->l2ad_evict) &&\n\t\t    !dev->l2ad_first)\n\t\t\tgoto out;\n\n\t\tkpreempt(KPREEMPT_SYNC);\n\t\tfor (;;) {\n\t\t\tmutex_enter(&l2arc_rebuild_thr_lock);\n\t\t\tif (dev->l2ad_rebuild_cancel) {\n\t\t\t\tdev->l2ad_rebuild = B_FALSE;\n\t\t\t\tcv_signal(&l2arc_rebuild_thr_cv);\n\t\t\t\tmutex_exit(&l2arc_rebuild_thr_lock);\n\t\t\t\terr = SET_ERROR(ECANCELED);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tmutex_exit(&l2arc_rebuild_thr_lock);\n\t\t\tif (spa_config_tryenter(spa, SCL_L2ARC, vd,\n\t\t\t    RW_READER)) {\n\t\t\t\tlock_held = B_TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t \n\t\t\tdelay(1);\n\t\t}\n\n\t\t \n\t\tlbps[0] = lbps[1];\n\t\tlbps[1] = this_lb->lb_prev_lbp;\n\t\tPTR_SWAP(this_lb, next_lb);\n\t\tthis_io = next_io;\n\t\tnext_io = NULL;\n\t}\n\n\tif (this_io != NULL)\n\t\tl2arc_log_blk_fetch_abort(this_io);\nout:\n\tif (next_io != NULL)\n\t\tl2arc_log_blk_fetch_abort(next_io);\n\tvmem_free(this_lb, sizeof (*this_lb));\n\tvmem_free(next_lb, sizeof (*next_lb));\n\n\tif (!l2arc_rebuild_enabled) {\n\t\tspa_history_log_internal(spa, \"L2ARC rebuild\", NULL,\n\t\t    \"disabled\");\n\t} else if (err == 0 && zfs_refcount_count(&dev->l2ad_lb_count) > 0) {\n\t\tARCSTAT_BUMP(arcstat_l2_rebuild_success);\n\t\tspa_history_log_internal(spa, \"L2ARC rebuild\", NULL,\n\t\t    \"successful, restored %llu blocks\",\n\t\t    (u_longlong_t)zfs_refcount_count(&dev->l2ad_lb_count));\n\t} else if (err == 0 && zfs_refcount_count(&dev->l2ad_lb_count) == 0) {\n\t\t \n\t\tspa_history_log_internal(spa, \"L2ARC rebuild\", NULL,\n\t\t    \"no valid log blocks\");\n\t\tmemset(l2dhdr, 0, dev->l2ad_dev_hdr_asize);\n\t\tl2arc_dev_hdr_update(dev);\n\t} else if (err == ECANCELED) {\n\t\t \n\t\tzfs_dbgmsg(\"L2ARC rebuild aborted, restored %llu blocks\",\n\t\t    (u_longlong_t)zfs_refcount_count(&dev->l2ad_lb_count));\n\t} else if (err != 0) {\n\t\tspa_history_log_internal(spa, \"L2ARC rebuild\", NULL,\n\t\t    \"aborted, restored %llu blocks\",\n\t\t    (u_longlong_t)zfs_refcount_count(&dev->l2ad_lb_count));\n\t}\n\n\tif (lock_held)\n\t\tspa_config_exit(spa, SCL_L2ARC, vd);\n\n\treturn (err);\n}\n\n \nstatic int\nl2arc_dev_hdr_read(l2arc_dev_t *dev)\n{\n\tint\t\t\terr;\n\tuint64_t\t\tguid;\n\tl2arc_dev_hdr_phys_t\t*l2dhdr = dev->l2ad_dev_hdr;\n\tconst uint64_t\t\tl2dhdr_asize = dev->l2ad_dev_hdr_asize;\n\tabd_t \t\t\t*abd;\n\n\tguid = spa_guid(dev->l2ad_vdev->vdev_spa);\n\n\tabd = abd_get_from_buf(l2dhdr, l2dhdr_asize);\n\n\terr = zio_wait(zio_read_phys(NULL, dev->l2ad_vdev,\n\t    VDEV_LABEL_START_SIZE, l2dhdr_asize, abd,\n\t    ZIO_CHECKSUM_LABEL, NULL, NULL, ZIO_PRIORITY_SYNC_READ,\n\t    ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_RETRY |\n\t    ZIO_FLAG_SPECULATIVE, B_FALSE));\n\n\tabd_free(abd);\n\n\tif (err != 0) {\n\t\tARCSTAT_BUMP(arcstat_l2_rebuild_abort_dh_errors);\n\t\tzfs_dbgmsg(\"L2ARC IO error (%d) while reading device header, \"\n\t\t    \"vdev guid: %llu\", err,\n\t\t    (u_longlong_t)dev->l2ad_vdev->vdev_guid);\n\t\treturn (err);\n\t}\n\n\tif (l2dhdr->dh_magic == BSWAP_64(L2ARC_DEV_HDR_MAGIC))\n\t\tbyteswap_uint64_array(l2dhdr, sizeof (*l2dhdr));\n\n\tif (l2dhdr->dh_magic != L2ARC_DEV_HDR_MAGIC ||\n\t    l2dhdr->dh_spa_guid != guid ||\n\t    l2dhdr->dh_vdev_guid != dev->l2ad_vdev->vdev_guid ||\n\t    l2dhdr->dh_version != L2ARC_PERSISTENT_VERSION ||\n\t    l2dhdr->dh_log_entries != dev->l2ad_log_entries ||\n\t    l2dhdr->dh_end != dev->l2ad_end ||\n\t    !l2arc_range_check_overlap(dev->l2ad_start, dev->l2ad_end,\n\t    l2dhdr->dh_evict) ||\n\t    (l2dhdr->dh_trim_state != VDEV_TRIM_COMPLETE &&\n\t    l2arc_trim_ahead > 0)) {\n\t\t \n\t\tARCSTAT_BUMP(arcstat_l2_rebuild_abort_unsupported);\n\t\treturn (SET_ERROR(ENOTSUP));\n\t}\n\n\treturn (0);\n}\n\n \nstatic int\nl2arc_log_blk_read(l2arc_dev_t *dev,\n    const l2arc_log_blkptr_t *this_lbp, const l2arc_log_blkptr_t *next_lbp,\n    l2arc_log_blk_phys_t *this_lb, l2arc_log_blk_phys_t *next_lb,\n    zio_t *this_io, zio_t **next_io)\n{\n\tint\t\terr = 0;\n\tzio_cksum_t\tcksum;\n\tabd_t\t\t*abd = NULL;\n\tuint64_t\tasize;\n\n\tASSERT(this_lbp != NULL && next_lbp != NULL);\n\tASSERT(this_lb != NULL && next_lb != NULL);\n\tASSERT(next_io != NULL && *next_io == NULL);\n\tASSERT(l2arc_log_blkptr_valid(dev, this_lbp));\n\n\t \n\tif (this_io == NULL) {\n\t\tthis_io = l2arc_log_blk_fetch(dev->l2ad_vdev, this_lbp,\n\t\t    this_lb);\n\t}\n\n\t \n\tif (l2arc_log_blkptr_valid(dev, next_lbp)) {\n\t\t \n\t\t*next_io = l2arc_log_blk_fetch(dev->l2ad_vdev, next_lbp,\n\t\t    next_lb);\n\t}\n\n\t \n\tif ((err = zio_wait(this_io)) != 0) {\n\t\tARCSTAT_BUMP(arcstat_l2_rebuild_abort_io_errors);\n\t\tzfs_dbgmsg(\"L2ARC IO error (%d) while reading log block, \"\n\t\t    \"offset: %llu, vdev guid: %llu\", err,\n\t\t    (u_longlong_t)this_lbp->lbp_daddr,\n\t\t    (u_longlong_t)dev->l2ad_vdev->vdev_guid);\n\t\tgoto cleanup;\n\t}\n\n\t \n\tasize = L2BLK_GET_PSIZE((this_lbp)->lbp_prop);\n\tfletcher_4_native(this_lb, asize, NULL, &cksum);\n\tif (!ZIO_CHECKSUM_EQUAL(cksum, this_lbp->lbp_cksum)) {\n\t\tARCSTAT_BUMP(arcstat_l2_rebuild_abort_cksum_lb_errors);\n\t\tzfs_dbgmsg(\"L2ARC log block cksum failed, offset: %llu, \"\n\t\t    \"vdev guid: %llu, l2ad_hand: %llu, l2ad_evict: %llu\",\n\t\t    (u_longlong_t)this_lbp->lbp_daddr,\n\t\t    (u_longlong_t)dev->l2ad_vdev->vdev_guid,\n\t\t    (u_longlong_t)dev->l2ad_hand,\n\t\t    (u_longlong_t)dev->l2ad_evict);\n\t\terr = SET_ERROR(ECKSUM);\n\t\tgoto cleanup;\n\t}\n\n\t \n\tswitch (L2BLK_GET_COMPRESS((this_lbp)->lbp_prop)) {\n\tcase ZIO_COMPRESS_OFF:\n\t\tbreak;\n\tcase ZIO_COMPRESS_LZ4:\n\t\tabd = abd_alloc_for_io(asize, B_TRUE);\n\t\tabd_copy_from_buf_off(abd, this_lb, 0, asize);\n\t\tif ((err = zio_decompress_data(\n\t\t    L2BLK_GET_COMPRESS((this_lbp)->lbp_prop),\n\t\t    abd, this_lb, asize, sizeof (*this_lb), NULL)) != 0) {\n\t\t\terr = SET_ERROR(EINVAL);\n\t\t\tgoto cleanup;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\terr = SET_ERROR(EINVAL);\n\t\tgoto cleanup;\n\t}\n\tif (this_lb->lb_magic == BSWAP_64(L2ARC_LOG_BLK_MAGIC))\n\t\tbyteswap_uint64_array(this_lb, sizeof (*this_lb));\n\tif (this_lb->lb_magic != L2ARC_LOG_BLK_MAGIC) {\n\t\terr = SET_ERROR(EINVAL);\n\t\tgoto cleanup;\n\t}\ncleanup:\n\t \n\tif (err != 0 && *next_io != NULL) {\n\t\tl2arc_log_blk_fetch_abort(*next_io);\n\t\t*next_io = NULL;\n\t}\n\tif (abd != NULL)\n\t\tabd_free(abd);\n\treturn (err);\n}\n\n \nstatic void\nl2arc_log_blk_restore(l2arc_dev_t *dev, const l2arc_log_blk_phys_t *lb,\n    uint64_t lb_asize)\n{\n\tuint64_t\tsize = 0, asize = 0;\n\tuint64_t\tlog_entries = dev->l2ad_log_entries;\n\n\t \n\tarc_adapt(log_entries * HDR_L2ONLY_SIZE);\n\n\tfor (int i = log_entries - 1; i >= 0; i--) {\n\t\t \n\t\tsize += L2BLK_GET_LSIZE((&lb->lb_entries[i])->le_prop);\n\t\tasize += vdev_psize_to_asize(dev->l2ad_vdev,\n\t\t    L2BLK_GET_PSIZE((&lb->lb_entries[i])->le_prop));\n\t\tl2arc_hdr_restore(&lb->lb_entries[i], dev);\n\t}\n\n\t \n\tARCSTAT_INCR(arcstat_l2_rebuild_size, size);\n\tARCSTAT_INCR(arcstat_l2_rebuild_asize, asize);\n\tARCSTAT_INCR(arcstat_l2_rebuild_bufs, log_entries);\n\tARCSTAT_F_AVG(arcstat_l2_log_blk_avg_asize, lb_asize);\n\tARCSTAT_F_AVG(arcstat_l2_data_to_meta_ratio, asize / lb_asize);\n\tARCSTAT_BUMP(arcstat_l2_rebuild_log_blks);\n}\n\n \nstatic void\nl2arc_hdr_restore(const l2arc_log_ent_phys_t *le, l2arc_dev_t *dev)\n{\n\tarc_buf_hdr_t\t\t*hdr, *exists;\n\tkmutex_t\t\t*hash_lock;\n\tarc_buf_contents_t\ttype = L2BLK_GET_TYPE((le)->le_prop);\n\tuint64_t\t\tasize;\n\n\t \n\thdr = arc_buf_alloc_l2only(L2BLK_GET_LSIZE((le)->le_prop), type,\n\t    dev, le->le_dva, le->le_daddr,\n\t    L2BLK_GET_PSIZE((le)->le_prop), le->le_birth,\n\t    L2BLK_GET_COMPRESS((le)->le_prop), le->le_complevel,\n\t    L2BLK_GET_PROTECTED((le)->le_prop),\n\t    L2BLK_GET_PREFETCH((le)->le_prop),\n\t    L2BLK_GET_STATE((le)->le_prop));\n\tasize = vdev_psize_to_asize(dev->l2ad_vdev,\n\t    L2BLK_GET_PSIZE((le)->le_prop));\n\n\t \n\tl2arc_hdr_arcstats_increment(hdr);\n\tvdev_space_update(dev->l2ad_vdev, asize, 0, 0);\n\n\tmutex_enter(&dev->l2ad_mtx);\n\tlist_insert_tail(&dev->l2ad_buflist, hdr);\n\t(void) zfs_refcount_add_many(&dev->l2ad_alloc, arc_hdr_size(hdr), hdr);\n\tmutex_exit(&dev->l2ad_mtx);\n\n\texists = buf_hash_insert(hdr, &hash_lock);\n\tif (exists) {\n\t\t \n\t\tarc_hdr_destroy(hdr);\n\t\t \n\t\tif (!HDR_HAS_L2HDR(exists)) {\n\t\t\tarc_hdr_set_flags(exists, ARC_FLAG_HAS_L2HDR);\n\t\t\texists->b_l2hdr.b_dev = dev;\n\t\t\texists->b_l2hdr.b_daddr = le->le_daddr;\n\t\t\texists->b_l2hdr.b_arcs_state =\n\t\t\t    L2BLK_GET_STATE((le)->le_prop);\n\t\t\tmutex_enter(&dev->l2ad_mtx);\n\t\t\tlist_insert_tail(&dev->l2ad_buflist, exists);\n\t\t\t(void) zfs_refcount_add_many(&dev->l2ad_alloc,\n\t\t\t    arc_hdr_size(exists), exists);\n\t\t\tmutex_exit(&dev->l2ad_mtx);\n\t\t\tl2arc_hdr_arcstats_increment(exists);\n\t\t\tvdev_space_update(dev->l2ad_vdev, asize, 0, 0);\n\t\t}\n\t\tARCSTAT_BUMP(arcstat_l2_rebuild_bufs_precached);\n\t}\n\n\tmutex_exit(hash_lock);\n}\n\n \nstatic zio_t *\nl2arc_log_blk_fetch(vdev_t *vd, const l2arc_log_blkptr_t *lbp,\n    l2arc_log_blk_phys_t *lb)\n{\n\tuint32_t\t\tasize;\n\tzio_t\t\t\t*pio;\n\tl2arc_read_callback_t\t*cb;\n\n\t \n\tasize = L2BLK_GET_PSIZE((lbp)->lbp_prop);\n\tASSERT(asize <= sizeof (l2arc_log_blk_phys_t));\n\n\tcb = kmem_zalloc(sizeof (l2arc_read_callback_t), KM_SLEEP);\n\tcb->l2rcb_abd = abd_get_from_buf(lb, asize);\n\tpio = zio_root(vd->vdev_spa, l2arc_blk_fetch_done, cb,\n\t    ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_RETRY);\n\t(void) zio_nowait(zio_read_phys(pio, vd, lbp->lbp_daddr, asize,\n\t    cb->l2rcb_abd, ZIO_CHECKSUM_OFF, NULL, NULL,\n\t    ZIO_PRIORITY_ASYNC_READ, ZIO_FLAG_CANFAIL |\n\t    ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_RETRY, B_FALSE));\n\n\treturn (pio);\n}\n\n \nstatic void\nl2arc_log_blk_fetch_abort(zio_t *zio)\n{\n\t(void) zio_wait(zio);\n}\n\n \nvoid\nl2arc_dev_hdr_update(l2arc_dev_t *dev)\n{\n\tl2arc_dev_hdr_phys_t\t*l2dhdr = dev->l2ad_dev_hdr;\n\tconst uint64_t\t\tl2dhdr_asize = dev->l2ad_dev_hdr_asize;\n\tabd_t\t\t\t*abd;\n\tint\t\t\terr;\n\n\tVERIFY(spa_config_held(dev->l2ad_spa, SCL_STATE_ALL, RW_READER));\n\n\tl2dhdr->dh_magic = L2ARC_DEV_HDR_MAGIC;\n\tl2dhdr->dh_version = L2ARC_PERSISTENT_VERSION;\n\tl2dhdr->dh_spa_guid = spa_guid(dev->l2ad_vdev->vdev_spa);\n\tl2dhdr->dh_vdev_guid = dev->l2ad_vdev->vdev_guid;\n\tl2dhdr->dh_log_entries = dev->l2ad_log_entries;\n\tl2dhdr->dh_evict = dev->l2ad_evict;\n\tl2dhdr->dh_start = dev->l2ad_start;\n\tl2dhdr->dh_end = dev->l2ad_end;\n\tl2dhdr->dh_lb_asize = zfs_refcount_count(&dev->l2ad_lb_asize);\n\tl2dhdr->dh_lb_count = zfs_refcount_count(&dev->l2ad_lb_count);\n\tl2dhdr->dh_flags = 0;\n\tl2dhdr->dh_trim_action_time = dev->l2ad_vdev->vdev_trim_action_time;\n\tl2dhdr->dh_trim_state = dev->l2ad_vdev->vdev_trim_state;\n\tif (dev->l2ad_first)\n\t\tl2dhdr->dh_flags |= L2ARC_DEV_HDR_EVICT_FIRST;\n\n\tabd = abd_get_from_buf(l2dhdr, l2dhdr_asize);\n\n\terr = zio_wait(zio_write_phys(NULL, dev->l2ad_vdev,\n\t    VDEV_LABEL_START_SIZE, l2dhdr_asize, abd, ZIO_CHECKSUM_LABEL, NULL,\n\t    NULL, ZIO_PRIORITY_ASYNC_WRITE, ZIO_FLAG_CANFAIL, B_FALSE));\n\n\tabd_free(abd);\n\n\tif (err != 0) {\n\t\tzfs_dbgmsg(\"L2ARC IO error (%d) while writing device header, \"\n\t\t    \"vdev guid: %llu\", err,\n\t\t    (u_longlong_t)dev->l2ad_vdev->vdev_guid);\n\t}\n}\n\n \nstatic uint64_t\nl2arc_log_blk_commit(l2arc_dev_t *dev, zio_t *pio, l2arc_write_callback_t *cb)\n{\n\tl2arc_log_blk_phys_t\t*lb = &dev->l2ad_log_blk;\n\tl2arc_dev_hdr_phys_t\t*l2dhdr = dev->l2ad_dev_hdr;\n\tuint64_t\t\tpsize, asize;\n\tzio_t\t\t\t*wzio;\n\tl2arc_lb_abd_buf_t\t*abd_buf;\n\tuint8_t\t\t\t*tmpbuf = NULL;\n\tl2arc_lb_ptr_buf_t\t*lb_ptr_buf;\n\n\tVERIFY3S(dev->l2ad_log_ent_idx, ==, dev->l2ad_log_entries);\n\n\tabd_buf = zio_buf_alloc(sizeof (*abd_buf));\n\tabd_buf->abd = abd_get_from_buf(lb, sizeof (*lb));\n\tlb_ptr_buf = kmem_zalloc(sizeof (l2arc_lb_ptr_buf_t), KM_SLEEP);\n\tlb_ptr_buf->lb_ptr = kmem_zalloc(sizeof (l2arc_log_blkptr_t), KM_SLEEP);\n\n\t \n\tlb->lb_prev_lbp = l2dhdr->dh_start_lbps[1];\n\tlb->lb_magic = L2ARC_LOG_BLK_MAGIC;\n\n\t \n\tlist_insert_tail(&cb->l2wcb_abd_list, abd_buf);\n\n\t \n\tpsize = zio_compress_data(ZIO_COMPRESS_LZ4,\n\t    abd_buf->abd, (void **) &tmpbuf, sizeof (*lb), 0);\n\n\t \n\tASSERT(psize != 0);\n\tasize = vdev_psize_to_asize(dev->l2ad_vdev, psize);\n\tASSERT(asize <= sizeof (*lb));\n\n\t \n\tl2dhdr->dh_start_lbps[1] = l2dhdr->dh_start_lbps[0];\n\tl2dhdr->dh_start_lbps[0].lbp_daddr = dev->l2ad_hand;\n\tl2dhdr->dh_start_lbps[0].lbp_payload_asize =\n\t    dev->l2ad_log_blk_payload_asize;\n\tl2dhdr->dh_start_lbps[0].lbp_payload_start =\n\t    dev->l2ad_log_blk_payload_start;\n\tL2BLK_SET_LSIZE(\n\t    (&l2dhdr->dh_start_lbps[0])->lbp_prop, sizeof (*lb));\n\tL2BLK_SET_PSIZE(\n\t    (&l2dhdr->dh_start_lbps[0])->lbp_prop, asize);\n\tL2BLK_SET_CHECKSUM(\n\t    (&l2dhdr->dh_start_lbps[0])->lbp_prop,\n\t    ZIO_CHECKSUM_FLETCHER_4);\n\tif (asize < sizeof (*lb)) {\n\t\t \n\t\tmemset(tmpbuf + psize, 0, asize - psize);\n\t\tL2BLK_SET_COMPRESS(\n\t\t    (&l2dhdr->dh_start_lbps[0])->lbp_prop,\n\t\t    ZIO_COMPRESS_LZ4);\n\t} else {\n\t\t \n\t\tmemcpy(tmpbuf, lb, sizeof (*lb));\n\t\tL2BLK_SET_COMPRESS(\n\t\t    (&l2dhdr->dh_start_lbps[0])->lbp_prop,\n\t\t    ZIO_COMPRESS_OFF);\n\t}\n\n\t \n\tfletcher_4_native(tmpbuf, asize, NULL,\n\t    &l2dhdr->dh_start_lbps[0].lbp_cksum);\n\n\tabd_free(abd_buf->abd);\n\n\t \n\tabd_buf->abd = abd_get_from_buf(tmpbuf, sizeof (*lb));\n\tabd_take_ownership_of_buf(abd_buf->abd, B_TRUE);\n\twzio = zio_write_phys(pio, dev->l2ad_vdev, dev->l2ad_hand,\n\t    asize, abd_buf->abd, ZIO_CHECKSUM_OFF, NULL, NULL,\n\t    ZIO_PRIORITY_ASYNC_WRITE, ZIO_FLAG_CANFAIL, B_FALSE);\n\tDTRACE_PROBE2(l2arc__write, vdev_t *, dev->l2ad_vdev, zio_t *, wzio);\n\t(void) zio_nowait(wzio);\n\n\tdev->l2ad_hand += asize;\n\t \n\tmemcpy(lb_ptr_buf->lb_ptr, &l2dhdr->dh_start_lbps[0],\n\t    sizeof (l2arc_log_blkptr_t));\n\tmutex_enter(&dev->l2ad_mtx);\n\tlist_insert_head(&dev->l2ad_lbptr_list, lb_ptr_buf);\n\tARCSTAT_INCR(arcstat_l2_log_blk_asize, asize);\n\tARCSTAT_BUMP(arcstat_l2_log_blk_count);\n\tzfs_refcount_add_many(&dev->l2ad_lb_asize, asize, lb_ptr_buf);\n\tzfs_refcount_add(&dev->l2ad_lb_count, lb_ptr_buf);\n\tmutex_exit(&dev->l2ad_mtx);\n\tvdev_space_update(dev->l2ad_vdev, asize, 0, 0);\n\n\t \n\tARCSTAT_INCR(arcstat_l2_write_bytes, asize);\n\tARCSTAT_BUMP(arcstat_l2_log_blk_writes);\n\tARCSTAT_F_AVG(arcstat_l2_log_blk_avg_asize, asize);\n\tARCSTAT_F_AVG(arcstat_l2_data_to_meta_ratio,\n\t    dev->l2ad_log_blk_payload_asize / asize);\n\n\t \n\tdev->l2ad_log_ent_idx = 0;\n\tdev->l2ad_log_blk_payload_asize = 0;\n\tdev->l2ad_log_blk_payload_start = 0;\n\n\treturn (asize);\n}\n\n \nboolean_t\nl2arc_log_blkptr_valid(l2arc_dev_t *dev, const l2arc_log_blkptr_t *lbp)\n{\n\t \n\tuint64_t asize = L2BLK_GET_PSIZE((lbp)->lbp_prop);\n\tuint64_t end = lbp->lbp_daddr + asize - 1;\n\tuint64_t start = lbp->lbp_payload_start;\n\tboolean_t evicted = B_FALSE;\n\n\t \n\n\tevicted =\n\t    l2arc_range_check_overlap(start, end, dev->l2ad_hand) ||\n\t    l2arc_range_check_overlap(start, end, dev->l2ad_evict) ||\n\t    l2arc_range_check_overlap(dev->l2ad_hand, dev->l2ad_evict, start) ||\n\t    l2arc_range_check_overlap(dev->l2ad_hand, dev->l2ad_evict, end);\n\n\treturn (start >= dev->l2ad_start && end <= dev->l2ad_end &&\n\t    asize > 0 && asize <= sizeof (l2arc_log_blk_phys_t) &&\n\t    (!evicted || dev->l2ad_first));\n}\n\n \nstatic boolean_t\nl2arc_log_blk_insert(l2arc_dev_t *dev, const arc_buf_hdr_t *hdr)\n{\n\tl2arc_log_blk_phys_t\t*lb = &dev->l2ad_log_blk;\n\tl2arc_log_ent_phys_t\t*le;\n\n\tif (dev->l2ad_log_entries == 0)\n\t\treturn (B_FALSE);\n\n\tint index = dev->l2ad_log_ent_idx++;\n\n\tASSERT3S(index, <, dev->l2ad_log_entries);\n\tASSERT(HDR_HAS_L2HDR(hdr));\n\n\tle = &lb->lb_entries[index];\n\tmemset(le, 0, sizeof (*le));\n\tle->le_dva = hdr->b_dva;\n\tle->le_birth = hdr->b_birth;\n\tle->le_daddr = hdr->b_l2hdr.b_daddr;\n\tif (index == 0)\n\t\tdev->l2ad_log_blk_payload_start = le->le_daddr;\n\tL2BLK_SET_LSIZE((le)->le_prop, HDR_GET_LSIZE(hdr));\n\tL2BLK_SET_PSIZE((le)->le_prop, HDR_GET_PSIZE(hdr));\n\tL2BLK_SET_COMPRESS((le)->le_prop, HDR_GET_COMPRESS(hdr));\n\tle->le_complevel = hdr->b_complevel;\n\tL2BLK_SET_TYPE((le)->le_prop, hdr->b_type);\n\tL2BLK_SET_PROTECTED((le)->le_prop, !!(HDR_PROTECTED(hdr)));\n\tL2BLK_SET_PREFETCH((le)->le_prop, !!(HDR_PREFETCH(hdr)));\n\tL2BLK_SET_STATE((le)->le_prop, hdr->b_l1hdr.b_state->arcs_state);\n\n\tdev->l2ad_log_blk_payload_asize += vdev_psize_to_asize(dev->l2ad_vdev,\n\t    HDR_GET_PSIZE(hdr));\n\n\treturn (dev->l2ad_log_ent_idx == dev->l2ad_log_entries);\n}\n\n \nboolean_t\nl2arc_range_check_overlap(uint64_t bottom, uint64_t top, uint64_t check)\n{\n\tif (bottom < top)\n\t\treturn (bottom <= check && check <= top);\n\telse if (bottom > top)\n\t\treturn (check <= top || bottom <= check);\n\telse\n\t\treturn (check == top);\n}\n\nEXPORT_SYMBOL(arc_buf_size);\nEXPORT_SYMBOL(arc_write);\nEXPORT_SYMBOL(arc_read);\nEXPORT_SYMBOL(arc_buf_info);\nEXPORT_SYMBOL(arc_getbuf_func);\nEXPORT_SYMBOL(arc_add_prune_callback);\nEXPORT_SYMBOL(arc_remove_prune_callback);\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, min, param_set_arc_min,\n\tspl_param_get_u64, ZMOD_RW, \"Minimum ARC size in bytes\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, max, param_set_arc_max,\n\tspl_param_get_u64, ZMOD_RW, \"Maximum ARC size in bytes\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, meta_balance, UINT, ZMOD_RW,\n\t\"Balance between metadata and data on ghost hits.\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, grow_retry, param_set_arc_int,\n\tparam_get_uint, ZMOD_RW, \"Seconds before growing ARC size\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, shrink_shift, param_set_arc_int,\n\tparam_get_uint, ZMOD_RW, \"log2(fraction of ARC to reclaim)\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, pc_percent, UINT, ZMOD_RW,\n\t\"Percent of pagecache to reclaim ARC to\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, average_blocksize, UINT, ZMOD_RD,\n\t\"Target average block size\");\n\nZFS_MODULE_PARAM(zfs, zfs_, compressed_arc_enabled, INT, ZMOD_RW,\n\t\"Disable compressed ARC buffers\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, min_prefetch_ms, param_set_arc_int,\n\tparam_get_uint, ZMOD_RW, \"Min life of prefetch block in ms\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, min_prescient_prefetch_ms,\n    param_set_arc_int, param_get_uint, ZMOD_RW,\n\t\"Min life of prescient prefetched block in ms\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, write_max, U64, ZMOD_RW,\n\t\"Max write bytes per interval\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, write_boost, U64, ZMOD_RW,\n\t\"Extra write bytes during device warmup\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, headroom, U64, ZMOD_RW,\n\t\"Number of max device writes to precache\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, headroom_boost, U64, ZMOD_RW,\n\t\"Compressed l2arc_headroom multiplier\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, trim_ahead, U64, ZMOD_RW,\n\t\"TRIM ahead L2ARC write size multiplier\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, feed_secs, U64, ZMOD_RW,\n\t\"Seconds between L2ARC writing\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, feed_min_ms, U64, ZMOD_RW,\n\t\"Min feed interval in milliseconds\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, noprefetch, INT, ZMOD_RW,\n\t\"Skip caching prefetched buffers\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, feed_again, INT, ZMOD_RW,\n\t\"Turbo L2ARC warmup\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, norw, INT, ZMOD_RW,\n\t\"No reads during writes\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, meta_percent, UINT, ZMOD_RW,\n\t\"Percent of ARC size allowed for L2ARC-only headers\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, rebuild_enabled, INT, ZMOD_RW,\n\t\"Rebuild the L2ARC when importing a pool\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, rebuild_blocks_min_l2size, U64, ZMOD_RW,\n\t\"Min size in bytes to write rebuild log blocks in L2ARC\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, mfuonly, INT, ZMOD_RW,\n\t\"Cache only MFU data from ARC into L2ARC\");\n\nZFS_MODULE_PARAM(zfs_l2arc, l2arc_, exclude_special, INT, ZMOD_RW,\n\t\"Exclude dbufs on special vdevs from being cached to L2ARC if set.\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, lotsfree_percent, param_set_arc_int,\n\tparam_get_uint, ZMOD_RW, \"System free memory I/O throttle in bytes\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, sys_free, param_set_arc_u64,\n\tspl_param_get_u64, ZMOD_RW, \"System free memory target size in bytes\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, dnode_limit, param_set_arc_u64,\n\tspl_param_get_u64, ZMOD_RW, \"Minimum bytes of dnodes in ARC\");\n\nZFS_MODULE_PARAM_CALL(zfs_arc, zfs_arc_, dnode_limit_percent,\n    param_set_arc_int, param_get_uint, ZMOD_RW,\n\t\"Percent of ARC meta buffers for dnodes\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, dnode_reduce_percent, UINT, ZMOD_RW,\n\t\"Percentage of excess dnodes to try to unpin\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, eviction_pct, UINT, ZMOD_RW,\n\t\"When full, ARC allocation waits for eviction of this % of alloc size\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, evict_batch_limit, UINT, ZMOD_RW,\n\t\"The number of headers to evict per sublist before moving to the next\");\n\nZFS_MODULE_PARAM(zfs_arc, zfs_arc_, prune_task_threads, INT, ZMOD_RW,\n\t\"Number of arc_prune threads\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}