{
  "module_name": "spa.c",
  "hash_id": "93a74b40256707eb2167860e99afce2e128c099b73c185532ce53baab8523076",
  "original_prompt": "Ingested from zfs-2.2.2/module/zfs/spa.c",
  "human_readable_source": " \n\n \n\n \n\n#include <sys/zfs_context.h>\n#include <sys/fm/fs/zfs.h>\n#include <sys/spa_impl.h>\n#include <sys/zio.h>\n#include <sys/zio_checksum.h>\n#include <sys/dmu.h>\n#include <sys/dmu_tx.h>\n#include <sys/zap.h>\n#include <sys/zil.h>\n#include <sys/brt.h>\n#include <sys/ddt.h>\n#include <sys/vdev_impl.h>\n#include <sys/vdev_removal.h>\n#include <sys/vdev_indirect_mapping.h>\n#include <sys/vdev_indirect_births.h>\n#include <sys/vdev_initialize.h>\n#include <sys/vdev_rebuild.h>\n#include <sys/vdev_trim.h>\n#include <sys/vdev_disk.h>\n#include <sys/vdev_draid.h>\n#include <sys/metaslab.h>\n#include <sys/metaslab_impl.h>\n#include <sys/mmp.h>\n#include <sys/uberblock_impl.h>\n#include <sys/txg.h>\n#include <sys/avl.h>\n#include <sys/bpobj.h>\n#include <sys/dmu_traverse.h>\n#include <sys/dmu_objset.h>\n#include <sys/unique.h>\n#include <sys/dsl_pool.h>\n#include <sys/dsl_dataset.h>\n#include <sys/dsl_dir.h>\n#include <sys/dsl_prop.h>\n#include <sys/dsl_synctask.h>\n#include <sys/fs/zfs.h>\n#include <sys/arc.h>\n#include <sys/callb.h>\n#include <sys/systeminfo.h>\n#include <sys/zfs_ioctl.h>\n#include <sys/dsl_scan.h>\n#include <sys/zfeature.h>\n#include <sys/dsl_destroy.h>\n#include <sys/zvol.h>\n\n#ifdef\t_KERNEL\n#include <sys/fm/protocol.h>\n#include <sys/fm/util.h>\n#include <sys/callb.h>\n#include <sys/zone.h>\n#include <sys/vmsystm.h>\n#endif\t \n\n#include \"zfs_prop.h\"\n#include \"zfs_comutil.h\"\n\n \nint zfs_ccw_retry_interval = 300;\n\ntypedef enum zti_modes {\n\tZTI_MODE_FIXED,\t\t\t \n\tZTI_MODE_BATCH,\t\t\t \n\tZTI_MODE_SCALE,\t\t\t \n\tZTI_MODE_NULL,\t\t\t \n\tZTI_NMODES\n} zti_modes_t;\n\n#define\tZTI_P(n, q)\t{ ZTI_MODE_FIXED, (n), (q) }\n#define\tZTI_PCT(n)\t{ ZTI_MODE_ONLINE_PERCENT, (n), 1 }\n#define\tZTI_BATCH\t{ ZTI_MODE_BATCH, 0, 1 }\n#define\tZTI_SCALE\t{ ZTI_MODE_SCALE, 0, 1 }\n#define\tZTI_NULL\t{ ZTI_MODE_NULL, 0, 0 }\n\n#define\tZTI_N(n)\tZTI_P(n, 1)\n#define\tZTI_ONE\t\tZTI_N(1)\n\ntypedef struct zio_taskq_info {\n\tzti_modes_t zti_mode;\n\tuint_t zti_value;\n\tuint_t zti_count;\n} zio_taskq_info_t;\n\nstatic const char *const zio_taskq_types[ZIO_TASKQ_TYPES] = {\n\t\"iss\", \"iss_h\", \"int\", \"int_h\"\n};\n\n \nstatic const zio_taskq_info_t zio_taskqs[ZIO_TYPES][ZIO_TASKQ_TYPES] = {\n\t \n\t{ ZTI_ONE,\tZTI_NULL,\tZTI_ONE,\tZTI_NULL },  \n\t{ ZTI_N(8),\tZTI_NULL,\tZTI_SCALE,\tZTI_NULL },  \n\t{ ZTI_BATCH,\tZTI_N(5),\tZTI_SCALE,\tZTI_N(5) },  \n\t{ ZTI_SCALE,\tZTI_NULL,\tZTI_ONE,\tZTI_NULL },  \n\t{ ZTI_ONE,\tZTI_NULL,\tZTI_ONE,\tZTI_NULL },  \n\t{ ZTI_ONE,\tZTI_NULL,\tZTI_ONE,\tZTI_NULL },  \n\t{ ZTI_N(4),\tZTI_NULL,\tZTI_ONE,\tZTI_NULL },  \n};\n\nstatic void spa_sync_version(void *arg, dmu_tx_t *tx);\nstatic void spa_sync_props(void *arg, dmu_tx_t *tx);\nstatic boolean_t spa_has_active_shared_spare(spa_t *spa);\nstatic int spa_load_impl(spa_t *spa, spa_import_type_t type,\n    const char **ereport);\nstatic void spa_vdev_resilver_done(spa_t *spa);\n\n \nstatic uint_t metaslab_preload_pct = 50;\n\nstatic uint_t\tzio_taskq_batch_pct = 80;\t   \nstatic uint_t\tzio_taskq_batch_tpq;\t\t   \nstatic const boolean_t\tzio_taskq_sysdc = B_TRUE;  \nstatic const uint_t\tzio_taskq_basedc = 80;\t   \n\nstatic const boolean_t spa_create_process = B_TRUE;  \n\n \nboolean_t\tspa_load_verify_dryrun = B_FALSE;\n\n \nboolean_t\tspa_mode_readable_spacemaps = B_FALSE;\n\n \n#define\tTRYIMPORT_NAME\t\"$import\"\n\n \nstatic int\t\tspa_load_print_vdev_tree = B_FALSE;\n\n \nuint64_t\tzfs_max_missing_tvds = 0;\n\n \nuint64_t\tzfs_max_missing_tvds_cachefile = SPA_DVAS_PER_BP - 1;\n\n \nuint64_t\tzfs_max_missing_tvds_scan = 0;\n\n \nstatic const boolean_t\tzfs_pause_spa_sync = B_FALSE;\n\n \nstatic int zfs_livelist_condense_zthr_pause = 0;\nstatic int zfs_livelist_condense_sync_pause = 0;\n\n \nstatic int zfs_livelist_condense_sync_cancel = 0;\nstatic int zfs_livelist_condense_zthr_cancel = 0;\n\n \nstatic int zfs_livelist_condense_new_alloc = 0;\n\n \n\n \nstatic void\nspa_prop_add_list(nvlist_t *nvl, zpool_prop_t prop, const char *strval,\n    uint64_t intval, zprop_source_t src)\n{\n\tconst char *propname = zpool_prop_to_name(prop);\n\tnvlist_t *propval;\n\n\tpropval = fnvlist_alloc();\n\tfnvlist_add_uint64(propval, ZPROP_SOURCE, src);\n\n\tif (strval != NULL)\n\t\tfnvlist_add_string(propval, ZPROP_VALUE, strval);\n\telse\n\t\tfnvlist_add_uint64(propval, ZPROP_VALUE, intval);\n\n\tfnvlist_add_nvlist(nvl, propname, propval);\n\tnvlist_free(propval);\n}\n\n \nstatic void\nspa_prop_add_user(nvlist_t *nvl, const char *propname, char *strval,\n    zprop_source_t src)\n{\n\tnvlist_t *propval;\n\n\tVERIFY(nvlist_alloc(&propval, NV_UNIQUE_NAME, KM_SLEEP) == 0);\n\tVERIFY(nvlist_add_uint64(propval, ZPROP_SOURCE, src) == 0);\n\tVERIFY(nvlist_add_string(propval, ZPROP_VALUE, strval) == 0);\n\tVERIFY(nvlist_add_nvlist(nvl, propname, propval) == 0);\n\tnvlist_free(propval);\n}\n\n \nstatic void\nspa_prop_get_config(spa_t *spa, nvlist_t **nvp)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tdsl_pool_t *pool = spa->spa_dsl_pool;\n\tuint64_t size, alloc, cap, version;\n\tconst zprop_source_t src = ZPROP_SRC_NONE;\n\tspa_config_dirent_t *dp;\n\tmetaslab_class_t *mc = spa_normal_class(spa);\n\n\tASSERT(MUTEX_HELD(&spa->spa_props_lock));\n\n\tif (rvd != NULL) {\n\t\talloc = metaslab_class_get_alloc(mc);\n\t\talloc += metaslab_class_get_alloc(spa_special_class(spa));\n\t\talloc += metaslab_class_get_alloc(spa_dedup_class(spa));\n\t\talloc += metaslab_class_get_alloc(spa_embedded_log_class(spa));\n\n\t\tsize = metaslab_class_get_space(mc);\n\t\tsize += metaslab_class_get_space(spa_special_class(spa));\n\t\tsize += metaslab_class_get_space(spa_dedup_class(spa));\n\t\tsize += metaslab_class_get_space(spa_embedded_log_class(spa));\n\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_NAME, spa_name(spa), 0, src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_SIZE, NULL, size, src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_ALLOCATED, NULL, alloc, src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_FREE, NULL,\n\t\t    size - alloc, src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_CHECKPOINT, NULL,\n\t\t    spa->spa_checkpoint_info.sci_dspace, src);\n\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_FRAGMENTATION, NULL,\n\t\t    metaslab_class_fragmentation(mc), src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_EXPANDSZ, NULL,\n\t\t    metaslab_class_expandable_space(mc), src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_READONLY, NULL,\n\t\t    (spa_mode(spa) == SPA_MODE_READ), src);\n\n\t\tcap = (size == 0) ? 0 : (alloc * 100 / size);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_CAPACITY, NULL, cap, src);\n\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_DEDUPRATIO, NULL,\n\t\t    ddt_get_pool_dedup_ratio(spa), src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_BCLONEUSED, NULL,\n\t\t    brt_get_used(spa), src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_BCLONESAVED, NULL,\n\t\t    brt_get_saved(spa), src);\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_BCLONERATIO, NULL,\n\t\t    brt_get_ratio(spa), src);\n\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_HEALTH, NULL,\n\t\t    rvd->vdev_state, src);\n\n\t\tversion = spa_version(spa);\n\t\tif (version == zpool_prop_default_numeric(ZPOOL_PROP_VERSION)) {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_VERSION, NULL,\n\t\t\t    version, ZPROP_SRC_DEFAULT);\n\t\t} else {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_VERSION, NULL,\n\t\t\t    version, ZPROP_SRC_LOCAL);\n\t\t}\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_LOAD_GUID,\n\t\t    NULL, spa_load_guid(spa), src);\n\t}\n\n\tif (pool != NULL) {\n\t\t \n\t\tif (pool->dp_free_dir != NULL) {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_FREEING, NULL,\n\t\t\t    dsl_dir_phys(pool->dp_free_dir)->dd_used_bytes,\n\t\t\t    src);\n\t\t} else {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_FREEING,\n\t\t\t    NULL, 0, src);\n\t\t}\n\n\t\tif (pool->dp_leak_dir != NULL) {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_LEAKED, NULL,\n\t\t\t    dsl_dir_phys(pool->dp_leak_dir)->dd_used_bytes,\n\t\t\t    src);\n\t\t} else {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_LEAKED,\n\t\t\t    NULL, 0, src);\n\t\t}\n\t}\n\n\tspa_prop_add_list(*nvp, ZPOOL_PROP_GUID, NULL, spa_guid(spa), src);\n\n\tif (spa->spa_comment != NULL) {\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_COMMENT, spa->spa_comment,\n\t\t    0, ZPROP_SRC_LOCAL);\n\t}\n\n\tif (spa->spa_compatibility != NULL) {\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_COMPATIBILITY,\n\t\t    spa->spa_compatibility, 0, ZPROP_SRC_LOCAL);\n\t}\n\n\tif (spa->spa_root != NULL)\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_ALTROOT, spa->spa_root,\n\t\t    0, ZPROP_SRC_LOCAL);\n\n\tif (spa_feature_is_enabled(spa, SPA_FEATURE_LARGE_BLOCKS)) {\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_MAXBLOCKSIZE, NULL,\n\t\t    MIN(zfs_max_recordsize, SPA_MAXBLOCKSIZE), ZPROP_SRC_NONE);\n\t} else {\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_MAXBLOCKSIZE, NULL,\n\t\t    SPA_OLD_MAXBLOCKSIZE, ZPROP_SRC_NONE);\n\t}\n\n\tif (spa_feature_is_enabled(spa, SPA_FEATURE_LARGE_DNODE)) {\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_MAXDNODESIZE, NULL,\n\t\t    DNODE_MAX_SIZE, ZPROP_SRC_NONE);\n\t} else {\n\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_MAXDNODESIZE, NULL,\n\t\t    DNODE_MIN_SIZE, ZPROP_SRC_NONE);\n\t}\n\n\tif ((dp = list_head(&spa->spa_config_list)) != NULL) {\n\t\tif (dp->scd_path == NULL) {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_CACHEFILE,\n\t\t\t    \"none\", 0, ZPROP_SRC_LOCAL);\n\t\t} else if (strcmp(dp->scd_path, spa_config_path) != 0) {\n\t\t\tspa_prop_add_list(*nvp, ZPOOL_PROP_CACHEFILE,\n\t\t\t    dp->scd_path, 0, ZPROP_SRC_LOCAL);\n\t\t}\n\t}\n}\n\n \nint\nspa_prop_get(spa_t *spa, nvlist_t **nvp)\n{\n\tobjset_t *mos = spa->spa_meta_objset;\n\tzap_cursor_t zc;\n\tzap_attribute_t za;\n\tdsl_pool_t *dp;\n\tint err;\n\n\terr = nvlist_alloc(nvp, NV_UNIQUE_NAME, KM_SLEEP);\n\tif (err)\n\t\treturn (err);\n\n\tdp = spa_get_dsl(spa);\n\tdsl_pool_config_enter(dp, FTAG);\n\tmutex_enter(&spa->spa_props_lock);\n\n\t \n\tspa_prop_get_config(spa, nvp);\n\n\t \n\tif (mos == NULL || spa->spa_pool_props_object == 0)\n\t\tgoto out;\n\n\t \n\tfor (zap_cursor_init(&zc, mos, spa->spa_pool_props_object);\n\t    (err = zap_cursor_retrieve(&zc, &za)) == 0;\n\t    zap_cursor_advance(&zc)) {\n\t\tuint64_t intval = 0;\n\t\tchar *strval = NULL;\n\t\tzprop_source_t src = ZPROP_SRC_DEFAULT;\n\t\tzpool_prop_t prop;\n\n\t\tif ((prop = zpool_name_to_prop(za.za_name)) ==\n\t\t    ZPOOL_PROP_INVAL && !zfs_prop_user(za.za_name))\n\t\t\tcontinue;\n\n\t\tswitch (za.za_integer_length) {\n\t\tcase 8:\n\t\t\t \n\t\t\tif (za.za_first_integer !=\n\t\t\t    zpool_prop_default_numeric(prop))\n\t\t\t\tsrc = ZPROP_SRC_LOCAL;\n\n\t\t\tif (prop == ZPOOL_PROP_BOOTFS) {\n\t\t\t\tdsl_dataset_t *ds = NULL;\n\n\t\t\t\terr = dsl_dataset_hold_obj(dp,\n\t\t\t\t    za.za_first_integer, FTAG, &ds);\n\t\t\t\tif (err != 0)\n\t\t\t\t\tbreak;\n\n\t\t\t\tstrval = kmem_alloc(ZFS_MAX_DATASET_NAME_LEN,\n\t\t\t\t    KM_SLEEP);\n\t\t\t\tdsl_dataset_name(ds, strval);\n\t\t\t\tdsl_dataset_rele(ds, FTAG);\n\t\t\t} else {\n\t\t\t\tstrval = NULL;\n\t\t\t\tintval = za.za_first_integer;\n\t\t\t}\n\n\t\t\tspa_prop_add_list(*nvp, prop, strval, intval, src);\n\n\t\t\tif (strval != NULL)\n\t\t\t\tkmem_free(strval, ZFS_MAX_DATASET_NAME_LEN);\n\n\t\t\tbreak;\n\n\t\tcase 1:\n\t\t\t \n\t\t\tstrval = kmem_alloc(za.za_num_integers, KM_SLEEP);\n\t\t\terr = zap_lookup(mos, spa->spa_pool_props_object,\n\t\t\t    za.za_name, 1, za.za_num_integers, strval);\n\t\t\tif (err) {\n\t\t\t\tkmem_free(strval, za.za_num_integers);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (prop != ZPOOL_PROP_INVAL) {\n\t\t\t\tspa_prop_add_list(*nvp, prop, strval, 0, src);\n\t\t\t} else {\n\t\t\t\tsrc = ZPROP_SRC_LOCAL;\n\t\t\t\tspa_prop_add_user(*nvp, za.za_name, strval,\n\t\t\t\t    src);\n\t\t\t}\n\t\t\tkmem_free(strval, za.za_num_integers);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tzap_cursor_fini(&zc);\nout:\n\tmutex_exit(&spa->spa_props_lock);\n\tdsl_pool_config_exit(dp, FTAG);\n\tif (err && err != ENOENT) {\n\t\tnvlist_free(*nvp);\n\t\t*nvp = NULL;\n\t\treturn (err);\n\t}\n\n\treturn (0);\n}\n\n \nstatic int\nspa_prop_validate(spa_t *spa, nvlist_t *props)\n{\n\tnvpair_t *elem;\n\tint error = 0, reset_bootfs = 0;\n\tuint64_t objnum = 0;\n\tboolean_t has_feature = B_FALSE;\n\n\telem = NULL;\n\twhile ((elem = nvlist_next_nvpair(props, elem)) != NULL) {\n\t\tuint64_t intval;\n\t\tconst char *strval, *slash, *check, *fname;\n\t\tconst char *propname = nvpair_name(elem);\n\t\tzpool_prop_t prop = zpool_name_to_prop(propname);\n\n\t\tswitch (prop) {\n\t\tcase ZPOOL_PROP_INVAL:\n\t\t\t \n\t\t\tif (zfs_prop_user(propname)) {\n\t\t\t\tif (strlen(propname) >= ZAP_MAXNAMELEN) {\n\t\t\t\t\terror = SET_ERROR(ENAMETOOLONG);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (strlen(fnvpair_value_string(elem)) >=\n\t\t\t\t    ZAP_MAXVALUELEN) {\n\t\t\t\t\terror = SET_ERROR(E2BIG);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else if (zpool_prop_feature(propname)) {\n\t\t\t\tif (nvpair_type(elem) != DATA_TYPE_UINT64) {\n\t\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (nvpair_value_uint64(elem, &intval) != 0) {\n\t\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (intval != 0) {\n\t\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tfname = strchr(propname, '@') + 1;\n\t\t\t\tif (zfeature_lookup_name(fname, NULL) != 0) {\n\t\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\thas_feature = B_TRUE;\n\t\t\t} else {\n\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_VERSION:\n\t\t\terror = nvpair_value_uint64(elem, &intval);\n\t\t\tif (!error &&\n\t\t\t    (intval < spa_version(spa) ||\n\t\t\t    intval > SPA_VERSION_BEFORE_FEATURES ||\n\t\t\t    has_feature))\n\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_DELEGATION:\n\t\tcase ZPOOL_PROP_AUTOREPLACE:\n\t\tcase ZPOOL_PROP_LISTSNAPS:\n\t\tcase ZPOOL_PROP_AUTOEXPAND:\n\t\tcase ZPOOL_PROP_AUTOTRIM:\n\t\t\terror = nvpair_value_uint64(elem, &intval);\n\t\t\tif (!error && intval > 1)\n\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_MULTIHOST:\n\t\t\terror = nvpair_value_uint64(elem, &intval);\n\t\t\tif (!error && intval > 1)\n\t\t\t\terror = SET_ERROR(EINVAL);\n\n\t\t\tif (!error) {\n\t\t\t\tuint32_t hostid = zone_get_hostid(NULL);\n\t\t\t\tif (hostid)\n\t\t\t\t\tspa->spa_hostid = hostid;\n\t\t\t\telse\n\t\t\t\t\terror = SET_ERROR(ENOTSUP);\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_BOOTFS:\n\t\t\t \n\t\t\tif (spa_version(spa) < SPA_VERSION_BOOTFS) {\n\t\t\t\terror = SET_ERROR(ENOTSUP);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!vdev_is_bootable(spa->spa_root_vdev)) {\n\t\t\t\terror = SET_ERROR(ENOTSUP);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\treset_bootfs = 1;\n\n\t\t\terror = nvpair_value_string(elem, &strval);\n\n\t\t\tif (!error) {\n\t\t\t\tobjset_t *os;\n\n\t\t\t\tif (strval == NULL || strval[0] == '\\0') {\n\t\t\t\t\tobjnum = zpool_prop_default_numeric(\n\t\t\t\t\t    ZPOOL_PROP_BOOTFS);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\terror = dmu_objset_hold(strval, FTAG, &os);\n\t\t\t\tif (error != 0)\n\t\t\t\t\tbreak;\n\n\t\t\t\t \n\t\t\t\tif (dmu_objset_type(os) != DMU_OST_ZFS) {\n\t\t\t\t\terror = SET_ERROR(ENOTSUP);\n\t\t\t\t} else {\n\t\t\t\t\tobjnum = dmu_objset_id(os);\n\t\t\t\t}\n\t\t\t\tdmu_objset_rele(os, FTAG);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_FAILUREMODE:\n\t\t\terror = nvpair_value_uint64(elem, &intval);\n\t\t\tif (!error && intval > ZIO_FAILURE_MODE_PANIC)\n\t\t\t\terror = SET_ERROR(EINVAL);\n\n\t\t\t \n\t\t\tif (!error && spa_suspended(spa)) {\n\t\t\t\tspa->spa_failmode = intval;\n\t\t\t\terror = SET_ERROR(EIO);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_CACHEFILE:\n\t\t\tif ((error = nvpair_value_string(elem, &strval)) != 0)\n\t\t\t\tbreak;\n\n\t\t\tif (strval[0] == '\\0')\n\t\t\t\tbreak;\n\n\t\t\tif (strcmp(strval, \"none\") == 0)\n\t\t\t\tbreak;\n\n\t\t\tif (strval[0] != '/') {\n\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tslash = strrchr(strval, '/');\n\t\t\tASSERT(slash != NULL);\n\n\t\t\tif (slash[1] == '\\0' || strcmp(slash, \"/.\") == 0 ||\n\t\t\t    strcmp(slash, \"/..\") == 0)\n\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_COMMENT:\n\t\t\tif ((error = nvpair_value_string(elem, &strval)) != 0)\n\t\t\t\tbreak;\n\t\t\tfor (check = strval; *check != '\\0'; check++) {\n\t\t\t\tif (!isprint(*check)) {\n\t\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (strlen(strval) > ZPROP_MAX_COMMENT)\n\t\t\t\terror = SET_ERROR(E2BIG);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (error)\n\t\t\tbreak;\n\t}\n\n\t(void) nvlist_remove_all(props,\n\t    zpool_prop_to_name(ZPOOL_PROP_DEDUPDITTO));\n\n\tif (!error && reset_bootfs) {\n\t\terror = nvlist_remove(props,\n\t\t    zpool_prop_to_name(ZPOOL_PROP_BOOTFS), DATA_TYPE_STRING);\n\n\t\tif (!error) {\n\t\t\terror = nvlist_add_uint64(props,\n\t\t\t    zpool_prop_to_name(ZPOOL_PROP_BOOTFS), objnum);\n\t\t}\n\t}\n\n\treturn (error);\n}\n\nvoid\nspa_configfile_set(spa_t *spa, nvlist_t *nvp, boolean_t need_sync)\n{\n\tconst char *cachefile;\n\tspa_config_dirent_t *dp;\n\n\tif (nvlist_lookup_string(nvp, zpool_prop_to_name(ZPOOL_PROP_CACHEFILE),\n\t    &cachefile) != 0)\n\t\treturn;\n\n\tdp = kmem_alloc(sizeof (spa_config_dirent_t),\n\t    KM_SLEEP);\n\n\tif (cachefile[0] == '\\0')\n\t\tdp->scd_path = spa_strdup(spa_config_path);\n\telse if (strcmp(cachefile, \"none\") == 0)\n\t\tdp->scd_path = NULL;\n\telse\n\t\tdp->scd_path = spa_strdup(cachefile);\n\n\tlist_insert_head(&spa->spa_config_list, dp);\n\tif (need_sync)\n\t\tspa_async_request(spa, SPA_ASYNC_CONFIG_UPDATE);\n}\n\nint\nspa_prop_set(spa_t *spa, nvlist_t *nvp)\n{\n\tint error;\n\tnvpair_t *elem = NULL;\n\tboolean_t need_sync = B_FALSE;\n\n\tif ((error = spa_prop_validate(spa, nvp)) != 0)\n\t\treturn (error);\n\n\twhile ((elem = nvlist_next_nvpair(nvp, elem)) != NULL) {\n\t\tzpool_prop_t prop = zpool_name_to_prop(nvpair_name(elem));\n\n\t\tif (prop == ZPOOL_PROP_CACHEFILE ||\n\t\t    prop == ZPOOL_PROP_ALTROOT ||\n\t\t    prop == ZPOOL_PROP_READONLY)\n\t\t\tcontinue;\n\n\t\tif (prop == ZPOOL_PROP_INVAL &&\n\t\t    zfs_prop_user(nvpair_name(elem))) {\n\t\t\tneed_sync = B_TRUE;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (prop == ZPOOL_PROP_VERSION || prop == ZPOOL_PROP_INVAL) {\n\t\t\tuint64_t ver = 0;\n\n\t\t\tif (prop == ZPOOL_PROP_VERSION) {\n\t\t\t\tVERIFY(nvpair_value_uint64(elem, &ver) == 0);\n\t\t\t} else {\n\t\t\t\tASSERT(zpool_prop_feature(nvpair_name(elem)));\n\t\t\t\tver = SPA_VERSION_FEATURES;\n\t\t\t\tneed_sync = B_TRUE;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (ver == spa_version(spa))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\terror = dsl_sync_task(spa->spa_name, NULL,\n\t\t\t    spa_sync_version, &ver,\n\t\t\t    6, ZFS_SPACE_CHECK_RESERVED);\n\t\t\tif (error)\n\t\t\t\treturn (error);\n\t\t\tcontinue;\n\t\t}\n\n\t\tneed_sync = B_TRUE;\n\t\tbreak;\n\t}\n\n\tif (need_sync) {\n\t\treturn (dsl_sync_task(spa->spa_name, NULL, spa_sync_props,\n\t\t    nvp, 6, ZFS_SPACE_CHECK_RESERVED));\n\t}\n\n\treturn (0);\n}\n\n \nvoid\nspa_prop_clear_bootfs(spa_t *spa, uint64_t dsobj, dmu_tx_t *tx)\n{\n\tif (spa->spa_bootfs == dsobj && spa->spa_pool_props_object != 0) {\n\t\tVERIFY(zap_remove(spa->spa_meta_objset,\n\t\t    spa->spa_pool_props_object,\n\t\t    zpool_prop_to_name(ZPOOL_PROP_BOOTFS), tx) == 0);\n\t\tspa->spa_bootfs = 0;\n\t}\n}\n\nstatic int\nspa_change_guid_check(void *arg, dmu_tx_t *tx)\n{\n\tuint64_t *newguid __maybe_unused = arg;\n\tspa_t *spa = dmu_tx_pool(tx)->dp_spa;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tuint64_t vdev_state;\n\n\tif (spa_feature_is_active(spa, SPA_FEATURE_POOL_CHECKPOINT)) {\n\t\tint error = (spa_has_checkpoint(spa)) ?\n\t\t    ZFS_ERR_CHECKPOINT_EXISTS : ZFS_ERR_DISCARDING_CHECKPOINT;\n\t\treturn (SET_ERROR(error));\n\t}\n\n\tspa_config_enter(spa, SCL_STATE, FTAG, RW_READER);\n\tvdev_state = rvd->vdev_state;\n\tspa_config_exit(spa, SCL_STATE, FTAG);\n\n\tif (vdev_state != VDEV_STATE_HEALTHY)\n\t\treturn (SET_ERROR(ENXIO));\n\n\tASSERT3U(spa_guid(spa), !=, *newguid);\n\n\treturn (0);\n}\n\nstatic void\nspa_change_guid_sync(void *arg, dmu_tx_t *tx)\n{\n\tuint64_t *newguid = arg;\n\tspa_t *spa = dmu_tx_pool(tx)->dp_spa;\n\tuint64_t oldguid;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\toldguid = spa_guid(spa);\n\n\tspa_config_enter(spa, SCL_STATE, FTAG, RW_READER);\n\trvd->vdev_guid = *newguid;\n\trvd->vdev_guid_sum += (*newguid - oldguid);\n\tvdev_config_dirty(rvd);\n\tspa_config_exit(spa, SCL_STATE, FTAG);\n\n\tspa_history_log_internal(spa, \"guid change\", tx, \"old=%llu new=%llu\",\n\t    (u_longlong_t)oldguid, (u_longlong_t)*newguid);\n}\n\n \nint\nspa_change_guid(spa_t *spa)\n{\n\tint error;\n\tuint64_t guid;\n\n\tmutex_enter(&spa->spa_vdev_top_lock);\n\tmutex_enter(&spa_namespace_lock);\n\tguid = spa_generate_guid(NULL);\n\n\terror = dsl_sync_task(spa->spa_name, spa_change_guid_check,\n\t    spa_change_guid_sync, &guid, 5, ZFS_SPACE_CHECK_RESERVED);\n\n\tif (error == 0) {\n\t\t \n\t\tvdev_clear_kobj_evt(spa->spa_root_vdev);\n\t\tfor (int i = 0; i < spa->spa_l2cache.sav_count; i++)\n\t\t\tvdev_clear_kobj_evt(spa->spa_l2cache.sav_vdevs[i]);\n\n\t\tspa_write_cachefile(spa, B_FALSE, B_TRUE, B_TRUE);\n\t\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_POOL_REGUID);\n\t}\n\n\tmutex_exit(&spa_namespace_lock);\n\tmutex_exit(&spa->spa_vdev_top_lock);\n\n\treturn (error);\n}\n\n \n\nstatic int\nspa_error_entry_compare(const void *a, const void *b)\n{\n\tconst spa_error_entry_t *sa = (const spa_error_entry_t *)a;\n\tconst spa_error_entry_t *sb = (const spa_error_entry_t *)b;\n\tint ret;\n\n\tret = memcmp(&sa->se_bookmark, &sb->se_bookmark,\n\t    sizeof (zbookmark_phys_t));\n\n\treturn (TREE_ISIGN(ret));\n}\n\n \nvoid\nspa_get_errlists(spa_t *spa, avl_tree_t *last, avl_tree_t *scrub)\n{\n\tASSERT(MUTEX_HELD(&spa->spa_errlist_lock));\n\n\tmemcpy(last, &spa->spa_errlist_last, sizeof (avl_tree_t));\n\tmemcpy(scrub, &spa->spa_errlist_scrub, sizeof (avl_tree_t));\n\n\tavl_create(&spa->spa_errlist_scrub,\n\t    spa_error_entry_compare, sizeof (spa_error_entry_t),\n\t    offsetof(spa_error_entry_t, se_avl));\n\tavl_create(&spa->spa_errlist_last,\n\t    spa_error_entry_compare, sizeof (spa_error_entry_t),\n\t    offsetof(spa_error_entry_t, se_avl));\n}\n\nstatic void\nspa_taskqs_init(spa_t *spa, zio_type_t t, zio_taskq_type_t q)\n{\n\tconst zio_taskq_info_t *ztip = &zio_taskqs[t][q];\n\tenum zti_modes mode = ztip->zti_mode;\n\tuint_t value = ztip->zti_value;\n\tuint_t count = ztip->zti_count;\n\tspa_taskqs_t *tqs = &spa->spa_zio_taskq[t][q];\n\tuint_t cpus, flags = TASKQ_DYNAMIC;\n\tboolean_t batch = B_FALSE;\n\n\tswitch (mode) {\n\tcase ZTI_MODE_FIXED:\n\t\tASSERT3U(value, >, 0);\n\t\tbreak;\n\n\tcase ZTI_MODE_BATCH:\n\t\tbatch = B_TRUE;\n\t\tflags |= TASKQ_THREADS_CPU_PCT;\n\t\tvalue = MIN(zio_taskq_batch_pct, 100);\n\t\tbreak;\n\n\tcase ZTI_MODE_SCALE:\n\t\tflags |= TASKQ_THREADS_CPU_PCT;\n\t\t \n\t\tcpus = MAX(1, boot_ncpus * zio_taskq_batch_pct / 100);\n\t\tif (zio_taskq_batch_tpq > 0) {\n\t\t\tcount = MAX(1, (cpus + zio_taskq_batch_tpq / 2) /\n\t\t\t    zio_taskq_batch_tpq);\n\t\t} else {\n\t\t\t \n\t\t\tcount = 1 + cpus / 6;\n\t\t\twhile (count * count > cpus)\n\t\t\t\tcount--;\n\t\t}\n\t\t \n\t\tcount = MAX(count, (zio_taskq_batch_pct + 99) / 100);\n\t\tvalue = (zio_taskq_batch_pct + count / 2) / count;\n\t\tbreak;\n\n\tcase ZTI_MODE_NULL:\n\t\ttqs->stqs_count = 0;\n\t\ttqs->stqs_taskq = NULL;\n\t\treturn;\n\n\tdefault:\n\t\tpanic(\"unrecognized mode for %s_%s taskq (%u:%u) in \"\n\t\t    \"spa_activate()\",\n\t\t    zio_type_name[t], zio_taskq_types[q], mode, value);\n\t\tbreak;\n\t}\n\n\tASSERT3U(count, >, 0);\n\ttqs->stqs_count = count;\n\ttqs->stqs_taskq = kmem_alloc(count * sizeof (taskq_t *), KM_SLEEP);\n\n\tfor (uint_t i = 0; i < count; i++) {\n\t\ttaskq_t *tq;\n\t\tchar name[32];\n\n\t\tif (count > 1)\n\t\t\t(void) snprintf(name, sizeof (name), \"%s_%s_%u\",\n\t\t\t    zio_type_name[t], zio_taskq_types[q], i);\n\t\telse\n\t\t\t(void) snprintf(name, sizeof (name), \"%s_%s\",\n\t\t\t    zio_type_name[t], zio_taskq_types[q]);\n\n\t\tif (zio_taskq_sysdc && spa->spa_proc != &p0) {\n\t\t\tif (batch)\n\t\t\t\tflags |= TASKQ_DC_BATCH;\n\n\t\t\t(void) zio_taskq_basedc;\n\t\t\ttq = taskq_create_sysdc(name, value, 50, INT_MAX,\n\t\t\t    spa->spa_proc, zio_taskq_basedc, flags);\n\t\t} else {\n\t\t\tpri_t pri = maxclsyspri;\n\t\t\t \n\t\t\tif (t == ZIO_TYPE_WRITE && q == ZIO_TASKQ_ISSUE) {\n#if defined(__linux__)\n\t\t\t\tpri++;\n#elif defined(__FreeBSD__)\n\t\t\t\tpri += 4;\n#else\n#error \"unknown OS\"\n#endif\n\t\t\t}\n\t\t\ttq = taskq_create_proc(name, value, pri, 50,\n\t\t\t    INT_MAX, spa->spa_proc, flags);\n\t\t}\n\n\t\ttqs->stqs_taskq[i] = tq;\n\t}\n}\n\nstatic void\nspa_taskqs_fini(spa_t *spa, zio_type_t t, zio_taskq_type_t q)\n{\n\tspa_taskqs_t *tqs = &spa->spa_zio_taskq[t][q];\n\n\tif (tqs->stqs_taskq == NULL) {\n\t\tASSERT3U(tqs->stqs_count, ==, 0);\n\t\treturn;\n\t}\n\n\tfor (uint_t i = 0; i < tqs->stqs_count; i++) {\n\t\tASSERT3P(tqs->stqs_taskq[i], !=, NULL);\n\t\ttaskq_destroy(tqs->stqs_taskq[i]);\n\t}\n\n\tkmem_free(tqs->stqs_taskq, tqs->stqs_count * sizeof (taskq_t *));\n\ttqs->stqs_taskq = NULL;\n}\n\n \nvoid\nspa_taskq_dispatch_ent(spa_t *spa, zio_type_t t, zio_taskq_type_t q,\n    task_func_t *func, void *arg, uint_t flags, taskq_ent_t *ent)\n{\n\tspa_taskqs_t *tqs = &spa->spa_zio_taskq[t][q];\n\ttaskq_t *tq;\n\n\tASSERT3P(tqs->stqs_taskq, !=, NULL);\n\tASSERT3U(tqs->stqs_count, !=, 0);\n\n\tif (tqs->stqs_count == 1) {\n\t\ttq = tqs->stqs_taskq[0];\n\t} else {\n\t\ttq = tqs->stqs_taskq[((uint64_t)gethrtime()) % tqs->stqs_count];\n\t}\n\n\ttaskq_dispatch_ent(tq, func, arg, flags, ent);\n}\n\n \nvoid\nspa_taskq_dispatch_sync(spa_t *spa, zio_type_t t, zio_taskq_type_t q,\n    task_func_t *func, void *arg, uint_t flags)\n{\n\tspa_taskqs_t *tqs = &spa->spa_zio_taskq[t][q];\n\ttaskq_t *tq;\n\ttaskqid_t id;\n\n\tASSERT3P(tqs->stqs_taskq, !=, NULL);\n\tASSERT3U(tqs->stqs_count, !=, 0);\n\n\tif (tqs->stqs_count == 1) {\n\t\ttq = tqs->stqs_taskq[0];\n\t} else {\n\t\ttq = tqs->stqs_taskq[((uint64_t)gethrtime()) % tqs->stqs_count];\n\t}\n\n\tid = taskq_dispatch(tq, func, arg, flags);\n\tif (id)\n\t\ttaskq_wait_id(tq, id);\n}\n\nstatic void\nspa_create_zio_taskqs(spa_t *spa)\n{\n\tfor (int t = 0; t < ZIO_TYPES; t++) {\n\t\tfor (int q = 0; q < ZIO_TASKQ_TYPES; q++) {\n\t\t\tspa_taskqs_init(spa, t, q);\n\t\t}\n\t}\n}\n\n \n#undef HAVE_SPA_THREAD\n\n#if defined(_KERNEL) && defined(HAVE_SPA_THREAD)\nstatic void\nspa_thread(void *arg)\n{\n\tpsetid_t zio_taskq_psrset_bind = PS_NONE;\n\tcallb_cpr_t cprinfo;\n\n\tspa_t *spa = arg;\n\tuser_t *pu = PTOU(curproc);\n\n\tCALLB_CPR_INIT(&cprinfo, &spa->spa_proc_lock, callb_generic_cpr,\n\t    spa->spa_name);\n\n\tASSERT(curproc != &p0);\n\t(void) snprintf(pu->u_psargs, sizeof (pu->u_psargs),\n\t    \"zpool-%s\", spa->spa_name);\n\t(void) strlcpy(pu->u_comm, pu->u_psargs, sizeof (pu->u_comm));\n\n\t \n\tif (zio_taskq_psrset_bind != PS_NONE) {\n\t\tpool_lock();\n\t\tmutex_enter(&cpu_lock);\n\t\tmutex_enter(&pidlock);\n\t\tmutex_enter(&curproc->p_lock);\n\n\t\tif (cpupart_bind_thread(curthread, zio_taskq_psrset_bind,\n\t\t    0, NULL, NULL) == 0)  {\n\t\t\tcurthread->t_bind_pset = zio_taskq_psrset_bind;\n\t\t} else {\n\t\t\tcmn_err(CE_WARN,\n\t\t\t    \"Couldn't bind process for zfs pool \\\"%s\\\" to \"\n\t\t\t    \"pset %d\\n\", spa->spa_name, zio_taskq_psrset_bind);\n\t\t}\n\n\t\tmutex_exit(&curproc->p_lock);\n\t\tmutex_exit(&pidlock);\n\t\tmutex_exit(&cpu_lock);\n\t\tpool_unlock();\n\t}\n\n\tif (zio_taskq_sysdc) {\n\t\tsysdc_thread_enter(curthread, 100, 0);\n\t}\n\n\tspa->spa_proc = curproc;\n\tspa->spa_did = curthread->t_did;\n\n\tspa_create_zio_taskqs(spa);\n\n\tmutex_enter(&spa->spa_proc_lock);\n\tASSERT(spa->spa_proc_state == SPA_PROC_CREATED);\n\n\tspa->spa_proc_state = SPA_PROC_ACTIVE;\n\tcv_broadcast(&spa->spa_proc_cv);\n\n\tCALLB_CPR_SAFE_BEGIN(&cprinfo);\n\twhile (spa->spa_proc_state == SPA_PROC_ACTIVE)\n\t\tcv_wait(&spa->spa_proc_cv, &spa->spa_proc_lock);\n\tCALLB_CPR_SAFE_END(&cprinfo, &spa->spa_proc_lock);\n\n\tASSERT(spa->spa_proc_state == SPA_PROC_DEACTIVATE);\n\tspa->spa_proc_state = SPA_PROC_GONE;\n\tspa->spa_proc = &p0;\n\tcv_broadcast(&spa->spa_proc_cv);\n\tCALLB_CPR_EXIT(&cprinfo);\t \n\n\tmutex_enter(&curproc->p_lock);\n\tlwp_exit();\n}\n#endif\n\n \nstatic void\nspa_activate(spa_t *spa, spa_mode_t mode)\n{\n\tASSERT(spa->spa_state == POOL_STATE_UNINITIALIZED);\n\n\tspa->spa_state = POOL_STATE_ACTIVE;\n\tspa->spa_mode = mode;\n\tspa->spa_read_spacemaps = spa_mode_readable_spacemaps;\n\n\tspa->spa_normal_class = metaslab_class_create(spa, &zfs_metaslab_ops);\n\tspa->spa_log_class = metaslab_class_create(spa, &zfs_metaslab_ops);\n\tspa->spa_embedded_log_class =\n\t    metaslab_class_create(spa, &zfs_metaslab_ops);\n\tspa->spa_special_class = metaslab_class_create(spa, &zfs_metaslab_ops);\n\tspa->spa_dedup_class = metaslab_class_create(spa, &zfs_metaslab_ops);\n\n\t \n\tmutex_enter(&spa->spa_proc_lock);\n\tASSERT(spa->spa_proc_state == SPA_PROC_NONE);\n\tASSERT(spa->spa_proc == &p0);\n\tspa->spa_did = 0;\n\n\t(void) spa_create_process;\n#ifdef HAVE_SPA_THREAD\n\t \n\tif (spa_create_process && strcmp(spa->spa_name, TRYIMPORT_NAME) != 0) {\n\t\tif (newproc(spa_thread, (caddr_t)spa, syscid, maxclsyspri,\n\t\t    NULL, 0) == 0) {\n\t\t\tspa->spa_proc_state = SPA_PROC_CREATED;\n\t\t\twhile (spa->spa_proc_state == SPA_PROC_CREATED) {\n\t\t\t\tcv_wait(&spa->spa_proc_cv,\n\t\t\t\t    &spa->spa_proc_lock);\n\t\t\t}\n\t\t\tASSERT(spa->spa_proc_state == SPA_PROC_ACTIVE);\n\t\t\tASSERT(spa->spa_proc != &p0);\n\t\t\tASSERT(spa->spa_did != 0);\n\t\t} else {\n#ifdef _KERNEL\n\t\t\tcmn_err(CE_WARN,\n\t\t\t    \"Couldn't create process for zfs pool \\\"%s\\\"\\n\",\n\t\t\t    spa->spa_name);\n#endif\n\t\t}\n\t}\n#endif  \n\tmutex_exit(&spa->spa_proc_lock);\n\n\t \n\tif (spa->spa_proc == &p0) {\n\t\tspa_create_zio_taskqs(spa);\n\t}\n\n\tfor (size_t i = 0; i < TXG_SIZE; i++) {\n\t\tspa->spa_txg_zio[i] = zio_root(spa, NULL, NULL,\n\t\t    ZIO_FLAG_CANFAIL);\n\t}\n\n\tlist_create(&spa->spa_config_dirty_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_config_dirty_node));\n\tlist_create(&spa->spa_evicting_os_list, sizeof (objset_t),\n\t    offsetof(objset_t, os_evicting_node));\n\tlist_create(&spa->spa_state_dirty_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_state_dirty_node));\n\n\ttxg_list_create(&spa->spa_vdev_txg_list, spa,\n\t    offsetof(struct vdev, vdev_txg_node));\n\n\tavl_create(&spa->spa_errlist_scrub,\n\t    spa_error_entry_compare, sizeof (spa_error_entry_t),\n\t    offsetof(spa_error_entry_t, se_avl));\n\tavl_create(&spa->spa_errlist_last,\n\t    spa_error_entry_compare, sizeof (spa_error_entry_t),\n\t    offsetof(spa_error_entry_t, se_avl));\n\tavl_create(&spa->spa_errlist_healed,\n\t    spa_error_entry_compare, sizeof (spa_error_entry_t),\n\t    offsetof(spa_error_entry_t, se_avl));\n\n\tspa_activate_os(spa);\n\n\tspa_keystore_init(&spa->spa_keystore);\n\n\t \n\tspa->spa_zvol_taskq = taskq_create(\"z_zvol\", 1, defclsyspri,\n\t    1, INT_MAX, 0);\n\n\t \n\tspa->spa_metaslab_taskq = taskq_create(\"z_metaslab\",\n\t    metaslab_preload_pct, maxclsyspri, 1, INT_MAX,\n\t    TASKQ_DYNAMIC | TASKQ_THREADS_CPU_PCT);\n\n\t \n\tspa->spa_prefetch_taskq = taskq_create(\"z_prefetch\", 100,\n\t    defclsyspri, 1, INT_MAX, TASKQ_DYNAMIC | TASKQ_THREADS_CPU_PCT);\n\n\t \n\tspa->spa_upgrade_taskq = taskq_create(\"z_upgrade\", 100,\n\t    defclsyspri, 1, INT_MAX, TASKQ_DYNAMIC | TASKQ_THREADS_CPU_PCT);\n}\n\n \nstatic void\nspa_deactivate(spa_t *spa)\n{\n\tASSERT(spa->spa_sync_on == B_FALSE);\n\tASSERT(spa->spa_dsl_pool == NULL);\n\tASSERT(spa->spa_root_vdev == NULL);\n\tASSERT(spa->spa_async_zio_root == NULL);\n\tASSERT(spa->spa_state != POOL_STATE_UNINITIALIZED);\n\n\tspa_evicting_os_wait(spa);\n\n\tif (spa->spa_zvol_taskq) {\n\t\ttaskq_destroy(spa->spa_zvol_taskq);\n\t\tspa->spa_zvol_taskq = NULL;\n\t}\n\n\tif (spa->spa_metaslab_taskq) {\n\t\ttaskq_destroy(spa->spa_metaslab_taskq);\n\t\tspa->spa_metaslab_taskq = NULL;\n\t}\n\n\tif (spa->spa_prefetch_taskq) {\n\t\ttaskq_destroy(spa->spa_prefetch_taskq);\n\t\tspa->spa_prefetch_taskq = NULL;\n\t}\n\n\tif (spa->spa_upgrade_taskq) {\n\t\ttaskq_destroy(spa->spa_upgrade_taskq);\n\t\tspa->spa_upgrade_taskq = NULL;\n\t}\n\n\ttxg_list_destroy(&spa->spa_vdev_txg_list);\n\n\tlist_destroy(&spa->spa_config_dirty_list);\n\tlist_destroy(&spa->spa_evicting_os_list);\n\tlist_destroy(&spa->spa_state_dirty_list);\n\n\ttaskq_cancel_id(system_delay_taskq, spa->spa_deadman_tqid);\n\n\tfor (int t = 0; t < ZIO_TYPES; t++) {\n\t\tfor (int q = 0; q < ZIO_TASKQ_TYPES; q++) {\n\t\t\tspa_taskqs_fini(spa, t, q);\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < TXG_SIZE; i++) {\n\t\tASSERT3P(spa->spa_txg_zio[i], !=, NULL);\n\t\tVERIFY0(zio_wait(spa->spa_txg_zio[i]));\n\t\tspa->spa_txg_zio[i] = NULL;\n\t}\n\n\tmetaslab_class_destroy(spa->spa_normal_class);\n\tspa->spa_normal_class = NULL;\n\n\tmetaslab_class_destroy(spa->spa_log_class);\n\tspa->spa_log_class = NULL;\n\n\tmetaslab_class_destroy(spa->spa_embedded_log_class);\n\tspa->spa_embedded_log_class = NULL;\n\n\tmetaslab_class_destroy(spa->spa_special_class);\n\tspa->spa_special_class = NULL;\n\n\tmetaslab_class_destroy(spa->spa_dedup_class);\n\tspa->spa_dedup_class = NULL;\n\n\t \n\tspa_errlog_drain(spa);\n\tavl_destroy(&spa->spa_errlist_scrub);\n\tavl_destroy(&spa->spa_errlist_last);\n\tavl_destroy(&spa->spa_errlist_healed);\n\n\tspa_keystore_fini(&spa->spa_keystore);\n\n\tspa->spa_state = POOL_STATE_UNINITIALIZED;\n\n\tmutex_enter(&spa->spa_proc_lock);\n\tif (spa->spa_proc_state != SPA_PROC_NONE) {\n\t\tASSERT(spa->spa_proc_state == SPA_PROC_ACTIVE);\n\t\tspa->spa_proc_state = SPA_PROC_DEACTIVATE;\n\t\tcv_broadcast(&spa->spa_proc_cv);\n\t\twhile (spa->spa_proc_state == SPA_PROC_DEACTIVATE) {\n\t\t\tASSERT(spa->spa_proc != &p0);\n\t\t\tcv_wait(&spa->spa_proc_cv, &spa->spa_proc_lock);\n\t\t}\n\t\tASSERT(spa->spa_proc_state == SPA_PROC_GONE);\n\t\tspa->spa_proc_state = SPA_PROC_NONE;\n\t}\n\tASSERT(spa->spa_proc == &p0);\n\tmutex_exit(&spa->spa_proc_lock);\n\n\t \n\tif (spa->spa_did != 0) {\n\t\tthread_join(spa->spa_did);\n\t\tspa->spa_did = 0;\n\t}\n\n\tspa_deactivate_os(spa);\n\n}\n\n \nint\nspa_config_parse(spa_t *spa, vdev_t **vdp, nvlist_t *nv, vdev_t *parent,\n    uint_t id, int atype)\n{\n\tnvlist_t **child;\n\tuint_t children;\n\tint error;\n\n\tif ((error = vdev_alloc(spa, vdp, nv, parent, id, atype)) != 0)\n\t\treturn (error);\n\n\tif ((*vdp)->vdev_ops->vdev_op_leaf)\n\t\treturn (0);\n\n\terror = nvlist_lookup_nvlist_array(nv, ZPOOL_CONFIG_CHILDREN,\n\t    &child, &children);\n\n\tif (error == ENOENT)\n\t\treturn (0);\n\n\tif (error) {\n\t\tvdev_free(*vdp);\n\t\t*vdp = NULL;\n\t\treturn (SET_ERROR(EINVAL));\n\t}\n\n\tfor (int c = 0; c < children; c++) {\n\t\tvdev_t *vd;\n\t\tif ((error = spa_config_parse(spa, &vd, child[c], *vdp, c,\n\t\t    atype)) != 0) {\n\t\t\tvdev_free(*vdp);\n\t\t\t*vdp = NULL;\n\t\t\treturn (error);\n\t\t}\n\t}\n\n\tASSERT(*vdp != NULL);\n\n\treturn (0);\n}\n\nstatic boolean_t\nspa_should_flush_logs_on_unload(spa_t *spa)\n{\n\tif (!spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP))\n\t\treturn (B_FALSE);\n\n\tif (!spa_writeable(spa))\n\t\treturn (B_FALSE);\n\n\tif (!spa->spa_sync_on)\n\t\treturn (B_FALSE);\n\n\tif (spa_state(spa) != POOL_STATE_EXPORTED)\n\t\treturn (B_FALSE);\n\n\tif (zfs_keep_log_spacemaps_at_export)\n\t\treturn (B_FALSE);\n\n\treturn (B_TRUE);\n}\n\n \nstatic void\nspa_unload_log_sm_flush_all(spa_t *spa)\n{\n\tdmu_tx_t *tx = dmu_tx_create_dd(spa_get_dsl(spa)->dp_mos_dir);\n\tVERIFY0(dmu_tx_assign(tx, TXG_WAIT));\n\n\tASSERT3U(spa->spa_log_flushall_txg, ==, 0);\n\tspa->spa_log_flushall_txg = dmu_tx_get_txg(tx);\n\n\tdmu_tx_commit(tx);\n\ttxg_wait_synced(spa_get_dsl(spa), spa->spa_log_flushall_txg);\n}\n\nstatic void\nspa_unload_log_sm_metadata(spa_t *spa)\n{\n\tvoid *cookie = NULL;\n\tspa_log_sm_t *sls;\n\tlog_summary_entry_t *e;\n\n\twhile ((sls = avl_destroy_nodes(&spa->spa_sm_logs_by_txg,\n\t    &cookie)) != NULL) {\n\t\tVERIFY0(sls->sls_mscount);\n\t\tkmem_free(sls, sizeof (spa_log_sm_t));\n\t}\n\n\twhile ((e = list_remove_head(&spa->spa_log_summary)) != NULL) {\n\t\tVERIFY0(e->lse_mscount);\n\t\tkmem_free(e, sizeof (log_summary_entry_t));\n\t}\n\n\tspa->spa_unflushed_stats.sus_nblocks = 0;\n\tspa->spa_unflushed_stats.sus_memused = 0;\n\tspa->spa_unflushed_stats.sus_blocklimit = 0;\n}\n\nstatic void\nspa_destroy_aux_threads(spa_t *spa)\n{\n\tif (spa->spa_condense_zthr != NULL) {\n\t\tzthr_destroy(spa->spa_condense_zthr);\n\t\tspa->spa_condense_zthr = NULL;\n\t}\n\tif (spa->spa_checkpoint_discard_zthr != NULL) {\n\t\tzthr_destroy(spa->spa_checkpoint_discard_zthr);\n\t\tspa->spa_checkpoint_discard_zthr = NULL;\n\t}\n\tif (spa->spa_livelist_delete_zthr != NULL) {\n\t\tzthr_destroy(spa->spa_livelist_delete_zthr);\n\t\tspa->spa_livelist_delete_zthr = NULL;\n\t}\n\tif (spa->spa_livelist_condense_zthr != NULL) {\n\t\tzthr_destroy(spa->spa_livelist_condense_zthr);\n\t\tspa->spa_livelist_condense_zthr = NULL;\n\t}\n}\n\n \nstatic void\nspa_unload(spa_t *spa)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tASSERT(spa_state(spa) != POOL_STATE_UNINITIALIZED);\n\n\tspa_import_progress_remove(spa_guid(spa));\n\tspa_load_note(spa, \"UNLOADING\");\n\n\tspa_wake_waiters(spa);\n\n\t \n\tif (spa->spa_final_txg == UINT64_MAX) {\n\t\t \n\t\tif (spa_should_flush_logs_on_unload(spa))\n\t\t\tspa_unload_log_sm_flush_all(spa);\n\n\t\t \n\t\tspa_async_suspend(spa);\n\n\t\tif (spa->spa_root_vdev) {\n\t\t\tvdev_t *root_vdev = spa->spa_root_vdev;\n\t\t\tvdev_initialize_stop_all(root_vdev,\n\t\t\t    VDEV_INITIALIZE_ACTIVE);\n\t\t\tvdev_trim_stop_all(root_vdev, VDEV_TRIM_ACTIVE);\n\t\t\tvdev_autotrim_stop_all(spa);\n\t\t\tvdev_rebuild_stop_all(spa);\n\t\t}\n\t}\n\n\t \n\tif (spa->spa_sync_on) {\n\t\ttxg_sync_stop(spa->spa_dsl_pool);\n\t\tspa->spa_sync_on = B_FALSE;\n\t}\n\n\t \n\ttaskq_wait(spa->spa_metaslab_taskq);\n\n\tif (spa->spa_mmp.mmp_thread)\n\t\tmmp_thread_stop(spa);\n\n\t \n\tif (spa->spa_async_zio_root != NULL) {\n\t\tfor (int i = 0; i < max_ncpus; i++)\n\t\t\t(void) zio_wait(spa->spa_async_zio_root[i]);\n\t\tkmem_free(spa->spa_async_zio_root, max_ncpus * sizeof (void *));\n\t\tspa->spa_async_zio_root = NULL;\n\t}\n\n\tif (spa->spa_vdev_removal != NULL) {\n\t\tspa_vdev_removal_destroy(spa->spa_vdev_removal);\n\t\tspa->spa_vdev_removal = NULL;\n\t}\n\n\tspa_destroy_aux_threads(spa);\n\n\tspa_condense_fini(spa);\n\n\tbpobj_close(&spa->spa_deferred_bpobj);\n\n\tspa_config_enter(spa, SCL_ALL, spa, RW_WRITER);\n\n\t \n\tif (spa->spa_root_vdev)\n\t\tvdev_free(spa->spa_root_vdev);\n\tASSERT(spa->spa_root_vdev == NULL);\n\n\t \n\tif (spa->spa_dsl_pool) {\n\t\tdsl_pool_close(spa->spa_dsl_pool);\n\t\tspa->spa_dsl_pool = NULL;\n\t\tspa->spa_meta_objset = NULL;\n\t}\n\n\tddt_unload(spa);\n\tbrt_unload(spa);\n\tspa_unload_log_sm_metadata(spa);\n\n\t \n\tspa_l2cache_drop(spa);\n\n\tif (spa->spa_spares.sav_vdevs) {\n\t\tfor (int i = 0; i < spa->spa_spares.sav_count; i++)\n\t\t\tvdev_free(spa->spa_spares.sav_vdevs[i]);\n\t\tkmem_free(spa->spa_spares.sav_vdevs,\n\t\t    spa->spa_spares.sav_count * sizeof (void *));\n\t\tspa->spa_spares.sav_vdevs = NULL;\n\t}\n\tif (spa->spa_spares.sav_config) {\n\t\tnvlist_free(spa->spa_spares.sav_config);\n\t\tspa->spa_spares.sav_config = NULL;\n\t}\n\tspa->spa_spares.sav_count = 0;\n\n\tif (spa->spa_l2cache.sav_vdevs) {\n\t\tfor (int i = 0; i < spa->spa_l2cache.sav_count; i++) {\n\t\t\tvdev_clear_stats(spa->spa_l2cache.sav_vdevs[i]);\n\t\t\tvdev_free(spa->spa_l2cache.sav_vdevs[i]);\n\t\t}\n\t\tkmem_free(spa->spa_l2cache.sav_vdevs,\n\t\t    spa->spa_l2cache.sav_count * sizeof (void *));\n\t\tspa->spa_l2cache.sav_vdevs = NULL;\n\t}\n\tif (spa->spa_l2cache.sav_config) {\n\t\tnvlist_free(spa->spa_l2cache.sav_config);\n\t\tspa->spa_l2cache.sav_config = NULL;\n\t}\n\tspa->spa_l2cache.sav_count = 0;\n\n\tspa->spa_async_suspended = 0;\n\n\tspa->spa_indirect_vdevs_loaded = B_FALSE;\n\n\tif (spa->spa_comment != NULL) {\n\t\tspa_strfree(spa->spa_comment);\n\t\tspa->spa_comment = NULL;\n\t}\n\tif (spa->spa_compatibility != NULL) {\n\t\tspa_strfree(spa->spa_compatibility);\n\t\tspa->spa_compatibility = NULL;\n\t}\n\n\tspa_config_exit(spa, SCL_ALL, spa);\n}\n\n \nvoid\nspa_load_spares(spa_t *spa)\n{\n\tnvlist_t **spares;\n\tuint_t nspares;\n\tint i;\n\tvdev_t *vd, *tvd;\n\n#ifndef _KERNEL\n\t \n\tif (!spa_writeable(spa))\n\t\treturn;\n#endif\n\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == SCL_ALL);\n\n\t \n\tif (spa->spa_spares.sav_vdevs) {\n\t\tfor (i = 0; i < spa->spa_spares.sav_count; i++) {\n\t\t\tvd = spa->spa_spares.sav_vdevs[i];\n\n\t\t\t \n\t\t\tif ((tvd = spa_lookup_by_guid(spa, vd->vdev_guid,\n\t\t\t    B_FALSE)) != NULL && tvd->vdev_isspare)\n\t\t\t\tspa_spare_remove(tvd);\n\t\t\tvdev_close(vd);\n\t\t\tvdev_free(vd);\n\t\t}\n\n\t\tkmem_free(spa->spa_spares.sav_vdevs,\n\t\t    spa->spa_spares.sav_count * sizeof (void *));\n\t}\n\n\tif (spa->spa_spares.sav_config == NULL)\n\t\tnspares = 0;\n\telse\n\t\tVERIFY0(nvlist_lookup_nvlist_array(spa->spa_spares.sav_config,\n\t\t    ZPOOL_CONFIG_SPARES, &spares, &nspares));\n\n\tspa->spa_spares.sav_count = (int)nspares;\n\tspa->spa_spares.sav_vdevs = NULL;\n\n\tif (nspares == 0)\n\t\treturn;\n\n\t \n\tspa->spa_spares.sav_vdevs = kmem_zalloc(nspares * sizeof (void *),\n\t    KM_SLEEP);\n\tfor (i = 0; i < spa->spa_spares.sav_count; i++) {\n\t\tVERIFY(spa_config_parse(spa, &vd, spares[i], NULL, 0,\n\t\t    VDEV_ALLOC_SPARE) == 0);\n\t\tASSERT(vd != NULL);\n\n\t\tspa->spa_spares.sav_vdevs[i] = vd;\n\n\t\tif ((tvd = spa_lookup_by_guid(spa, vd->vdev_guid,\n\t\t    B_FALSE)) != NULL) {\n\t\t\tif (!tvd->vdev_isspare)\n\t\t\t\tspa_spare_add(tvd);\n\n\t\t\t \n\t\t\tif (!vdev_is_dead(tvd))\n\t\t\t\tspa_spare_activate(tvd);\n\t\t}\n\n\t\tvd->vdev_top = vd;\n\t\tvd->vdev_aux = &spa->spa_spares;\n\n\t\tif (vdev_open(vd) != 0)\n\t\t\tcontinue;\n\n\t\tif (vdev_validate_aux(vd) == 0)\n\t\t\tspa_spare_add(vd);\n\t}\n\n\t \n\tfnvlist_remove(spa->spa_spares.sav_config, ZPOOL_CONFIG_SPARES);\n\n\tspares = kmem_alloc(spa->spa_spares.sav_count * sizeof (void *),\n\t    KM_SLEEP);\n\tfor (i = 0; i < spa->spa_spares.sav_count; i++)\n\t\tspares[i] = vdev_config_generate(spa,\n\t\t    spa->spa_spares.sav_vdevs[i], B_TRUE, VDEV_CONFIG_SPARE);\n\tfnvlist_add_nvlist_array(spa->spa_spares.sav_config,\n\t    ZPOOL_CONFIG_SPARES, (const nvlist_t * const *)spares,\n\t    spa->spa_spares.sav_count);\n\tfor (i = 0; i < spa->spa_spares.sav_count; i++)\n\t\tnvlist_free(spares[i]);\n\tkmem_free(spares, spa->spa_spares.sav_count * sizeof (void *));\n}\n\n \nvoid\nspa_load_l2cache(spa_t *spa)\n{\n\tnvlist_t **l2cache = NULL;\n\tuint_t nl2cache;\n\tint i, j, oldnvdevs;\n\tuint64_t guid;\n\tvdev_t *vd, **oldvdevs, **newvdevs;\n\tspa_aux_vdev_t *sav = &spa->spa_l2cache;\n\n#ifndef _KERNEL\n\t \n\tif (!spa_writeable(spa))\n\t\treturn;\n#endif\n\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == SCL_ALL);\n\n\toldvdevs = sav->sav_vdevs;\n\toldnvdevs = sav->sav_count;\n\tsav->sav_vdevs = NULL;\n\tsav->sav_count = 0;\n\n\tif (sav->sav_config == NULL) {\n\t\tnl2cache = 0;\n\t\tnewvdevs = NULL;\n\t\tgoto out;\n\t}\n\n\tVERIFY0(nvlist_lookup_nvlist_array(sav->sav_config,\n\t    ZPOOL_CONFIG_L2CACHE, &l2cache, &nl2cache));\n\tnewvdevs = kmem_alloc(nl2cache * sizeof (void *), KM_SLEEP);\n\n\t \n\tfor (i = 0; i < nl2cache; i++) {\n\t\tguid = fnvlist_lookup_uint64(l2cache[i], ZPOOL_CONFIG_GUID);\n\n\t\tnewvdevs[i] = NULL;\n\t\tfor (j = 0; j < oldnvdevs; j++) {\n\t\t\tvd = oldvdevs[j];\n\t\t\tif (vd != NULL && guid == vd->vdev_guid) {\n\t\t\t\t \n\t\t\t\tnewvdevs[i] = vd;\n\t\t\t\toldvdevs[j] = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (newvdevs[i] == NULL) {\n\t\t\t \n\t\t\tVERIFY(spa_config_parse(spa, &vd, l2cache[i], NULL, 0,\n\t\t\t    VDEV_ALLOC_L2CACHE) == 0);\n\t\t\tASSERT(vd != NULL);\n\t\t\tnewvdevs[i] = vd;\n\n\t\t\t \n\t\t\tspa_l2cache_add(vd);\n\n\t\t\tvd->vdev_top = vd;\n\t\t\tvd->vdev_aux = sav;\n\n\t\t\tspa_l2cache_activate(vd);\n\n\t\t\tif (vdev_open(vd) != 0)\n\t\t\t\tcontinue;\n\n\t\t\t(void) vdev_validate_aux(vd);\n\n\t\t\tif (!vdev_is_dead(vd))\n\t\t\t\tl2arc_add_vdev(spa, vd);\n\n\t\t\t \n\t\t\tspa_async_request(spa, SPA_ASYNC_L2CACHE_TRIM);\n\t\t}\n\t}\n\n\tsav->sav_vdevs = newvdevs;\n\tsav->sav_count = (int)nl2cache;\n\n\t \n\tfnvlist_remove(sav->sav_config, ZPOOL_CONFIG_L2CACHE);\n\n\tif (sav->sav_count > 0)\n\t\tl2cache = kmem_alloc(sav->sav_count * sizeof (void *),\n\t\t    KM_SLEEP);\n\tfor (i = 0; i < sav->sav_count; i++)\n\t\tl2cache[i] = vdev_config_generate(spa,\n\t\t    sav->sav_vdevs[i], B_TRUE, VDEV_CONFIG_L2CACHE);\n\tfnvlist_add_nvlist_array(sav->sav_config, ZPOOL_CONFIG_L2CACHE,\n\t    (const nvlist_t * const *)l2cache, sav->sav_count);\n\nout:\n\t \n\tif (oldvdevs) {\n\t\tfor (i = 0; i < oldnvdevs; i++) {\n\t\t\tuint64_t pool;\n\n\t\t\tvd = oldvdevs[i];\n\t\t\tif (vd != NULL) {\n\t\t\t\tASSERT(vd->vdev_isl2cache);\n\n\t\t\t\tif (spa_l2cache_exists(vd->vdev_guid, &pool) &&\n\t\t\t\t    pool != 0ULL && l2arc_vdev_present(vd))\n\t\t\t\t\tl2arc_remove_vdev(vd);\n\t\t\t\tvdev_clear_stats(vd);\n\t\t\t\tvdev_free(vd);\n\t\t\t}\n\t\t}\n\n\t\tkmem_free(oldvdevs, oldnvdevs * sizeof (void *));\n\t}\n\n\tfor (i = 0; i < sav->sav_count; i++)\n\t\tnvlist_free(l2cache[i]);\n\tif (sav->sav_count)\n\t\tkmem_free(l2cache, sav->sav_count * sizeof (void *));\n}\n\nstatic int\nload_nvlist(spa_t *spa, uint64_t obj, nvlist_t **value)\n{\n\tdmu_buf_t *db;\n\tchar *packed = NULL;\n\tsize_t nvsize = 0;\n\tint error;\n\t*value = NULL;\n\n\terror = dmu_bonus_hold(spa->spa_meta_objset, obj, FTAG, &db);\n\tif (error)\n\t\treturn (error);\n\n\tnvsize = *(uint64_t *)db->db_data;\n\tdmu_buf_rele(db, FTAG);\n\n\tpacked = vmem_alloc(nvsize, KM_SLEEP);\n\terror = dmu_read(spa->spa_meta_objset, obj, 0, nvsize, packed,\n\t    DMU_READ_PREFETCH);\n\tif (error == 0)\n\t\terror = nvlist_unpack(packed, nvsize, value, 0);\n\tvmem_free(packed, nvsize);\n\n\treturn (error);\n}\n\n \nstatic uint64_t\nspa_healthy_core_tvds(spa_t *spa)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tuint64_t tvds = 0;\n\n\tfor (uint64_t i = 0; i < rvd->vdev_children; i++) {\n\t\tvdev_t *vd = rvd->vdev_child[i];\n\t\tif (vd->vdev_islog)\n\t\t\tcontinue;\n\t\tif (vdev_is_concrete(vd) && !vdev_is_dead(vd))\n\t\t\ttvds++;\n\t}\n\n\treturn (tvds);\n}\n\n \nstatic void\nspa_check_removed(vdev_t *vd)\n{\n\tfor (uint64_t c = 0; c < vd->vdev_children; c++)\n\t\tspa_check_removed(vd->vdev_child[c]);\n\n\tif (vd->vdev_ops->vdev_op_leaf && vdev_is_dead(vd) &&\n\t    vdev_is_concrete(vd)) {\n\t\tzfs_post_autoreplace(vd->vdev_spa, vd);\n\t\tspa_event_notify(vd->vdev_spa, vd, NULL, ESC_ZFS_VDEV_CHECK);\n\t}\n}\n\nstatic int\nspa_check_for_missing_logs(spa_t *spa)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t \n\tif (!(spa->spa_import_flags & ZFS_IMPORT_MISSING_LOG)) {\n\t\tnvlist_t **child, *nv;\n\t\tuint64_t idx = 0;\n\n\t\tchild = kmem_alloc(rvd->vdev_children * sizeof (nvlist_t *),\n\t\t    KM_SLEEP);\n\t\tnv = fnvlist_alloc();\n\n\t\tfor (uint64_t c = 0; c < rvd->vdev_children; c++) {\n\t\t\tvdev_t *tvd = rvd->vdev_child[c];\n\n\t\t\t \n\t\t\tif (tvd->vdev_islog &&\n\t\t\t    tvd->vdev_state == VDEV_STATE_CANT_OPEN) {\n\t\t\t\tchild[idx++] = vdev_config_generate(spa, tvd,\n\t\t\t\t    B_FALSE, VDEV_CONFIG_MISSING);\n\t\t\t}\n\t\t}\n\n\t\tif (idx > 0) {\n\t\t\tfnvlist_add_nvlist_array(nv, ZPOOL_CONFIG_CHILDREN,\n\t\t\t    (const nvlist_t * const *)child, idx);\n\t\t\tfnvlist_add_nvlist(spa->spa_load_info,\n\t\t\t    ZPOOL_CONFIG_MISSING_DEVICES, nv);\n\n\t\t\tfor (uint64_t i = 0; i < idx; i++)\n\t\t\t\tnvlist_free(child[i]);\n\t\t}\n\t\tnvlist_free(nv);\n\t\tkmem_free(child, rvd->vdev_children * sizeof (char **));\n\n\t\tif (idx > 0) {\n\t\t\tspa_load_failed(spa, \"some log devices are missing\");\n\t\t\tvdev_dbgmsg_print_tree(rvd, 2);\n\t\t\treturn (SET_ERROR(ENXIO));\n\t\t}\n\t} else {\n\t\tfor (uint64_t c = 0; c < rvd->vdev_children; c++) {\n\t\t\tvdev_t *tvd = rvd->vdev_child[c];\n\n\t\t\tif (tvd->vdev_islog &&\n\t\t\t    tvd->vdev_state == VDEV_STATE_CANT_OPEN) {\n\t\t\t\tspa_set_log_state(spa, SPA_LOG_CLEAR);\n\t\t\t\tspa_load_note(spa, \"some log devices are \"\n\t\t\t\t    \"missing, ZIL is dropped.\");\n\t\t\t\tvdev_dbgmsg_print_tree(rvd, 2);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn (0);\n}\n\n \nstatic boolean_t\nspa_check_logs(spa_t *spa)\n{\n\tboolean_t rv = B_FALSE;\n\tdsl_pool_t *dp = spa_get_dsl(spa);\n\n\tswitch (spa->spa_log_state) {\n\tdefault:\n\t\tbreak;\n\tcase SPA_LOG_MISSING:\n\t\t \n\tcase SPA_LOG_UNKNOWN:\n\t\trv = (dmu_objset_find_dp(dp, dp->dp_root_dir_obj,\n\t\t    zil_check_log_chain, NULL, DS_FIND_CHILDREN) != 0);\n\t\tif (rv)\n\t\t\tspa_set_log_state(spa, SPA_LOG_MISSING);\n\t\tbreak;\n\t}\n\treturn (rv);\n}\n\n \nstatic boolean_t\nspa_passivate_log(spa_t *spa)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tboolean_t slog_found = B_FALSE;\n\n\tASSERT(spa_config_held(spa, SCL_ALLOC, RW_WRITER));\n\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *tvd = rvd->vdev_child[c];\n\n\t\tif (tvd->vdev_islog) {\n\t\t\tASSERT3P(tvd->vdev_log_mg, ==, NULL);\n\t\t\tmetaslab_group_passivate(tvd->vdev_mg);\n\t\t\tslog_found = B_TRUE;\n\t\t}\n\t}\n\n\treturn (slog_found);\n}\n\n \nstatic void\nspa_activate_log(spa_t *spa)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\tASSERT(spa_config_held(spa, SCL_ALLOC, RW_WRITER));\n\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *tvd = rvd->vdev_child[c];\n\n\t\tif (tvd->vdev_islog) {\n\t\t\tASSERT3P(tvd->vdev_log_mg, ==, NULL);\n\t\t\tmetaslab_group_activate(tvd->vdev_mg);\n\t\t}\n\t}\n}\n\nint\nspa_reset_logs(spa_t *spa)\n{\n\tint error;\n\n\terror = dmu_objset_find(spa_name(spa), zil_reset,\n\t    NULL, DS_FIND_CHILDREN);\n\tif (error == 0) {\n\t\t \n\t\ttxg_wait_synced(spa->spa_dsl_pool, 0);\n\t}\n\treturn (error);\n}\n\nstatic void\nspa_aux_check_removed(spa_aux_vdev_t *sav)\n{\n\tfor (int i = 0; i < sav->sav_count; i++)\n\t\tspa_check_removed(sav->sav_vdevs[i]);\n}\n\nvoid\nspa_claim_notify(zio_t *zio)\n{\n\tspa_t *spa = zio->io_spa;\n\n\tif (zio->io_error)\n\t\treturn;\n\n\tmutex_enter(&spa->spa_props_lock);\t \n\tif (spa->spa_claim_max_txg < zio->io_bp->blk_birth)\n\t\tspa->spa_claim_max_txg = zio->io_bp->blk_birth;\n\tmutex_exit(&spa->spa_props_lock);\n}\n\ntypedef struct spa_load_error {\n\tboolean_t\tsle_verify_data;\n\tuint64_t\tsle_meta_count;\n\tuint64_t\tsle_data_count;\n} spa_load_error_t;\n\nstatic void\nspa_load_verify_done(zio_t *zio)\n{\n\tblkptr_t *bp = zio->io_bp;\n\tspa_load_error_t *sle = zio->io_private;\n\tdmu_object_type_t type = BP_GET_TYPE(bp);\n\tint error = zio->io_error;\n\tspa_t *spa = zio->io_spa;\n\n\tabd_free(zio->io_abd);\n\tif (error) {\n\t\tif ((BP_GET_LEVEL(bp) != 0 || DMU_OT_IS_METADATA(type)) &&\n\t\t    type != DMU_OT_INTENT_LOG)\n\t\t\tatomic_inc_64(&sle->sle_meta_count);\n\t\telse\n\t\t\tatomic_inc_64(&sle->sle_data_count);\n\t}\n\n\tmutex_enter(&spa->spa_scrub_lock);\n\tspa->spa_load_verify_bytes -= BP_GET_PSIZE(bp);\n\tcv_broadcast(&spa->spa_scrub_io_cv);\n\tmutex_exit(&spa->spa_scrub_lock);\n}\n\n \nstatic uint_t spa_load_verify_shift = 4;\nstatic int spa_load_verify_metadata = B_TRUE;\nstatic int spa_load_verify_data = B_TRUE;\n\nstatic int\nspa_load_verify_cb(spa_t *spa, zilog_t *zilog, const blkptr_t *bp,\n    const zbookmark_phys_t *zb, const dnode_phys_t *dnp, void *arg)\n{\n\tzio_t *rio = arg;\n\tspa_load_error_t *sle = rio->io_private;\n\n\t(void) zilog, (void) dnp;\n\n\t \n\tif (!spa_load_verify_metadata)\n\t\treturn (0);\n\n\t \n\tif (!zfs_blkptr_verify(spa, bp, BLK_CONFIG_NEEDED, BLK_VERIFY_LOG)) {\n\t\tatomic_inc_64(&sle->sle_meta_count);\n\t\treturn (0);\n\t}\n\n\tif (zb->zb_level == ZB_DNODE_LEVEL || BP_IS_HOLE(bp) ||\n\t    BP_IS_EMBEDDED(bp) || BP_IS_REDACTED(bp))\n\t\treturn (0);\n\n\tif (!BP_IS_METADATA(bp) &&\n\t    (!spa_load_verify_data || !sle->sle_verify_data))\n\t\treturn (0);\n\n\tuint64_t maxinflight_bytes =\n\t    arc_target_bytes() >> spa_load_verify_shift;\n\tsize_t size = BP_GET_PSIZE(bp);\n\n\tmutex_enter(&spa->spa_scrub_lock);\n\twhile (spa->spa_load_verify_bytes >= maxinflight_bytes)\n\t\tcv_wait(&spa->spa_scrub_io_cv, &spa->spa_scrub_lock);\n\tspa->spa_load_verify_bytes += size;\n\tmutex_exit(&spa->spa_scrub_lock);\n\n\tzio_nowait(zio_read(rio, spa, bp, abd_alloc_for_io(size, B_FALSE), size,\n\t    spa_load_verify_done, rio->io_private, ZIO_PRIORITY_SCRUB,\n\t    ZIO_FLAG_SPECULATIVE | ZIO_FLAG_CANFAIL |\n\t    ZIO_FLAG_SCRUB | ZIO_FLAG_RAW, zb));\n\treturn (0);\n}\n\nstatic int\nverify_dataset_name_len(dsl_pool_t *dp, dsl_dataset_t *ds, void *arg)\n{\n\t(void) dp, (void) arg;\n\n\tif (dsl_dataset_namelen(ds) >= ZFS_MAX_DATASET_NAME_LEN)\n\t\treturn (SET_ERROR(ENAMETOOLONG));\n\n\treturn (0);\n}\n\nstatic int\nspa_load_verify(spa_t *spa)\n{\n\tzio_t *rio;\n\tspa_load_error_t sle = { 0 };\n\tzpool_load_policy_t policy;\n\tboolean_t verify_ok = B_FALSE;\n\tint error = 0;\n\n\tzpool_get_load_policy(spa->spa_config, &policy);\n\n\tif (policy.zlp_rewind & ZPOOL_NEVER_REWIND ||\n\t    policy.zlp_maxmeta == UINT64_MAX)\n\t\treturn (0);\n\n\tdsl_pool_config_enter(spa->spa_dsl_pool, FTAG);\n\terror = dmu_objset_find_dp(spa->spa_dsl_pool,\n\t    spa->spa_dsl_pool->dp_root_dir_obj, verify_dataset_name_len, NULL,\n\t    DS_FIND_CHILDREN);\n\tdsl_pool_config_exit(spa->spa_dsl_pool, FTAG);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\tsle.sle_verify_data = (policy.zlp_rewind & ZPOOL_REWIND_MASK) ||\n\t    (policy.zlp_maxdata < UINT64_MAX);\n\n\trio = zio_root(spa, NULL, &sle,\n\t    ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE);\n\n\tif (spa_load_verify_metadata) {\n\t\tif (spa->spa_extreme_rewind) {\n\t\t\tspa_load_note(spa, \"performing a complete scan of the \"\n\t\t\t    \"pool since extreme rewind is on. This may take \"\n\t\t\t    \"a very long time.\\n  (spa_load_verify_data=%u, \"\n\t\t\t    \"spa_load_verify_metadata=%u)\",\n\t\t\t    spa_load_verify_data, spa_load_verify_metadata);\n\t\t}\n\n\t\terror = traverse_pool(spa, spa->spa_verify_min_txg,\n\t\t    TRAVERSE_PRE | TRAVERSE_PREFETCH_METADATA |\n\t\t    TRAVERSE_NO_DECRYPT, spa_load_verify_cb, rio);\n\t}\n\n\t(void) zio_wait(rio);\n\tASSERT0(spa->spa_load_verify_bytes);\n\n\tspa->spa_load_meta_errors = sle.sle_meta_count;\n\tspa->spa_load_data_errors = sle.sle_data_count;\n\n\tif (sle.sle_meta_count != 0 || sle.sle_data_count != 0) {\n\t\tspa_load_note(spa, \"spa_load_verify found %llu metadata errors \"\n\t\t    \"and %llu data errors\", (u_longlong_t)sle.sle_meta_count,\n\t\t    (u_longlong_t)sle.sle_data_count);\n\t}\n\n\tif (spa_load_verify_dryrun ||\n\t    (!error && sle.sle_meta_count <= policy.zlp_maxmeta &&\n\t    sle.sle_data_count <= policy.zlp_maxdata)) {\n\t\tint64_t loss = 0;\n\n\t\tverify_ok = B_TRUE;\n\t\tspa->spa_load_txg = spa->spa_uberblock.ub_txg;\n\t\tspa->spa_load_txg_ts = spa->spa_uberblock.ub_timestamp;\n\n\t\tloss = spa->spa_last_ubsync_txg_ts - spa->spa_load_txg_ts;\n\t\tfnvlist_add_uint64(spa->spa_load_info, ZPOOL_CONFIG_LOAD_TIME,\n\t\t    spa->spa_load_txg_ts);\n\t\tfnvlist_add_int64(spa->spa_load_info, ZPOOL_CONFIG_REWIND_TIME,\n\t\t    loss);\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_LOAD_META_ERRORS, sle.sle_meta_count);\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_LOAD_DATA_ERRORS, sle.sle_data_count);\n\t} else {\n\t\tspa->spa_load_max_txg = spa->spa_uberblock.ub_txg;\n\t}\n\n\tif (spa_load_verify_dryrun)\n\t\treturn (0);\n\n\tif (error) {\n\t\tif (error != ENXIO && error != EIO)\n\t\t\terror = SET_ERROR(EIO);\n\t\treturn (error);\n\t}\n\n\treturn (verify_ok ? 0 : EIO);\n}\n\n \nstatic void\nspa_prop_find(spa_t *spa, zpool_prop_t prop, uint64_t *val)\n{\n\t(void) zap_lookup(spa->spa_meta_objset, spa->spa_pool_props_object,\n\t    zpool_prop_to_name(prop), sizeof (uint64_t), 1, val);\n}\n\n \nstatic int\nspa_dir_prop(spa_t *spa, const char *name, uint64_t *val, boolean_t log_enoent)\n{\n\tint error = zap_lookup(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,\n\t    name, sizeof (uint64_t), 1, val);\n\n\tif (error != 0 && (error != ENOENT || log_enoent)) {\n\t\tspa_load_failed(spa, \"couldn't get '%s' value in MOS directory \"\n\t\t    \"[error=%d]\", name, error);\n\t}\n\n\treturn (error);\n}\n\nstatic int\nspa_vdev_err(vdev_t *vdev, vdev_aux_t aux, int err)\n{\n\tvdev_set_state(vdev, B_TRUE, VDEV_STATE_CANT_OPEN, aux);\n\treturn (SET_ERROR(err));\n}\n\nboolean_t\nspa_livelist_delete_check(spa_t *spa)\n{\n\treturn (spa->spa_livelists_to_delete != 0);\n}\n\nstatic boolean_t\nspa_livelist_delete_cb_check(void *arg, zthr_t *z)\n{\n\t(void) z;\n\tspa_t *spa = arg;\n\treturn (spa_livelist_delete_check(spa));\n}\n\nstatic int\ndelete_blkptr_cb(void *arg, const blkptr_t *bp, dmu_tx_t *tx)\n{\n\tspa_t *spa = arg;\n\tzio_free(spa, tx->tx_txg, bp);\n\tdsl_dir_diduse_space(tx->tx_pool->dp_free_dir, DD_USED_HEAD,\n\t    -bp_get_dsize_sync(spa, bp),\n\t    -BP_GET_PSIZE(bp), -BP_GET_UCSIZE(bp), tx);\n\treturn (0);\n}\n\nstatic int\ndsl_get_next_livelist_obj(objset_t *os, uint64_t zap_obj, uint64_t *llp)\n{\n\tint err;\n\tzap_cursor_t zc;\n\tzap_attribute_t za;\n\tzap_cursor_init(&zc, os, zap_obj);\n\terr = zap_cursor_retrieve(&zc, &za);\n\tzap_cursor_fini(&zc);\n\tif (err == 0)\n\t\t*llp = za.za_first_integer;\n\treturn (err);\n}\n\n \ntypedef struct sublist_delete_arg {\n\tspa_t *spa;\n\tdsl_deadlist_t *ll;\n\tuint64_t key;\n\tbplist_t *to_free;\n} sublist_delete_arg_t;\n\nstatic void\nsublist_delete_sync(void *arg, dmu_tx_t *tx)\n{\n\tsublist_delete_arg_t *sda = arg;\n\tspa_t *spa = sda->spa;\n\tdsl_deadlist_t *ll = sda->ll;\n\tuint64_t key = sda->key;\n\tbplist_t *to_free = sda->to_free;\n\n\tbplist_iterate(to_free, delete_blkptr_cb, spa, tx);\n\tdsl_deadlist_remove_entry(ll, key, tx);\n}\n\ntypedef struct livelist_delete_arg {\n\tspa_t *spa;\n\tuint64_t ll_obj;\n\tuint64_t zap_obj;\n} livelist_delete_arg_t;\n\nstatic void\nlivelist_delete_sync(void *arg, dmu_tx_t *tx)\n{\n\tlivelist_delete_arg_t *lda = arg;\n\tspa_t *spa = lda->spa;\n\tuint64_t ll_obj = lda->ll_obj;\n\tuint64_t zap_obj = lda->zap_obj;\n\tobjset_t *mos = spa->spa_meta_objset;\n\tuint64_t count;\n\n\t \n\tVERIFY0(zap_remove_int(mos, zap_obj, ll_obj, tx));\n\tdsl_deadlist_free(mos, ll_obj, tx);\n\tspa_feature_decr(spa, SPA_FEATURE_LIVELIST, tx);\n\tVERIFY0(zap_count(mos, zap_obj, &count));\n\tif (count == 0) {\n\t\t \n\t\tVERIFY0(zap_remove(mos, DMU_POOL_DIRECTORY_OBJECT,\n\t\t    DMU_POOL_DELETED_CLONES, tx));\n\t\tVERIFY0(zap_destroy(mos, zap_obj, tx));\n\t\tspa->spa_livelists_to_delete = 0;\n\t\tspa_notify_waiters(spa);\n\t}\n}\n\n \nstatic void\nspa_livelist_delete_cb(void *arg, zthr_t *z)\n{\n\tspa_t *spa = arg;\n\tuint64_t ll_obj = 0, count;\n\tobjset_t *mos = spa->spa_meta_objset;\n\tuint64_t zap_obj = spa->spa_livelists_to_delete;\n\t \n\tVERIFY0(dsl_get_next_livelist_obj(mos, zap_obj, &ll_obj));\n\tVERIFY0(zap_count(mos, ll_obj, &count));\n\tif (count > 0) {\n\t\tdsl_deadlist_t *ll;\n\t\tdsl_deadlist_entry_t *dle;\n\t\tbplist_t to_free;\n\t\tll = kmem_zalloc(sizeof (dsl_deadlist_t), KM_SLEEP);\n\t\tdsl_deadlist_open(ll, mos, ll_obj);\n\t\tdle = dsl_deadlist_first(ll);\n\t\tASSERT3P(dle, !=, NULL);\n\t\tbplist_create(&to_free);\n\t\tint err = dsl_process_sub_livelist(&dle->dle_bpobj, &to_free,\n\t\t    z, NULL);\n\t\tif (err == 0) {\n\t\t\tsublist_delete_arg_t sync_arg = {\n\t\t\t    .spa = spa,\n\t\t\t    .ll = ll,\n\t\t\t    .key = dle->dle_mintxg,\n\t\t\t    .to_free = &to_free\n\t\t\t};\n\t\t\tzfs_dbgmsg(\"deleting sublist (id %llu) from\"\n\t\t\t    \" livelist %llu, %lld remaining\",\n\t\t\t    (u_longlong_t)dle->dle_bpobj.bpo_object,\n\t\t\t    (u_longlong_t)ll_obj, (longlong_t)count - 1);\n\t\t\tVERIFY0(dsl_sync_task(spa_name(spa), NULL,\n\t\t\t    sublist_delete_sync, &sync_arg, 0,\n\t\t\t    ZFS_SPACE_CHECK_DESTROY));\n\t\t} else {\n\t\t\tVERIFY3U(err, ==, EINTR);\n\t\t}\n\t\tbplist_clear(&to_free);\n\t\tbplist_destroy(&to_free);\n\t\tdsl_deadlist_close(ll);\n\t\tkmem_free(ll, sizeof (dsl_deadlist_t));\n\t} else {\n\t\tlivelist_delete_arg_t sync_arg = {\n\t\t    .spa = spa,\n\t\t    .ll_obj = ll_obj,\n\t\t    .zap_obj = zap_obj\n\t\t};\n\t\tzfs_dbgmsg(\"deletion of livelist %llu completed\",\n\t\t    (u_longlong_t)ll_obj);\n\t\tVERIFY0(dsl_sync_task(spa_name(spa), NULL, livelist_delete_sync,\n\t\t    &sync_arg, 0, ZFS_SPACE_CHECK_DESTROY));\n\t}\n}\n\nstatic void\nspa_start_livelist_destroy_thread(spa_t *spa)\n{\n\tASSERT3P(spa->spa_livelist_delete_zthr, ==, NULL);\n\tspa->spa_livelist_delete_zthr =\n\t    zthr_create(\"z_livelist_destroy\",\n\t    spa_livelist_delete_cb_check, spa_livelist_delete_cb, spa,\n\t    minclsyspri);\n}\n\ntypedef struct livelist_new_arg {\n\tbplist_t *allocs;\n\tbplist_t *frees;\n} livelist_new_arg_t;\n\nstatic int\nlivelist_track_new_cb(void *arg, const blkptr_t *bp, boolean_t bp_freed,\n    dmu_tx_t *tx)\n{\n\tASSERT(tx == NULL);\n\tlivelist_new_arg_t *lna = arg;\n\tif (bp_freed) {\n\t\tbplist_append(lna->frees, bp);\n\t} else {\n\t\tbplist_append(lna->allocs, bp);\n\t\tzfs_livelist_condense_new_alloc++;\n\t}\n\treturn (0);\n}\n\ntypedef struct livelist_condense_arg {\n\tspa_t *spa;\n\tbplist_t to_keep;\n\tuint64_t first_size;\n\tuint64_t next_size;\n} livelist_condense_arg_t;\n\nstatic void\nspa_livelist_condense_sync(void *arg, dmu_tx_t *tx)\n{\n\tlivelist_condense_arg_t *lca = arg;\n\tspa_t *spa = lca->spa;\n\tbplist_t new_frees;\n\tdsl_dataset_t *ds = spa->spa_to_condense.ds;\n\n\t \n\tif (spa->spa_to_condense.cancelled) {\n\t\tzfs_livelist_condense_sync_cancel++;\n\t\tgoto out;\n\t}\n\n\tdsl_deadlist_entry_t *first = spa->spa_to_condense.first;\n\tdsl_deadlist_entry_t *next = spa->spa_to_condense.next;\n\tdsl_deadlist_t *ll = &ds->ds_dir->dd_livelist;\n\n\t \n\tuint64_t first_obj = first->dle_bpobj.bpo_object;\n\tuint64_t next_obj = next->dle_bpobj.bpo_object;\n\tuint64_t cur_first_size = first->dle_bpobj.bpo_phys->bpo_num_blkptrs;\n\tuint64_t cur_next_size = next->dle_bpobj.bpo_phys->bpo_num_blkptrs;\n\n\tbplist_create(&new_frees);\n\tlivelist_new_arg_t new_bps = {\n\t    .allocs = &lca->to_keep,\n\t    .frees = &new_frees,\n\t};\n\n\tif (cur_first_size > lca->first_size) {\n\t\tVERIFY0(livelist_bpobj_iterate_from_nofree(&first->dle_bpobj,\n\t\t    livelist_track_new_cb, &new_bps, lca->first_size));\n\t}\n\tif (cur_next_size > lca->next_size) {\n\t\tVERIFY0(livelist_bpobj_iterate_from_nofree(&next->dle_bpobj,\n\t\t    livelist_track_new_cb, &new_bps, lca->next_size));\n\t}\n\n\tdsl_deadlist_clear_entry(first, ll, tx);\n\tASSERT(bpobj_is_empty(&first->dle_bpobj));\n\tdsl_deadlist_remove_entry(ll, next->dle_mintxg, tx);\n\n\tbplist_iterate(&lca->to_keep, dsl_deadlist_insert_alloc_cb, ll, tx);\n\tbplist_iterate(&new_frees, dsl_deadlist_insert_free_cb, ll, tx);\n\tbplist_destroy(&new_frees);\n\n\tchar dsname[ZFS_MAX_DATASET_NAME_LEN];\n\tdsl_dataset_name(ds, dsname);\n\tzfs_dbgmsg(\"txg %llu condensing livelist of %s (id %llu), bpobj %llu \"\n\t    \"(%llu blkptrs) and bpobj %llu (%llu blkptrs) -> bpobj %llu \"\n\t    \"(%llu blkptrs)\", (u_longlong_t)tx->tx_txg, dsname,\n\t    (u_longlong_t)ds->ds_object, (u_longlong_t)first_obj,\n\t    (u_longlong_t)cur_first_size, (u_longlong_t)next_obj,\n\t    (u_longlong_t)cur_next_size,\n\t    (u_longlong_t)first->dle_bpobj.bpo_object,\n\t    (u_longlong_t)first->dle_bpobj.bpo_phys->bpo_num_blkptrs);\nout:\n\tdmu_buf_rele(ds->ds_dbuf, spa);\n\tspa->spa_to_condense.ds = NULL;\n\tbplist_clear(&lca->to_keep);\n\tbplist_destroy(&lca->to_keep);\n\tkmem_free(lca, sizeof (livelist_condense_arg_t));\n\tspa->spa_to_condense.syncing = B_FALSE;\n}\n\nstatic void\nspa_livelist_condense_cb(void *arg, zthr_t *t)\n{\n\twhile (zfs_livelist_condense_zthr_pause &&\n\t    !(zthr_has_waiters(t) || zthr_iscancelled(t)))\n\t\tdelay(1);\n\n\tspa_t *spa = arg;\n\tdsl_deadlist_entry_t *first = spa->spa_to_condense.first;\n\tdsl_deadlist_entry_t *next = spa->spa_to_condense.next;\n\tuint64_t first_size, next_size;\n\n\tlivelist_condense_arg_t *lca =\n\t    kmem_alloc(sizeof (livelist_condense_arg_t), KM_SLEEP);\n\tbplist_create(&lca->to_keep);\n\n\t \n\tint err = dsl_process_sub_livelist(&first->dle_bpobj, &lca->to_keep, t,\n\t    &first_size);\n\tif (err == 0)\n\t\terr = dsl_process_sub_livelist(&next->dle_bpobj, &lca->to_keep,\n\t\t    t, &next_size);\n\n\tif (err == 0) {\n\t\twhile (zfs_livelist_condense_sync_pause &&\n\t\t    !(zthr_has_waiters(t) || zthr_iscancelled(t)))\n\t\t\tdelay(1);\n\n\t\tdmu_tx_t *tx = dmu_tx_create_dd(spa_get_dsl(spa)->dp_mos_dir);\n\t\tdmu_tx_mark_netfree(tx);\n\t\tdmu_tx_hold_space(tx, 1);\n\t\terr = dmu_tx_assign(tx, TXG_NOWAIT | TXG_NOTHROTTLE);\n\t\tif (err == 0) {\n\t\t\t \n\t\t\tspa->spa_to_condense.syncing = B_TRUE;\n\t\t\tlca->spa = spa;\n\t\t\tlca->first_size = first_size;\n\t\t\tlca->next_size = next_size;\n\t\t\tdsl_sync_task_nowait(spa_get_dsl(spa),\n\t\t\t    spa_livelist_condense_sync, lca, tx);\n\t\t\tdmu_tx_commit(tx);\n\t\t\treturn;\n\t\t}\n\t}\n\t \n\tASSERT(err != 0);\n\tbplist_clear(&lca->to_keep);\n\tbplist_destroy(&lca->to_keep);\n\tkmem_free(lca, sizeof (livelist_condense_arg_t));\n\tdmu_buf_rele(spa->spa_to_condense.ds->ds_dbuf, spa);\n\tspa->spa_to_condense.ds = NULL;\n\tif (err == EINTR)\n\t\tzfs_livelist_condense_zthr_cancel++;\n}\n\n \nstatic boolean_t\nspa_livelist_condense_cb_check(void *arg, zthr_t *z)\n{\n\t(void) z;\n\tspa_t *spa = arg;\n\tif ((spa->spa_to_condense.ds != NULL) &&\n\t    (spa->spa_to_condense.syncing == B_FALSE) &&\n\t    (spa->spa_to_condense.cancelled == B_FALSE)) {\n\t\treturn (B_TRUE);\n\t}\n\treturn (B_FALSE);\n}\n\nstatic void\nspa_start_livelist_condensing_thread(spa_t *spa)\n{\n\tspa->spa_to_condense.ds = NULL;\n\tspa->spa_to_condense.first = NULL;\n\tspa->spa_to_condense.next = NULL;\n\tspa->spa_to_condense.syncing = B_FALSE;\n\tspa->spa_to_condense.cancelled = B_FALSE;\n\n\tASSERT3P(spa->spa_livelist_condense_zthr, ==, NULL);\n\tspa->spa_livelist_condense_zthr =\n\t    zthr_create(\"z_livelist_condense\",\n\t    spa_livelist_condense_cb_check,\n\t    spa_livelist_condense_cb, spa, minclsyspri);\n}\n\nstatic void\nspa_spawn_aux_threads(spa_t *spa)\n{\n\tASSERT(spa_writeable(spa));\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tspa_start_indirect_condensing_thread(spa);\n\tspa_start_livelist_destroy_thread(spa);\n\tspa_start_livelist_condensing_thread(spa);\n\n\tASSERT3P(spa->spa_checkpoint_discard_zthr, ==, NULL);\n\tspa->spa_checkpoint_discard_zthr =\n\t    zthr_create(\"z_checkpoint_discard\",\n\t    spa_checkpoint_discard_thread_check,\n\t    spa_checkpoint_discard_thread, spa, minclsyspri);\n}\n\n \nstatic void\nspa_try_repair(spa_t *spa, nvlist_t *config)\n{\n\tuint_t extracted;\n\tuint64_t *glist;\n\tuint_t i, gcount;\n\tnvlist_t *nvl;\n\tvdev_t **vd;\n\tboolean_t attempt_reopen;\n\n\tif (nvlist_lookup_nvlist(config, ZPOOL_CONFIG_SPLIT, &nvl) != 0)\n\t\treturn;\n\n\t \n\tif (nvlist_lookup_uint64_array(nvl, ZPOOL_CONFIG_SPLIT_LIST,\n\t    &glist, &gcount) != 0)\n\t\treturn;\n\n\tvd = kmem_zalloc(gcount * sizeof (vdev_t *), KM_SLEEP);\n\n\t \n\tattempt_reopen = B_TRUE;\n\tfor (i = 0; i < gcount; i++) {\n\t\tif (glist[i] == 0)\t \n\t\t\tcontinue;\n\n\t\tvd[i] = spa_lookup_by_guid(spa, glist[i], B_FALSE);\n\t\tif (vd[i] == NULL) {\n\t\t\t \n\t\t\tattempt_reopen = B_FALSE;\n\t\t} else {\n\t\t\t \n\t\t\tvd[i]->vdev_offline = B_FALSE;\n\t\t}\n\t}\n\n\tif (attempt_reopen) {\n\t\tvdev_reopen(spa->spa_root_vdev);\n\n\t\t \n\t\tfor (extracted = 0, i = 0; i < gcount; i++) {\n\t\t\tif (vd[i] != NULL &&\n\t\t\t    vd[i]->vdev_stat.vs_aux != VDEV_AUX_SPLIT_POOL)\n\t\t\t\tbreak;\n\t\t\t++extracted;\n\t\t}\n\t}\n\n\t \n\tif (!attempt_reopen || gcount == extracted) {\n\t\tfor (i = 0; i < gcount; i++)\n\t\t\tif (vd[i] != NULL)\n\t\t\t\tvdev_split(vd[i]);\n\t\tvdev_reopen(spa->spa_root_vdev);\n\t}\n\n\tkmem_free(vd, gcount * sizeof (vdev_t *));\n}\n\nstatic int\nspa_load(spa_t *spa, spa_load_state_t state, spa_import_type_t type)\n{\n\tconst char *ereport = FM_EREPORT_ZFS_POOL;\n\tint error;\n\n\tspa->spa_load_state = state;\n\t(void) spa_import_progress_set_state(spa_guid(spa),\n\t    spa_load_state(spa));\n\n\tgethrestime(&spa->spa_loaded_ts);\n\terror = spa_load_impl(spa, type, &ereport);\n\n\t \n\tspa_evicting_os_wait(spa);\n\tspa->spa_minref = zfs_refcount_count(&spa->spa_refcount);\n\tif (error) {\n\t\tif (error != EEXIST) {\n\t\t\tspa->spa_loaded_ts.tv_sec = 0;\n\t\t\tspa->spa_loaded_ts.tv_nsec = 0;\n\t\t}\n\t\tif (error != EBADF) {\n\t\t\t(void) zfs_ereport_post(ereport, spa,\n\t\t\t    NULL, NULL, NULL, 0);\n\t\t}\n\t}\n\tspa->spa_load_state = error ? SPA_LOAD_ERROR : SPA_LOAD_NONE;\n\tspa->spa_ena = 0;\n\n\t(void) spa_import_progress_set_state(spa_guid(spa),\n\t    spa_load_state(spa));\n\n\treturn (error);\n}\n\n#ifdef ZFS_DEBUG\n \nstatic uint64_t\nvdev_count_verify_zaps(vdev_t *vd)\n{\n\tspa_t *spa = vd->vdev_spa;\n\tuint64_t total = 0;\n\n\tif (spa_feature_is_active(vd->vdev_spa, SPA_FEATURE_AVZ_V2) &&\n\t    vd->vdev_root_zap != 0) {\n\t\ttotal++;\n\t\tASSERT0(zap_lookup_int(spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps, vd->vdev_root_zap));\n\t}\n\tif (vd->vdev_top_zap != 0) {\n\t\ttotal++;\n\t\tASSERT0(zap_lookup_int(spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps, vd->vdev_top_zap));\n\t}\n\tif (vd->vdev_leaf_zap != 0) {\n\t\ttotal++;\n\t\tASSERT0(zap_lookup_int(spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps, vd->vdev_leaf_zap));\n\t}\n\n\tfor (uint64_t i = 0; i < vd->vdev_children; i++) {\n\t\ttotal += vdev_count_verify_zaps(vd->vdev_child[i]);\n\t}\n\n\treturn (total);\n}\n#else\n#define\tvdev_count_verify_zaps(vd) ((void) sizeof (vd), 0)\n#endif\n\n \nstatic boolean_t\nspa_activity_check_required(spa_t *spa, uberblock_t *ub, nvlist_t *label,\n    nvlist_t *config)\n{\n\tuint64_t state = 0;\n\tuint64_t hostid = 0;\n\tuint64_t tryconfig_txg = 0;\n\tuint64_t tryconfig_timestamp = 0;\n\tuint16_t tryconfig_mmp_seq = 0;\n\tnvlist_t *nvinfo;\n\n\tif (nvlist_exists(config, ZPOOL_CONFIG_LOAD_INFO)) {\n\t\tnvinfo = fnvlist_lookup_nvlist(config, ZPOOL_CONFIG_LOAD_INFO);\n\t\t(void) nvlist_lookup_uint64(nvinfo, ZPOOL_CONFIG_MMP_TXG,\n\t\t    &tryconfig_txg);\n\t\t(void) nvlist_lookup_uint64(config, ZPOOL_CONFIG_TIMESTAMP,\n\t\t    &tryconfig_timestamp);\n\t\t(void) nvlist_lookup_uint16(nvinfo, ZPOOL_CONFIG_MMP_SEQ,\n\t\t    &tryconfig_mmp_seq);\n\t}\n\n\t(void) nvlist_lookup_uint64(config, ZPOOL_CONFIG_POOL_STATE, &state);\n\n\t \n\tif (spa->spa_import_flags & ZFS_IMPORT_SKIP_MMP)\n\t\treturn (B_FALSE);\n\n\t \n\tif (ub->ub_mmp_magic == MMP_MAGIC && ub->ub_mmp_delay == 0)\n\t\treturn (B_FALSE);\n\n\t \n\tif (tryconfig_txg && tryconfig_txg == ub->ub_txg &&\n\t    tryconfig_timestamp && tryconfig_timestamp == ub->ub_timestamp &&\n\t    tryconfig_mmp_seq && tryconfig_mmp_seq ==\n\t    (MMP_SEQ_VALID(ub) ? MMP_SEQ(ub) : 0))\n\t\treturn (B_FALSE);\n\n\t \n\tif (nvlist_exists(label, ZPOOL_CONFIG_HOSTID))\n\t\thostid = fnvlist_lookup_uint64(label, ZPOOL_CONFIG_HOSTID);\n\n\tif (hostid == spa_get_hostid(spa))\n\t\treturn (B_FALSE);\n\n\t \n\tif (state != POOL_STATE_ACTIVE)\n\t\treturn (B_FALSE);\n\n\treturn (B_TRUE);\n}\n\n \nstatic uint64_t\nspa_activity_check_duration(spa_t *spa, uberblock_t *ub)\n{\n\tuint64_t import_intervals = MAX(zfs_multihost_import_intervals, 1);\n\tuint64_t multihost_interval = MSEC2NSEC(\n\t    MMP_INTERVAL_OK(zfs_multihost_interval));\n\tuint64_t import_delay = MAX(NANOSEC, import_intervals *\n\t    multihost_interval);\n\n\t \n\n\tASSERT(MMP_IMPORT_SAFETY_FACTOR >= 100);\n\n\tif (MMP_INTERVAL_VALID(ub) && MMP_FAIL_INT_VALID(ub) &&\n\t    MMP_FAIL_INT(ub) > 0) {\n\n\t\t \n\t\timport_delay = MMP_FAIL_INT(ub) * MSEC2NSEC(MMP_INTERVAL(ub)) *\n\t\t    MMP_IMPORT_SAFETY_FACTOR / 100;\n\n\t\tzfs_dbgmsg(\"fail_intvals>0 import_delay=%llu ub_mmp \"\n\t\t    \"mmp_fails=%llu ub_mmp mmp_interval=%llu \"\n\t\t    \"import_intervals=%llu\", (u_longlong_t)import_delay,\n\t\t    (u_longlong_t)MMP_FAIL_INT(ub),\n\t\t    (u_longlong_t)MMP_INTERVAL(ub),\n\t\t    (u_longlong_t)import_intervals);\n\n\t} else if (MMP_INTERVAL_VALID(ub) && MMP_FAIL_INT_VALID(ub) &&\n\t    MMP_FAIL_INT(ub) == 0) {\n\n\t\t \n\t\timport_delay = MAX(import_delay, (MSEC2NSEC(MMP_INTERVAL(ub)) +\n\t\t    ub->ub_mmp_delay) * import_intervals);\n\n\t\tzfs_dbgmsg(\"fail_intvals=0 import_delay=%llu ub_mmp \"\n\t\t    \"mmp_interval=%llu ub_mmp_delay=%llu \"\n\t\t    \"import_intervals=%llu\", (u_longlong_t)import_delay,\n\t\t    (u_longlong_t)MMP_INTERVAL(ub),\n\t\t    (u_longlong_t)ub->ub_mmp_delay,\n\t\t    (u_longlong_t)import_intervals);\n\n\t} else if (MMP_VALID(ub)) {\n\t\t \n\n\t\timport_delay = MAX(import_delay, (multihost_interval +\n\t\t    ub->ub_mmp_delay) * import_intervals);\n\n\t\tzfs_dbgmsg(\"import_delay=%llu ub_mmp_delay=%llu \"\n\t\t    \"import_intervals=%llu leaves=%u\",\n\t\t    (u_longlong_t)import_delay,\n\t\t    (u_longlong_t)ub->ub_mmp_delay,\n\t\t    (u_longlong_t)import_intervals,\n\t\t    vdev_count_leaves(spa));\n\t} else {\n\t\t \n\t\tzfs_dbgmsg(\"pool last imported on non-MMP aware \"\n\t\t    \"host using import_delay=%llu multihost_interval=%llu \"\n\t\t    \"import_intervals=%llu\", (u_longlong_t)import_delay,\n\t\t    (u_longlong_t)multihost_interval,\n\t\t    (u_longlong_t)import_intervals);\n\t}\n\n\treturn (import_delay);\n}\n\n \nstatic int\nspa_activity_check(spa_t *spa, uberblock_t *ub, nvlist_t *config)\n{\n\tuint64_t txg = ub->ub_txg;\n\tuint64_t timestamp = ub->ub_timestamp;\n\tuint64_t mmp_config = ub->ub_mmp_config;\n\tuint16_t mmp_seq = MMP_SEQ_VALID(ub) ? MMP_SEQ(ub) : 0;\n\tuint64_t import_delay;\n\thrtime_t import_expire;\n\tnvlist_t *mmp_label = NULL;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tkcondvar_t cv;\n\tkmutex_t mtx;\n\tint error = 0;\n\n\tcv_init(&cv, NULL, CV_DEFAULT, NULL);\n\tmutex_init(&mtx, NULL, MUTEX_DEFAULT, NULL);\n\tmutex_enter(&mtx);\n\n\t \n\tif (nvlist_exists(config, ZPOOL_CONFIG_LOAD_INFO)) {\n\t\tnvlist_t *nvinfo = fnvlist_lookup_nvlist(config,\n\t\t    ZPOOL_CONFIG_LOAD_INFO);\n\n\t\tif (nvlist_exists(nvinfo, ZPOOL_CONFIG_MMP_TXG) &&\n\t\t    fnvlist_lookup_uint64(nvinfo, ZPOOL_CONFIG_MMP_TXG) == 0) {\n\t\t\tvdev_uberblock_load(rvd, ub, &mmp_label);\n\t\t\terror = SET_ERROR(EREMOTEIO);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\timport_delay = spa_activity_check_duration(spa, ub);\n\n\t \n\timport_delay += import_delay * random_in_range(250) / 1000;\n\n\timport_expire = gethrtime() + import_delay;\n\n\twhile (gethrtime() < import_expire) {\n\t\t(void) spa_import_progress_set_mmp_check(spa_guid(spa),\n\t\t    NSEC2SEC(import_expire - gethrtime()));\n\n\t\tvdev_uberblock_load(rvd, ub, &mmp_label);\n\n\t\tif (txg != ub->ub_txg || timestamp != ub->ub_timestamp ||\n\t\t    mmp_seq != (MMP_SEQ_VALID(ub) ? MMP_SEQ(ub) : 0)) {\n\t\t\tzfs_dbgmsg(\"multihost activity detected \"\n\t\t\t    \"txg %llu ub_txg  %llu \"\n\t\t\t    \"timestamp %llu ub_timestamp  %llu \"\n\t\t\t    \"mmp_config %#llx ub_mmp_config %#llx\",\n\t\t\t    (u_longlong_t)txg, (u_longlong_t)ub->ub_txg,\n\t\t\t    (u_longlong_t)timestamp,\n\t\t\t    (u_longlong_t)ub->ub_timestamp,\n\t\t\t    (u_longlong_t)mmp_config,\n\t\t\t    (u_longlong_t)ub->ub_mmp_config);\n\n\t\t\terror = SET_ERROR(EREMOTEIO);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (mmp_label) {\n\t\t\tnvlist_free(mmp_label);\n\t\t\tmmp_label = NULL;\n\t\t}\n\n\t\terror = cv_timedwait_sig(&cv, &mtx, ddi_get_lbolt() + hz);\n\t\tif (error != -1) {\n\t\t\terror = SET_ERROR(EINTR);\n\t\t\tbreak;\n\t\t}\n\t\terror = 0;\n\t}\n\nout:\n\tmutex_exit(&mtx);\n\tmutex_destroy(&mtx);\n\tcv_destroy(&cv);\n\n\t \n\tif (error == EREMOTEIO) {\n\t\tconst char *hostname = \"<unknown>\";\n\t\tuint64_t hostid = 0;\n\n\t\tif (mmp_label) {\n\t\t\tif (nvlist_exists(mmp_label, ZPOOL_CONFIG_HOSTNAME)) {\n\t\t\t\thostname = fnvlist_lookup_string(mmp_label,\n\t\t\t\t    ZPOOL_CONFIG_HOSTNAME);\n\t\t\t\tfnvlist_add_string(spa->spa_load_info,\n\t\t\t\t    ZPOOL_CONFIG_MMP_HOSTNAME, hostname);\n\t\t\t}\n\n\t\t\tif (nvlist_exists(mmp_label, ZPOOL_CONFIG_HOSTID)) {\n\t\t\t\thostid = fnvlist_lookup_uint64(mmp_label,\n\t\t\t\t    ZPOOL_CONFIG_HOSTID);\n\t\t\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t\t\t    ZPOOL_CONFIG_MMP_HOSTID, hostid);\n\t\t\t}\n\t\t}\n\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_MMP_STATE, MMP_STATE_ACTIVE);\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_MMP_TXG, 0);\n\n\t\terror = spa_vdev_err(rvd, VDEV_AUX_ACTIVE, EREMOTEIO);\n\t}\n\n\tif (mmp_label)\n\t\tnvlist_free(mmp_label);\n\n\treturn (error);\n}\n\nstatic int\nspa_verify_host(spa_t *spa, nvlist_t *mos_config)\n{\n\tuint64_t hostid;\n\tconst char *hostname;\n\tuint64_t myhostid = 0;\n\n\tif (!spa_is_root(spa) && nvlist_lookup_uint64(mos_config,\n\t    ZPOOL_CONFIG_HOSTID, &hostid) == 0) {\n\t\thostname = fnvlist_lookup_string(mos_config,\n\t\t    ZPOOL_CONFIG_HOSTNAME);\n\n\t\tmyhostid = zone_get_hostid(NULL);\n\n\t\tif (hostid != 0 && myhostid != 0 && hostid != myhostid) {\n\t\t\tcmn_err(CE_WARN, \"pool '%s' could not be \"\n\t\t\t    \"loaded as it was last accessed by \"\n\t\t\t    \"another system (host: %s hostid: 0x%llx). \"\n\t\t\t    \"See: https:\n\t\t\t    \"ZFS-8000-EY\",\n\t\t\t    spa_name(spa), hostname, (u_longlong_t)hostid);\n\t\t\tspa_load_failed(spa, \"hostid verification failed: pool \"\n\t\t\t    \"last accessed by host: %s (hostid: 0x%llx)\",\n\t\t\t    hostname, (u_longlong_t)hostid);\n\t\t\treturn (SET_ERROR(EBADF));\n\t\t}\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_parse_config(spa_t *spa, spa_import_type_t type)\n{\n\tint error = 0;\n\tnvlist_t *nvtree, *nvl, *config = spa->spa_config;\n\tint parse;\n\tvdev_t *rvd;\n\tuint64_t pool_guid;\n\tconst char *comment;\n\tconst char *compatibility;\n\n\t/*\n\t * Versioning wasn't explicitly added to the label until later, so if\n\t * it's not present treat it as the initial version.\n\t */\n\tif (nvlist_lookup_uint64(config, ZPOOL_CONFIG_VERSION,\n\t    &spa->spa_ubsync.ub_version) != 0)\n\t\tspa->spa_ubsync.ub_version = SPA_VERSION_INITIAL;\n\n\tif (nvlist_lookup_uint64(config, ZPOOL_CONFIG_POOL_GUID, &pool_guid)) {\n\t\tspa_load_failed(spa, \"invalid config provided: '%s' missing\",\n\t\t    ZPOOL_CONFIG_POOL_GUID);\n\t\treturn (SET_ERROR(EINVAL));\n\t}\n\n\t/*\n\t * If we are doing an import, ensure that the pool is not already\n\t * imported by checking if its pool guid already exists in the\n\t * spa namespace.\n\t *\n\t * The only case that we allow an already imported pool to be\n\t * imported again, is when the pool is checkpointed and we want to\n\t * look at its checkpointed state from userland tools like zdb.\n\t */\n#ifdef _KERNEL\n\tif ((spa->spa_load_state == SPA_LOAD_IMPORT ||\n\t    spa->spa_load_state == SPA_LOAD_TRYIMPORT) &&\n\t    spa_guid_exists(pool_guid, 0)) {\n#else\n\tif ((spa->spa_load_state == SPA_LOAD_IMPORT ||\n\t    spa->spa_load_state == SPA_LOAD_TRYIMPORT) &&\n\t    spa_guid_exists(pool_guid, 0) &&\n\t    !spa_importing_readonly_checkpoint(spa)) {\n#endif\n\t\tspa_load_failed(spa, \"a pool with guid %llu is already open\",\n\t\t    (u_longlong_t)pool_guid);\n\t\treturn (SET_ERROR(EEXIST));\n\t}\n\n\tspa->spa_config_guid = pool_guid;\n\n\tnvlist_free(spa->spa_load_info);\n\tspa->spa_load_info = fnvlist_alloc();\n\n\tASSERT(spa->spa_comment == NULL);\n\tif (nvlist_lookup_string(config, ZPOOL_CONFIG_COMMENT, &comment) == 0)\n\t\tspa->spa_comment = spa_strdup(comment);\n\n\tASSERT(spa->spa_compatibility == NULL);\n\tif (nvlist_lookup_string(config, ZPOOL_CONFIG_COMPATIBILITY,\n\t    &compatibility) == 0)\n\t\tspa->spa_compatibility = spa_strdup(compatibility);\n\n\t(void) nvlist_lookup_uint64(config, ZPOOL_CONFIG_POOL_TXG,\n\t    &spa->spa_config_txg);\n\n\tif (nvlist_lookup_nvlist(config, ZPOOL_CONFIG_SPLIT, &nvl) == 0)\n\t\tspa->spa_config_splitting = fnvlist_dup(nvl);\n\n\tif (nvlist_lookup_nvlist(config, ZPOOL_CONFIG_VDEV_TREE, &nvtree)) {\n\t\tspa_load_failed(spa, \"invalid config provided: '%s' missing\",\n\t\t    ZPOOL_CONFIG_VDEV_TREE);\n\t\treturn (SET_ERROR(EINVAL));\n\t}\n\n\t/*\n\t * Create \"The Godfather\" zio to hold all async IOs\n\t */\n\tspa->spa_async_zio_root = kmem_alloc(max_ncpus * sizeof (void *),\n\t    KM_SLEEP);\n\tfor (int i = 0; i < max_ncpus; i++) {\n\t\tspa->spa_async_zio_root[i] = zio_root(spa, NULL, NULL,\n\t\t    ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE |\n\t\t    ZIO_FLAG_GODFATHER);\n\t}\n\n\t/*\n\t * Parse the configuration into a vdev tree.  We explicitly set the\n\t * value that will be returned by spa_version() since parsing the\n\t * configuration requires knowing the version number.\n\t */\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\tparse = (type == SPA_IMPORT_EXISTING ?\n\t    VDEV_ALLOC_LOAD : VDEV_ALLOC_SPLIT);\n\terror = spa_config_parse(spa, &rvd, nvtree, NULL, 0, parse);\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"unable to parse config [error=%d]\",\n\t\t    error);\n\t\treturn (error);\n\t}\n\n\tASSERT(spa->spa_root_vdev == rvd);\n\tASSERT3U(spa->spa_min_ashift, >=, SPA_MINBLOCKSHIFT);\n\tASSERT3U(spa->spa_max_ashift, <=, SPA_MAXBLOCKSHIFT);\n\n\tif (type != SPA_IMPORT_ASSEMBLE) {\n\t\tASSERT(spa_guid(spa) == pool_guid);\n\t}\n\n\treturn (0);\n}\n\n/*\n * Recursively open all vdevs in the vdev tree. This function is called twice:\n * first with the untrusted config, then with the trusted config.\n */\nstatic int\nspa_ld_open_vdevs(spa_t *spa)\n{\n\tint error = 0;\n\n\t/*\n\t * spa_missing_tvds_allowed defines how many top-level vdevs can be\n\t * missing/unopenable for the root vdev to be still considered openable.\n\t */\n\tif (spa->spa_trust_config) {\n\t\tspa->spa_missing_tvds_allowed = zfs_max_missing_tvds;\n\t} else if (spa->spa_config_source == SPA_CONFIG_SRC_CACHEFILE) {\n\t\tspa->spa_missing_tvds_allowed = zfs_max_missing_tvds_cachefile;\n\t} else if (spa->spa_config_source == SPA_CONFIG_SRC_SCAN) {\n\t\tspa->spa_missing_tvds_allowed = zfs_max_missing_tvds_scan;\n\t} else {\n\t\tspa->spa_missing_tvds_allowed = 0;\n\t}\n\n\tspa->spa_missing_tvds_allowed =\n\t    MAX(zfs_max_missing_tvds, spa->spa_missing_tvds_allowed);\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\terror = vdev_open(spa->spa_root_vdev);\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\tif (spa->spa_missing_tvds != 0) {\n\t\tspa_load_note(spa, \"vdev tree has %lld missing top-level \"\n\t\t    \"vdevs.\", (u_longlong_t)spa->spa_missing_tvds);\n\t\tif (spa->spa_trust_config && (spa->spa_mode & SPA_MODE_WRITE)) {\n\t\t\t/*\n\t\t\t * Although theoretically we could allow users to open\n\t\t\t * incomplete pools in RW mode, we'd need to add a lot\n\t\t\t * of extra logic (e.g. adjust pool space to account\n\t\t\t * for missing vdevs).\n\t\t\t * This limitation also prevents users from accidentally\n\t\t\t * opening the pool in RW mode during data recovery and\n\t\t\t * damaging it further.\n\t\t\t */\n\t\t\tspa_load_note(spa, \"pools with missing top-level \"\n\t\t\t    \"vdevs can only be opened in read-only mode.\");\n\t\t\terror = SET_ERROR(ENXIO);\n\t\t} else {\n\t\t\tspa_load_note(spa, \"current settings allow for maximum \"\n\t\t\t    \"%lld missing top-level vdevs at this stage.\",\n\t\t\t    (u_longlong_t)spa->spa_missing_tvds_allowed);\n\t\t}\n\t}\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"unable to open vdev tree [error=%d]\",\n\t\t    error);\n\t}\n\tif (spa->spa_missing_tvds != 0 || error != 0)\n\t\tvdev_dbgmsg_print_tree(spa->spa_root_vdev, 2);\n\n\treturn (error);\n}\n\n/*\n * We need to validate the vdev labels against the configuration that\n * we have in hand. This function is called twice: first with an untrusted\n * config, then with a trusted config. The validation is more strict when the\n * config is trusted.\n */\nstatic int\nspa_ld_validate_vdevs(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\terror = vdev_validate(rvd);\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"vdev_validate failed [error=%d]\", error);\n\t\treturn (error);\n\t}\n\n\tif (rvd->vdev_state <= VDEV_STATE_CANT_OPEN) {\n\t\tspa_load_failed(spa, \"cannot open vdev tree after invalidating \"\n\t\t    \"some vdevs\");\n\t\tvdev_dbgmsg_print_tree(rvd, 2);\n\t\treturn (SET_ERROR(ENXIO));\n\t}\n\n\treturn (0);\n}\n\nstatic void\nspa_ld_select_uberblock_done(spa_t *spa, uberblock_t *ub)\n{\n\tspa->spa_state = POOL_STATE_ACTIVE;\n\tspa->spa_ubsync = spa->spa_uberblock;\n\tspa->spa_verify_min_txg = spa->spa_extreme_rewind ?\n\t    TXG_INITIAL - 1 : spa_last_synced_txg(spa) - TXG_DEFER_SIZE - 1;\n\tspa->spa_first_txg = spa->spa_last_ubsync_txg ?\n\t    spa->spa_last_ubsync_txg : spa_last_synced_txg(spa) + 1;\n\tspa->spa_claim_max_txg = spa->spa_first_txg;\n\tspa->spa_prev_software_version = ub->ub_software_version;\n}\n\nstatic int\nspa_ld_select_uberblock(spa_t *spa, spa_import_type_t type)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tnvlist_t *label;\n\tuberblock_t *ub = &spa->spa_uberblock;\n\tboolean_t activity_check = B_FALSE;\n\n\t/*\n\t * If we are opening the checkpointed state of the pool by\n\t * rewinding to it, at this point we will have written the\n\t * checkpointed uberblock to the vdev labels, so searching\n\t * the labels will find the right uberblock.  However, if\n\t * we are opening the checkpointed state read-only, we have\n\t * not modified the labels. Therefore, we must ignore the\n\t * labels and continue using the spa_uberblock that was set\n\t * by spa_ld_checkpoint_rewind.\n\t *\n\t * Note that it would be fine to ignore the labels when\n\t * rewinding (opening writeable) as well. However, if we\n\t * crash just after writing the labels, we will end up\n\t * searching the labels. Doing so in the common case means\n\t * that this code path gets exercised normally, rather than\n\t * just in the edge case.\n\t */\n\tif (ub->ub_checkpoint_txg != 0 &&\n\t    spa_importing_readonly_checkpoint(spa)) {\n\t\tspa_ld_select_uberblock_done(spa, ub);\n\t\treturn (0);\n\t}\n\n\t/*\n\t * Find the best uberblock.\n\t */\n\tvdev_uberblock_load(rvd, ub, &label);\n\n\t/*\n\t * If we weren't able to find a single valid uberblock, return failure.\n\t */\n\tif (ub->ub_txg == 0) {\n\t\tnvlist_free(label);\n\t\tspa_load_failed(spa, \"no valid uberblock found\");\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, ENXIO));\n\t}\n\n\tif (spa->spa_load_max_txg != UINT64_MAX) {\n\t\t(void) spa_import_progress_set_max_txg(spa_guid(spa),\n\t\t    (u_longlong_t)spa->spa_load_max_txg);\n\t}\n\tspa_load_note(spa, \"using uberblock with txg=%llu\",\n\t    (u_longlong_t)ub->ub_txg);\n\n\n\t/*\n\t * For pools which have the multihost property on determine if the\n\t * pool is truly inactive and can be safely imported.  Prevent\n\t * hosts which don't have a hostid set from importing the pool.\n\t */\n\tactivity_check = spa_activity_check_required(spa, ub, label,\n\t    spa->spa_config);\n\tif (activity_check) {\n\t\tif (ub->ub_mmp_magic == MMP_MAGIC && ub->ub_mmp_delay &&\n\t\t    spa_get_hostid(spa) == 0) {\n\t\t\tnvlist_free(label);\n\t\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t\t    ZPOOL_CONFIG_MMP_STATE, MMP_STATE_NO_HOSTID);\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_ACTIVE, EREMOTEIO));\n\t\t}\n\n\t\tint error = spa_activity_check(spa, ub, spa->spa_config);\n\t\tif (error) {\n\t\t\tnvlist_free(label);\n\t\t\treturn (error);\n\t\t}\n\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_MMP_STATE, MMP_STATE_INACTIVE);\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_MMP_TXG, ub->ub_txg);\n\t\tfnvlist_add_uint16(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_MMP_SEQ,\n\t\t    (MMP_SEQ_VALID(ub) ? MMP_SEQ(ub) : 0));\n\t}\n\n\t/*\n\t * If the pool has an unsupported version we can't open it.\n\t */\n\tif (!SPA_VERSION_IS_SUPPORTED(ub->ub_version)) {\n\t\tnvlist_free(label);\n\t\tspa_load_failed(spa, \"version %llu is not supported\",\n\t\t    (u_longlong_t)ub->ub_version);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_VERSION_NEWER, ENOTSUP));\n\t}\n\n\tif (ub->ub_version >= SPA_VERSION_FEATURES) {\n\t\tnvlist_t *features;\n\n\t\t/*\n\t\t * If we weren't able to find what's necessary for reading the\n\t\t * MOS in the label, return failure.\n\t\t */\n\t\tif (label == NULL) {\n\t\t\tspa_load_failed(spa, \"label config unavailable\");\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA,\n\t\t\t    ENXIO));\n\t\t}\n\n\t\tif (nvlist_lookup_nvlist(label, ZPOOL_CONFIG_FEATURES_FOR_READ,\n\t\t    &features) != 0) {\n\t\t\tnvlist_free(label);\n\t\t\tspa_load_failed(spa, \"invalid label: '%s' missing\",\n\t\t\t    ZPOOL_CONFIG_FEATURES_FOR_READ);\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA,\n\t\t\t    ENXIO));\n\t\t}\n\n\t\t/*\n\t\t * Update our in-core representation with the definitive values\n\t\t * from the label.\n\t\t */\n\t\tnvlist_free(spa->spa_label_features);\n\t\tspa->spa_label_features = fnvlist_dup(features);\n\t}\n\n\tnvlist_free(label);\n\n\t/*\n\t * Look through entries in the label nvlist's features_for_read. If\n\t * there is a feature listed there which we don't understand then we\n\t * cannot open a pool.\n\t */\n\tif (ub->ub_version >= SPA_VERSION_FEATURES) {\n\t\tnvlist_t *unsup_feat;\n\n\t\tunsup_feat = fnvlist_alloc();\n\n\t\tfor (nvpair_t *nvp = nvlist_next_nvpair(spa->spa_label_features,\n\t\t    NULL); nvp != NULL;\n\t\t    nvp = nvlist_next_nvpair(spa->spa_label_features, nvp)) {\n\t\t\tif (!zfeature_is_supported(nvpair_name(nvp))) {\n\t\t\t\tfnvlist_add_string(unsup_feat,\n\t\t\t\t    nvpair_name(nvp), \"\");\n\t\t\t}\n\t\t}\n\n\t\tif (!nvlist_empty(unsup_feat)) {\n\t\t\tfnvlist_add_nvlist(spa->spa_load_info,\n\t\t\t    ZPOOL_CONFIG_UNSUP_FEAT, unsup_feat);\n\t\t\tnvlist_free(unsup_feat);\n\t\t\tspa_load_failed(spa, \"some features are unsupported\");\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_UNSUP_FEAT,\n\t\t\t    ENOTSUP));\n\t\t}\n\n\t\tnvlist_free(unsup_feat);\n\t}\n\n\tif (type != SPA_IMPORT_ASSEMBLE && spa->spa_config_splitting) {\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_try_repair(spa, spa->spa_config);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tnvlist_free(spa->spa_config_splitting);\n\t\tspa->spa_config_splitting = NULL;\n\t}\n\n\t/*\n\t * Initialize internal SPA structures.\n\t */\n\tspa_ld_select_uberblock_done(spa, ub);\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_open_rootbp(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\terror = dsl_pool_init(spa, spa->spa_first_txg, &spa->spa_dsl_pool);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"unable to open rootbp in dsl_pool_init \"\n\t\t    \"[error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\tspa->spa_meta_objset = spa->spa_dsl_pool->dp_meta_objset;\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_trusted_config(spa_t *spa, spa_import_type_t type,\n    boolean_t reloading)\n{\n\tvdev_t *mrvd, *rvd = spa->spa_root_vdev;\n\tnvlist_t *nv, *mos_config, *policy;\n\tint error = 0, copy_error;\n\tuint64_t healthy_tvds, healthy_tvds_mos;\n\tuint64_t mos_config_txg;\n\n\tif (spa_dir_prop(spa, DMU_POOL_CONFIG, &spa->spa_config_object, B_TRUE)\n\t    != 0)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\t/*\n\t * If we're assembling a pool from a split, the config provided is\n\t * already trusted so there is nothing to do.\n\t */\n\tif (type == SPA_IMPORT_ASSEMBLE)\n\t\treturn (0);\n\n\thealthy_tvds = spa_healthy_core_tvds(spa);\n\n\tif (load_nvlist(spa, spa->spa_config_object, &mos_config)\n\t    != 0) {\n\t\tspa_load_failed(spa, \"unable to retrieve MOS config\");\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\t/*\n\t * If we are doing an open, pool owner wasn't verified yet, thus do\n\t * the verification here.\n\t */\n\tif (spa->spa_load_state == SPA_LOAD_OPEN) {\n\t\terror = spa_verify_host(spa, mos_config);\n\t\tif (error != 0) {\n\t\t\tnvlist_free(mos_config);\n\t\t\treturn (error);\n\t\t}\n\t}\n\n\tnv = fnvlist_lookup_nvlist(mos_config, ZPOOL_CONFIG_VDEV_TREE);\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\n\t/*\n\t * Build a new vdev tree from the trusted config\n\t */\n\terror = spa_config_parse(spa, &mrvd, nv, NULL, 0, VDEV_ALLOC_LOAD);\n\tif (error != 0) {\n\t\tnvlist_free(mos_config);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tspa_load_failed(spa, \"spa_config_parse failed [error=%d]\",\n\t\t    error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, error));\n\t}\n\n\t/*\n\t * Vdev paths in the MOS may be obsolete. If the untrusted config was\n\t * obtained by scanning /dev/dsk, then it will have the right vdev\n\t * paths. We update the trusted MOS config with this information.\n\t * We first try to copy the paths with vdev_copy_path_strict, which\n\t * succeeds only when both configs have exactly the same vdev tree.\n\t * If that fails, we fall back to a more flexible method that has a\n\t * best effort policy.\n\t */\n\tcopy_error = vdev_copy_path_strict(rvd, mrvd);\n\tif (copy_error != 0 || spa_load_print_vdev_tree) {\n\t\tspa_load_note(spa, \"provided vdev tree:\");\n\t\tvdev_dbgmsg_print_tree(rvd, 2);\n\t\tspa_load_note(spa, \"MOS vdev tree:\");\n\t\tvdev_dbgmsg_print_tree(mrvd, 2);\n\t}\n\tif (copy_error != 0) {\n\t\tspa_load_note(spa, \"vdev_copy_path_strict failed, falling \"\n\t\t    \"back to vdev_copy_path_relaxed\");\n\t\tvdev_copy_path_relaxed(rvd, mrvd);\n\t}\n\n\tvdev_close(rvd);\n\tvdev_free(rvd);\n\tspa->spa_root_vdev = mrvd;\n\trvd = mrvd;\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\t/*\n\t * If 'zpool import' used a cached config, then the on-disk hostid and\n\t * hostname may be different to the cached config in ways that should\n\t * prevent import.  Userspace can't discover this without a scan, but\n\t * we know, so we add these values to LOAD_INFO so the caller can know\n\t * the difference.\n\t *\n\t * Note that we have to do this before the config is regenerated,\n\t * because the new config will have the hostid and hostname for this\n\t * host, in readiness for import.\n\t */\n\tif (nvlist_exists(mos_config, ZPOOL_CONFIG_HOSTID))\n\t\tfnvlist_add_uint64(spa->spa_load_info, ZPOOL_CONFIG_HOSTID,\n\t\t    fnvlist_lookup_uint64(mos_config, ZPOOL_CONFIG_HOSTID));\n\tif (nvlist_exists(mos_config, ZPOOL_CONFIG_HOSTNAME))\n\t\tfnvlist_add_string(spa->spa_load_info, ZPOOL_CONFIG_HOSTNAME,\n\t\t    fnvlist_lookup_string(mos_config, ZPOOL_CONFIG_HOSTNAME));\n\n\t/*\n\t * We will use spa_config if we decide to reload the spa or if spa_load\n\t * fails and we rewind. We must thus regenerate the config using the\n\t * MOS information with the updated paths. ZPOOL_LOAD_POLICY is used to\n\t * pass settings on how to load the pool and is not stored in the MOS.\n\t * We copy it over to our new, trusted config.\n\t */\n\tmos_config_txg = fnvlist_lookup_uint64(mos_config,\n\t    ZPOOL_CONFIG_POOL_TXG);\n\tnvlist_free(mos_config);\n\tmos_config = spa_config_generate(spa, NULL, mos_config_txg, B_FALSE);\n\tif (nvlist_lookup_nvlist(spa->spa_config, ZPOOL_LOAD_POLICY,\n\t    &policy) == 0)\n\t\tfnvlist_add_nvlist(mos_config, ZPOOL_LOAD_POLICY, policy);\n\tspa_config_set(spa, mos_config);\n\tspa->spa_config_source = SPA_CONFIG_SRC_MOS;\n\n\t/*\n\t * Now that we got the config from the MOS, we should be more strict\n\t * in checking blkptrs and can make assumptions about the consistency\n\t * of the vdev tree. spa_trust_config must be set to true before opening\n\t * vdevs in order for them to be writeable.\n\t */\n\tspa->spa_trust_config = B_TRUE;\n\n\t/*\n\t * Open and validate the new vdev tree\n\t */\n\terror = spa_ld_open_vdevs(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\terror = spa_ld_validate_vdevs(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\tif (copy_error != 0 || spa_load_print_vdev_tree) {\n\t\tspa_load_note(spa, \"final vdev tree:\");\n\t\tvdev_dbgmsg_print_tree(rvd, 2);\n\t}\n\n\tif (spa->spa_load_state != SPA_LOAD_TRYIMPORT &&\n\t    !spa->spa_extreme_rewind && zfs_max_missing_tvds == 0) {\n\t\t/*\n\t\t * Sanity check to make sure that we are indeed loading the\n\t\t * latest uberblock. If we missed SPA_SYNC_MIN_VDEVS tvds\n\t\t * in the config provided and they happened to be the only ones\n\t\t * to have the latest uberblock, we could involuntarily perform\n\t\t * an extreme rewind.\n\t\t */\n\t\thealthy_tvds_mos = spa_healthy_core_tvds(spa);\n\t\tif (healthy_tvds_mos - healthy_tvds >=\n\t\t    SPA_SYNC_MIN_VDEVS) {\n\t\t\tspa_load_note(spa, \"config provided misses too many \"\n\t\t\t    \"top-level vdevs compared to MOS (%lld vs %lld). \",\n\t\t\t    (u_longlong_t)healthy_tvds,\n\t\t\t    (u_longlong_t)healthy_tvds_mos);\n\t\t\tspa_load_note(spa, \"vdev tree:\");\n\t\t\tvdev_dbgmsg_print_tree(rvd, 2);\n\t\t\tif (reloading) {\n\t\t\t\tspa_load_failed(spa, \"config was already \"\n\t\t\t\t    \"provided from MOS. Aborting.\");\n\t\t\t\treturn (spa_vdev_err(rvd,\n\t\t\t\t    VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t\t}\n\t\t\tspa_load_note(spa, \"spa must be reloaded using MOS \"\n\t\t\t    \"config\");\n\t\t\treturn (SET_ERROR(EAGAIN));\n\t\t}\n\t}\n\n\terror = spa_check_for_missing_logs(spa);\n\tif (error != 0)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_BAD_GUID_SUM, ENXIO));\n\n\tif (rvd->vdev_guid_sum != spa->spa_uberblock.ub_guid_sum) {\n\t\tspa_load_failed(spa, \"uberblock guid sum doesn't match MOS \"\n\t\t    \"guid sum (%llu != %llu)\",\n\t\t    (u_longlong_t)spa->spa_uberblock.ub_guid_sum,\n\t\t    (u_longlong_t)rvd->vdev_guid_sum);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_BAD_GUID_SUM,\n\t\t    ENXIO));\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_open_indirect_vdev_metadata(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t/*\n\t * Everything that we read before spa_remove_init() must be stored\n\t * on concreted vdevs.  Therefore we do this as early as possible.\n\t */\n\terror = spa_remove_init(spa);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"spa_remove_init failed [error=%d]\",\n\t\t    error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\t/*\n\t * Retrieve information needed to condense indirect vdev mappings.\n\t */\n\terror = spa_condense_init(spa);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"spa_condense_init failed [error=%d]\",\n\t\t    error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, error));\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_check_features(spa_t *spa, boolean_t *missing_feat_writep)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\tif (spa_version(spa) >= SPA_VERSION_FEATURES) {\n\t\tboolean_t missing_feat_read = B_FALSE;\n\t\tnvlist_t *unsup_feat, *enabled_feat;\n\n\t\tif (spa_dir_prop(spa, DMU_POOL_FEATURES_FOR_READ,\n\t\t    &spa->spa_feat_for_read_obj, B_TRUE) != 0) {\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t}\n\n\t\tif (spa_dir_prop(spa, DMU_POOL_FEATURES_FOR_WRITE,\n\t\t    &spa->spa_feat_for_write_obj, B_TRUE) != 0) {\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t}\n\n\t\tif (spa_dir_prop(spa, DMU_POOL_FEATURE_DESCRIPTIONS,\n\t\t    &spa->spa_feat_desc_obj, B_TRUE) != 0) {\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t}\n\n\t\tenabled_feat = fnvlist_alloc();\n\t\tunsup_feat = fnvlist_alloc();\n\n\t\tif (!spa_features_check(spa, B_FALSE,\n\t\t    unsup_feat, enabled_feat))\n\t\t\tmissing_feat_read = B_TRUE;\n\n\t\tif (spa_writeable(spa) ||\n\t\t    spa->spa_load_state == SPA_LOAD_TRYIMPORT) {\n\t\t\tif (!spa_features_check(spa, B_TRUE,\n\t\t\t    unsup_feat, enabled_feat)) {\n\t\t\t\t*missing_feat_writep = B_TRUE;\n\t\t\t}\n\t\t}\n\n\t\tfnvlist_add_nvlist(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_ENABLED_FEAT, enabled_feat);\n\n\t\tif (!nvlist_empty(unsup_feat)) {\n\t\t\tfnvlist_add_nvlist(spa->spa_load_info,\n\t\t\t    ZPOOL_CONFIG_UNSUP_FEAT, unsup_feat);\n\t\t}\n\n\t\tfnvlist_free(enabled_feat);\n\t\tfnvlist_free(unsup_feat);\n\n\t\tif (!missing_feat_read) {\n\t\t\tfnvlist_add_boolean(spa->spa_load_info,\n\t\t\t    ZPOOL_CONFIG_CAN_RDONLY);\n\t\t}\n\n\t\t/*\n\t\t * If the state is SPA_LOAD_TRYIMPORT, our objective is\n\t\t * twofold: to determine whether the pool is available for\n\t\t * import in read-write mode and (if it is not) whether the\n\t\t * pool is available for import in read-only mode. If the pool\n\t\t * is available for import in read-write mode, it is displayed\n\t\t * as available in userland; if it is not available for import\n\t\t * in read-only mode, it is displayed as unavailable in\n\t\t * userland. If the pool is available for import in read-only\n\t\t * mode but not read-write mode, it is displayed as unavailable\n\t\t * in userland with a special note that the pool is actually\n\t\t * available for open in read-only mode.\n\t\t *\n\t\t * As a result, if the state is SPA_LOAD_TRYIMPORT and we are\n\t\t * missing a feature for write, we must first determine whether\n\t\t * the pool can be opened read-only before returning to\n\t\t * userland in order to know whether to display the\n\t\t * abovementioned note.\n\t\t */\n\t\tif (missing_feat_read || (*missing_feat_writep &&\n\t\t    spa_writeable(spa))) {\n\t\t\tspa_load_failed(spa, \"pool uses unsupported features\");\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_UNSUP_FEAT,\n\t\t\t    ENOTSUP));\n\t\t}\n\n\t\t/*\n\t\t * Load refcounts for ZFS features from disk into an in-memory\n\t\t * cache during SPA initialization.\n\t\t */\n\t\tfor (spa_feature_t i = 0; i < SPA_FEATURES; i++) {\n\t\t\tuint64_t refcount;\n\n\t\t\terror = feature_get_refcount_from_disk(spa,\n\t\t\t    &spa_feature_table[i], &refcount);\n\t\t\tif (error == 0) {\n\t\t\t\tspa->spa_feat_refcount_cache[i] = refcount;\n\t\t\t} else if (error == ENOTSUP) {\n\t\t\t\tspa->spa_feat_refcount_cache[i] =\n\t\t\t\t    SPA_FEATURE_DISABLED;\n\t\t\t} else {\n\t\t\t\tspa_load_failed(spa, \"error getting refcount \"\n\t\t\t\t    \"for feature %s [error=%d]\",\n\t\t\t\t    spa_feature_table[i].fi_guid, error);\n\t\t\t\treturn (spa_vdev_err(rvd,\n\t\t\t\t    VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t\t}\n\t\t}\n\t}\n\n\tif (spa_feature_is_active(spa, SPA_FEATURE_ENABLED_TXG)) {\n\t\tif (spa_dir_prop(spa, DMU_POOL_FEATURE_ENABLED_TXG,\n\t\t    &spa->spa_feat_enabled_txg_obj, B_TRUE) != 0)\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\t/*\n\t * Encryption was added before bookmark_v2, even though bookmark_v2\n\t * is now a dependency. If this pool has encryption enabled without\n\t * bookmark_v2, trigger an errata message.\n\t */\n\tif (spa_feature_is_enabled(spa, SPA_FEATURE_ENCRYPTION) &&\n\t    !spa_feature_is_enabled(spa, SPA_FEATURE_BOOKMARK_V2)) {\n\t\tspa->spa_errata = ZPOOL_ERRATA_ZOL_8308_ENCRYPTION;\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_load_special_directories(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\tspa->spa_is_initializing = B_TRUE;\n\terror = dsl_pool_open(spa->spa_dsl_pool);\n\tspa->spa_is_initializing = B_FALSE;\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"dsl_pool_open failed [error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_get_props(spa_t *spa)\n{\n\tint error = 0;\n\tuint64_t obj;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t/* Grab the checksum salt from the MOS. */\n\terror = zap_lookup(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,\n\t    DMU_POOL_CHECKSUM_SALT, 1,\n\t    sizeof (spa->spa_cksum_salt.zcs_bytes),\n\t    spa->spa_cksum_salt.zcs_bytes);\n\tif (error == ENOENT) {\n\t\t/* Generate a new salt for subsequent use */\n\t\t(void) random_get_pseudo_bytes(spa->spa_cksum_salt.zcs_bytes,\n\t\t    sizeof (spa->spa_cksum_salt.zcs_bytes));\n\t} else if (error != 0) {\n\t\tspa_load_failed(spa, \"unable to retrieve checksum salt from \"\n\t\t    \"MOS [error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\tif (spa_dir_prop(spa, DMU_POOL_SYNC_BPOBJ, &obj, B_TRUE) != 0)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\terror = bpobj_open(&spa->spa_deferred_bpobj, spa->spa_meta_objset, obj);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"error opening deferred-frees bpobj \"\n\t\t    \"[error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\t/*\n\t * Load the bit that tells us to use the new accounting function\n\t * (raid-z deflation).  If we have an older pool, this will not\n\t * be present.\n\t */\n\terror = spa_dir_prop(spa, DMU_POOL_DEFLATE, &spa->spa_deflate, B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\terror = spa_dir_prop(spa, DMU_POOL_CREATION_VERSION,\n\t    &spa->spa_creation_version, B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\t/*\n\t * Load the persistent error log.  If we have an older pool, this will\n\t * not be present.\n\t */\n\terror = spa_dir_prop(spa, DMU_POOL_ERRLOG_LAST, &spa->spa_errlog_last,\n\t    B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\terror = spa_dir_prop(spa, DMU_POOL_ERRLOG_SCRUB,\n\t    &spa->spa_errlog_scrub, B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\t/*\n\t * Load the livelist deletion field. If a livelist is queued for\n\t * deletion, indicate that in the spa\n\t */\n\terror = spa_dir_prop(spa, DMU_POOL_DELETED_CLONES,\n\t    &spa->spa_livelists_to_delete, B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\t/*\n\t * Load the history object.  If we have an older pool, this\n\t * will not be present.\n\t */\n\terror = spa_dir_prop(spa, DMU_POOL_HISTORY, &spa->spa_history, B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\t/*\n\t * Load the per-vdev ZAP map. If we have an older pool, this will not\n\t * be present; in this case, defer its creation to a later time to\n\t * avoid dirtying the MOS this early / out of sync context. See\n\t * spa_sync_config_object.\n\t */\n\n\t/* The sentinel is only available in the MOS config. */\n\tnvlist_t *mos_config;\n\tif (load_nvlist(spa, spa->spa_config_object, &mos_config) != 0) {\n\t\tspa_load_failed(spa, \"unable to retrieve MOS config\");\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\terror = spa_dir_prop(spa, DMU_POOL_VDEV_ZAP_MAP,\n\t    &spa->spa_all_vdev_zaps, B_FALSE);\n\n\tif (error == ENOENT) {\n\t\tVERIFY(!nvlist_exists(mos_config,\n\t\t    ZPOOL_CONFIG_HAS_PER_VDEV_ZAPS));\n\t\tspa->spa_avz_action = AVZ_ACTION_INITIALIZE;\n\t\tASSERT0(vdev_count_verify_zaps(spa->spa_root_vdev));\n\t} else if (error != 0) {\n\t\tnvlist_free(mos_config);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t} else if (!nvlist_exists(mos_config, ZPOOL_CONFIG_HAS_PER_VDEV_ZAPS)) {\n\t\t/*\n\t\t * An older version of ZFS overwrote the sentinel value, so\n\t\t * we have orphaned per-vdev ZAPs in the MOS. Defer their\n\t\t * destruction to later; see spa_sync_config_object.\n\t\t */\n\t\tspa->spa_avz_action = AVZ_ACTION_DESTROY;\n\t\t/*\n\t\t * We're assuming that no vdevs have had their ZAPs created\n\t\t * before this. Better be sure of it.\n\t\t */\n\t\tASSERT0(vdev_count_verify_zaps(spa->spa_root_vdev));\n\t}\n\tnvlist_free(mos_config);\n\n\tspa->spa_delegation = zpool_prop_default_numeric(ZPOOL_PROP_DELEGATION);\n\n\terror = spa_dir_prop(spa, DMU_POOL_PROPS, &spa->spa_pool_props_object,\n\t    B_FALSE);\n\tif (error && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\n\tif (error == 0) {\n\t\tuint64_t autoreplace = 0;\n\n\t\tspa_prop_find(spa, ZPOOL_PROP_BOOTFS, &spa->spa_bootfs);\n\t\tspa_prop_find(spa, ZPOOL_PROP_AUTOREPLACE, &autoreplace);\n\t\tspa_prop_find(spa, ZPOOL_PROP_DELEGATION, &spa->spa_delegation);\n\t\tspa_prop_find(spa, ZPOOL_PROP_FAILUREMODE, &spa->spa_failmode);\n\t\tspa_prop_find(spa, ZPOOL_PROP_AUTOEXPAND, &spa->spa_autoexpand);\n\t\tspa_prop_find(spa, ZPOOL_PROP_MULTIHOST, &spa->spa_multihost);\n\t\tspa_prop_find(spa, ZPOOL_PROP_AUTOTRIM, &spa->spa_autotrim);\n\t\tspa->spa_autoreplace = (autoreplace != 0);\n\t}\n\n\t \n\tif (spa->spa_missing_tvds > 0 &&\n\t    spa->spa_failmode != ZIO_FAILURE_MODE_CONTINUE &&\n\t    spa->spa_load_state != SPA_LOAD_TRYIMPORT) {\n\t\tspa_load_note(spa, \"forcing failmode to 'continue' \"\n\t\t    \"as some top level vdevs are missing\");\n\t\tspa->spa_failmode = ZIO_FAILURE_MODE_CONTINUE;\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_open_aux_vdevs(spa_t *spa, spa_import_type_t type)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t \n\n\t \n\terror = spa_dir_prop(spa, DMU_POOL_SPARES, &spa->spa_spares.sav_object,\n\t    B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\tif (error == 0 && type != SPA_IMPORT_ASSEMBLE) {\n\t\tASSERT(spa_version(spa) >= SPA_VERSION_SPARES);\n\t\tif (load_nvlist(spa, spa->spa_spares.sav_object,\n\t\t    &spa->spa_spares.sav_config) != 0) {\n\t\t\tspa_load_failed(spa, \"error loading spares nvlist\");\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t}\n\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_load_spares(spa);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t} else if (error == 0) {\n\t\tspa->spa_spares.sav_sync = B_TRUE;\n\t}\n\n\t \n\terror = spa_dir_prop(spa, DMU_POOL_L2CACHE,\n\t    &spa->spa_l2cache.sav_object, B_FALSE);\n\tif (error != 0 && error != ENOENT)\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\tif (error == 0 && type != SPA_IMPORT_ASSEMBLE) {\n\t\tASSERT(spa_version(spa) >= SPA_VERSION_L2CACHE);\n\t\tif (load_nvlist(spa, spa->spa_l2cache.sav_object,\n\t\t    &spa->spa_l2cache.sav_config) != 0) {\n\t\t\tspa_load_failed(spa, \"error loading l2cache nvlist\");\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t\t}\n\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_load_l2cache(spa);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t} else if (error == 0) {\n\t\tspa->spa_l2cache.sav_sync = B_TRUE;\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_load_vdev_metadata(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t \n\tif (spa_multihost(spa) && spa_get_hostid(spa) == 0 &&\n\t    (spa->spa_import_flags & ZFS_IMPORT_SKIP_MMP) == 0) {\n\t\tfnvlist_add_uint64(spa->spa_load_info,\n\t\t    ZPOOL_CONFIG_MMP_STATE, MMP_STATE_NO_HOSTID);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_ACTIVE, EREMOTEIO));\n\t}\n\n\t \n\tif (spa->spa_autoreplace && spa->spa_load_state != SPA_LOAD_TRYIMPORT) {\n\t\tspa_check_removed(spa->spa_root_vdev);\n\t\t \n\t\tif (spa->spa_load_state != SPA_LOAD_IMPORT) {\n\t\t\tspa_aux_check_removed(&spa->spa_spares);\n\t\t\tspa_aux_check_removed(&spa->spa_l2cache);\n\t\t}\n\t}\n\n\t \n\terror = vdev_load(rvd);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"vdev_load failed [error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, error));\n\t}\n\n\terror = spa_ld_log_spacemaps(spa);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"spa_ld_log_spacemaps failed [error=%d]\",\n\t\t    error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, error));\n\t}\n\n\t \n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\tvdev_dtl_reassess(rvd, 0, 0, B_FALSE, B_FALSE);\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_load_dedup_tables(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\terror = ddt_load(spa);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"ddt_load failed [error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_load_brt(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\terror = brt_load(spa);\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"brt_load failed [error=%d]\", error);\n\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA, EIO));\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_verify_logs(spa_t *spa, spa_import_type_t type, const char **ereport)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\tif (type != SPA_IMPORT_ASSEMBLE && spa_writeable(spa)) {\n\t\tboolean_t missing = spa_check_logs(spa);\n\t\tif (missing) {\n\t\t\tif (spa->spa_missing_tvds != 0) {\n\t\t\t\tspa_load_note(spa, \"spa_check_logs failed \"\n\t\t\t\t    \"so dropping the logs\");\n\t\t\t} else {\n\t\t\t\t*ereport = FM_EREPORT_ZFS_LOG_REPLAY;\n\t\t\t\tspa_load_failed(spa, \"spa_check_logs failed\");\n\t\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_BAD_LOG,\n\t\t\t\t    ENXIO));\n\t\t\t}\n\t\t}\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_verify_pool_data(spa_t *spa)\n{\n\tint error = 0;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t \n\tif (spa->spa_load_state != SPA_LOAD_TRYIMPORT) {\n\t\terror = spa_load_verify(spa);\n\t\tif (error != 0) {\n\t\t\tspa_load_failed(spa, \"spa_load_verify failed \"\n\t\t\t    \"[error=%d]\", error);\n\t\t\treturn (spa_vdev_err(rvd, VDEV_AUX_CORRUPT_DATA,\n\t\t\t    error));\n\t\t}\n\t}\n\n\treturn (0);\n}\n\nstatic void\nspa_ld_claim_log_blocks(spa_t *spa)\n{\n\tdmu_tx_t *tx;\n\tdsl_pool_t *dp = spa_get_dsl(spa);\n\n\t \n\tspa->spa_claiming = B_TRUE;\n\n\ttx = dmu_tx_create_assigned(dp, spa_first_txg(spa));\n\t(void) dmu_objset_find_dp(dp, dp->dp_root_dir_obj,\n\t    zil_claim, tx, DS_FIND_CHILDREN);\n\tdmu_tx_commit(tx);\n\n\tspa->spa_claiming = B_FALSE;\n\n\tspa_set_log_state(spa, SPA_LOG_GOOD);\n}\n\nstatic void\nspa_ld_check_for_config_update(spa_t *spa, uint64_t config_cache_txg,\n    boolean_t update_config_cache)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tint need_update = B_FALSE;\n\n\t \n\tif (update_config_cache || config_cache_txg != spa->spa_config_txg ||\n\t    spa->spa_load_state == SPA_LOAD_IMPORT ||\n\t    spa->spa_load_state == SPA_LOAD_RECOVER ||\n\t    (spa->spa_import_flags & ZFS_IMPORT_VERBATIM))\n\t\tneed_update = B_TRUE;\n\n\tfor (int c = 0; c < rvd->vdev_children; c++)\n\t\tif (rvd->vdev_child[c]->vdev_ms_array == 0)\n\t\t\tneed_update = B_TRUE;\n\n\t \n\tif (need_update)\n\t\tspa_async_request(spa, SPA_ASYNC_CONFIG_UPDATE);\n}\n\nstatic void\nspa_ld_prepare_for_reload(spa_t *spa)\n{\n\tspa_mode_t mode = spa->spa_mode;\n\tint async_suspended = spa->spa_async_suspended;\n\n\tspa_unload(spa);\n\tspa_deactivate(spa);\n\tspa_activate(spa, mode);\n\n\t \n\tspa->spa_async_suspended = async_suspended;\n}\n\nstatic int\nspa_ld_read_checkpoint_txg(spa_t *spa)\n{\n\tuberblock_t checkpoint;\n\tint error = 0;\n\n\tASSERT0(spa->spa_checkpoint_txg);\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\terror = zap_lookup(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,\n\t    DMU_POOL_ZPOOL_CHECKPOINT, sizeof (uint64_t),\n\t    sizeof (uberblock_t) / sizeof (uint64_t), &checkpoint);\n\n\tif (error == ENOENT)\n\t\treturn (0);\n\n\tif (error != 0)\n\t\treturn (error);\n\n\tASSERT3U(checkpoint.ub_txg, !=, 0);\n\tASSERT3U(checkpoint.ub_checkpoint_txg, !=, 0);\n\tASSERT3U(checkpoint.ub_timestamp, !=, 0);\n\tspa->spa_checkpoint_txg = checkpoint.ub_txg;\n\tspa->spa_checkpoint_info.sci_timestamp = checkpoint.ub_timestamp;\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_mos_init(spa_t *spa, spa_import_type_t type)\n{\n\tint error = 0;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tASSERT(spa->spa_config_source != SPA_CONFIG_SRC_NONE);\n\n\t \n\tif (type != SPA_IMPORT_ASSEMBLE)\n\t\tspa->spa_trust_config = B_FALSE;\n\n\t \n\terror = spa_ld_parse_config(spa, type);\n\tif (error != 0)\n\t\treturn (error);\n\n\tspa_import_progress_add(spa);\n\n\t \n\terror = spa_ld_open_vdevs(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\tif (type != SPA_IMPORT_ASSEMBLE) {\n\t\terror = spa_ld_validate_vdevs(spa);\n\t\tif (error != 0)\n\t\t\treturn (error);\n\t}\n\n\t \n\terror = spa_ld_select_uberblock(spa, type);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_open_rootbp(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_checkpoint_rewind(spa_t *spa)\n{\n\tuberblock_t checkpoint;\n\tint error = 0;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tASSERT(spa->spa_import_flags & ZFS_IMPORT_CHECKPOINT);\n\n\terror = zap_lookup(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,\n\t    DMU_POOL_ZPOOL_CHECKPOINT, sizeof (uint64_t),\n\t    sizeof (uberblock_t) / sizeof (uint64_t), &checkpoint);\n\n\tif (error != 0) {\n\t\tspa_load_failed(spa, \"unable to retrieve checkpointed \"\n\t\t    \"uberblock from the MOS config [error=%d]\", error);\n\n\t\tif (error == ENOENT)\n\t\t\terror = ZFS_ERR_NO_CHECKPOINT;\n\n\t\treturn (error);\n\t}\n\n\tASSERT3U(checkpoint.ub_txg, <, spa->spa_uberblock.ub_txg);\n\tASSERT3U(checkpoint.ub_txg, ==, checkpoint.ub_checkpoint_txg);\n\n\t \n\tcheckpoint.ub_txg = spa->spa_uberblock.ub_txg + 1;\n\tcheckpoint.ub_timestamp = gethrestime_sec();\n\n\t \n\tspa->spa_uberblock = checkpoint;\n\n\t \n\tif (spa_writeable(spa)) {\n\t\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tvdev_t *svd[SPA_SYNC_MIN_VDEVS] = { NULL };\n\t\tint svdcount = 0;\n\t\tint children = rvd->vdev_children;\n\t\tint c0 = random_in_range(children);\n\n\t\tfor (int c = 0; c < children; c++) {\n\t\t\tvdev_t *vd = rvd->vdev_child[(c0 + c) % children];\n\n\t\t\t \n\t\t\tif (c > 0 && svd[0] == vd)\n\t\t\t\tbreak;\n\n\t\t\tif (vd->vdev_ms_array == 0 || vd->vdev_islog ||\n\t\t\t    !vdev_is_concrete(vd))\n\t\t\t\tcontinue;\n\n\t\t\tsvd[svdcount++] = vd;\n\t\t\tif (svdcount == SPA_SYNC_MIN_VDEVS)\n\t\t\t\tbreak;\n\t\t}\n\t\terror = vdev_config_sync(svd, svdcount, spa->spa_first_txg);\n\t\tif (error == 0)\n\t\t\tspa->spa_last_synced_guid = rvd->vdev_guid;\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\t\tif (error != 0) {\n\t\t\tspa_load_failed(spa, \"failed to write checkpointed \"\n\t\t\t    \"uberblock to the vdev labels [error=%d]\", error);\n\t\t\treturn (error);\n\t\t}\n\t}\n\n\treturn (0);\n}\n\nstatic int\nspa_ld_mos_with_trusted_config(spa_t *spa, spa_import_type_t type,\n    boolean_t *update_config_cache)\n{\n\tint error;\n\n\t \n\terror = spa_ld_mos_init(spa, type);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_trusted_config(spa, type, B_FALSE);\n\tif (error == EAGAIN) {\n\t\tif (update_config_cache != NULL)\n\t\t\t*update_config_cache = B_TRUE;\n\n\t\t \n\t\tspa_ld_prepare_for_reload(spa);\n\t\tspa_load_note(spa, \"RELOADING\");\n\t\terror = spa_ld_mos_init(spa, type);\n\t\tif (error != 0)\n\t\t\treturn (error);\n\n\t\terror = spa_ld_trusted_config(spa, type, B_TRUE);\n\t\tif (error != 0)\n\t\t\treturn (error);\n\n\t} else if (error != 0) {\n\t\treturn (error);\n\t}\n\n\treturn (0);\n}\n\n \nstatic int\nspa_load_impl(spa_t *spa, spa_import_type_t type, const char **ereport)\n{\n\tint error = 0;\n\tboolean_t missing_feat_write = B_FALSE;\n\tboolean_t checkpoint_rewind =\n\t    (spa->spa_import_flags & ZFS_IMPORT_CHECKPOINT);\n\tboolean_t update_config_cache = B_FALSE;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tASSERT(spa->spa_config_source != SPA_CONFIG_SRC_NONE);\n\n\tspa_load_note(spa, \"LOADING\");\n\n\terror = spa_ld_mos_with_trusted_config(spa, type, &update_config_cache);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\tif (checkpoint_rewind) {\n\t\t \n\t\tupdate_config_cache = B_TRUE;\n\n\t\t \n\t\terror = spa_ld_checkpoint_rewind(spa);\n\t\tif (error != 0)\n\t\t\treturn (error);\n\n\t\t \n\t\tspa_ld_prepare_for_reload(spa);\n\t\tspa_load_note(spa, \"LOADING checkpointed uberblock\");\n\t\terror = spa_ld_mos_with_trusted_config(spa, type, NULL);\n\t\tif (error != 0)\n\t\t\treturn (error);\n\t}\n\n\t \n\terror = spa_ld_read_checkpoint_txg(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_open_indirect_vdev_metadata(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_check_features(spa, &missing_feat_write);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_load_special_directories(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_get_props(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_open_aux_vdevs(spa, type);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_load_vdev_metadata(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\terror = spa_ld_load_dedup_tables(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\terror = spa_ld_load_brt(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\terror = spa_ld_verify_logs(spa, type, ereport);\n\tif (error != 0)\n\t\treturn (error);\n\n\tif (missing_feat_write) {\n\t\tASSERT(spa->spa_load_state == SPA_LOAD_TRYIMPORT);\n\n\t\t \n\t\treturn (spa_vdev_err(spa->spa_root_vdev, VDEV_AUX_UNSUP_FEAT,\n\t\t    ENOTSUP));\n\t}\n\n\t \n\terror = spa_ld_verify_pool_data(spa);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\tspa_update_dspace(spa);\n\n\t \n\tif (spa_writeable(spa) && (spa->spa_load_state == SPA_LOAD_RECOVER ||\n\t    spa->spa_load_max_txg == UINT64_MAX)) {\n\t\tuint64_t config_cache_txg = spa->spa_config_txg;\n\n\t\tASSERT(spa->spa_load_state != SPA_LOAD_TRYIMPORT);\n\n\t\t \n\t\tif (checkpoint_rewind) {\n\t\t\tspa_history_log_internal(spa, \"checkpoint rewind\",\n\t\t\t    NULL, \"rewound state to txg=%llu\",\n\t\t\t    (u_longlong_t)spa->spa_uberblock.ub_checkpoint_txg);\n\t\t}\n\n\t\t \n\t\tspa_ld_claim_log_blocks(spa);\n\n\t\t \n\t\tspa->spa_sync_on = B_TRUE;\n\t\ttxg_sync_start(spa->spa_dsl_pool);\n\t\tmmp_thread_start(spa);\n\n\t\t \n\t\ttxg_wait_synced(spa->spa_dsl_pool, spa->spa_claim_max_txg);\n\n\t\t \n\t\tspa_ld_check_for_config_update(spa, config_cache_txg,\n\t\t    update_config_cache);\n\n\t\t \n\t\tif (vdev_rebuild_active(spa->spa_root_vdev)) {\n\t\t\tvdev_rebuild_restart(spa);\n\t\t} else if (!dsl_scan_resilvering(spa->spa_dsl_pool) &&\n\t\t    vdev_resilver_needed(spa->spa_root_vdev, NULL, NULL)) {\n\t\t\tspa_async_request(spa, SPA_ASYNC_RESILVER);\n\t\t}\n\n\t\t \n\t\tspa_history_log_version(spa, \"open\", NULL);\n\n\t\tspa_restart_removal(spa);\n\t\tspa_spawn_aux_threads(spa);\n\n\t\t \n\t\t(void) dmu_objset_find(spa_name(spa),\n\t\t    dsl_destroy_inconsistent, NULL, DS_FIND_CHILDREN);\n\n\t\t \n\t\tdsl_pool_clean_tmp_userrefs(spa->spa_dsl_pool);\n\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tvdev_initialize_restart(spa->spa_root_vdev);\n\t\tvdev_trim_restart(spa->spa_root_vdev);\n\t\tvdev_autotrim_restart(spa);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t}\n\n\tspa_import_progress_remove(spa_guid(spa));\n\tspa_async_request(spa, SPA_ASYNC_L2CACHE_REBUILD);\n\n\tspa_load_note(spa, \"LOADED\");\n\n\treturn (0);\n}\n\nstatic int\nspa_load_retry(spa_t *spa, spa_load_state_t state)\n{\n\tspa_mode_t mode = spa->spa_mode;\n\n\tspa_unload(spa);\n\tspa_deactivate(spa);\n\n\tspa->spa_load_max_txg = spa->spa_uberblock.ub_txg - 1;\n\n\tspa_activate(spa, mode);\n\tspa_async_suspend(spa);\n\n\tspa_load_note(spa, \"spa_load_retry: rewind, max txg: %llu\",\n\t    (u_longlong_t)spa->spa_load_max_txg);\n\n\treturn (spa_load(spa, state, SPA_IMPORT_EXISTING));\n}\n\n \nstatic int\nspa_load_best(spa_t *spa, spa_load_state_t state, uint64_t max_request,\n    int rewind_flags)\n{\n\tnvlist_t *loadinfo = NULL;\n\tnvlist_t *config = NULL;\n\tint load_error, rewind_error;\n\tuint64_t safe_rewind_txg;\n\tuint64_t min_txg;\n\n\tif (spa->spa_load_txg && state == SPA_LOAD_RECOVER) {\n\t\tspa->spa_load_max_txg = spa->spa_load_txg;\n\t\tspa_set_log_state(spa, SPA_LOG_CLEAR);\n\t} else {\n\t\tspa->spa_load_max_txg = max_request;\n\t\tif (max_request != UINT64_MAX)\n\t\t\tspa->spa_extreme_rewind = B_TRUE;\n\t}\n\n\tload_error = rewind_error = spa_load(spa, state, SPA_IMPORT_EXISTING);\n\tif (load_error == 0)\n\t\treturn (0);\n\tif (load_error == ZFS_ERR_NO_CHECKPOINT) {\n\t\t \n\t\tASSERT(spa->spa_import_flags & ZFS_IMPORT_CHECKPOINT);\n\t\tspa_import_progress_remove(spa_guid(spa));\n\t\treturn (load_error);\n\t}\n\n\tif (spa->spa_root_vdev != NULL)\n\t\tconfig = spa_config_generate(spa, NULL, -1ULL, B_TRUE);\n\n\tspa->spa_last_ubsync_txg = spa->spa_uberblock.ub_txg;\n\tspa->spa_last_ubsync_txg_ts = spa->spa_uberblock.ub_timestamp;\n\n\tif (rewind_flags & ZPOOL_NEVER_REWIND) {\n\t\tnvlist_free(config);\n\t\tspa_import_progress_remove(spa_guid(spa));\n\t\treturn (load_error);\n\t}\n\n\tif (state == SPA_LOAD_RECOVER) {\n\t\t \n\t\tspa_set_log_state(spa, SPA_LOG_CLEAR);\n\t} else {\n\t\t \n\t\tloadinfo = spa->spa_load_info;\n\t\tspa->spa_load_info = fnvlist_alloc();\n\t}\n\n\tspa->spa_load_max_txg = spa->spa_last_ubsync_txg;\n\tsafe_rewind_txg = spa->spa_last_ubsync_txg - TXG_DEFER_SIZE;\n\tmin_txg = (rewind_flags & ZPOOL_EXTREME_REWIND) ?\n\t    TXG_INITIAL : safe_rewind_txg;\n\n\t \n\twhile (rewind_error && spa->spa_uberblock.ub_txg >= min_txg &&\n\t    spa->spa_uberblock.ub_txg <= spa->spa_load_max_txg) {\n\t\tif (spa->spa_load_max_txg < safe_rewind_txg)\n\t\t\tspa->spa_extreme_rewind = B_TRUE;\n\t\trewind_error = spa_load_retry(spa, state);\n\t}\n\n\tspa->spa_extreme_rewind = B_FALSE;\n\tspa->spa_load_max_txg = UINT64_MAX;\n\n\tif (config && (rewind_error || state != SPA_LOAD_RECOVER))\n\t\tspa_config_set(spa, config);\n\telse\n\t\tnvlist_free(config);\n\n\tif (state == SPA_LOAD_RECOVER) {\n\t\tASSERT3P(loadinfo, ==, NULL);\n\t\tspa_import_progress_remove(spa_guid(spa));\n\t\treturn (rewind_error);\n\t} else {\n\t\t \n\t\tfnvlist_add_nvlist(loadinfo, ZPOOL_CONFIG_REWIND_INFO,\n\t\t    spa->spa_load_info);\n\n\t\t \n\t\tfnvlist_free(spa->spa_load_info);\n\t\tspa->spa_load_info = loadinfo;\n\n\t\tspa_import_progress_remove(spa_guid(spa));\n\t\treturn (load_error);\n\t}\n}\n\n \nstatic int\nspa_open_common(const char *pool, spa_t **spapp, const void *tag,\n    nvlist_t *nvpolicy, nvlist_t **config)\n{\n\tspa_t *spa;\n\tspa_load_state_t state = SPA_LOAD_OPEN;\n\tint error;\n\tint locked = B_FALSE;\n\tint firstopen = B_FALSE;\n\n\t*spapp = NULL;\n\n\t \n\tif (MUTEX_NOT_HELD(&spa_namespace_lock)) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tlocked = B_TRUE;\n\t}\n\n\tif ((spa = spa_lookup(pool)) == NULL) {\n\t\tif (locked)\n\t\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (SET_ERROR(ENOENT));\n\t}\n\n\tif (spa->spa_state == POOL_STATE_UNINITIALIZED) {\n\t\tzpool_load_policy_t policy;\n\n\t\tfirstopen = B_TRUE;\n\n\t\tzpool_get_load_policy(nvpolicy ? nvpolicy : spa->spa_config,\n\t\t    &policy);\n\t\tif (policy.zlp_rewind & ZPOOL_DO_REWIND)\n\t\t\tstate = SPA_LOAD_RECOVER;\n\n\t\tspa_activate(spa, spa_mode_global);\n\n\t\tif (state != SPA_LOAD_RECOVER)\n\t\t\tspa->spa_last_ubsync_txg = spa->spa_load_txg = 0;\n\t\tspa->spa_config_source = SPA_CONFIG_SRC_CACHEFILE;\n\n\t\tzfs_dbgmsg(\"spa_open_common: opening %s\", pool);\n\t\terror = spa_load_best(spa, state, policy.zlp_txg,\n\t\t    policy.zlp_rewind);\n\n\t\tif (error == EBADF) {\n\t\t\t \n\t\t\tspa_unload(spa);\n\t\t\tspa_deactivate(spa);\n\t\t\tspa_write_cachefile(spa, B_TRUE, B_TRUE, B_FALSE);\n\t\t\tspa_remove(spa);\n\t\t\tif (locked)\n\t\t\t\tmutex_exit(&spa_namespace_lock);\n\t\t\treturn (SET_ERROR(ENOENT));\n\t\t}\n\n\t\tif (error) {\n\t\t\t \n\t\t\tif (config != NULL && spa->spa_config) {\n\t\t\t\t*config = fnvlist_dup(spa->spa_config);\n\t\t\t\tfnvlist_add_nvlist(*config,\n\t\t\t\t    ZPOOL_CONFIG_LOAD_INFO,\n\t\t\t\t    spa->spa_load_info);\n\t\t\t}\n\t\t\tspa_unload(spa);\n\t\t\tspa_deactivate(spa);\n\t\t\tspa->spa_last_open_failed = error;\n\t\t\tif (locked)\n\t\t\t\tmutex_exit(&spa_namespace_lock);\n\t\t\t*spapp = NULL;\n\t\t\treturn (error);\n\t\t}\n\t}\n\n\tspa_open_ref(spa, tag);\n\n\tif (config != NULL)\n\t\t*config = spa_config_generate(spa, NULL, -1ULL, B_TRUE);\n\n\t \n\tif (state == SPA_LOAD_RECOVER && config != NULL) {\n\t\tfnvlist_add_nvlist(*config, ZPOOL_CONFIG_LOAD_INFO,\n\t\t    spa->spa_load_info);\n\t}\n\n\tif (locked) {\n\t\tspa->spa_last_open_failed = 0;\n\t\tspa->spa_last_ubsync_txg = 0;\n\t\tspa->spa_load_txg = 0;\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\tif (firstopen)\n\t\tzvol_create_minors_recursive(spa_name(spa));\n\n\t*spapp = spa;\n\n\treturn (0);\n}\n\nint\nspa_open_rewind(const char *name, spa_t **spapp, const void *tag,\n    nvlist_t *policy, nvlist_t **config)\n{\n\treturn (spa_open_common(name, spapp, tag, policy, config));\n}\n\nint\nspa_open(const char *name, spa_t **spapp, const void *tag)\n{\n\treturn (spa_open_common(name, spapp, tag, NULL, NULL));\n}\n\n \nspa_t *\nspa_inject_addref(char *name)\n{\n\tspa_t *spa;\n\n\tmutex_enter(&spa_namespace_lock);\n\tif ((spa = spa_lookup(name)) == NULL) {\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (NULL);\n\t}\n\tspa->spa_inject_ref++;\n\tmutex_exit(&spa_namespace_lock);\n\n\treturn (spa);\n}\n\nvoid\nspa_inject_delref(spa_t *spa)\n{\n\tmutex_enter(&spa_namespace_lock);\n\tspa->spa_inject_ref--;\n\tmutex_exit(&spa_namespace_lock);\n}\n\n \nstatic void\nspa_add_spares(spa_t *spa, nvlist_t *config)\n{\n\tnvlist_t **spares;\n\tuint_t i, nspares;\n\tnvlist_t *nvroot;\n\tuint64_t guid;\n\tvdev_stat_t *vs;\n\tuint_t vsc;\n\tuint64_t pool;\n\n\tASSERT(spa_config_held(spa, SCL_CONFIG, RW_READER));\n\n\tif (spa->spa_spares.sav_count == 0)\n\t\treturn;\n\n\tnvroot = fnvlist_lookup_nvlist(config, ZPOOL_CONFIG_VDEV_TREE);\n\tVERIFY0(nvlist_lookup_nvlist_array(spa->spa_spares.sav_config,\n\t    ZPOOL_CONFIG_SPARES, &spares, &nspares));\n\tif (nspares != 0) {\n\t\tfnvlist_add_nvlist_array(nvroot, ZPOOL_CONFIG_SPARES,\n\t\t    (const nvlist_t * const *)spares, nspares);\n\t\tVERIFY0(nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_SPARES,\n\t\t    &spares, &nspares));\n\n\t\t \n\t\tfor (i = 0; i < nspares; i++) {\n\t\t\tguid = fnvlist_lookup_uint64(spares[i],\n\t\t\t    ZPOOL_CONFIG_GUID);\n\t\t\tVERIFY0(nvlist_lookup_uint64_array(spares[i],\n\t\t\t    ZPOOL_CONFIG_VDEV_STATS, (uint64_t **)&vs, &vsc));\n\t\t\tif (spa_spare_exists(guid, &pool, NULL) &&\n\t\t\t    pool != 0ULL) {\n\t\t\t\tvs->vs_state = VDEV_STATE_CANT_OPEN;\n\t\t\t\tvs->vs_aux = VDEV_AUX_SPARED;\n\t\t\t} else {\n\t\t\t\tvs->vs_state =\n\t\t\t\t    spa->spa_spares.sav_vdevs[i]->vdev_state;\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nstatic void\nspa_add_l2cache(spa_t *spa, nvlist_t *config)\n{\n\tnvlist_t **l2cache;\n\tuint_t i, j, nl2cache;\n\tnvlist_t *nvroot;\n\tuint64_t guid;\n\tvdev_t *vd;\n\tvdev_stat_t *vs;\n\tuint_t vsc;\n\n\tASSERT(spa_config_held(spa, SCL_CONFIG, RW_READER));\n\n\tif (spa->spa_l2cache.sav_count == 0)\n\t\treturn;\n\n\tnvroot = fnvlist_lookup_nvlist(config, ZPOOL_CONFIG_VDEV_TREE);\n\tVERIFY0(nvlist_lookup_nvlist_array(spa->spa_l2cache.sav_config,\n\t    ZPOOL_CONFIG_L2CACHE, &l2cache, &nl2cache));\n\tif (nl2cache != 0) {\n\t\tfnvlist_add_nvlist_array(nvroot, ZPOOL_CONFIG_L2CACHE,\n\t\t    (const nvlist_t * const *)l2cache, nl2cache);\n\t\tVERIFY0(nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_L2CACHE,\n\t\t    &l2cache, &nl2cache));\n\n\t\t \n\n\t\tfor (i = 0; i < nl2cache; i++) {\n\t\t\tguid = fnvlist_lookup_uint64(l2cache[i],\n\t\t\t    ZPOOL_CONFIG_GUID);\n\n\t\t\tvd = NULL;\n\t\t\tfor (j = 0; j < spa->spa_l2cache.sav_count; j++) {\n\t\t\t\tif (guid ==\n\t\t\t\t    spa->spa_l2cache.sav_vdevs[j]->vdev_guid) {\n\t\t\t\t\tvd = spa->spa_l2cache.sav_vdevs[j];\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tASSERT(vd != NULL);\n\n\t\t\tVERIFY0(nvlist_lookup_uint64_array(l2cache[i],\n\t\t\t    ZPOOL_CONFIG_VDEV_STATS, (uint64_t **)&vs, &vsc));\n\t\t\tvdev_get_stats(vd, vs);\n\t\t\tvdev_config_generate_stats(vd, l2cache[i]);\n\n\t\t}\n\t}\n}\n\nstatic void\nspa_feature_stats_from_disk(spa_t *spa, nvlist_t *features)\n{\n\tzap_cursor_t zc;\n\tzap_attribute_t za;\n\n\tif (spa->spa_feat_for_read_obj != 0) {\n\t\tfor (zap_cursor_init(&zc, spa->spa_meta_objset,\n\t\t    spa->spa_feat_for_read_obj);\n\t\t    zap_cursor_retrieve(&zc, &za) == 0;\n\t\t    zap_cursor_advance(&zc)) {\n\t\t\tASSERT(za.za_integer_length == sizeof (uint64_t) &&\n\t\t\t    za.za_num_integers == 1);\n\t\t\tVERIFY0(nvlist_add_uint64(features, za.za_name,\n\t\t\t    za.za_first_integer));\n\t\t}\n\t\tzap_cursor_fini(&zc);\n\t}\n\n\tif (spa->spa_feat_for_write_obj != 0) {\n\t\tfor (zap_cursor_init(&zc, spa->spa_meta_objset,\n\t\t    spa->spa_feat_for_write_obj);\n\t\t    zap_cursor_retrieve(&zc, &za) == 0;\n\t\t    zap_cursor_advance(&zc)) {\n\t\t\tASSERT(za.za_integer_length == sizeof (uint64_t) &&\n\t\t\t    za.za_num_integers == 1);\n\t\t\tVERIFY0(nvlist_add_uint64(features, za.za_name,\n\t\t\t    za.za_first_integer));\n\t\t}\n\t\tzap_cursor_fini(&zc);\n\t}\n}\n\nstatic void\nspa_feature_stats_from_cache(spa_t *spa, nvlist_t *features)\n{\n\tint i;\n\n\tfor (i = 0; i < SPA_FEATURES; i++) {\n\t\tzfeature_info_t feature = spa_feature_table[i];\n\t\tuint64_t refcount;\n\n\t\tif (feature_get_refcount(spa, &feature, &refcount) != 0)\n\t\t\tcontinue;\n\n\t\tVERIFY0(nvlist_add_uint64(features, feature.fi_guid, refcount));\n\t}\n}\n\n \nstatic void\nspa_add_feature_stats(spa_t *spa, nvlist_t *config)\n{\n\tnvlist_t *features;\n\n\tASSERT(spa_config_held(spa, SCL_CONFIG, RW_READER));\n\n\tmutex_enter(&spa->spa_feat_stats_lock);\n\tfeatures = spa->spa_feat_stats;\n\n\tif (features != NULL) {\n\t\tspa_feature_stats_from_cache(spa, features);\n\t} else {\n\t\tVERIFY0(nvlist_alloc(&features, NV_UNIQUE_NAME, KM_SLEEP));\n\t\tspa->spa_feat_stats = features;\n\t\tspa_feature_stats_from_disk(spa, features);\n\t}\n\n\tVERIFY0(nvlist_add_nvlist(config, ZPOOL_CONFIG_FEATURE_STATS,\n\t    features));\n\n\tmutex_exit(&spa->spa_feat_stats_lock);\n}\n\nint\nspa_get_stats(const char *name, nvlist_t **config,\n    char *altroot, size_t buflen)\n{\n\tint error;\n\tspa_t *spa;\n\n\t*config = NULL;\n\terror = spa_open_common(name, &spa, FTAG, NULL, config);\n\n\tif (spa != NULL) {\n\t\t \n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\n\t\tif (*config != NULL) {\n\t\t\tuint64_t loadtimes[2];\n\n\t\t\tloadtimes[0] = spa->spa_loaded_ts.tv_sec;\n\t\t\tloadtimes[1] = spa->spa_loaded_ts.tv_nsec;\n\t\t\tfnvlist_add_uint64_array(*config,\n\t\t\t    ZPOOL_CONFIG_LOADED_TIME, loadtimes, 2);\n\n\t\t\tfnvlist_add_uint64(*config,\n\t\t\t    ZPOOL_CONFIG_ERRCOUNT,\n\t\t\t    spa_approx_errlog_size(spa));\n\n\t\t\tif (spa_suspended(spa)) {\n\t\t\t\tfnvlist_add_uint64(*config,\n\t\t\t\t    ZPOOL_CONFIG_SUSPENDED,\n\t\t\t\t    spa->spa_failmode);\n\t\t\t\tfnvlist_add_uint64(*config,\n\t\t\t\t    ZPOOL_CONFIG_SUSPENDED_REASON,\n\t\t\t\t    spa->spa_suspended);\n\t\t\t}\n\n\t\t\tspa_add_spares(spa, *config);\n\t\t\tspa_add_l2cache(spa, *config);\n\t\t\tspa_add_feature_stats(spa, *config);\n\t\t}\n\t}\n\n\t \n\tif (altroot) {\n\t\tif (spa == NULL) {\n\t\t\tmutex_enter(&spa_namespace_lock);\n\t\t\tspa = spa_lookup(name);\n\t\t\tif (spa)\n\t\t\t\tspa_altroot(spa, altroot, buflen);\n\t\t\telse\n\t\t\t\taltroot[0] = '\\0';\n\t\t\tspa = NULL;\n\t\t\tmutex_exit(&spa_namespace_lock);\n\t\t} else {\n\t\t\tspa_altroot(spa, altroot, buflen);\n\t\t}\n\t}\n\n\tif (spa != NULL) {\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t\tspa_close(spa, FTAG);\n\t}\n\n\treturn (error);\n}\n\n \nstatic int\nspa_validate_aux_devs(spa_t *spa, nvlist_t *nvroot, uint64_t crtxg, int mode,\n    spa_aux_vdev_t *sav, const char *config, uint64_t version,\n    vdev_labeltype_t label)\n{\n\tnvlist_t **dev;\n\tuint_t i, ndev;\n\tvdev_t *vd;\n\tint error;\n\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == SCL_ALL);\n\n\t \n\tif (nvlist_lookup_nvlist_array(nvroot, config, &dev, &ndev) != 0)\n\t\treturn (0);\n\n\tif (ndev == 0)\n\t\treturn (SET_ERROR(EINVAL));\n\n\t \n\tif (spa_version(spa) < version)\n\t\treturn (SET_ERROR(ENOTSUP));\n\n\t \n\tsav->sav_pending = dev;\n\tsav->sav_npending = ndev;\n\n\tfor (i = 0; i < ndev; i++) {\n\t\tif ((error = spa_config_parse(spa, &vd, dev[i], NULL, 0,\n\t\t    mode)) != 0)\n\t\t\tgoto out;\n\n\t\tif (!vd->vdev_ops->vdev_op_leaf) {\n\t\t\tvdev_free(vd);\n\t\t\terror = SET_ERROR(EINVAL);\n\t\t\tgoto out;\n\t\t}\n\n\t\tvd->vdev_top = vd;\n\n\t\tif ((error = vdev_open(vd)) == 0 &&\n\t\t    (error = vdev_label_init(vd, crtxg, label)) == 0) {\n\t\t\tfnvlist_add_uint64(dev[i], ZPOOL_CONFIG_GUID,\n\t\t\t    vd->vdev_guid);\n\t\t}\n\n\t\tvdev_free(vd);\n\n\t\tif (error &&\n\t\t    (mode != VDEV_ALLOC_SPARE && mode != VDEV_ALLOC_L2CACHE))\n\t\t\tgoto out;\n\t\telse\n\t\t\terror = 0;\n\t}\n\nout:\n\tsav->sav_pending = NULL;\n\tsav->sav_npending = 0;\n\treturn (error);\n}\n\nstatic int\nspa_validate_aux(spa_t *spa, nvlist_t *nvroot, uint64_t crtxg, int mode)\n{\n\tint error;\n\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == SCL_ALL);\n\n\tif ((error = spa_validate_aux_devs(spa, nvroot, crtxg, mode,\n\t    &spa->spa_spares, ZPOOL_CONFIG_SPARES, SPA_VERSION_SPARES,\n\t    VDEV_LABEL_SPARE)) != 0) {\n\t\treturn (error);\n\t}\n\n\treturn (spa_validate_aux_devs(spa, nvroot, crtxg, mode,\n\t    &spa->spa_l2cache, ZPOOL_CONFIG_L2CACHE, SPA_VERSION_L2CACHE,\n\t    VDEV_LABEL_L2CACHE));\n}\n\nstatic void\nspa_set_aux_vdevs(spa_aux_vdev_t *sav, nvlist_t **devs, int ndevs,\n    const char *config)\n{\n\tint i;\n\n\tif (sav->sav_config != NULL) {\n\t\tnvlist_t **olddevs;\n\t\tuint_t oldndevs;\n\t\tnvlist_t **newdevs;\n\n\t\t \n\t\tVERIFY0(nvlist_lookup_nvlist_array(sav->sav_config, config,\n\t\t    &olddevs, &oldndevs));\n\n\t\tnewdevs = kmem_alloc(sizeof (void *) *\n\t\t    (ndevs + oldndevs), KM_SLEEP);\n\t\tfor (i = 0; i < oldndevs; i++)\n\t\t\tnewdevs[i] = fnvlist_dup(olddevs[i]);\n\t\tfor (i = 0; i < ndevs; i++)\n\t\t\tnewdevs[i + oldndevs] = fnvlist_dup(devs[i]);\n\n\t\tfnvlist_remove(sav->sav_config, config);\n\n\t\tfnvlist_add_nvlist_array(sav->sav_config, config,\n\t\t    (const nvlist_t * const *)newdevs, ndevs + oldndevs);\n\t\tfor (i = 0; i < oldndevs + ndevs; i++)\n\t\t\tnvlist_free(newdevs[i]);\n\t\tkmem_free(newdevs, (oldndevs + ndevs) * sizeof (void *));\n\t} else {\n\t\t \n\t\tsav->sav_config = fnvlist_alloc();\n\t\tfnvlist_add_nvlist_array(sav->sav_config, config,\n\t\t    (const nvlist_t * const *)devs, ndevs);\n\t}\n}\n\n \nvoid\nspa_l2cache_drop(spa_t *spa)\n{\n\tvdev_t *vd;\n\tint i;\n\tspa_aux_vdev_t *sav = &spa->spa_l2cache;\n\n\tfor (i = 0; i < sav->sav_count; i++) {\n\t\tuint64_t pool;\n\n\t\tvd = sav->sav_vdevs[i];\n\t\tASSERT(vd != NULL);\n\n\t\tif (spa_l2cache_exists(vd->vdev_guid, &pool) &&\n\t\t    pool != 0ULL && l2arc_vdev_present(vd))\n\t\t\tl2arc_remove_vdev(vd);\n\t}\n}\n\n \nstatic int\nspa_create_check_encryption_params(dsl_crypto_params_t *dcp,\n    boolean_t has_encryption)\n{\n\tif (dcp->cp_crypt != ZIO_CRYPT_OFF &&\n\t    dcp->cp_crypt != ZIO_CRYPT_INHERIT &&\n\t    !has_encryption)\n\t\treturn (SET_ERROR(ENOTSUP));\n\n\treturn (dmu_objset_create_crypt_check(NULL, dcp, NULL));\n}\n\n \nint\nspa_create(const char *pool, nvlist_t *nvroot, nvlist_t *props,\n    nvlist_t *zplprops, dsl_crypto_params_t *dcp)\n{\n\tspa_t *spa;\n\tconst char *altroot = NULL;\n\tvdev_t *rvd;\n\tdsl_pool_t *dp;\n\tdmu_tx_t *tx;\n\tint error = 0;\n\tuint64_t txg = TXG_INITIAL;\n\tnvlist_t **spares, **l2cache;\n\tuint_t nspares, nl2cache;\n\tuint64_t version, obj, ndraid = 0;\n\tboolean_t has_features;\n\tboolean_t has_encryption;\n\tboolean_t has_allocclass;\n\tspa_feature_t feat;\n\tconst char *feat_name;\n\tconst char *poolname;\n\tnvlist_t *nvl;\n\n\tif (props == NULL ||\n\t    nvlist_lookup_string(props, \"tname\", &poolname) != 0)\n\t\tpoolname = (char *)pool;\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\tif (spa_lookup(poolname) != NULL) {\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (SET_ERROR(EEXIST));\n\t}\n\n\t \n\tnvl = fnvlist_alloc();\n\tfnvlist_add_string(nvl, ZPOOL_CONFIG_POOL_NAME, pool);\n\t(void) nvlist_lookup_string(props,\n\t    zpool_prop_to_name(ZPOOL_PROP_ALTROOT), &altroot);\n\tspa = spa_add(poolname, nvl, altroot);\n\tfnvlist_free(nvl);\n\tspa_activate(spa, spa_mode_global);\n\n\tif (props && (error = spa_prop_validate(spa, props))) {\n\t\tspa_deactivate(spa);\n\t\tspa_remove(spa);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (error);\n\t}\n\n\t \n\tif (poolname != pool)\n\t\tspa->spa_import_flags |= ZFS_IMPORT_TEMP_NAME;\n\n\thas_features = B_FALSE;\n\thas_encryption = B_FALSE;\n\thas_allocclass = B_FALSE;\n\tfor (nvpair_t *elem = nvlist_next_nvpair(props, NULL);\n\t    elem != NULL; elem = nvlist_next_nvpair(props, elem)) {\n\t\tif (zpool_prop_feature(nvpair_name(elem))) {\n\t\t\thas_features = B_TRUE;\n\n\t\t\tfeat_name = strchr(nvpair_name(elem), '@') + 1;\n\t\t\tVERIFY0(zfeature_lookup_name(feat_name, &feat));\n\t\t\tif (feat == SPA_FEATURE_ENCRYPTION)\n\t\t\t\thas_encryption = B_TRUE;\n\t\t\tif (feat == SPA_FEATURE_ALLOCATION_CLASSES)\n\t\t\t\thas_allocclass = B_TRUE;\n\t\t}\n\t}\n\n\t \n\tif (dcp != NULL) {\n\t\terror = spa_create_check_encryption_params(dcp, has_encryption);\n\t\tif (error != 0) {\n\t\t\tspa_deactivate(spa);\n\t\t\tspa_remove(spa);\n\t\t\tmutex_exit(&spa_namespace_lock);\n\t\t\treturn (error);\n\t\t}\n\t}\n\tif (!has_allocclass && zfs_special_devs(nvroot, NULL)) {\n\t\tspa_deactivate(spa);\n\t\tspa_remove(spa);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (ENOTSUP);\n\t}\n\n\tif (has_features || nvlist_lookup_uint64(props,\n\t    zpool_prop_to_name(ZPOOL_PROP_VERSION), &version) != 0) {\n\t\tversion = SPA_VERSION;\n\t}\n\tASSERT(SPA_VERSION_IS_SUPPORTED(version));\n\n\tspa->spa_first_txg = txg;\n\tspa->spa_uberblock.ub_txg = txg - 1;\n\tspa->spa_uberblock.ub_version = version;\n\tspa->spa_ubsync = spa->spa_uberblock;\n\tspa->spa_load_state = SPA_LOAD_CREATE;\n\tspa->spa_removing_phys.sr_state = DSS_NONE;\n\tspa->spa_removing_phys.sr_removing_vdev = -1;\n\tspa->spa_removing_phys.sr_prev_indirect_vdev = -1;\n\tspa->spa_indirect_vdevs_loaded = B_TRUE;\n\n\t \n\tspa->spa_async_zio_root = kmem_alloc(max_ncpus * sizeof (void *),\n\t    KM_SLEEP);\n\tfor (int i = 0; i < max_ncpus; i++) {\n\t\tspa->spa_async_zio_root[i] = zio_root(spa, NULL, NULL,\n\t\t    ZIO_FLAG_CANFAIL | ZIO_FLAG_SPECULATIVE |\n\t\t    ZIO_FLAG_GODFATHER);\n\t}\n\n\t \n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\n\terror = spa_config_parse(spa, &rvd, nvroot, NULL, 0, VDEV_ALLOC_ADD);\n\n\tASSERT(error != 0 || rvd != NULL);\n\tASSERT(error != 0 || spa->spa_root_vdev == rvd);\n\n\tif (error == 0 && !zfs_allocatable_devs(nvroot))\n\t\terror = SET_ERROR(EINVAL);\n\n\tif (error == 0 &&\n\t    (error = vdev_create(rvd, txg, B_FALSE)) == 0 &&\n\t    (error = vdev_draid_spare_create(nvroot, rvd, &ndraid, 0)) == 0 &&\n\t    (error = spa_validate_aux(spa, nvroot, txg, VDEV_ALLOC_ADD)) == 0) {\n\t\t \n\t\tfor (int c = 0; error == 0 && c < rvd->vdev_children; c++) {\n\t\t\tvdev_t *vd = rvd->vdev_child[c];\n\n\t\t\tvdev_metaslab_set_size(vd);\n\t\t\tvdev_expand(vd, txg);\n\t\t}\n\t}\n\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\tif (error != 0) {\n\t\tspa_unload(spa);\n\t\tspa_deactivate(spa);\n\t\tspa_remove(spa);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (error);\n\t}\n\n\t \n\tif (nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_SPARES,\n\t    &spares, &nspares) == 0) {\n\t\tspa->spa_spares.sav_config = fnvlist_alloc();\n\t\tfnvlist_add_nvlist_array(spa->spa_spares.sav_config,\n\t\t    ZPOOL_CONFIG_SPARES, (const nvlist_t * const *)spares,\n\t\t    nspares);\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_load_spares(spa);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tspa->spa_spares.sav_sync = B_TRUE;\n\t}\n\n\t \n\tif (nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_L2CACHE,\n\t    &l2cache, &nl2cache) == 0) {\n\t\tVERIFY0(nvlist_alloc(&spa->spa_l2cache.sav_config,\n\t\t    NV_UNIQUE_NAME, KM_SLEEP));\n\t\tfnvlist_add_nvlist_array(spa->spa_l2cache.sav_config,\n\t\t    ZPOOL_CONFIG_L2CACHE, (const nvlist_t * const *)l2cache,\n\t\t    nl2cache);\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_load_l2cache(spa);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tspa->spa_l2cache.sav_sync = B_TRUE;\n\t}\n\n\tspa->spa_is_initializing = B_TRUE;\n\tspa->spa_dsl_pool = dp = dsl_pool_create(spa, zplprops, dcp, txg);\n\tspa->spa_is_initializing = B_FALSE;\n\n\t \n\tddt_create(spa);\n\t \n\tbrt_create(spa);\n\n\tspa_update_dspace(spa);\n\n\ttx = dmu_tx_create_assigned(dp, txg);\n\n\t \n\tif (version >= SPA_VERSION_ZPOOL_HISTORY && !spa->spa_history)\n\t\tspa_history_create_obj(spa, tx);\n\n\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_POOL_CREATE);\n\tspa_history_log_version(spa, \"create\", tx);\n\n\t \n\tspa->spa_config_object = dmu_object_alloc(spa->spa_meta_objset,\n\t    DMU_OT_PACKED_NVLIST, SPA_CONFIG_BLOCKSIZE,\n\t    DMU_OT_PACKED_NVLIST_SIZE, sizeof (uint64_t), tx);\n\n\tif (zap_add(spa->spa_meta_objset,\n\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_CONFIG,\n\t    sizeof (uint64_t), 1, &spa->spa_config_object, tx) != 0) {\n\t\tcmn_err(CE_PANIC, \"failed to add pool config\");\n\t}\n\n\tif (zap_add(spa->spa_meta_objset,\n\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_CREATION_VERSION,\n\t    sizeof (uint64_t), 1, &version, tx) != 0) {\n\t\tcmn_err(CE_PANIC, \"failed to add pool version\");\n\t}\n\n\t \n\tif (version >= SPA_VERSION_RAIDZ_DEFLATE) {\n\t\tspa->spa_deflate = TRUE;\n\t\tif (zap_add(spa->spa_meta_objset,\n\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_DEFLATE,\n\t\t    sizeof (uint64_t), 1, &spa->spa_deflate, tx) != 0) {\n\t\t\tcmn_err(CE_PANIC, \"failed to add deflate\");\n\t\t}\n\t}\n\n\t \n\tobj = bpobj_alloc(spa->spa_meta_objset, 1 << 14, tx);\n\tdmu_object_set_compress(spa->spa_meta_objset, obj,\n\t    ZIO_COMPRESS_OFF, tx);\n\tif (zap_add(spa->spa_meta_objset,\n\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_SYNC_BPOBJ,\n\t    sizeof (uint64_t), 1, &obj, tx) != 0) {\n\t\tcmn_err(CE_PANIC, \"failed to add bpobj\");\n\t}\n\tVERIFY3U(0, ==, bpobj_open(&spa->spa_deferred_bpobj,\n\t    spa->spa_meta_objset, obj));\n\n\t \n\t(void) random_get_pseudo_bytes(spa->spa_cksum_salt.zcs_bytes,\n\t    sizeof (spa->spa_cksum_salt.zcs_bytes));\n\n\t \n\tspa->spa_bootfs = zpool_prop_default_numeric(ZPOOL_PROP_BOOTFS);\n\tspa->spa_delegation = zpool_prop_default_numeric(ZPOOL_PROP_DELEGATION);\n\tspa->spa_failmode = zpool_prop_default_numeric(ZPOOL_PROP_FAILUREMODE);\n\tspa->spa_autoexpand = zpool_prop_default_numeric(ZPOOL_PROP_AUTOEXPAND);\n\tspa->spa_multihost = zpool_prop_default_numeric(ZPOOL_PROP_MULTIHOST);\n\tspa->spa_autotrim = zpool_prop_default_numeric(ZPOOL_PROP_AUTOTRIM);\n\n\tif (props != NULL) {\n\t\tspa_configfile_set(spa, props, B_FALSE);\n\t\tspa_sync_props(props, tx);\n\t}\n\n\tfor (int i = 0; i < ndraid; i++)\n\t\tspa_feature_incr(spa, SPA_FEATURE_DRAID, tx);\n\n\tdmu_tx_commit(tx);\n\n\tspa->spa_sync_on = B_TRUE;\n\ttxg_sync_start(dp);\n\tmmp_thread_start(spa);\n\ttxg_wait_synced(dp, txg);\n\n\tspa_spawn_aux_threads(spa);\n\n\tspa_write_cachefile(spa, B_FALSE, B_TRUE, B_TRUE);\n\n\t \n\tspa_evicting_os_wait(spa);\n\tspa->spa_minref = zfs_refcount_count(&spa->spa_refcount);\n\tspa->spa_load_state = SPA_LOAD_NONE;\n\n\tspa_import_os(spa);\n\n\tmutex_exit(&spa_namespace_lock);\n\n\treturn (0);\n}\n\n \nint\nspa_import(char *pool, nvlist_t *config, nvlist_t *props, uint64_t flags)\n{\n\tspa_t *spa;\n\tconst char *altroot = NULL;\n\tspa_load_state_t state = SPA_LOAD_IMPORT;\n\tzpool_load_policy_t policy;\n\tspa_mode_t mode = spa_mode_global;\n\tuint64_t readonly = B_FALSE;\n\tint error;\n\tnvlist_t *nvroot;\n\tnvlist_t **spares, **l2cache;\n\tuint_t nspares, nl2cache;\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\tif (spa_lookup(pool) != NULL) {\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (SET_ERROR(EEXIST));\n\t}\n\n\t \n\t(void) nvlist_lookup_string(props,\n\t    zpool_prop_to_name(ZPOOL_PROP_ALTROOT), &altroot);\n\t(void) nvlist_lookup_uint64(props,\n\t    zpool_prop_to_name(ZPOOL_PROP_READONLY), &readonly);\n\tif (readonly)\n\t\tmode = SPA_MODE_READ;\n\tspa = spa_add(pool, config, altroot);\n\tspa->spa_import_flags = flags;\n\n\t \n\tif (spa->spa_import_flags & ZFS_IMPORT_VERBATIM) {\n\t\tif (props != NULL)\n\t\t\tspa_configfile_set(spa, props, B_FALSE);\n\n\t\tspa_write_cachefile(spa, B_FALSE, B_TRUE, B_FALSE);\n\t\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_POOL_IMPORT);\n\t\tzfs_dbgmsg(\"spa_import: verbatim import of %s\", pool);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (0);\n\t}\n\n\tspa_activate(spa, mode);\n\n\t \n\tspa_async_suspend(spa);\n\n\tzpool_get_load_policy(config, &policy);\n\tif (policy.zlp_rewind & ZPOOL_DO_REWIND)\n\t\tstate = SPA_LOAD_RECOVER;\n\n\tspa->spa_config_source = SPA_CONFIG_SRC_TRYIMPORT;\n\n\tif (state != SPA_LOAD_RECOVER) {\n\t\tspa->spa_last_ubsync_txg = spa->spa_load_txg = 0;\n\t\tzfs_dbgmsg(\"spa_import: importing %s\", pool);\n\t} else {\n\t\tzfs_dbgmsg(\"spa_import: importing %s, max_txg=%lld \"\n\t\t    \"(RECOVERY MODE)\", pool, (longlong_t)policy.zlp_txg);\n\t}\n\terror = spa_load_best(spa, state, policy.zlp_txg, policy.zlp_rewind);\n\n\t \n\tfnvlist_add_nvlist(config, ZPOOL_CONFIG_LOAD_INFO, spa->spa_load_info);\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t \n\tif (spa->spa_spares.sav_config) {\n\t\tnvlist_free(spa->spa_spares.sav_config);\n\t\tspa->spa_spares.sav_config = NULL;\n\t\tspa_load_spares(spa);\n\t}\n\tif (spa->spa_l2cache.sav_config) {\n\t\tnvlist_free(spa->spa_l2cache.sav_config);\n\t\tspa->spa_l2cache.sav_config = NULL;\n\t\tspa_load_l2cache(spa);\n\t}\n\n\tnvroot = fnvlist_lookup_nvlist(config, ZPOOL_CONFIG_VDEV_TREE);\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\tif (props != NULL)\n\t\tspa_configfile_set(spa, props, B_FALSE);\n\n\tif (error != 0 || (props && spa_writeable(spa) &&\n\t    (error = spa_prop_set(spa, props)))) {\n\t\tspa_unload(spa);\n\t\tspa_deactivate(spa);\n\t\tspa_remove(spa);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (error);\n\t}\n\n\tspa_async_resume(spa);\n\n\t \n\tif (nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_SPARES,\n\t    &spares, &nspares) == 0) {\n\t\tif (spa->spa_spares.sav_config)\n\t\t\tfnvlist_remove(spa->spa_spares.sav_config,\n\t\t\t    ZPOOL_CONFIG_SPARES);\n\t\telse\n\t\t\tspa->spa_spares.sav_config = fnvlist_alloc();\n\t\tfnvlist_add_nvlist_array(spa->spa_spares.sav_config,\n\t\t    ZPOOL_CONFIG_SPARES, (const nvlist_t * const *)spares,\n\t\t    nspares);\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_load_spares(spa);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tspa->spa_spares.sav_sync = B_TRUE;\n\t}\n\tif (nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_L2CACHE,\n\t    &l2cache, &nl2cache) == 0) {\n\t\tif (spa->spa_l2cache.sav_config)\n\t\t\tfnvlist_remove(spa->spa_l2cache.sav_config,\n\t\t\t    ZPOOL_CONFIG_L2CACHE);\n\t\telse\n\t\t\tspa->spa_l2cache.sav_config = fnvlist_alloc();\n\t\tfnvlist_add_nvlist_array(spa->spa_l2cache.sav_config,\n\t\t    ZPOOL_CONFIG_L2CACHE, (const nvlist_t * const *)l2cache,\n\t\t    nl2cache);\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\tspa_load_l2cache(spa);\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tspa->spa_l2cache.sav_sync = B_TRUE;\n\t}\n\n\t \n\tif (spa->spa_autoreplace) {\n\t\tspa_aux_check_removed(&spa->spa_spares);\n\t\tspa_aux_check_removed(&spa->spa_l2cache);\n\t}\n\n\tif (spa_writeable(spa)) {\n\t\t \n\t\tspa_config_update(spa, SPA_CONFIG_UPDATE_POOL);\n\t}\n\n\t \n\tspa_async_request(spa, SPA_ASYNC_AUTOEXPAND);\n\n\tspa_history_log_version(spa, \"import\", NULL);\n\n\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_POOL_IMPORT);\n\n\tmutex_exit(&spa_namespace_lock);\n\n\tzvol_create_minors_recursive(pool);\n\n\tspa_import_os(spa);\n\n\treturn (0);\n}\n\nnvlist_t *\nspa_tryimport(nvlist_t *tryconfig)\n{\n\tnvlist_t *config = NULL;\n\tconst char *poolname, *cachefile;\n\tspa_t *spa;\n\tuint64_t state;\n\tint error;\n\tzpool_load_policy_t policy;\n\n\tif (nvlist_lookup_string(tryconfig, ZPOOL_CONFIG_POOL_NAME, &poolname))\n\t\treturn (NULL);\n\n\tif (nvlist_lookup_uint64(tryconfig, ZPOOL_CONFIG_POOL_STATE, &state))\n\t\treturn (NULL);\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\tspa = spa_add(TRYIMPORT_NAME, tryconfig, NULL);\n\tspa_activate(spa, SPA_MODE_READ);\n\n\t \n\tzpool_get_load_policy(spa->spa_config, &policy);\n\tif (policy.zlp_txg != UINT64_MAX) {\n\t\tspa->spa_load_max_txg = policy.zlp_txg;\n\t\tspa->spa_extreme_rewind = B_TRUE;\n\t\tzfs_dbgmsg(\"spa_tryimport: importing %s, max_txg=%lld\",\n\t\t    poolname, (longlong_t)policy.zlp_txg);\n\t} else {\n\t\tzfs_dbgmsg(\"spa_tryimport: importing %s\", poolname);\n\t}\n\n\tif (nvlist_lookup_string(tryconfig, ZPOOL_CONFIG_CACHEFILE, &cachefile)\n\t    == 0) {\n\t\tzfs_dbgmsg(\"spa_tryimport: using cachefile '%s'\", cachefile);\n\t\tspa->spa_config_source = SPA_CONFIG_SRC_CACHEFILE;\n\t} else {\n\t\tspa->spa_config_source = SPA_CONFIG_SRC_SCAN;\n\t}\n\n\t \n\tspa->spa_import_flags |= ZFS_IMPORT_MISSING_LOG;\n\n\terror = spa_load(spa, SPA_LOAD_TRYIMPORT, SPA_IMPORT_EXISTING);\n\n\t \n\tif (spa->spa_root_vdev != NULL) {\n\t\tconfig = spa_config_generate(spa, NULL, -1ULL, B_TRUE);\n\t\tfnvlist_add_string(config, ZPOOL_CONFIG_POOL_NAME, poolname);\n\t\tfnvlist_add_uint64(config, ZPOOL_CONFIG_POOL_STATE, state);\n\t\tfnvlist_add_uint64(config, ZPOOL_CONFIG_TIMESTAMP,\n\t\t    spa->spa_uberblock.ub_timestamp);\n\t\tfnvlist_add_nvlist(config, ZPOOL_CONFIG_LOAD_INFO,\n\t\t    spa->spa_load_info);\n\t\tfnvlist_add_uint64(config, ZPOOL_CONFIG_ERRATA,\n\t\t    spa->spa_errata);\n\n\t\t \n\t\tif ((!error || error == EEXIST) && spa->spa_bootfs) {\n\t\t\tchar *tmpname = kmem_alloc(MAXPATHLEN, KM_SLEEP);\n\n\t\t\t \n\t\t\tif (dsl_dsobj_to_dsname(spa_name(spa),\n\t\t\t    spa->spa_bootfs, tmpname) == 0) {\n\t\t\t\tchar *cp;\n\t\t\t\tchar *dsname;\n\n\t\t\t\tdsname = kmem_alloc(MAXPATHLEN, KM_SLEEP);\n\n\t\t\t\tcp = strchr(tmpname, '/');\n\t\t\t\tif (cp == NULL) {\n\t\t\t\t\t(void) strlcpy(dsname, tmpname,\n\t\t\t\t\t    MAXPATHLEN);\n\t\t\t\t} else {\n\t\t\t\t\t(void) snprintf(dsname, MAXPATHLEN,\n\t\t\t\t\t    \"%s/%s\", poolname, ++cp);\n\t\t\t\t}\n\t\t\t\tfnvlist_add_string(config, ZPOOL_CONFIG_BOOTFS,\n\t\t\t\t    dsname);\n\t\t\t\tkmem_free(dsname, MAXPATHLEN);\n\t\t\t}\n\t\t\tkmem_free(tmpname, MAXPATHLEN);\n\t\t}\n\n\t\t \n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tspa_add_spares(spa, config);\n\t\tspa_add_l2cache(spa, config);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t}\n\n\tspa_unload(spa);\n\tspa_deactivate(spa);\n\tspa_remove(spa);\n\tmutex_exit(&spa_namespace_lock);\n\n\treturn (config);\n}\n\n \nstatic int\nspa_export_common(const char *pool, int new_state, nvlist_t **oldconfig,\n    boolean_t force, boolean_t hardforce)\n{\n\tint error;\n\tspa_t *spa;\n\n\tif (oldconfig)\n\t\t*oldconfig = NULL;\n\n\tif (!(spa_mode_global & SPA_MODE_WRITE))\n\t\treturn (SET_ERROR(EROFS));\n\n\tmutex_enter(&spa_namespace_lock);\n\tif ((spa = spa_lookup(pool)) == NULL) {\n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (SET_ERROR(ENOENT));\n\t}\n\n\tif (spa->spa_is_exporting) {\n\t\t \n\t\tmutex_exit(&spa_namespace_lock);\n\t\treturn (SET_ERROR(ZFS_ERR_EXPORT_IN_PROGRESS));\n\t}\n\tspa->spa_is_exporting = B_TRUE;\n\n\t \n\tspa_open_ref(spa, FTAG);\n\tmutex_exit(&spa_namespace_lock);\n\tspa_async_suspend(spa);\n\tif (spa->spa_zvol_taskq) {\n\t\tzvol_remove_minors(spa, spa_name(spa), B_TRUE);\n\t\ttaskq_wait(spa->spa_zvol_taskq);\n\t}\n\tmutex_enter(&spa_namespace_lock);\n\tspa_close(spa, FTAG);\n\n\tif (spa->spa_state == POOL_STATE_UNINITIALIZED)\n\t\tgoto export_spa;\n\t \n\tif (spa->spa_sync_on) {\n\t\ttxg_wait_synced(spa->spa_dsl_pool, 0);\n\t\tspa_evicting_os_wait(spa);\n\t}\n\n\t \n\tif (!spa_refcount_zero(spa) || (spa->spa_inject_ref != 0)) {\n\t\terror = SET_ERROR(EBUSY);\n\t\tgoto fail;\n\t}\n\n\tif (spa->spa_sync_on) {\n\t\tvdev_t *rvd = spa->spa_root_vdev;\n\t\t \n\t\tif (!force && new_state == POOL_STATE_EXPORTED &&\n\t\t    spa_has_active_shared_spare(spa)) {\n\t\t\terror = SET_ERROR(EXDEV);\n\t\t\tgoto fail;\n\t\t}\n\n\t\t \n\t\tvdev_initialize_stop_all(rvd, VDEV_INITIALIZE_ACTIVE);\n\t\tvdev_trim_stop_all(rvd, VDEV_TRIM_ACTIVE);\n\t\tvdev_autotrim_stop_all(spa);\n\t\tvdev_rebuild_stop_all(spa);\n\n\t\t \n\t\tif (new_state != POOL_STATE_UNINITIALIZED && !hardforce) {\n\t\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\t\tspa->spa_state = new_state;\n\t\t\tvdev_config_dirty(rvd);\n\t\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\t}\n\n\t\t \n\t\tif (spa_should_flush_logs_on_unload(spa))\n\t\t\tspa_unload_log_sm_flush_all(spa);\n\n\t\tif (new_state != POOL_STATE_UNINITIALIZED && !hardforce) {\n\t\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t\t\tspa->spa_final_txg = spa_last_synced_txg(spa) +\n\t\t\t    TXG_DEFER_SIZE + 1;\n\t\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\t}\n\t}\n\nexport_spa:\n\tspa_export_os(spa);\n\n\tif (new_state == POOL_STATE_DESTROYED)\n\t\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_POOL_DESTROY);\n\telse if (new_state == POOL_STATE_EXPORTED)\n\t\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_POOL_EXPORT);\n\n\tif (spa->spa_state != POOL_STATE_UNINITIALIZED) {\n\t\tspa_unload(spa);\n\t\tspa_deactivate(spa);\n\t}\n\n\tif (oldconfig && spa->spa_config)\n\t\t*oldconfig = fnvlist_dup(spa->spa_config);\n\n\tif (new_state != POOL_STATE_UNINITIALIZED) {\n\t\tif (!hardforce)\n\t\t\tspa_write_cachefile(spa, B_TRUE, B_TRUE, B_FALSE);\n\t\tspa_remove(spa);\n\t} else {\n\t\t \n\t\tspa->spa_is_exporting = B_FALSE;\n\t}\n\n\tmutex_exit(&spa_namespace_lock);\n\treturn (0);\n\nfail:\n\tspa->spa_is_exporting = B_FALSE;\n\tspa_async_resume(spa);\n\tmutex_exit(&spa_namespace_lock);\n\treturn (error);\n}\n\n \nint\nspa_destroy(const char *pool)\n{\n\treturn (spa_export_common(pool, POOL_STATE_DESTROYED, NULL,\n\t    B_FALSE, B_FALSE));\n}\n\n \nint\nspa_export(const char *pool, nvlist_t **oldconfig, boolean_t force,\n    boolean_t hardforce)\n{\n\treturn (spa_export_common(pool, POOL_STATE_EXPORTED, oldconfig,\n\t    force, hardforce));\n}\n\n \nint\nspa_reset(const char *pool)\n{\n\treturn (spa_export_common(pool, POOL_STATE_UNINITIALIZED, NULL,\n\t    B_FALSE, B_FALSE));\n}\n\n \n\n \nstatic void\nspa_draid_feature_incr(void *arg, dmu_tx_t *tx)\n{\n\tspa_t *spa = dmu_tx_pool(tx)->dp_spa;\n\tint draid = (int)(uintptr_t)arg;\n\n\tfor (int c = 0; c < draid; c++)\n\t\tspa_feature_incr(spa, SPA_FEATURE_DRAID, tx);\n}\n\n \nint\nspa_vdev_add(spa_t *spa, nvlist_t *nvroot)\n{\n\tuint64_t txg, ndraid = 0;\n\tint error;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tvdev_t *vd, *tvd;\n\tnvlist_t **spares, **l2cache;\n\tuint_t nspares, nl2cache;\n\n\tASSERT(spa_writeable(spa));\n\n\ttxg = spa_vdev_enter(spa);\n\n\tif ((error = spa_config_parse(spa, &vd, nvroot, NULL, 0,\n\t    VDEV_ALLOC_ADD)) != 0)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, error));\n\n\tspa->spa_pending_vdev = vd;\t \n\n\tif (nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_SPARES, &spares,\n\t    &nspares) != 0)\n\t\tnspares = 0;\n\n\tif (nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_L2CACHE, &l2cache,\n\t    &nl2cache) != 0)\n\t\tnl2cache = 0;\n\n\tif (vd->vdev_children == 0 && nspares == 0 && nl2cache == 0)\n\t\treturn (spa_vdev_exit(spa, vd, txg, EINVAL));\n\n\tif (vd->vdev_children != 0 &&\n\t    (error = vdev_create(vd, txg, B_FALSE)) != 0) {\n\t\treturn (spa_vdev_exit(spa, vd, txg, error));\n\t}\n\n\t \n\tif ((error = vdev_draid_spare_create(nvroot, vd, &ndraid,\n\t    rvd->vdev_children)) == 0) {\n\t\tif (ndraid > 0 && nvlist_lookup_nvlist_array(nvroot,\n\t\t    ZPOOL_CONFIG_SPARES, &spares, &nspares) != 0)\n\t\t\tnspares = 0;\n\t} else {\n\t\treturn (spa_vdev_exit(spa, vd, txg, error));\n\t}\n\n\t \n\tif ((error = spa_validate_aux(spa, nvroot, txg, VDEV_ALLOC_ADD)) != 0)\n\t\treturn (spa_vdev_exit(spa, vd, txg, error));\n\n\t \n\tif (spa->spa_vdev_removal != NULL ||\n\t    spa->spa_removing_phys.sr_prev_indirect_vdev != -1) {\n\t\tfor (int c = 0; c < vd->vdev_children; c++) {\n\t\t\ttvd = vd->vdev_child[c];\n\t\t\tif (spa->spa_vdev_removal != NULL &&\n\t\t\t    tvd->vdev_ashift != spa->spa_max_ashift) {\n\t\t\t\treturn (spa_vdev_exit(spa, vd, txg, EINVAL));\n\t\t\t}\n\t\t\t \n\t\t\tif (vdev_get_nparity(tvd) != 0)\n\t\t\t\treturn (spa_vdev_exit(spa, vd, txg, EINVAL));\n\n\t\t\t \n\t\t\tif (tvd->vdev_ops == &vdev_mirror_ops) {\n\t\t\t\tfor (uint64_t cid = 0;\n\t\t\t\t    cid < tvd->vdev_children; cid++) {\n\t\t\t\t\tvdev_t *cvd = tvd->vdev_child[cid];\n\t\t\t\t\tif (!cvd->vdev_ops->vdev_op_leaf) {\n\t\t\t\t\t\treturn (spa_vdev_exit(spa, vd,\n\t\t\t\t\t\t    txg, EINVAL));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (int c = 0; c < vd->vdev_children; c++) {\n\t\ttvd = vd->vdev_child[c];\n\t\tvdev_remove_child(vd, tvd);\n\t\ttvd->vdev_id = rvd->vdev_children;\n\t\tvdev_add_child(rvd, tvd);\n\t\tvdev_config_dirty(tvd);\n\t}\n\n\tif (nspares != 0) {\n\t\tspa_set_aux_vdevs(&spa->spa_spares, spares, nspares,\n\t\t    ZPOOL_CONFIG_SPARES);\n\t\tspa_load_spares(spa);\n\t\tspa->spa_spares.sav_sync = B_TRUE;\n\t}\n\n\tif (nl2cache != 0) {\n\t\tspa_set_aux_vdevs(&spa->spa_l2cache, l2cache, nl2cache,\n\t\t    ZPOOL_CONFIG_L2CACHE);\n\t\tspa_load_l2cache(spa);\n\t\tspa->spa_l2cache.sav_sync = B_TRUE;\n\t}\n\n\t \n\tif (ndraid != 0) {\n\t\tdmu_tx_t *tx;\n\n\t\ttx = dmu_tx_create_assigned(spa->spa_dsl_pool, txg);\n\t\tdsl_sync_task_nowait(spa->spa_dsl_pool, spa_draid_feature_incr,\n\t\t    (void *)(uintptr_t)ndraid, tx);\n\t\tdmu_tx_commit(tx);\n\t}\n\n\t \n\t(void) spa_vdev_exit(spa, vd, txg, 0);\n\n\tmutex_enter(&spa_namespace_lock);\n\tspa_config_update(spa, SPA_CONFIG_UPDATE_POOL);\n\tspa_event_notify(spa, NULL, NULL, ESC_ZFS_VDEV_ADD);\n\tmutex_exit(&spa_namespace_lock);\n\n\treturn (0);\n}\n\n \nint\nspa_vdev_attach(spa_t *spa, uint64_t guid, nvlist_t *nvroot, int replacing,\n    int rebuild)\n{\n\tuint64_t txg, dtl_max_txg;\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tvdev_t *oldvd, *newvd, *newrootvd, *pvd, *tvd;\n\tvdev_ops_t *pvops;\n\tchar *oldvdpath, *newvdpath;\n\tint newvd_isspare;\n\tint error;\n\n\tASSERT(spa_writeable(spa));\n\n\ttxg = spa_vdev_enter(spa);\n\n\toldvd = spa_lookup_by_guid(spa, guid, B_FALSE);\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tif (spa_feature_is_active(spa, SPA_FEATURE_POOL_CHECKPOINT)) {\n\t\terror = (spa_has_checkpoint(spa)) ?\n\t\t    ZFS_ERR_CHECKPOINT_EXISTS : ZFS_ERR_DISCARDING_CHECKPOINT;\n\t\treturn (spa_vdev_exit(spa, NULL, txg, error));\n\t}\n\n\tif (rebuild) {\n\t\tif (!spa_feature_is_enabled(spa, SPA_FEATURE_DEVICE_REBUILD))\n\t\t\treturn (spa_vdev_exit(spa, NULL, txg, ENOTSUP));\n\n\t\tif (dsl_scan_resilvering(spa_get_dsl(spa)) ||\n\t\t    dsl_scan_resilver_scheduled(spa_get_dsl(spa))) {\n\t\t\treturn (spa_vdev_exit(spa, NULL, txg,\n\t\t\t    ZFS_ERR_RESILVER_IN_PROGRESS));\n\t\t}\n\t} else {\n\t\tif (vdev_rebuild_active(rvd))\n\t\t\treturn (spa_vdev_exit(spa, NULL, txg,\n\t\t\t    ZFS_ERR_REBUILD_IN_PROGRESS));\n\t}\n\n\tif (spa->spa_vdev_removal != NULL)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EBUSY));\n\n\tif (oldvd == NULL)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, ENODEV));\n\n\tif (!oldvd->vdev_ops->vdev_op_leaf)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, ENOTSUP));\n\n\tpvd = oldvd->vdev_parent;\n\n\tif (spa_config_parse(spa, &newrootvd, nvroot, NULL, 0,\n\t    VDEV_ALLOC_ATTACH) != 0)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EINVAL));\n\n\tif (newrootvd->vdev_children != 1)\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, EINVAL));\n\n\tnewvd = newrootvd->vdev_child[0];\n\n\tif (!newvd->vdev_ops->vdev_op_leaf)\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, EINVAL));\n\n\tif ((error = vdev_create(newrootvd, txg, replacing)) != 0)\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, error));\n\n\t \n\tif ((oldvd->vdev_top->vdev_alloc_bias != VDEV_BIAS_NONE ||\n\t    oldvd->vdev_top->vdev_islog) && newvd->vdev_isspare) {\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\t}\n\n\t \n\tif (newvd->vdev_ops == &vdev_draid_spare_ops &&\n\t    oldvd->vdev_top != vdev_draid_spare_get_parent(newvd)) {\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\t}\n\n\tif (rebuild) {\n\t\t \n\t\ttvd = pvd;\n\t\tif (pvd->vdev_top != NULL)\n\t\t\ttvd = pvd->vdev_top;\n\n\t\tif (tvd->vdev_ops != &vdev_mirror_ops &&\n\t\t    tvd->vdev_ops != &vdev_root_ops &&\n\t\t    tvd->vdev_ops != &vdev_draid_ops) {\n\t\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\t\t}\n\t}\n\n\tif (!replacing) {\n\t\t \n\t\tif (pvd->vdev_ops != &vdev_mirror_ops &&\n\t\t    pvd->vdev_ops != &vdev_root_ops)\n\t\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\n\t\tpvops = &vdev_mirror_ops;\n\t} else {\n\t\t \n\t\tif (pvd->vdev_ops == &vdev_spare_ops &&\n\t\t    oldvd->vdev_isspare &&\n\t\t    !spa_has_spare(spa, newvd->vdev_guid))\n\t\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\n\t\t \n\t\tif (pvd->vdev_ops == &vdev_replacing_ops &&\n\t\t    spa_version(spa) < SPA_VERSION_MULTI_REPLACE) {\n\t\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\t\t} else if (pvd->vdev_ops == &vdev_spare_ops &&\n\t\t    newvd->vdev_isspare != oldvd->vdev_isspare) {\n\t\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\t\t}\n\n\t\tif (newvd->vdev_isspare)\n\t\t\tpvops = &vdev_spare_ops;\n\t\telse\n\t\t\tpvops = &vdev_replacing_ops;\n\t}\n\n\t \n\tif (newvd->vdev_asize < vdev_get_min_asize(oldvd))\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, EOVERFLOW));\n\n\t \n\tif (newvd->vdev_ashift > oldvd->vdev_top->vdev_ashift)\n\t\treturn (spa_vdev_exit(spa, newrootvd, txg, ENOTSUP));\n\n\t \n\tif (strcmp(oldvd->vdev_path, newvd->vdev_path) == 0) {\n\t\tspa_strfree(oldvd->vdev_path);\n\t\toldvd->vdev_path = kmem_alloc(strlen(newvd->vdev_path) + 5,\n\t\t    KM_SLEEP);\n\t\t(void) snprintf(oldvd->vdev_path, strlen(newvd->vdev_path) + 5,\n\t\t    \"%s/%s\", newvd->vdev_path, \"old\");\n\t\tif (oldvd->vdev_devid != NULL) {\n\t\t\tspa_strfree(oldvd->vdev_devid);\n\t\t\toldvd->vdev_devid = NULL;\n\t\t}\n\t}\n\n\t \n\tif (pvd->vdev_ops != pvops)\n\t\tpvd = vdev_add_parent(oldvd, pvops);\n\n\tASSERT(pvd->vdev_top->vdev_parent == rvd);\n\tASSERT(pvd->vdev_ops == pvops);\n\tASSERT(oldvd->vdev_parent == pvd);\n\n\t \n\tvdev_remove_child(newrootvd, newvd);\n\tnewvd->vdev_id = pvd->vdev_children;\n\tnewvd->vdev_crtxg = oldvd->vdev_crtxg;\n\tvdev_add_child(pvd, newvd);\n\n\t \n\tvdev_propagate_state(pvd);\n\n\ttvd = newvd->vdev_top;\n\tASSERT(pvd->vdev_top == tvd);\n\tASSERT(tvd->vdev_parent == rvd);\n\n\tvdev_config_dirty(tvd);\n\n\t \n\tdtl_max_txg = txg + TXG_CONCURRENT_STATES;\n\n\tvdev_dtl_dirty(newvd, DTL_MISSING,\n\t    TXG_INITIAL, dtl_max_txg - TXG_INITIAL);\n\n\tif (newvd->vdev_isspare) {\n\t\tspa_spare_activate(newvd);\n\t\tspa_event_notify(spa, newvd, NULL, ESC_ZFS_VDEV_SPARE);\n\t}\n\n\toldvdpath = spa_strdup(oldvd->vdev_path);\n\tnewvdpath = spa_strdup(newvd->vdev_path);\n\tnewvd_isspare = newvd->vdev_isspare;\n\n\t \n\tvdev_dirty(tvd, VDD_DTL, newvd, txg);\n\n\t \n\tif (rebuild) {\n\t\tnewvd->vdev_rebuild_txg = txg;\n\n\t\tvdev_rebuild(tvd);\n\t} else {\n\t\tnewvd->vdev_resilver_txg = txg;\n\n\t\tif (dsl_scan_resilvering(spa_get_dsl(spa)) &&\n\t\t    spa_feature_is_enabled(spa, SPA_FEATURE_RESILVER_DEFER)) {\n\t\t\tvdev_defer_resilver(newvd);\n\t\t} else {\n\t\t\tdsl_scan_restart_resilver(spa->spa_dsl_pool,\n\t\t\t    dtl_max_txg);\n\t\t}\n\t}\n\n\tif (spa->spa_bootfs)\n\t\tspa_event_notify(spa, newvd, NULL, ESC_ZFS_BOOTFS_VDEV_ATTACH);\n\n\tspa_event_notify(spa, newvd, NULL, ESC_ZFS_VDEV_ATTACH);\n\n\t \n\t(void) spa_vdev_exit(spa, newrootvd, dtl_max_txg, 0);\n\n\tspa_history_log_internal(spa, \"vdev attach\", NULL,\n\t    \"%s vdev=%s %s vdev=%s\",\n\t    replacing && newvd_isspare ? \"spare in\" :\n\t    replacing ? \"replace\" : \"attach\", newvdpath,\n\t    replacing ? \"for\" : \"to\", oldvdpath);\n\n\tspa_strfree(oldvdpath);\n\tspa_strfree(newvdpath);\n\n\treturn (0);\n}\n\n \nint\nspa_vdev_detach(spa_t *spa, uint64_t guid, uint64_t pguid, int replace_done)\n{\n\tuint64_t txg;\n\tint error;\n\tvdev_t *rvd __maybe_unused = spa->spa_root_vdev;\n\tvdev_t *vd, *pvd, *cvd, *tvd;\n\tboolean_t unspare = B_FALSE;\n\tuint64_t unspare_guid = 0;\n\tchar *vdpath;\n\n\tASSERT(spa_writeable(spa));\n\n\ttxg = spa_vdev_detach_enter(spa, guid);\n\n\tvd = spa_lookup_by_guid(spa, guid, B_FALSE);\n\n\t \n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tif (spa_feature_is_active(spa, SPA_FEATURE_POOL_CHECKPOINT)) {\n\t\terror = (spa_has_checkpoint(spa)) ?\n\t\t    ZFS_ERR_CHECKPOINT_EXISTS : ZFS_ERR_DISCARDING_CHECKPOINT;\n\t\treturn (spa_vdev_exit(spa, NULL, txg, error));\n\t}\n\n\tif (vd == NULL)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, ENODEV));\n\n\tif (!vd->vdev_ops->vdev_op_leaf)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, ENOTSUP));\n\n\tpvd = vd->vdev_parent;\n\n\t \n\tif (pvd->vdev_guid != pguid && pguid != 0)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EBUSY));\n\n\t \n\tif (replace_done && pvd->vdev_ops != &vdev_replacing_ops &&\n\t    pvd->vdev_ops != &vdev_spare_ops)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, ENOTSUP));\n\n\tASSERT(pvd->vdev_ops != &vdev_spare_ops ||\n\t    spa_version(spa) >= SPA_VERSION_SPARES);\n\n\t \n\tif (pvd->vdev_ops != &vdev_replacing_ops &&\n\t    pvd->vdev_ops != &vdev_mirror_ops &&\n\t    pvd->vdev_ops != &vdev_spare_ops)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, ENOTSUP));\n\n\t \n\tif (vdev_dtl_required(vd))\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EBUSY));\n\n\tASSERT(pvd->vdev_children >= 2);\n\n\t \n\tif (pvd->vdev_ops == &vdev_replacing_ops && vd->vdev_id > 0 &&\n\t    vd->vdev_path != NULL) {\n\t\tsize_t len = strlen(vd->vdev_path);\n\n\t\tfor (int c = 0; c < pvd->vdev_children; c++) {\n\t\t\tcvd = pvd->vdev_child[c];\n\n\t\t\tif (cvd == vd || cvd->vdev_path == NULL)\n\t\t\t\tcontinue;\n\n\t\t\tif (strncmp(cvd->vdev_path, vd->vdev_path, len) == 0 &&\n\t\t\t    strcmp(cvd->vdev_path + len, \"/old\") == 0) {\n\t\t\t\tspa_strfree(cvd->vdev_path);\n\t\t\t\tcvd->vdev_path = spa_strdup(vd->vdev_path);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (pvd->vdev_ops == &vdev_spare_ops && vd->vdev_id == 0) {\n\t\tvdev_t *last_cvd = pvd->vdev_child[pvd->vdev_children - 1];\n\n\t\tif (last_cvd->vdev_isspare &&\n\t\t    last_cvd->vdev_ops != &vdev_draid_spare_ops) {\n\t\t\tunspare = B_TRUE;\n\t\t}\n\t}\n\n\t \n\t(void) vdev_label_init(vd, 0, VDEV_LABEL_REMOVE);\n\n\t \n\tvdev_remove_child(pvd, vd);\n\tvdev_compact_children(pvd);\n\n\t \n\tcvd = pvd->vdev_child[pvd->vdev_children - 1];\n\n\t \n\tif (unspare) {\n\t\tASSERT(cvd->vdev_isspare);\n\t\tspa_spare_remove(cvd);\n\t\tunspare_guid = cvd->vdev_guid;\n\t\t(void) spa_vdev_remove(spa, unspare_guid, B_TRUE);\n\t\tcvd->vdev_unspare = B_TRUE;\n\t}\n\n\t \n\tif (pvd->vdev_children == 1) {\n\t\tif (pvd->vdev_ops == &vdev_spare_ops)\n\t\t\tcvd->vdev_unspare = B_FALSE;\n\t\tvdev_remove_parent(cvd);\n\t}\n\n\t \n\ttvd = cvd->vdev_top;\n\tASSERT(tvd->vdev_parent == rvd);\n\n\t \n\tvdev_propagate_state(cvd);\n\n\t \n\tif (spa->spa_autoexpand) {\n\t\tvdev_reopen(tvd);\n\t\tvdev_expand(tvd, txg);\n\t}\n\n\tvdev_config_dirty(tvd);\n\n\t \n\tvdpath = spa_strdup(vd->vdev_path ? vd->vdev_path : \"none\");\n\tfor (int t = 0; t < TXG_SIZE; t++)\n\t\t(void) txg_list_remove_this(&tvd->vdev_dtl_list, vd, t);\n\tvd->vdev_detached = B_TRUE;\n\tvdev_dirty(tvd, VDD_DTL, vd, txg);\n\n\tspa_event_notify(spa, vd, NULL, ESC_ZFS_VDEV_REMOVE);\n\tspa_notify_waiters(spa);\n\n\t \n\tspa_open_ref(spa, FTAG);\n\n\terror = spa_vdev_exit(spa, vd, txg, 0);\n\n\tspa_history_log_internal(spa, \"detach\", NULL,\n\t    \"vdev=%s\", vdpath);\n\tspa_strfree(vdpath);\n\n\t \n\tif (unspare) {\n\t\tspa_t *altspa = NULL;\n\n\t\tmutex_enter(&spa_namespace_lock);\n\t\twhile ((altspa = spa_next(altspa)) != NULL) {\n\t\t\tif (altspa->spa_state != POOL_STATE_ACTIVE ||\n\t\t\t    altspa == spa)\n\t\t\t\tcontinue;\n\n\t\t\tspa_open_ref(altspa, FTAG);\n\t\t\tmutex_exit(&spa_namespace_lock);\n\t\t\t(void) spa_vdev_remove(altspa, unspare_guid, B_TRUE);\n\t\t\tmutex_enter(&spa_namespace_lock);\n\t\t\tspa_close(altspa, FTAG);\n\t\t}\n\t\tmutex_exit(&spa_namespace_lock);\n\n\t\t \n\t\tspa_vdev_resilver_done(spa);\n\t}\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\tspa_close(spa, FTAG);\n\tmutex_exit(&spa_namespace_lock);\n\n\treturn (error);\n}\n\nstatic int\nspa_vdev_initialize_impl(spa_t *spa, uint64_t guid, uint64_t cmd_type,\n    list_t *vd_list)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tspa_config_enter(spa, SCL_CONFIG | SCL_STATE, FTAG, RW_READER);\n\n\t \n\tvdev_t *vd = spa_lookup_by_guid(spa, guid, B_FALSE);\n\tif (vd == NULL || vd->vdev_detached) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(ENODEV));\n\t} else if (!vd->vdev_ops->vdev_op_leaf || !vdev_is_concrete(vd)) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(EINVAL));\n\t} else if (!vdev_writeable(vd)) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(EROFS));\n\t}\n\tmutex_enter(&vd->vdev_initialize_lock);\n\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\n\t \n\tif (cmd_type == POOL_INITIALIZE_START &&\n\t    (vd->vdev_initialize_thread != NULL ||\n\t    vd->vdev_top->vdev_removing)) {\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t\treturn (SET_ERROR(EBUSY));\n\t} else if (cmd_type == POOL_INITIALIZE_CANCEL &&\n\t    (vd->vdev_initialize_state != VDEV_INITIALIZE_ACTIVE &&\n\t    vd->vdev_initialize_state != VDEV_INITIALIZE_SUSPENDED)) {\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t\treturn (SET_ERROR(ESRCH));\n\t} else if (cmd_type == POOL_INITIALIZE_SUSPEND &&\n\t    vd->vdev_initialize_state != VDEV_INITIALIZE_ACTIVE) {\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t\treturn (SET_ERROR(ESRCH));\n\t} else if (cmd_type == POOL_INITIALIZE_UNINIT &&\n\t    vd->vdev_initialize_thread != NULL) {\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t\treturn (SET_ERROR(EBUSY));\n\t}\n\n\tswitch (cmd_type) {\n\tcase POOL_INITIALIZE_START:\n\t\tvdev_initialize(vd);\n\t\tbreak;\n\tcase POOL_INITIALIZE_CANCEL:\n\t\tvdev_initialize_stop(vd, VDEV_INITIALIZE_CANCELED, vd_list);\n\t\tbreak;\n\tcase POOL_INITIALIZE_SUSPEND:\n\t\tvdev_initialize_stop(vd, VDEV_INITIALIZE_SUSPENDED, vd_list);\n\t\tbreak;\n\tcase POOL_INITIALIZE_UNINIT:\n\t\tvdev_uninitialize(vd);\n\t\tbreak;\n\tdefault:\n\t\tpanic(\"invalid cmd_type %llu\", (unsigned long long)cmd_type);\n\t}\n\tmutex_exit(&vd->vdev_initialize_lock);\n\n\treturn (0);\n}\n\nint\nspa_vdev_initialize(spa_t *spa, nvlist_t *nv, uint64_t cmd_type,\n    nvlist_t *vdev_errlist)\n{\n\tint total_errors = 0;\n\tlist_t vd_list;\n\n\tlist_create(&vd_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_initialize_node));\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\n\tfor (nvpair_t *pair = nvlist_next_nvpair(nv, NULL);\n\t    pair != NULL; pair = nvlist_next_nvpair(nv, pair)) {\n\t\tuint64_t vdev_guid = fnvpair_value_uint64(pair);\n\n\t\tint error = spa_vdev_initialize_impl(spa, vdev_guid, cmd_type,\n\t\t    &vd_list);\n\t\tif (error != 0) {\n\t\t\tchar guid_as_str[MAXNAMELEN];\n\n\t\t\t(void) snprintf(guid_as_str, sizeof (guid_as_str),\n\t\t\t    \"%llu\", (unsigned long long)vdev_guid);\n\t\t\tfnvlist_add_int64(vdev_errlist, guid_as_str, error);\n\t\t\ttotal_errors++;\n\t\t}\n\t}\n\n\t \n\tvdev_initialize_stop_wait(spa, &vd_list);\n\n\t \n\ttxg_wait_synced(spa->spa_dsl_pool, 0);\n\tmutex_exit(&spa_namespace_lock);\n\n\tlist_destroy(&vd_list);\n\n\treturn (total_errors);\n}\n\nstatic int\nspa_vdev_trim_impl(spa_t *spa, uint64_t guid, uint64_t cmd_type,\n    uint64_t rate, boolean_t partial, boolean_t secure, list_t *vd_list)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tspa_config_enter(spa, SCL_CONFIG | SCL_STATE, FTAG, RW_READER);\n\n\t \n\tvdev_t *vd = spa_lookup_by_guid(spa, guid, B_FALSE);\n\tif (vd == NULL || vd->vdev_detached) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(ENODEV));\n\t} else if (!vd->vdev_ops->vdev_op_leaf || !vdev_is_concrete(vd)) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(EINVAL));\n\t} else if (!vdev_writeable(vd)) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(EROFS));\n\t} else if (!vd->vdev_has_trim) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(EOPNOTSUPP));\n\t} else if (secure && !vd->vdev_has_securetrim) {\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\treturn (SET_ERROR(EOPNOTSUPP));\n\t}\n\tmutex_enter(&vd->vdev_trim_lock);\n\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\n\t \n\tif (cmd_type == POOL_TRIM_START &&\n\t    (vd->vdev_trim_thread != NULL || vd->vdev_top->vdev_removing)) {\n\t\tmutex_exit(&vd->vdev_trim_lock);\n\t\treturn (SET_ERROR(EBUSY));\n\t} else if (cmd_type == POOL_TRIM_CANCEL &&\n\t    (vd->vdev_trim_state != VDEV_TRIM_ACTIVE &&\n\t    vd->vdev_trim_state != VDEV_TRIM_SUSPENDED)) {\n\t\tmutex_exit(&vd->vdev_trim_lock);\n\t\treturn (SET_ERROR(ESRCH));\n\t} else if (cmd_type == POOL_TRIM_SUSPEND &&\n\t    vd->vdev_trim_state != VDEV_TRIM_ACTIVE) {\n\t\tmutex_exit(&vd->vdev_trim_lock);\n\t\treturn (SET_ERROR(ESRCH));\n\t}\n\n\tswitch (cmd_type) {\n\tcase POOL_TRIM_START:\n\t\tvdev_trim(vd, rate, partial, secure);\n\t\tbreak;\n\tcase POOL_TRIM_CANCEL:\n\t\tvdev_trim_stop(vd, VDEV_TRIM_CANCELED, vd_list);\n\t\tbreak;\n\tcase POOL_TRIM_SUSPEND:\n\t\tvdev_trim_stop(vd, VDEV_TRIM_SUSPENDED, vd_list);\n\t\tbreak;\n\tdefault:\n\t\tpanic(\"invalid cmd_type %llu\", (unsigned long long)cmd_type);\n\t}\n\tmutex_exit(&vd->vdev_trim_lock);\n\n\treturn (0);\n}\n\n \nint\nspa_vdev_trim(spa_t *spa, nvlist_t *nv, uint64_t cmd_type, uint64_t rate,\n    boolean_t partial, boolean_t secure, nvlist_t *vdev_errlist)\n{\n\tint total_errors = 0;\n\tlist_t vd_list;\n\n\tlist_create(&vd_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_trim_node));\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\n\tfor (nvpair_t *pair = nvlist_next_nvpair(nv, NULL);\n\t    pair != NULL; pair = nvlist_next_nvpair(nv, pair)) {\n\t\tuint64_t vdev_guid = fnvpair_value_uint64(pair);\n\n\t\tint error = spa_vdev_trim_impl(spa, vdev_guid, cmd_type,\n\t\t    rate, partial, secure, &vd_list);\n\t\tif (error != 0) {\n\t\t\tchar guid_as_str[MAXNAMELEN];\n\n\t\t\t(void) snprintf(guid_as_str, sizeof (guid_as_str),\n\t\t\t    \"%llu\", (unsigned long long)vdev_guid);\n\t\t\tfnvlist_add_int64(vdev_errlist, guid_as_str, error);\n\t\t\ttotal_errors++;\n\t\t}\n\t}\n\n\t \n\tvdev_trim_stop_wait(spa, &vd_list);\n\n\t \n\ttxg_wait_synced(spa->spa_dsl_pool, 0);\n\tmutex_exit(&spa_namespace_lock);\n\n\tlist_destroy(&vd_list);\n\n\treturn (total_errors);\n}\n\n \nint\nspa_vdev_split_mirror(spa_t *spa, const char *newname, nvlist_t *config,\n    nvlist_t *props, boolean_t exp)\n{\n\tint error = 0;\n\tuint64_t txg, *glist;\n\tspa_t *newspa;\n\tuint_t c, children, lastlog;\n\tnvlist_t **child, *nvl, *tmp;\n\tdmu_tx_t *tx;\n\tconst char *altroot = NULL;\n\tvdev_t *rvd, **vml = NULL;\t\t\t \n\tboolean_t activate_slog;\n\n\tASSERT(spa_writeable(spa));\n\n\ttxg = spa_vdev_enter(spa);\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tif (spa_feature_is_active(spa, SPA_FEATURE_POOL_CHECKPOINT)) {\n\t\terror = (spa_has_checkpoint(spa)) ?\n\t\t    ZFS_ERR_CHECKPOINT_EXISTS : ZFS_ERR_DISCARDING_CHECKPOINT;\n\t\treturn (spa_vdev_exit(spa, NULL, txg, error));\n\t}\n\n\t \n\tactivate_slog = spa_passivate_log(spa);\n\t(void) spa_vdev_config_exit(spa, NULL, txg, 0, FTAG);\n\terror = spa_reset_logs(spa);\n\ttxg = spa_vdev_config_enter(spa);\n\n\tif (activate_slog)\n\t\tspa_activate_log(spa);\n\n\tif (error != 0)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, error));\n\n\t \n\tif (spa_lookup(newname) != NULL)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EEXIST));\n\n\t \n\tif (nvlist_lookup_nvlist(config, ZPOOL_CONFIG_VDEV_TREE, &nvl) != 0 ||\n\t    nvlist_lookup_nvlist_array(nvl, ZPOOL_CONFIG_CHILDREN, &child,\n\t    &children) != 0)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EINVAL));\n\n\t \n\trvd = spa->spa_root_vdev;\n\tlastlog = 0;\n\tfor (c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *vd = rvd->vdev_child[c];\n\n\t\t \n\t\tif (vd->vdev_islog || (vd->vdev_ops != &vdev_indirect_ops &&\n\t\t    !vdev_is_concrete(vd))) {\n\t\t\tif (lastlog == 0)\n\t\t\t\tlastlog = c;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlastlog = 0;\n\t}\n\tif (children != (lastlog != 0 ? lastlog : rvd->vdev_children))\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EINVAL));\n\n\t \n\tif (nvlist_lookup_nvlist(nvl, ZPOOL_CONFIG_SPARES, &tmp) == 0 ||\n\t    nvlist_lookup_nvlist(nvl, ZPOOL_CONFIG_L2CACHE, &tmp) == 0)\n\t\treturn (spa_vdev_exit(spa, NULL, txg, EINVAL));\n\n\tvml = kmem_zalloc(children * sizeof (vdev_t *), KM_SLEEP);\n\tglist = kmem_zalloc(children * sizeof (uint64_t), KM_SLEEP);\n\n\t \n\tfor (c = 0; c < children; c++) {\n\t\tuint64_t is_hole = 0;\n\n\t\t(void) nvlist_lookup_uint64(child[c], ZPOOL_CONFIG_IS_HOLE,\n\t\t    &is_hole);\n\n\t\tif (is_hole != 0) {\n\t\t\tif (spa->spa_root_vdev->vdev_child[c]->vdev_ishole ||\n\t\t\t    spa->spa_root_vdev->vdev_child[c]->vdev_islog) {\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\terror = SET_ERROR(EINVAL);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (spa->spa_root_vdev->vdev_child[c]->vdev_ops ==\n\t\t    &vdev_indirect_ops)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (nvlist_lookup_uint64(child[c], ZPOOL_CONFIG_GUID,\n\t\t    &glist[c]) != 0) {\n\t\t\terror = SET_ERROR(EINVAL);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tvml[c] = spa_lookup_by_guid(spa, glist[c], B_FALSE);\n\t\tif (vml[c] == NULL) {\n\t\t\terror = SET_ERROR(ENODEV);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (vml[c]->vdev_parent->vdev_ops != &vdev_mirror_ops ||\n\t\t    vml[c]->vdev_islog ||\n\t\t    !vdev_is_concrete(vml[c]) ||\n\t\t    vml[c]->vdev_isspare ||\n\t\t    vml[c]->vdev_isl2cache ||\n\t\t    !vdev_writeable(vml[c]) ||\n\t\t    vml[c]->vdev_children != 0 ||\n\t\t    vml[c]->vdev_state != VDEV_STATE_HEALTHY ||\n\t\t    c != spa->spa_root_vdev->vdev_child[c]->vdev_id) {\n\t\t\terror = SET_ERROR(EINVAL);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (vdev_dtl_required(vml[c]) ||\n\t\t    vdev_resilver_needed(vml[c], NULL, NULL)) {\n\t\t\terror = SET_ERROR(EBUSY);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tfnvlist_add_uint64(child[c], ZPOOL_CONFIG_METASLAB_ARRAY,\n\t\t    vml[c]->vdev_top->vdev_ms_array);\n\t\tfnvlist_add_uint64(child[c], ZPOOL_CONFIG_METASLAB_SHIFT,\n\t\t    vml[c]->vdev_top->vdev_ms_shift);\n\t\tfnvlist_add_uint64(child[c], ZPOOL_CONFIG_ASIZE,\n\t\t    vml[c]->vdev_top->vdev_asize);\n\t\tfnvlist_add_uint64(child[c], ZPOOL_CONFIG_ASHIFT,\n\t\t    vml[c]->vdev_top->vdev_ashift);\n\n\t\t \n\t\tASSERT3U(vml[c]->vdev_leaf_zap, !=, 0);\n\t\tVERIFY0(nvlist_add_uint64(child[c],\n\t\t    ZPOOL_CONFIG_VDEV_LEAF_ZAP, vml[c]->vdev_leaf_zap));\n\n\t\tASSERT3U(vml[c]->vdev_top->vdev_top_zap, !=, 0);\n\t\tVERIFY0(nvlist_add_uint64(child[c],\n\t\t    ZPOOL_CONFIG_VDEV_TOP_ZAP,\n\t\t    vml[c]->vdev_parent->vdev_top_zap));\n\t}\n\n\tif (error != 0) {\n\t\tkmem_free(vml, children * sizeof (vdev_t *));\n\t\tkmem_free(glist, children * sizeof (uint64_t));\n\t\treturn (spa_vdev_exit(spa, NULL, txg, error));\n\t}\n\n\t \n\tfor (c = 0; c < children; c++) {\n\t\tif (vml[c] != NULL)\n\t\t\tvml[c]->vdev_offline = B_TRUE;\n\t}\n\tvdev_reopen(spa->spa_root_vdev);\n\n\t \n\tnvl = fnvlist_alloc();\n\tfnvlist_add_uint64_array(nvl, ZPOOL_CONFIG_SPLIT_LIST, glist, children);\n\tkmem_free(glist, children * sizeof (uint64_t));\n\n\tmutex_enter(&spa->spa_props_lock);\n\tfnvlist_add_nvlist(spa->spa_config, ZPOOL_CONFIG_SPLIT, nvl);\n\tmutex_exit(&spa->spa_props_lock);\n\tspa->spa_config_splitting = nvl;\n\tvdev_config_dirty(spa->spa_root_vdev);\n\n\t \n\tfnvlist_add_string(config, ZPOOL_CONFIG_POOL_NAME, newname);\n\tfnvlist_add_uint64(config, ZPOOL_CONFIG_POOL_STATE,\n\t    exp ? POOL_STATE_EXPORTED : POOL_STATE_ACTIVE);\n\tfnvlist_add_uint64(config, ZPOOL_CONFIG_VERSION, spa_version(spa));\n\tfnvlist_add_uint64(config, ZPOOL_CONFIG_POOL_TXG, spa->spa_config_txg);\n\tfnvlist_add_uint64(config, ZPOOL_CONFIG_POOL_GUID,\n\t    spa_generate_guid(NULL));\n\tVERIFY0(nvlist_add_boolean(config, ZPOOL_CONFIG_HAS_PER_VDEV_ZAPS));\n\t(void) nvlist_lookup_string(props,\n\t    zpool_prop_to_name(ZPOOL_PROP_ALTROOT), &altroot);\n\n\t \n\tnewspa = spa_add(newname, config, altroot);\n\tnewspa->spa_avz_action = AVZ_ACTION_REBUILD;\n\tnewspa->spa_config_txg = spa->spa_config_txg;\n\tspa_set_log_state(newspa, SPA_LOG_CLEAR);\n\n\t \n\tspa_vdev_config_exit(spa, NULL, txg, 0, FTAG);\n\n\tif (zio_injection_enabled)\n\t\tzio_handle_panic_injection(spa, FTAG, 1);\n\n\tspa_activate(newspa, spa_mode_global);\n\tspa_async_suspend(newspa);\n\n\t \n\tlist_t vd_initialize_list;\n\tlist_create(&vd_initialize_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_initialize_node));\n\n\tlist_t vd_trim_list;\n\tlist_create(&vd_trim_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_trim_node));\n\n\tfor (c = 0; c < children; c++) {\n\t\tif (vml[c] != NULL && vml[c]->vdev_ops != &vdev_indirect_ops) {\n\t\t\tmutex_enter(&vml[c]->vdev_initialize_lock);\n\t\t\tvdev_initialize_stop(vml[c],\n\t\t\t    VDEV_INITIALIZE_ACTIVE, &vd_initialize_list);\n\t\t\tmutex_exit(&vml[c]->vdev_initialize_lock);\n\n\t\t\tmutex_enter(&vml[c]->vdev_trim_lock);\n\t\t\tvdev_trim_stop(vml[c], VDEV_TRIM_ACTIVE, &vd_trim_list);\n\t\t\tmutex_exit(&vml[c]->vdev_trim_lock);\n\t\t}\n\t}\n\n\tvdev_initialize_stop_wait(spa, &vd_initialize_list);\n\tvdev_trim_stop_wait(spa, &vd_trim_list);\n\n\tlist_destroy(&vd_initialize_list);\n\tlist_destroy(&vd_trim_list);\n\n\tnewspa->spa_config_source = SPA_CONFIG_SRC_SPLIT;\n\tnewspa->spa_is_splitting = B_TRUE;\n\n\t \n\terror = spa_load(newspa, SPA_LOAD_IMPORT, SPA_IMPORT_ASSEMBLE);\n\tif (error)\n\t\tgoto out;\n\n\t \n\tif (newspa->spa_root_vdev != NULL) {\n\t\tnewspa->spa_config_splitting = fnvlist_alloc();\n\t\tfnvlist_add_uint64(newspa->spa_config_splitting,\n\t\t    ZPOOL_CONFIG_SPLIT_GUID, spa_guid(spa));\n\t\tspa_config_set(newspa, spa_config_generate(newspa, NULL, -1ULL,\n\t\t    B_TRUE));\n\t}\n\n\t \n\tif (props != NULL) {\n\t\tspa_configfile_set(newspa, props, B_FALSE);\n\t\terror = spa_prop_set(newspa, props);\n\t\tif (error)\n\t\t\tgoto out;\n\t}\n\n\t \n\ttxg = spa_vdev_config_enter(newspa);\n\tvdev_config_dirty(newspa->spa_root_vdev);\n\t(void) spa_vdev_config_exit(newspa, NULL, txg, 0, FTAG);\n\n\tif (zio_injection_enabled)\n\t\tzio_handle_panic_injection(spa, FTAG, 2);\n\n\tspa_async_resume(newspa);\n\n\t \n\ttxg = spa_vdev_config_enter(spa);\n\ttx = dmu_tx_create_dd(spa_get_dsl(spa)->dp_mos_dir);\n\terror = dmu_tx_assign(tx, TXG_WAIT);\n\tif (error != 0)\n\t\tdmu_tx_abort(tx);\n\tfor (c = 0; c < children; c++) {\n\t\tif (vml[c] != NULL && vml[c]->vdev_ops != &vdev_indirect_ops) {\n\t\t\tvdev_t *tvd = vml[c]->vdev_top;\n\n\t\t\t \n\t\t\tfor (int t = 0; t < TXG_SIZE; t++) {\n\t\t\t\t(void) txg_list_remove_this(\n\t\t\t\t    &tvd->vdev_dtl_list, vml[c], t);\n\t\t\t}\n\n\t\t\tvdev_split(vml[c]);\n\t\t\tif (error == 0)\n\t\t\t\tspa_history_log_internal(spa, \"detach\", tx,\n\t\t\t\t    \"vdev=%s\", vml[c]->vdev_path);\n\n\t\t\tvdev_free(vml[c]);\n\t\t}\n\t}\n\tspa->spa_avz_action = AVZ_ACTION_REBUILD;\n\tvdev_config_dirty(spa->spa_root_vdev);\n\tspa->spa_config_splitting = NULL;\n\tnvlist_free(nvl);\n\tif (error == 0)\n\t\tdmu_tx_commit(tx);\n\t(void) spa_vdev_exit(spa, NULL, txg, 0);\n\n\tif (zio_injection_enabled)\n\t\tzio_handle_panic_injection(spa, FTAG, 3);\n\n\t \n\tspa_history_log_internal(newspa, \"split\", NULL,\n\t    \"from pool %s\", spa_name(spa));\n\n\tnewspa->spa_is_splitting = B_FALSE;\n\tkmem_free(vml, children * sizeof (vdev_t *));\n\n\t \n\tif (exp)\n\t\terror = spa_export_common(newname, POOL_STATE_EXPORTED, NULL,\n\t\t    B_FALSE, B_FALSE);\n\n\treturn (error);\n\nout:\n\tspa_unload(newspa);\n\tspa_deactivate(newspa);\n\tspa_remove(newspa);\n\n\ttxg = spa_vdev_config_enter(spa);\n\n\t \n\tfor (c = 0; c < children; c++) {\n\t\tif (vml[c] != NULL)\n\t\t\tvml[c]->vdev_offline = B_FALSE;\n\t}\n\n\t \n\tspa_async_request(spa, SPA_ASYNC_INITIALIZE_RESTART);\n\tspa_async_request(spa, SPA_ASYNC_TRIM_RESTART);\n\tspa_async_request(spa, SPA_ASYNC_AUTOTRIM_RESTART);\n\n\tvdev_reopen(spa->spa_root_vdev);\n\n\tnvlist_free(spa->spa_config_splitting);\n\tspa->spa_config_splitting = NULL;\n\t(void) spa_vdev_exit(spa, NULL, txg, error);\n\n\tkmem_free(vml, children * sizeof (vdev_t *));\n\treturn (error);\n}\n\n \nstatic vdev_t *\nspa_vdev_resilver_done_hunt(vdev_t *vd)\n{\n\tvdev_t *newvd, *oldvd;\n\n\tfor (int c = 0; c < vd->vdev_children; c++) {\n\t\toldvd = spa_vdev_resilver_done_hunt(vd->vdev_child[c]);\n\t\tif (oldvd != NULL)\n\t\t\treturn (oldvd);\n\t}\n\n\t \n\tif (vd->vdev_ops == &vdev_replacing_ops) {\n\t\tASSERT(vd->vdev_children > 1);\n\n\t\tnewvd = vd->vdev_child[vd->vdev_children - 1];\n\t\toldvd = vd->vdev_child[0];\n\n\t\tif (vdev_dtl_empty(newvd, DTL_MISSING) &&\n\t\t    vdev_dtl_empty(newvd, DTL_OUTAGE) &&\n\t\t    !vdev_dtl_required(oldvd))\n\t\t\treturn (oldvd);\n\t}\n\n\t \n\tif (vd->vdev_ops == &vdev_spare_ops) {\n\t\tvdev_t *first = vd->vdev_child[0];\n\t\tvdev_t *last = vd->vdev_child[vd->vdev_children - 1];\n\n\t\tif (last->vdev_unspare) {\n\t\t\toldvd = first;\n\t\t\tnewvd = last;\n\t\t} else if (first->vdev_unspare) {\n\t\t\toldvd = last;\n\t\t\tnewvd = first;\n\t\t} else {\n\t\t\toldvd = NULL;\n\t\t}\n\n\t\tif (oldvd != NULL &&\n\t\t    vdev_dtl_empty(newvd, DTL_MISSING) &&\n\t\t    vdev_dtl_empty(newvd, DTL_OUTAGE) &&\n\t\t    !vdev_dtl_required(oldvd))\n\t\t\treturn (oldvd);\n\n\t\tvdev_propagate_state(vd);\n\n\t\t \n\t\tif (vd->vdev_children > 2) {\n\t\t\tnewvd = vd->vdev_child[1];\n\n\t\t\tif (newvd->vdev_isspare && last->vdev_isspare &&\n\t\t\t    vdev_dtl_empty(last, DTL_MISSING) &&\n\t\t\t    vdev_dtl_empty(last, DTL_OUTAGE) &&\n\t\t\t    !vdev_dtl_required(newvd))\n\t\t\t\treturn (newvd);\n\t\t}\n\t}\n\n\treturn (NULL);\n}\n\nstatic void\nspa_vdev_resilver_done(spa_t *spa)\n{\n\tvdev_t *vd, *pvd, *ppvd;\n\tuint64_t guid, sguid, pguid, ppguid;\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\n\twhile ((vd = spa_vdev_resilver_done_hunt(spa->spa_root_vdev)) != NULL) {\n\t\tpvd = vd->vdev_parent;\n\t\tppvd = pvd->vdev_parent;\n\t\tguid = vd->vdev_guid;\n\t\tpguid = pvd->vdev_guid;\n\t\tppguid = ppvd->vdev_guid;\n\t\tsguid = 0;\n\t\t \n\t\tif (ppvd->vdev_ops == &vdev_spare_ops && pvd->vdev_id == 0 &&\n\t\t    ppvd->vdev_children == 2) {\n\t\t\tASSERT(pvd->vdev_ops == &vdev_replacing_ops);\n\t\t\tsguid = ppvd->vdev_child[1]->vdev_guid;\n\t\t}\n\t\tASSERT(vd->vdev_resilver_txg == 0 || !vdev_dtl_required(vd));\n\n\t\tspa_config_exit(spa, SCL_ALL, FTAG);\n\t\tif (spa_vdev_detach(spa, guid, pguid, B_TRUE) != 0)\n\t\t\treturn;\n\t\tif (sguid && spa_vdev_detach(spa, sguid, ppguid, B_TRUE) != 0)\n\t\t\treturn;\n\t\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\t}\n\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\t \n\tspa_notify_waiters(spa);\n}\n\n \nstatic int\nspa_vdev_set_common(spa_t *spa, uint64_t guid, const char *value,\n    boolean_t ispath)\n{\n\tvdev_t *vd;\n\tboolean_t sync = B_FALSE;\n\n\tASSERT(spa_writeable(spa));\n\n\tspa_vdev_state_enter(spa, SCL_ALL);\n\n\tif ((vd = spa_lookup_by_guid(spa, guid, B_TRUE)) == NULL)\n\t\treturn (spa_vdev_state_exit(spa, NULL, ENOENT));\n\n\tif (!vd->vdev_ops->vdev_op_leaf)\n\t\treturn (spa_vdev_state_exit(spa, NULL, ENOTSUP));\n\n\tif (ispath) {\n\t\tif (strcmp(value, vd->vdev_path) != 0) {\n\t\t\tspa_strfree(vd->vdev_path);\n\t\t\tvd->vdev_path = spa_strdup(value);\n\t\t\tsync = B_TRUE;\n\t\t}\n\t} else {\n\t\tif (vd->vdev_fru == NULL) {\n\t\t\tvd->vdev_fru = spa_strdup(value);\n\t\t\tsync = B_TRUE;\n\t\t} else if (strcmp(value, vd->vdev_fru) != 0) {\n\t\t\tspa_strfree(vd->vdev_fru);\n\t\t\tvd->vdev_fru = spa_strdup(value);\n\t\t\tsync = B_TRUE;\n\t\t}\n\t}\n\n\treturn (spa_vdev_state_exit(spa, sync ? vd : NULL, 0));\n}\n\nint\nspa_vdev_setpath(spa_t *spa, uint64_t guid, const char *newpath)\n{\n\treturn (spa_vdev_set_common(spa, guid, newpath, B_TRUE));\n}\n\nint\nspa_vdev_setfru(spa_t *spa, uint64_t guid, const char *newfru)\n{\n\treturn (spa_vdev_set_common(spa, guid, newfru, B_FALSE));\n}\n\n \nint\nspa_scrub_pause_resume(spa_t *spa, pool_scrub_cmd_t cmd)\n{\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == 0);\n\n\tif (dsl_scan_resilvering(spa->spa_dsl_pool))\n\t\treturn (SET_ERROR(EBUSY));\n\n\treturn (dsl_scrub_set_pause_resume(spa->spa_dsl_pool, cmd));\n}\n\nint\nspa_scan_stop(spa_t *spa)\n{\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == 0);\n\tif (dsl_scan_resilvering(spa->spa_dsl_pool))\n\t\treturn (SET_ERROR(EBUSY));\n\n\treturn (dsl_scan_cancel(spa->spa_dsl_pool));\n}\n\nint\nspa_scan(spa_t *spa, pool_scan_func_t func)\n{\n\tASSERT(spa_config_held(spa, SCL_ALL, RW_WRITER) == 0);\n\n\tif (func >= POOL_SCAN_FUNCS || func == POOL_SCAN_NONE)\n\t\treturn (SET_ERROR(ENOTSUP));\n\n\tif (func == POOL_SCAN_RESILVER &&\n\t    !spa_feature_is_enabled(spa, SPA_FEATURE_RESILVER_DEFER))\n\t\treturn (SET_ERROR(ENOTSUP));\n\n\t \n\tif (func == POOL_SCAN_RESILVER &&\n\t    !vdev_resilver_needed(spa->spa_root_vdev, NULL, NULL)) {\n\t\tspa_async_request(spa, SPA_ASYNC_RESILVER_DONE);\n\t\treturn (0);\n\t}\n\n\tif (func == POOL_SCAN_ERRORSCRUB &&\n\t    !spa_feature_is_enabled(spa, SPA_FEATURE_HEAD_ERRLOG))\n\t\treturn (SET_ERROR(ENOTSUP));\n\n\treturn (dsl_scan(spa->spa_dsl_pool, func));\n}\n\n \n\nstatic void\nspa_async_remove(spa_t *spa, vdev_t *vd)\n{\n\tif (vd->vdev_remove_wanted) {\n\t\tvd->vdev_remove_wanted = B_FALSE;\n\t\tvd->vdev_delayed_close = B_FALSE;\n\t\tvdev_set_state(vd, B_FALSE, VDEV_STATE_REMOVED, VDEV_AUX_NONE);\n\n\t\t \n\t\tvd->vdev_stat.vs_read_errors = 0;\n\t\tvd->vdev_stat.vs_write_errors = 0;\n\t\tvd->vdev_stat.vs_checksum_errors = 0;\n\n\t\tvdev_state_dirty(vd->vdev_top);\n\n\t\t \n\t\tzfs_post_remove(spa, vd);\n\t}\n\n\tfor (int c = 0; c < vd->vdev_children; c++)\n\t\tspa_async_remove(spa, vd->vdev_child[c]);\n}\n\nstatic void\nspa_async_probe(spa_t *spa, vdev_t *vd)\n{\n\tif (vd->vdev_probe_wanted) {\n\t\tvd->vdev_probe_wanted = B_FALSE;\n\t\tvdev_reopen(vd);\t \n\t}\n\n\tfor (int c = 0; c < vd->vdev_children; c++)\n\t\tspa_async_probe(spa, vd->vdev_child[c]);\n}\n\nstatic void\nspa_async_autoexpand(spa_t *spa, vdev_t *vd)\n{\n\tif (!spa->spa_autoexpand)\n\t\treturn;\n\n\tfor (int c = 0; c < vd->vdev_children; c++) {\n\t\tvdev_t *cvd = vd->vdev_child[c];\n\t\tspa_async_autoexpand(spa, cvd);\n\t}\n\n\tif (!vd->vdev_ops->vdev_op_leaf || vd->vdev_physpath == NULL)\n\t\treturn;\n\n\tspa_event_notify(vd->vdev_spa, vd, NULL, ESC_ZFS_VDEV_AUTOEXPAND);\n}\n\nstatic __attribute__((noreturn)) void\nspa_async_thread(void *arg)\n{\n\tspa_t *spa = (spa_t *)arg;\n\tdsl_pool_t *dp = spa->spa_dsl_pool;\n\tint tasks;\n\n\tASSERT(spa->spa_sync_on);\n\n\tmutex_enter(&spa->spa_async_lock);\n\ttasks = spa->spa_async_tasks;\n\tspa->spa_async_tasks = 0;\n\tmutex_exit(&spa->spa_async_lock);\n\n\t \n\tif (tasks & SPA_ASYNC_CONFIG_UPDATE) {\n\t\tuint64_t old_space, new_space;\n\n\t\tmutex_enter(&spa_namespace_lock);\n\t\told_space = metaslab_class_get_space(spa_normal_class(spa));\n\t\told_space += metaslab_class_get_space(spa_special_class(spa));\n\t\told_space += metaslab_class_get_space(spa_dedup_class(spa));\n\t\told_space += metaslab_class_get_space(\n\t\t    spa_embedded_log_class(spa));\n\n\t\tspa_config_update(spa, SPA_CONFIG_UPDATE_POOL);\n\n\t\tnew_space = metaslab_class_get_space(spa_normal_class(spa));\n\t\tnew_space += metaslab_class_get_space(spa_special_class(spa));\n\t\tnew_space += metaslab_class_get_space(spa_dedup_class(spa));\n\t\tnew_space += metaslab_class_get_space(\n\t\t    spa_embedded_log_class(spa));\n\t\tmutex_exit(&spa_namespace_lock);\n\n\t\t \n\t\tif (new_space != old_space) {\n\t\t\tspa_history_log_internal(spa, \"vdev online\", NULL,\n\t\t\t    \"pool '%s' size: %llu(+%llu)\",\n\t\t\t    spa_name(spa), (u_longlong_t)new_space,\n\t\t\t    (u_longlong_t)(new_space - old_space));\n\t\t}\n\t}\n\n\t \n\tif (tasks & SPA_ASYNC_REMOVE) {\n\t\tspa_vdev_state_enter(spa, SCL_NONE);\n\t\tspa_async_remove(spa, spa->spa_root_vdev);\n\t\tfor (int i = 0; i < spa->spa_l2cache.sav_count; i++)\n\t\t\tspa_async_remove(spa, spa->spa_l2cache.sav_vdevs[i]);\n\t\tfor (int i = 0; i < spa->spa_spares.sav_count; i++)\n\t\t\tspa_async_remove(spa, spa->spa_spares.sav_vdevs[i]);\n\t\t(void) spa_vdev_state_exit(spa, NULL, 0);\n\t}\n\n\tif ((tasks & SPA_ASYNC_AUTOEXPAND) && !spa_suspended(spa)) {\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tspa_async_autoexpand(spa, spa->spa_root_vdev);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t}\n\n\t \n\tif (tasks & SPA_ASYNC_PROBE) {\n\t\tspa_vdev_state_enter(spa, SCL_NONE);\n\t\tspa_async_probe(spa, spa->spa_root_vdev);\n\t\t(void) spa_vdev_state_exit(spa, NULL, 0);\n\t}\n\n\t \n\tif (tasks & SPA_ASYNC_RESILVER_DONE ||\n\t    tasks & SPA_ASYNC_REBUILD_DONE ||\n\t    tasks & SPA_ASYNC_DETACH_SPARE) {\n\t\tspa_vdev_resilver_done(spa);\n\t}\n\n\t \n\tif (tasks & SPA_ASYNC_RESILVER &&\n\t    !vdev_rebuild_active(spa->spa_root_vdev) &&\n\t    (!dsl_scan_resilvering(dp) ||\n\t    !spa_feature_is_enabled(dp->dp_spa, SPA_FEATURE_RESILVER_DEFER)))\n\t\tdsl_scan_restart_resilver(dp, 0);\n\n\tif (tasks & SPA_ASYNC_INITIALIZE_RESTART) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tvdev_initialize_restart(spa->spa_root_vdev);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\tif (tasks & SPA_ASYNC_TRIM_RESTART) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tvdev_trim_restart(spa->spa_root_vdev);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\tif (tasks & SPA_ASYNC_AUTOTRIM_RESTART) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tvdev_autotrim_restart(spa);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\t \n\tif (tasks & SPA_ASYNC_L2CACHE_TRIM) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\t\tvdev_trim_l2arc(spa);\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\t \n\tif (tasks & SPA_ASYNC_L2CACHE_REBUILD) {\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_config_enter(spa, SCL_L2ARC, FTAG, RW_READER);\n\t\tl2arc_spa_rebuild_start(spa);\n\t\tspa_config_exit(spa, SCL_L2ARC, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t}\n\n\t \n\tmutex_enter(&spa->spa_async_lock);\n\tspa->spa_async_thread = NULL;\n\tcv_broadcast(&spa->spa_async_cv);\n\tmutex_exit(&spa->spa_async_lock);\n\tthread_exit();\n}\n\nvoid\nspa_async_suspend(spa_t *spa)\n{\n\tmutex_enter(&spa->spa_async_lock);\n\tspa->spa_async_suspended++;\n\twhile (spa->spa_async_thread != NULL)\n\t\tcv_wait(&spa->spa_async_cv, &spa->spa_async_lock);\n\tmutex_exit(&spa->spa_async_lock);\n\n\tspa_vdev_remove_suspend(spa);\n\n\tzthr_t *condense_thread = spa->spa_condense_zthr;\n\tif (condense_thread != NULL)\n\t\tzthr_cancel(condense_thread);\n\n\tzthr_t *discard_thread = spa->spa_checkpoint_discard_zthr;\n\tif (discard_thread != NULL)\n\t\tzthr_cancel(discard_thread);\n\n\tzthr_t *ll_delete_thread = spa->spa_livelist_delete_zthr;\n\tif (ll_delete_thread != NULL)\n\t\tzthr_cancel(ll_delete_thread);\n\n\tzthr_t *ll_condense_thread = spa->spa_livelist_condense_zthr;\n\tif (ll_condense_thread != NULL)\n\t\tzthr_cancel(ll_condense_thread);\n}\n\nvoid\nspa_async_resume(spa_t *spa)\n{\n\tmutex_enter(&spa->spa_async_lock);\n\tASSERT(spa->spa_async_suspended != 0);\n\tspa->spa_async_suspended--;\n\tmutex_exit(&spa->spa_async_lock);\n\tspa_restart_removal(spa);\n\n\tzthr_t *condense_thread = spa->spa_condense_zthr;\n\tif (condense_thread != NULL)\n\t\tzthr_resume(condense_thread);\n\n\tzthr_t *discard_thread = spa->spa_checkpoint_discard_zthr;\n\tif (discard_thread != NULL)\n\t\tzthr_resume(discard_thread);\n\n\tzthr_t *ll_delete_thread = spa->spa_livelist_delete_zthr;\n\tif (ll_delete_thread != NULL)\n\t\tzthr_resume(ll_delete_thread);\n\n\tzthr_t *ll_condense_thread = spa->spa_livelist_condense_zthr;\n\tif (ll_condense_thread != NULL)\n\t\tzthr_resume(ll_condense_thread);\n}\n\nstatic boolean_t\nspa_async_tasks_pending(spa_t *spa)\n{\n\tuint_t non_config_tasks;\n\tuint_t config_task;\n\tboolean_t config_task_suspended;\n\n\tnon_config_tasks = spa->spa_async_tasks & ~SPA_ASYNC_CONFIG_UPDATE;\n\tconfig_task = spa->spa_async_tasks & SPA_ASYNC_CONFIG_UPDATE;\n\tif (spa->spa_ccw_fail_time == 0) {\n\t\tconfig_task_suspended = B_FALSE;\n\t} else {\n\t\tconfig_task_suspended =\n\t\t    (gethrtime() - spa->spa_ccw_fail_time) <\n\t\t    ((hrtime_t)zfs_ccw_retry_interval * NANOSEC);\n\t}\n\n\treturn (non_config_tasks || (config_task && !config_task_suspended));\n}\n\nstatic void\nspa_async_dispatch(spa_t *spa)\n{\n\tmutex_enter(&spa->spa_async_lock);\n\tif (spa_async_tasks_pending(spa) &&\n\t    !spa->spa_async_suspended &&\n\t    spa->spa_async_thread == NULL)\n\t\tspa->spa_async_thread = thread_create(NULL, 0,\n\t\t    spa_async_thread, spa, 0, &p0, TS_RUN, maxclsyspri);\n\tmutex_exit(&spa->spa_async_lock);\n}\n\nvoid\nspa_async_request(spa_t *spa, int task)\n{\n\tzfs_dbgmsg(\"spa=%s async request task=%u\", spa->spa_name, task);\n\tmutex_enter(&spa->spa_async_lock);\n\tspa->spa_async_tasks |= task;\n\tmutex_exit(&spa->spa_async_lock);\n}\n\nint\nspa_async_tasks(spa_t *spa)\n{\n\treturn (spa->spa_async_tasks);\n}\n\n \n\n\nstatic int\nbpobj_enqueue_cb(void *arg, const blkptr_t *bp, boolean_t bp_freed,\n    dmu_tx_t *tx)\n{\n\tbpobj_t *bpo = arg;\n\tbpobj_enqueue(bpo, bp, bp_freed, tx);\n\treturn (0);\n}\n\nint\nbpobj_enqueue_alloc_cb(void *arg, const blkptr_t *bp, dmu_tx_t *tx)\n{\n\treturn (bpobj_enqueue_cb(arg, bp, B_FALSE, tx));\n}\n\nint\nbpobj_enqueue_free_cb(void *arg, const blkptr_t *bp, dmu_tx_t *tx)\n{\n\treturn (bpobj_enqueue_cb(arg, bp, B_TRUE, tx));\n}\n\nstatic int\nspa_free_sync_cb(void *arg, const blkptr_t *bp, dmu_tx_t *tx)\n{\n\tzio_t *pio = arg;\n\n\tzio_nowait(zio_free_sync(pio, pio->io_spa, dmu_tx_get_txg(tx), bp,\n\t    pio->io_flags));\n\treturn (0);\n}\n\nstatic int\nbpobj_spa_free_sync_cb(void *arg, const blkptr_t *bp, boolean_t bp_freed,\n    dmu_tx_t *tx)\n{\n\tASSERT(!bp_freed);\n\treturn (spa_free_sync_cb(arg, bp, tx));\n}\n\n \nstatic void\nspa_sync_frees(spa_t *spa, bplist_t *bpl, dmu_tx_t *tx)\n{\n\tzio_t *zio = zio_root(spa, NULL, NULL, 0);\n\tbplist_iterate(bpl, spa_free_sync_cb, zio, tx);\n\tVERIFY(zio_wait(zio) == 0);\n}\n\n \nstatic void\nspa_sync_deferred_frees(spa_t *spa, dmu_tx_t *tx)\n{\n\tif (spa_sync_pass(spa) != 1)\n\t\treturn;\n\n\t \n\tzio_t *zio = zio_root(spa, NULL, NULL, 0);\n\tVERIFY3U(bpobj_iterate(&spa->spa_deferred_bpobj,\n\t    bpobj_spa_free_sync_cb, zio, tx), ==, 0);\n\tVERIFY0(zio_wait(zio));\n}\n\nstatic void\nspa_sync_nvlist(spa_t *spa, uint64_t obj, nvlist_t *nv, dmu_tx_t *tx)\n{\n\tchar *packed = NULL;\n\tsize_t bufsize;\n\tsize_t nvsize = 0;\n\tdmu_buf_t *db;\n\n\tVERIFY(nvlist_size(nv, &nvsize, NV_ENCODE_XDR) == 0);\n\n\t \n\tbufsize = P2ROUNDUP((uint64_t)nvsize, SPA_CONFIG_BLOCKSIZE);\n\tpacked = vmem_alloc(bufsize, KM_SLEEP);\n\n\tVERIFY(nvlist_pack(nv, &packed, &nvsize, NV_ENCODE_XDR,\n\t    KM_SLEEP) == 0);\n\tmemset(packed + nvsize, 0, bufsize - nvsize);\n\n\tdmu_write(spa->spa_meta_objset, obj, 0, bufsize, packed, tx);\n\n\tvmem_free(packed, bufsize);\n\n\tVERIFY(0 == dmu_bonus_hold(spa->spa_meta_objset, obj, FTAG, &db));\n\tdmu_buf_will_dirty(db, tx);\n\t*(uint64_t *)db->db_data = nvsize;\n\tdmu_buf_rele(db, FTAG);\n}\n\nstatic void\nspa_sync_aux_dev(spa_t *spa, spa_aux_vdev_t *sav, dmu_tx_t *tx,\n    const char *config, const char *entry)\n{\n\tnvlist_t *nvroot;\n\tnvlist_t **list;\n\tint i;\n\n\tif (!sav->sav_sync)\n\t\treturn;\n\n\t \n\tif (sav->sav_object == 0) {\n\t\tsav->sav_object = dmu_object_alloc(spa->spa_meta_objset,\n\t\t    DMU_OT_PACKED_NVLIST, 1 << 14, DMU_OT_PACKED_NVLIST_SIZE,\n\t\t    sizeof (uint64_t), tx);\n\t\tVERIFY(zap_update(spa->spa_meta_objset,\n\t\t    DMU_POOL_DIRECTORY_OBJECT, entry, sizeof (uint64_t), 1,\n\t\t    &sav->sav_object, tx) == 0);\n\t}\n\n\tnvroot = fnvlist_alloc();\n\tif (sav->sav_count == 0) {\n\t\tfnvlist_add_nvlist_array(nvroot, config,\n\t\t    (const nvlist_t * const *)NULL, 0);\n\t} else {\n\t\tlist = kmem_alloc(sav->sav_count*sizeof (void *), KM_SLEEP);\n\t\tfor (i = 0; i < sav->sav_count; i++)\n\t\t\tlist[i] = vdev_config_generate(spa, sav->sav_vdevs[i],\n\t\t\t    B_FALSE, VDEV_CONFIG_L2CACHE);\n\t\tfnvlist_add_nvlist_array(nvroot, config,\n\t\t    (const nvlist_t * const *)list, sav->sav_count);\n\t\tfor (i = 0; i < sav->sav_count; i++)\n\t\t\tnvlist_free(list[i]);\n\t\tkmem_free(list, sav->sav_count * sizeof (void *));\n\t}\n\n\tspa_sync_nvlist(spa, sav->sav_object, nvroot, tx);\n\tnvlist_free(nvroot);\n\n\tsav->sav_sync = B_FALSE;\n}\n\n \nstatic void\nspa_avz_build(vdev_t *vd, uint64_t avz, dmu_tx_t *tx)\n{\n\tspa_t *spa = vd->vdev_spa;\n\n\tif (vd->vdev_root_zap != 0 &&\n\t    spa_feature_is_active(spa, SPA_FEATURE_AVZ_V2)) {\n\t\tVERIFY0(zap_add_int(spa->spa_meta_objset, avz,\n\t\t    vd->vdev_root_zap, tx));\n\t}\n\tif (vd->vdev_top_zap != 0) {\n\t\tVERIFY0(zap_add_int(spa->spa_meta_objset, avz,\n\t\t    vd->vdev_top_zap, tx));\n\t}\n\tif (vd->vdev_leaf_zap != 0) {\n\t\tVERIFY0(zap_add_int(spa->spa_meta_objset, avz,\n\t\t    vd->vdev_leaf_zap, tx));\n\t}\n\tfor (uint64_t i = 0; i < vd->vdev_children; i++) {\n\t\tspa_avz_build(vd->vdev_child[i], avz, tx);\n\t}\n}\n\nstatic void\nspa_sync_config_object(spa_t *spa, dmu_tx_t *tx)\n{\n\tnvlist_t *config;\n\n\t \n\tif (list_is_empty(&spa->spa_config_dirty_list) &&\n\t    spa->spa_avz_action == AVZ_ACTION_NONE)\n\t\treturn;\n\n\tspa_config_enter(spa, SCL_STATE, FTAG, RW_READER);\n\n\tASSERT(spa->spa_avz_action == AVZ_ACTION_NONE ||\n\t    spa->spa_avz_action == AVZ_ACTION_INITIALIZE ||\n\t    spa->spa_all_vdev_zaps != 0);\n\n\tif (spa->spa_avz_action == AVZ_ACTION_REBUILD) {\n\t\t \n\t\tuint64_t new_avz = zap_create(spa->spa_meta_objset,\n\t\t    DMU_OTN_ZAP_METADATA, DMU_OT_NONE, 0, tx);\n\t\tspa_avz_build(spa->spa_root_vdev, new_avz, tx);\n\n\t\t \n\t\tzap_cursor_t zc;\n\t\tzap_attribute_t za;\n\n\t\tfor (zap_cursor_init(&zc, spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps);\n\t\t    zap_cursor_retrieve(&zc, &za) == 0;\n\t\t    zap_cursor_advance(&zc)) {\n\t\t\tuint64_t vdzap = za.za_first_integer;\n\t\t\tif (zap_lookup_int(spa->spa_meta_objset, new_avz,\n\t\t\t    vdzap) == ENOENT) {\n\t\t\t\t \n\t\t\t\tVERIFY0(zap_destroy(spa->spa_meta_objset, vdzap,\n\t\t\t\t    tx));\n\t\t\t}\n\t\t}\n\n\t\tzap_cursor_fini(&zc);\n\n\t\t \n\t\tVERIFY0(zap_destroy(spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps, tx));\n\n\t\t \n\t\tVERIFY0(zap_update(spa->spa_meta_objset,\n\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_VDEV_ZAP_MAP,\n\t\t    sizeof (new_avz), 1, &new_avz, tx));\n\n\t\tspa->spa_all_vdev_zaps = new_avz;\n\t} else if (spa->spa_avz_action == AVZ_ACTION_DESTROY) {\n\t\tzap_cursor_t zc;\n\t\tzap_attribute_t za;\n\n\t\t \n\t\tfor (zap_cursor_init(&zc, spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps);\n\t\t    zap_cursor_retrieve(&zc, &za) == 0;\n\t\t    zap_cursor_advance(&zc)) {\n\t\t\tuint64_t zap = za.za_first_integer;\n\t\t\tVERIFY0(zap_destroy(spa->spa_meta_objset, zap, tx));\n\t\t}\n\n\t\tzap_cursor_fini(&zc);\n\n\t\t \n\t\tVERIFY0(zap_destroy(spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps, tx));\n\t\tVERIFY0(zap_remove(spa->spa_meta_objset,\n\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_VDEV_ZAP_MAP, tx));\n\t\tspa->spa_all_vdev_zaps = 0;\n\t}\n\n\tif (spa->spa_all_vdev_zaps == 0) {\n\t\tspa->spa_all_vdev_zaps = zap_create_link(spa->spa_meta_objset,\n\t\t    DMU_OTN_ZAP_METADATA, DMU_POOL_DIRECTORY_OBJECT,\n\t\t    DMU_POOL_VDEV_ZAP_MAP, tx);\n\t}\n\tspa->spa_avz_action = AVZ_ACTION_NONE;\n\n\t \n\tvdev_construct_zaps(spa->spa_root_vdev, tx);\n\n\tconfig = spa_config_generate(spa, spa->spa_root_vdev,\n\t    dmu_tx_get_txg(tx), B_FALSE);\n\n\t \n\tif (spa->spa_ubsync.ub_version < spa->spa_uberblock.ub_version)\n\t\tfnvlist_add_uint64(config, ZPOOL_CONFIG_VERSION,\n\t\t    spa->spa_uberblock.ub_version);\n\n\tspa_config_exit(spa, SCL_STATE, FTAG);\n\n\tnvlist_free(spa->spa_config_syncing);\n\tspa->spa_config_syncing = config;\n\n\tspa_sync_nvlist(spa, spa->spa_config_object, config, tx);\n}\n\nstatic void\nspa_sync_version(void *arg, dmu_tx_t *tx)\n{\n\tuint64_t *versionp = arg;\n\tuint64_t version = *versionp;\n\tspa_t *spa = dmu_tx_pool(tx)->dp_spa;\n\n\t \n\tASSERT(tx->tx_txg != TXG_INITIAL);\n\n\tASSERT(SPA_VERSION_IS_SUPPORTED(version));\n\tASSERT(version >= spa_version(spa));\n\n\tspa->spa_uberblock.ub_version = version;\n\tvdev_config_dirty(spa->spa_root_vdev);\n\tspa_history_log_internal(spa, \"set\", tx, \"version=%lld\",\n\t    (longlong_t)version);\n}\n\n \nstatic void\nspa_sync_props(void *arg, dmu_tx_t *tx)\n{\n\tnvlist_t *nvp = arg;\n\tspa_t *spa = dmu_tx_pool(tx)->dp_spa;\n\tobjset_t *mos = spa->spa_meta_objset;\n\tnvpair_t *elem = NULL;\n\n\tmutex_enter(&spa->spa_props_lock);\n\n\twhile ((elem = nvlist_next_nvpair(nvp, elem))) {\n\t\tuint64_t intval;\n\t\tconst char *strval, *fname;\n\t\tzpool_prop_t prop;\n\t\tconst char *propname;\n\t\tconst char *elemname = nvpair_name(elem);\n\t\tzprop_type_t proptype;\n\t\tspa_feature_t fid;\n\n\t\tswitch (prop = zpool_name_to_prop(elemname)) {\n\t\tcase ZPOOL_PROP_VERSION:\n\t\t\tintval = fnvpair_value_uint64(elem);\n\t\t\t \n\t\t\tASSERT3U(spa_version(spa), >=, intval);\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_ALTROOT:\n\t\t\t \n\t\t\tASSERT(spa->spa_root != NULL);\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_READONLY:\n\t\tcase ZPOOL_PROP_CACHEFILE:\n\t\t\t \n\t\t\tbreak;\n\t\tcase ZPOOL_PROP_COMMENT:\n\t\t\tstrval = fnvpair_value_string(elem);\n\t\t\tif (spa->spa_comment != NULL)\n\t\t\t\tspa_strfree(spa->spa_comment);\n\t\t\tspa->spa_comment = spa_strdup(strval);\n\t\t\t \n\t\t\tif (tx->tx_txg != TXG_INITIAL) {\n\t\t\t\tvdev_config_dirty(spa->spa_root_vdev);\n\t\t\t\tspa_async_request(spa, SPA_ASYNC_CONFIG_UPDATE);\n\t\t\t}\n\t\t\tspa_history_log_internal(spa, \"set\", tx,\n\t\t\t    \"%s=%s\", elemname, strval);\n\t\t\tbreak;\n\t\tcase ZPOOL_PROP_COMPATIBILITY:\n\t\t\tstrval = fnvpair_value_string(elem);\n\t\t\tif (spa->spa_compatibility != NULL)\n\t\t\t\tspa_strfree(spa->spa_compatibility);\n\t\t\tspa->spa_compatibility = spa_strdup(strval);\n\t\t\t \n\t\t\tif (tx->tx_txg != TXG_INITIAL) {\n\t\t\t\tvdev_config_dirty(spa->spa_root_vdev);\n\t\t\t\tspa_async_request(spa, SPA_ASYNC_CONFIG_UPDATE);\n\t\t\t}\n\n\t\t\tspa_history_log_internal(spa, \"set\", tx,\n\t\t\t    \"%s=%s\", nvpair_name(elem), strval);\n\t\t\tbreak;\n\n\t\tcase ZPOOL_PROP_INVAL:\n\t\t\tif (zpool_prop_feature(elemname)) {\n\t\t\t\tfname = strchr(elemname, '@') + 1;\n\t\t\t\tVERIFY0(zfeature_lookup_name(fname, &fid));\n\n\t\t\t\tspa_feature_enable(spa, fid, tx);\n\t\t\t\tspa_history_log_internal(spa, \"set\", tx,\n\t\t\t\t    \"%s=enabled\", elemname);\n\t\t\t\tbreak;\n\t\t\t} else if (!zfs_prop_user(elemname)) {\n\t\t\t\tASSERT(zpool_prop_feature(elemname));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tzfs_fallthrough;\n\t\tdefault:\n\t\t\t \n\t\t\tif (spa->spa_pool_props_object == 0) {\n\t\t\t\tspa->spa_pool_props_object =\n\t\t\t\t    zap_create_link(mos, DMU_OT_POOL_PROPS,\n\t\t\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_PROPS,\n\t\t\t\t    tx);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (prop == ZPOOL_PROP_INVAL) {\n\t\t\t\tpropname = elemname;\n\t\t\t\tproptype = PROP_TYPE_STRING;\n\t\t\t} else {\n\t\t\t\tpropname = zpool_prop_to_name(prop);\n\t\t\t\tproptype = zpool_prop_get_type(prop);\n\t\t\t}\n\n\t\t\tif (nvpair_type(elem) == DATA_TYPE_STRING) {\n\t\t\t\tASSERT(proptype == PROP_TYPE_STRING);\n\t\t\t\tstrval = fnvpair_value_string(elem);\n\t\t\t\tVERIFY0(zap_update(mos,\n\t\t\t\t    spa->spa_pool_props_object, propname,\n\t\t\t\t    1, strlen(strval) + 1, strval, tx));\n\t\t\t\tspa_history_log_internal(spa, \"set\", tx,\n\t\t\t\t    \"%s=%s\", elemname, strval);\n\t\t\t} else if (nvpair_type(elem) == DATA_TYPE_UINT64) {\n\t\t\t\tintval = fnvpair_value_uint64(elem);\n\n\t\t\t\tif (proptype == PROP_TYPE_INDEX) {\n\t\t\t\t\tconst char *unused;\n\t\t\t\t\tVERIFY0(zpool_prop_index_to_string(\n\t\t\t\t\t    prop, intval, &unused));\n\t\t\t\t}\n\t\t\t\tVERIFY0(zap_update(mos,\n\t\t\t\t    spa->spa_pool_props_object, propname,\n\t\t\t\t    8, 1, &intval, tx));\n\t\t\t\tspa_history_log_internal(spa, \"set\", tx,\n\t\t\t\t    \"%s=%lld\", elemname,\n\t\t\t\t    (longlong_t)intval);\n\n\t\t\t\tswitch (prop) {\n\t\t\t\tcase ZPOOL_PROP_DELEGATION:\n\t\t\t\t\tspa->spa_delegation = intval;\n\t\t\t\t\tbreak;\n\t\t\t\tcase ZPOOL_PROP_BOOTFS:\n\t\t\t\t\tspa->spa_bootfs = intval;\n\t\t\t\t\tbreak;\n\t\t\t\tcase ZPOOL_PROP_FAILUREMODE:\n\t\t\t\t\tspa->spa_failmode = intval;\n\t\t\t\t\tbreak;\n\t\t\t\tcase ZPOOL_PROP_AUTOTRIM:\n\t\t\t\t\tspa->spa_autotrim = intval;\n\t\t\t\t\tspa_async_request(spa,\n\t\t\t\t\t    SPA_ASYNC_AUTOTRIM_RESTART);\n\t\t\t\t\tbreak;\n\t\t\t\tcase ZPOOL_PROP_AUTOEXPAND:\n\t\t\t\t\tspa->spa_autoexpand = intval;\n\t\t\t\t\tif (tx->tx_txg != TXG_INITIAL)\n\t\t\t\t\t\tspa_async_request(spa,\n\t\t\t\t\t\t    SPA_ASYNC_AUTOEXPAND);\n\t\t\t\t\tbreak;\n\t\t\t\tcase ZPOOL_PROP_MULTIHOST:\n\t\t\t\t\tspa->spa_multihost = intval;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tASSERT(0);  \n\t\t\t}\n\t\t}\n\n\t}\n\n\tmutex_exit(&spa->spa_props_lock);\n}\n\n \nstatic void\nspa_sync_upgrades(spa_t *spa, dmu_tx_t *tx)\n{\n\tif (spa_sync_pass(spa) != 1)\n\t\treturn;\n\n\tdsl_pool_t *dp = spa->spa_dsl_pool;\n\trrw_enter(&dp->dp_config_rwlock, RW_WRITER, FTAG);\n\n\tif (spa->spa_ubsync.ub_version < SPA_VERSION_ORIGIN &&\n\t    spa->spa_uberblock.ub_version >= SPA_VERSION_ORIGIN) {\n\t\tdsl_pool_create_origin(dp, tx);\n\n\t\t \n\t\tspa->spa_minref += 3;\n\t}\n\n\tif (spa->spa_ubsync.ub_version < SPA_VERSION_NEXT_CLONES &&\n\t    spa->spa_uberblock.ub_version >= SPA_VERSION_NEXT_CLONES) {\n\t\tdsl_pool_upgrade_clones(dp, tx);\n\t}\n\n\tif (spa->spa_ubsync.ub_version < SPA_VERSION_DIR_CLONES &&\n\t    spa->spa_uberblock.ub_version >= SPA_VERSION_DIR_CLONES) {\n\t\tdsl_pool_upgrade_dir_clones(dp, tx);\n\n\t\t \n\t\tspa->spa_minref += 3;\n\t}\n\n\tif (spa->spa_ubsync.ub_version < SPA_VERSION_FEATURES &&\n\t    spa->spa_uberblock.ub_version >= SPA_VERSION_FEATURES) {\n\t\tspa_feature_create_zap_objects(spa, tx);\n\t}\n\n\t \n\tif (spa->spa_uberblock.ub_version >= SPA_VERSION_FEATURES) {\n\t\tboolean_t lz4_en = spa_feature_is_enabled(spa,\n\t\t    SPA_FEATURE_LZ4_COMPRESS);\n\t\tboolean_t lz4_ac = spa_feature_is_active(spa,\n\t\t    SPA_FEATURE_LZ4_COMPRESS);\n\n\t\tif (lz4_en && !lz4_ac)\n\t\t\tspa_feature_incr(spa, SPA_FEATURE_LZ4_COMPRESS, tx);\n\t}\n\n\t \n\tif (zap_contains(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,\n\t    DMU_POOL_CHECKSUM_SALT) == ENOENT) {\n\t\tVERIFY0(zap_add(spa->spa_meta_objset,\n\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_CHECKSUM_SALT, 1,\n\t\t    sizeof (spa->spa_cksum_salt.zcs_bytes),\n\t\t    spa->spa_cksum_salt.zcs_bytes, tx));\n\t}\n\n\trrw_exit(&dp->dp_config_rwlock, FTAG);\n}\n\nstatic void\nvdev_indirect_state_sync_verify(vdev_t *vd)\n{\n\tvdev_indirect_mapping_t *vim __maybe_unused = vd->vdev_indirect_mapping;\n\tvdev_indirect_births_t *vib __maybe_unused = vd->vdev_indirect_births;\n\n\tif (vd->vdev_ops == &vdev_indirect_ops) {\n\t\tASSERT(vim != NULL);\n\t\tASSERT(vib != NULL);\n\t}\n\n\tuint64_t obsolete_sm_object = 0;\n\tASSERT0(vdev_obsolete_sm_object(vd, &obsolete_sm_object));\n\tif (obsolete_sm_object != 0) {\n\t\tASSERT(vd->vdev_obsolete_sm != NULL);\n\t\tASSERT(vd->vdev_removing ||\n\t\t    vd->vdev_ops == &vdev_indirect_ops);\n\t\tASSERT(vdev_indirect_mapping_num_entries(vim) > 0);\n\t\tASSERT(vdev_indirect_mapping_bytes_mapped(vim) > 0);\n\t\tASSERT3U(obsolete_sm_object, ==,\n\t\t    space_map_object(vd->vdev_obsolete_sm));\n\t\tASSERT3U(vdev_indirect_mapping_bytes_mapped(vim), >=,\n\t\t    space_map_allocated(vd->vdev_obsolete_sm));\n\t}\n\tASSERT(vd->vdev_obsolete_segments != NULL);\n\n\t \n\tASSERT0(range_tree_space(vd->vdev_obsolete_segments));\n}\n\n \nstatic void\nspa_sync_adjust_vdev_max_queue_depth(spa_t *spa)\n{\n\tASSERT(spa_writeable(spa));\n\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tuint32_t max_queue_depth = zfs_vdev_async_write_max_active *\n\t    zfs_vdev_queue_depth_pct / 100;\n\tmetaslab_class_t *normal = spa_normal_class(spa);\n\tmetaslab_class_t *special = spa_special_class(spa);\n\tmetaslab_class_t *dedup = spa_dedup_class(spa);\n\n\tuint64_t slots_per_allocator = 0;\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *tvd = rvd->vdev_child[c];\n\n\t\tmetaslab_group_t *mg = tvd->vdev_mg;\n\t\tif (mg == NULL || !metaslab_group_initialized(mg))\n\t\t\tcontinue;\n\n\t\tmetaslab_class_t *mc = mg->mg_class;\n\t\tif (mc != normal && mc != special && mc != dedup)\n\t\t\tcontinue;\n\n\t\t \n\t\tfor (int i = 0; i < mg->mg_allocators; i++) {\n\t\t\tASSERT0(zfs_refcount_count(\n\t\t\t    &(mg->mg_allocator[i].mga_alloc_queue_depth)));\n\t\t}\n\t\tmg->mg_max_alloc_queue_depth = max_queue_depth;\n\n\t\tfor (int i = 0; i < mg->mg_allocators; i++) {\n\t\t\tmg->mg_allocator[i].mga_cur_max_alloc_queue_depth =\n\t\t\t    zfs_vdev_def_queue_depth;\n\t\t}\n\t\tslots_per_allocator += zfs_vdev_def_queue_depth;\n\t}\n\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tASSERT0(zfs_refcount_count(&normal->mc_allocator[i].\n\t\t    mca_alloc_slots));\n\t\tASSERT0(zfs_refcount_count(&special->mc_allocator[i].\n\t\t    mca_alloc_slots));\n\t\tASSERT0(zfs_refcount_count(&dedup->mc_allocator[i].\n\t\t    mca_alloc_slots));\n\t\tnormal->mc_allocator[i].mca_alloc_max_slots =\n\t\t    slots_per_allocator;\n\t\tspecial->mc_allocator[i].mca_alloc_max_slots =\n\t\t    slots_per_allocator;\n\t\tdedup->mc_allocator[i].mca_alloc_max_slots =\n\t\t    slots_per_allocator;\n\t}\n\tnormal->mc_alloc_throttle_enabled = zio_dva_throttle_enabled;\n\tspecial->mc_alloc_throttle_enabled = zio_dva_throttle_enabled;\n\tdedup->mc_alloc_throttle_enabled = zio_dva_throttle_enabled;\n}\n\nstatic void\nspa_sync_condense_indirect(spa_t *spa, dmu_tx_t *tx)\n{\n\tASSERT(spa_writeable(spa));\n\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tfor (int c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *vd = rvd->vdev_child[c];\n\t\tvdev_indirect_state_sync_verify(vd);\n\n\t\tif (vdev_indirect_should_condense(vd)) {\n\t\t\tspa_condense_indirect_start_sync(vd, tx);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void\nspa_sync_iterate_to_convergence(spa_t *spa, dmu_tx_t *tx)\n{\n\tobjset_t *mos = spa->spa_meta_objset;\n\tdsl_pool_t *dp = spa->spa_dsl_pool;\n\tuint64_t txg = tx->tx_txg;\n\tbplist_t *free_bpl = &spa->spa_free_bplist[txg & TXG_MASK];\n\n\tdo {\n\t\tint pass = ++spa->spa_sync_pass;\n\n\t\tspa_sync_config_object(spa, tx);\n\t\tspa_sync_aux_dev(spa, &spa->spa_spares, tx,\n\t\t    ZPOOL_CONFIG_SPARES, DMU_POOL_SPARES);\n\t\tspa_sync_aux_dev(spa, &spa->spa_l2cache, tx,\n\t\t    ZPOOL_CONFIG_L2CACHE, DMU_POOL_L2CACHE);\n\t\tspa_errlog_sync(spa, txg);\n\t\tdsl_pool_sync(dp, txg);\n\n\t\tif (pass < zfs_sync_pass_deferred_free ||\n\t\t    spa_feature_is_active(spa, SPA_FEATURE_LOG_SPACEMAP)) {\n\t\t\t \n\t\t\tspa_sync_frees(spa, free_bpl, tx);\n\t\t} else {\n\t\t\t \n\t\t\tASSERT3U(pass, >, 1);\n\t\t\tbplist_iterate(free_bpl, bpobj_enqueue_alloc_cb,\n\t\t\t    &spa->spa_deferred_bpobj, tx);\n\t\t}\n\n\t\tbrt_sync(spa, txg);\n\t\tddt_sync(spa, txg);\n\t\tdsl_scan_sync(dp, tx);\n\t\tdsl_errorscrub_sync(dp, tx);\n\t\tsvr_sync(spa, tx);\n\t\tspa_sync_upgrades(spa, tx);\n\n\t\tspa_flush_metaslabs(spa, tx);\n\n\t\tvdev_t *vd = NULL;\n\t\twhile ((vd = txg_list_remove(&spa->spa_vdev_txg_list, txg))\n\t\t    != NULL)\n\t\t\tvdev_sync(vd, txg);\n\n\t\t \n\t\tif (pass == 1 &&\n\t\t    spa->spa_uberblock.ub_rootbp.blk_birth < txg &&\n\t\t    !dmu_objset_is_dirty(mos, txg)) {\n\t\t\t \n\t\t\tASSERT(txg_list_empty(&dp->dp_dirty_datasets, txg));\n\t\t\tASSERT(txg_list_empty(&dp->dp_dirty_dirs, txg));\n\t\t\tASSERT(txg_list_empty(&dp->dp_sync_tasks, txg));\n\t\t\tASSERT(txg_list_empty(&dp->dp_early_sync_tasks, txg));\n\t\t\tbreak;\n\t\t}\n\n\t\tspa_sync_deferred_frees(spa, tx);\n\t} while (dmu_objset_is_dirty(mos, txg));\n}\n\n \nstatic void\nspa_sync_rewrite_vdev_config(spa_t *spa, dmu_tx_t *tx)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\tuint64_t txg = tx->tx_txg;\n\n\tfor (;;) {\n\t\tint error = 0;\n\n\t\t \n\t\tspa_config_enter(spa, SCL_STATE, FTAG, RW_READER);\n\n\t\tif (list_is_empty(&spa->spa_config_dirty_list)) {\n\t\t\tvdev_t *svd[SPA_SYNC_MIN_VDEVS] = { NULL };\n\t\t\tint svdcount = 0;\n\t\t\tint children = rvd->vdev_children;\n\t\t\tint c0 = random_in_range(children);\n\n\t\t\tfor (int c = 0; c < children; c++) {\n\t\t\t\tvdev_t *vd =\n\t\t\t\t    rvd->vdev_child[(c0 + c) % children];\n\n\t\t\t\t \n\t\t\t\tif (c > 0 && svd[0] == vd)\n\t\t\t\t\tbreak;\n\n\t\t\t\tif (vd->vdev_ms_array == 0 ||\n\t\t\t\t    vd->vdev_islog ||\n\t\t\t\t    !vdev_is_concrete(vd))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tsvd[svdcount++] = vd;\n\t\t\t\tif (svdcount == SPA_SYNC_MIN_VDEVS)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\terror = vdev_config_sync(svd, svdcount, txg);\n\t\t} else {\n\t\t\terror = vdev_config_sync(rvd->vdev_child,\n\t\t\t    rvd->vdev_children, txg);\n\t\t}\n\n\t\tif (error == 0)\n\t\t\tspa->spa_last_synced_guid = rvd->vdev_guid;\n\n\t\tspa_config_exit(spa, SCL_STATE, FTAG);\n\n\t\tif (error == 0)\n\t\t\tbreak;\n\t\tzio_suspend(spa, NULL, ZIO_SUSPEND_IOERR);\n\t\tzio_resume_wait(spa);\n\t}\n}\n\n \nvoid\nspa_sync(spa_t *spa, uint64_t txg)\n{\n\tvdev_t *vd = NULL;\n\n\tVERIFY(spa_writeable(spa));\n\n\t \n\t(void) zio_wait(spa->spa_txg_zio[txg & TXG_MASK]);\n\tspa->spa_txg_zio[txg & TXG_MASK] = zio_root(spa, NULL, NULL,\n\t    ZIO_FLAG_CANFAIL);\n\n\t \n\tbrt_pending_apply(spa, txg);\n\n\t \n\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\n\tspa->spa_syncing_txg = txg;\n\tspa->spa_sync_pass = 0;\n\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tmutex_enter(&spa->spa_allocs[i].spaa_lock);\n\t\tVERIFY0(avl_numnodes(&spa->spa_allocs[i].spaa_tree));\n\t\tmutex_exit(&spa->spa_allocs[i].spaa_lock);\n\t}\n\n\t \n\tspa_config_enter(spa, SCL_STATE, FTAG, RW_READER);\n\twhile ((vd = list_head(&spa->spa_state_dirty_list)) != NULL) {\n\t\t \n\t\tif (vd->vdev_aux == NULL) {\n\t\t\tvdev_state_clean(vd);\n\t\t\tvdev_config_dirty(vd);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\tspa_config_enter(spa, SCL_CONFIG | SCL_STATE, FTAG, RW_WRITER);\n\t\twhile ((vd = list_head(&spa->spa_state_dirty_list)) != NULL) {\n\t\t\tvdev_state_clean(vd);\n\t\t\tvdev_config_dirty(vd);\n\t\t}\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\tspa_config_enter(spa, SCL_CONFIG | SCL_STATE, FTAG, RW_READER);\n\t}\n\tspa_config_exit(spa, SCL_STATE, FTAG);\n\n\tdsl_pool_t *dp = spa->spa_dsl_pool;\n\tdmu_tx_t *tx = dmu_tx_create_assigned(dp, txg);\n\n\tspa->spa_sync_starttime = gethrtime();\n\ttaskq_cancel_id(system_delay_taskq, spa->spa_deadman_tqid);\n\tspa->spa_deadman_tqid = taskq_dispatch_delay(system_delay_taskq,\n\t    spa_deadman, spa, TQ_SLEEP, ddi_get_lbolt() +\n\t    NSEC_TO_TICK(spa->spa_deadman_synctime));\n\n\t \n\tif (spa->spa_ubsync.ub_version < SPA_VERSION_RAIDZ_DEFLATE &&\n\t    spa->spa_uberblock.ub_version >= SPA_VERSION_RAIDZ_DEFLATE) {\n\t\tvdev_t *rvd = spa->spa_root_vdev;\n\n\t\tint i;\n\t\tfor (i = 0; i < rvd->vdev_children; i++) {\n\t\t\tvd = rvd->vdev_child[i];\n\t\t\tif (vd->vdev_deflate_ratio != SPA_MINBLOCKSIZE)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == rvd->vdev_children) {\n\t\t\tspa->spa_deflate = TRUE;\n\t\t\tVERIFY0(zap_add(spa->spa_meta_objset,\n\t\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_DEFLATE,\n\t\t\t    sizeof (uint64_t), 1, &spa->spa_deflate, tx));\n\t\t}\n\t}\n\n\tspa_sync_adjust_vdev_max_queue_depth(spa);\n\n\tspa_sync_condense_indirect(spa, tx);\n\n\tspa_sync_iterate_to_convergence(spa, tx);\n\n#ifdef ZFS_DEBUG\n\tif (!list_is_empty(&spa->spa_config_dirty_list)) {\n\t \n\t\tuint64_t all_vdev_zap_entry_count;\n\t\tASSERT0(zap_count(spa->spa_meta_objset,\n\t\t    spa->spa_all_vdev_zaps, &all_vdev_zap_entry_count));\n\t\tASSERT3U(vdev_count_verify_zaps(spa->spa_root_vdev), ==,\n\t\t    all_vdev_zap_entry_count);\n\t}\n#endif\n\n\tif (spa->spa_vdev_removal != NULL) {\n\t\tASSERT0(spa->spa_vdev_removal->svr_bytes_done[txg & TXG_MASK]);\n\t}\n\n\tspa_sync_rewrite_vdev_config(spa, tx);\n\tdmu_tx_commit(tx);\n\n\ttaskq_cancel_id(system_delay_taskq, spa->spa_deadman_tqid);\n\tspa->spa_deadman_tqid = 0;\n\n\t \n\twhile ((vd = list_head(&spa->spa_config_dirty_list)) != NULL)\n\t\tvdev_config_clean(vd);\n\n\t \n\tif (spa->spa_config_syncing != NULL) {\n\t\tspa_config_set(spa, spa->spa_config_syncing);\n\t\tspa->spa_config_txg = txg;\n\t\tspa->spa_config_syncing = NULL;\n\t}\n\n\tdsl_pool_sync_done(dp, txg);\n\n\tfor (int i = 0; i < spa->spa_alloc_count; i++) {\n\t\tmutex_enter(&spa->spa_allocs[i].spaa_lock);\n\t\tVERIFY0(avl_numnodes(&spa->spa_allocs[i].spaa_tree));\n\t\tmutex_exit(&spa->spa_allocs[i].spaa_lock);\n\t}\n\n\t \n\twhile ((vd = txg_list_remove(&spa->spa_vdev_txg_list, TXG_CLEAN(txg)))\n\t    != NULL)\n\t\tvdev_sync_done(vd, txg);\n\n\tmetaslab_class_evict_old(spa->spa_normal_class, txg);\n\tmetaslab_class_evict_old(spa->spa_log_class, txg);\n\n\tspa_sync_close_syncing_log_sm(spa);\n\n\tspa_update_dspace(spa);\n\n\tif (spa_get_autotrim(spa) == SPA_AUTOTRIM_ON)\n\t\tvdev_autotrim_kick(spa);\n\n\t \n\tASSERT(txg_list_empty(&dp->dp_dirty_datasets, txg));\n\tASSERT(txg_list_empty(&dp->dp_dirty_dirs, txg));\n\tASSERT(txg_list_empty(&spa->spa_vdev_txg_list, txg));\n\n\twhile (zfs_pause_spa_sync)\n\t\tdelay(1);\n\n\tspa->spa_sync_pass = 0;\n\n\t \n\tspa->spa_ubsync = spa->spa_uberblock;\n\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\n\tspa_handle_ignored_writes(spa);\n\n\t \n\tspa_async_dispatch(spa);\n}\n\n \nvoid\nspa_sync_allpools(void)\n{\n\tspa_t *spa = NULL;\n\tmutex_enter(&spa_namespace_lock);\n\twhile ((spa = spa_next(spa)) != NULL) {\n\t\tif (spa_state(spa) != POOL_STATE_ACTIVE ||\n\t\t    !spa_writeable(spa) || spa_suspended(spa))\n\t\t\tcontinue;\n\t\tspa_open_ref(spa, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\ttxg_wait_synced(spa_get_dsl(spa), 0);\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_close(spa, FTAG);\n\t}\n\tmutex_exit(&spa_namespace_lock);\n}\n\n \n\n \nvoid\nspa_evict_all(void)\n{\n\tspa_t *spa;\n\n\t \n\tmutex_enter(&spa_namespace_lock);\n\twhile ((spa = spa_next(NULL)) != NULL) {\n\t\t \n\t\tspa_open_ref(spa, FTAG);\n\t\tmutex_exit(&spa_namespace_lock);\n\t\tspa_async_suspend(spa);\n\t\tmutex_enter(&spa_namespace_lock);\n\t\tspa_close(spa, FTAG);\n\n\t\tif (spa->spa_state != POOL_STATE_UNINITIALIZED) {\n\t\t\tspa_unload(spa);\n\t\t\tspa_deactivate(spa);\n\t\t}\n\t\tspa_remove(spa);\n\t}\n\tmutex_exit(&spa_namespace_lock);\n}\n\nvdev_t *\nspa_lookup_by_guid(spa_t *spa, uint64_t guid, boolean_t aux)\n{\n\tvdev_t *vd;\n\tint i;\n\n\tif ((vd = vdev_lookup_by_guid(spa->spa_root_vdev, guid)) != NULL)\n\t\treturn (vd);\n\n\tif (aux) {\n\t\tfor (i = 0; i < spa->spa_l2cache.sav_count; i++) {\n\t\t\tvd = spa->spa_l2cache.sav_vdevs[i];\n\t\t\tif (vd->vdev_guid == guid)\n\t\t\t\treturn (vd);\n\t\t}\n\n\t\tfor (i = 0; i < spa->spa_spares.sav_count; i++) {\n\t\t\tvd = spa->spa_spares.sav_vdevs[i];\n\t\t\tif (vd->vdev_guid == guid)\n\t\t\t\treturn (vd);\n\t\t}\n\t}\n\n\treturn (NULL);\n}\n\nvoid\nspa_upgrade(spa_t *spa, uint64_t version)\n{\n\tASSERT(spa_writeable(spa));\n\n\tspa_config_enter(spa, SCL_ALL, FTAG, RW_WRITER);\n\n\t \n\tASSERT(SPA_VERSION_IS_SUPPORTED(spa->spa_uberblock.ub_version));\n\tASSERT3U(version, >=, spa->spa_uberblock.ub_version);\n\n\tspa->spa_uberblock.ub_version = version;\n\tvdev_config_dirty(spa->spa_root_vdev);\n\n\tspa_config_exit(spa, SCL_ALL, FTAG);\n\n\ttxg_wait_synced(spa_get_dsl(spa), 0);\n}\n\nstatic boolean_t\nspa_has_aux_vdev(spa_t *spa, uint64_t guid, spa_aux_vdev_t *sav)\n{\n\t(void) spa;\n\tint i;\n\tuint64_t vdev_guid;\n\n\tfor (i = 0; i < sav->sav_count; i++)\n\t\tif (sav->sav_vdevs[i]->vdev_guid == guid)\n\t\t\treturn (B_TRUE);\n\n\tfor (i = 0; i < sav->sav_npending; i++) {\n\t\tif (nvlist_lookup_uint64(sav->sav_pending[i], ZPOOL_CONFIG_GUID,\n\t\t    &vdev_guid) == 0 && vdev_guid == guid)\n\t\t\treturn (B_TRUE);\n\t}\n\n\treturn (B_FALSE);\n}\n\nboolean_t\nspa_has_l2cache(spa_t *spa, uint64_t guid)\n{\n\treturn (spa_has_aux_vdev(spa, guid, &spa->spa_l2cache));\n}\n\nboolean_t\nspa_has_spare(spa_t *spa, uint64_t guid)\n{\n\treturn (spa_has_aux_vdev(spa, guid, &spa->spa_spares));\n}\n\n \nstatic boolean_t\nspa_has_active_shared_spare(spa_t *spa)\n{\n\tint i, refcnt;\n\tuint64_t pool;\n\tspa_aux_vdev_t *sav = &spa->spa_spares;\n\n\tfor (i = 0; i < sav->sav_count; i++) {\n\t\tif (spa_spare_exists(sav->sav_vdevs[i]->vdev_guid, &pool,\n\t\t    &refcnt) && pool != 0ULL && pool == spa_guid(spa) &&\n\t\t    refcnt > 2)\n\t\t\treturn (B_TRUE);\n\t}\n\n\treturn (B_FALSE);\n}\n\nuint64_t\nspa_total_metaslabs(spa_t *spa)\n{\n\tvdev_t *rvd = spa->spa_root_vdev;\n\n\tuint64_t m = 0;\n\tfor (uint64_t c = 0; c < rvd->vdev_children; c++) {\n\t\tvdev_t *vd = rvd->vdev_child[c];\n\t\tif (!vdev_is_concrete(vd))\n\t\t\tcontinue;\n\t\tm += vd->vdev_ms_count;\n\t}\n\treturn (m);\n}\n\n \nvoid\nspa_notify_waiters(spa_t *spa)\n{\n\t \n\tmutex_enter(&spa->spa_activities_lock);\n\tcv_broadcast(&spa->spa_activities_cv);\n\tmutex_exit(&spa->spa_activities_lock);\n}\n\n \nvoid\nspa_wake_waiters(spa_t *spa)\n{\n\tmutex_enter(&spa->spa_activities_lock);\n\tspa->spa_waiters_cancel = B_TRUE;\n\tcv_broadcast(&spa->spa_activities_cv);\n\twhile (spa->spa_waiters != 0)\n\t\tcv_wait(&spa->spa_waiters_cv, &spa->spa_activities_lock);\n\tspa->spa_waiters_cancel = B_FALSE;\n\tmutex_exit(&spa->spa_activities_lock);\n}\n\n \nstatic boolean_t\nspa_vdev_activity_in_progress_impl(vdev_t *vd, zpool_wait_activity_t activity)\n{\n\tspa_t *spa = vd->vdev_spa;\n\n\tASSERT(spa_config_held(spa, SCL_CONFIG | SCL_STATE, RW_READER));\n\tASSERT(MUTEX_HELD(&spa->spa_activities_lock));\n\tASSERT(activity == ZPOOL_WAIT_INITIALIZE ||\n\t    activity == ZPOOL_WAIT_TRIM);\n\n\tkmutex_t *lock = activity == ZPOOL_WAIT_INITIALIZE ?\n\t    &vd->vdev_initialize_lock : &vd->vdev_trim_lock;\n\n\tmutex_exit(&spa->spa_activities_lock);\n\tmutex_enter(lock);\n\tmutex_enter(&spa->spa_activities_lock);\n\n\tboolean_t in_progress = (activity == ZPOOL_WAIT_INITIALIZE) ?\n\t    (vd->vdev_initialize_state == VDEV_INITIALIZE_ACTIVE) :\n\t    (vd->vdev_trim_state == VDEV_TRIM_ACTIVE);\n\tmutex_exit(lock);\n\n\tif (in_progress)\n\t\treturn (B_TRUE);\n\n\tfor (int i = 0; i < vd->vdev_children; i++) {\n\t\tif (spa_vdev_activity_in_progress_impl(vd->vdev_child[i],\n\t\t    activity))\n\t\t\treturn (B_TRUE);\n\t}\n\n\treturn (B_FALSE);\n}\n\n \nstatic int\nspa_vdev_activity_in_progress(spa_t *spa, boolean_t use_guid, uint64_t guid,\n    zpool_wait_activity_t activity, boolean_t *in_progress)\n{\n\tmutex_exit(&spa->spa_activities_lock);\n\tspa_config_enter(spa, SCL_CONFIG | SCL_STATE, FTAG, RW_READER);\n\tmutex_enter(&spa->spa_activities_lock);\n\n\tvdev_t *vd;\n\tif (use_guid) {\n\t\tvd = spa_lookup_by_guid(spa, guid, B_FALSE);\n\t\tif (vd == NULL || !vd->vdev_ops->vdev_op_leaf) {\n\t\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\t\treturn (EINVAL);\n\t\t}\n\t} else {\n\t\tvd = spa->spa_root_vdev;\n\t}\n\n\t*in_progress = spa_vdev_activity_in_progress_impl(vd, activity);\n\n\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\treturn (0);\n}\n\n \n\nstatic int\nspa_activity_in_progress(spa_t *spa, zpool_wait_activity_t activity,\n    boolean_t use_tag, uint64_t tag, boolean_t *in_progress)\n{\n\tint error = 0;\n\n\tASSERT(MUTEX_HELD(&spa->spa_activities_lock));\n\n\tswitch (activity) {\n\tcase ZPOOL_WAIT_CKPT_DISCARD:\n\t\t*in_progress =\n\t\t    (spa_feature_is_active(spa, SPA_FEATURE_POOL_CHECKPOINT) &&\n\t\t    zap_contains(spa_meta_objset(spa),\n\t\t    DMU_POOL_DIRECTORY_OBJECT, DMU_POOL_ZPOOL_CHECKPOINT) ==\n\t\t    ENOENT);\n\t\tbreak;\n\tcase ZPOOL_WAIT_FREE:\n\t\t*in_progress = ((spa_version(spa) >= SPA_VERSION_DEADLISTS &&\n\t\t    !bpobj_is_empty(&spa->spa_dsl_pool->dp_free_bpobj)) ||\n\t\t    spa_feature_is_active(spa, SPA_FEATURE_ASYNC_DESTROY) ||\n\t\t    spa_livelist_delete_check(spa));\n\t\tbreak;\n\tcase ZPOOL_WAIT_INITIALIZE:\n\tcase ZPOOL_WAIT_TRIM:\n\t\terror = spa_vdev_activity_in_progress(spa, use_tag, tag,\n\t\t    activity, in_progress);\n\t\tbreak;\n\tcase ZPOOL_WAIT_REPLACE:\n\t\tmutex_exit(&spa->spa_activities_lock);\n\t\tspa_config_enter(spa, SCL_CONFIG | SCL_STATE, FTAG, RW_READER);\n\t\tmutex_enter(&spa->spa_activities_lock);\n\n\t\t*in_progress = vdev_replace_in_progress(spa->spa_root_vdev);\n\t\tspa_config_exit(spa, SCL_CONFIG | SCL_STATE, FTAG);\n\t\tbreak;\n\tcase ZPOOL_WAIT_REMOVE:\n\t\t*in_progress = (spa->spa_removing_phys.sr_state ==\n\t\t    DSS_SCANNING);\n\t\tbreak;\n\tcase ZPOOL_WAIT_RESILVER:\n\t\tif ((*in_progress = vdev_rebuild_active(spa->spa_root_vdev)))\n\t\t\tbreak;\n\t\tzfs_fallthrough;\n\tcase ZPOOL_WAIT_SCRUB:\n\t{\n\t\tboolean_t scanning, paused, is_scrub;\n\t\tdsl_scan_t *scn =  spa->spa_dsl_pool->dp_scan;\n\n\t\tis_scrub = (scn->scn_phys.scn_func == POOL_SCAN_SCRUB);\n\t\tscanning = (scn->scn_phys.scn_state == DSS_SCANNING);\n\t\tpaused = dsl_scan_is_paused_scrub(scn);\n\t\t*in_progress = (scanning && !paused &&\n\t\t    is_scrub == (activity == ZPOOL_WAIT_SCRUB));\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpanic(\"unrecognized value for activity %d\", activity);\n\t}\n\n\treturn (error);\n}\n\nstatic int\nspa_wait_common(const char *pool, zpool_wait_activity_t activity,\n    boolean_t use_tag, uint64_t tag, boolean_t *waited)\n{\n\t \n\tif (use_tag && activity != ZPOOL_WAIT_INITIALIZE &&\n\t    activity != ZPOOL_WAIT_TRIM)\n\t\treturn (EINVAL);\n\n\tif (activity < 0 || activity >= ZPOOL_WAIT_NUM_ACTIVITIES)\n\t\treturn (EINVAL);\n\n\tspa_t *spa;\n\tint error = spa_open(pool, &spa, FTAG);\n\tif (error != 0)\n\t\treturn (error);\n\n\t \n\tmutex_enter(&spa->spa_activities_lock);\n\tspa->spa_waiters++;\n\tspa_close(spa, FTAG);\n\n\t*waited = B_FALSE;\n\tfor (;;) {\n\t\tboolean_t in_progress;\n\t\terror = spa_activity_in_progress(spa, activity, use_tag, tag,\n\t\t    &in_progress);\n\n\t\tif (error || !in_progress || spa->spa_waiters_cancel)\n\t\t\tbreak;\n\n\t\t*waited = B_TRUE;\n\n\t\tif (cv_wait_sig(&spa->spa_activities_cv,\n\t\t    &spa->spa_activities_lock) == 0) {\n\t\t\terror = EINTR;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspa->spa_waiters--;\n\tcv_signal(&spa->spa_waiters_cv);\n\tmutex_exit(&spa->spa_activities_lock);\n\n\treturn (error);\n}\n\n \nint\nspa_wait_tag(const char *pool, zpool_wait_activity_t activity, uint64_t tag,\n    boolean_t *waited)\n{\n\treturn (spa_wait_common(pool, activity, B_TRUE, tag, waited));\n}\n\n \nint\nspa_wait(const char *pool, zpool_wait_activity_t activity, boolean_t *waited)\n{\n\n\treturn (spa_wait_common(pool, activity, B_FALSE, 0, waited));\n}\n\nsysevent_t *\nspa_event_create(spa_t *spa, vdev_t *vd, nvlist_t *hist_nvl, const char *name)\n{\n\tsysevent_t *ev = NULL;\n#ifdef _KERNEL\n\tnvlist_t *resource;\n\n\tresource = zfs_event_create(spa, vd, FM_SYSEVENT_CLASS, name, hist_nvl);\n\tif (resource) {\n\t\tev = kmem_alloc(sizeof (sysevent_t), KM_SLEEP);\n\t\tev->resource = resource;\n\t}\n#else\n\t(void) spa, (void) vd, (void) hist_nvl, (void) name;\n#endif\n\treturn (ev);\n}\n\nvoid\nspa_event_post(sysevent_t *ev)\n{\n#ifdef _KERNEL\n\tif (ev) {\n\t\tzfs_zevent_post(ev->resource, NULL, zfs_zevent_post_cb);\n\t\tkmem_free(ev, sizeof (*ev));\n\t}\n#else\n\t(void) ev;\n#endif\n}\n\n \nvoid\nspa_event_notify(spa_t *spa, vdev_t *vd, nvlist_t *hist_nvl, const char *name)\n{\n\tspa_event_post(spa_event_create(spa, vd, hist_nvl, name));\n}\n\n \nEXPORT_SYMBOL(spa_open);\nEXPORT_SYMBOL(spa_open_rewind);\nEXPORT_SYMBOL(spa_get_stats);\nEXPORT_SYMBOL(spa_create);\nEXPORT_SYMBOL(spa_import);\nEXPORT_SYMBOL(spa_tryimport);\nEXPORT_SYMBOL(spa_destroy);\nEXPORT_SYMBOL(spa_export);\nEXPORT_SYMBOL(spa_reset);\nEXPORT_SYMBOL(spa_async_request);\nEXPORT_SYMBOL(spa_async_suspend);\nEXPORT_SYMBOL(spa_async_resume);\nEXPORT_SYMBOL(spa_inject_addref);\nEXPORT_SYMBOL(spa_inject_delref);\nEXPORT_SYMBOL(spa_scan_stat_init);\nEXPORT_SYMBOL(spa_scan_get_stats);\n\n \nEXPORT_SYMBOL(spa_vdev_add);\nEXPORT_SYMBOL(spa_vdev_attach);\nEXPORT_SYMBOL(spa_vdev_detach);\nEXPORT_SYMBOL(spa_vdev_setpath);\nEXPORT_SYMBOL(spa_vdev_setfru);\nEXPORT_SYMBOL(spa_vdev_split_mirror);\n\n \nEXPORT_SYMBOL(spa_spare_add);\nEXPORT_SYMBOL(spa_spare_remove);\nEXPORT_SYMBOL(spa_spare_exists);\nEXPORT_SYMBOL(spa_spare_activate);\n\n \nEXPORT_SYMBOL(spa_l2cache_add);\nEXPORT_SYMBOL(spa_l2cache_remove);\nEXPORT_SYMBOL(spa_l2cache_exists);\nEXPORT_SYMBOL(spa_l2cache_activate);\nEXPORT_SYMBOL(spa_l2cache_drop);\n\n \nEXPORT_SYMBOL(spa_scan);\nEXPORT_SYMBOL(spa_scan_stop);\n\n \nEXPORT_SYMBOL(spa_sync);  \nEXPORT_SYMBOL(spa_sync_allpools);\n\n \nEXPORT_SYMBOL(spa_prop_set);\nEXPORT_SYMBOL(spa_prop_get);\nEXPORT_SYMBOL(spa_prop_clear_bootfs);\n\n \nEXPORT_SYMBOL(spa_event_notify);\n\nZFS_MODULE_PARAM(zfs_metaslab, metaslab_, preload_pct, UINT, ZMOD_RW,\n\t\"Percentage of CPUs to run a metaslab preload taskq\");\n\n \nZFS_MODULE_PARAM(zfs_spa, spa_, load_verify_shift, UINT, ZMOD_RW,\n\t\"log2 fraction of arc that can be used by inflight I/Os when \"\n\t\"verifying pool during import\");\n \n\nZFS_MODULE_PARAM(zfs_spa, spa_, load_verify_metadata, INT, ZMOD_RW,\n\t\"Set to traverse metadata on pool import\");\n\nZFS_MODULE_PARAM(zfs_spa, spa_, load_verify_data, INT, ZMOD_RW,\n\t\"Set to traverse data on pool import\");\n\nZFS_MODULE_PARAM(zfs_spa, spa_, load_print_vdev_tree, INT, ZMOD_RW,\n\t\"Print vdev tree to zfs_dbgmsg during pool import\");\n\nZFS_MODULE_PARAM(zfs_zio, zio_, taskq_batch_pct, UINT, ZMOD_RD,\n\t\"Percentage of CPUs to run an IO worker thread\");\n\nZFS_MODULE_PARAM(zfs_zio, zio_, taskq_batch_tpq, UINT, ZMOD_RD,\n\t\"Number of threads per IO worker taskqueue\");\n\n \nZFS_MODULE_PARAM(zfs, zfs_, max_missing_tvds, U64, ZMOD_RW,\n\t\"Allow importing pool with up to this number of missing top-level \"\n\t\"vdevs (in read-only mode)\");\n \n\nZFS_MODULE_PARAM(zfs_livelist_condense, zfs_livelist_condense_, zthr_pause, INT,\n\tZMOD_RW, \"Set the livelist condense zthr to pause\");\n\nZFS_MODULE_PARAM(zfs_livelist_condense, zfs_livelist_condense_, sync_pause, INT,\n\tZMOD_RW, \"Set the livelist condense synctask to pause\");\n\n \nZFS_MODULE_PARAM(zfs_livelist_condense, zfs_livelist_condense_, sync_cancel,\n\tINT, ZMOD_RW,\n\t\"Whether livelist condensing was canceled in the synctask\");\n\nZFS_MODULE_PARAM(zfs_livelist_condense, zfs_livelist_condense_, zthr_cancel,\n\tINT, ZMOD_RW,\n\t\"Whether livelist condensing was canceled in the zthr function\");\n\nZFS_MODULE_PARAM(zfs_livelist_condense, zfs_livelist_condense_, new_alloc, INT,\n\tZMOD_RW,\n\t\"Whether extra ALLOC blkptrs were added to a livelist entry while it \"\n\t\"was being condensed\");\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}