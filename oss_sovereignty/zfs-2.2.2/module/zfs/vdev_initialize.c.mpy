{
  "module_name": "vdev_initialize.c",
  "hash_id": "ebcb0ed16d5a450dfddcdf539df4476e5e2469f246d72798bf82f18ccafbcf3f",
  "original_prompt": "Ingested from zfs-2.2.2/module/zfs/vdev_initialize.c",
  "human_readable_source": " \n\n \n\n#include <sys/spa.h>\n#include <sys/spa_impl.h>\n#include <sys/txg.h>\n#include <sys/vdev_impl.h>\n#include <sys/metaslab_impl.h>\n#include <sys/dsl_synctask.h>\n#include <sys/zap.h>\n#include <sys/dmu_tx.h>\n#include <sys/vdev_initialize.h>\n\n \nstatic uint64_t zfs_initialize_value = 0xdeadbeefdeadbeeeULL;\n\n \nstatic const int zfs_initialize_limit = 1;\n\n \nstatic uint64_t zfs_initialize_chunk_size = 1024 * 1024;\n\nstatic boolean_t\nvdev_initialize_should_stop(vdev_t *vd)\n{\n\treturn (vd->vdev_initialize_exit_wanted || !vdev_writeable(vd) ||\n\t    vd->vdev_detached || vd->vdev_top->vdev_removing);\n}\n\nstatic void\nvdev_initialize_zap_update_sync(void *arg, dmu_tx_t *tx)\n{\n\t \n\tuint64_t guid = *(uint64_t *)arg;\n\tuint64_t txg = dmu_tx_get_txg(tx);\n\tkmem_free(arg, sizeof (uint64_t));\n\n\tvdev_t *vd = spa_lookup_by_guid(tx->tx_pool->dp_spa, guid, B_FALSE);\n\tif (vd == NULL || vd->vdev_top->vdev_removing || !vdev_is_concrete(vd))\n\t\treturn;\n\n\tuint64_t last_offset = vd->vdev_initialize_offset[txg & TXG_MASK];\n\tvd->vdev_initialize_offset[txg & TXG_MASK] = 0;\n\n\tVERIFY(vd->vdev_leaf_zap != 0);\n\n\tobjset_t *mos = vd->vdev_spa->spa_meta_objset;\n\n\tif (last_offset > 0) {\n\t\tvd->vdev_initialize_last_offset = last_offset;\n\t\tVERIFY0(zap_update(mos, vd->vdev_leaf_zap,\n\t\t    VDEV_LEAF_ZAP_INITIALIZE_LAST_OFFSET,\n\t\t    sizeof (last_offset), 1, &last_offset, tx));\n\t}\n\tif (vd->vdev_initialize_action_time > 0) {\n\t\tuint64_t val = (uint64_t)vd->vdev_initialize_action_time;\n\t\tVERIFY0(zap_update(mos, vd->vdev_leaf_zap,\n\t\t    VDEV_LEAF_ZAP_INITIALIZE_ACTION_TIME, sizeof (val),\n\t\t    1, &val, tx));\n\t}\n\n\tuint64_t initialize_state = vd->vdev_initialize_state;\n\tVERIFY0(zap_update(mos, vd->vdev_leaf_zap,\n\t    VDEV_LEAF_ZAP_INITIALIZE_STATE, sizeof (initialize_state), 1,\n\t    &initialize_state, tx));\n}\n\nstatic void\nvdev_initialize_zap_remove_sync(void *arg, dmu_tx_t *tx)\n{\n\tuint64_t guid = *(uint64_t *)arg;\n\n\tkmem_free(arg, sizeof (uint64_t));\n\n\tvdev_t *vd = spa_lookup_by_guid(tx->tx_pool->dp_spa, guid, B_FALSE);\n\tif (vd == NULL || vd->vdev_top->vdev_removing || !vdev_is_concrete(vd))\n\t\treturn;\n\n\tASSERT3S(vd->vdev_initialize_state, ==, VDEV_INITIALIZE_NONE);\n\tASSERT3U(vd->vdev_leaf_zap, !=, 0);\n\n\tvd->vdev_initialize_last_offset = 0;\n\tvd->vdev_initialize_action_time = 0;\n\n\tobjset_t *mos = vd->vdev_spa->spa_meta_objset;\n\tint error;\n\n\terror = zap_remove(mos, vd->vdev_leaf_zap,\n\t    VDEV_LEAF_ZAP_INITIALIZE_LAST_OFFSET, tx);\n\tVERIFY(error == 0 || error == ENOENT);\n\n\terror = zap_remove(mos, vd->vdev_leaf_zap,\n\t    VDEV_LEAF_ZAP_INITIALIZE_STATE, tx);\n\tVERIFY(error == 0 || error == ENOENT);\n\n\terror = zap_remove(mos, vd->vdev_leaf_zap,\n\t    VDEV_LEAF_ZAP_INITIALIZE_ACTION_TIME, tx);\n\tVERIFY(error == 0 || error == ENOENT);\n}\n\nstatic void\nvdev_initialize_change_state(vdev_t *vd, vdev_initializing_state_t new_state)\n{\n\tASSERT(MUTEX_HELD(&vd->vdev_initialize_lock));\n\tspa_t *spa = vd->vdev_spa;\n\n\tif (new_state == vd->vdev_initialize_state)\n\t\treturn;\n\n\t \n\tuint64_t *guid = kmem_zalloc(sizeof (uint64_t), KM_SLEEP);\n\t*guid = vd->vdev_guid;\n\n\t \n\tif (vd->vdev_initialize_state != VDEV_INITIALIZE_SUSPENDED) {\n\t\tvd->vdev_initialize_action_time = gethrestime_sec();\n\t}\n\n\tvdev_initializing_state_t old_state = vd->vdev_initialize_state;\n\tvd->vdev_initialize_state = new_state;\n\n\tdmu_tx_t *tx = dmu_tx_create_dd(spa_get_dsl(spa)->dp_mos_dir);\n\tVERIFY0(dmu_tx_assign(tx, TXG_WAIT));\n\n\tif (new_state != VDEV_INITIALIZE_NONE) {\n\t\tdsl_sync_task_nowait(spa_get_dsl(spa),\n\t\t    vdev_initialize_zap_update_sync, guid, tx);\n\t} else {\n\t\tdsl_sync_task_nowait(spa_get_dsl(spa),\n\t\t    vdev_initialize_zap_remove_sync, guid, tx);\n\t}\n\n\tswitch (new_state) {\n\tcase VDEV_INITIALIZE_ACTIVE:\n\t\tspa_history_log_internal(spa, \"initialize\", tx,\n\t\t    \"vdev=%s activated\", vd->vdev_path);\n\t\tbreak;\n\tcase VDEV_INITIALIZE_SUSPENDED:\n\t\tspa_history_log_internal(spa, \"initialize\", tx,\n\t\t    \"vdev=%s suspended\", vd->vdev_path);\n\t\tbreak;\n\tcase VDEV_INITIALIZE_CANCELED:\n\t\tif (old_state == VDEV_INITIALIZE_ACTIVE ||\n\t\t    old_state == VDEV_INITIALIZE_SUSPENDED)\n\t\t\tspa_history_log_internal(spa, \"initialize\", tx,\n\t\t\t    \"vdev=%s canceled\", vd->vdev_path);\n\t\tbreak;\n\tcase VDEV_INITIALIZE_COMPLETE:\n\t\tspa_history_log_internal(spa, \"initialize\", tx,\n\t\t    \"vdev=%s complete\", vd->vdev_path);\n\t\tbreak;\n\tcase VDEV_INITIALIZE_NONE:\n\t\tspa_history_log_internal(spa, \"uninitialize\", tx,\n\t\t    \"vdev=%s\", vd->vdev_path);\n\t\tbreak;\n\tdefault:\n\t\tpanic(\"invalid state %llu\", (unsigned long long)new_state);\n\t}\n\n\tdmu_tx_commit(tx);\n\n\tif (new_state != VDEV_INITIALIZE_ACTIVE)\n\t\tspa_notify_waiters(spa);\n}\n\nstatic void\nvdev_initialize_cb(zio_t *zio)\n{\n\tvdev_t *vd = zio->io_vd;\n\tmutex_enter(&vd->vdev_initialize_io_lock);\n\tif (zio->io_error == ENXIO && !vdev_writeable(vd)) {\n\t\t \n\t\tuint64_t *off =\n\t\t    &vd->vdev_initialize_offset[zio->io_txg & TXG_MASK];\n\t\t*off = MIN(*off, zio->io_offset);\n\t} else {\n\t\t \n\t\tif (zio->io_error != 0)\n\t\t\tvd->vdev_stat.vs_initialize_errors++;\n\n\t\tvd->vdev_initialize_bytes_done += zio->io_orig_size;\n\t}\n\tASSERT3U(vd->vdev_initialize_inflight, >, 0);\n\tvd->vdev_initialize_inflight--;\n\tcv_broadcast(&vd->vdev_initialize_io_cv);\n\tmutex_exit(&vd->vdev_initialize_io_lock);\n\n\tspa_config_exit(vd->vdev_spa, SCL_STATE_ALL, vd);\n}\n\n \nstatic int\nvdev_initialize_write(vdev_t *vd, uint64_t start, uint64_t size, abd_t *data)\n{\n\tspa_t *spa = vd->vdev_spa;\n\n\t \n\tmutex_enter(&vd->vdev_initialize_io_lock);\n\twhile (vd->vdev_initialize_inflight >= zfs_initialize_limit) {\n\t\tcv_wait(&vd->vdev_initialize_io_cv,\n\t\t    &vd->vdev_initialize_io_lock);\n\t}\n\tvd->vdev_initialize_inflight++;\n\tmutex_exit(&vd->vdev_initialize_io_lock);\n\n\tdmu_tx_t *tx = dmu_tx_create_dd(spa_get_dsl(spa)->dp_mos_dir);\n\tVERIFY0(dmu_tx_assign(tx, TXG_WAIT));\n\tuint64_t txg = dmu_tx_get_txg(tx);\n\n\tspa_config_enter(spa, SCL_STATE_ALL, vd, RW_READER);\n\tmutex_enter(&vd->vdev_initialize_lock);\n\n\tif (vd->vdev_initialize_offset[txg & TXG_MASK] == 0) {\n\t\tuint64_t *guid = kmem_zalloc(sizeof (uint64_t), KM_SLEEP);\n\t\t*guid = vd->vdev_guid;\n\n\t\t \n\t\tdsl_sync_task_nowait(spa_get_dsl(spa),\n\t\t    vdev_initialize_zap_update_sync, guid, tx);\n\t}\n\n\t \n\tif (vdev_initialize_should_stop(vd)) {\n\t\tmutex_enter(&vd->vdev_initialize_io_lock);\n\t\tASSERT3U(vd->vdev_initialize_inflight, >, 0);\n\t\tvd->vdev_initialize_inflight--;\n\t\tmutex_exit(&vd->vdev_initialize_io_lock);\n\t\tspa_config_exit(vd->vdev_spa, SCL_STATE_ALL, vd);\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t\tdmu_tx_commit(tx);\n\t\treturn (SET_ERROR(EINTR));\n\t}\n\tmutex_exit(&vd->vdev_initialize_lock);\n\n\tvd->vdev_initialize_offset[txg & TXG_MASK] = start + size;\n\tzio_nowait(zio_write_phys(spa->spa_txg_zio[txg & TXG_MASK], vd, start,\n\t    size, data, ZIO_CHECKSUM_OFF, vdev_initialize_cb, NULL,\n\t    ZIO_PRIORITY_INITIALIZING, ZIO_FLAG_CANFAIL, B_FALSE));\n\t \n\n\tdmu_tx_commit(tx);\n\n\treturn (0);\n}\n\n \nstatic int\nvdev_initialize_block_fill(void *buf, size_t len, void *unused)\n{\n\t(void) unused;\n\n\tASSERT0(len % sizeof (uint64_t));\n\tfor (uint64_t i = 0; i < len; i += sizeof (uint64_t)) {\n\t\t*(uint64_t *)((char *)(buf) + i) = zfs_initialize_value;\n\t}\n\treturn (0);\n}\n\nstatic abd_t *\nvdev_initialize_block_alloc(void)\n{\n\t \n\tabd_t *data = abd_alloc_for_io(zfs_initialize_chunk_size, B_FALSE);\n\n\tASSERT0(zfs_initialize_chunk_size % sizeof (uint64_t));\n\t(void) abd_iterate_func(data, 0, zfs_initialize_chunk_size,\n\t    vdev_initialize_block_fill, NULL);\n\n\treturn (data);\n}\n\nstatic void\nvdev_initialize_block_free(abd_t *data)\n{\n\tabd_free(data);\n}\n\nstatic int\nvdev_initialize_ranges(vdev_t *vd, abd_t *data)\n{\n\trange_tree_t *rt = vd->vdev_initialize_tree;\n\tzfs_btree_t *bt = &rt->rt_root;\n\tzfs_btree_index_t where;\n\n\tfor (range_seg_t *rs = zfs_btree_first(bt, &where); rs != NULL;\n\t    rs = zfs_btree_next(bt, &where, &where)) {\n\t\tuint64_t size = rs_get_end(rs, rt) - rs_get_start(rs, rt);\n\n\t\t \n\t\tuint64_t writes_required =\n\t\t    ((size - 1) / zfs_initialize_chunk_size) + 1;\n\n\t\tfor (uint64_t w = 0; w < writes_required; w++) {\n\t\t\tint error;\n\n\t\t\terror = vdev_initialize_write(vd,\n\t\t\t    VDEV_LABEL_START_SIZE + rs_get_start(rs, rt) +\n\t\t\t    (w * zfs_initialize_chunk_size),\n\t\t\t    MIN(size - (w * zfs_initialize_chunk_size),\n\t\t\t    zfs_initialize_chunk_size), data);\n\t\t\tif (error != 0)\n\t\t\t\treturn (error);\n\t\t}\n\t}\n\treturn (0);\n}\n\nstatic void\nvdev_initialize_xlate_last_rs_end(void *arg, range_seg64_t *physical_rs)\n{\n\tuint64_t *last_rs_end = (uint64_t *)arg;\n\n\tif (physical_rs->rs_end > *last_rs_end)\n\t\t*last_rs_end = physical_rs->rs_end;\n}\n\nstatic void\nvdev_initialize_xlate_progress(void *arg, range_seg64_t *physical_rs)\n{\n\tvdev_t *vd = (vdev_t *)arg;\n\n\tuint64_t size = physical_rs->rs_end - physical_rs->rs_start;\n\tvd->vdev_initialize_bytes_est += size;\n\n\tif (vd->vdev_initialize_last_offset > physical_rs->rs_end) {\n\t\tvd->vdev_initialize_bytes_done += size;\n\t} else if (vd->vdev_initialize_last_offset > physical_rs->rs_start &&\n\t    vd->vdev_initialize_last_offset < physical_rs->rs_end) {\n\t\tvd->vdev_initialize_bytes_done +=\n\t\t    vd->vdev_initialize_last_offset - physical_rs->rs_start;\n\t}\n}\n\nstatic void\nvdev_initialize_calculate_progress(vdev_t *vd)\n{\n\tASSERT(spa_config_held(vd->vdev_spa, SCL_CONFIG, RW_READER) ||\n\t    spa_config_held(vd->vdev_spa, SCL_CONFIG, RW_WRITER));\n\tASSERT(vd->vdev_leaf_zap != 0);\n\n\tvd->vdev_initialize_bytes_est = 0;\n\tvd->vdev_initialize_bytes_done = 0;\n\n\tfor (uint64_t i = 0; i < vd->vdev_top->vdev_ms_count; i++) {\n\t\tmetaslab_t *msp = vd->vdev_top->vdev_ms[i];\n\t\tmutex_enter(&msp->ms_lock);\n\n\t\tuint64_t ms_free = (msp->ms_size -\n\t\t    metaslab_allocated_space(msp)) /\n\t\t    vdev_get_ndisks(vd->vdev_top);\n\n\t\t \n\t\trange_seg64_t logical_rs, physical_rs, remain_rs;\n\t\tlogical_rs.rs_start = msp->ms_start;\n\t\tlogical_rs.rs_end = msp->ms_start + msp->ms_size;\n\n\t\t \n\t\tvdev_xlate(vd, &logical_rs, &physical_rs, &remain_rs);\n\t\tif (vd->vdev_initialize_last_offset <= physical_rs.rs_start) {\n\t\t\tvd->vdev_initialize_bytes_est += ms_free;\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tuint64_t last_rs_end = physical_rs.rs_end;\n\t\tif (!vdev_xlate_is_empty(&remain_rs)) {\n\t\t\tvdev_xlate_walk(vd, &remain_rs,\n\t\t\t    vdev_initialize_xlate_last_rs_end, &last_rs_end);\n\t\t}\n\n\t\tif (vd->vdev_initialize_last_offset > last_rs_end) {\n\t\t\tvd->vdev_initialize_bytes_done += ms_free;\n\t\t\tvd->vdev_initialize_bytes_est += ms_free;\n\t\t\tmutex_exit(&msp->ms_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tVERIFY0(metaslab_load(msp));\n\n\t\tzfs_btree_index_t where;\n\t\trange_tree_t *rt = msp->ms_allocatable;\n\t\tfor (range_seg_t *rs =\n\t\t    zfs_btree_first(&rt->rt_root, &where); rs;\n\t\t    rs = zfs_btree_next(&rt->rt_root, &where,\n\t\t    &where)) {\n\t\t\tlogical_rs.rs_start = rs_get_start(rs, rt);\n\t\t\tlogical_rs.rs_end = rs_get_end(rs, rt);\n\n\t\t\tvdev_xlate_walk(vd, &logical_rs,\n\t\t\t    vdev_initialize_xlate_progress, vd);\n\t\t}\n\t\tmutex_exit(&msp->ms_lock);\n\t}\n}\n\nstatic int\nvdev_initialize_load(vdev_t *vd)\n{\n\tint err = 0;\n\tASSERT(spa_config_held(vd->vdev_spa, SCL_CONFIG, RW_READER) ||\n\t    spa_config_held(vd->vdev_spa, SCL_CONFIG, RW_WRITER));\n\tASSERT(vd->vdev_leaf_zap != 0);\n\n\tif (vd->vdev_initialize_state == VDEV_INITIALIZE_ACTIVE ||\n\t    vd->vdev_initialize_state == VDEV_INITIALIZE_SUSPENDED) {\n\t\terr = zap_lookup(vd->vdev_spa->spa_meta_objset,\n\t\t    vd->vdev_leaf_zap, VDEV_LEAF_ZAP_INITIALIZE_LAST_OFFSET,\n\t\t    sizeof (vd->vdev_initialize_last_offset), 1,\n\t\t    &vd->vdev_initialize_last_offset);\n\t\tif (err == ENOENT) {\n\t\t\tvd->vdev_initialize_last_offset = 0;\n\t\t\terr = 0;\n\t\t}\n\t}\n\n\tvdev_initialize_calculate_progress(vd);\n\treturn (err);\n}\n\nstatic void\nvdev_initialize_xlate_range_add(void *arg, range_seg64_t *physical_rs)\n{\n\tvdev_t *vd = arg;\n\n\t \n\tif (physical_rs->rs_end <= vd->vdev_initialize_last_offset)\n\t\treturn;\n\n\t \n\tif (vd->vdev_initialize_last_offset > physical_rs->rs_start) {\n\t\tzfs_dbgmsg(\"range write: vd %s changed (%llu, %llu) to \"\n\t\t    \"(%llu, %llu)\", vd->vdev_path,\n\t\t    (u_longlong_t)physical_rs->rs_start,\n\t\t    (u_longlong_t)physical_rs->rs_end,\n\t\t    (u_longlong_t)vd->vdev_initialize_last_offset,\n\t\t    (u_longlong_t)physical_rs->rs_end);\n\t\tASSERT3U(physical_rs->rs_end, >,\n\t\t    vd->vdev_initialize_last_offset);\n\t\tphysical_rs->rs_start = vd->vdev_initialize_last_offset;\n\t}\n\n\tASSERT3U(physical_rs->rs_end, >, physical_rs->rs_start);\n\n\trange_tree_add(vd->vdev_initialize_tree, physical_rs->rs_start,\n\t    physical_rs->rs_end - physical_rs->rs_start);\n}\n\n \nstatic void\nvdev_initialize_range_add(void *arg, uint64_t start, uint64_t size)\n{\n\tvdev_t *vd = arg;\n\trange_seg64_t logical_rs;\n\tlogical_rs.rs_start = start;\n\tlogical_rs.rs_end = start + size;\n\n\tASSERT(vd->vdev_ops->vdev_op_leaf);\n\tvdev_xlate_walk(vd, &logical_rs, vdev_initialize_xlate_range_add, arg);\n}\n\nstatic __attribute__((noreturn)) void\nvdev_initialize_thread(void *arg)\n{\n\tvdev_t *vd = arg;\n\tspa_t *spa = vd->vdev_spa;\n\tint error = 0;\n\tuint64_t ms_count = 0;\n\n\tASSERT(vdev_is_concrete(vd));\n\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\n\tvd->vdev_initialize_last_offset = 0;\n\tVERIFY0(vdev_initialize_load(vd));\n\n\tabd_t *deadbeef = vdev_initialize_block_alloc();\n\n\tvd->vdev_initialize_tree = range_tree_create(NULL, RANGE_SEG64, NULL,\n\t    0, 0);\n\n\tfor (uint64_t i = 0; !vd->vdev_detached &&\n\t    i < vd->vdev_top->vdev_ms_count; i++) {\n\t\tmetaslab_t *msp = vd->vdev_top->vdev_ms[i];\n\t\tboolean_t unload_when_done = B_FALSE;\n\n\t\t \n\t\tif (vd->vdev_top->vdev_ms_count != ms_count) {\n\t\t\tvdev_initialize_calculate_progress(vd);\n\t\t\tms_count = vd->vdev_top->vdev_ms_count;\n\t\t}\n\n\t\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\t\tmetaslab_disable(msp);\n\t\tmutex_enter(&msp->ms_lock);\n\t\tif (!msp->ms_loaded && !msp->ms_loading)\n\t\t\tunload_when_done = B_TRUE;\n\t\tVERIFY0(metaslab_load(msp));\n\n\t\trange_tree_walk(msp->ms_allocatable, vdev_initialize_range_add,\n\t\t    vd);\n\t\tmutex_exit(&msp->ms_lock);\n\n\t\terror = vdev_initialize_ranges(vd, deadbeef);\n\t\tmetaslab_enable(msp, B_TRUE, unload_when_done);\n\t\tspa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);\n\n\t\trange_tree_vacate(vd->vdev_initialize_tree, NULL, NULL);\n\t\tif (error != 0)\n\t\t\tbreak;\n\t}\n\n\tspa_config_exit(spa, SCL_CONFIG, FTAG);\n\tmutex_enter(&vd->vdev_initialize_io_lock);\n\twhile (vd->vdev_initialize_inflight > 0) {\n\t\tcv_wait(&vd->vdev_initialize_io_cv,\n\t\t    &vd->vdev_initialize_io_lock);\n\t}\n\tmutex_exit(&vd->vdev_initialize_io_lock);\n\n\trange_tree_destroy(vd->vdev_initialize_tree);\n\tvdev_initialize_block_free(deadbeef);\n\tvd->vdev_initialize_tree = NULL;\n\n\tmutex_enter(&vd->vdev_initialize_lock);\n\tif (!vd->vdev_initialize_exit_wanted) {\n\t\tif (vdev_writeable(vd)) {\n\t\t\tvdev_initialize_change_state(vd,\n\t\t\t    VDEV_INITIALIZE_COMPLETE);\n\t\t} else if (vd->vdev_faulted) {\n\t\t\tvdev_initialize_change_state(vd,\n\t\t\t    VDEV_INITIALIZE_CANCELED);\n\t\t}\n\t}\n\tASSERT(vd->vdev_initialize_thread != NULL ||\n\t    vd->vdev_initialize_inflight == 0);\n\n\t \n\tmutex_exit(&vd->vdev_initialize_lock);\n\ttxg_wait_synced(spa_get_dsl(spa), 0);\n\tmutex_enter(&vd->vdev_initialize_lock);\n\n\tvd->vdev_initialize_thread = NULL;\n\tcv_broadcast(&vd->vdev_initialize_cv);\n\tmutex_exit(&vd->vdev_initialize_lock);\n\n\tthread_exit();\n}\n\n \nvoid\nvdev_initialize(vdev_t *vd)\n{\n\tASSERT(MUTEX_HELD(&vd->vdev_initialize_lock));\n\tASSERT(vd->vdev_ops->vdev_op_leaf);\n\tASSERT(vdev_is_concrete(vd));\n\tASSERT3P(vd->vdev_initialize_thread, ==, NULL);\n\tASSERT(!vd->vdev_detached);\n\tASSERT(!vd->vdev_initialize_exit_wanted);\n\tASSERT(!vd->vdev_top->vdev_removing);\n\n\tvdev_initialize_change_state(vd, VDEV_INITIALIZE_ACTIVE);\n\tvd->vdev_initialize_thread = thread_create(NULL, 0,\n\t    vdev_initialize_thread, vd, 0, &p0, TS_RUN, maxclsyspri);\n}\n\n \nvoid\nvdev_uninitialize(vdev_t *vd)\n{\n\tASSERT(MUTEX_HELD(&vd->vdev_initialize_lock));\n\tASSERT(vd->vdev_ops->vdev_op_leaf);\n\tASSERT(vdev_is_concrete(vd));\n\tASSERT3P(vd->vdev_initialize_thread, ==, NULL);\n\tASSERT(!vd->vdev_detached);\n\tASSERT(!vd->vdev_initialize_exit_wanted);\n\tASSERT(!vd->vdev_top->vdev_removing);\n\n\tvdev_initialize_change_state(vd, VDEV_INITIALIZE_NONE);\n}\n\n \nstatic void\nvdev_initialize_stop_wait_impl(vdev_t *vd)\n{\n\tASSERT(MUTEX_HELD(&vd->vdev_initialize_lock));\n\n\twhile (vd->vdev_initialize_thread != NULL)\n\t\tcv_wait(&vd->vdev_initialize_cv, &vd->vdev_initialize_lock);\n\n\tASSERT3P(vd->vdev_initialize_thread, ==, NULL);\n\tvd->vdev_initialize_exit_wanted = B_FALSE;\n}\n\n \nvoid\nvdev_initialize_stop_wait(spa_t *spa, list_t *vd_list)\n{\n\t(void) spa;\n\tvdev_t *vd;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\twhile ((vd = list_remove_head(vd_list)) != NULL) {\n\t\tmutex_enter(&vd->vdev_initialize_lock);\n\t\tvdev_initialize_stop_wait_impl(vd);\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t}\n}\n\n \nvoid\nvdev_initialize_stop(vdev_t *vd, vdev_initializing_state_t tgt_state,\n    list_t *vd_list)\n{\n\tASSERT(!spa_config_held(vd->vdev_spa, SCL_CONFIG|SCL_STATE, RW_WRITER));\n\tASSERT(MUTEX_HELD(&vd->vdev_initialize_lock));\n\tASSERT(vd->vdev_ops->vdev_op_leaf);\n\tASSERT(vdev_is_concrete(vd));\n\n\t \n\tif (vd->vdev_initialize_thread == NULL &&\n\t    tgt_state != VDEV_INITIALIZE_CANCELED) {\n\t\treturn;\n\t}\n\n\tvdev_initialize_change_state(vd, tgt_state);\n\tvd->vdev_initialize_exit_wanted = B_TRUE;\n\n\tif (vd_list == NULL) {\n\t\tvdev_initialize_stop_wait_impl(vd);\n\t} else {\n\t\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\t\tlist_insert_tail(vd_list, vd);\n\t}\n}\n\nstatic void\nvdev_initialize_stop_all_impl(vdev_t *vd, vdev_initializing_state_t tgt_state,\n    list_t *vd_list)\n{\n\tif (vd->vdev_ops->vdev_op_leaf && vdev_is_concrete(vd)) {\n\t\tmutex_enter(&vd->vdev_initialize_lock);\n\t\tvdev_initialize_stop(vd, tgt_state, vd_list);\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t\treturn;\n\t}\n\n\tfor (uint64_t i = 0; i < vd->vdev_children; i++) {\n\t\tvdev_initialize_stop_all_impl(vd->vdev_child[i], tgt_state,\n\t\t    vd_list);\n\t}\n}\n\n \nvoid\nvdev_initialize_stop_all(vdev_t *vd, vdev_initializing_state_t tgt_state)\n{\n\tspa_t *spa = vd->vdev_spa;\n\tlist_t vd_list;\n\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\n\tlist_create(&vd_list, sizeof (vdev_t),\n\t    offsetof(vdev_t, vdev_initialize_node));\n\n\tvdev_initialize_stop_all_impl(vd, tgt_state, &vd_list);\n\tvdev_initialize_stop_wait(spa, &vd_list);\n\n\tif (vd->vdev_spa->spa_sync_on) {\n\t\t \n\t\ttxg_wait_synced(spa_get_dsl(vd->vdev_spa), 0);\n\t}\n\n\tlist_destroy(&vd_list);\n}\n\nvoid\nvdev_initialize_restart(vdev_t *vd)\n{\n\tASSERT(MUTEX_HELD(&spa_namespace_lock));\n\tASSERT(!spa_config_held(vd->vdev_spa, SCL_ALL, RW_WRITER));\n\n\tif (vd->vdev_leaf_zap != 0) {\n\t\tmutex_enter(&vd->vdev_initialize_lock);\n\t\tuint64_t initialize_state = VDEV_INITIALIZE_NONE;\n\t\tint err = zap_lookup(vd->vdev_spa->spa_meta_objset,\n\t\t    vd->vdev_leaf_zap, VDEV_LEAF_ZAP_INITIALIZE_STATE,\n\t\t    sizeof (initialize_state), 1, &initialize_state);\n\t\tASSERT(err == 0 || err == ENOENT);\n\t\tvd->vdev_initialize_state = initialize_state;\n\n\t\tuint64_t timestamp = 0;\n\t\terr = zap_lookup(vd->vdev_spa->spa_meta_objset,\n\t\t    vd->vdev_leaf_zap, VDEV_LEAF_ZAP_INITIALIZE_ACTION_TIME,\n\t\t    sizeof (timestamp), 1, &timestamp);\n\t\tASSERT(err == 0 || err == ENOENT);\n\t\tvd->vdev_initialize_action_time = timestamp;\n\n\t\tif (vd->vdev_initialize_state == VDEV_INITIALIZE_SUSPENDED ||\n\t\t    vd->vdev_offline) {\n\t\t\t \n\t\t\tVERIFY0(vdev_initialize_load(vd));\n\t\t} else if (vd->vdev_initialize_state ==\n\t\t    VDEV_INITIALIZE_ACTIVE && vdev_writeable(vd) &&\n\t\t    !vd->vdev_top->vdev_removing &&\n\t\t    vd->vdev_initialize_thread == NULL) {\n\t\t\tvdev_initialize(vd);\n\t\t}\n\n\t\tmutex_exit(&vd->vdev_initialize_lock);\n\t}\n\n\tfor (uint64_t i = 0; i < vd->vdev_children; i++) {\n\t\tvdev_initialize_restart(vd->vdev_child[i]);\n\t}\n}\n\nEXPORT_SYMBOL(vdev_initialize);\nEXPORT_SYMBOL(vdev_uninitialize);\nEXPORT_SYMBOL(vdev_initialize_stop);\nEXPORT_SYMBOL(vdev_initialize_stop_all);\nEXPORT_SYMBOL(vdev_initialize_stop_wait);\nEXPORT_SYMBOL(vdev_initialize_restart);\n\nZFS_MODULE_PARAM(zfs, zfs_, initialize_value, U64, ZMOD_RW,\n\t\"Value written during zpool initialize\");\n\nZFS_MODULE_PARAM(zfs, zfs_, initialize_chunk_size, U64, ZMOD_RW,\n\t\"Size in bytes of writes by zpool initialize\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}