{
  "module_name": "zfs_zstd.c",
  "hash_id": "9c596db9227919e3463b51929274b561913156757ebe48b84d8508f7f679e679",
  "original_prompt": "Ingested from zfs-2.2.2/module/zstd/zfs_zstd.c",
  "human_readable_source": " \n\n \n\n#include <sys/param.h>\n#include <sys/sysmacros.h>\n#include <sys/zfs_context.h>\n#include <sys/zio_compress.h>\n#include <sys/spa.h>\n#include <sys/zstd/zstd.h>\n\n#define\tZSTD_STATIC_LINKING_ONLY\n#include \"lib/zstd.h\"\n#include \"lib/common/zstd_errors.h\"\n\nstatic uint_t zstd_earlyabort_pass = 1;\nstatic int zstd_cutoff_level = ZIO_ZSTD_LEVEL_3;\nstatic unsigned int zstd_abort_size = (128 * 1024);\n\nstatic kstat_t *zstd_ksp = NULL;\n\ntypedef struct zstd_stats {\n\tkstat_named_t\tzstd_stat_alloc_fail;\n\tkstat_named_t\tzstd_stat_alloc_fallback;\n\tkstat_named_t\tzstd_stat_com_alloc_fail;\n\tkstat_named_t\tzstd_stat_dec_alloc_fail;\n\tkstat_named_t\tzstd_stat_com_inval;\n\tkstat_named_t\tzstd_stat_dec_inval;\n\tkstat_named_t\tzstd_stat_dec_header_inval;\n\tkstat_named_t\tzstd_stat_com_fail;\n\tkstat_named_t\tzstd_stat_dec_fail;\n\t \n\tkstat_named_t\tzstd_stat_lz4pass_allowed;\n\tkstat_named_t\tzstd_stat_lz4pass_rejected;\n\t \n\tkstat_named_t\tzstd_stat_zstdpass_allowed;\n\tkstat_named_t\tzstd_stat_zstdpass_rejected;\n\t \n\tkstat_named_t\tzstd_stat_passignored;\n\tkstat_named_t\tzstd_stat_passignored_size;\n\tkstat_named_t\tzstd_stat_buffers;\n\tkstat_named_t\tzstd_stat_size;\n} zstd_stats_t;\n\nstatic zstd_stats_t zstd_stats = {\n\t{ \"alloc_fail\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"alloc_fallback\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"compress_alloc_fail\",\tKSTAT_DATA_UINT64 },\n\t{ \"decompress_alloc_fail\",\tKSTAT_DATA_UINT64 },\n\t{ \"compress_level_invalid\",\tKSTAT_DATA_UINT64 },\n\t{ \"decompress_level_invalid\",\tKSTAT_DATA_UINT64 },\n\t{ \"decompress_header_invalid\",\tKSTAT_DATA_UINT64 },\n\t{ \"compress_failed\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"decompress_failed\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"lz4pass_allowed\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"lz4pass_rejected\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"zstdpass_allowed\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"zstdpass_rejected\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"passignored\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"passignored_size\",\t\tKSTAT_DATA_UINT64 },\n\t{ \"buffers\",\t\t\tKSTAT_DATA_UINT64 },\n\t{ \"size\",\t\t\tKSTAT_DATA_UINT64 },\n};\n\n#ifdef _KERNEL\nstatic int\nkstat_zstd_update(kstat_t *ksp, int rw)\n{\n\tASSERT(ksp != NULL);\n\n\tif (rw == KSTAT_WRITE && ksp == zstd_ksp) {\n\t\tZSTDSTAT_ZERO(zstd_stat_alloc_fail);\n\t\tZSTDSTAT_ZERO(zstd_stat_alloc_fallback);\n\t\tZSTDSTAT_ZERO(zstd_stat_com_alloc_fail);\n\t\tZSTDSTAT_ZERO(zstd_stat_dec_alloc_fail);\n\t\tZSTDSTAT_ZERO(zstd_stat_com_inval);\n\t\tZSTDSTAT_ZERO(zstd_stat_dec_inval);\n\t\tZSTDSTAT_ZERO(zstd_stat_dec_header_inval);\n\t\tZSTDSTAT_ZERO(zstd_stat_com_fail);\n\t\tZSTDSTAT_ZERO(zstd_stat_dec_fail);\n\t\tZSTDSTAT_ZERO(zstd_stat_lz4pass_allowed);\n\t\tZSTDSTAT_ZERO(zstd_stat_lz4pass_rejected);\n\t\tZSTDSTAT_ZERO(zstd_stat_zstdpass_allowed);\n\t\tZSTDSTAT_ZERO(zstd_stat_zstdpass_rejected);\n\t\tZSTDSTAT_ZERO(zstd_stat_passignored);\n\t\tZSTDSTAT_ZERO(zstd_stat_passignored_size);\n\t}\n\n\treturn (0);\n}\n#endif\n\n \nenum zstd_kmem_type {\n\tZSTD_KMEM_UNKNOWN = 0,\n\t \n\tZSTD_KMEM_DEFAULT,\n\t \n\tZSTD_KMEM_POOL,\n\t \n\tZSTD_KMEM_DCTX,\n\tZSTD_KMEM_COUNT,\n};\n\n \nstruct zstd_pool {\n\tvoid *mem;\n\tsize_t size;\n\tkmutex_t barrier;\n\thrtime_t timeout;\n};\n\n \nstruct zstd_kmem {\n\tenum zstd_kmem_type kmem_type;\n\tsize_t kmem_size;\n\tstruct zstd_pool *pool;\n};\n\n \nstruct zstd_fallback_mem {\n\tsize_t mem_size;\n\tvoid *mem;\n\tkmutex_t barrier;\n};\n\nstruct zstd_levelmap {\n\tint16_t zstd_level;\n\tenum zio_zstd_levels level;\n};\n\n \nstatic void *zstd_alloc(void *opaque, size_t size);\nstatic void *zstd_dctx_alloc(void *opaque, size_t size);\nstatic void zstd_free(void *opaque, void *ptr);\n\n \nstatic const ZSTD_customMem zstd_malloc = {\n\tzstd_alloc,\n\tzstd_free,\n\tNULL,\n};\n\n \nstatic const ZSTD_customMem zstd_dctx_malloc = {\n\tzstd_dctx_alloc,\n\tzstd_free,\n\tNULL,\n};\n\n \nstatic struct zstd_levelmap zstd_levels[] = {\n\t{ZIO_ZSTD_LEVEL_1, ZIO_ZSTD_LEVEL_1},\n\t{ZIO_ZSTD_LEVEL_2, ZIO_ZSTD_LEVEL_2},\n\t{ZIO_ZSTD_LEVEL_3, ZIO_ZSTD_LEVEL_3},\n\t{ZIO_ZSTD_LEVEL_4, ZIO_ZSTD_LEVEL_4},\n\t{ZIO_ZSTD_LEVEL_5, ZIO_ZSTD_LEVEL_5},\n\t{ZIO_ZSTD_LEVEL_6, ZIO_ZSTD_LEVEL_6},\n\t{ZIO_ZSTD_LEVEL_7, ZIO_ZSTD_LEVEL_7},\n\t{ZIO_ZSTD_LEVEL_8, ZIO_ZSTD_LEVEL_8},\n\t{ZIO_ZSTD_LEVEL_9, ZIO_ZSTD_LEVEL_9},\n\t{ZIO_ZSTD_LEVEL_10, ZIO_ZSTD_LEVEL_10},\n\t{ZIO_ZSTD_LEVEL_11, ZIO_ZSTD_LEVEL_11},\n\t{ZIO_ZSTD_LEVEL_12, ZIO_ZSTD_LEVEL_12},\n\t{ZIO_ZSTD_LEVEL_13, ZIO_ZSTD_LEVEL_13},\n\t{ZIO_ZSTD_LEVEL_14, ZIO_ZSTD_LEVEL_14},\n\t{ZIO_ZSTD_LEVEL_15, ZIO_ZSTD_LEVEL_15},\n\t{ZIO_ZSTD_LEVEL_16, ZIO_ZSTD_LEVEL_16},\n\t{ZIO_ZSTD_LEVEL_17, ZIO_ZSTD_LEVEL_17},\n\t{ZIO_ZSTD_LEVEL_18, ZIO_ZSTD_LEVEL_18},\n\t{ZIO_ZSTD_LEVEL_19, ZIO_ZSTD_LEVEL_19},\n\t{-1, ZIO_ZSTD_LEVEL_FAST_1},\n\t{-2, ZIO_ZSTD_LEVEL_FAST_2},\n\t{-3, ZIO_ZSTD_LEVEL_FAST_3},\n\t{-4, ZIO_ZSTD_LEVEL_FAST_4},\n\t{-5, ZIO_ZSTD_LEVEL_FAST_5},\n\t{-6, ZIO_ZSTD_LEVEL_FAST_6},\n\t{-7, ZIO_ZSTD_LEVEL_FAST_7},\n\t{-8, ZIO_ZSTD_LEVEL_FAST_8},\n\t{-9, ZIO_ZSTD_LEVEL_FAST_9},\n\t{-10, ZIO_ZSTD_LEVEL_FAST_10},\n\t{-20, ZIO_ZSTD_LEVEL_FAST_20},\n\t{-30, ZIO_ZSTD_LEVEL_FAST_30},\n\t{-40, ZIO_ZSTD_LEVEL_FAST_40},\n\t{-50, ZIO_ZSTD_LEVEL_FAST_50},\n\t{-60, ZIO_ZSTD_LEVEL_FAST_60},\n\t{-70, ZIO_ZSTD_LEVEL_FAST_70},\n\t{-80, ZIO_ZSTD_LEVEL_FAST_80},\n\t{-90, ZIO_ZSTD_LEVEL_FAST_90},\n\t{-100, ZIO_ZSTD_LEVEL_FAST_100},\n\t{-500, ZIO_ZSTD_LEVEL_FAST_500},\n\t{-1000, ZIO_ZSTD_LEVEL_FAST_1000},\n};\n\n \nstatic int pool_count = 16;\n\n#define\tZSTD_POOL_MAX\t\tpool_count\n#define\tZSTD_POOL_TIMEOUT\t60 * 2\n\nstatic struct zstd_fallback_mem zstd_dctx_fallback;\nstatic struct zstd_pool *zstd_mempool_cctx;\nstatic struct zstd_pool *zstd_mempool_dctx;\n\n \n#if defined(ZFS_ASAN_ENABLED)\n#define\tADDRESS_SANITIZER 1\n#endif\n#if defined(_KERNEL) && defined(ADDRESS_SANITIZER)\nvoid __asan_unpoison_memory_region(void const volatile *addr, size_t size);\nvoid __asan_poison_memory_region(void const volatile *addr, size_t size);\nvoid __asan_unpoison_memory_region(void const volatile *addr, size_t size) {};\nvoid __asan_poison_memory_region(void const volatile *addr, size_t size) {};\n#endif\n\n\nstatic void\nzstd_mempool_reap(struct zstd_pool *zstd_mempool)\n{\n\tstruct zstd_pool *pool;\n\n\tif (!zstd_mempool || !ZSTDSTAT(zstd_stat_buffers)) {\n\t\treturn;\n\t}\n\n\t \n\tfor (int i = 0; i < ZSTD_POOL_MAX; i++) {\n\t\tpool = &zstd_mempool[i];\n\t\tif (pool->mem && mutex_tryenter(&pool->barrier)) {\n\t\t\t \n\t\t\tif (pool->mem && gethrestime_sec() > pool->timeout) {\n\t\t\t\tvmem_free(pool->mem, pool->size);\n\t\t\t\tZSTDSTAT_SUB(zstd_stat_buffers, 1);\n\t\t\t\tZSTDSTAT_SUB(zstd_stat_size, pool->size);\n\t\t\t\tpool->mem = NULL;\n\t\t\t\tpool->size = 0;\n\t\t\t\tpool->timeout = 0;\n\t\t\t}\n\t\t\tmutex_exit(&pool->barrier);\n\t\t}\n\t}\n}\n\n \n\nstatic void *\nzstd_mempool_alloc(struct zstd_pool *zstd_mempool, size_t size)\n{\n\tstruct zstd_pool *pool;\n\tstruct zstd_kmem *mem = NULL;\n\n\tif (!zstd_mempool) {\n\t\treturn (NULL);\n\t}\n\n\t \n\tfor (int i = 0; i < ZSTD_POOL_MAX; i++) {\n\t\tpool = &zstd_mempool[i];\n\t\t \n\t\tif (mutex_tryenter(&pool->barrier)) {\n\t\t\t \n\t\t\tif (pool->mem && size <= pool->size) {\n\t\t\t\tpool->timeout = gethrestime_sec() +\n\t\t\t\t    ZSTD_POOL_TIMEOUT;\n\t\t\t\tmem = pool->mem;\n\t\t\t\treturn (mem);\n\t\t\t}\n\t\t\tmutex_exit(&pool->barrier);\n\t\t}\n\t}\n\n\t \n\tfor (int i = 0; i < ZSTD_POOL_MAX; i++) {\n\t\tpool = &zstd_mempool[i];\n\t\tif (mutex_tryenter(&pool->barrier)) {\n\t\t\t \n\t\t\tif (!pool->mem) {\n\t\t\t\tmem = vmem_alloc(size, KM_SLEEP);\n\t\t\t\tif (mem) {\n\t\t\t\t\tZSTDSTAT_ADD(zstd_stat_buffers, 1);\n\t\t\t\t\tZSTDSTAT_ADD(zstd_stat_size, size);\n\t\t\t\t\tpool->mem = mem;\n\t\t\t\t\tpool->size = size;\n\t\t\t\t\t \n\t\t\t\t\tmem->pool = pool;\n\t\t\t\t\tmem->kmem_type = ZSTD_KMEM_POOL;\n\t\t\t\t\tmem->kmem_size = size;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (size <= pool->size) {\n\t\t\t\t \n\t\t\t\tpool->timeout = gethrestime_sec() +\n\t\t\t\t    ZSTD_POOL_TIMEOUT;\n\n\t\t\t\treturn (pool->mem);\n\t\t\t}\n\n\t\t\tmutex_exit(&pool->barrier);\n\t\t}\n\t}\n\n\t \n\tif (!mem) {\n\t\tmem = vmem_alloc(size, KM_NOSLEEP);\n\t\tif (mem) {\n\t\t\tmem->pool = NULL;\n\t\t\tmem->kmem_type = ZSTD_KMEM_DEFAULT;\n\t\t\tmem->kmem_size = size;\n\t\t}\n\t}\n\n\treturn (mem);\n}\n\n \nstatic void\nzstd_mempool_free(struct zstd_kmem *z)\n{\n\tmutex_exit(&z->pool->barrier);\n}\n\n \nstatic int\nzstd_enum_to_level(enum zio_zstd_levels level, int16_t *zstd_level)\n{\n\tif (level > 0 && level <= ZIO_ZSTD_LEVEL_19) {\n\t\t*zstd_level = zstd_levels[level - 1].zstd_level;\n\t\treturn (0);\n\t}\n\tif (level >= ZIO_ZSTD_LEVEL_FAST_1 &&\n\t    level <= ZIO_ZSTD_LEVEL_FAST_1000) {\n\t\t*zstd_level = zstd_levels[level - ZIO_ZSTD_LEVEL_FAST_1\n\t\t    + ZIO_ZSTD_LEVEL_19].zstd_level;\n\t\treturn (0);\n\t}\n\n\t \n\treturn (1);\n}\n\n\nsize_t\nzfs_zstd_compress_wrap(void *s_start, void *d_start, size_t s_len, size_t d_len,\n    int level)\n{\n\tint16_t zstd_level;\n\tif (zstd_enum_to_level(level, &zstd_level)) {\n\t\tZSTDSTAT_BUMP(zstd_stat_com_inval);\n\t\treturn (s_len);\n\t}\n\t \n\tsize_t actual_abort_size = zstd_abort_size;\n\tif (zstd_earlyabort_pass > 0 && zstd_level >= zstd_cutoff_level &&\n\t    s_len >= actual_abort_size) {\n\t\tint pass_len = 1;\n\t\tpass_len = lz4_compress_zfs(s_start, d_start, s_len, d_len, 0);\n\t\tif (pass_len < d_len) {\n\t\t\tZSTDSTAT_BUMP(zstd_stat_lz4pass_allowed);\n\t\t\tgoto keep_trying;\n\t\t}\n\t\tZSTDSTAT_BUMP(zstd_stat_lz4pass_rejected);\n\n\t\tpass_len = zfs_zstd_compress(s_start, d_start, s_len, d_len,\n\t\t    ZIO_ZSTD_LEVEL_1);\n\t\tif (pass_len == s_len || pass_len <= 0 || pass_len > d_len) {\n\t\t\tZSTDSTAT_BUMP(zstd_stat_zstdpass_rejected);\n\t\t\treturn (s_len);\n\t\t}\n\t\tZSTDSTAT_BUMP(zstd_stat_zstdpass_allowed);\n\t} else {\n\t\tZSTDSTAT_BUMP(zstd_stat_passignored);\n\t\tif (s_len < actual_abort_size) {\n\t\t\tZSTDSTAT_BUMP(zstd_stat_passignored_size);\n\t\t}\n\t}\nkeep_trying:\n\treturn (zfs_zstd_compress(s_start, d_start, s_len, d_len, level));\n\n}\n\n \nsize_t\nzfs_zstd_compress(void *s_start, void *d_start, size_t s_len, size_t d_len,\n    int level)\n{\n\tsize_t c_len;\n\tint16_t zstd_level;\n\tzfs_zstdhdr_t *hdr;\n\tZSTD_CCtx *cctx;\n\n\thdr = (zfs_zstdhdr_t *)d_start;\n\n\t \n\tif (zstd_enum_to_level(level, &zstd_level)) {\n\t\tZSTDSTAT_BUMP(zstd_stat_com_inval);\n\t\treturn (s_len);\n\t}\n\n\tASSERT3U(d_len, >=, sizeof (*hdr));\n\tASSERT3U(d_len, <=, s_len);\n\tASSERT3U(zstd_level, !=, 0);\n\n\tcctx = ZSTD_createCCtx_advanced(zstd_malloc);\n\n\t \n\tif (!cctx) {\n\t\tZSTDSTAT_BUMP(zstd_stat_com_alloc_fail);\n\t\treturn (s_len);\n\t}\n\n\t \n\tZSTD_CCtx_setParameter(cctx, ZSTD_c_compressionLevel, zstd_level);\n\n\t \n\tZSTD_CCtx_setParameter(cctx, ZSTD_c_format, ZSTD_f_zstd1_magicless);\n\n\t \n\tZSTD_CCtx_setParameter(cctx, ZSTD_c_checksumFlag, 0);\n\tZSTD_CCtx_setParameter(cctx, ZSTD_c_contentSizeFlag, 0);\n\n\tc_len = ZSTD_compress2(cctx,\n\t    hdr->data,\n\t    d_len - sizeof (*hdr),\n\t    s_start, s_len);\n\n\tZSTD_freeCCtx(cctx);\n\n\t \n\tif (ZSTD_isError(c_len)) {\n\t\t \n\t\tint err = ZSTD_getErrorCode(c_len);\n\t\tif (err != ZSTD_error_dstSize_tooSmall) {\n\t\t\tZSTDSTAT_BUMP(zstd_stat_com_fail);\n\t\t\tdprintf(\"Error: %s\", ZSTD_getErrorString(err));\n\t\t}\n\t\treturn (s_len);\n\t}\n\n\t \n\thdr->c_len = BE_32(c_len);\n\n\t \n\tASSERT3U(ZSTD_VERSION_NUMBER, <=, 0xFFFFFF);\n\n\t \n\tzfs_set_hdrversion(hdr, ZSTD_VERSION_NUMBER);\n\tzfs_set_hdrlevel(hdr, level);\n\thdr->raw_version_level = BE_32(hdr->raw_version_level);\n\n\treturn (c_len + sizeof (*hdr));\n}\n\n \nint\nzfs_zstd_decompress_level(void *s_start, void *d_start, size_t s_len,\n    size_t d_len, uint8_t *level)\n{\n\tZSTD_DCtx *dctx;\n\tsize_t result;\n\tint16_t zstd_level;\n\tuint32_t c_len;\n\tconst zfs_zstdhdr_t *hdr;\n\tzfs_zstdhdr_t hdr_copy;\n\n\thdr = (const zfs_zstdhdr_t *)s_start;\n\tc_len = BE_32(hdr->c_len);\n\n\t \n\thdr_copy.raw_version_level = BE_32(hdr->raw_version_level);\n\tuint8_t curlevel = zfs_get_hdrlevel(&hdr_copy);\n\n\t \n\n\t \n\tif (zstd_enum_to_level(curlevel, &zstd_level)) {\n\t\tZSTDSTAT_BUMP(zstd_stat_dec_inval);\n\t\treturn (1);\n\t}\n\n\tASSERT3U(d_len, >=, s_len);\n\tASSERT3U(curlevel, !=, ZIO_COMPLEVEL_INHERIT);\n\n\t \n\tif (c_len + sizeof (*hdr) > s_len) {\n\t\tZSTDSTAT_BUMP(zstd_stat_dec_header_inval);\n\t\treturn (1);\n\t}\n\n\tdctx = ZSTD_createDCtx_advanced(zstd_dctx_malloc);\n\tif (!dctx) {\n\t\tZSTDSTAT_BUMP(zstd_stat_dec_alloc_fail);\n\t\treturn (1);\n\t}\n\n\t \n\tZSTD_DCtx_setParameter(dctx, ZSTD_d_format, ZSTD_f_zstd1_magicless);\n\n\t \n\tresult = ZSTD_decompressDCtx(dctx, d_start, d_len, hdr->data, c_len);\n\tZSTD_freeDCtx(dctx);\n\n\t \n\tif (ZSTD_isError(result)) {\n\t\tZSTDSTAT_BUMP(zstd_stat_dec_fail);\n\t\treturn (1);\n\t}\n\n\tif (level) {\n\t\t*level = curlevel;\n\t}\n\n\treturn (0);\n}\n\n \nint\nzfs_zstd_decompress(void *s_start, void *d_start, size_t s_len, size_t d_len,\n    int level __maybe_unused)\n{\n\n\treturn (zfs_zstd_decompress_level(s_start, d_start, s_len, d_len,\n\t    NULL));\n}\n\n \nstatic void *\nzstd_alloc(void *opaque __maybe_unused, size_t size)\n{\n\tsize_t nbytes = sizeof (struct zstd_kmem) + size;\n\tstruct zstd_kmem *z = NULL;\n\n\tz = (struct zstd_kmem *)zstd_mempool_alloc(zstd_mempool_cctx, nbytes);\n\n\tif (!z) {\n\t\tZSTDSTAT_BUMP(zstd_stat_alloc_fail);\n\t\treturn (NULL);\n\t}\n\n\treturn ((void*)z + (sizeof (struct zstd_kmem)));\n}\n\n \nstatic void *\nzstd_dctx_alloc(void *opaque __maybe_unused, size_t size)\n{\n\tsize_t nbytes = sizeof (struct zstd_kmem) + size;\n\tstruct zstd_kmem *z = NULL;\n\tenum zstd_kmem_type type = ZSTD_KMEM_DEFAULT;\n\n\tz = (struct zstd_kmem *)zstd_mempool_alloc(zstd_mempool_dctx, nbytes);\n\tif (!z) {\n\t\t \n\t\tz = vmem_alloc(nbytes, KM_SLEEP);\n\t\tif (z) {\n\t\t\tz->pool = NULL;\n\t\t}\n\t\tZSTDSTAT_BUMP(zstd_stat_alloc_fail);\n\t} else {\n\t\treturn ((void*)z + (sizeof (struct zstd_kmem)));\n\t}\n\n\t \n\tif (!z) {\n\t\t \n\t\tmutex_enter(&zstd_dctx_fallback.barrier);\n\n\t\tz = zstd_dctx_fallback.mem;\n\t\ttype = ZSTD_KMEM_DCTX;\n\t\tZSTDSTAT_BUMP(zstd_stat_alloc_fallback);\n\t}\n\n\t \n\tif (!z) {\n\t\treturn (NULL);\n\t}\n\n\tz->kmem_type = type;\n\tz->kmem_size = nbytes;\n\n\treturn ((void*)z + (sizeof (struct zstd_kmem)));\n}\n\n \nstatic void\nzstd_free(void *opaque __maybe_unused, void *ptr)\n{\n\tstruct zstd_kmem *z = (ptr - sizeof (struct zstd_kmem));\n\tenum zstd_kmem_type type;\n\n\tASSERT3U(z->kmem_type, <, ZSTD_KMEM_COUNT);\n\tASSERT3U(z->kmem_type, >, ZSTD_KMEM_UNKNOWN);\n\n\ttype = z->kmem_type;\n\tswitch (type) {\n\tcase ZSTD_KMEM_DEFAULT:\n\t\tvmem_free(z, z->kmem_size);\n\t\tbreak;\n\tcase ZSTD_KMEM_POOL:\n\t\tzstd_mempool_free(z);\n\t\tbreak;\n\tcase ZSTD_KMEM_DCTX:\n\t\tmutex_exit(&zstd_dctx_fallback.barrier);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nstatic void __init\ncreate_fallback_mem(struct zstd_fallback_mem *mem, size_t size)\n{\n\tmem->mem_size = size;\n\tmem->mem = vmem_zalloc(mem->mem_size, KM_SLEEP);\n\tmutex_init(&mem->barrier, NULL, MUTEX_DEFAULT, NULL);\n}\n\n \nstatic void __init\nzstd_mempool_init(void)\n{\n\tzstd_mempool_cctx =\n\t    kmem_zalloc(ZSTD_POOL_MAX * sizeof (struct zstd_pool), KM_SLEEP);\n\tzstd_mempool_dctx =\n\t    kmem_zalloc(ZSTD_POOL_MAX * sizeof (struct zstd_pool), KM_SLEEP);\n\n\tfor (int i = 0; i < ZSTD_POOL_MAX; i++) {\n\t\tmutex_init(&zstd_mempool_cctx[i].barrier, NULL,\n\t\t    MUTEX_DEFAULT, NULL);\n\t\tmutex_init(&zstd_mempool_dctx[i].barrier, NULL,\n\t\t    MUTEX_DEFAULT, NULL);\n\t}\n}\n\n \nstatic int __init\nzstd_meminit(void)\n{\n\tzstd_mempool_init();\n\n\t \n\tcreate_fallback_mem(&zstd_dctx_fallback,\n\t    P2ROUNDUP(ZSTD_estimateDCtxSize() + sizeof (struct zstd_kmem),\n\t    PAGESIZE));\n\n\treturn (0);\n}\n\n \nstatic void\nrelease_pool(struct zstd_pool *pool)\n{\n\tmutex_destroy(&pool->barrier);\n\tvmem_free(pool->mem, pool->size);\n\tpool->mem = NULL;\n\tpool->size = 0;\n}\n\n \nstatic void\nzstd_mempool_deinit(void)\n{\n\tfor (int i = 0; i < ZSTD_POOL_MAX; i++) {\n\t\trelease_pool(&zstd_mempool_cctx[i]);\n\t\trelease_pool(&zstd_mempool_dctx[i]);\n\t}\n\n\tkmem_free(zstd_mempool_dctx, ZSTD_POOL_MAX * sizeof (struct zstd_pool));\n\tkmem_free(zstd_mempool_cctx, ZSTD_POOL_MAX * sizeof (struct zstd_pool));\n\tzstd_mempool_dctx = NULL;\n\tzstd_mempool_cctx = NULL;\n}\n\n \n\nvoid\nzfs_zstd_cache_reap_now(void)\n{\n\t \n\tzstd_mempool_reap(zstd_mempool_cctx);\n\tzstd_mempool_reap(zstd_mempool_dctx);\n}\n\nextern int __init\nzstd_init(void)\n{\n\t \n\tpool_count = (boot_ncpus * 4);\n\tzstd_meminit();\n\n\t \n\tzstd_ksp = kstat_create(\"zfs\", 0, \"zstd\", \"misc\",\n\t    KSTAT_TYPE_NAMED, sizeof (zstd_stats) / sizeof (kstat_named_t),\n\t    KSTAT_FLAG_VIRTUAL);\n\tif (zstd_ksp != NULL) {\n\t\tzstd_ksp->ks_data = &zstd_stats;\n\t\tkstat_install(zstd_ksp);\n#ifdef _KERNEL\n\t\tzstd_ksp->ks_update = kstat_zstd_update;\n#endif\n\t}\n\n\treturn (0);\n}\n\nextern void\nzstd_fini(void)\n{\n\t \n\tif (zstd_ksp != NULL) {\n\t\tkstat_delete(zstd_ksp);\n\t\tzstd_ksp = NULL;\n\t}\n\n\t \n\tvmem_free(zstd_dctx_fallback.mem, zstd_dctx_fallback.mem_size);\n\tmutex_destroy(&zstd_dctx_fallback.barrier);\n\n\t \n\tzstd_mempool_deinit();\n}\n\n#if defined(_KERNEL)\n#ifdef __FreeBSD__\nmodule_init(zstd_init);\nmodule_exit(zstd_fini);\n#endif\n\nZFS_MODULE_PARAM(zfs, zstd_, earlyabort_pass, UINT, ZMOD_RW,\n\t\"Enable early abort attempts when using zstd\");\nZFS_MODULE_PARAM(zfs, zstd_, abort_size, UINT, ZMOD_RW,\n\t\"Minimal size of block to attempt early abort\");\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}