{
  "module_name": "zstd_cwksp.h",
  "hash_id": "194c185b38ec75384bfca37cf3e071c176e5e2b89866ce5b497aa32d79328a60",
  "original_prompt": "Ingested from zfs-2.2.2/module/zstd/lib/compress/zstd_cwksp.h",
  "human_readable_source": " \n\n#ifndef ZSTD_CWKSP_H\n#define ZSTD_CWKSP_H\n\n \n#include \"../common/zstd_internal.h\"\n\n#if defined (__cplusplus)\nextern \"C\" {\n#endif\n\n \n\n \n#ifndef ZSTD_CWKSP_ASAN_REDZONE_SIZE\n#define ZSTD_CWKSP_ASAN_REDZONE_SIZE 128\n#endif\n\n \ntypedef enum {\n    ZSTD_cwksp_alloc_objects,\n    ZSTD_cwksp_alloc_buffers,\n    ZSTD_cwksp_alloc_aligned\n} ZSTD_cwksp_alloc_phase_e;\n\n \ntypedef struct {\n    void* workspace;\n    void* workspaceEnd;\n\n    void* objectEnd;\n    void* tableEnd;\n    void* tableValidEnd;\n    void* allocStart;\n\n    int allocFailed;\n    int workspaceOversizedDuration;\n    ZSTD_cwksp_alloc_phase_e phase;\n} ZSTD_cwksp;\n\n \n\nMEM_STATIC size_t ZSTD_cwksp_available_space(ZSTD_cwksp* ws);\n\nMEM_STATIC void ZSTD_cwksp_assert_internal_consistency(ZSTD_cwksp* ws) {\n    (void)ws;\n    assert(ws->workspace <= ws->objectEnd);\n    assert(ws->objectEnd <= ws->tableEnd);\n    assert(ws->objectEnd <= ws->tableValidEnd);\n    assert(ws->tableEnd <= ws->allocStart);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    assert(ws->allocStart <= ws->workspaceEnd);\n}\n\n \nMEM_STATIC size_t ZSTD_cwksp_align(size_t size, size_t const align) {\n    size_t const mask = align - 1;\n    assert((align & mask) == 0);\n    return (size + mask) & ~mask;\n}\n\n \nMEM_STATIC size_t ZSTD_cwksp_alloc_size(size_t size) {\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n    return size + 2 * ZSTD_CWKSP_ASAN_REDZONE_SIZE;\n#else\n    return size;\n#endif\n}\n\nMEM_STATIC void ZSTD_cwksp_internal_advance_phase(\n        ZSTD_cwksp* ws, ZSTD_cwksp_alloc_phase_e phase) {\n    assert(phase >= ws->phase);\n    if (phase > ws->phase) {\n        if (ws->phase < ZSTD_cwksp_alloc_buffers &&\n                phase >= ZSTD_cwksp_alloc_buffers) {\n            ws->tableValidEnd = ws->objectEnd;\n        }\n        if (ws->phase < ZSTD_cwksp_alloc_aligned &&\n                phase >= ZSTD_cwksp_alloc_aligned) {\n             \n             \n            ws->allocStart = (BYTE*)ws->allocStart - ((size_t)ws->allocStart & (sizeof(U32)-1));\n            if (ws->allocStart < ws->tableValidEnd) {\n                ws->tableValidEnd = ws->allocStart;\n            }\n        }\n        ws->phase = phase;\n    }\n}\n\n \nMEM_STATIC int ZSTD_cwksp_owns_buffer(const ZSTD_cwksp* ws, const void* ptr) {\n    return (ptr != NULL) && (ws->workspace <= ptr) && (ptr <= ws->workspaceEnd);\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_internal(\n        ZSTD_cwksp* ws, size_t bytes, ZSTD_cwksp_alloc_phase_e phase) {\n    void* alloc;\n    void* bottom = ws->tableEnd;\n    ZSTD_cwksp_internal_advance_phase(ws, phase);\n    alloc = (BYTE *)ws->allocStart - bytes;\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n     \n    alloc = (BYTE *)alloc - 2 * ZSTD_CWKSP_ASAN_REDZONE_SIZE;\n#endif\n\n    DEBUGLOG(5, \"cwksp: reserving %p %zd bytes, %zd bytes remaining\",\n        alloc, bytes, ZSTD_cwksp_available_space(ws) - bytes);\n    ZSTD_cwksp_assert_internal_consistency(ws);\n    assert(alloc >= bottom);\n    if (alloc < bottom) {\n        DEBUGLOG(4, \"cwksp: alloc failed!\");\n        ws->allocFailed = 1;\n        return NULL;\n    }\n    if (alloc < ws->tableValidEnd) {\n        ws->tableValidEnd = alloc;\n    }\n    ws->allocStart = alloc;\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n     \n    alloc = (BYTE *)alloc + ZSTD_CWKSP_ASAN_REDZONE_SIZE;\n    __asan_unpoison_memory_region(alloc, bytes);\n#endif\n\n    return alloc;\n}\n\n \nMEM_STATIC BYTE* ZSTD_cwksp_reserve_buffer(ZSTD_cwksp* ws, size_t bytes) {\n    return (BYTE*)ZSTD_cwksp_reserve_internal(ws, bytes, ZSTD_cwksp_alloc_buffers);\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_aligned(ZSTD_cwksp* ws, size_t bytes) {\n    assert((bytes & (sizeof(U32)-1)) == 0);\n    return ZSTD_cwksp_reserve_internal(ws, ZSTD_cwksp_align(bytes, sizeof(U32)), ZSTD_cwksp_alloc_aligned);\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_table(ZSTD_cwksp* ws, size_t bytes) {\n    const ZSTD_cwksp_alloc_phase_e phase = ZSTD_cwksp_alloc_aligned;\n    void* alloc = ws->tableEnd;\n    void* end = (BYTE *)alloc + bytes;\n    void* top = ws->allocStart;\n\n    DEBUGLOG(5, \"cwksp: reserving %p table %zd bytes, %zd bytes remaining\",\n        alloc, bytes, ZSTD_cwksp_available_space(ws) - bytes);\n    assert((bytes & (sizeof(U32)-1)) == 0);\n    ZSTD_cwksp_internal_advance_phase(ws, phase);\n    ZSTD_cwksp_assert_internal_consistency(ws);\n    assert(end <= top);\n    if (end > top) {\n        DEBUGLOG(4, \"cwksp: table alloc failed!\");\n        ws->allocFailed = 1;\n        return NULL;\n    }\n    ws->tableEnd = end;\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n    __asan_unpoison_memory_region(alloc, bytes);\n#endif\n\n    return alloc;\n}\n\n \nMEM_STATIC void* ZSTD_cwksp_reserve_object(ZSTD_cwksp* ws, size_t bytes) {\n    size_t roundedBytes = ZSTD_cwksp_align(bytes, sizeof(void*));\n    void* alloc = ws->objectEnd;\n    void* end = (BYTE*)alloc + roundedBytes;\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n     \n    end = (BYTE *)end + 2 * ZSTD_CWKSP_ASAN_REDZONE_SIZE;\n#endif\n\n    DEBUGLOG(5,\n        \"cwksp: reserving %p object %zd bytes (rounded to %zd), %zd bytes remaining\",\n        alloc, bytes, roundedBytes, ZSTD_cwksp_available_space(ws) - roundedBytes);\n    assert(((size_t)alloc & (sizeof(void*)-1)) == 0);\n    assert((bytes & (sizeof(void*)-1)) == 0);\n    ZSTD_cwksp_assert_internal_consistency(ws);\n     \n    if (ws->phase != ZSTD_cwksp_alloc_objects || end > ws->workspaceEnd) {\n        DEBUGLOG(4, \"cwksp: object alloc failed!\");\n        ws->allocFailed = 1;\n        return NULL;\n    }\n    ws->objectEnd = end;\n    ws->tableEnd = end;\n    ws->tableValidEnd = end;\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n     \n    alloc = (BYTE *)alloc + ZSTD_CWKSP_ASAN_REDZONE_SIZE;\n    __asan_unpoison_memory_region(alloc, bytes);\n#endif\n\n    return alloc;\n}\n\nMEM_STATIC void ZSTD_cwksp_mark_tables_dirty(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: ZSTD_cwksp_mark_tables_dirty\");\n\n#if defined (MEMORY_SANITIZER) && !defined (ZSTD_MSAN_DONT_POISON_WORKSPACE)\n     \n    {\n        size_t size = (BYTE*)ws->tableValidEnd - (BYTE*)ws->objectEnd;\n        assert(__msan_test_shadow(ws->objectEnd, size) == -1);\n        __msan_poison(ws->objectEnd, size);\n    }\n#endif\n\n    assert(ws->tableValidEnd >= ws->objectEnd);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    ws->tableValidEnd = ws->objectEnd;\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\nMEM_STATIC void ZSTD_cwksp_mark_tables_clean(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: ZSTD_cwksp_mark_tables_clean\");\n    assert(ws->tableValidEnd >= ws->objectEnd);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    if (ws->tableValidEnd < ws->tableEnd) {\n        ws->tableValidEnd = ws->tableEnd;\n    }\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_clean_tables(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: ZSTD_cwksp_clean_tables\");\n    assert(ws->tableValidEnd >= ws->objectEnd);\n    assert(ws->tableValidEnd <= ws->allocStart);\n    if (ws->tableValidEnd < ws->tableEnd) {\n        memset(ws->tableValidEnd, 0, (BYTE*)ws->tableEnd - (BYTE*)ws->tableValidEnd);\n    }\n    ZSTD_cwksp_mark_tables_clean(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_clear_tables(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: clearing tables!\");\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n    {\n        size_t size = (BYTE*)ws->tableValidEnd - (BYTE*)ws->objectEnd;\n        __asan_poison_memory_region(ws->objectEnd, size);\n    }\n#endif\n\n    ws->tableEnd = ws->objectEnd;\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_clear(ZSTD_cwksp* ws) {\n    DEBUGLOG(4, \"cwksp: clearing!\");\n\n#if defined (MEMORY_SANITIZER) && !defined (ZSTD_MSAN_DONT_POISON_WORKSPACE)\n     \n    {\n        size_t size = (BYTE*)ws->workspaceEnd - (BYTE*)ws->tableValidEnd;\n        __msan_poison(ws->tableValidEnd, size);\n    }\n#endif\n\n#if defined (ADDRESS_SANITIZER) && !defined (ZSTD_ASAN_DONT_POISON_WORKSPACE)\n    {\n        size_t size = (BYTE*)ws->workspaceEnd - (BYTE*)ws->objectEnd;\n        __asan_poison_memory_region(ws->objectEnd, size);\n    }\n#endif\n\n    ws->tableEnd = ws->objectEnd;\n    ws->allocStart = ws->workspaceEnd;\n    ws->allocFailed = 0;\n    if (ws->phase > ZSTD_cwksp_alloc_buffers) {\n        ws->phase = ZSTD_cwksp_alloc_buffers;\n    }\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_init(ZSTD_cwksp* ws, void* start, size_t size) {\n    DEBUGLOG(4, \"cwksp: init'ing workspace with %zd bytes\", size);\n    assert(((size_t)start & (sizeof(void*)-1)) == 0);  \n    ws->workspace = start;\n    ws->workspaceEnd = (BYTE*)start + size;\n    ws->objectEnd = ws->workspace;\n    ws->tableValidEnd = ws->objectEnd;\n    ws->phase = ZSTD_cwksp_alloc_objects;\n    ZSTD_cwksp_clear(ws);\n    ws->workspaceOversizedDuration = 0;\n    ZSTD_cwksp_assert_internal_consistency(ws);\n}\n\nMEM_STATIC size_t ZSTD_cwksp_create(ZSTD_cwksp* ws, size_t size, ZSTD_customMem customMem) {\n    void* workspace = ZSTD_malloc(size, customMem);\n    DEBUGLOG(4, \"cwksp: creating new workspace with %zd bytes\", size);\n    RETURN_ERROR_IF(workspace == NULL, memory_allocation, \"NULL pointer!\");\n    ZSTD_cwksp_init(ws, workspace, size);\n    return 0;\n}\n\nMEM_STATIC void ZSTD_cwksp_free(ZSTD_cwksp* ws, ZSTD_customMem customMem) {\n    void *ptr = ws->workspace;\n    DEBUGLOG(4, \"cwksp: freeing workspace\");\n    memset(ws, 0, sizeof(ZSTD_cwksp));\n    ZSTD_free(ptr, customMem);\n}\n\n \nMEM_STATIC void ZSTD_cwksp_move(ZSTD_cwksp* dst, ZSTD_cwksp* src) {\n    *dst = *src;\n    memset(src, 0, sizeof(ZSTD_cwksp));\n}\n\nMEM_STATIC size_t ZSTD_cwksp_sizeof(const ZSTD_cwksp* ws) {\n    return (size_t)((BYTE*)ws->workspaceEnd - (BYTE*)ws->workspace);\n}\n\nMEM_STATIC int ZSTD_cwksp_reserve_failed(const ZSTD_cwksp* ws) {\n    return ws->allocFailed;\n}\n\n \n\nMEM_STATIC size_t ZSTD_cwksp_available_space(ZSTD_cwksp* ws) {\n    return (size_t)((BYTE*)ws->allocStart - (BYTE*)ws->tableEnd);\n}\n\nMEM_STATIC int ZSTD_cwksp_check_available(ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    return ZSTD_cwksp_available_space(ws) >= additionalNeededSpace;\n}\n\nMEM_STATIC int ZSTD_cwksp_check_too_large(ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    return ZSTD_cwksp_check_available(\n        ws, additionalNeededSpace * ZSTD_WORKSPACETOOLARGE_FACTOR);\n}\n\nMEM_STATIC int ZSTD_cwksp_check_wasteful(ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    return ZSTD_cwksp_check_too_large(ws, additionalNeededSpace)\n        && ws->workspaceOversizedDuration > ZSTD_WORKSPACETOOLARGE_MAXDURATION;\n}\n\nMEM_STATIC void ZSTD_cwksp_bump_oversized_duration(\n        ZSTD_cwksp* ws, size_t additionalNeededSpace) {\n    if (ZSTD_cwksp_check_too_large(ws, additionalNeededSpace)) {\n        ws->workspaceOversizedDuration++;\n    } else {\n        ws->workspaceOversizedDuration = 0;\n    }\n}\n\n#if defined (__cplusplus)\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}