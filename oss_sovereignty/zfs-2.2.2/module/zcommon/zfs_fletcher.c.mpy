{
  "module_name": "zfs_fletcher.c",
  "hash_id": "6d14985548bcdf5c44bef994627cd31f2aab66cbc40779491fafc839a47c0629",
  "original_prompt": "Ingested from zfs-2.2.2/module/zcommon/zfs_fletcher.c",
  "human_readable_source": " \n \n \n\n \n\n \n\n#include <sys/types.h>\n#include <sys/sysmacros.h>\n#include <sys/byteorder.h>\n#include <sys/simd.h>\n#include <sys/spa.h>\n#include <sys/zio_checksum.h>\n#include <sys/zfs_context.h>\n#include <zfs_fletcher.h>\n\n#define\tFLETCHER_MIN_SIMD_SIZE\t64\n\nstatic void fletcher_4_scalar_init(fletcher_4_ctx_t *ctx);\nstatic void fletcher_4_scalar_fini(fletcher_4_ctx_t *ctx, zio_cksum_t *zcp);\nstatic void fletcher_4_scalar_native(fletcher_4_ctx_t *ctx,\n    const void *buf, uint64_t size);\nstatic void fletcher_4_scalar_byteswap(fletcher_4_ctx_t *ctx,\n    const void *buf, uint64_t size);\nstatic boolean_t fletcher_4_scalar_valid(void);\n\nstatic const fletcher_4_ops_t fletcher_4_scalar_ops = {\n\t.init_native = fletcher_4_scalar_init,\n\t.fini_native = fletcher_4_scalar_fini,\n\t.compute_native = fletcher_4_scalar_native,\n\t.init_byteswap = fletcher_4_scalar_init,\n\t.fini_byteswap = fletcher_4_scalar_fini,\n\t.compute_byteswap = fletcher_4_scalar_byteswap,\n\t.valid = fletcher_4_scalar_valid,\n\t.uses_fpu = B_FALSE,\n\t.name = \"scalar\"\n};\n\nstatic fletcher_4_ops_t fletcher_4_fastest_impl = {\n\t.name = \"fastest\",\n\t.valid = fletcher_4_scalar_valid\n};\n\nstatic const fletcher_4_ops_t *fletcher_4_impls[] = {\n\t&fletcher_4_scalar_ops,\n\t&fletcher_4_superscalar_ops,\n\t&fletcher_4_superscalar4_ops,\n#if defined(HAVE_SSE2)\n\t&fletcher_4_sse2_ops,\n#endif\n#if defined(HAVE_SSE2) && defined(HAVE_SSSE3)\n\t&fletcher_4_ssse3_ops,\n#endif\n#if defined(HAVE_AVX) && defined(HAVE_AVX2)\n\t&fletcher_4_avx2_ops,\n#endif\n#if defined(__x86_64) && defined(HAVE_AVX512F)\n\t&fletcher_4_avx512f_ops,\n#endif\n#if defined(__x86_64) && defined(HAVE_AVX512BW)\n\t&fletcher_4_avx512bw_ops,\n#endif\n#if defined(__aarch64__) && !defined(__FreeBSD__)\n\t&fletcher_4_aarch64_neon_ops,\n#endif\n};\n\n \nstatic uint32_t fletcher_4_supp_impls_cnt = 0;\nstatic fletcher_4_ops_t *fletcher_4_supp_impls[ARRAY_SIZE(fletcher_4_impls)];\n\n \n#define\tIMPL_FASTEST\t(UINT32_MAX)\n#define\tIMPL_CYCLE\t(UINT32_MAX - 1)\n#define\tIMPL_SCALAR\t(0)\n\nstatic uint32_t fletcher_4_impl_chosen = IMPL_FASTEST;\n\n#define\tIMPL_READ(i)\t(*(volatile uint32_t *) &(i))\n\nstatic struct fletcher_4_impl_selector {\n\tconst char\t*fis_name;\n\tuint32_t\tfis_sel;\n} fletcher_4_impl_selectors[] = {\n\t{ \"cycle\",\tIMPL_CYCLE },\n\t{ \"fastest\",\tIMPL_FASTEST },\n\t{ \"scalar\",\tIMPL_SCALAR }\n};\n\n#if defined(_KERNEL)\nstatic kstat_t *fletcher_4_kstat;\n\nstatic struct fletcher_4_kstat {\n\tuint64_t native;\n\tuint64_t byteswap;\n} fletcher_4_stat_data[ARRAY_SIZE(fletcher_4_impls) + 1];\n#endif\n\n \nstatic boolean_t fletcher_4_initialized = B_FALSE;\n\nvoid\nfletcher_init(zio_cksum_t *zcp)\n{\n\tZIO_SET_CHECKSUM(zcp, 0, 0, 0, 0);\n}\n\nint\nfletcher_2_incremental_native(void *buf, size_t size, void *data)\n{\n\tzio_cksum_t *zcp = data;\n\n\tconst uint64_t *ip = buf;\n\tconst uint64_t *ipend = ip + (size / sizeof (uint64_t));\n\tuint64_t a0, b0, a1, b1;\n\n\ta0 = zcp->zc_word[0];\n\ta1 = zcp->zc_word[1];\n\tb0 = zcp->zc_word[2];\n\tb1 = zcp->zc_word[3];\n\n\tfor (; ip < ipend; ip += 2) {\n\t\ta0 += ip[0];\n\t\ta1 += ip[1];\n\t\tb0 += a0;\n\t\tb1 += a1;\n\t}\n\n\tZIO_SET_CHECKSUM(zcp, a0, a1, b0, b1);\n\treturn (0);\n}\n\nvoid\nfletcher_2_native(const void *buf, uint64_t size,\n    const void *ctx_template, zio_cksum_t *zcp)\n{\n\t(void) ctx_template;\n\tfletcher_init(zcp);\n\t(void) fletcher_2_incremental_native((void *) buf, size, zcp);\n}\n\nint\nfletcher_2_incremental_byteswap(void *buf, size_t size, void *data)\n{\n\tzio_cksum_t *zcp = data;\n\n\tconst uint64_t *ip = buf;\n\tconst uint64_t *ipend = ip + (size / sizeof (uint64_t));\n\tuint64_t a0, b0, a1, b1;\n\n\ta0 = zcp->zc_word[0];\n\ta1 = zcp->zc_word[1];\n\tb0 = zcp->zc_word[2];\n\tb1 = zcp->zc_word[3];\n\n\tfor (; ip < ipend; ip += 2) {\n\t\ta0 += BSWAP_64(ip[0]);\n\t\ta1 += BSWAP_64(ip[1]);\n\t\tb0 += a0;\n\t\tb1 += a1;\n\t}\n\n\tZIO_SET_CHECKSUM(zcp, a0, a1, b0, b1);\n\treturn (0);\n}\n\nvoid\nfletcher_2_byteswap(const void *buf, uint64_t size,\n    const void *ctx_template, zio_cksum_t *zcp)\n{\n\t(void) ctx_template;\n\tfletcher_init(zcp);\n\t(void) fletcher_2_incremental_byteswap((void *) buf, size, zcp);\n}\n\nstatic void\nfletcher_4_scalar_init(fletcher_4_ctx_t *ctx)\n{\n\tZIO_SET_CHECKSUM(&ctx->scalar, 0, 0, 0, 0);\n}\n\nstatic void\nfletcher_4_scalar_fini(fletcher_4_ctx_t *ctx, zio_cksum_t *zcp)\n{\n\tmemcpy(zcp, &ctx->scalar, sizeof (zio_cksum_t));\n}\n\nstatic void\nfletcher_4_scalar_native(fletcher_4_ctx_t *ctx, const void *buf,\n    uint64_t size)\n{\n\tconst uint32_t *ip = buf;\n\tconst uint32_t *ipend = ip + (size / sizeof (uint32_t));\n\tuint64_t a, b, c, d;\n\n\ta = ctx->scalar.zc_word[0];\n\tb = ctx->scalar.zc_word[1];\n\tc = ctx->scalar.zc_word[2];\n\td = ctx->scalar.zc_word[3];\n\n\tfor (; ip < ipend; ip++) {\n\t\ta += ip[0];\n\t\tb += a;\n\t\tc += b;\n\t\td += c;\n\t}\n\n\tZIO_SET_CHECKSUM(&ctx->scalar, a, b, c, d);\n}\n\nstatic void\nfletcher_4_scalar_byteswap(fletcher_4_ctx_t *ctx, const void *buf,\n    uint64_t size)\n{\n\tconst uint32_t *ip = buf;\n\tconst uint32_t *ipend = ip + (size / sizeof (uint32_t));\n\tuint64_t a, b, c, d;\n\n\ta = ctx->scalar.zc_word[0];\n\tb = ctx->scalar.zc_word[1];\n\tc = ctx->scalar.zc_word[2];\n\td = ctx->scalar.zc_word[3];\n\n\tfor (; ip < ipend; ip++) {\n\t\ta += BSWAP_32(ip[0]);\n\t\tb += a;\n\t\tc += b;\n\t\td += c;\n\t}\n\n\tZIO_SET_CHECKSUM(&ctx->scalar, a, b, c, d);\n}\n\nstatic boolean_t\nfletcher_4_scalar_valid(void)\n{\n\treturn (B_TRUE);\n}\n\nint\nfletcher_4_impl_set(const char *val)\n{\n\tint err = -EINVAL;\n\tuint32_t impl = IMPL_READ(fletcher_4_impl_chosen);\n\tsize_t i, val_len;\n\n\tval_len = strlen(val);\n\twhile ((val_len > 0) && !!isspace(val[val_len-1]))  \n\t\tval_len--;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(fletcher_4_impl_selectors); i++) {\n\t\tconst char *name = fletcher_4_impl_selectors[i].fis_name;\n\n\t\tif (val_len == strlen(name) &&\n\t\t    strncmp(val, name, val_len) == 0) {\n\t\t\timpl = fletcher_4_impl_selectors[i].fis_sel;\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (err != 0 && fletcher_4_initialized) {\n\t\t \n\t\tfor (i = 0; i < fletcher_4_supp_impls_cnt; i++) {\n\t\t\tconst char *name = fletcher_4_supp_impls[i]->name;\n\n\t\t\tif (val_len == strlen(name) &&\n\t\t\t    strncmp(val, name, val_len) == 0) {\n\t\t\t\timpl = i;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (err == 0) {\n\t\tatomic_swap_32(&fletcher_4_impl_chosen, impl);\n\t\tmembar_producer();\n\t}\n\n\treturn (err);\n}\n\n \nstatic inline const fletcher_4_ops_t *\nfletcher_4_impl_get(void)\n{\n\tif (!kfpu_allowed())\n\t\treturn (&fletcher_4_superscalar4_ops);\n\n\tconst fletcher_4_ops_t *ops = NULL;\n\tuint32_t impl = IMPL_READ(fletcher_4_impl_chosen);\n\n\tswitch (impl) {\n\tcase IMPL_FASTEST:\n\t\tASSERT(fletcher_4_initialized);\n\t\tops = &fletcher_4_fastest_impl;\n\t\tbreak;\n\tcase IMPL_CYCLE:\n\t\t \n\t\tASSERT(fletcher_4_initialized);\n\t\tASSERT3U(fletcher_4_supp_impls_cnt, >, 0);\n\t\tstatic uint32_t cycle_count = 0;\n\t\tuint32_t idx = (++cycle_count) % fletcher_4_supp_impls_cnt;\n\t\tops = fletcher_4_supp_impls[idx];\n\t\tbreak;\n\tdefault:\n\t\tASSERT3U(fletcher_4_supp_impls_cnt, >, 0);\n\t\tASSERT3U(impl, <, fletcher_4_supp_impls_cnt);\n\t\tops = fletcher_4_supp_impls[impl];\n\t\tbreak;\n\t}\n\n\tASSERT3P(ops, !=, NULL);\n\n\treturn (ops);\n}\n\nstatic inline void\nfletcher_4_native_impl(const void *buf, uint64_t size, zio_cksum_t *zcp)\n{\n\tfletcher_4_ctx_t ctx;\n\tconst fletcher_4_ops_t *ops = fletcher_4_impl_get();\n\n\tif (ops->uses_fpu == B_TRUE) {\n\t\tkfpu_begin();\n\t}\n\tops->init_native(&ctx);\n\tops->compute_native(&ctx, buf, size);\n\tops->fini_native(&ctx, zcp);\n\tif (ops->uses_fpu == B_TRUE) {\n\t\tkfpu_end();\n\t}\n}\n\nvoid\nfletcher_4_native(const void *buf, uint64_t size,\n    const void *ctx_template, zio_cksum_t *zcp)\n{\n\t(void) ctx_template;\n\tconst uint64_t p2size = P2ALIGN(size, FLETCHER_MIN_SIMD_SIZE);\n\n\tASSERT(IS_P2ALIGNED(size, sizeof (uint32_t)));\n\n\tif (size == 0 || p2size == 0) {\n\t\tZIO_SET_CHECKSUM(zcp, 0, 0, 0, 0);\n\n\t\tif (size > 0)\n\t\t\tfletcher_4_scalar_native((fletcher_4_ctx_t *)zcp,\n\t\t\t    buf, size);\n\t} else {\n\t\tfletcher_4_native_impl(buf, p2size, zcp);\n\n\t\tif (p2size < size)\n\t\t\tfletcher_4_scalar_native((fletcher_4_ctx_t *)zcp,\n\t\t\t    (char *)buf + p2size, size - p2size);\n\t}\n}\n\nvoid\nfletcher_4_native_varsize(const void *buf, uint64_t size, zio_cksum_t *zcp)\n{\n\tZIO_SET_CHECKSUM(zcp, 0, 0, 0, 0);\n\tfletcher_4_scalar_native((fletcher_4_ctx_t *)zcp, buf, size);\n}\n\nstatic inline void\nfletcher_4_byteswap_impl(const void *buf, uint64_t size, zio_cksum_t *zcp)\n{\n\tfletcher_4_ctx_t ctx;\n\tconst fletcher_4_ops_t *ops = fletcher_4_impl_get();\n\n\tif (ops->uses_fpu == B_TRUE) {\n\t\tkfpu_begin();\n\t}\n\tops->init_byteswap(&ctx);\n\tops->compute_byteswap(&ctx, buf, size);\n\tops->fini_byteswap(&ctx, zcp);\n\tif (ops->uses_fpu == B_TRUE) {\n\t\tkfpu_end();\n\t}\n}\n\nvoid\nfletcher_4_byteswap(const void *buf, uint64_t size,\n    const void *ctx_template, zio_cksum_t *zcp)\n{\n\t(void) ctx_template;\n\tconst uint64_t p2size = P2ALIGN(size, FLETCHER_MIN_SIMD_SIZE);\n\n\tASSERT(IS_P2ALIGNED(size, sizeof (uint32_t)));\n\n\tif (size == 0 || p2size == 0) {\n\t\tZIO_SET_CHECKSUM(zcp, 0, 0, 0, 0);\n\n\t\tif (size > 0)\n\t\t\tfletcher_4_scalar_byteswap((fletcher_4_ctx_t *)zcp,\n\t\t\t    buf, size);\n\t} else {\n\t\tfletcher_4_byteswap_impl(buf, p2size, zcp);\n\n\t\tif (p2size < size)\n\t\t\tfletcher_4_scalar_byteswap((fletcher_4_ctx_t *)zcp,\n\t\t\t    (char *)buf + p2size, size - p2size);\n\t}\n}\n\n \n\n#define\tZFS_FLETCHER_4_INC_MAX_SIZE\t(8ULL << 20)\n\nstatic inline void\nfletcher_4_incremental_combine(zio_cksum_t *zcp, const uint64_t size,\n    const zio_cksum_t *nzcp)\n{\n\tconst uint64_t c1 = size / sizeof (uint32_t);\n\tconst uint64_t c2 = c1 * (c1 + 1) / 2;\n\tconst uint64_t c3 = c2 * (c1 + 2) / 3;\n\n\t \n\tASSERT3U(size, <=, ZFS_FLETCHER_4_INC_MAX_SIZE);\n\n\tzcp->zc_word[3] += nzcp->zc_word[3] + c1 * zcp->zc_word[2] +\n\t    c2 * zcp->zc_word[1] + c3 * zcp->zc_word[0];\n\tzcp->zc_word[2] += nzcp->zc_word[2] + c1 * zcp->zc_word[1] +\n\t    c2 * zcp->zc_word[0];\n\tzcp->zc_word[1] += nzcp->zc_word[1] + c1 * zcp->zc_word[0];\n\tzcp->zc_word[0] += nzcp->zc_word[0];\n}\n\nstatic inline void\nfletcher_4_incremental_impl(boolean_t native, const void *buf, uint64_t size,\n    zio_cksum_t *zcp)\n{\n\twhile (size > 0) {\n\t\tzio_cksum_t nzc;\n\t\tuint64_t len = MIN(size, ZFS_FLETCHER_4_INC_MAX_SIZE);\n\n\t\tif (native)\n\t\t\tfletcher_4_native(buf, len, NULL, &nzc);\n\t\telse\n\t\t\tfletcher_4_byteswap(buf, len, NULL, &nzc);\n\n\t\tfletcher_4_incremental_combine(zcp, len, &nzc);\n\n\t\tsize -= len;\n\t\tbuf += len;\n\t}\n}\n\nint\nfletcher_4_incremental_native(void *buf, size_t size, void *data)\n{\n\tzio_cksum_t *zcp = data;\n\t \n\tif (size < SPA_MINBLOCKSIZE)\n\t\tfletcher_4_scalar_native((fletcher_4_ctx_t *)zcp, buf, size);\n\telse\n\t\tfletcher_4_incremental_impl(B_TRUE, buf, size, zcp);\n\treturn (0);\n}\n\nint\nfletcher_4_incremental_byteswap(void *buf, size_t size, void *data)\n{\n\tzio_cksum_t *zcp = data;\n\t \n\tif (size < SPA_MINBLOCKSIZE)\n\t\tfletcher_4_scalar_byteswap((fletcher_4_ctx_t *)zcp, buf, size);\n\telse\n\t\tfletcher_4_incremental_impl(B_FALSE, buf, size, zcp);\n\treturn (0);\n}\n\n#if defined(_KERNEL)\n \nstatic int\nfletcher_4_kstat_headers(char *buf, size_t size)\n{\n\tssize_t off = 0;\n\n\toff += snprintf(buf + off, size, \"%-17s\", \"implementation\");\n\toff += snprintf(buf + off, size - off, \"%-15s\", \"native\");\n\t(void) snprintf(buf + off, size - off, \"%-15s\\n\", \"byteswap\");\n\n\treturn (0);\n}\n\nstatic int\nfletcher_4_kstat_data(char *buf, size_t size, void *data)\n{\n\tstruct fletcher_4_kstat *fastest_stat =\n\t    &fletcher_4_stat_data[fletcher_4_supp_impls_cnt];\n\tstruct fletcher_4_kstat *curr_stat = (struct fletcher_4_kstat *)data;\n\tssize_t off = 0;\n\n\tif (curr_stat == fastest_stat) {\n\t\toff += snprintf(buf + off, size - off, \"%-17s\", \"fastest\");\n\t\toff += snprintf(buf + off, size - off, \"%-15s\",\n\t\t    fletcher_4_supp_impls[fastest_stat->native]->name);\n\t\t(void) snprintf(buf + off, size - off, \"%-15s\\n\",\n\t\t    fletcher_4_supp_impls[fastest_stat->byteswap]->name);\n\t} else {\n\t\tptrdiff_t id = curr_stat - fletcher_4_stat_data;\n\n\t\toff += snprintf(buf + off, size - off, \"%-17s\",\n\t\t    fletcher_4_supp_impls[id]->name);\n\t\toff += snprintf(buf + off, size - off, \"%-15llu\",\n\t\t    (u_longlong_t)curr_stat->native);\n\t\t(void) snprintf(buf + off, size - off, \"%-15llu\\n\",\n\t\t    (u_longlong_t)curr_stat->byteswap);\n\t}\n\n\treturn (0);\n}\n\nstatic void *\nfletcher_4_kstat_addr(kstat_t *ksp, loff_t n)\n{\n\tif (n <= fletcher_4_supp_impls_cnt)\n\t\tksp->ks_private = (void *) (fletcher_4_stat_data + n);\n\telse\n\t\tksp->ks_private = NULL;\n\n\treturn (ksp->ks_private);\n}\n#endif\n\n#define\tFLETCHER_4_FASTEST_FN_COPY(type, src)\t\t\t\t  \\\n{\t\t\t\t\t\t\t\t\t  \\\n\tfletcher_4_fastest_impl.init_ ## type = src->init_ ## type;\t  \\\n\tfletcher_4_fastest_impl.fini_ ## type = src->fini_ ## type;\t  \\\n\tfletcher_4_fastest_impl.compute_ ## type = src->compute_ ## type; \\\n\tfletcher_4_fastest_impl.uses_fpu = src->uses_fpu;\t\t  \\\n}\n\n#define\tFLETCHER_4_BENCH_NS\t(MSEC2NSEC(1))\t\t \n\ntypedef void fletcher_checksum_func_t(const void *, uint64_t, const void *,\n\t\t\t\t\tzio_cksum_t *);\n\n#if defined(_KERNEL)\nstatic void\nfletcher_4_benchmark_impl(boolean_t native, char *data, uint64_t data_size)\n{\n\n\tstruct fletcher_4_kstat *fastest_stat =\n\t    &fletcher_4_stat_data[fletcher_4_supp_impls_cnt];\n\thrtime_t start;\n\tuint64_t run_bw, run_time_ns, best_run = 0;\n\tzio_cksum_t zc;\n\tuint32_t i, l, sel_save = IMPL_READ(fletcher_4_impl_chosen);\n\n\tfletcher_checksum_func_t *fletcher_4_test = native ?\n\t    fletcher_4_native : fletcher_4_byteswap;\n\n\tfor (i = 0; i < fletcher_4_supp_impls_cnt; i++) {\n\t\tstruct fletcher_4_kstat *stat = &fletcher_4_stat_data[i];\n\t\tuint64_t run_count = 0;\n\n\t\t \n\t\tfletcher_4_impl_chosen = i;\n\n\t\tkpreempt_disable();\n\t\tstart = gethrtime();\n\t\tdo {\n\t\t\tfor (l = 0; l < 32; l++, run_count++)\n\t\t\t\tfletcher_4_test(data, data_size, NULL, &zc);\n\n\t\t\trun_time_ns = gethrtime() - start;\n\t\t} while (run_time_ns < FLETCHER_4_BENCH_NS);\n\t\tkpreempt_enable();\n\n\t\trun_bw = data_size * run_count * NANOSEC;\n\t\trun_bw /= run_time_ns;\t \n\n\t\tif (native)\n\t\t\tstat->native = run_bw;\n\t\telse\n\t\t\tstat->byteswap = run_bw;\n\n\t\tif (run_bw > best_run) {\n\t\t\tbest_run = run_bw;\n\n\t\t\tif (native) {\n\t\t\t\tfastest_stat->native = i;\n\t\t\t\tFLETCHER_4_FASTEST_FN_COPY(native,\n\t\t\t\t    fletcher_4_supp_impls[i]);\n\t\t\t} else {\n\t\t\t\tfastest_stat->byteswap = i;\n\t\t\t\tFLETCHER_4_FASTEST_FN_COPY(byteswap,\n\t\t\t\t    fletcher_4_supp_impls[i]);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tatomic_swap_32(&fletcher_4_impl_chosen, sel_save);\n}\n#endif  \n\n \nstatic void\nfletcher_4_benchmark(void)\n{\n\tfletcher_4_ops_t *curr_impl;\n\tint i, c;\n\n\t \n\tfor (i = 0, c = 0; i < ARRAY_SIZE(fletcher_4_impls); i++) {\n\t\tcurr_impl = (fletcher_4_ops_t *)fletcher_4_impls[i];\n\n\t\tif (curr_impl->valid && curr_impl->valid())\n\t\t\tfletcher_4_supp_impls[c++] = curr_impl;\n\t}\n\tmembar_producer();\t \n\tfletcher_4_supp_impls_cnt = c;\t \n\n#if defined(_KERNEL)\n\tstatic const size_t data_size = 1 << SPA_OLD_MAXBLOCKSHIFT;  \n\tchar *databuf = vmem_alloc(data_size, KM_SLEEP);\n\n\tfor (i = 0; i < data_size / sizeof (uint64_t); i++)\n\t\t((uint64_t *)databuf)[i] = (uintptr_t)(databuf+i);  \n\n\tfletcher_4_benchmark_impl(B_FALSE, databuf, data_size);\n\tfletcher_4_benchmark_impl(B_TRUE, databuf, data_size);\n\n\tvmem_free(databuf, data_size);\n#else\n\t \n\tmemcpy(&fletcher_4_fastest_impl,\n\t    fletcher_4_supp_impls[fletcher_4_supp_impls_cnt - 1],\n\t    sizeof (fletcher_4_fastest_impl));\n\tfletcher_4_fastest_impl.name = \"fastest\";\n\tmembar_producer();\n#endif  \n}\n\nvoid\nfletcher_4_init(void)\n{\n\t \n\tfletcher_4_benchmark();\n\n#if defined(_KERNEL)\n\t \n\tfletcher_4_kstat = kstat_create(\"zfs\", 0, \"fletcher_4_bench\", \"misc\",\n\t    KSTAT_TYPE_RAW, 0, KSTAT_FLAG_VIRTUAL);\n\tif (fletcher_4_kstat != NULL) {\n\t\tfletcher_4_kstat->ks_data = NULL;\n\t\tfletcher_4_kstat->ks_ndata = UINT32_MAX;\n\t\tkstat_set_raw_ops(fletcher_4_kstat,\n\t\t    fletcher_4_kstat_headers,\n\t\t    fletcher_4_kstat_data,\n\t\t    fletcher_4_kstat_addr);\n\t\tkstat_install(fletcher_4_kstat);\n\t}\n#endif\n\n\t \n\tfletcher_4_initialized = B_TRUE;\n}\n\nvoid\nfletcher_4_fini(void)\n{\n#if defined(_KERNEL)\n\tif (fletcher_4_kstat != NULL) {\n\t\tkstat_delete(fletcher_4_kstat);\n\t\tfletcher_4_kstat = NULL;\n\t}\n#endif\n}\n\n \n\nstatic void\nabd_fletcher_4_init(zio_abd_checksum_data_t *cdp)\n{\n\tconst fletcher_4_ops_t *ops = fletcher_4_impl_get();\n\tcdp->acd_private = (void *) ops;\n\n\tif (ops->uses_fpu == B_TRUE) {\n\t\tkfpu_begin();\n\t}\n\tif (cdp->acd_byteorder == ZIO_CHECKSUM_NATIVE)\n\t\tops->init_native(cdp->acd_ctx);\n\telse\n\t\tops->init_byteswap(cdp->acd_ctx);\n\n}\n\nstatic void\nabd_fletcher_4_fini(zio_abd_checksum_data_t *cdp)\n{\n\tfletcher_4_ops_t *ops = (fletcher_4_ops_t *)cdp->acd_private;\n\n\tASSERT(ops);\n\n\tif (cdp->acd_byteorder == ZIO_CHECKSUM_NATIVE)\n\t\tops->fini_native(cdp->acd_ctx, cdp->acd_zcp);\n\telse\n\t\tops->fini_byteswap(cdp->acd_ctx, cdp->acd_zcp);\n\n\tif (ops->uses_fpu == B_TRUE) {\n\t\tkfpu_end();\n\t}\n}\n\n\nstatic void\nabd_fletcher_4_simd2scalar(boolean_t native, void *data, size_t size,\n    zio_abd_checksum_data_t *cdp)\n{\n\tzio_cksum_t *zcp = cdp->acd_zcp;\n\n\tASSERT3U(size, <, FLETCHER_MIN_SIMD_SIZE);\n\n\tabd_fletcher_4_fini(cdp);\n\tcdp->acd_private = (void *)&fletcher_4_scalar_ops;\n\n\tif (native)\n\t\tfletcher_4_incremental_native(data, size, zcp);\n\telse\n\t\tfletcher_4_incremental_byteswap(data, size, zcp);\n}\n\nstatic int\nabd_fletcher_4_iter(void *data, size_t size, void *private)\n{\n\tzio_abd_checksum_data_t *cdp = (zio_abd_checksum_data_t *)private;\n\tfletcher_4_ctx_t *ctx = cdp->acd_ctx;\n\tfletcher_4_ops_t *ops = (fletcher_4_ops_t *)cdp->acd_private;\n\tboolean_t native = cdp->acd_byteorder == ZIO_CHECKSUM_NATIVE;\n\tuint64_t asize = P2ALIGN(size, FLETCHER_MIN_SIMD_SIZE);\n\n\tASSERT(IS_P2ALIGNED(size, sizeof (uint32_t)));\n\n\tif (asize > 0) {\n\t\tif (native)\n\t\t\tops->compute_native(ctx, data, asize);\n\t\telse\n\t\t\tops->compute_byteswap(ctx, data, asize);\n\n\t\tsize -= asize;\n\t\tdata = (char *)data + asize;\n\t}\n\n\tif (size > 0) {\n\t\tASSERT3U(size, <, FLETCHER_MIN_SIMD_SIZE);\n\t\t \n\t\tabd_fletcher_4_simd2scalar(native, data, size, cdp);\n\t}\n\n\treturn (0);\n}\n\nzio_abd_checksum_func_t fletcher_4_abd_ops = {\n\t.acf_init = abd_fletcher_4_init,\n\t.acf_fini = abd_fletcher_4_fini,\n\t.acf_iter = abd_fletcher_4_iter\n};\n\n#if defined(_KERNEL)\n\n#define\tIMPL_FMT(impl, i)\t(((impl) == (i)) ? \"[%s] \" : \"%s \")\n\n#if defined(__linux__)\n\nstatic int\nfletcher_4_param_get(char *buffer, zfs_kernel_param_t *unused)\n{\n\tconst uint32_t impl = IMPL_READ(fletcher_4_impl_chosen);\n\tchar *fmt;\n\tint cnt = 0;\n\n\t \n\tfmt = IMPL_FMT(impl, IMPL_FASTEST);\n\tcnt += kmem_scnprintf(buffer + cnt, PAGE_SIZE - cnt, fmt, \"fastest\");\n\n\t \n\tfor (uint32_t i = 0; i < fletcher_4_supp_impls_cnt; ++i) {\n\t\tfmt = IMPL_FMT(impl, i);\n\t\tcnt += kmem_scnprintf(buffer + cnt, PAGE_SIZE - cnt, fmt,\n\t\t    fletcher_4_supp_impls[i]->name);\n\t}\n\n\treturn (cnt);\n}\n\nstatic int\nfletcher_4_param_set(const char *val, zfs_kernel_param_t *unused)\n{\n\treturn (fletcher_4_impl_set(val));\n}\n\n#else\n\n#include <sys/sbuf.h>\n\nstatic int\nfletcher_4_param(ZFS_MODULE_PARAM_ARGS)\n{\n\tint err;\n\n\tif (req->newptr == NULL) {\n\t\tconst uint32_t impl = IMPL_READ(fletcher_4_impl_chosen);\n\t\tconst int init_buflen = 64;\n\t\tconst char *fmt;\n\t\tstruct sbuf *s;\n\n\t\ts = sbuf_new_for_sysctl(NULL, NULL, init_buflen, req);\n\n\t\t \n\t\tfmt = IMPL_FMT(impl, IMPL_FASTEST);\n\t\t(void) sbuf_printf(s, fmt, \"fastest\");\n\n\t\t \n\t\tfor (uint32_t i = 0; i < fletcher_4_supp_impls_cnt; ++i) {\n\t\t\tfmt = IMPL_FMT(impl, i);\n\t\t\t(void) sbuf_printf(s, fmt,\n\t\t\t    fletcher_4_supp_impls[i]->name);\n\t\t}\n\n\t\terr = sbuf_finish(s);\n\t\tsbuf_delete(s);\n\n\t\treturn (err);\n\t}\n\n\tchar buf[16];\n\n\terr = sysctl_handle_string(oidp, buf, sizeof (buf), req);\n\tif (err)\n\t\treturn (err);\n\treturn (-fletcher_4_impl_set(buf));\n}\n\n#endif\n\n#undef IMPL_FMT\n\n \nZFS_MODULE_VIRTUAL_PARAM_CALL(zfs, zfs_, fletcher_4_impl,\n    fletcher_4_param_set, fletcher_4_param_get, ZMOD_RW,\n\t\"Select fletcher 4 implementation.\");\n\nEXPORT_SYMBOL(fletcher_init);\nEXPORT_SYMBOL(fletcher_2_incremental_native);\nEXPORT_SYMBOL(fletcher_2_incremental_byteswap);\nEXPORT_SYMBOL(fletcher_4_init);\nEXPORT_SYMBOL(fletcher_4_fini);\nEXPORT_SYMBOL(fletcher_2_native);\nEXPORT_SYMBOL(fletcher_2_byteswap);\nEXPORT_SYMBOL(fletcher_4_native);\nEXPORT_SYMBOL(fletcher_4_native_varsize);\nEXPORT_SYMBOL(fletcher_4_byteswap);\nEXPORT_SYMBOL(fletcher_4_incremental_native);\nEXPORT_SYMBOL(fletcher_4_incremental_byteswap);\nEXPORT_SYMBOL(fletcher_4_abd_ops);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}