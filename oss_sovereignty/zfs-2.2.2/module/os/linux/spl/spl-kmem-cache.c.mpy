{
  "module_name": "spl-kmem-cache.c",
  "hash_id": "e091d7d71302e3ea8ba2fbacfc3a0e9089f3b1b874319ad07fd27217c4a227a8",
  "original_prompt": "Ingested from zfs-2.2.2/module/os/linux/spl/spl-kmem-cache.c",
  "human_readable_source": " \n\n#include <linux/percpu_compat.h>\n#include <sys/kmem.h>\n#include <sys/kmem_cache.h>\n#include <sys/taskq.h>\n#include <sys/timer.h>\n#include <sys/vmem.h>\n#include <sys/wait.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/prefetch.h>\n\n \n#undef kmem_cache_destroy\n#undef kmem_cache_create\n#undef kmem_cache_alloc\n#undef kmem_cache_free\n\n\n \n#ifndef smp_mb__before_atomic\n#define\tsmp_mb__before_atomic(x) smp_mb__before_clear_bit(x)\n#endif\n\n#ifndef smp_mb__after_atomic\n#define\tsmp_mb__after_atomic(x) smp_mb__after_clear_bit(x)\n#endif\n\n \n \nstatic unsigned int spl_kmem_cache_magazine_size = 0;\nmodule_param(spl_kmem_cache_magazine_size, uint, 0444);\nMODULE_PARM_DESC(spl_kmem_cache_magazine_size,\n\t\"Default magazine size (2-256), set automatically (0)\");\n\n \nstatic unsigned int spl_kmem_cache_reclaim = 0  ;\nmodule_param(spl_kmem_cache_reclaim, uint, 0644);\nMODULE_PARM_DESC(spl_kmem_cache_reclaim, \"Single reclaim pass (0x1)\");\n\nstatic unsigned int spl_kmem_cache_obj_per_slab = SPL_KMEM_CACHE_OBJ_PER_SLAB;\nmodule_param(spl_kmem_cache_obj_per_slab, uint, 0644);\nMODULE_PARM_DESC(spl_kmem_cache_obj_per_slab, \"Number of objects per slab\");\n\nstatic unsigned int spl_kmem_cache_max_size = SPL_KMEM_CACHE_MAX_SIZE;\nmodule_param(spl_kmem_cache_max_size, uint, 0644);\nMODULE_PARM_DESC(spl_kmem_cache_max_size, \"Maximum size of slab in MB\");\n\n \nstatic unsigned int spl_kmem_cache_slab_limit = 16384;\nmodule_param(spl_kmem_cache_slab_limit, uint, 0644);\nMODULE_PARM_DESC(spl_kmem_cache_slab_limit,\n\t\"Objects less than N bytes use the Linux slab\");\n\n \nstatic unsigned int spl_kmem_cache_kmem_threads = 4;\nmodule_param(spl_kmem_cache_kmem_threads, uint, 0444);\nMODULE_PARM_DESC(spl_kmem_cache_kmem_threads,\n\t\"Number of spl_kmem_cache threads\");\n \n\n \n\nstruct list_head spl_kmem_cache_list;    \nstruct rw_semaphore spl_kmem_cache_sem;  \nstatic taskq_t *spl_kmem_cache_taskq;    \n\nstatic void spl_cache_shrink(spl_kmem_cache_t *skc, void *obj);\n\nstatic void *\nkv_alloc(spl_kmem_cache_t *skc, int size, int flags)\n{\n\tgfp_t lflags = kmem_flags_convert(flags);\n\tvoid *ptr;\n\n\tptr = spl_vmalloc(size, lflags | __GFP_HIGHMEM);\n\n\t \n\tASSERT(IS_P2ALIGNED(ptr, PAGE_SIZE));\n\n\treturn (ptr);\n}\n\nstatic void\nkv_free(spl_kmem_cache_t *skc, void *ptr, int size)\n{\n\tASSERT(IS_P2ALIGNED(ptr, PAGE_SIZE));\n\n\t \n\tif (current->reclaim_state)\n#ifdef\tHAVE_RECLAIM_STATE_RECLAIMED\n\t\tcurrent->reclaim_state->reclaimed += size >> PAGE_SHIFT;\n#else\n\t\tcurrent->reclaim_state->reclaimed_slab += size >> PAGE_SHIFT;\n#endif\n\tvfree(ptr);\n}\n\n \nstatic inline uint32_t\nspl_sks_size(spl_kmem_cache_t *skc)\n{\n\treturn (P2ROUNDUP_TYPED(sizeof (spl_kmem_slab_t),\n\t    skc->skc_obj_align, uint32_t));\n}\n\n \nstatic inline uint32_t\nspl_obj_size(spl_kmem_cache_t *skc)\n{\n\tuint32_t align = skc->skc_obj_align;\n\n\treturn (P2ROUNDUP_TYPED(skc->skc_obj_size, align, uint32_t) +\n\t    P2ROUNDUP_TYPED(sizeof (spl_kmem_obj_t), align, uint32_t));\n}\n\nuint64_t\nspl_kmem_cache_inuse(kmem_cache_t *cache)\n{\n\treturn (cache->skc_obj_total);\n}\nEXPORT_SYMBOL(spl_kmem_cache_inuse);\n\nuint64_t\nspl_kmem_cache_entry_size(kmem_cache_t *cache)\n{\n\treturn (cache->skc_obj_size);\n}\nEXPORT_SYMBOL(spl_kmem_cache_entry_size);\n\n \nstatic inline spl_kmem_obj_t *\nspl_sko_from_obj(spl_kmem_cache_t *skc, void *obj)\n{\n\treturn (obj + P2ROUNDUP_TYPED(skc->skc_obj_size,\n\t    skc->skc_obj_align, uint32_t));\n}\n\n \nstatic spl_kmem_slab_t *\nspl_slab_alloc(spl_kmem_cache_t *skc, int flags)\n{\n\tspl_kmem_slab_t *sks;\n\tvoid *base;\n\tuint32_t obj_size;\n\n\tbase = kv_alloc(skc, skc->skc_slab_size, flags);\n\tif (base == NULL)\n\t\treturn (NULL);\n\n\tsks = (spl_kmem_slab_t *)base;\n\tsks->sks_magic = SKS_MAGIC;\n\tsks->sks_objs = skc->skc_slab_objs;\n\tsks->sks_age = jiffies;\n\tsks->sks_cache = skc;\n\tINIT_LIST_HEAD(&sks->sks_list);\n\tINIT_LIST_HEAD(&sks->sks_free_list);\n\tsks->sks_ref = 0;\n\tobj_size = spl_obj_size(skc);\n\n\tfor (int i = 0; i < sks->sks_objs; i++) {\n\t\tvoid *obj = base + spl_sks_size(skc) + (i * obj_size);\n\n\t\tASSERT(IS_P2ALIGNED(obj, skc->skc_obj_align));\n\t\tspl_kmem_obj_t *sko = spl_sko_from_obj(skc, obj);\n\t\tsko->sko_addr = obj;\n\t\tsko->sko_magic = SKO_MAGIC;\n\t\tsko->sko_slab = sks;\n\t\tINIT_LIST_HEAD(&sko->sko_list);\n\t\tlist_add_tail(&sko->sko_list, &sks->sks_free_list);\n\t}\n\n\treturn (sks);\n}\n\n \nstatic void\nspl_slab_free(spl_kmem_slab_t *sks,\n    struct list_head *sks_list, struct list_head *sko_list)\n{\n\tspl_kmem_cache_t *skc;\n\n\tASSERT(sks->sks_magic == SKS_MAGIC);\n\tASSERT(sks->sks_ref == 0);\n\n\tskc = sks->sks_cache;\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\n\t \n\tskc->skc_obj_total -= sks->sks_objs;\n\tskc->skc_slab_total--;\n\tlist_del(&sks->sks_list);\n\tlist_add(&sks->sks_list, sks_list);\n\tlist_splice_init(&sks->sks_free_list, sko_list);\n}\n\n \nstatic void\nspl_slab_reclaim(spl_kmem_cache_t *skc)\n{\n\tspl_kmem_slab_t *sks = NULL, *m = NULL;\n\tspl_kmem_obj_t *sko = NULL, *n = NULL;\n\tLIST_HEAD(sks_list);\n\tLIST_HEAD(sko_list);\n\n\t \n\tspin_lock(&skc->skc_lock);\n\tlist_for_each_entry_safe_reverse(sks, m,\n\t    &skc->skc_partial_list, sks_list) {\n\n\t\tif (sks->sks_ref > 0)\n\t\t\tbreak;\n\n\t\tspl_slab_free(sks, &sks_list, &sko_list);\n\t}\n\tspin_unlock(&skc->skc_lock);\n\n\t \n\n\tlist_for_each_entry_safe(sko, n, &sko_list, sko_list) {\n\t\tASSERT(sko->sko_magic == SKO_MAGIC);\n\t}\n\n\tlist_for_each_entry_safe(sks, m, &sks_list, sks_list) {\n\t\tASSERT(sks->sks_magic == SKS_MAGIC);\n\t\tkv_free(skc, sks, skc->skc_slab_size);\n\t}\n}\n\nstatic spl_kmem_emergency_t *\nspl_emergency_search(struct rb_root *root, void *obj)\n{\n\tstruct rb_node *node = root->rb_node;\n\tspl_kmem_emergency_t *ske;\n\tunsigned long address = (unsigned long)obj;\n\n\twhile (node) {\n\t\tske = container_of(node, spl_kmem_emergency_t, ske_node);\n\n\t\tif (address < ske->ske_obj)\n\t\t\tnode = node->rb_left;\n\t\telse if (address > ske->ske_obj)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn (ske);\n\t}\n\n\treturn (NULL);\n}\n\nstatic int\nspl_emergency_insert(struct rb_root *root, spl_kmem_emergency_t *ske)\n{\n\tstruct rb_node **new = &(root->rb_node), *parent = NULL;\n\tspl_kmem_emergency_t *ske_tmp;\n\tunsigned long address = ske->ske_obj;\n\n\twhile (*new) {\n\t\tske_tmp = container_of(*new, spl_kmem_emergency_t, ske_node);\n\n\t\tparent = *new;\n\t\tif (address < ske_tmp->ske_obj)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse if (address > ske_tmp->ske_obj)\n\t\t\tnew = &((*new)->rb_right);\n\t\telse\n\t\t\treturn (0);\n\t}\n\n\trb_link_node(&ske->ske_node, parent, new);\n\trb_insert_color(&ske->ske_node, root);\n\n\treturn (1);\n}\n\n \nstatic int\nspl_emergency_alloc(spl_kmem_cache_t *skc, int flags, void **obj)\n{\n\tgfp_t lflags = kmem_flags_convert(flags);\n\tspl_kmem_emergency_t *ske;\n\tint order = get_order(skc->skc_obj_size);\n\tint empty;\n\n\t \n\tspin_lock(&skc->skc_lock);\n\tempty = list_empty(&skc->skc_partial_list);\n\tspin_unlock(&skc->skc_lock);\n\tif (!empty)\n\t\treturn (-EEXIST);\n\n\tske = kmalloc(sizeof (*ske), lflags);\n\tif (ske == NULL)\n\t\treturn (-ENOMEM);\n\n\tske->ske_obj = __get_free_pages(lflags, order);\n\tif (ske->ske_obj == 0) {\n\t\tkfree(ske);\n\t\treturn (-ENOMEM);\n\t}\n\n\tspin_lock(&skc->skc_lock);\n\tempty = spl_emergency_insert(&skc->skc_emergency_tree, ske);\n\tif (likely(empty)) {\n\t\tskc->skc_obj_total++;\n\t\tskc->skc_obj_emergency++;\n\t\tif (skc->skc_obj_emergency > skc->skc_obj_emergency_max)\n\t\t\tskc->skc_obj_emergency_max = skc->skc_obj_emergency;\n\t}\n\tspin_unlock(&skc->skc_lock);\n\n\tif (unlikely(!empty)) {\n\t\tfree_pages(ske->ske_obj, order);\n\t\tkfree(ske);\n\t\treturn (-EINVAL);\n\t}\n\n\t*obj = (void *)ske->ske_obj;\n\n\treturn (0);\n}\n\n \nstatic int\nspl_emergency_free(spl_kmem_cache_t *skc, void *obj)\n{\n\tspl_kmem_emergency_t *ske;\n\tint order = get_order(skc->skc_obj_size);\n\n\tspin_lock(&skc->skc_lock);\n\tske = spl_emergency_search(&skc->skc_emergency_tree, obj);\n\tif (ske) {\n\t\trb_erase(&ske->ske_node, &skc->skc_emergency_tree);\n\t\tskc->skc_obj_emergency--;\n\t\tskc->skc_obj_total--;\n\t}\n\tspin_unlock(&skc->skc_lock);\n\n\tif (ske == NULL)\n\t\treturn (-ENOENT);\n\n\tfree_pages(ske->ske_obj, order);\n\tkfree(ske);\n\n\treturn (0);\n}\n\n \nstatic void\nspl_cache_flush(spl_kmem_cache_t *skc, spl_kmem_magazine_t *skm, int flush)\n{\n\tspin_lock(&skc->skc_lock);\n\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(skm->skm_magic == SKM_MAGIC);\n\n\tint count = MIN(flush, skm->skm_avail);\n\tfor (int i = 0; i < count; i++)\n\t\tspl_cache_shrink(skc, skm->skm_objs[i]);\n\n\tskm->skm_avail -= count;\n\tmemmove(skm->skm_objs, &(skm->skm_objs[count]),\n\t    sizeof (void *) * skm->skm_avail);\n\n\tspin_unlock(&skc->skc_lock);\n}\n\n \nstatic int\nspl_slab_size(spl_kmem_cache_t *skc, uint32_t *objs, uint32_t *size)\n{\n\tuint32_t sks_size, obj_size, max_size, tgt_size, tgt_objs;\n\n\tsks_size = spl_sks_size(skc);\n\tobj_size = spl_obj_size(skc);\n\tmax_size = (spl_kmem_cache_max_size * 1024 * 1024);\n\ttgt_size = (spl_kmem_cache_obj_per_slab * obj_size + sks_size);\n\n\tif (tgt_size <= max_size) {\n\t\ttgt_objs = (tgt_size - sks_size) / obj_size;\n\t} else {\n\t\ttgt_objs = (max_size - sks_size) / obj_size;\n\t\ttgt_size = (tgt_objs * obj_size) + sks_size;\n\t}\n\n\tif (tgt_objs == 0)\n\t\treturn (-ENOSPC);\n\n\t*objs = tgt_objs;\n\t*size = tgt_size;\n\n\treturn (0);\n}\n\n \nstatic int\nspl_magazine_size(spl_kmem_cache_t *skc)\n{\n\tuint32_t obj_size = spl_obj_size(skc);\n\tint size;\n\n\tif (spl_kmem_cache_magazine_size > 0)\n\t\treturn (MAX(MIN(spl_kmem_cache_magazine_size, 256), 2));\n\n\t \n\tif (obj_size > (PAGE_SIZE * 256))\n\t\tsize = 4;   \n\telse if (obj_size > (PAGE_SIZE * 32))\n\t\tsize = 16;  \n\telse if (obj_size > (PAGE_SIZE))\n\t\tsize = 64;  \n\telse if (obj_size > (PAGE_SIZE / 4))\n\t\tsize = 128;  \n\telse\n\t\tsize = 256;\n\n\treturn (size);\n}\n\n \nstatic spl_kmem_magazine_t *\nspl_magazine_alloc(spl_kmem_cache_t *skc, int cpu)\n{\n\tspl_kmem_magazine_t *skm;\n\tint size = sizeof (spl_kmem_magazine_t) +\n\t    sizeof (void *) * skc->skc_mag_size;\n\n\tskm = kmalloc_node(size, GFP_KERNEL, cpu_to_node(cpu));\n\tif (skm) {\n\t\tskm->skm_magic = SKM_MAGIC;\n\t\tskm->skm_avail = 0;\n\t\tskm->skm_size = skc->skc_mag_size;\n\t\tskm->skm_refill = skc->skc_mag_refill;\n\t\tskm->skm_cache = skc;\n\t\tskm->skm_cpu = cpu;\n\t}\n\n\treturn (skm);\n}\n\n \nstatic void\nspl_magazine_free(spl_kmem_magazine_t *skm)\n{\n\tASSERT(skm->skm_magic == SKM_MAGIC);\n\tASSERT(skm->skm_avail == 0);\n\tkfree(skm);\n}\n\n \nstatic int\nspl_magazine_create(spl_kmem_cache_t *skc)\n{\n\tint i = 0;\n\n\tASSERT((skc->skc_flags & KMC_SLAB) == 0);\n\n\tskc->skc_mag = kzalloc(sizeof (spl_kmem_magazine_t *) *\n\t    num_possible_cpus(), kmem_flags_convert(KM_SLEEP));\n\tskc->skc_mag_size = spl_magazine_size(skc);\n\tskc->skc_mag_refill = (skc->skc_mag_size + 1) / 2;\n\n\tfor_each_possible_cpu(i) {\n\t\tskc->skc_mag[i] = spl_magazine_alloc(skc, i);\n\t\tif (!skc->skc_mag[i]) {\n\t\t\tfor (i--; i >= 0; i--)\n\t\t\t\tspl_magazine_free(skc->skc_mag[i]);\n\n\t\t\tkfree(skc->skc_mag);\n\t\t\treturn (-ENOMEM);\n\t\t}\n\t}\n\n\treturn (0);\n}\n\n \nstatic void\nspl_magazine_destroy(spl_kmem_cache_t *skc)\n{\n\tspl_kmem_magazine_t *skm;\n\tint i = 0;\n\n\tASSERT((skc->skc_flags & KMC_SLAB) == 0);\n\n\tfor_each_possible_cpu(i) {\n\t\tskm = skc->skc_mag[i];\n\t\tspl_cache_flush(skc, skm, skm->skm_avail);\n\t\tspl_magazine_free(skm);\n\t}\n\n\tkfree(skc->skc_mag);\n}\n\n \nspl_kmem_cache_t *\nspl_kmem_cache_create(const char *name, size_t size, size_t align,\n    spl_kmem_ctor_t ctor, spl_kmem_dtor_t dtor, void *reclaim,\n    void *priv, void *vmp, int flags)\n{\n\tgfp_t lflags = kmem_flags_convert(KM_SLEEP);\n\tspl_kmem_cache_t *skc;\n\tint rc;\n\n\t \n\tASSERT(vmp == NULL);\n\tASSERT(reclaim == NULL);\n\n\tmight_sleep();\n\n\tskc = kzalloc(sizeof (*skc), lflags);\n\tif (skc == NULL)\n\t\treturn (NULL);\n\n\tskc->skc_magic = SKC_MAGIC;\n\tskc->skc_name_size = strlen(name) + 1;\n\tskc->skc_name = kmalloc(skc->skc_name_size, lflags);\n\tif (skc->skc_name == NULL) {\n\t\tkfree(skc);\n\t\treturn (NULL);\n\t}\n\tstrlcpy(skc->skc_name, name, skc->skc_name_size);\n\n\tskc->skc_ctor = ctor;\n\tskc->skc_dtor = dtor;\n\tskc->skc_private = priv;\n\tskc->skc_vmp = vmp;\n\tskc->skc_linux_cache = NULL;\n\tskc->skc_flags = flags;\n\tskc->skc_obj_size = size;\n\tskc->skc_obj_align = SPL_KMEM_CACHE_ALIGN;\n\tatomic_set(&skc->skc_ref, 0);\n\n\tINIT_LIST_HEAD(&skc->skc_list);\n\tINIT_LIST_HEAD(&skc->skc_complete_list);\n\tINIT_LIST_HEAD(&skc->skc_partial_list);\n\tskc->skc_emergency_tree = RB_ROOT;\n\tspin_lock_init(&skc->skc_lock);\n\tinit_waitqueue_head(&skc->skc_waitq);\n\tskc->skc_slab_fail = 0;\n\tskc->skc_slab_create = 0;\n\tskc->skc_slab_destroy = 0;\n\tskc->skc_slab_total = 0;\n\tskc->skc_slab_alloc = 0;\n\tskc->skc_slab_max = 0;\n\tskc->skc_obj_total = 0;\n\tskc->skc_obj_alloc = 0;\n\tskc->skc_obj_max = 0;\n\tskc->skc_obj_deadlock = 0;\n\tskc->skc_obj_emergency = 0;\n\tskc->skc_obj_emergency_max = 0;\n\n\trc = percpu_counter_init_common(&skc->skc_linux_alloc, 0,\n\t    GFP_KERNEL);\n\tif (rc != 0) {\n\t\tkfree(skc);\n\t\treturn (NULL);\n\t}\n\n\t \n\tif (align) {\n\t\tVERIFY(ISP2(align));\n\t\tVERIFY3U(align, >=, SPL_KMEM_CACHE_ALIGN);\n\t\tVERIFY3U(align, <=, PAGE_SIZE);\n\t\tskc->skc_obj_align = align;\n\t}\n\n\t \n\tif (!(skc->skc_flags & (KMC_SLAB | KMC_KVMEM))) {\n\t\tif (spl_kmem_cache_slab_limit &&\n\t\t    size <= (size_t)spl_kmem_cache_slab_limit) {\n\t\t\t \n\t\t\tskc->skc_flags |= KMC_SLAB;\n\t\t} else {\n\t\t\t \n\t\t\tskc->skc_flags |= KMC_KVMEM;\n\t\t}\n\t}\n\n\t \n\tif (skc->skc_flags & KMC_KVMEM) {\n\t\trc = spl_slab_size(skc,\n\t\t    &skc->skc_slab_objs, &skc->skc_slab_size);\n\t\tif (rc)\n\t\t\tgoto out;\n\n\t\trc = spl_magazine_create(skc);\n\t\tif (rc)\n\t\t\tgoto out;\n\t} else {\n\t\tunsigned long slabflags = 0;\n\n\t\tif (size > (SPL_MAX_KMEM_ORDER_NR_PAGES * PAGE_SIZE))\n\t\t\tgoto out;\n\n#if defined(SLAB_USERCOPY)\n\t\t \n\t\tslabflags |= SLAB_USERCOPY;\n#endif\n\n#if defined(HAVE_KMEM_CACHE_CREATE_USERCOPY)\n\t\t \n\t\tskc->skc_linux_cache = kmem_cache_create_usercopy(\n\t\t    skc->skc_name, size, align, slabflags, 0, size, NULL);\n#else\n\t\tskc->skc_linux_cache = kmem_cache_create(\n\t\t    skc->skc_name, size, align, slabflags, NULL);\n#endif\n\t\tif (skc->skc_linux_cache == NULL)\n\t\t\tgoto out;\n\t}\n\n\tdown_write(&spl_kmem_cache_sem);\n\tlist_add_tail(&skc->skc_list, &spl_kmem_cache_list);\n\tup_write(&spl_kmem_cache_sem);\n\n\treturn (skc);\nout:\n\tkfree(skc->skc_name);\n\tpercpu_counter_destroy(&skc->skc_linux_alloc);\n\tkfree(skc);\n\treturn (NULL);\n}\nEXPORT_SYMBOL(spl_kmem_cache_create);\n\n \nvoid\nspl_kmem_cache_set_move(spl_kmem_cache_t *skc,\n    kmem_cbrc_t (move)(void *, void *, size_t, void *))\n{\n\tASSERT(move != NULL);\n}\nEXPORT_SYMBOL(spl_kmem_cache_set_move);\n\n \nvoid\nspl_kmem_cache_destroy(spl_kmem_cache_t *skc)\n{\n\tDECLARE_WAIT_QUEUE_HEAD(wq);\n\ttaskqid_t id;\n\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(skc->skc_flags & (KMC_KVMEM | KMC_SLAB));\n\n\tdown_write(&spl_kmem_cache_sem);\n\tlist_del_init(&skc->skc_list);\n\tup_write(&spl_kmem_cache_sem);\n\n\t \n\tVERIFY(!test_and_set_bit(KMC_BIT_DESTROY, &skc->skc_flags));\n\n\tspin_lock(&skc->skc_lock);\n\tid = skc->skc_taskqid;\n\tspin_unlock(&skc->skc_lock);\n\n\ttaskq_cancel_id(spl_kmem_cache_taskq, id);\n\n\t \n\twait_event(wq, atomic_read(&skc->skc_ref) == 0);\n\n\tif (skc->skc_flags & KMC_KVMEM) {\n\t\tspl_magazine_destroy(skc);\n\t\tspl_slab_reclaim(skc);\n\t} else {\n\t\tASSERT(skc->skc_flags & KMC_SLAB);\n\t\tkmem_cache_destroy(skc->skc_linux_cache);\n\t}\n\n\tspin_lock(&skc->skc_lock);\n\n\t \n\tASSERT3U(skc->skc_slab_alloc, ==, 0);\n\tASSERT3U(skc->skc_obj_alloc, ==, 0);\n\tASSERT3U(skc->skc_slab_total, ==, 0);\n\tASSERT3U(skc->skc_obj_total, ==, 0);\n\tASSERT3U(skc->skc_obj_emergency, ==, 0);\n\tASSERT(list_empty(&skc->skc_complete_list));\n\n\tASSERT3U(percpu_counter_sum(&skc->skc_linux_alloc), ==, 0);\n\tpercpu_counter_destroy(&skc->skc_linux_alloc);\n\n\tspin_unlock(&skc->skc_lock);\n\n\tkfree(skc->skc_name);\n\tkfree(skc);\n}\nEXPORT_SYMBOL(spl_kmem_cache_destroy);\n\n \nstatic void *\nspl_cache_obj(spl_kmem_cache_t *skc, spl_kmem_slab_t *sks)\n{\n\tspl_kmem_obj_t *sko;\n\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(sks->sks_magic == SKS_MAGIC);\n\n\tsko = list_entry(sks->sks_free_list.next, spl_kmem_obj_t, sko_list);\n\tASSERT(sko->sko_magic == SKO_MAGIC);\n\tASSERT(sko->sko_addr != NULL);\n\n\t \n\tlist_del_init(&sko->sko_list);\n\n\tsks->sks_age = jiffies;\n\tsks->sks_ref++;\n\tskc->skc_obj_alloc++;\n\n\t \n\tif (skc->skc_obj_alloc > skc->skc_obj_max)\n\t\tskc->skc_obj_max = skc->skc_obj_alloc;\n\n\t \n\tif (sks->sks_ref == 1) {\n\t\tskc->skc_slab_alloc++;\n\n\t\tif (skc->skc_slab_alloc > skc->skc_slab_max)\n\t\t\tskc->skc_slab_max = skc->skc_slab_alloc;\n\t}\n\n\treturn (sko->sko_addr);\n}\n\n \nstatic int\n__spl_cache_grow(spl_kmem_cache_t *skc, int flags)\n{\n\tspl_kmem_slab_t *sks;\n\n\tfstrans_cookie_t cookie = spl_fstrans_mark();\n\tsks = spl_slab_alloc(skc, flags);\n\tspl_fstrans_unmark(cookie);\n\n\tspin_lock(&skc->skc_lock);\n\tif (sks) {\n\t\tskc->skc_slab_total++;\n\t\tskc->skc_obj_total += sks->sks_objs;\n\t\tlist_add_tail(&sks->sks_list, &skc->skc_partial_list);\n\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(KMC_BIT_DEADLOCKED, &skc->skc_flags);\n\t\tsmp_mb__after_atomic();\n\t}\n\tspin_unlock(&skc->skc_lock);\n\n\treturn (sks == NULL ? -ENOMEM : 0);\n}\n\nstatic void\nspl_cache_grow_work(void *data)\n{\n\tspl_kmem_alloc_t *ska = (spl_kmem_alloc_t *)data;\n\tspl_kmem_cache_t *skc = ska->ska_cache;\n\n\tint error = __spl_cache_grow(skc, ska->ska_flags);\n\n\tatomic_dec(&skc->skc_ref);\n\tsmp_mb__before_atomic();\n\tclear_bit(KMC_BIT_GROWING, &skc->skc_flags);\n\tsmp_mb__after_atomic();\n\tif (error == 0)\n\t\twake_up_all(&skc->skc_waitq);\n\n\tkfree(ska);\n}\n\n \nstatic int\nspl_cache_grow_wait(spl_kmem_cache_t *skc)\n{\n\treturn (!test_bit(KMC_BIT_GROWING, &skc->skc_flags));\n}\n\n \nstatic int\nspl_cache_grow(spl_kmem_cache_t *skc, int flags, void **obj)\n{\n\tint remaining, rc = 0;\n\n\tASSERT0(flags & ~KM_PUBLIC_MASK);\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT((skc->skc_flags & KMC_SLAB) == 0);\n\n\t*obj = NULL;\n\n\t \n\tif (flags & KM_NOSLEEP)\n\t\treturn (spl_emergency_alloc(skc, flags, obj));\n\n\tmight_sleep();\n\n\t \n\tif (test_bit(KMC_BIT_REAPING, &skc->skc_flags)) {\n\t\trc = spl_wait_on_bit(&skc->skc_flags, KMC_BIT_REAPING,\n\t\t    TASK_UNINTERRUPTIBLE);\n\t\treturn (rc ? rc : -EAGAIN);\n\t}\n\n\t \n\n\t \n\tif (test_and_set_bit(KMC_BIT_GROWING, &skc->skc_flags) == 0) {\n\t\tspl_kmem_alloc_t *ska;\n\n\t\tska = kmalloc(sizeof (*ska), kmem_flags_convert(flags));\n\t\tif (ska == NULL) {\n\t\t\tclear_bit_unlock(KMC_BIT_GROWING, &skc->skc_flags);\n\t\t\tsmp_mb__after_atomic();\n\t\t\twake_up_all(&skc->skc_waitq);\n\t\t\treturn (-ENOMEM);\n\t\t}\n\n\t\tatomic_inc(&skc->skc_ref);\n\t\tska->ska_cache = skc;\n\t\tska->ska_flags = flags;\n\t\ttaskq_init_ent(&ska->ska_tqe);\n\t\ttaskq_dispatch_ent(spl_kmem_cache_taskq,\n\t\t    spl_cache_grow_work, ska, 0, &ska->ska_tqe);\n\t}\n\n\t \n\tif (test_bit(KMC_BIT_DEADLOCKED, &skc->skc_flags)) {\n\t\trc = spl_emergency_alloc(skc, flags, obj);\n\t} else {\n\t\tremaining = wait_event_timeout(skc->skc_waitq,\n\t\t    spl_cache_grow_wait(skc), HZ / 10);\n\n\t\tif (!remaining) {\n\t\t\tspin_lock(&skc->skc_lock);\n\t\t\tif (test_bit(KMC_BIT_GROWING, &skc->skc_flags)) {\n\t\t\t\tset_bit(KMC_BIT_DEADLOCKED, &skc->skc_flags);\n\t\t\t\tskc->skc_obj_deadlock++;\n\t\t\t}\n\t\t\tspin_unlock(&skc->skc_lock);\n\t\t}\n\n\t\trc = -ENOMEM;\n\t}\n\n\treturn (rc);\n}\n\n \nstatic void *\nspl_cache_refill(spl_kmem_cache_t *skc, spl_kmem_magazine_t *skm, int flags)\n{\n\tspl_kmem_slab_t *sks;\n\tint count = 0, rc, refill;\n\tvoid *obj = NULL;\n\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(skm->skm_magic == SKM_MAGIC);\n\n\trefill = MIN(skm->skm_refill, skm->skm_size - skm->skm_avail);\n\tspin_lock(&skc->skc_lock);\n\n\twhile (refill > 0) {\n\t\t \n\t\tif (list_empty(&skc->skc_partial_list)) {\n\t\t\tspin_unlock(&skc->skc_lock);\n\n\t\t\tlocal_irq_enable();\n\t\t\trc = spl_cache_grow(skc, flags, &obj);\n\t\t\tlocal_irq_disable();\n\n\t\t\t \n\t\t\tif (rc == 0 && obj != NULL)\n\t\t\t\treturn (obj);\n\n\t\t\tif (rc)\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\tif (skm != skc->skc_mag[smp_processor_id()])\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\trefill = MIN(refill, skm->skm_size - skm->skm_avail);\n\n\t\t\tspin_lock(&skc->skc_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tsks = list_entry((&skc->skc_partial_list)->next,\n\t\t    spl_kmem_slab_t, sks_list);\n\t\tASSERT(sks->sks_magic == SKS_MAGIC);\n\t\tASSERT(sks->sks_ref < sks->sks_objs);\n\t\tASSERT(!list_empty(&sks->sks_free_list));\n\n\t\t \n\t\twhile (sks->sks_ref < sks->sks_objs && refill-- > 0 &&\n\t\t    ++count) {\n\t\t\tASSERT(skm->skm_avail < skm->skm_size);\n\t\t\tASSERT(count < skm->skm_size);\n\t\t\tskm->skm_objs[skm->skm_avail++] =\n\t\t\t    spl_cache_obj(skc, sks);\n\t\t}\n\n\t\t \n\t\tif (sks->sks_ref == sks->sks_objs) {\n\t\t\tlist_del(&sks->sks_list);\n\t\t\tlist_add(&sks->sks_list, &skc->skc_complete_list);\n\t\t}\n\t}\n\n\tspin_unlock(&skc->skc_lock);\nout:\n\treturn (NULL);\n}\n\n \nstatic void\nspl_cache_shrink(spl_kmem_cache_t *skc, void *obj)\n{\n\tspl_kmem_slab_t *sks = NULL;\n\tspl_kmem_obj_t *sko = NULL;\n\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\n\tsko = spl_sko_from_obj(skc, obj);\n\tASSERT(sko->sko_magic == SKO_MAGIC);\n\tsks = sko->sko_slab;\n\tASSERT(sks->sks_magic == SKS_MAGIC);\n\tASSERT(sks->sks_cache == skc);\n\tlist_add(&sko->sko_list, &sks->sks_free_list);\n\n\tsks->sks_age = jiffies;\n\tsks->sks_ref--;\n\tskc->skc_obj_alloc--;\n\n\t \n\tif (sks->sks_ref == (sks->sks_objs - 1)) {\n\t\tlist_del(&sks->sks_list);\n\t\tlist_add(&sks->sks_list, &skc->skc_partial_list);\n\t}\n\n\t \n\tif (sks->sks_ref == 0) {\n\t\tlist_del(&sks->sks_list);\n\t\tlist_add_tail(&sks->sks_list, &skc->skc_partial_list);\n\t\tskc->skc_slab_alloc--;\n\t}\n}\n\n \nvoid *\nspl_kmem_cache_alloc(spl_kmem_cache_t *skc, int flags)\n{\n\tspl_kmem_magazine_t *skm;\n\tvoid *obj = NULL;\n\n\tASSERT0(flags & ~KM_PUBLIC_MASK);\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(!test_bit(KMC_BIT_DESTROY, &skc->skc_flags));\n\n\t \n\tif (skc->skc_flags & KMC_SLAB) {\n\t\tstruct kmem_cache *slc = skc->skc_linux_cache;\n\t\tdo {\n\t\t\tobj = kmem_cache_alloc(slc, kmem_flags_convert(flags));\n\t\t} while ((obj == NULL) && !(flags & KM_NOSLEEP));\n\n\t\tif (obj != NULL) {\n\t\t\t \n\t\t\tpercpu_counter_inc(&skc->skc_linux_alloc);\n\t\t}\n\t\tgoto ret;\n\t}\n\n\tlocal_irq_disable();\n\nrestart:\n\t \n\tskm = skc->skc_mag[smp_processor_id()];\n\tASSERT(skm->skm_magic == SKM_MAGIC);\n\n\tif (likely(skm->skm_avail)) {\n\t\t \n\t\tobj = skm->skm_objs[--skm->skm_avail];\n\t} else {\n\t\tobj = spl_cache_refill(skc, skm, flags);\n\t\tif ((obj == NULL) && !(flags & KM_NOSLEEP))\n\t\t\tgoto restart;\n\n\t\tlocal_irq_enable();\n\t\tgoto ret;\n\t}\n\n\tlocal_irq_enable();\n\tASSERT(obj);\n\tASSERT(IS_P2ALIGNED(obj, skc->skc_obj_align));\n\nret:\n\t \n\tif (obj) {\n\t\tif (obj && skc->skc_ctor)\n\t\t\tskc->skc_ctor(obj, skc->skc_private, flags);\n\t\telse\n\t\t\tprefetchw(obj);\n\t}\n\n\treturn (obj);\n}\nEXPORT_SYMBOL(spl_kmem_cache_alloc);\n\n \nvoid\nspl_kmem_cache_free(spl_kmem_cache_t *skc, void *obj)\n{\n\tspl_kmem_magazine_t *skm;\n\tunsigned long flags;\n\tint do_reclaim = 0;\n\tint do_emergency = 0;\n\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(!test_bit(KMC_BIT_DESTROY, &skc->skc_flags));\n\n\t \n\tif (skc->skc_dtor)\n\t\tskc->skc_dtor(obj, skc->skc_private);\n\n\t \n\tif (skc->skc_flags & KMC_SLAB) {\n\t\tkmem_cache_free(skc->skc_linux_cache, obj);\n\t\tpercpu_counter_dec(&skc->skc_linux_alloc);\n\t\treturn;\n\t}\n\n\t \n\tif (!is_vmalloc_addr(obj)) {\n\t\tspin_lock(&skc->skc_lock);\n\t\tdo_emergency = (skc->skc_obj_emergency > 0);\n\t\tspin_unlock(&skc->skc_lock);\n\n\t\tif (do_emergency && (spl_emergency_free(skc, obj) == 0))\n\t\t\treturn;\n\t}\n\n\tlocal_irq_save(flags);\n\n\t \n\tskm = skc->skc_mag[smp_processor_id()];\n\tASSERT(skm->skm_magic == SKM_MAGIC);\n\n\t \n\tif (unlikely(skm->skm_avail >= skm->skm_size)) {\n\t\tspl_cache_flush(skc, skm, skm->skm_refill);\n\t\tdo_reclaim = 1;\n\t}\n\n\t \n\tskm->skm_objs[skm->skm_avail++] = obj;\n\n\tlocal_irq_restore(flags);\n\n\tif (do_reclaim)\n\t\tspl_slab_reclaim(skc);\n}\nEXPORT_SYMBOL(spl_kmem_cache_free);\n\n \nvoid\nspl_kmem_cache_reap_now(spl_kmem_cache_t *skc)\n{\n\tASSERT(skc->skc_magic == SKC_MAGIC);\n\tASSERT(!test_bit(KMC_BIT_DESTROY, &skc->skc_flags));\n\n\tif (skc->skc_flags & KMC_SLAB)\n\t\treturn;\n\n\tatomic_inc(&skc->skc_ref);\n\n\t \n\tif (test_and_set_bit(KMC_BIT_REAPING, &skc->skc_flags))\n\t\tgoto out;\n\n\t \n\tunsigned long irq_flags;\n\tlocal_irq_save(irq_flags);\n\tspl_kmem_magazine_t *skm = skc->skc_mag[smp_processor_id()];\n\tspl_cache_flush(skc, skm, skm->skm_avail);\n\tlocal_irq_restore(irq_flags);\n\n\tspl_slab_reclaim(skc);\n\tclear_bit_unlock(KMC_BIT_REAPING, &skc->skc_flags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&skc->skc_flags, KMC_BIT_REAPING);\nout:\n\tatomic_dec(&skc->skc_ref);\n}\nEXPORT_SYMBOL(spl_kmem_cache_reap_now);\n\n \nint\nspl_kmem_cache_reap_active(void)\n{\n\treturn (0);\n}\nEXPORT_SYMBOL(spl_kmem_cache_reap_active);\n\n \nvoid\nspl_kmem_reap(void)\n{\n\tspl_kmem_cache_t *skc = NULL;\n\n\tdown_read(&spl_kmem_cache_sem);\n\tlist_for_each_entry(skc, &spl_kmem_cache_list, skc_list) {\n\t\tspl_kmem_cache_reap_now(skc);\n\t}\n\tup_read(&spl_kmem_cache_sem);\n}\nEXPORT_SYMBOL(spl_kmem_reap);\n\nint\nspl_kmem_cache_init(void)\n{\n\tinit_rwsem(&spl_kmem_cache_sem);\n\tINIT_LIST_HEAD(&spl_kmem_cache_list);\n\tspl_kmem_cache_taskq = taskq_create(\"spl_kmem_cache\",\n\t    spl_kmem_cache_kmem_threads, maxclsyspri,\n\t    spl_kmem_cache_kmem_threads * 8, INT_MAX,\n\t    TASKQ_PREPOPULATE | TASKQ_DYNAMIC);\n\n\tif (spl_kmem_cache_taskq == NULL)\n\t\treturn (-ENOMEM);\n\n\treturn (0);\n}\n\nvoid\nspl_kmem_cache_fini(void)\n{\n\ttaskq_destroy(spl_kmem_cache_taskq);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}