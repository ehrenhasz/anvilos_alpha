{
  "module_name": "vdev_disk.c",
  "hash_id": "e557c4b4a50788ee4566cde213078a6960223a6830a27f5f5c65c4a42876af73",
  "original_prompt": "Ingested from zfs-2.2.2/module/os/linux/zfs/vdev_disk.c",
  "human_readable_source": " \n \n\n#include <sys/zfs_context.h>\n#include <sys/spa_impl.h>\n#include <sys/vdev_disk.h>\n#include <sys/vdev_impl.h>\n#include <sys/vdev_trim.h>\n#include <sys/abd.h>\n#include <sys/fs/zfs.h>\n#include <sys/zio.h>\n#include <linux/blkpg.h>\n#include <linux/msdos_fs.h>\n#include <linux/vfs_compat.h>\n#ifdef HAVE_LINUX_BLK_CGROUP_HEADER\n#include <linux/blk-cgroup.h>\n#endif\n\ntypedef struct vdev_disk {\n\tstruct block_device\t\t*vd_bdev;\n\tkrwlock_t\t\t\tvd_lock;\n} vdev_disk_t;\n\n \nstatic void *zfs_vdev_holder = VDEV_HOLDER;\n\n \nstatic uint_t zfs_vdev_open_timeout_ms = 1000;\n\n \n#define\tEFI_MIN_RESV_SIZE\t(16 * 1024)\n\n \ntypedef struct dio_request {\n\tzio_t\t\t\t*dr_zio;\t \n\tatomic_t\t\tdr_ref;\t\t \n\tint\t\t\tdr_error;\t \n\tint\t\t\tdr_bio_count;\t \n\tstruct bio\t\t*dr_bio[];\t \n} dio_request_t;\n\n \n\nstatic unsigned int zfs_vdev_failfast_mask = 1;\n\n#ifdef HAVE_BLK_MODE_T\nstatic blk_mode_t\n#else\nstatic fmode_t\n#endif\nvdev_bdev_mode(spa_mode_t spa_mode)\n{\n#ifdef HAVE_BLK_MODE_T\n\tblk_mode_t mode = 0;\n\n\tif (spa_mode & SPA_MODE_READ)\n\t\tmode |= BLK_OPEN_READ;\n\n\tif (spa_mode & SPA_MODE_WRITE)\n\t\tmode |= BLK_OPEN_WRITE;\n#else\n\tfmode_t mode = 0;\n\n\tif (spa_mode & SPA_MODE_READ)\n\t\tmode |= FMODE_READ;\n\n\tif (spa_mode & SPA_MODE_WRITE)\n\t\tmode |= FMODE_WRITE;\n#endif\n\n\treturn (mode);\n}\n\n \nstatic uint64_t\nbdev_capacity(struct block_device *bdev)\n{\n\treturn (i_size_read(bdev->bd_inode));\n}\n\n#if !defined(HAVE_BDEV_WHOLE)\nstatic inline struct block_device *\nbdev_whole(struct block_device *bdev)\n{\n\treturn (bdev->bd_contains);\n}\n#endif\n\n#if defined(HAVE_BDEVNAME)\n#define\tvdev_bdevname(bdev, name)\tbdevname(bdev, name)\n#else\nstatic inline void\nvdev_bdevname(struct block_device *bdev, char *name)\n{\n\tsnprintf(name, BDEVNAME_SIZE, \"%pg\", bdev);\n}\n#endif\n\n \nstatic uint64_t\nbdev_max_capacity(struct block_device *bdev, uint64_t wholedisk)\n{\n\tuint64_t psize;\n\tint64_t available;\n\n\tif (wholedisk && bdev != bdev_whole(bdev)) {\n\t\t \n\t\tavailable = i_size_read(bdev_whole(bdev)->bd_inode) -\n\t\t    ((EFI_MIN_RESV_SIZE + NEW_START_BLOCK +\n\t\t    PARTITION_END_ALIGNMENT) << SECTOR_BITS);\n\t\tpsize = MAX(available, bdev_capacity(bdev));\n\t} else {\n\t\tpsize = bdev_capacity(bdev);\n\t}\n\n\treturn (psize);\n}\n\nstatic void\nvdev_disk_error(zio_t *zio)\n{\n\t \n\tprintk(KERN_WARNING \"zio pool=%s vdev=%s error=%d type=%d \"\n\t    \"offset=%llu size=%llu flags=%llu\\n\", spa_name(zio->io_spa),\n\t    zio->io_vd->vdev_path, zio->io_error, zio->io_type,\n\t    (u_longlong_t)zio->io_offset, (u_longlong_t)zio->io_size,\n\t    zio->io_flags);\n}\n\nstatic void\nvdev_disk_kobj_evt_post(vdev_t *v)\n{\n\tvdev_disk_t *vd = v->vdev_tsd;\n\tif (vd && vd->vd_bdev) {\n\t\tspl_signal_kobj_evt(vd->vd_bdev);\n\t} else {\n\t\tvdev_dbgmsg(v, \"vdev_disk_t is NULL for VDEV:%s\\n\",\n\t\t    v->vdev_path);\n\t}\n}\n\n#if !defined(HAVE_BLKDEV_GET_BY_PATH_4ARG)\n \nstruct blk_holder_ops {};\n#endif\n\nstatic struct block_device *\nvdev_blkdev_get_by_path(const char *path, spa_mode_t mode, void *holder,\n    const struct blk_holder_ops *hops)\n{\n#ifdef HAVE_BLKDEV_GET_BY_PATH_4ARG\n\treturn (blkdev_get_by_path(path,\n\t    vdev_bdev_mode(mode) | BLK_OPEN_EXCL, holder, hops));\n#else\n\treturn (blkdev_get_by_path(path,\n\t    vdev_bdev_mode(mode) | FMODE_EXCL, holder));\n#endif\n}\n\nstatic void\nvdev_blkdev_put(struct block_device *bdev, spa_mode_t mode, void *holder)\n{\n#ifdef HAVE_BLKDEV_PUT_HOLDER\n\treturn (blkdev_put(bdev, holder));\n#else\n\treturn (blkdev_put(bdev, vdev_bdev_mode(mode) | FMODE_EXCL));\n#endif\n}\n\nstatic int\nvdev_disk_open(vdev_t *v, uint64_t *psize, uint64_t *max_psize,\n    uint64_t *logical_ashift, uint64_t *physical_ashift)\n{\n\tstruct block_device *bdev;\n#ifdef HAVE_BLK_MODE_T\n\tblk_mode_t mode = vdev_bdev_mode(spa_mode(v->vdev_spa));\n#else\n\tfmode_t mode = vdev_bdev_mode(spa_mode(v->vdev_spa));\n#endif\n\thrtime_t timeout = MSEC2NSEC(zfs_vdev_open_timeout_ms);\n\tvdev_disk_t *vd;\n\n\t \n\tif (v->vdev_path == NULL || v->vdev_path[0] != '/') {\n\t\tv->vdev_stat.vs_aux = VDEV_AUX_BAD_LABEL;\n\t\tvdev_dbgmsg(v, \"invalid vdev_path\");\n\t\treturn (SET_ERROR(EINVAL));\n\t}\n\n\t \n\tvd = v->vdev_tsd;\n\tif (vd) {\n\t\tchar disk_name[BDEVNAME_SIZE + 6] = \"/dev/\";\n\t\tboolean_t reread_part = B_FALSE;\n\n\t\trw_enter(&vd->vd_lock, RW_WRITER);\n\t\tbdev = vd->vd_bdev;\n\t\tvd->vd_bdev = NULL;\n\n\t\tif (bdev) {\n\t\t\tif (v->vdev_expanding && bdev != bdev_whole(bdev)) {\n\t\t\t\tvdev_bdevname(bdev_whole(bdev), disk_name + 5);\n\t\t\t\t \n\t\t\t\tif (v->vdev_psize == bdev_capacity(bdev))\n\t\t\t\t\treread_part = B_TRUE;\n\t\t\t}\n\n\t\t\tvdev_blkdev_put(bdev, mode, zfs_vdev_holder);\n\t\t}\n\n\t\tif (reread_part) {\n\t\t\tbdev = vdev_blkdev_get_by_path(disk_name, mode,\n\t\t\t    zfs_vdev_holder, NULL);\n\t\t\tif (!IS_ERR(bdev)) {\n\t\t\t\tint error = vdev_bdev_reread_part(bdev);\n\t\t\t\tvdev_blkdev_put(bdev, mode, zfs_vdev_holder);\n\t\t\t\tif (error == 0) {\n\t\t\t\t\ttimeout = MSEC2NSEC(\n\t\t\t\t\t    zfs_vdev_open_timeout_ms * 2);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tvd = kmem_zalloc(sizeof (vdev_disk_t), KM_SLEEP);\n\n\t\trw_init(&vd->vd_lock, NULL, RW_DEFAULT, NULL);\n\t\trw_enter(&vd->vd_lock, RW_WRITER);\n\t}\n\n\t \n\thrtime_t start = gethrtime();\n\tbdev = ERR_PTR(-ENXIO);\n\twhile (IS_ERR(bdev) && ((gethrtime() - start) < timeout)) {\n\t\tbdev = vdev_blkdev_get_by_path(v->vdev_path, mode,\n\t\t    zfs_vdev_holder, NULL);\n\t\tif (unlikely(PTR_ERR(bdev) == -ENOENT)) {\n\t\t\t \n\t\t\tif (v->vdev_removed)\n\t\t\t\tbreak;\n\n\t\t\tschedule_timeout(MSEC_TO_TICK(10));\n\t\t} else if (unlikely(PTR_ERR(bdev) == -ERESTARTSYS)) {\n\t\t\ttimeout = MSEC2NSEC(zfs_vdev_open_timeout_ms * 10);\n\t\t\tcontinue;\n\t\t} else if (IS_ERR(bdev)) {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (IS_ERR(bdev)) {\n\t\tint error = -PTR_ERR(bdev);\n\t\tvdev_dbgmsg(v, \"open error=%d timeout=%llu/%llu\", error,\n\t\t    (u_longlong_t)(gethrtime() - start),\n\t\t    (u_longlong_t)timeout);\n\t\tvd->vd_bdev = NULL;\n\t\tv->vdev_tsd = vd;\n\t\trw_exit(&vd->vd_lock);\n\t\treturn (SET_ERROR(error));\n\t} else {\n\t\tvd->vd_bdev = bdev;\n\t\tv->vdev_tsd = vd;\n\t\trw_exit(&vd->vd_lock);\n\t}\n\n\t \n\tint physical_block_size = bdev_physical_block_size(vd->vd_bdev);\n\n\t \n\tint logical_block_size = bdev_logical_block_size(vd->vd_bdev);\n\n\t \n\tv->vdev_nowritecache = B_FALSE;\n\n\t \n\tv->vdev_has_trim = bdev_discard_supported(vd->vd_bdev);\n\n\t \n\tv->vdev_has_securetrim = bdev_secure_discard_supported(vd->vd_bdev);\n\n\t \n\tv->vdev_nonrot = blk_queue_nonrot(bdev_get_queue(vd->vd_bdev));\n\n\t \n\t*psize = bdev_capacity(vd->vd_bdev);\n\n\t \n\t*max_psize = bdev_max_capacity(vd->vd_bdev, v->vdev_wholedisk);\n\n\t \n\t*physical_ashift = highbit64(MAX(physical_block_size,\n\t    SPA_MINBLOCKSIZE)) - 1;\n\n\t*logical_ashift = highbit64(MAX(logical_block_size,\n\t    SPA_MINBLOCKSIZE)) - 1;\n\n\treturn (0);\n}\n\nstatic void\nvdev_disk_close(vdev_t *v)\n{\n\tvdev_disk_t *vd = v->vdev_tsd;\n\n\tif (v->vdev_reopening || vd == NULL)\n\t\treturn;\n\n\tif (vd->vd_bdev != NULL) {\n\t\tvdev_blkdev_put(vd->vd_bdev, spa_mode(v->vdev_spa),\n\t\t    zfs_vdev_holder);\n\t}\n\n\trw_destroy(&vd->vd_lock);\n\tkmem_free(vd, sizeof (vdev_disk_t));\n\tv->vdev_tsd = NULL;\n}\n\nstatic dio_request_t *\nvdev_disk_dio_alloc(int bio_count)\n{\n\tdio_request_t *dr = kmem_zalloc(sizeof (dio_request_t) +\n\t    sizeof (struct bio *) * bio_count, KM_SLEEP);\n\tatomic_set(&dr->dr_ref, 0);\n\tdr->dr_bio_count = bio_count;\n\tdr->dr_error = 0;\n\n\tfor (int i = 0; i < dr->dr_bio_count; i++)\n\t\tdr->dr_bio[i] = NULL;\n\n\treturn (dr);\n}\n\nstatic void\nvdev_disk_dio_free(dio_request_t *dr)\n{\n\tint i;\n\n\tfor (i = 0; i < dr->dr_bio_count; i++)\n\t\tif (dr->dr_bio[i])\n\t\t\tbio_put(dr->dr_bio[i]);\n\n\tkmem_free(dr, sizeof (dio_request_t) +\n\t    sizeof (struct bio *) * dr->dr_bio_count);\n}\n\nstatic void\nvdev_disk_dio_get(dio_request_t *dr)\n{\n\tatomic_inc(&dr->dr_ref);\n}\n\nstatic void\nvdev_disk_dio_put(dio_request_t *dr)\n{\n\tint rc = atomic_dec_return(&dr->dr_ref);\n\n\t \n\tif (rc == 0) {\n\t\tzio_t *zio = dr->dr_zio;\n\t\tint error = dr->dr_error;\n\n\t\tvdev_disk_dio_free(dr);\n\n\t\tif (zio) {\n\t\t\tzio->io_error = error;\n\t\t\tASSERT3S(zio->io_error, >=, 0);\n\t\t\tif (zio->io_error)\n\t\t\t\tvdev_disk_error(zio);\n\n\t\t\tzio_delay_interrupt(zio);\n\t\t}\n\t}\n}\n\nBIO_END_IO_PROTO(vdev_disk_physio_completion, bio, error)\n{\n\tdio_request_t *dr = bio->bi_private;\n\n\tif (dr->dr_error == 0) {\n#ifdef HAVE_1ARG_BIO_END_IO_T\n\t\tdr->dr_error = BIO_END_IO_ERROR(bio);\n#else\n\t\tif (error)\n\t\t\tdr->dr_error = -(error);\n\t\telse if (!test_bit(BIO_UPTODATE, &bio->bi_flags))\n\t\t\tdr->dr_error = EIO;\n#endif\n\t}\n\n\t \n\tvdev_disk_dio_put(dr);\n}\n\nstatic inline void\nvdev_submit_bio_impl(struct bio *bio)\n{\n#ifdef HAVE_1ARG_SUBMIT_BIO\n\t(void) submit_bio(bio);\n#else\n\t(void) submit_bio(bio_data_dir(bio), bio);\n#endif\n}\n\n \n#if defined(CONFIG_ARM64) && \\\n    defined(CONFIG_PREEMPTION) && \\\n    defined(CONFIG_BLK_CGROUP)\n#define\tpreempt_schedule_notrace(x) preempt_schedule(x)\n#endif\n\n \n#if !defined(HAVE_BIO_ALLOC_4ARG)\n\n#ifdef HAVE_BIO_SET_DEV\n#if defined(CONFIG_BLK_CGROUP) && defined(HAVE_BIO_SET_DEV_GPL_ONLY)\n \n#if defined(HAVE_BLKG_TRYGET_GPL_ONLY) || !defined(HAVE_BLKG_TRYGET)\nstatic inline bool\nvdev_blkg_tryget(struct blkcg_gq *blkg)\n{\n\tstruct percpu_ref *ref = &blkg->refcnt;\n\tunsigned long __percpu *count;\n\tbool rc;\n\n\trcu_read_lock_sched();\n\n\tif (__ref_is_percpu(ref, &count)) {\n\t\tthis_cpu_inc(*count);\n\t\trc = true;\n\t} else {\n#ifdef ZFS_PERCPU_REF_COUNT_IN_DATA\n\t\trc = atomic_long_inc_not_zero(&ref->data->count);\n#else\n\t\trc = atomic_long_inc_not_zero(&ref->count);\n#endif\n\t}\n\n\trcu_read_unlock_sched();\n\n\treturn (rc);\n}\n#else\n#define\tvdev_blkg_tryget(bg)\tblkg_tryget(bg)\n#endif\n#ifdef HAVE_BIO_SET_DEV_MACRO\n \nstatic inline void\nvdev_bio_associate_blkg(struct bio *bio)\n{\n#if defined(HAVE_BIO_BDEV_DISK)\n\tstruct request_queue *q = bio->bi_bdev->bd_disk->queue;\n#else\n\tstruct request_queue *q = bio->bi_disk->queue;\n#endif\n\n\tASSERT3P(q, !=, NULL);\n\tASSERT3P(bio->bi_blkg, ==, NULL);\n\n\tif (q->root_blkg && vdev_blkg_tryget(q->root_blkg))\n\t\tbio->bi_blkg = q->root_blkg;\n}\n\n#define\tbio_associate_blkg vdev_bio_associate_blkg\n#else\nstatic inline void\nvdev_bio_set_dev(struct bio *bio, struct block_device *bdev)\n{\n#if defined(HAVE_BIO_BDEV_DISK)\n\tstruct request_queue *q = bdev->bd_disk->queue;\n#else\n\tstruct request_queue *q = bio->bi_disk->queue;\n#endif\n\tbio_clear_flag(bio, BIO_REMAPPED);\n\tif (bio->bi_bdev != bdev)\n\t\tbio_clear_flag(bio, BIO_THROTTLED);\n\tbio->bi_bdev = bdev;\n\n\tASSERT3P(q, !=, NULL);\n\tASSERT3P(bio->bi_blkg, ==, NULL);\n\n\tif (q->root_blkg && vdev_blkg_tryget(q->root_blkg))\n\t\tbio->bi_blkg = q->root_blkg;\n}\n#define\tbio_set_dev\t\tvdev_bio_set_dev\n#endif\n#endif\n#else\n \nstatic inline void\nbio_set_dev(struct bio *bio, struct block_device *bdev)\n{\n\tbio->bi_bdev = bdev;\n}\n#endif  \n#endif  \n\nstatic inline void\nvdev_submit_bio(struct bio *bio)\n{\n\tstruct bio_list *bio_list = current->bio_list;\n\tcurrent->bio_list = NULL;\n\tvdev_submit_bio_impl(bio);\n\tcurrent->bio_list = bio_list;\n}\n\nstatic inline struct bio *\nvdev_bio_alloc(struct block_device *bdev, gfp_t gfp_mask,\n    unsigned short nr_vecs)\n{\n\tstruct bio *bio;\n\n#ifdef HAVE_BIO_ALLOC_4ARG\n\tbio = bio_alloc(bdev, nr_vecs, 0, gfp_mask);\n#else\n\tbio = bio_alloc(gfp_mask, nr_vecs);\n\tif (likely(bio != NULL))\n\t\tbio_set_dev(bio, bdev);\n#endif\n\n\treturn (bio);\n}\n\nstatic inline unsigned int\nvdev_bio_max_segs(zio_t *zio, int bio_size, uint64_t abd_offset)\n{\n\tunsigned long nr_segs = abd_nr_pages_off(zio->io_abd,\n\t    bio_size, abd_offset);\n\n#ifdef HAVE_BIO_MAX_SEGS\n\treturn (bio_max_segs(nr_segs));\n#else\n\treturn (MIN(nr_segs, BIO_MAX_PAGES));\n#endif\n}\n\nstatic int\n__vdev_disk_physio(struct block_device *bdev, zio_t *zio,\n    size_t io_size, uint64_t io_offset, int rw, int flags)\n{\n\tdio_request_t *dr;\n\tuint64_t abd_offset;\n\tuint64_t bio_offset;\n\tint bio_size;\n\tint bio_count = 16;\n\tint error = 0;\n\tstruct blk_plug plug;\n\tunsigned short nr_vecs;\n\n\t \n\tif (io_offset + io_size > bdev->bd_inode->i_size) {\n\t\tvdev_dbgmsg(zio->io_vd,\n\t\t    \"Illegal access %llu size %llu, device size %llu\",\n\t\t    (u_longlong_t)io_offset,\n\t\t    (u_longlong_t)io_size,\n\t\t    (u_longlong_t)i_size_read(bdev->bd_inode));\n\t\treturn (SET_ERROR(EIO));\n\t}\n\nretry:\n\tdr = vdev_disk_dio_alloc(bio_count);\n\n\tif (!(zio->io_flags & (ZIO_FLAG_IO_RETRY | ZIO_FLAG_TRYHARD)) &&\n\t    zio->io_vd->vdev_failfast == B_TRUE) {\n\t\tbio_set_flags_failfast(bdev, &flags, zfs_vdev_failfast_mask & 1,\n\t\t    zfs_vdev_failfast_mask & 2, zfs_vdev_failfast_mask & 4);\n\t}\n\n\tdr->dr_zio = zio;\n\n\t \n\n\tabd_offset = 0;\n\tbio_offset = io_offset;\n\tbio_size = io_size;\n\tfor (int i = 0; i <= dr->dr_bio_count; i++) {\n\n\t\t \n\t\tif (bio_size <= 0)\n\t\t\tbreak;\n\n\t\t \n\t\tif (dr->dr_bio_count == i) {\n\t\t\tvdev_disk_dio_free(dr);\n\t\t\tbio_count *= 2;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tnr_vecs = vdev_bio_max_segs(zio, bio_size, abd_offset);\n\t\tdr->dr_bio[i] = vdev_bio_alloc(bdev, GFP_NOIO, nr_vecs);\n\t\tif (unlikely(dr->dr_bio[i] == NULL)) {\n\t\t\tvdev_disk_dio_free(dr);\n\t\t\treturn (SET_ERROR(ENOMEM));\n\t\t}\n\n\t\t \n\t\tvdev_disk_dio_get(dr);\n\n\t\tBIO_BI_SECTOR(dr->dr_bio[i]) = bio_offset >> 9;\n\t\tdr->dr_bio[i]->bi_end_io = vdev_disk_physio_completion;\n\t\tdr->dr_bio[i]->bi_private = dr;\n\t\tbio_set_op_attrs(dr->dr_bio[i], rw, flags);\n\n\t\t \n\t\tbio_size = abd_bio_map_off(dr->dr_bio[i], zio->io_abd,\n\t\t    bio_size, abd_offset);\n\n\t\t \n\t\tabd_offset += BIO_BI_SIZE(dr->dr_bio[i]);\n\t\tbio_offset += BIO_BI_SIZE(dr->dr_bio[i]);\n\t}\n\n\t \n\tvdev_disk_dio_get(dr);\n\n\tif (dr->dr_bio_count > 1)\n\t\tblk_start_plug(&plug);\n\n\t \n\tfor (int i = 0; i < dr->dr_bio_count; i++) {\n\t\tif (dr->dr_bio[i])\n\t\t\tvdev_submit_bio(dr->dr_bio[i]);\n\t}\n\n\tif (dr->dr_bio_count > 1)\n\t\tblk_finish_plug(&plug);\n\n\tvdev_disk_dio_put(dr);\n\n\treturn (error);\n}\n\nBIO_END_IO_PROTO(vdev_disk_io_flush_completion, bio, error)\n{\n\tzio_t *zio = bio->bi_private;\n#ifdef HAVE_1ARG_BIO_END_IO_T\n\tzio->io_error = BIO_END_IO_ERROR(bio);\n#else\n\tzio->io_error = -error;\n#endif\n\n\tif (zio->io_error && (zio->io_error == EOPNOTSUPP))\n\t\tzio->io_vd->vdev_nowritecache = B_TRUE;\n\n\tbio_put(bio);\n\tASSERT3S(zio->io_error, >=, 0);\n\tif (zio->io_error)\n\t\tvdev_disk_error(zio);\n\tzio_interrupt(zio);\n}\n\nstatic int\nvdev_disk_io_flush(struct block_device *bdev, zio_t *zio)\n{\n\tstruct request_queue *q;\n\tstruct bio *bio;\n\n\tq = bdev_get_queue(bdev);\n\tif (!q)\n\t\treturn (SET_ERROR(ENXIO));\n\n\tbio = vdev_bio_alloc(bdev, GFP_NOIO, 0);\n\tif (unlikely(bio == NULL))\n\t\treturn (SET_ERROR(ENOMEM));\n\n\tbio->bi_end_io = vdev_disk_io_flush_completion;\n\tbio->bi_private = zio;\n\tbio_set_flush(bio);\n\tvdev_submit_bio(bio);\n\tinvalidate_bdev(bdev);\n\n\treturn (0);\n}\n\nstatic int\nvdev_disk_io_trim(zio_t *zio)\n{\n\tvdev_t *v = zio->io_vd;\n\tvdev_disk_t *vd = v->vdev_tsd;\n\n#if defined(HAVE_BLKDEV_ISSUE_SECURE_ERASE)\n\tif (zio->io_trim_flags & ZIO_TRIM_SECURE) {\n\t\treturn (-blkdev_issue_secure_erase(vd->vd_bdev,\n\t\t    zio->io_offset >> 9, zio->io_size >> 9, GFP_NOFS));\n\t} else {\n\t\treturn (-blkdev_issue_discard(vd->vd_bdev,\n\t\t    zio->io_offset >> 9, zio->io_size >> 9, GFP_NOFS));\n\t}\n#elif defined(HAVE_BLKDEV_ISSUE_DISCARD)\n\tunsigned long trim_flags = 0;\n#if defined(BLKDEV_DISCARD_SECURE)\n\tif (zio->io_trim_flags & ZIO_TRIM_SECURE)\n\t\ttrim_flags |= BLKDEV_DISCARD_SECURE;\n#endif\n\treturn (-blkdev_issue_discard(vd->vd_bdev,\n\t    zio->io_offset >> 9, zio->io_size >> 9, GFP_NOFS, trim_flags));\n#else\n#error \"Unsupported kernel\"\n#endif\n}\n\nstatic void\nvdev_disk_io_start(zio_t *zio)\n{\n\tvdev_t *v = zio->io_vd;\n\tvdev_disk_t *vd = v->vdev_tsd;\n\tint rw, error;\n\n\t \n\tif (vd == NULL) {\n\t\tzio->io_error = ENXIO;\n\t\tzio_interrupt(zio);\n\t\treturn;\n\t}\n\n\trw_enter(&vd->vd_lock, RW_READER);\n\n\t \n\tif (vd->vd_bdev == NULL) {\n\t\trw_exit(&vd->vd_lock);\n\t\tzio->io_error = ENXIO;\n\t\tzio_interrupt(zio);\n\t\treturn;\n\t}\n\n\tswitch (zio->io_type) {\n\tcase ZIO_TYPE_IOCTL:\n\n\t\tif (!vdev_readable(v)) {\n\t\t\trw_exit(&vd->vd_lock);\n\t\t\tzio->io_error = SET_ERROR(ENXIO);\n\t\t\tzio_interrupt(zio);\n\t\t\treturn;\n\t\t}\n\n\t\tswitch (zio->io_cmd) {\n\t\tcase DKIOCFLUSHWRITECACHE:\n\n\t\t\tif (zfs_nocacheflush)\n\t\t\t\tbreak;\n\n\t\t\tif (v->vdev_nowritecache) {\n\t\t\t\tzio->io_error = SET_ERROR(ENOTSUP);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terror = vdev_disk_io_flush(vd->vd_bdev, zio);\n\t\t\tif (error == 0) {\n\t\t\t\trw_exit(&vd->vd_lock);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tzio->io_error = error;\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tzio->io_error = SET_ERROR(ENOTSUP);\n\t\t}\n\n\t\trw_exit(&vd->vd_lock);\n\t\tzio_execute(zio);\n\t\treturn;\n\tcase ZIO_TYPE_WRITE:\n\t\trw = WRITE;\n\t\tbreak;\n\n\tcase ZIO_TYPE_READ:\n\t\trw = READ;\n\t\tbreak;\n\n\tcase ZIO_TYPE_TRIM:\n\t\tzio->io_error = vdev_disk_io_trim(zio);\n\t\trw_exit(&vd->vd_lock);\n\t\tzio_interrupt(zio);\n\t\treturn;\n\n\tdefault:\n\t\trw_exit(&vd->vd_lock);\n\t\tzio->io_error = SET_ERROR(ENOTSUP);\n\t\tzio_interrupt(zio);\n\t\treturn;\n\t}\n\n\tzio->io_target_timestamp = zio_handle_io_delay(zio);\n\terror = __vdev_disk_physio(vd->vd_bdev, zio,\n\t    zio->io_size, zio->io_offset, rw, 0);\n\trw_exit(&vd->vd_lock);\n\n\tif (error) {\n\t\tzio->io_error = error;\n\t\tzio_interrupt(zio);\n\t\treturn;\n\t}\n}\n\nstatic void\nvdev_disk_io_done(zio_t *zio)\n{\n\t \n\tif (zio->io_error == EIO) {\n\t\tvdev_t *v = zio->io_vd;\n\t\tvdev_disk_t *vd = v->vdev_tsd;\n\n\t\tif (!zfs_check_disk_status(vd->vd_bdev)) {\n\t\t\tinvalidate_bdev(vd->vd_bdev);\n\t\t\tv->vdev_remove_wanted = B_TRUE;\n\t\t\tspa_async_request(zio->io_spa, SPA_ASYNC_REMOVE);\n\t\t}\n\t}\n}\n\nstatic void\nvdev_disk_hold(vdev_t *vd)\n{\n\tASSERT(spa_config_held(vd->vdev_spa, SCL_STATE, RW_WRITER));\n\n\t \n\tif (vd->vdev_path == NULL || vd->vdev_path[0] != '/')\n\t\treturn;\n\n\t \n\tif (vd->vdev_tsd != NULL)\n\t\treturn;\n\n}\n\nstatic void\nvdev_disk_rele(vdev_t *vd)\n{\n\tASSERT(spa_config_held(vd->vdev_spa, SCL_STATE, RW_WRITER));\n\n\t \n}\n\nvdev_ops_t vdev_disk_ops = {\n\t.vdev_op_init = NULL,\n\t.vdev_op_fini = NULL,\n\t.vdev_op_open = vdev_disk_open,\n\t.vdev_op_close = vdev_disk_close,\n\t.vdev_op_asize = vdev_default_asize,\n\t.vdev_op_min_asize = vdev_default_min_asize,\n\t.vdev_op_min_alloc = NULL,\n\t.vdev_op_io_start = vdev_disk_io_start,\n\t.vdev_op_io_done = vdev_disk_io_done,\n\t.vdev_op_state_change = NULL,\n\t.vdev_op_need_resilver = NULL,\n\t.vdev_op_hold = vdev_disk_hold,\n\t.vdev_op_rele = vdev_disk_rele,\n\t.vdev_op_remap = NULL,\n\t.vdev_op_xlate = vdev_default_xlate,\n\t.vdev_op_rebuild_asize = NULL,\n\t.vdev_op_metaslab_init = NULL,\n\t.vdev_op_config_generate = NULL,\n\t.vdev_op_nparity = NULL,\n\t.vdev_op_ndisks = NULL,\n\t.vdev_op_type = VDEV_TYPE_DISK,\t\t \n\t.vdev_op_leaf = B_TRUE,\t\t\t \n\t.vdev_op_kobj_evt_post = vdev_disk_kobj_evt_post\n};\n\n \nstatic int\nparam_set_vdev_scheduler(const char *val, zfs_kernel_param_t *kp)\n{\n\tint error = param_set_charp(val, kp);\n\tif (error == 0) {\n\t\tprintk(KERN_INFO \"The 'zfs_vdev_scheduler' module option \"\n\t\t    \"is not supported.\\n\");\n\t}\n\n\treturn (error);\n}\n\nstatic const char *zfs_vdev_scheduler = \"unused\";\nmodule_param_call(zfs_vdev_scheduler, param_set_vdev_scheduler,\n    param_get_charp, &zfs_vdev_scheduler, 0644);\nMODULE_PARM_DESC(zfs_vdev_scheduler, \"I/O scheduler\");\n\nint\nparam_set_min_auto_ashift(const char *buf, zfs_kernel_param_t *kp)\n{\n\tuint_t val;\n\tint error;\n\n\terror = kstrtouint(buf, 0, &val);\n\tif (error < 0)\n\t\treturn (SET_ERROR(error));\n\n\tif (val < ASHIFT_MIN || val > zfs_vdev_max_auto_ashift)\n\t\treturn (SET_ERROR(-EINVAL));\n\n\terror = param_set_uint(buf, kp);\n\tif (error < 0)\n\t\treturn (SET_ERROR(error));\n\n\treturn (0);\n}\n\nint\nparam_set_max_auto_ashift(const char *buf, zfs_kernel_param_t *kp)\n{\n\tuint_t val;\n\tint error;\n\n\terror = kstrtouint(buf, 0, &val);\n\tif (error < 0)\n\t\treturn (SET_ERROR(error));\n\n\tif (val > ASHIFT_MAX || val < zfs_vdev_min_auto_ashift)\n\t\treturn (SET_ERROR(-EINVAL));\n\n\terror = param_set_uint(buf, kp);\n\tif (error < 0)\n\t\treturn (SET_ERROR(error));\n\n\treturn (0);\n}\n\nZFS_MODULE_PARAM(zfs_vdev, zfs_vdev_, open_timeout_ms, UINT, ZMOD_RW,\n\t\"Timeout before determining that a device is missing\");\n\nZFS_MODULE_PARAM(zfs_vdev, zfs_vdev_, failfast_mask, UINT, ZMOD_RW,\n\t\"Defines failfast mask: 1 - device, 2 - transport, 4 - driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}