{
  "module_name": "aesni-gcm-x86_64.S",
  "hash_id": "19514b3ddec7f7a32b8d9416f2db046f1583e0ce801a00afaf9a872c7052145b",
  "original_prompt": "Ingested from zfs-2.2.2/module/icp/asm-x86_64/modes/aesni-gcm-x86_64.S",
  "human_readable_source": "# Copyright 2013-2016 The OpenSSL Project Authors. All Rights Reserved.\n#\n# Licensed under the Apache License 2.0 (the \"License\").  You may not use\n# this file except in compliance with the License.  You can obtain a copy\n# in the file LICENSE in the source distribution or at\n# https: \n\n#\n# ====================================================================\n# Written by Andy Polyakov <appro@openssl.org> for the OpenSSL\n# project. The module is, however, dual licensed under OpenSSL and\n# CRYPTOGAMS licenses depending on where you obtain it. For further\n# details see http: \n# ====================================================================\n#\n#\n# AES-NI-CTR+GHASH stitch.\n#\n# February 2013\n#\n# OpenSSL GCM implementation is organized in such way that its\n# performance is rather close to the sum of its streamed components,\n# in the context parallelized AES-NI CTR and modulo-scheduled\n# PCLMULQDQ-enabled GHASH. Unfortunately, as no stitch implementation\n# was observed to perform significantly better than the sum of the\n# components on contemporary CPUs, the effort was deemed impossible to\n# justify. This module is based on combination of Intel submissions,\n# [1] and [2], with MOVBE twist suggested by Ilya Albrekht and Max\n# Locktyukhin of Intel Corp. who verified that it reduces shuffles\n# pressure with notable relative improvement, achieving 1.0 cycle per\n# byte processed with 128-bit key on Haswell processor, 0.74 - on\n# Broadwell, 0.63 - on Skylake... [Mentioned results are raw profiled\n# measurements for favourable packet size, one divisible by 96.\n# Applications using the EVP interface will observe a few percent\n# worse performance.]\n#\n# Knights Landing processes 1 byte in 1.25 cycles (measured with EVP).\n#\n# [1] http: \n# [2] http: \n\n# Generated once from\n# https: \n# and modified for ICP. Modification are kept at a bare minimum to ease later\n# upstream merges.\n\n#if defined(__x86_64__) && defined(HAVE_AVX) && \\\n    defined(HAVE_AES) && defined(HAVE_PCLMULQDQ)\n\n#define _ASM\n#include <sys/asm_linkage.h>\n\n \n#if !defined (_WIN32) || defined (_KERNEL)\n\n \n#if defined (__APPLE__)\n#define\tgcm_avx_can_use_movbe _gcm_avx_can_use_movbe\n#endif\n\n.extern gcm_avx_can_use_movbe\n\n.text\n\n#ifdef HAVE_MOVBE\n.balign 32\nFUNCTION(_aesni_ctr32_ghash_6x)\n.cfi_startproc\n\tENDBR\n\tvmovdqu\t32(%r11),%xmm2\n\tsubq\t$6,%rdx\n\tvpxor\t%xmm4,%xmm4,%xmm4\n\tvmovdqu\t0-128(%rcx),%xmm15\n\tvpaddb\t%xmm2,%xmm1,%xmm10\n\tvpaddb\t%xmm2,%xmm10,%xmm11\n\tvpaddb\t%xmm2,%xmm11,%xmm12\n\tvpaddb\t%xmm2,%xmm12,%xmm13\n\tvpaddb\t%xmm2,%xmm13,%xmm14\n\tvpxor\t%xmm15,%xmm1,%xmm9\n\tvmovdqu\t%xmm4,16+8(%rsp)\n\tjmp\t.Loop6x\n\n.balign\t32\n.Loop6x:\n\taddl\t$100663296,%ebx\n\tjc\t.Lhandle_ctr32\n\tvmovdqu\t0-32(%r9),%xmm3\n\tvpaddb\t%xmm2,%xmm14,%xmm1\n\tvpxor\t%xmm15,%xmm10,%xmm10\n\tvpxor\t%xmm15,%xmm11,%xmm11\n\n.Lresume_ctr32:\n\tvmovdqu\t%xmm1,(%r8)\n\tvpclmulqdq\t$0x10,%xmm3,%xmm7,%xmm5\n\tvpxor\t%xmm15,%xmm12,%xmm12\n\tvmovups\t16-128(%rcx),%xmm2\n\tvpclmulqdq\t$0x01,%xmm3,%xmm7,%xmm6\n\txorq\t%r12,%r12\n\tcmpq\t%r14,%r15\n\n\tvaesenc\t%xmm2,%xmm9,%xmm9\n\tvmovdqu\t48+8(%rsp),%xmm0\n\tvpxor\t%xmm15,%xmm13,%xmm13\n\tvpclmulqdq\t$0x00,%xmm3,%xmm7,%xmm1\n\tvaesenc\t%xmm2,%xmm10,%xmm10\n\tvpxor\t%xmm15,%xmm14,%xmm14\n\tsetnc\t%r12b\n\tvpclmulqdq\t$0x11,%xmm3,%xmm7,%xmm7\n\tvaesenc\t%xmm2,%xmm11,%xmm11\n\tvmovdqu\t16-32(%r9),%xmm3\n\tnegq\t%r12\n\tvaesenc\t%xmm2,%xmm12,%xmm12\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm3,%xmm0,%xmm5\n\tvpxor\t%xmm4,%xmm8,%xmm8\n\tvaesenc\t%xmm2,%xmm13,%xmm13\n\tvpxor\t%xmm5,%xmm1,%xmm4\n\tandq\t$0x60,%r12\n\tvmovups\t32-128(%rcx),%xmm15\n\tvpclmulqdq\t$0x10,%xmm3,%xmm0,%xmm1\n\tvaesenc\t%xmm2,%xmm14,%xmm14\n\n\tvpclmulqdq\t$0x01,%xmm3,%xmm0,%xmm2\n\tleaq\t(%r14,%r12,1),%r14\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t16+8(%rsp),%xmm8,%xmm8\n\tvpclmulqdq\t$0x11,%xmm3,%xmm0,%xmm3\n\tvmovdqu\t64+8(%rsp),%xmm0\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovbeq\t88(%r14),%r13\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovbeq\t80(%r14),%r12\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,32+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,40+8(%rsp)\n\tvmovdqu\t48-32(%r9),%xmm5\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t48-128(%rcx),%xmm15\n\tvpxor\t%xmm1,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm5,%xmm0,%xmm1\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm2,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm5,%xmm0,%xmm2\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpxor\t%xmm3,%xmm7,%xmm7\n\tvpclmulqdq\t$0x01,%xmm5,%xmm0,%xmm3\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvpclmulqdq\t$0x11,%xmm5,%xmm0,%xmm5\n\tvmovdqu\t80+8(%rsp),%xmm0\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvmovdqu\t64-32(%r9),%xmm1\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t64-128(%rcx),%xmm15\n\tvpxor\t%xmm2,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm1,%xmm0,%xmm2\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm3,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm1,%xmm0,%xmm3\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovbeq\t72(%r14),%r13\n\tvpxor\t%xmm5,%xmm7,%xmm7\n\tvpclmulqdq\t$0x01,%xmm1,%xmm0,%xmm5\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovbeq\t64(%r14),%r12\n\tvpclmulqdq\t$0x11,%xmm1,%xmm0,%xmm1\n\tvmovdqu\t96+8(%rsp),%xmm0\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,48+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,56+8(%rsp)\n\tvpxor\t%xmm2,%xmm4,%xmm4\n\tvmovdqu\t96-32(%r9),%xmm2\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t80-128(%rcx),%xmm15\n\tvpxor\t%xmm3,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm2,%xmm0,%xmm3\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm2,%xmm0,%xmm5\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovbeq\t56(%r14),%r13\n\tvpxor\t%xmm1,%xmm7,%xmm7\n\tvpclmulqdq\t$0x01,%xmm2,%xmm0,%xmm1\n\tvpxor\t112+8(%rsp),%xmm8,%xmm8\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovbeq\t48(%r14),%r12\n\tvpclmulqdq\t$0x11,%xmm2,%xmm0,%xmm2\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,64+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,72+8(%rsp)\n\tvpxor\t%xmm3,%xmm4,%xmm4\n\tvmovdqu\t112-32(%r9),%xmm3\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t96-128(%rcx),%xmm15\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm3,%xmm8,%xmm5\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm1,%xmm6,%xmm6\n\tvpclmulqdq\t$0x01,%xmm3,%xmm8,%xmm1\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovbeq\t40(%r14),%r13\n\tvpxor\t%xmm2,%xmm7,%xmm7\n\tvpclmulqdq\t$0x00,%xmm3,%xmm8,%xmm2\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovbeq\t32(%r14),%r12\n\tvpclmulqdq\t$0x11,%xmm3,%xmm8,%xmm8\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,80+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,88+8(%rsp)\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\tvpxor\t%xmm1,%xmm6,%xmm6\n\n\tvmovups\t112-128(%rcx),%xmm15\n\tvpslldq\t$8,%xmm6,%xmm5\n\tvpxor\t%xmm2,%xmm4,%xmm4\n\tvmovdqu\t16(%r11),%xmm3\n\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm8,%xmm7,%xmm7\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpxor\t%xmm5,%xmm4,%xmm4\n\tmovbeq\t24(%r14),%r13\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovbeq\t16(%r14),%r12\n\tvpalignr\t$8,%xmm4,%xmm4,%xmm0\n\tvpclmulqdq\t$0x10,%xmm3,%xmm4,%xmm4\n\tmovq\t%r13,96+8(%rsp)\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r12,104+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvmovups\t128-128(%rcx),%xmm1\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvaesenc\t%xmm1,%xmm9,%xmm9\n\tvmovups\t144-128(%rcx),%xmm15\n\tvaesenc\t%xmm1,%xmm10,%xmm10\n\tvpsrldq\t$8,%xmm6,%xmm6\n\tvaesenc\t%xmm1,%xmm11,%xmm11\n\tvpxor\t%xmm6,%xmm7,%xmm7\n\tvaesenc\t%xmm1,%xmm12,%xmm12\n\tvpxor\t%xmm0,%xmm4,%xmm4\n\tmovbeq\t8(%r14),%r13\n\tvaesenc\t%xmm1,%xmm13,%xmm13\n\tmovbeq\t0(%r14),%r12\n\tvaesenc\t%xmm1,%xmm14,%xmm14\n\tvmovups\t160-128(%rcx),%xmm1\n\tcmpl\t$12,%ebp\t \n\tjb\t.Lenc_tail\n\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvaesenc\t%xmm1,%xmm9,%xmm9\n\tvaesenc\t%xmm1,%xmm10,%xmm10\n\tvaesenc\t%xmm1,%xmm11,%xmm11\n\tvaesenc\t%xmm1,%xmm12,%xmm12\n\tvaesenc\t%xmm1,%xmm13,%xmm13\n\tvmovups\t176-128(%rcx),%xmm15\n\tvaesenc\t%xmm1,%xmm14,%xmm14\n\tvmovups\t192-128(%rcx),%xmm1\n\tcmpl\t$14,%ebp\t \n\tjb\t.Lenc_tail\n\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvaesenc\t%xmm1,%xmm9,%xmm9\n\tvaesenc\t%xmm1,%xmm10,%xmm10\n\tvaesenc\t%xmm1,%xmm11,%xmm11\n\tvaesenc\t%xmm1,%xmm12,%xmm12\n\tvaesenc\t%xmm1,%xmm13,%xmm13\n\tvmovups\t208-128(%rcx),%xmm15\n\tvaesenc\t%xmm1,%xmm14,%xmm14\n\tvmovups\t224-128(%rcx),%xmm1\n\tjmp\t.Lenc_tail\n\n.balign\t32\n.Lhandle_ctr32:\n\tvmovdqu\t(%r11),%xmm0\n\tvpshufb\t%xmm0,%xmm1,%xmm6\n\tvmovdqu\t48(%r11),%xmm5\n\tvpaddd\t64(%r11),%xmm6,%xmm10\n\tvpaddd\t%xmm5,%xmm6,%xmm11\n\tvmovdqu\t0-32(%r9),%xmm3\n\tvpaddd\t%xmm5,%xmm10,%xmm12\n\tvpshufb\t%xmm0,%xmm10,%xmm10\n\tvpaddd\t%xmm5,%xmm11,%xmm13\n\tvpshufb\t%xmm0,%xmm11,%xmm11\n\tvpxor\t%xmm15,%xmm10,%xmm10\n\tvpaddd\t%xmm5,%xmm12,%xmm14\n\tvpshufb\t%xmm0,%xmm12,%xmm12\n\tvpxor\t%xmm15,%xmm11,%xmm11\n\tvpaddd\t%xmm5,%xmm13,%xmm1\n\tvpshufb\t%xmm0,%xmm13,%xmm13\n\tvpshufb\t%xmm0,%xmm14,%xmm14\n\tvpshufb\t%xmm0,%xmm1,%xmm1\n\tjmp\t.Lresume_ctr32\n\n.balign\t32\n.Lenc_tail:\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvmovdqu\t%xmm7,16+8(%rsp)\n\tvpalignr\t$8,%xmm4,%xmm4,%xmm8\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpclmulqdq\t$0x10,%xmm3,%xmm4,%xmm4\n\tvpxor\t0(%rdi),%xmm1,%xmm2\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvpxor\t16(%rdi),%xmm1,%xmm0\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvpxor\t32(%rdi),%xmm1,%xmm5\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvpxor\t48(%rdi),%xmm1,%xmm6\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\tvpxor\t64(%rdi),%xmm1,%xmm7\n\tvpxor\t80(%rdi),%xmm1,%xmm3\n\tvmovdqu\t(%r8),%xmm1\n\n\tvaesenclast\t%xmm2,%xmm9,%xmm9\n\tvmovdqu\t32(%r11),%xmm2\n\tvaesenclast\t%xmm0,%xmm10,%xmm10\n\tvpaddb\t%xmm2,%xmm1,%xmm0\n\tmovq\t%r13,112+8(%rsp)\n\tleaq\t96(%rdi),%rdi\n\tvaesenclast\t%xmm5,%xmm11,%xmm11\n\tvpaddb\t%xmm2,%xmm0,%xmm5\n\tmovq\t%r12,120+8(%rsp)\n\tleaq\t96(%rsi),%rsi\n\tvmovdqu\t0-128(%rcx),%xmm15\n\tvaesenclast\t%xmm6,%xmm12,%xmm12\n\tvpaddb\t%xmm2,%xmm5,%xmm6\n\tvaesenclast\t%xmm7,%xmm13,%xmm13\n\tvpaddb\t%xmm2,%xmm6,%xmm7\n\tvaesenclast\t%xmm3,%xmm14,%xmm14\n\tvpaddb\t%xmm2,%xmm7,%xmm3\n\n\taddq\t$0x60,%r10\n\tsubq\t$0x6,%rdx\n\tjc\t.L6x_done\n\n\tvmovups\t%xmm9,-96(%rsi)\n\tvpxor\t%xmm15,%xmm1,%xmm9\n\tvmovups\t%xmm10,-80(%rsi)\n\tvmovdqa\t%xmm0,%xmm10\n\tvmovups\t%xmm11,-64(%rsi)\n\tvmovdqa\t%xmm5,%xmm11\n\tvmovups\t%xmm12,-48(%rsi)\n\tvmovdqa\t%xmm6,%xmm12\n\tvmovups\t%xmm13,-32(%rsi)\n\tvmovdqa\t%xmm7,%xmm13\n\tvmovups\t%xmm14,-16(%rsi)\n\tvmovdqa\t%xmm3,%xmm14\n\tvmovdqu\t32+8(%rsp),%xmm7\n\tjmp\t.Loop6x\n\n.L6x_done:\n\tvpxor\t16+8(%rsp),%xmm8,%xmm8\n\tvpxor\t%xmm4,%xmm8,%xmm8\n\n\tRET\n.cfi_endproc\nSET_SIZE(_aesni_ctr32_ghash_6x)\n#endif  \n\n.balign 32\nFUNCTION(_aesni_ctr32_ghash_no_movbe_6x)\n.cfi_startproc\n\tENDBR\n\tvmovdqu\t32(%r11),%xmm2\n\tsubq\t$6,%rdx\n\tvpxor\t%xmm4,%xmm4,%xmm4\n\tvmovdqu\t0-128(%rcx),%xmm15\n\tvpaddb\t%xmm2,%xmm1,%xmm10\n\tvpaddb\t%xmm2,%xmm10,%xmm11\n\tvpaddb\t%xmm2,%xmm11,%xmm12\n\tvpaddb\t%xmm2,%xmm12,%xmm13\n\tvpaddb\t%xmm2,%xmm13,%xmm14\n\tvpxor\t%xmm15,%xmm1,%xmm9\n\tvmovdqu\t%xmm4,16+8(%rsp)\n\tjmp\t.Loop6x_nmb\n\n.balign\t32\n.Loop6x_nmb:\n\taddl\t$100663296,%ebx\n\tjc\t.Lhandle_ctr32_nmb\n\tvmovdqu\t0-32(%r9),%xmm3\n\tvpaddb\t%xmm2,%xmm14,%xmm1\n\tvpxor\t%xmm15,%xmm10,%xmm10\n\tvpxor\t%xmm15,%xmm11,%xmm11\n\n.Lresume_ctr32_nmb:\n\tvmovdqu\t%xmm1,(%r8)\n\tvpclmulqdq\t$0x10,%xmm3,%xmm7,%xmm5\n\tvpxor\t%xmm15,%xmm12,%xmm12\n\tvmovups\t16-128(%rcx),%xmm2\n\tvpclmulqdq\t$0x01,%xmm3,%xmm7,%xmm6\n\txorq\t%r12,%r12\n\tcmpq\t%r14,%r15\n\n\tvaesenc\t%xmm2,%xmm9,%xmm9\n\tvmovdqu\t48+8(%rsp),%xmm0\n\tvpxor\t%xmm15,%xmm13,%xmm13\n\tvpclmulqdq\t$0x00,%xmm3,%xmm7,%xmm1\n\tvaesenc\t%xmm2,%xmm10,%xmm10\n\tvpxor\t%xmm15,%xmm14,%xmm14\n\tsetnc\t%r12b\n\tvpclmulqdq\t$0x11,%xmm3,%xmm7,%xmm7\n\tvaesenc\t%xmm2,%xmm11,%xmm11\n\tvmovdqu\t16-32(%r9),%xmm3\n\tnegq\t%r12\n\tvaesenc\t%xmm2,%xmm12,%xmm12\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm3,%xmm0,%xmm5\n\tvpxor\t%xmm4,%xmm8,%xmm8\n\tvaesenc\t%xmm2,%xmm13,%xmm13\n\tvpxor\t%xmm5,%xmm1,%xmm4\n\tandq\t$0x60,%r12\n\tvmovups\t32-128(%rcx),%xmm15\n\tvpclmulqdq\t$0x10,%xmm3,%xmm0,%xmm1\n\tvaesenc\t%xmm2,%xmm14,%xmm14\n\n\tvpclmulqdq\t$0x01,%xmm3,%xmm0,%xmm2\n\tleaq\t(%r14,%r12,1),%r14\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t16+8(%rsp),%xmm8,%xmm8\n\tvpclmulqdq\t$0x11,%xmm3,%xmm0,%xmm3\n\tvmovdqu\t64+8(%rsp),%xmm0\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovq\t88(%r14),%r13\n\tbswapq\t%r13\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovq\t80(%r14),%r12\n\tbswapq\t%r12\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,32+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,40+8(%rsp)\n\tvmovdqu\t48-32(%r9),%xmm5\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t48-128(%rcx),%xmm15\n\tvpxor\t%xmm1,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm5,%xmm0,%xmm1\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm2,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm5,%xmm0,%xmm2\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpxor\t%xmm3,%xmm7,%xmm7\n\tvpclmulqdq\t$0x01,%xmm5,%xmm0,%xmm3\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvpclmulqdq\t$0x11,%xmm5,%xmm0,%xmm5\n\tvmovdqu\t80+8(%rsp),%xmm0\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvmovdqu\t64-32(%r9),%xmm1\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t64-128(%rcx),%xmm15\n\tvpxor\t%xmm2,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm1,%xmm0,%xmm2\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm3,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm1,%xmm0,%xmm3\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovq\t72(%r14),%r13\n\tbswapq\t%r13\n\tvpxor\t%xmm5,%xmm7,%xmm7\n\tvpclmulqdq\t$0x01,%xmm1,%xmm0,%xmm5\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovq\t64(%r14),%r12\n\tbswapq\t%r12\n\tvpclmulqdq\t$0x11,%xmm1,%xmm0,%xmm1\n\tvmovdqu\t96+8(%rsp),%xmm0\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,48+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,56+8(%rsp)\n\tvpxor\t%xmm2,%xmm4,%xmm4\n\tvmovdqu\t96-32(%r9),%xmm2\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t80-128(%rcx),%xmm15\n\tvpxor\t%xmm3,%xmm6,%xmm6\n\tvpclmulqdq\t$0x00,%xmm2,%xmm0,%xmm3\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm2,%xmm0,%xmm5\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovq\t56(%r14),%r13\n\tbswapq\t%r13\n\tvpxor\t%xmm1,%xmm7,%xmm7\n\tvpclmulqdq\t$0x01,%xmm2,%xmm0,%xmm1\n\tvpxor\t112+8(%rsp),%xmm8,%xmm8\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovq\t48(%r14),%r12\n\tbswapq\t%r12\n\tvpclmulqdq\t$0x11,%xmm2,%xmm0,%xmm2\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,64+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,72+8(%rsp)\n\tvpxor\t%xmm3,%xmm4,%xmm4\n\tvmovdqu\t112-32(%r9),%xmm3\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvmovups\t96-128(%rcx),%xmm15\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm3,%xmm8,%xmm5\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm1,%xmm6,%xmm6\n\tvpclmulqdq\t$0x01,%xmm3,%xmm8,%xmm1\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tmovq\t40(%r14),%r13\n\tbswapq\t%r13\n\tvpxor\t%xmm2,%xmm7,%xmm7\n\tvpclmulqdq\t$0x00,%xmm3,%xmm8,%xmm2\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovq\t32(%r14),%r12\n\tbswapq\t%r12\n\tvpclmulqdq\t$0x11,%xmm3,%xmm8,%xmm8\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r13,80+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tmovq\t%r12,88+8(%rsp)\n\tvpxor\t%xmm5,%xmm6,%xmm6\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\tvpxor\t%xmm1,%xmm6,%xmm6\n\n\tvmovups\t112-128(%rcx),%xmm15\n\tvpslldq\t$8,%xmm6,%xmm5\n\tvpxor\t%xmm2,%xmm4,%xmm4\n\tvmovdqu\t16(%r11),%xmm3\n\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm8,%xmm7,%xmm7\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpxor\t%xmm5,%xmm4,%xmm4\n\tmovq\t24(%r14),%r13\n\tbswapq\t%r13\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tmovq\t16(%r14),%r12\n\tbswapq\t%r12\n\tvpalignr\t$8,%xmm4,%xmm4,%xmm0\n\tvpclmulqdq\t$0x10,%xmm3,%xmm4,%xmm4\n\tmovq\t%r13,96+8(%rsp)\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tmovq\t%r12,104+8(%rsp)\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvmovups\t128-128(%rcx),%xmm1\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvaesenc\t%xmm1,%xmm9,%xmm9\n\tvmovups\t144-128(%rcx),%xmm15\n\tvaesenc\t%xmm1,%xmm10,%xmm10\n\tvpsrldq\t$8,%xmm6,%xmm6\n\tvaesenc\t%xmm1,%xmm11,%xmm11\n\tvpxor\t%xmm6,%xmm7,%xmm7\n\tvaesenc\t%xmm1,%xmm12,%xmm12\n\tvpxor\t%xmm0,%xmm4,%xmm4\n\tmovq\t8(%r14),%r13\n\tbswapq\t%r13\n\tvaesenc\t%xmm1,%xmm13,%xmm13\n\tmovq\t0(%r14),%r12\n\tbswapq\t%r12\n\tvaesenc\t%xmm1,%xmm14,%xmm14\n\tvmovups\t160-128(%rcx),%xmm1\n\tcmpl\t$12,%ebp\t \n\tjb\t.Lenc_tail_nmb\n\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvaesenc\t%xmm1,%xmm9,%xmm9\n\tvaesenc\t%xmm1,%xmm10,%xmm10\n\tvaesenc\t%xmm1,%xmm11,%xmm11\n\tvaesenc\t%xmm1,%xmm12,%xmm12\n\tvaesenc\t%xmm1,%xmm13,%xmm13\n\tvmovups\t176-128(%rcx),%xmm15\n\tvaesenc\t%xmm1,%xmm14,%xmm14\n\tvmovups\t192-128(%rcx),%xmm1\n\tcmpl\t$14,%ebp\t \n\tjb\t.Lenc_tail_nmb\n\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\n\tvaesenc\t%xmm1,%xmm9,%xmm9\n\tvaesenc\t%xmm1,%xmm10,%xmm10\n\tvaesenc\t%xmm1,%xmm11,%xmm11\n\tvaesenc\t%xmm1,%xmm12,%xmm12\n\tvaesenc\t%xmm1,%xmm13,%xmm13\n\tvmovups\t208-128(%rcx),%xmm15\n\tvaesenc\t%xmm1,%xmm14,%xmm14\n\tvmovups\t224-128(%rcx),%xmm1\n\tjmp\t.Lenc_tail_nmb\n\n.balign\t32\n.Lhandle_ctr32_nmb:\n\tvmovdqu\t(%r11),%xmm0\n\tvpshufb\t%xmm0,%xmm1,%xmm6\n\tvmovdqu\t48(%r11),%xmm5\n\tvpaddd\t64(%r11),%xmm6,%xmm10\n\tvpaddd\t%xmm5,%xmm6,%xmm11\n\tvmovdqu\t0-32(%r9),%xmm3\n\tvpaddd\t%xmm5,%xmm10,%xmm12\n\tvpshufb\t%xmm0,%xmm10,%xmm10\n\tvpaddd\t%xmm5,%xmm11,%xmm13\n\tvpshufb\t%xmm0,%xmm11,%xmm11\n\tvpxor\t%xmm15,%xmm10,%xmm10\n\tvpaddd\t%xmm5,%xmm12,%xmm14\n\tvpshufb\t%xmm0,%xmm12,%xmm12\n\tvpxor\t%xmm15,%xmm11,%xmm11\n\tvpaddd\t%xmm5,%xmm13,%xmm1\n\tvpshufb\t%xmm0,%xmm13,%xmm13\n\tvpshufb\t%xmm0,%xmm14,%xmm14\n\tvpshufb\t%xmm0,%xmm1,%xmm1\n\tjmp\t.Lresume_ctr32_nmb\n\n.balign\t32\n.Lenc_tail_nmb:\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvmovdqu\t%xmm7,16+8(%rsp)\n\tvpalignr\t$8,%xmm4,%xmm4,%xmm8\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpclmulqdq\t$0x10,%xmm3,%xmm4,%xmm4\n\tvpxor\t0(%rdi),%xmm1,%xmm2\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvpxor\t16(%rdi),%xmm1,%xmm0\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvpxor\t32(%rdi),%xmm1,%xmm5\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvpxor\t48(%rdi),%xmm1,%xmm6\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\tvpxor\t64(%rdi),%xmm1,%xmm7\n\tvpxor\t80(%rdi),%xmm1,%xmm3\n\tvmovdqu\t(%r8),%xmm1\n\n\tvaesenclast\t%xmm2,%xmm9,%xmm9\n\tvmovdqu\t32(%r11),%xmm2\n\tvaesenclast\t%xmm0,%xmm10,%xmm10\n\tvpaddb\t%xmm2,%xmm1,%xmm0\n\tmovq\t%r13,112+8(%rsp)\n\tleaq\t96(%rdi),%rdi\n\tvaesenclast\t%xmm5,%xmm11,%xmm11\n\tvpaddb\t%xmm2,%xmm0,%xmm5\n\tmovq\t%r12,120+8(%rsp)\n\tleaq\t96(%rsi),%rsi\n\tvmovdqu\t0-128(%rcx),%xmm15\n\tvaesenclast\t%xmm6,%xmm12,%xmm12\n\tvpaddb\t%xmm2,%xmm5,%xmm6\n\tvaesenclast\t%xmm7,%xmm13,%xmm13\n\tvpaddb\t%xmm2,%xmm6,%xmm7\n\tvaesenclast\t%xmm3,%xmm14,%xmm14\n\tvpaddb\t%xmm2,%xmm7,%xmm3\n\n\taddq\t$0x60,%r10\n\tsubq\t$0x6,%rdx\n\tjc\t.L6x_done_nmb\n\n\tvmovups\t%xmm9,-96(%rsi)\n\tvpxor\t%xmm15,%xmm1,%xmm9\n\tvmovups\t%xmm10,-80(%rsi)\n\tvmovdqa\t%xmm0,%xmm10\n\tvmovups\t%xmm11,-64(%rsi)\n\tvmovdqa\t%xmm5,%xmm11\n\tvmovups\t%xmm12,-48(%rsi)\n\tvmovdqa\t%xmm6,%xmm12\n\tvmovups\t%xmm13,-32(%rsi)\n\tvmovdqa\t%xmm7,%xmm13\n\tvmovups\t%xmm14,-16(%rsi)\n\tvmovdqa\t%xmm3,%xmm14\n\tvmovdqu\t32+8(%rsp),%xmm7\n\tjmp\t.Loop6x_nmb\n\n.L6x_done_nmb:\n\tvpxor\t16+8(%rsp),%xmm8,%xmm8\n\tvpxor\t%xmm4,%xmm8,%xmm8\n\n\tRET\n.cfi_endproc\nSET_SIZE(_aesni_ctr32_ghash_no_movbe_6x)\n\nENTRY_ALIGN(aesni_gcm_decrypt, 32)\n.cfi_startproc\n\tENDBR\n\txorq\t%r10,%r10\n\tcmpq\t$0x60,%rdx\n\tjb\t.Lgcm_dec_abort\n\n\tleaq\t(%rsp),%rax\n.cfi_def_cfa_register\t%rax\n\tpushq\t%rbx\n.cfi_offset\t%rbx,-16\n\tpushq\t%rbp\n.cfi_offset\t%rbp,-24\n\tpushq\t%r12\n.cfi_offset\t%r12,-32\n\tpushq\t%r13\n.cfi_offset\t%r13,-40\n\tpushq\t%r14\n.cfi_offset\t%r14,-48\n\tpushq\t%r15\n.cfi_offset\t%r15,-56\n\tpushq\t%r9\n.cfi_offset\t%r9,-64\n\tvzeroupper\n\n\tvmovdqu\t(%r8),%xmm1\n\taddq\t$-128,%rsp\n\tmovl\t12(%r8),%ebx\n\tleaq\t.Lbswap_mask(%rip),%r11\n\tleaq\t-128(%rcx),%r14\n\tmovq\t$0xf80,%r15\n\tvmovdqu\t(%r9),%xmm8\n\tandq\t$-128,%rsp\n\tvmovdqu\t(%r11),%xmm0\n\tleaq\t128(%rcx),%rcx\n\tmovq\t32(%r9),%r9\n\tleaq\t32(%r9),%r9\n\tmovl\t504-128(%rcx),%ebp\t \n\tvpshufb\t%xmm0,%xmm8,%xmm8\n\n\tandq\t%r15,%r14\n\tandq\t%rsp,%r15\n\tsubq\t%r14,%r15\n\tjc\t.Ldec_no_key_aliasing\n\tcmpq\t$768,%r15\n\tjnc\t.Ldec_no_key_aliasing\n\tsubq\t%r15,%rsp\n.Ldec_no_key_aliasing:\n\n\tvmovdqu\t80(%rdi),%xmm7\n\tleaq\t(%rdi),%r14\n\tvmovdqu\t64(%rdi),%xmm4\n\tleaq\t-192(%rdi,%rdx,1),%r15\n\tvmovdqu\t48(%rdi),%xmm5\n\tshrq\t$4,%rdx\n\txorq\t%r10,%r10\n\tvmovdqu\t32(%rdi),%xmm6\n\tvpshufb\t%xmm0,%xmm7,%xmm7\n\tvmovdqu\t16(%rdi),%xmm2\n\tvpshufb\t%xmm0,%xmm4,%xmm4\n\tvmovdqu\t(%rdi),%xmm3\n\tvpshufb\t%xmm0,%xmm5,%xmm5\n\tvmovdqu\t%xmm4,48(%rsp)\n\tvpshufb\t%xmm0,%xmm6,%xmm6\n\tvmovdqu\t%xmm5,64(%rsp)\n\tvpshufb\t%xmm0,%xmm2,%xmm2\n\tvmovdqu\t%xmm6,80(%rsp)\n\tvpshufb\t%xmm0,%xmm3,%xmm3\n\tvmovdqu\t%xmm2,96(%rsp)\n\tvmovdqu\t%xmm3,112(%rsp)\n\n#ifdef HAVE_MOVBE\n#ifdef _KERNEL\n\ttestl\t$1,gcm_avx_can_use_movbe(%rip)\n#else\n\ttestl\t$1,gcm_avx_can_use_movbe@GOTPCREL(%rip)\n#endif\n\tjz\t1f\n\tcall\t_aesni_ctr32_ghash_6x\n\tjmp\t2f\n1:\n#endif\n\tcall\t_aesni_ctr32_ghash_no_movbe_6x\n2:\n\tvmovups\t%xmm9,-96(%rsi)\n\tvmovups\t%xmm10,-80(%rsi)\n\tvmovups\t%xmm11,-64(%rsi)\n\tvmovups\t%xmm12,-48(%rsi)\n\tvmovups\t%xmm13,-32(%rsi)\n\tvmovups\t%xmm14,-16(%rsi)\n\n\tvpshufb\t(%r11),%xmm8,%xmm8\n\tmovq\t-56(%rax),%r9\n.cfi_restore\t%r9\n\tvmovdqu\t%xmm8,(%r9)\n\n\tvzeroupper\n\tmovq\t-48(%rax),%r15\n.cfi_restore\t%r15\n\tmovq\t-40(%rax),%r14\n.cfi_restore\t%r14\n\tmovq\t-32(%rax),%r13\n.cfi_restore\t%r13\n\tmovq\t-24(%rax),%r12\n.cfi_restore\t%r12\n\tmovq\t-16(%rax),%rbp\n.cfi_restore\t%rbp\n\tmovq\t-8(%rax),%rbx\n.cfi_restore\t%rbx\n\tleaq\t(%rax),%rsp\n.cfi_def_cfa_register\t%rsp\n.Lgcm_dec_abort:\n\tmovq\t%r10,%rax\n\tRET\n.cfi_endproc\nSET_SIZE(aesni_gcm_decrypt)\n\n.balign 32\nFUNCTION(_aesni_ctr32_6x)\n.cfi_startproc\n\tENDBR\n\tvmovdqu\t0-128(%rcx),%xmm4\n\tvmovdqu\t32(%r11),%xmm2\n\tleaq\t-2(%rbp),%r13\t \n\tvmovups\t16-128(%rcx),%xmm15\n\tleaq\t32-128(%rcx),%r12\n\tvpxor\t%xmm4,%xmm1,%xmm9\n\taddl\t$100663296,%ebx\n\tjc\t.Lhandle_ctr32_2\n\tvpaddb\t%xmm2,%xmm1,%xmm10\n\tvpaddb\t%xmm2,%xmm10,%xmm11\n\tvpxor\t%xmm4,%xmm10,%xmm10\n\tvpaddb\t%xmm2,%xmm11,%xmm12\n\tvpxor\t%xmm4,%xmm11,%xmm11\n\tvpaddb\t%xmm2,%xmm12,%xmm13\n\tvpxor\t%xmm4,%xmm12,%xmm12\n\tvpaddb\t%xmm2,%xmm13,%xmm14\n\tvpxor\t%xmm4,%xmm13,%xmm13\n\tvpaddb\t%xmm2,%xmm14,%xmm1\n\tvpxor\t%xmm4,%xmm14,%xmm14\n\tjmp\t.Loop_ctr32\n\n.balign\t16\n.Loop_ctr32:\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\tvmovups\t(%r12),%xmm15\n\tleaq\t16(%r12),%r12\n\tdecl\t%r13d\n\tjnz\t.Loop_ctr32\n\n\tvmovdqu\t(%r12),%xmm3\n\tvaesenc\t%xmm15,%xmm9,%xmm9\n\tvpxor\t0(%rdi),%xmm3,%xmm4\n\tvaesenc\t%xmm15,%xmm10,%xmm10\n\tvpxor\t16(%rdi),%xmm3,%xmm5\n\tvaesenc\t%xmm15,%xmm11,%xmm11\n\tvpxor\t32(%rdi),%xmm3,%xmm6\n\tvaesenc\t%xmm15,%xmm12,%xmm12\n\tvpxor\t48(%rdi),%xmm3,%xmm8\n\tvaesenc\t%xmm15,%xmm13,%xmm13\n\tvpxor\t64(%rdi),%xmm3,%xmm2\n\tvaesenc\t%xmm15,%xmm14,%xmm14\n\tvpxor\t80(%rdi),%xmm3,%xmm3\n\tleaq\t96(%rdi),%rdi\n\n\tvaesenclast\t%xmm4,%xmm9,%xmm9\n\tvaesenclast\t%xmm5,%xmm10,%xmm10\n\tvaesenclast\t%xmm6,%xmm11,%xmm11\n\tvaesenclast\t%xmm8,%xmm12,%xmm12\n\tvaesenclast\t%xmm2,%xmm13,%xmm13\n\tvaesenclast\t%xmm3,%xmm14,%xmm14\n\tvmovups\t%xmm9,0(%rsi)\n\tvmovups\t%xmm10,16(%rsi)\n\tvmovups\t%xmm11,32(%rsi)\n\tvmovups\t%xmm12,48(%rsi)\n\tvmovups\t%xmm13,64(%rsi)\n\tvmovups\t%xmm14,80(%rsi)\n\tleaq\t96(%rsi),%rsi\n\n\tRET\n.balign\t32\n.Lhandle_ctr32_2:\n\tvpshufb\t%xmm0,%xmm1,%xmm6\n\tvmovdqu\t48(%r11),%xmm5\n\tvpaddd\t64(%r11),%xmm6,%xmm10\n\tvpaddd\t%xmm5,%xmm6,%xmm11\n\tvpaddd\t%xmm5,%xmm10,%xmm12\n\tvpshufb\t%xmm0,%xmm10,%xmm10\n\tvpaddd\t%xmm5,%xmm11,%xmm13\n\tvpshufb\t%xmm0,%xmm11,%xmm11\n\tvpxor\t%xmm4,%xmm10,%xmm10\n\tvpaddd\t%xmm5,%xmm12,%xmm14\n\tvpshufb\t%xmm0,%xmm12,%xmm12\n\tvpxor\t%xmm4,%xmm11,%xmm11\n\tvpaddd\t%xmm5,%xmm13,%xmm1\n\tvpshufb\t%xmm0,%xmm13,%xmm13\n\tvpxor\t%xmm4,%xmm12,%xmm12\n\tvpshufb\t%xmm0,%xmm14,%xmm14\n\tvpxor\t%xmm4,%xmm13,%xmm13\n\tvpshufb\t%xmm0,%xmm1,%xmm1\n\tvpxor\t%xmm4,%xmm14,%xmm14\n\tjmp\t.Loop_ctr32\n.cfi_endproc\nSET_SIZE(_aesni_ctr32_6x)\n\nENTRY_ALIGN(aesni_gcm_encrypt, 32)\n.cfi_startproc\n\tENDBR\n\txorq\t%r10,%r10\n\tcmpq\t$288,%rdx\n\tjb\t.Lgcm_enc_abort\n\n\tleaq\t(%rsp),%rax\n.cfi_def_cfa_register\t%rax\n\tpushq\t%rbx\n.cfi_offset\t%rbx,-16\n\tpushq\t%rbp\n.cfi_offset\t%rbp,-24\n\tpushq\t%r12\n.cfi_offset\t%r12,-32\n\tpushq\t%r13\n.cfi_offset\t%r13,-40\n\tpushq\t%r14\n.cfi_offset\t%r14,-48\n\tpushq\t%r15\n.cfi_offset\t%r15,-56\n\tpushq\t%r9\n.cfi_offset\t%r9,-64\n\tvzeroupper\n\n\tvmovdqu\t(%r8),%xmm1\n\taddq\t$-128,%rsp\n\tmovl\t12(%r8),%ebx\n\tleaq\t.Lbswap_mask(%rip),%r11\n\tleaq\t-128(%rcx),%r14\n\tmovq\t$0xf80,%r15\n\tleaq\t128(%rcx),%rcx\n\tvmovdqu\t(%r11),%xmm0\n\tandq\t$-128,%rsp\n\tmovl\t504-128(%rcx),%ebp\t \n\n\tandq\t%r15,%r14\n\tandq\t%rsp,%r15\n\tsubq\t%r14,%r15\n\tjc\t.Lenc_no_key_aliasing\n\tcmpq\t$768,%r15\n\tjnc\t.Lenc_no_key_aliasing\n\tsubq\t%r15,%rsp\n.Lenc_no_key_aliasing:\n\n\tleaq\t(%rsi),%r14\n\tleaq\t-192(%rsi,%rdx,1),%r15\n\tshrq\t$4,%rdx\n\n\tcall\t_aesni_ctr32_6x\n\tvpshufb\t%xmm0,%xmm9,%xmm8\n\tvpshufb\t%xmm0,%xmm10,%xmm2\n\tvmovdqu\t%xmm8,112(%rsp)\n\tvpshufb\t%xmm0,%xmm11,%xmm4\n\tvmovdqu\t%xmm2,96(%rsp)\n\tvpshufb\t%xmm0,%xmm12,%xmm5\n\tvmovdqu\t%xmm4,80(%rsp)\n\tvpshufb\t%xmm0,%xmm13,%xmm6\n\tvmovdqu\t%xmm5,64(%rsp)\n\tvpshufb\t%xmm0,%xmm14,%xmm7\n\tvmovdqu\t%xmm6,48(%rsp)\n\n\tcall\t_aesni_ctr32_6x\n\n\tvmovdqu\t(%r9),%xmm8\n\tmovq\t32(%r9),%r9\n\tleaq\t32(%r9),%r9\n\tsubq\t$12,%rdx\n\tmovq\t$192,%r10\n\tvpshufb\t%xmm0,%xmm8,%xmm8\n\n#ifdef HAVE_MOVBE\n#ifdef _KERNEL\n\ttestl\t$1,gcm_avx_can_use_movbe(%rip)\n#else\n\ttestl\t$1,gcm_avx_can_use_movbe@GOTPCREL(%rip)\n#endif\n\tjz\t1f\n\tcall\t_aesni_ctr32_ghash_6x\n\tjmp\t2f\n1:\n#endif\n\tcall\t_aesni_ctr32_ghash_no_movbe_6x\n2:\n\tvmovdqu\t32(%rsp),%xmm7\n\tvmovdqu\t(%r11),%xmm0\n\tvmovdqu\t0-32(%r9),%xmm3\n\tvpunpckhqdq\t%xmm7,%xmm7,%xmm1\n\tvmovdqu\t32-32(%r9),%xmm15\n\tvmovups\t%xmm9,-96(%rsi)\n\tvpshufb\t%xmm0,%xmm9,%xmm9\n\tvpxor\t%xmm7,%xmm1,%xmm1\n\tvmovups\t%xmm10,-80(%rsi)\n\tvpshufb\t%xmm0,%xmm10,%xmm10\n\tvmovups\t%xmm11,-64(%rsi)\n\tvpshufb\t%xmm0,%xmm11,%xmm11\n\tvmovups\t%xmm12,-48(%rsi)\n\tvpshufb\t%xmm0,%xmm12,%xmm12\n\tvmovups\t%xmm13,-32(%rsi)\n\tvpshufb\t%xmm0,%xmm13,%xmm13\n\tvmovups\t%xmm14,-16(%rsi)\n\tvpshufb\t%xmm0,%xmm14,%xmm14\n\tvmovdqu\t%xmm9,16(%rsp)\n\tvmovdqu\t48(%rsp),%xmm6\n\tvmovdqu\t16-32(%r9),%xmm0\n\tvpunpckhqdq\t%xmm6,%xmm6,%xmm2\n\tvpclmulqdq\t$0x00,%xmm3,%xmm7,%xmm5\n\tvpxor\t%xmm6,%xmm2,%xmm2\n\tvpclmulqdq\t$0x11,%xmm3,%xmm7,%xmm7\n\tvpclmulqdq\t$0x00,%xmm15,%xmm1,%xmm1\n\n\tvmovdqu\t64(%rsp),%xmm9\n\tvpclmulqdq\t$0x00,%xmm0,%xmm6,%xmm4\n\tvmovdqu\t48-32(%r9),%xmm3\n\tvpxor\t%xmm5,%xmm4,%xmm4\n\tvpunpckhqdq\t%xmm9,%xmm9,%xmm5\n\tvpclmulqdq\t$0x11,%xmm0,%xmm6,%xmm6\n\tvpxor\t%xmm9,%xmm5,%xmm5\n\tvpxor\t%xmm7,%xmm6,%xmm6\n\tvpclmulqdq\t$0x10,%xmm15,%xmm2,%xmm2\n\tvmovdqu\t80-32(%r9),%xmm15\n\tvpxor\t%xmm1,%xmm2,%xmm2\n\n\tvmovdqu\t80(%rsp),%xmm1\n\tvpclmulqdq\t$0x00,%xmm3,%xmm9,%xmm7\n\tvmovdqu\t64-32(%r9),%xmm0\n\tvpxor\t%xmm4,%xmm7,%xmm7\n\tvpunpckhqdq\t%xmm1,%xmm1,%xmm4\n\tvpclmulqdq\t$0x11,%xmm3,%xmm9,%xmm9\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpxor\t%xmm6,%xmm9,%xmm9\n\tvpclmulqdq\t$0x00,%xmm15,%xmm5,%xmm5\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\n\tvmovdqu\t96(%rsp),%xmm2\n\tvpclmulqdq\t$0x00,%xmm0,%xmm1,%xmm6\n\tvmovdqu\t96-32(%r9),%xmm3\n\tvpxor\t%xmm7,%xmm6,%xmm6\n\tvpunpckhqdq\t%xmm2,%xmm2,%xmm7\n\tvpclmulqdq\t$0x11,%xmm0,%xmm1,%xmm1\n\tvpxor\t%xmm2,%xmm7,%xmm7\n\tvpxor\t%xmm9,%xmm1,%xmm1\n\tvpclmulqdq\t$0x10,%xmm15,%xmm4,%xmm4\n\tvmovdqu\t128-32(%r9),%xmm15\n\tvpxor\t%xmm5,%xmm4,%xmm4\n\n\tvpxor\t112(%rsp),%xmm8,%xmm8\n\tvpclmulqdq\t$0x00,%xmm3,%xmm2,%xmm5\n\tvmovdqu\t112-32(%r9),%xmm0\n\tvpunpckhqdq\t%xmm8,%xmm8,%xmm9\n\tvpxor\t%xmm6,%xmm5,%xmm5\n\tvpclmulqdq\t$0x11,%xmm3,%xmm2,%xmm2\n\tvpxor\t%xmm8,%xmm9,%xmm9\n\tvpxor\t%xmm1,%xmm2,%xmm2\n\tvpclmulqdq\t$0x00,%xmm15,%xmm7,%xmm7\n\tvpxor\t%xmm4,%xmm7,%xmm4\n\n\tvpclmulqdq\t$0x00,%xmm0,%xmm8,%xmm6\n\tvmovdqu\t0-32(%r9),%xmm3\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm1\n\tvpclmulqdq\t$0x11,%xmm0,%xmm8,%xmm8\n\tvpxor\t%xmm14,%xmm1,%xmm1\n\tvpxor\t%xmm5,%xmm6,%xmm5\n\tvpclmulqdq\t$0x10,%xmm15,%xmm9,%xmm9\n\tvmovdqu\t32-32(%r9),%xmm15\n\tvpxor\t%xmm2,%xmm8,%xmm7\n\tvpxor\t%xmm4,%xmm9,%xmm6\n\n\tvmovdqu\t16-32(%r9),%xmm0\n\tvpxor\t%xmm5,%xmm7,%xmm9\n\tvpclmulqdq\t$0x00,%xmm3,%xmm14,%xmm4\n\tvpxor\t%xmm9,%xmm6,%xmm6\n\tvpunpckhqdq\t%xmm13,%xmm13,%xmm2\n\tvpclmulqdq\t$0x11,%xmm3,%xmm14,%xmm14\n\tvpxor\t%xmm13,%xmm2,%xmm2\n\tvpslldq\t$8,%xmm6,%xmm9\n\tvpclmulqdq\t$0x00,%xmm15,%xmm1,%xmm1\n\tvpxor\t%xmm9,%xmm5,%xmm8\n\tvpsrldq\t$8,%xmm6,%xmm6\n\tvpxor\t%xmm6,%xmm7,%xmm7\n\n\tvpclmulqdq\t$0x00,%xmm0,%xmm13,%xmm5\n\tvmovdqu\t48-32(%r9),%xmm3\n\tvpxor\t%xmm4,%xmm5,%xmm5\n\tvpunpckhqdq\t%xmm12,%xmm12,%xmm9\n\tvpclmulqdq\t$0x11,%xmm0,%xmm13,%xmm13\n\tvpxor\t%xmm12,%xmm9,%xmm9\n\tvpxor\t%xmm14,%xmm13,%xmm13\n\tvpalignr\t$8,%xmm8,%xmm8,%xmm14\n\tvpclmulqdq\t$0x10,%xmm15,%xmm2,%xmm2\n\tvmovdqu\t80-32(%r9),%xmm15\n\tvpxor\t%xmm1,%xmm2,%xmm2\n\n\tvpclmulqdq\t$0x00,%xmm3,%xmm12,%xmm4\n\tvmovdqu\t64-32(%r9),%xmm0\n\tvpxor\t%xmm5,%xmm4,%xmm4\n\tvpunpckhqdq\t%xmm11,%xmm11,%xmm1\n\tvpclmulqdq\t$0x11,%xmm3,%xmm12,%xmm12\n\tvpxor\t%xmm11,%xmm1,%xmm1\n\tvpxor\t%xmm13,%xmm12,%xmm12\n\tvxorps\t16(%rsp),%xmm7,%xmm7\n\tvpclmulqdq\t$0x00,%xmm15,%xmm9,%xmm9\n\tvpxor\t%xmm2,%xmm9,%xmm9\n\n\tvpclmulqdq\t$0x10,16(%r11),%xmm8,%xmm8\n\tvxorps\t%xmm14,%xmm8,%xmm8\n\n\tvpclmulqdq\t$0x00,%xmm0,%xmm11,%xmm5\n\tvmovdqu\t96-32(%r9),%xmm3\n\tvpxor\t%xmm4,%xmm5,%xmm5\n\tvpunpckhqdq\t%xmm10,%xmm10,%xmm2\n\tvpclmulqdq\t$0x11,%xmm0,%xmm11,%xmm11\n\tvpxor\t%xmm10,%xmm2,%xmm2\n\tvpalignr\t$8,%xmm8,%xmm8,%xmm14\n\tvpxor\t%xmm12,%xmm11,%xmm11\n\tvpclmulqdq\t$0x10,%xmm15,%xmm1,%xmm1\n\tvmovdqu\t128-32(%r9),%xmm15\n\tvpxor\t%xmm9,%xmm1,%xmm1\n\n\tvxorps\t%xmm7,%xmm14,%xmm14\n\tvpclmulqdq\t$0x10,16(%r11),%xmm8,%xmm8\n\tvxorps\t%xmm14,%xmm8,%xmm8\n\n\tvpclmulqdq\t$0x00,%xmm3,%xmm10,%xmm4\n\tvmovdqu\t112-32(%r9),%xmm0\n\tvpxor\t%xmm5,%xmm4,%xmm4\n\tvpunpckhqdq\t%xmm8,%xmm8,%xmm9\n\tvpclmulqdq\t$0x11,%xmm3,%xmm10,%xmm10\n\tvpxor\t%xmm8,%xmm9,%xmm9\n\tvpxor\t%xmm11,%xmm10,%xmm10\n\tvpclmulqdq\t$0x00,%xmm15,%xmm2,%xmm2\n\tvpxor\t%xmm1,%xmm2,%xmm2\n\n\tvpclmulqdq\t$0x00,%xmm0,%xmm8,%xmm5\n\tvpclmulqdq\t$0x11,%xmm0,%xmm8,%xmm7\n\tvpxor\t%xmm4,%xmm5,%xmm5\n\tvpclmulqdq\t$0x10,%xmm15,%xmm9,%xmm6\n\tvpxor\t%xmm10,%xmm7,%xmm7\n\tvpxor\t%xmm2,%xmm6,%xmm6\n\n\tvpxor\t%xmm5,%xmm7,%xmm4\n\tvpxor\t%xmm4,%xmm6,%xmm6\n\tvpslldq\t$8,%xmm6,%xmm1\n\tvmovdqu\t16(%r11),%xmm3\n\tvpsrldq\t$8,%xmm6,%xmm6\n\tvpxor\t%xmm1,%xmm5,%xmm8\n\tvpxor\t%xmm6,%xmm7,%xmm7\n\n\tvpalignr\t$8,%xmm8,%xmm8,%xmm2\n\tvpclmulqdq\t$0x10,%xmm3,%xmm8,%xmm8\n\tvpxor\t%xmm2,%xmm8,%xmm8\n\n\tvpalignr\t$8,%xmm8,%xmm8,%xmm2\n\tvpclmulqdq\t$0x10,%xmm3,%xmm8,%xmm8\n\tvpxor\t%xmm7,%xmm2,%xmm2\n\tvpxor\t%xmm2,%xmm8,%xmm8\n\tvpshufb\t(%r11),%xmm8,%xmm8\n\tmovq\t-56(%rax),%r9\n.cfi_restore\t%r9\n\tvmovdqu\t%xmm8,(%r9)\n\n\tvzeroupper\n\tmovq\t-48(%rax),%r15\n.cfi_restore\t%r15\n\tmovq\t-40(%rax),%r14\n.cfi_restore\t%r14\n\tmovq\t-32(%rax),%r13\n.cfi_restore\t%r13\n\tmovq\t-24(%rax),%r12\n.cfi_restore\t%r12\n\tmovq\t-16(%rax),%rbp\n.cfi_restore\t%rbp\n\tmovq\t-8(%rax),%rbx\n.cfi_restore\t%rbx\n\tleaq\t(%rax),%rsp\n.cfi_def_cfa_register\t%rsp\n.Lgcm_enc_abort:\n\tmovq\t%r10,%rax\n\tRET\n.cfi_endproc\nSET_SIZE(aesni_gcm_encrypt)\n\n#endif  \n\n \n\n \nENTRY_ALIGN(clear_fpu_regs_avx, 32)\n\tvzeroall\n\tRET\nSET_SIZE(clear_fpu_regs_avx)\n\n \nENTRY_ALIGN(gcm_xor_avx, 32)\n\tmovdqu  (%rdi), %xmm0\n\tmovdqu  (%rsi), %xmm1\n\tpxor    %xmm1, %xmm0\n\tmovdqu  %xmm0, (%rsi)\n\tRET\nSET_SIZE(gcm_xor_avx)\n\n \nENTRY_ALIGN(atomic_toggle_boolean_nv, 32)\n\txorl\t%eax, %eax\n\tlock\n\txorl\t$1, (%rdi)\n\tjz\t1f\n\tmovl\t$1, %eax\n1:\n\tRET\nSET_SIZE(atomic_toggle_boolean_nv)\n\nSECTION_STATIC\n\n.balign\t64\n.Lbswap_mask:\n.byte\t15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0\n.Lpoly:\n.byte\t0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xc2\n.Lone_msb:\n.byte\t0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1\n.Ltwo_lsb:\n.byte\t2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n.Lone_lsb:\n.byte\t1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n.byte\t65,69,83,45,78,73,32,71,67,77,32,109,111,100,117,108,101,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0\n.balign\t64\n\n \n#if defined(__linux__) && defined(__ELF__)\n.section .note.GNU-stack,\"\",%progbits\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}