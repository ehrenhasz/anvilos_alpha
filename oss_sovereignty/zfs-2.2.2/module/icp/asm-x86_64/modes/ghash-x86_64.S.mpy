{
  "module_name": "ghash-x86_64.S",
  "hash_id": "ae87ae8195e42f631c7650c353cbed0b16a0aa3b757ba44445fdc2ab3c71a108",
  "original_prompt": "Ingested from zfs-2.2.2/module/icp/asm-x86_64/modes/ghash-x86_64.S",
  "human_readable_source": "# Copyright 2010-2016 The OpenSSL Project Authors. All Rights Reserved.\n#\n# Licensed under the Apache License 2.0 (the \"License\").  You may not use\n# this file except in compliance with the License.  You can obtain a copy\n# in the file LICENSE in the source distribution or at\n# https: \n\n#\n# ====================================================================\n# Written by Andy Polyakov <appro@openssl.org> for the OpenSSL\n# project. The module is, however, dual licensed under OpenSSL and\n# CRYPTOGAMS licenses depending on where you obtain it. For further\n# details see http: \n# ====================================================================\n#\n# March, June 2010\n#\n# The module implements \"4-bit\" GCM GHASH function and underlying\n# single multiplication operation in GF(2^128). \"4-bit\" means that\n# it uses 256 bytes per-key table [+128 bytes shared table]. GHASH\n# function features so called \"528B\" variant utilizing additional\n# 256+16 bytes of per-key storage [+512 bytes shared table].\n# Performance results are for this streamed GHASH subroutine and are\n# expressed in cycles per processed byte, less is better:\n#\n#\t\tgcc 3.4.x(*)\tassembler\n#\n# P4\t\t28.6\t\t14.0\t\t+100%\n# Opteron\t19.3\t\t7.7\t\t+150%\n# Core2\t\t17.8\t\t8.1(**)\t\t+120%\n# Atom\t\t31.6\t\t16.8\t\t+88%\n# VIA Nano\t21.8\t\t10.1\t\t+115%\n#\n# (*)\tcomparison is not completely fair, because C results are\n#\tfor vanilla \"256B\" implementation, while assembler results\n#\tare for \"528B\";-)\n# (**)\tit's mystery [to me] why Core2 result is not same as for\n#\tOpteron;\n\n# May 2010\n#\n# Add PCLMULQDQ version performing at 2.02 cycles per processed byte.\n# See ghash-x86.pl for background information and details about coding\n# techniques.\n#\n# Special thanks to David Woodhouse for providing access to a\n# Westmere-based system on behalf of Intel Open Source Technology Centre.\n\n# December 2012\n#\n# Overhaul: aggregate Karatsuba post-processing, improve ILP in\n# reduction_alg9, increase reduction aggregate factor to 4x. As for\n# the latter. ghash-x86.pl discusses that it makes lesser sense to\n# increase aggregate factor. Then why increase here? Critical path\n# consists of 3 independent pclmulqdq instructions, Karatsuba post-\n# processing and reduction. \"On top\" of this we lay down aggregated\n# multiplication operations, triplets of independent pclmulqdq's. As\n# issue rate for pclmulqdq is limited, it makes lesser sense to\n# aggregate more multiplications than it takes to perform remaining\n# non-multiplication operations. 2x is near-optimal coefficient for\n# contemporary Intel CPUs (therefore modest improvement coefficient),\n# but not for Bulldozer. Latter is because logical SIMD operations\n# are twice as slow in comparison to Intel, so that critical path is\n# longer. A CPU with higher pclmulqdq issue rate would also benefit\n# from higher aggregate factor...\n#\n# Westmere\t1.78(+13%)\n# Sandy Bridge\t1.80(+8%)\n# Ivy Bridge\t1.80(+7%)\n# Haswell\t0.55(+93%) (if system doesn't support AVX)\n# Broadwell\t0.45(+110%)(if system doesn't support AVX)\n# Skylake\t0.44(+110%)(if system doesn't support AVX)\n# Bulldozer\t1.49(+27%)\n# Silvermont\t2.88(+13%)\n# Knights L\t2.12(-)    (if system doesn't support AVX)\n# Goldmont\t1.08(+24%)\n\n# March 2013\n#\n# ... 8x aggregate factor AVX code path is using reduction algorithm\n# suggested by Shay Gueron[1]. Even though contemporary AVX-capable\n# CPUs such as Sandy and Ivy Bridge can execute it, the code performs\n# sub-optimally in comparison to above mentioned version. But thanks\n# to Ilya Albrekht and Max Locktyukhin of Intel Corp. we knew that\n# it performs in 0.41 cycles per byte on Haswell processor, in\n# 0.29 on Broadwell, and in 0.36 on Skylake.\n#\n# Knights Landing achieves 1.09 cpb.\n#\n# [1] http: \n\n# Generated once from\n# https: \n# and modified for ICP. Modification are kept at a bare minimum to ease later\n# upstream merges.\n\n#if defined(__x86_64__) && defined(HAVE_AVX) && \\\n    defined(HAVE_AES) && defined(HAVE_PCLMULQDQ)\n\n#define _ASM\n#include <sys/asm_linkage.h>\n\n.text\n\n \n#if !defined (_WIN32) || defined (_KERNEL)\nENTRY_ALIGN(gcm_gmult_clmul, 16)\n\n.cfi_startproc\n\tENDBR\n\n.L_gmult_clmul:\n\tmovdqu\t(%rdi),%xmm0\n\tmovdqa\t.Lbswap_mask(%rip),%xmm5\n\tmovdqu\t(%rsi),%xmm2\n\tmovdqu\t32(%rsi),%xmm4\n.byte\t102,15,56,0,197\n\tmovdqa\t%xmm0,%xmm1\n\tpshufd\t$78,%xmm0,%xmm3\n\tpxor\t%xmm0,%xmm3\n.byte\t102,15,58,68,194,0\n.byte\t102,15,58,68,202,17\n.byte\t102,15,58,68,220,0\n\tpxor\t%xmm0,%xmm3\n\tpxor\t%xmm1,%xmm3\n\n\tmovdqa\t%xmm3,%xmm4\n\tpsrldq\t$8,%xmm3\n\tpslldq\t$8,%xmm4\n\tpxor\t%xmm3,%xmm1\n\tpxor\t%xmm4,%xmm0\n\n\tmovdqa\t%xmm0,%xmm4\n\tmovdqa\t%xmm0,%xmm3\n\tpsllq\t$5,%xmm0\n\tpxor\t%xmm0,%xmm3\n\tpsllq\t$1,%xmm0\n\tpxor\t%xmm3,%xmm0\n\tpsllq\t$57,%xmm0\n\tmovdqa\t%xmm0,%xmm3\n\tpslldq\t$8,%xmm0\n\tpsrldq\t$8,%xmm3\n\tpxor\t%xmm4,%xmm0\n\tpxor\t%xmm3,%xmm1\n\n\n\tmovdqa\t%xmm0,%xmm4\n\tpsrlq\t$1,%xmm0\n\tpxor\t%xmm4,%xmm1\n\tpxor\t%xmm0,%xmm4\n\tpsrlq\t$5,%xmm0\n\tpxor\t%xmm4,%xmm0\n\tpsrlq\t$1,%xmm0\n\tpxor\t%xmm1,%xmm0\n.byte\t102,15,56,0,197\n\tmovdqu\t%xmm0,(%rdi)\n\tRET\n.cfi_endproc\nSET_SIZE(gcm_gmult_clmul)\n#endif  \n\nENTRY_ALIGN(gcm_init_htab_avx, 32)\n.cfi_startproc\n\tENDBR\n\tvzeroupper\n\n\tvmovdqu\t(%rsi),%xmm2\n\t \n\t \n\tvmovdqu\t.Lbswap_mask(%rip),%xmm4\n\tvpshufb\t%xmm4,%xmm2,%xmm2\n\n\n\tvpshufd\t$255,%xmm2,%xmm4\n\tvpsrlq\t$63,%xmm2,%xmm3\n\tvpsllq\t$1,%xmm2,%xmm2\n\tvpxor\t%xmm5,%xmm5,%xmm5\n\tvpcmpgtd\t%xmm4,%xmm5,%xmm5\n\tvpslldq\t$8,%xmm3,%xmm3\n\tvpor\t%xmm3,%xmm2,%xmm2\n\n\n\tvpand\t.L0x1c2_polynomial(%rip),%xmm5,%xmm5\n\tvpxor\t%xmm5,%xmm2,%xmm2\n\n\tvpunpckhqdq\t%xmm2,%xmm2,%xmm6\n\tvmovdqa\t%xmm2,%xmm0\n\tvpxor\t%xmm2,%xmm6,%xmm6\n\tmovq\t$4,%r10\n\tjmp\t.Linit_start_avx\n.balign\t32\n.Linit_loop_avx:\n\tvpalignr\t$8,%xmm3,%xmm4,%xmm5\n\tvmovdqu\t%xmm5,-16(%rdi)\n\tvpunpckhqdq\t%xmm0,%xmm0,%xmm3\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x11,%xmm2,%xmm0,%xmm1\n\tvpclmulqdq\t$0x00,%xmm2,%xmm0,%xmm0\n\tvpclmulqdq\t$0x00,%xmm6,%xmm3,%xmm3\n\tvpxor\t%xmm0,%xmm1,%xmm4\n\tvpxor\t%xmm4,%xmm3,%xmm3\n\n\tvpslldq\t$8,%xmm3,%xmm4\n\tvpsrldq\t$8,%xmm3,%xmm3\n\tvpxor\t%xmm4,%xmm0,%xmm0\n\tvpxor\t%xmm3,%xmm1,%xmm1\n\tvpsllq\t$57,%xmm0,%xmm3\n\tvpsllq\t$62,%xmm0,%xmm4\n\tvpxor\t%xmm3,%xmm4,%xmm4\n\tvpsllq\t$63,%xmm0,%xmm3\n\tvpxor\t%xmm3,%xmm4,%xmm4\n\tvpslldq\t$8,%xmm4,%xmm3\n\tvpsrldq\t$8,%xmm4,%xmm4\n\tvpxor\t%xmm3,%xmm0,%xmm0\n\tvpxor\t%xmm4,%xmm1,%xmm1\n\n\tvpsrlq\t$1,%xmm0,%xmm4\n\tvpxor\t%xmm0,%xmm1,%xmm1\n\tvpxor\t%xmm4,%xmm0,%xmm0\n\tvpsrlq\t$5,%xmm4,%xmm4\n\tvpxor\t%xmm4,%xmm0,%xmm0\n\tvpsrlq\t$1,%xmm0,%xmm0\n\tvpxor\t%xmm1,%xmm0,%xmm0\n.Linit_start_avx:\n\tvmovdqa\t%xmm0,%xmm5\n\tvpunpckhqdq\t%xmm0,%xmm0,%xmm3\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x11,%xmm2,%xmm0,%xmm1\n\tvpclmulqdq\t$0x00,%xmm2,%xmm0,%xmm0\n\tvpclmulqdq\t$0x00,%xmm6,%xmm3,%xmm3\n\tvpxor\t%xmm0,%xmm1,%xmm4\n\tvpxor\t%xmm4,%xmm3,%xmm3\n\n\tvpslldq\t$8,%xmm3,%xmm4\n\tvpsrldq\t$8,%xmm3,%xmm3\n\tvpxor\t%xmm4,%xmm0,%xmm0\n\tvpxor\t%xmm3,%xmm1,%xmm1\n\tvpsllq\t$57,%xmm0,%xmm3\n\tvpsllq\t$62,%xmm0,%xmm4\n\tvpxor\t%xmm3,%xmm4,%xmm4\n\tvpsllq\t$63,%xmm0,%xmm3\n\tvpxor\t%xmm3,%xmm4,%xmm4\n\tvpslldq\t$8,%xmm4,%xmm3\n\tvpsrldq\t$8,%xmm4,%xmm4\n\tvpxor\t%xmm3,%xmm0,%xmm0\n\tvpxor\t%xmm4,%xmm1,%xmm1\n\n\tvpsrlq\t$1,%xmm0,%xmm4\n\tvpxor\t%xmm0,%xmm1,%xmm1\n\tvpxor\t%xmm4,%xmm0,%xmm0\n\tvpsrlq\t$5,%xmm4,%xmm4\n\tvpxor\t%xmm4,%xmm0,%xmm0\n\tvpsrlq\t$1,%xmm0,%xmm0\n\tvpxor\t%xmm1,%xmm0,%xmm0\n\tvpshufd\t$78,%xmm5,%xmm3\n\tvpshufd\t$78,%xmm0,%xmm4\n\tvpxor\t%xmm5,%xmm3,%xmm3\n\tvmovdqu\t%xmm5,0(%rdi)\n\tvpxor\t%xmm0,%xmm4,%xmm4\n\tvmovdqu\t%xmm0,16(%rdi)\n\tleaq\t48(%rdi),%rdi\n\tsubq\t$1,%r10\n\tjnz\t.Linit_loop_avx\n\n\tvpalignr\t$8,%xmm4,%xmm3,%xmm5\n\tvmovdqu\t%xmm5,-16(%rdi)\n\n\tvzeroupper\n\tRET\n.cfi_endproc\nSET_SIZE(gcm_init_htab_avx)\n\n#if !defined (_WIN32) || defined (_KERNEL)\nENTRY_ALIGN(gcm_gmult_avx, 32)\n.cfi_startproc\n\tENDBR\n\tjmp\t.L_gmult_clmul\n.cfi_endproc\nSET_SIZE(gcm_gmult_avx)\n\nENTRY_ALIGN(gcm_ghash_avx, 32)\n.cfi_startproc\n\tENDBR\n\tvzeroupper\n\n\tvmovdqu\t(%rdi),%xmm10\n\tleaq\t.L0x1c2_polynomial(%rip),%r10\n\tleaq\t64(%rsi),%rsi\n\tvmovdqu\t.Lbswap_mask(%rip),%xmm13\n\tvpshufb\t%xmm13,%xmm10,%xmm10\n\tcmpq\t$0x80,%rcx\n\tjb\t.Lshort_avx\n\tsubq\t$0x80,%rcx\n\n\tvmovdqu\t112(%rdx),%xmm14\n\tvmovdqu\t0-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvmovdqu\t32-64(%rsi),%xmm7\n\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvmovdqu\t96(%rdx),%xmm15\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t16-64(%rsi),%xmm6\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvmovdqu\t80(%rdx),%xmm14\n\tvpclmulqdq\t$0x00,%xmm7,%xmm9,%xmm2\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm3\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm4\n\tvmovdqu\t48-64(%rsi),%xmm6\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\tvmovdqu\t64(%rdx),%xmm15\n\tvpclmulqdq\t$0x10,%xmm7,%xmm8,%xmm5\n\tvmovdqu\t80-64(%rsi),%xmm7\n\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t64-64(%rsi),%xmm6\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm9,%xmm2\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\n\tvmovdqu\t48(%rdx),%xmm14\n\tvpxor\t%xmm3,%xmm0,%xmm0\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm3\n\tvpxor\t%xmm4,%xmm1,%xmm1\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm4\n\tvmovdqu\t96-64(%rsi),%xmm6\n\tvpxor\t%xmm5,%xmm2,%xmm2\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpclmulqdq\t$0x10,%xmm7,%xmm8,%xmm5\n\tvmovdqu\t128-64(%rsi),%xmm7\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\n\tvmovdqu\t32(%rdx),%xmm15\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t112-64(%rsi),%xmm6\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpclmulqdq\t$0x00,%xmm7,%xmm9,%xmm2\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\n\tvmovdqu\t16(%rdx),%xmm14\n\tvpxor\t%xmm3,%xmm0,%xmm0\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm3\n\tvpxor\t%xmm4,%xmm1,%xmm1\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm4\n\tvmovdqu\t144-64(%rsi),%xmm6\n\tvpxor\t%xmm5,%xmm2,%xmm2\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpclmulqdq\t$0x10,%xmm7,%xmm8,%xmm5\n\tvmovdqu\t176-64(%rsi),%xmm7\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\n\tvmovdqu\t(%rdx),%xmm15\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t160-64(%rsi),%xmm6\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x10,%xmm7,%xmm9,%xmm2\n\n\tleaq\t128(%rdx),%rdx\n\tcmpq\t$0x80,%rcx\n\tjb\t.Ltail_avx\n\n\tvpxor\t%xmm10,%xmm15,%xmm15\n\tsubq\t$0x80,%rcx\n\tjmp\t.Loop8x_avx\n\n.balign\t32\n.Loop8x_avx:\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvmovdqu\t112(%rdx),%xmm14\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm10\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm11\n\tvmovdqu\t0-64(%rsi),%xmm6\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm12\n\tvmovdqu\t32-64(%rsi),%xmm7\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\n\tvmovdqu\t96(%rdx),%xmm15\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpxor\t%xmm3,%xmm10,%xmm10\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvxorps\t%xmm4,%xmm11,%xmm11\n\tvmovdqu\t16-64(%rsi),%xmm6\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpclmulqdq\t$0x00,%xmm7,%xmm9,%xmm2\n\tvpxor\t%xmm5,%xmm12,%xmm12\n\tvxorps\t%xmm15,%xmm8,%xmm8\n\n\tvmovdqu\t80(%rdx),%xmm14\n\tvpxor\t%xmm10,%xmm12,%xmm12\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm3\n\tvpxor\t%xmm11,%xmm12,%xmm12\n\tvpslldq\t$8,%xmm12,%xmm9\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm4\n\tvpsrldq\t$8,%xmm12,%xmm12\n\tvpxor\t%xmm9,%xmm10,%xmm10\n\tvmovdqu\t48-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvxorps\t%xmm12,%xmm11,%xmm11\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpclmulqdq\t$0x10,%xmm7,%xmm8,%xmm5\n\tvmovdqu\t80-64(%rsi),%xmm7\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\n\tvmovdqu\t64(%rdx),%xmm15\n\tvpalignr\t$8,%xmm10,%xmm10,%xmm12\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpxor\t%xmm3,%xmm0,%xmm0\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t64-64(%rsi),%xmm6\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm4,%xmm1,%xmm1\n\tvpclmulqdq\t$0x00,%xmm7,%xmm9,%xmm2\n\tvxorps\t%xmm15,%xmm8,%xmm8\n\tvpxor\t%xmm5,%xmm2,%xmm2\n\n\tvmovdqu\t48(%rdx),%xmm14\n\tvpclmulqdq\t$0x10,(%r10),%xmm10,%xmm10\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm3\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm4\n\tvmovdqu\t96-64(%rsi),%xmm6\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x10,%xmm7,%xmm8,%xmm5\n\tvmovdqu\t128-64(%rsi),%xmm7\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\n\tvmovdqu\t32(%rdx),%xmm15\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpxor\t%xmm3,%xmm0,%xmm0\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t112-64(%rsi),%xmm6\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm4,%xmm1,%xmm1\n\tvpclmulqdq\t$0x00,%xmm7,%xmm9,%xmm2\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvpxor\t%xmm5,%xmm2,%xmm2\n\tvxorps\t%xmm12,%xmm10,%xmm10\n\n\tvmovdqu\t16(%rdx),%xmm14\n\tvpalignr\t$8,%xmm10,%xmm10,%xmm12\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm3\n\tvpshufb\t%xmm13,%xmm14,%xmm14\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm4\n\tvmovdqu\t144-64(%rsi),%xmm6\n\tvpclmulqdq\t$0x10,(%r10),%xmm10,%xmm10\n\tvxorps\t%xmm11,%xmm12,%xmm12\n\tvpunpckhqdq\t%xmm14,%xmm14,%xmm9\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x10,%xmm7,%xmm8,%xmm5\n\tvmovdqu\t176-64(%rsi),%xmm7\n\tvpxor\t%xmm14,%xmm9,%xmm9\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\n\tvmovdqu\t(%rdx),%xmm15\n\tvpclmulqdq\t$0x00,%xmm6,%xmm14,%xmm0\n\tvpshufb\t%xmm13,%xmm15,%xmm15\n\tvpclmulqdq\t$0x11,%xmm6,%xmm14,%xmm1\n\tvmovdqu\t160-64(%rsi),%xmm6\n\tvpxor\t%xmm12,%xmm15,%xmm15\n\tvpclmulqdq\t$0x10,%xmm7,%xmm9,%xmm2\n\tvpxor\t%xmm10,%xmm15,%xmm15\n\n\tleaq\t128(%rdx),%rdx\n\tsubq\t$0x80,%rcx\n\tjnc\t.Loop8x_avx\n\n\taddq\t$0x80,%rcx\n\tjmp\t.Ltail_no_xor_avx\n\n.balign\t32\n.Lshort_avx:\n\tvmovdqu\t-16(%rdx,%rcx,1),%xmm14\n\tleaq\t(%rdx,%rcx,1),%rdx\n\tvmovdqu\t0-64(%rsi),%xmm6\n\tvmovdqu\t32-64(%rsi),%xmm7\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\n\tvmovdqa\t%xmm0,%xmm3\n\tvmovdqa\t%xmm1,%xmm4\n\tvmovdqa\t%xmm2,%xmm5\n\tsubq\t$0x10,%rcx\n\tjz\t.Ltail_avx\n\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvmovdqu\t-32(%rdx),%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvmovdqu\t16-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\tvpsrldq\t$8,%xmm7,%xmm7\n\tsubq\t$0x10,%rcx\n\tjz\t.Ltail_avx\n\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvmovdqu\t-48(%rdx),%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvmovdqu\t48-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\tvmovdqu\t80-64(%rsi),%xmm7\n\tsubq\t$0x10,%rcx\n\tjz\t.Ltail_avx\n\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvmovdqu\t-64(%rdx),%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvmovdqu\t64-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\tvpsrldq\t$8,%xmm7,%xmm7\n\tsubq\t$0x10,%rcx\n\tjz\t.Ltail_avx\n\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvmovdqu\t-80(%rdx),%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvmovdqu\t96-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\tvmovdqu\t128-64(%rsi),%xmm7\n\tsubq\t$0x10,%rcx\n\tjz\t.Ltail_avx\n\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvmovdqu\t-96(%rdx),%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvmovdqu\t112-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\tvpsrldq\t$8,%xmm7,%xmm7\n\tsubq\t$0x10,%rcx\n\tjz\t.Ltail_avx\n\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvmovdqu\t-112(%rdx),%xmm14\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvmovdqu\t144-64(%rsi),%xmm6\n\tvpshufb\t%xmm13,%xmm14,%xmm15\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\tvmovq\t184-64(%rsi),%xmm7\n\tsubq\t$0x10,%rcx\n\tjmp\t.Ltail_avx\n\n.balign\t32\n.Ltail_avx:\n\tvpxor\t%xmm10,%xmm15,%xmm15\n.Ltail_no_xor_avx:\n\tvpunpckhqdq\t%xmm15,%xmm15,%xmm8\n\tvpxor\t%xmm0,%xmm3,%xmm3\n\tvpclmulqdq\t$0x00,%xmm6,%xmm15,%xmm0\n\tvpxor\t%xmm15,%xmm8,%xmm8\n\tvpxor\t%xmm1,%xmm4,%xmm4\n\tvpclmulqdq\t$0x11,%xmm6,%xmm15,%xmm1\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\tvpclmulqdq\t$0x00,%xmm7,%xmm8,%xmm2\n\n\tvmovdqu\t(%r10),%xmm12\n\n\tvpxor\t%xmm0,%xmm3,%xmm10\n\tvpxor\t%xmm1,%xmm4,%xmm11\n\tvpxor\t%xmm2,%xmm5,%xmm5\n\n\tvpxor\t%xmm10,%xmm5,%xmm5\n\tvpxor\t%xmm11,%xmm5,%xmm5\n\tvpslldq\t$8,%xmm5,%xmm9\n\tvpsrldq\t$8,%xmm5,%xmm5\n\tvpxor\t%xmm9,%xmm10,%xmm10\n\tvpxor\t%xmm5,%xmm11,%xmm11\n\n\tvpclmulqdq\t$0x10,%xmm12,%xmm10,%xmm9\n\tvpalignr\t$8,%xmm10,%xmm10,%xmm10\n\tvpxor\t%xmm9,%xmm10,%xmm10\n\n\tvpclmulqdq\t$0x10,%xmm12,%xmm10,%xmm9\n\tvpalignr\t$8,%xmm10,%xmm10,%xmm10\n\tvpxor\t%xmm11,%xmm10,%xmm10\n\tvpxor\t%xmm9,%xmm10,%xmm10\n\n\tcmpq\t$0,%rcx\n\tjne\t.Lshort_avx\n\n\tvpshufb\t%xmm13,%xmm10,%xmm10\n\tvmovdqu\t%xmm10,(%rdi)\n\tvzeroupper\n\tRET\n.cfi_endproc\nSET_SIZE(gcm_ghash_avx)\n\n#endif  \n\nSECTION_STATIC\n.balign\t64\n.Lbswap_mask:\n.byte\t15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0\n.L0x1c2_polynomial:\n.byte\t1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xc2\n.L7_mask:\n.long\t7,0,7,0\n.L7_mask_poly:\n.long\t7,0,450,0\n.balign\t64\nSET_OBJ(.Lrem_4bit)\n.Lrem_4bit:\n.long\t0,0,0,471859200,0,943718400,0,610271232\n.long\t0,1887436800,0,1822425088,0,1220542464,0,1423966208\n.long\t0,3774873600,0,4246732800,0,3644850176,0,3311403008\n.long\t0,2441084928,0,2376073216,0,2847932416,0,3051356160\nSET_OBJ(.Lrem_8bit)\n.Lrem_8bit:\n.value\t0x0000,0x01C2,0x0384,0x0246,0x0708,0x06CA,0x048C,0x054E\n.value\t0x0E10,0x0FD2,0x0D94,0x0C56,0x0918,0x08DA,0x0A9C,0x0B5E\n.value\t0x1C20,0x1DE2,0x1FA4,0x1E66,0x1B28,0x1AEA,0x18AC,0x196E\n.value\t0x1230,0x13F2,0x11B4,0x1076,0x1538,0x14FA,0x16BC,0x177E\n.value\t0x3840,0x3982,0x3BC4,0x3A06,0x3F48,0x3E8A,0x3CCC,0x3D0E\n.value\t0x3650,0x3792,0x35D4,0x3416,0x3158,0x309A,0x32DC,0x331E\n.value\t0x2460,0x25A2,0x27E4,0x2626,0x2368,0x22AA,0x20EC,0x212E\n.value\t0x2A70,0x2BB2,0x29F4,0x2836,0x2D78,0x2CBA,0x2EFC,0x2F3E\n.value\t0x7080,0x7142,0x7304,0x72C6,0x7788,0x764A,0x740C,0x75CE\n.value\t0x7E90,0x7F52,0x7D14,0x7CD6,0x7998,0x785A,0x7A1C,0x7BDE\n.value\t0x6CA0,0x6D62,0x6F24,0x6EE6,0x6BA8,0x6A6A,0x682C,0x69EE\n.value\t0x62B0,0x6372,0x6134,0x60F6,0x65B8,0x647A,0x663C,0x67FE\n.value\t0x48C0,0x4902,0x4B44,0x4A86,0x4FC8,0x4E0A,0x4C4C,0x4D8E\n.value\t0x46D0,0x4712,0x4554,0x4496,0x41D8,0x401A,0x425C,0x439E\n.value\t0x54E0,0x5522,0x5764,0x56A6,0x53E8,0x522A,0x506C,0x51AE\n.value\t0x5AF0,0x5B32,0x5974,0x58B6,0x5DF8,0x5C3A,0x5E7C,0x5FBE\n.value\t0xE100,0xE0C2,0xE284,0xE346,0xE608,0xE7CA,0xE58C,0xE44E\n.value\t0xEF10,0xEED2,0xEC94,0xED56,0xE818,0xE9DA,0xEB9C,0xEA5E\n.value\t0xFD20,0xFCE2,0xFEA4,0xFF66,0xFA28,0xFBEA,0xF9AC,0xF86E\n.value\t0xF330,0xF2F2,0xF0B4,0xF176,0xF438,0xF5FA,0xF7BC,0xF67E\n.value\t0xD940,0xD882,0xDAC4,0xDB06,0xDE48,0xDF8A,0xDDCC,0xDC0E\n.value\t0xD750,0xD692,0xD4D4,0xD516,0xD058,0xD19A,0xD3DC,0xD21E\n.value\t0xC560,0xC4A2,0xC6E4,0xC726,0xC268,0xC3AA,0xC1EC,0xC02E\n.value\t0xCB70,0xCAB2,0xC8F4,0xC936,0xCC78,0xCDBA,0xCFFC,0xCE3E\n.value\t0x9180,0x9042,0x9204,0x93C6,0x9688,0x974A,0x950C,0x94CE\n.value\t0x9F90,0x9E52,0x9C14,0x9DD6,0x9898,0x995A,0x9B1C,0x9ADE\n.value\t0x8DA0,0x8C62,0x8E24,0x8FE6,0x8AA8,0x8B6A,0x892C,0x88EE\n.value\t0x83B0,0x8272,0x8034,0x81F6,0x84B8,0x857A,0x873C,0x86FE\n.value\t0xA9C0,0xA802,0xAA44,0xAB86,0xAEC8,0xAF0A,0xAD4C,0xAC8E\n.value\t0xA7D0,0xA612,0xA454,0xA596,0xA0D8,0xA11A,0xA35C,0xA29E\n.value\t0xB5E0,0xB422,0xB664,0xB7A6,0xB2E8,0xB32A,0xB16C,0xB0AE\n.value\t0xBBF0,0xBA32,0xB874,0xB9B6,0xBCF8,0xBD3A,0xBF7C,0xBEBE\n\n.byte\t71,72,65,83,72,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0\n.balign\t64\n\n \n#if defined(__linux__) && defined(__ELF__)\n.section .note.GNU-stack,\"\",%progbits\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}