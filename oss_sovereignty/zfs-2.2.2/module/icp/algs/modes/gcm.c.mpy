{
  "module_name": "gcm.c",
  "hash_id": "ebdc8701ca7b9a0f59531501c01da06a28820381459a9bb09138abd44eaf6e86",
  "original_prompt": "Ingested from zfs-2.2.2/module/icp/algs/modes/gcm.c",
  "human_readable_source": " \n \n\n#include <sys/zfs_context.h>\n#include <sys/cmn_err.h>\n#include <modes/modes.h>\n#include <sys/crypto/common.h>\n#include <sys/crypto/icp.h>\n#include <sys/crypto/impl.h>\n#include <sys/byteorder.h>\n#include <sys/simd.h>\n#include <modes/gcm_impl.h>\n#ifdef CAN_USE_GCM_ASM\n#include <aes/aes_impl.h>\n#endif\n\n#define\tGHASH(c, d, t, o) \\\n\txor_block((uint8_t *)(d), (uint8_t *)(c)->gcm_ghash); \\\n\t(o)->mul((uint64_t *)(void *)(c)->gcm_ghash, (c)->gcm_H, \\\n\t(uint64_t *)(void *)(t));\n\n \n#define\tIMPL_FASTEST\t(UINT32_MAX)\n#define\tIMPL_CYCLE\t(UINT32_MAX-1)\n#ifdef CAN_USE_GCM_ASM\n#define\tIMPL_AVX\t(UINT32_MAX-2)\n#endif\n#define\tGCM_IMPL_READ(i) (*(volatile uint32_t *) &(i))\nstatic uint32_t icp_gcm_impl = IMPL_FASTEST;\nstatic uint32_t user_sel_impl = IMPL_FASTEST;\n\nstatic inline int gcm_init_ctx_impl(boolean_t, gcm_ctx_t *, char *, size_t,\n    int (*)(const void *, const uint8_t *, uint8_t *),\n    void (*)(uint8_t *, uint8_t *),\n    void (*)(uint8_t *, uint8_t *));\n\n#ifdef CAN_USE_GCM_ASM\n \nboolean_t gcm_avx_can_use_movbe = B_FALSE;\n \nstatic boolean_t gcm_use_avx = B_FALSE;\n#define\tGCM_IMPL_USE_AVX\t(*(volatile boolean_t *)&gcm_use_avx)\n\nextern boolean_t ASMABI atomic_toggle_boolean_nv(volatile boolean_t *);\n\nstatic inline boolean_t gcm_avx_will_work(void);\nstatic inline void gcm_set_avx(boolean_t);\nstatic inline boolean_t gcm_toggle_avx(void);\nstatic inline size_t gcm_simd_get_htab_size(boolean_t);\n\nstatic int gcm_mode_encrypt_contiguous_blocks_avx(gcm_ctx_t *, char *, size_t,\n    crypto_data_t *, size_t);\n\nstatic int gcm_encrypt_final_avx(gcm_ctx_t *, crypto_data_t *, size_t);\nstatic int gcm_decrypt_final_avx(gcm_ctx_t *, crypto_data_t *, size_t);\nstatic int gcm_init_avx(gcm_ctx_t *, const uint8_t *, size_t, const uint8_t *,\n    size_t, size_t);\n#endif  \n\n \nint\ngcm_mode_encrypt_contiguous_blocks(gcm_ctx_t *ctx, char *data, size_t length,\n    crypto_data_t *out, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n#ifdef CAN_USE_GCM_ASM\n\tif (ctx->gcm_use_avx == B_TRUE)\n\t\treturn (gcm_mode_encrypt_contiguous_blocks_avx(\n\t\t    ctx, data, length, out, block_size));\n#endif\n\n\tconst gcm_impl_ops_t *gops;\n\tsize_t remainder = length;\n\tsize_t need = 0;\n\tuint8_t *datap = (uint8_t *)data;\n\tuint8_t *blockp;\n\tuint8_t *lastp;\n\tvoid *iov_or_mp;\n\toffset_t offset;\n\tuint8_t *out_data_1;\n\tuint8_t *out_data_2;\n\tsize_t out_data_1_len;\n\tuint64_t counter;\n\tuint64_t counter_mask = ntohll(0x00000000ffffffffULL);\n\n\tif (length + ctx->gcm_remainder_len < block_size) {\n\t\t \n\t\tmemcpy((uint8_t *)ctx->gcm_remainder + ctx->gcm_remainder_len,\n\t\t    datap,\n\t\t    length);\n\t\tctx->gcm_remainder_len += length;\n\t\tif (ctx->gcm_copy_to == NULL) {\n\t\t\tctx->gcm_copy_to = datap;\n\t\t}\n\t\treturn (CRYPTO_SUCCESS);\n\t}\n\n\tcrypto_init_ptrs(out, &iov_or_mp, &offset);\n\n\tgops = gcm_impl_get_ops();\n\tdo {\n\t\t \n\t\tif (ctx->gcm_remainder_len > 0) {\n\t\t\tneed = block_size - ctx->gcm_remainder_len;\n\n\t\t\tif (need > remainder)\n\t\t\t\treturn (CRYPTO_DATA_LEN_RANGE);\n\n\t\t\tmemcpy(&((uint8_t *)ctx->gcm_remainder)\n\t\t\t    [ctx->gcm_remainder_len], datap, need);\n\n\t\t\tblockp = (uint8_t *)ctx->gcm_remainder;\n\t\t} else {\n\t\t\tblockp = datap;\n\t\t}\n\n\t\t \n\t\tcounter = ntohll(ctx->gcm_cb[1] & counter_mask);\n\t\tcounter = htonll(counter + 1);\n\t\tcounter &= counter_mask;\n\t\tctx->gcm_cb[1] = (ctx->gcm_cb[1] & ~counter_mask) | counter;\n\n\t\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_cb,\n\t\t    (uint8_t *)ctx->gcm_tmp);\n\t\txor_block(blockp, (uint8_t *)ctx->gcm_tmp);\n\n\t\tlastp = (uint8_t *)ctx->gcm_tmp;\n\n\t\tctx->gcm_processed_data_len += block_size;\n\n\t\tcrypto_get_ptrs(out, &iov_or_mp, &offset, &out_data_1,\n\t\t    &out_data_1_len, &out_data_2, block_size);\n\n\t\t \n\t\tif (out_data_1_len == block_size) {\n\t\t\tcopy_block(lastp, out_data_1);\n\t\t} else {\n\t\t\tmemcpy(out_data_1, lastp, out_data_1_len);\n\t\t\tif (out_data_2 != NULL) {\n\t\t\t\tmemcpy(out_data_2,\n\t\t\t\t    lastp + out_data_1_len,\n\t\t\t\t    block_size - out_data_1_len);\n\t\t\t}\n\t\t}\n\t\t \n\t\tout->cd_offset += block_size;\n\n\t\t \n\t\tGHASH(ctx, ctx->gcm_tmp, ctx->gcm_ghash, gops);\n\n\t\t \n\t\tif (ctx->gcm_remainder_len != 0) {\n\t\t\tdatap += need;\n\t\t\tctx->gcm_remainder_len = 0;\n\t\t} else {\n\t\t\tdatap += block_size;\n\t\t}\n\n\t\tremainder = (size_t)&data[length] - (size_t)datap;\n\n\t\t \n\t\tif (remainder > 0 && remainder < block_size) {\n\t\t\tmemcpy(ctx->gcm_remainder, datap, remainder);\n\t\t\tctx->gcm_remainder_len = remainder;\n\t\t\tctx->gcm_copy_to = datap;\n\t\t\tgoto out;\n\t\t}\n\t\tctx->gcm_copy_to = NULL;\n\n\t} while (remainder > 0);\nout:\n\treturn (CRYPTO_SUCCESS);\n}\n\nint\ngcm_encrypt_final(gcm_ctx_t *ctx, crypto_data_t *out, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\t(void) copy_block;\n#ifdef CAN_USE_GCM_ASM\n\tif (ctx->gcm_use_avx == B_TRUE)\n\t\treturn (gcm_encrypt_final_avx(ctx, out, block_size));\n#endif\n\n\tconst gcm_impl_ops_t *gops;\n\tuint64_t counter_mask = ntohll(0x00000000ffffffffULL);\n\tuint8_t *ghash, *macp = NULL;\n\tint i, rv;\n\n\tif (out->cd_length <\n\t    (ctx->gcm_remainder_len + ctx->gcm_tag_len)) {\n\t\treturn (CRYPTO_DATA_LEN_RANGE);\n\t}\n\n\tgops = gcm_impl_get_ops();\n\tghash = (uint8_t *)ctx->gcm_ghash;\n\n\tif (ctx->gcm_remainder_len > 0) {\n\t\tuint64_t counter;\n\t\tuint8_t *tmpp = (uint8_t *)ctx->gcm_tmp;\n\n\t\t \n\n\t\t \n\t\tcounter = ntohll(ctx->gcm_cb[1] & counter_mask);\n\t\tcounter = htonll(counter + 1);\n\t\tcounter &= counter_mask;\n\t\tctx->gcm_cb[1] = (ctx->gcm_cb[1] & ~counter_mask) | counter;\n\n\t\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_cb,\n\t\t    (uint8_t *)ctx->gcm_tmp);\n\n\t\tmacp = (uint8_t *)ctx->gcm_remainder;\n\t\tmemset(macp + ctx->gcm_remainder_len, 0,\n\t\t    block_size - ctx->gcm_remainder_len);\n\n\t\t \n\t\tfor (i = 0; i < ctx->gcm_remainder_len; i++) {\n\t\t\tmacp[i] ^= tmpp[i];\n\t\t}\n\n\t\t \n\t\tGHASH(ctx, macp, ghash, gops);\n\n\t\tctx->gcm_processed_data_len += ctx->gcm_remainder_len;\n\t}\n\n\tctx->gcm_len_a_len_c[1] =\n\t    htonll(CRYPTO_BYTES2BITS(ctx->gcm_processed_data_len));\n\tGHASH(ctx, ctx->gcm_len_a_len_c, ghash, gops);\n\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_J0,\n\t    (uint8_t *)ctx->gcm_J0);\n\txor_block((uint8_t *)ctx->gcm_J0, ghash);\n\n\tif (ctx->gcm_remainder_len > 0) {\n\t\trv = crypto_put_output_data(macp, out, ctx->gcm_remainder_len);\n\t\tif (rv != CRYPTO_SUCCESS)\n\t\t\treturn (rv);\n\t}\n\tout->cd_offset += ctx->gcm_remainder_len;\n\tctx->gcm_remainder_len = 0;\n\trv = crypto_put_output_data(ghash, out, ctx->gcm_tag_len);\n\tif (rv != CRYPTO_SUCCESS)\n\t\treturn (rv);\n\tout->cd_offset += ctx->gcm_tag_len;\n\n\treturn (CRYPTO_SUCCESS);\n}\n\n \nstatic void\ngcm_decrypt_incomplete_block(gcm_ctx_t *ctx, size_t block_size, size_t index,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\tuint8_t *datap, *outp, *counterp;\n\tuint64_t counter;\n\tuint64_t counter_mask = ntohll(0x00000000ffffffffULL);\n\tint i;\n\n\t \n\tcounter = ntohll(ctx->gcm_cb[1] & counter_mask);\n\tcounter = htonll(counter + 1);\n\tcounter &= counter_mask;\n\tctx->gcm_cb[1] = (ctx->gcm_cb[1] & ~counter_mask) | counter;\n\n\tdatap = (uint8_t *)ctx->gcm_remainder;\n\toutp = &((ctx->gcm_pt_buf)[index]);\n\tcounterp = (uint8_t *)ctx->gcm_tmp;\n\n\t \n\tmemset((uint8_t *)ctx->gcm_tmp, 0, block_size);\n\tmemcpy((uint8_t *)ctx->gcm_tmp, datap, ctx->gcm_remainder_len);\n\n\t \n\tGHASH(ctx, ctx->gcm_tmp, ctx->gcm_ghash, gcm_impl_get_ops());\n\n\t \n\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_cb, counterp);\n\n\t \n\tfor (i = 0; i < ctx->gcm_remainder_len; i++) {\n\t\toutp[i] = datap[i] ^ counterp[i];\n\t}\n}\n\nint\ngcm_mode_decrypt_contiguous_blocks(gcm_ctx_t *ctx, char *data, size_t length,\n    crypto_data_t *out, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\t(void) out, (void) block_size, (void) encrypt_block, (void) copy_block,\n\t    (void) xor_block;\n\tsize_t new_len;\n\tuint8_t *new;\n\n\t \n\tif (length > 0) {\n\t\tnew_len = ctx->gcm_pt_buf_len + length;\n\t\tnew = vmem_alloc(new_len, KM_SLEEP);\n\t\tif (new == NULL) {\n\t\t\tvmem_free(ctx->gcm_pt_buf, ctx->gcm_pt_buf_len);\n\t\t\tctx->gcm_pt_buf = NULL;\n\t\t\treturn (CRYPTO_HOST_MEMORY);\n\t\t}\n\n\t\tif (ctx->gcm_pt_buf != NULL) {\n\t\t\tmemcpy(new, ctx->gcm_pt_buf, ctx->gcm_pt_buf_len);\n\t\t\tvmem_free(ctx->gcm_pt_buf, ctx->gcm_pt_buf_len);\n\t\t} else {\n\t\t\tASSERT0(ctx->gcm_pt_buf_len);\n\t\t}\n\n\t\tctx->gcm_pt_buf = new;\n\t\tctx->gcm_pt_buf_len = new_len;\n\t\tmemcpy(&ctx->gcm_pt_buf[ctx->gcm_processed_data_len], data,\n\t\t    length);\n\t\tctx->gcm_processed_data_len += length;\n\t}\n\n\tctx->gcm_remainder_len = 0;\n\treturn (CRYPTO_SUCCESS);\n}\n\nint\ngcm_decrypt_final(gcm_ctx_t *ctx, crypto_data_t *out, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n#ifdef CAN_USE_GCM_ASM\n\tif (ctx->gcm_use_avx == B_TRUE)\n\t\treturn (gcm_decrypt_final_avx(ctx, out, block_size));\n#endif\n\n\tconst gcm_impl_ops_t *gops;\n\tsize_t pt_len;\n\tsize_t remainder;\n\tuint8_t *ghash;\n\tuint8_t *blockp;\n\tuint8_t *cbp;\n\tuint64_t counter;\n\tuint64_t counter_mask = ntohll(0x00000000ffffffffULL);\n\tint processed = 0, rv;\n\n\tASSERT(ctx->gcm_processed_data_len == ctx->gcm_pt_buf_len);\n\n\tgops = gcm_impl_get_ops();\n\tpt_len = ctx->gcm_processed_data_len - ctx->gcm_tag_len;\n\tghash = (uint8_t *)ctx->gcm_ghash;\n\tblockp = ctx->gcm_pt_buf;\n\tremainder = pt_len;\n\twhile (remainder > 0) {\n\t\t \n\t\tif (remainder < block_size) {\n\t\t\tmemcpy(ctx->gcm_remainder, blockp, remainder);\n\t\t\tctx->gcm_remainder_len = remainder;\n\t\t\t \n\t\t\tgcm_decrypt_incomplete_block(ctx, block_size,\n\t\t\t    processed, encrypt_block, xor_block);\n\t\t\tctx->gcm_remainder_len = 0;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tGHASH(ctx, blockp, ghash, gops);\n\n\t\t \n\t\tcounter = ntohll(ctx->gcm_cb[1] & counter_mask);\n\t\tcounter = htonll(counter + 1);\n\t\tcounter &= counter_mask;\n\t\tctx->gcm_cb[1] = (ctx->gcm_cb[1] & ~counter_mask) | counter;\n\n\t\tcbp = (uint8_t *)ctx->gcm_tmp;\n\t\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_cb, cbp);\n\n\t\t \n\t\txor_block(cbp, blockp);\n\n\t\tprocessed += block_size;\n\t\tblockp += block_size;\n\t\tremainder -= block_size;\n\t}\nout:\n\tctx->gcm_len_a_len_c[1] = htonll(CRYPTO_BYTES2BITS(pt_len));\n\tGHASH(ctx, ctx->gcm_len_a_len_c, ghash, gops);\n\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_J0,\n\t    (uint8_t *)ctx->gcm_J0);\n\txor_block((uint8_t *)ctx->gcm_J0, ghash);\n\n\t \n\tif (memcmp(&ctx->gcm_pt_buf[pt_len], ghash, ctx->gcm_tag_len)) {\n\t\t \n\t\treturn (CRYPTO_INVALID_MAC);\n\t} else {\n\t\trv = crypto_put_output_data(ctx->gcm_pt_buf, out, pt_len);\n\t\tif (rv != CRYPTO_SUCCESS)\n\t\t\treturn (rv);\n\t\tout->cd_offset += pt_len;\n\t}\n\treturn (CRYPTO_SUCCESS);\n}\n\nstatic int\ngcm_validate_args(CK_AES_GCM_PARAMS *gcm_param)\n{\n\tsize_t tag_len;\n\n\t \n\ttag_len = gcm_param->ulTagBits;\n\tswitch (tag_len) {\n\tcase 32:\n\tcase 64:\n\tcase 96:\n\tcase 104:\n\tcase 112:\n\tcase 120:\n\tcase 128:\n\t\tbreak;\n\tdefault:\n\t\treturn (CRYPTO_MECHANISM_PARAM_INVALID);\n\t}\n\n\tif (gcm_param->ulIvLen == 0)\n\t\treturn (CRYPTO_MECHANISM_PARAM_INVALID);\n\n\treturn (CRYPTO_SUCCESS);\n}\n\nstatic void\ngcm_format_initial_blocks(const uint8_t *iv, ulong_t iv_len,\n    gcm_ctx_t *ctx, size_t block_size,\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\tconst gcm_impl_ops_t *gops;\n\tuint8_t *cb;\n\tulong_t remainder = iv_len;\n\tulong_t processed = 0;\n\tuint8_t *datap, *ghash;\n\tuint64_t len_a_len_c[2];\n\n\tgops = gcm_impl_get_ops();\n\tghash = (uint8_t *)ctx->gcm_ghash;\n\tcb = (uint8_t *)ctx->gcm_cb;\n\tif (iv_len == 12) {\n\t\tmemcpy(cb, iv, 12);\n\t\tcb[12] = 0;\n\t\tcb[13] = 0;\n\t\tcb[14] = 0;\n\t\tcb[15] = 1;\n\t\t \n\t\tcopy_block(cb, (uint8_t *)ctx->gcm_J0);\n\t} else {\n\t\t \n\t\tdo {\n\t\t\tif (remainder < block_size) {\n\t\t\t\tmemset(cb, 0, block_size);\n\t\t\t\tmemcpy(cb, &(iv[processed]), remainder);\n\t\t\t\tdatap = (uint8_t *)cb;\n\t\t\t\tremainder = 0;\n\t\t\t} else {\n\t\t\t\tdatap = (uint8_t *)(&(iv[processed]));\n\t\t\t\tprocessed += block_size;\n\t\t\t\tremainder -= block_size;\n\t\t\t}\n\t\t\tGHASH(ctx, datap, ghash, gops);\n\t\t} while (remainder > 0);\n\n\t\tlen_a_len_c[0] = 0;\n\t\tlen_a_len_c[1] = htonll(CRYPTO_BYTES2BITS(iv_len));\n\t\tGHASH(ctx, len_a_len_c, ctx->gcm_J0, gops);\n\n\t\t \n\t\tcopy_block((uint8_t *)ctx->gcm_J0, (uint8_t *)cb);\n\t}\n}\n\nstatic int\ngcm_init(gcm_ctx_t *ctx, const uint8_t *iv, size_t iv_len,\n    const uint8_t *auth_data, size_t auth_data_len, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\tconst gcm_impl_ops_t *gops;\n\tuint8_t *ghash, *datap, *authp;\n\tsize_t remainder, processed;\n\n\t \n\tmemset(ctx->gcm_H, 0, sizeof (ctx->gcm_H));\n\tencrypt_block(ctx->gcm_keysched, (uint8_t *)ctx->gcm_H,\n\t    (uint8_t *)ctx->gcm_H);\n\n\tgcm_format_initial_blocks(iv, iv_len, ctx, block_size,\n\t    copy_block, xor_block);\n\n\tgops = gcm_impl_get_ops();\n\tauthp = (uint8_t *)ctx->gcm_tmp;\n\tghash = (uint8_t *)ctx->gcm_ghash;\n\tmemset(authp, 0, block_size);\n\tmemset(ghash, 0, block_size);\n\n\tprocessed = 0;\n\tremainder = auth_data_len;\n\tdo {\n\t\tif (remainder < block_size) {\n\t\t\t \n\n\t\t\tif (auth_data != NULL) {\n\t\t\t\tmemset(authp, 0, block_size);\n\t\t\t\tmemcpy(authp, &(auth_data[processed]),\n\t\t\t\t    remainder);\n\t\t\t} else {\n\t\t\t\tASSERT0(remainder);\n\t\t\t}\n\n\t\t\tdatap = (uint8_t *)authp;\n\t\t\tremainder = 0;\n\t\t} else {\n\t\t\tdatap = (uint8_t *)(&(auth_data[processed]));\n\t\t\tprocessed += block_size;\n\t\t\tremainder -= block_size;\n\t\t}\n\n\t\t \n\t\tGHASH(ctx, datap, ghash, gops);\n\n\t} while (remainder > 0);\n\n\treturn (CRYPTO_SUCCESS);\n}\n\n \nint\ngcm_init_ctx(gcm_ctx_t *gcm_ctx, char *param, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\treturn (gcm_init_ctx_impl(B_FALSE, gcm_ctx, param, block_size,\n\t    encrypt_block, copy_block, xor_block));\n}\n\n \nint\ngmac_init_ctx(gcm_ctx_t *gcm_ctx, char *param, size_t block_size,\n    int (*encrypt_block)(const void *, const uint8_t *, uint8_t *),\n    void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\treturn (gcm_init_ctx_impl(B_TRUE, gcm_ctx, param, block_size,\n\t    encrypt_block, copy_block, xor_block));\n}\n\n \nstatic inline int\ngcm_init_ctx_impl(boolean_t gmac_mode, gcm_ctx_t *gcm_ctx, char *param,\n    size_t block_size, int (*encrypt_block)(const void *, const uint8_t *,\n    uint8_t *), void (*copy_block)(uint8_t *, uint8_t *),\n    void (*xor_block)(uint8_t *, uint8_t *))\n{\n\tCK_AES_GCM_PARAMS *gcm_param;\n\tint rv = CRYPTO_SUCCESS;\n\tsize_t tag_len, iv_len;\n\n\tif (param != NULL) {\n\t\tgcm_param = (CK_AES_GCM_PARAMS *)(void *)param;\n\n\t\tif (gmac_mode == B_FALSE) {\n\t\t\t \n\t\t\tif ((rv = gcm_validate_args(gcm_param)) != 0) {\n\t\t\t\treturn (rv);\n\t\t\t}\n\t\t\tgcm_ctx->gcm_flags |= GCM_MODE;\n\n\t\t\tsize_t tbits = gcm_param->ulTagBits;\n\t\t\ttag_len = CRYPTO_BITS2BYTES(tbits);\n\t\t\tiv_len = gcm_param->ulIvLen;\n\t\t} else {\n\t\t\t \n\t\t\tgcm_ctx->gcm_flags |= GMAC_MODE;\n\t\t\ttag_len = CRYPTO_BITS2BYTES(AES_GMAC_TAG_BITS);\n\t\t\tiv_len = AES_GMAC_IV_LEN;\n\t\t}\n\t\tgcm_ctx->gcm_tag_len = tag_len;\n\t\tgcm_ctx->gcm_processed_data_len = 0;\n\n\t\t \n\t\tgcm_ctx->gcm_len_a_len_c[0]\n\t\t    = htonll(CRYPTO_BYTES2BITS(gcm_param->ulAADLen));\n\t} else {\n\t\treturn (CRYPTO_MECHANISM_PARAM_INVALID);\n\t}\n\n\tconst uint8_t *iv = (const uint8_t *)gcm_param->pIv;\n\tconst uint8_t *aad = (const uint8_t *)gcm_param->pAAD;\n\tsize_t aad_len = gcm_param->ulAADLen;\n\n#ifdef CAN_USE_GCM_ASM\n\tboolean_t needs_bswap =\n\t    ((aes_key_t *)gcm_ctx->gcm_keysched)->ops->needs_byteswap;\n\n\tif (GCM_IMPL_READ(icp_gcm_impl) != IMPL_CYCLE) {\n\t\tgcm_ctx->gcm_use_avx = GCM_IMPL_USE_AVX;\n\t} else {\n\t\t \n\t\tgcm_ctx->gcm_use_avx = gcm_toggle_avx();\n\n\t\t \n\t\tif (gcm_ctx->gcm_use_avx == B_TRUE && needs_bswap == B_TRUE) {\n\t\t\tgcm_ctx->gcm_use_avx = B_FALSE;\n\t\t}\n\t\t \n\t\tif (gcm_ctx->gcm_use_avx == B_TRUE && gmac_mode == B_FALSE &&\n\t\t    zfs_movbe_available() == B_TRUE) {\n\t\t\t(void) atomic_toggle_boolean_nv(\n\t\t\t    (volatile boolean_t *)&gcm_avx_can_use_movbe);\n\t\t}\n\t}\n\t \n\tif (gcm_ctx->gcm_use_avx == B_TRUE && needs_bswap == B_TRUE) {\n\t\tgcm_ctx->gcm_use_avx = B_FALSE;\n\n\t\tcmn_err_once(CE_WARN,\n\t\t    \"ICP: Can't use the aes generic or cycle implementations \"\n\t\t    \"in combination with the gcm avx implementation!\");\n\t\tcmn_err_once(CE_WARN,\n\t\t    \"ICP: Falling back to a compatible implementation, \"\n\t\t    \"aes-gcm performance will likely be degraded.\");\n\t\tcmn_err_once(CE_WARN,\n\t\t    \"ICP: Choose at least the x86_64 aes implementation to \"\n\t\t    \"restore performance.\");\n\t}\n\n\t \n\tif (gcm_ctx->gcm_use_avx == B_TRUE) {\n\t\tsize_t htab_len = gcm_simd_get_htab_size(gcm_ctx->gcm_use_avx);\n\n\t\tif (htab_len == 0) {\n\t\t\treturn (CRYPTO_MECHANISM_PARAM_INVALID);\n\t\t}\n\t\tgcm_ctx->gcm_htab_len = htab_len;\n\t\tgcm_ctx->gcm_Htable =\n\t\t    kmem_alloc(htab_len, KM_SLEEP);\n\n\t\tif (gcm_ctx->gcm_Htable == NULL) {\n\t\t\treturn (CRYPTO_HOST_MEMORY);\n\t\t}\n\t}\n\t \n\tif (gcm_ctx->gcm_use_avx == B_FALSE) {\n#endif  \n\t\tif (gcm_init(gcm_ctx, iv, iv_len, aad, aad_len, block_size,\n\t\t    encrypt_block, copy_block, xor_block) != CRYPTO_SUCCESS) {\n\t\t\trv = CRYPTO_MECHANISM_PARAM_INVALID;\n\t\t}\n#ifdef CAN_USE_GCM_ASM\n\t} else {\n\t\tif (gcm_init_avx(gcm_ctx, iv, iv_len, aad, aad_len,\n\t\t    block_size) != CRYPTO_SUCCESS) {\n\t\t\trv = CRYPTO_MECHANISM_PARAM_INVALID;\n\t\t}\n\t}\n#endif  \n\n\treturn (rv);\n}\n\nvoid *\ngcm_alloc_ctx(int kmflag)\n{\n\tgcm_ctx_t *gcm_ctx;\n\n\tif ((gcm_ctx = kmem_zalloc(sizeof (gcm_ctx_t), kmflag)) == NULL)\n\t\treturn (NULL);\n\n\tgcm_ctx->gcm_flags = GCM_MODE;\n\treturn (gcm_ctx);\n}\n\nvoid *\ngmac_alloc_ctx(int kmflag)\n{\n\tgcm_ctx_t *gcm_ctx;\n\n\tif ((gcm_ctx = kmem_zalloc(sizeof (gcm_ctx_t), kmflag)) == NULL)\n\t\treturn (NULL);\n\n\tgcm_ctx->gcm_flags = GMAC_MODE;\n\treturn (gcm_ctx);\n}\n\n \nstatic gcm_impl_ops_t gcm_fastest_impl = {\n\t.name = \"fastest\"\n};\n\n \nstatic const gcm_impl_ops_t *gcm_all_impl[] = {\n\t&gcm_generic_impl,\n#if defined(__x86_64) && defined(HAVE_PCLMULQDQ)\n\t&gcm_pclmulqdq_impl,\n#endif\n};\n\n \nstatic boolean_t gcm_impl_initialized = B_FALSE;\n\n \nstatic size_t gcm_supp_impl_cnt = 0;\nstatic gcm_impl_ops_t *gcm_supp_impl[ARRAY_SIZE(gcm_all_impl)];\n\n \nconst gcm_impl_ops_t *\ngcm_impl_get_ops(void)\n{\n\tif (!kfpu_allowed())\n\t\treturn (&gcm_generic_impl);\n\n\tconst gcm_impl_ops_t *ops = NULL;\n\tconst uint32_t impl = GCM_IMPL_READ(icp_gcm_impl);\n\n\tswitch (impl) {\n\tcase IMPL_FASTEST:\n\t\tASSERT(gcm_impl_initialized);\n\t\tops = &gcm_fastest_impl;\n\t\tbreak;\n\tcase IMPL_CYCLE:\n\t\t \n\t\tASSERT(gcm_impl_initialized);\n\t\tASSERT3U(gcm_supp_impl_cnt, >, 0);\n\t\tstatic size_t cycle_impl_idx = 0;\n\t\tsize_t idx = (++cycle_impl_idx) % gcm_supp_impl_cnt;\n\t\tops = gcm_supp_impl[idx];\n\t\tbreak;\n#ifdef CAN_USE_GCM_ASM\n\tcase IMPL_AVX:\n\t\t \n\t\tops = &gcm_generic_impl;\n\t\tbreak;\n#endif\n\tdefault:\n\t\tASSERT3U(impl, <, gcm_supp_impl_cnt);\n\t\tASSERT3U(gcm_supp_impl_cnt, >, 0);\n\t\tif (impl < ARRAY_SIZE(gcm_all_impl))\n\t\t\tops = gcm_supp_impl[impl];\n\t\tbreak;\n\t}\n\n\tASSERT3P(ops, !=, NULL);\n\n\treturn (ops);\n}\n\n \nvoid\ngcm_impl_init(void)\n{\n\tgcm_impl_ops_t *curr_impl;\n\tint i, c;\n\n\t \n\tfor (i = 0, c = 0; i < ARRAY_SIZE(gcm_all_impl); i++) {\n\t\tcurr_impl = (gcm_impl_ops_t *)gcm_all_impl[i];\n\n\t\tif (curr_impl->is_supported())\n\t\t\tgcm_supp_impl[c++] = (gcm_impl_ops_t *)curr_impl;\n\t}\n\tgcm_supp_impl_cnt = c;\n\n\t \n#if defined(__x86_64) && defined(HAVE_PCLMULQDQ)\n\tif (gcm_pclmulqdq_impl.is_supported()) {\n\t\tmemcpy(&gcm_fastest_impl, &gcm_pclmulqdq_impl,\n\t\t    sizeof (gcm_fastest_impl));\n\t} else\n#endif\n\t{\n\t\tmemcpy(&gcm_fastest_impl, &gcm_generic_impl,\n\t\t    sizeof (gcm_fastest_impl));\n\t}\n\n\tstrlcpy(gcm_fastest_impl.name, \"fastest\", GCM_IMPL_NAME_MAX);\n\n#ifdef CAN_USE_GCM_ASM\n\t \n\tif (gcm_avx_will_work()) {\n#ifdef HAVE_MOVBE\n\t\tif (zfs_movbe_available() == B_TRUE) {\n\t\t\tatomic_swap_32(&gcm_avx_can_use_movbe, B_TRUE);\n\t\t}\n#endif\n\t\tif (GCM_IMPL_READ(user_sel_impl) == IMPL_FASTEST) {\n\t\t\tgcm_set_avx(B_TRUE);\n\t\t}\n\t}\n#endif\n\t \n\tatomic_swap_32(&icp_gcm_impl, user_sel_impl);\n\tgcm_impl_initialized = B_TRUE;\n}\n\nstatic const struct {\n\tconst char *name;\n\tuint32_t sel;\n} gcm_impl_opts[] = {\n\t\t{ \"cycle\",\tIMPL_CYCLE },\n\t\t{ \"fastest\",\tIMPL_FASTEST },\n#ifdef CAN_USE_GCM_ASM\n\t\t{ \"avx\",\tIMPL_AVX },\n#endif\n};\n\n \nint\ngcm_impl_set(const char *val)\n{\n\tint err = -EINVAL;\n\tchar req_name[GCM_IMPL_NAME_MAX];\n\tuint32_t impl = GCM_IMPL_READ(user_sel_impl);\n\tsize_t i;\n\n\t \n\ti = strnlen(val, GCM_IMPL_NAME_MAX);\n\tif (i == 0 || i >= GCM_IMPL_NAME_MAX)\n\t\treturn (err);\n\n\tstrlcpy(req_name, val, GCM_IMPL_NAME_MAX);\n\twhile (i > 0 && isspace(req_name[i-1]))\n\t\ti--;\n\treq_name[i] = '\\0';\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(gcm_impl_opts); i++) {\n#ifdef CAN_USE_GCM_ASM\n\t\t \n\t\tif (gcm_impl_opts[i].sel == IMPL_AVX && !gcm_avx_will_work()) {\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tif (strcmp(req_name, gcm_impl_opts[i].name) == 0) {\n\t\t\timpl = gcm_impl_opts[i].sel;\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (err != 0 && gcm_impl_initialized) {\n\t\t \n\t\tfor (i = 0; i < gcm_supp_impl_cnt; i++) {\n\t\t\tif (strcmp(req_name, gcm_supp_impl[i]->name) == 0) {\n\t\t\t\timpl = i;\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n#ifdef CAN_USE_GCM_ASM\n\t \n\tif (gcm_avx_will_work() == B_TRUE &&\n\t    (impl == IMPL_AVX || impl == IMPL_FASTEST)) {\n\t\tgcm_set_avx(B_TRUE);\n\t} else {\n\t\tgcm_set_avx(B_FALSE);\n\t}\n#endif\n\n\tif (err == 0) {\n\t\tif (gcm_impl_initialized)\n\t\t\tatomic_swap_32(&icp_gcm_impl, impl);\n\t\telse\n\t\t\tatomic_swap_32(&user_sel_impl, impl);\n\t}\n\n\treturn (err);\n}\n\n#if defined(_KERNEL) && defined(__linux__)\n\nstatic int\nicp_gcm_impl_set(const char *val, zfs_kernel_param_t *kp)\n{\n\treturn (gcm_impl_set(val));\n}\n\nstatic int\nicp_gcm_impl_get(char *buffer, zfs_kernel_param_t *kp)\n{\n\tint i, cnt = 0;\n\tchar *fmt;\n\tconst uint32_t impl = GCM_IMPL_READ(icp_gcm_impl);\n\n\tASSERT(gcm_impl_initialized);\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(gcm_impl_opts); i++) {\n#ifdef CAN_USE_GCM_ASM\n\t\t \n\t\tif (gcm_impl_opts[i].sel == IMPL_AVX && !gcm_avx_will_work()) {\n\t\t\tcontinue;\n\t\t}\n#endif\n\t\tfmt = (impl == gcm_impl_opts[i].sel) ? \"[%s] \" : \"%s \";\n\t\tcnt += kmem_scnprintf(buffer + cnt, PAGE_SIZE - cnt, fmt,\n\t\t    gcm_impl_opts[i].name);\n\t}\n\n\t \n\tfor (i = 0; i < gcm_supp_impl_cnt; i++) {\n\t\tfmt = (i == impl) ? \"[%s] \" : \"%s \";\n\t\tcnt += kmem_scnprintf(buffer + cnt, PAGE_SIZE - cnt, fmt,\n\t\t    gcm_supp_impl[i]->name);\n\t}\n\n\treturn (cnt);\n}\n\nmodule_param_call(icp_gcm_impl, icp_gcm_impl_set, icp_gcm_impl_get,\n    NULL, 0644);\nMODULE_PARM_DESC(icp_gcm_impl, \"Select gcm implementation.\");\n#endif  \n\n#ifdef CAN_USE_GCM_ASM\n#define\tGCM_BLOCK_LEN 16\n \n#define\tGCM_AVX_MIN_DECRYPT_BYTES (GCM_BLOCK_LEN * 6)\n#define\tGCM_AVX_MIN_ENCRYPT_BYTES (GCM_BLOCK_LEN * 6 * 3)\n \n#define\tGCM_AVX_MAX_CHUNK_SIZE \\\n\t(((128*1024)/GCM_AVX_MIN_DECRYPT_BYTES) * GCM_AVX_MIN_DECRYPT_BYTES)\n\n \n#define\tclear_fpu_regs() clear_fpu_regs_avx()\n#define\tGHASH_AVX(ctx, in, len) \\\n    gcm_ghash_avx((ctx)->gcm_ghash, (const uint64_t *)(ctx)->gcm_Htable, \\\n    in, len)\n\n#define\tgcm_incr_counter_block(ctx) gcm_incr_counter_block_by(ctx, 1)\n\n \n#define\tGCM_CHUNK_SIZE_READ *(volatile uint32_t *) &gcm_avx_chunk_size\n\n \nstatic uint32_t gcm_avx_chunk_size =\n\t((32 * 1024) / GCM_AVX_MIN_DECRYPT_BYTES) * GCM_AVX_MIN_DECRYPT_BYTES;\n\nextern void ASMABI clear_fpu_regs_avx(void);\nextern void ASMABI gcm_xor_avx(const uint8_t *src, uint8_t *dst);\nextern void ASMABI aes_encrypt_intel(const uint32_t rk[], int nr,\n    const uint32_t pt[4], uint32_t ct[4]);\n\nextern void ASMABI gcm_init_htab_avx(uint64_t *Htable, const uint64_t H[2]);\nextern void ASMABI gcm_ghash_avx(uint64_t ghash[2], const uint64_t *Htable,\n    const uint8_t *in, size_t len);\n\nextern size_t ASMABI aesni_gcm_encrypt(const uint8_t *, uint8_t *, size_t,\n    const void *, uint64_t *, uint64_t *);\n\nextern size_t ASMABI aesni_gcm_decrypt(const uint8_t *, uint8_t *, size_t,\n    const void *, uint64_t *, uint64_t *);\n\nstatic inline boolean_t\ngcm_avx_will_work(void)\n{\n\t \n\treturn (kfpu_allowed() &&\n\t    zfs_avx_available() && zfs_aes_available() &&\n\t    zfs_pclmulqdq_available());\n}\n\nstatic inline void\ngcm_set_avx(boolean_t val)\n{\n\tif (gcm_avx_will_work() == B_TRUE) {\n\t\tatomic_swap_32(&gcm_use_avx, val);\n\t}\n}\n\nstatic inline boolean_t\ngcm_toggle_avx(void)\n{\n\tif (gcm_avx_will_work() == B_TRUE) {\n\t\treturn (atomic_toggle_boolean_nv(&GCM_IMPL_USE_AVX));\n\t} else {\n\t\treturn (B_FALSE);\n\t}\n}\n\nstatic inline size_t\ngcm_simd_get_htab_size(boolean_t simd_mode)\n{\n\tswitch (simd_mode) {\n\tcase B_TRUE:\n\t\treturn (2 * 6 * 2 * sizeof (uint64_t));\n\n\tdefault:\n\t\treturn (0);\n\t}\n}\n\n\n \nstatic inline void\ngcm_incr_counter_block_by(gcm_ctx_t *ctx, int n)\n{\n\tuint64_t counter_mask = ntohll(0x00000000ffffffffULL);\n\tuint64_t counter = ntohll(ctx->gcm_cb[1] & counter_mask);\n\n\tcounter = htonll(counter + n);\n\tcounter &= counter_mask;\n\tctx->gcm_cb[1] = (ctx->gcm_cb[1] & ~counter_mask) | counter;\n}\n\n \nstatic int\ngcm_mode_encrypt_contiguous_blocks_avx(gcm_ctx_t *ctx, char *data,\n    size_t length, crypto_data_t *out, size_t block_size)\n{\n\tsize_t bleft = length;\n\tsize_t need = 0;\n\tsize_t done = 0;\n\tuint8_t *datap = (uint8_t *)data;\n\tsize_t chunk_size = (size_t)GCM_CHUNK_SIZE_READ;\n\tconst aes_key_t *key = ((aes_key_t *)ctx->gcm_keysched);\n\tuint64_t *ghash = ctx->gcm_ghash;\n\tuint64_t *cb = ctx->gcm_cb;\n\tuint8_t *ct_buf = NULL;\n\tuint8_t *tmp = (uint8_t *)ctx->gcm_tmp;\n\tint rv = CRYPTO_SUCCESS;\n\n\tASSERT(block_size == GCM_BLOCK_LEN);\n\tASSERT3S(((aes_key_t *)ctx->gcm_keysched)->ops->needs_byteswap, ==,\n\t    B_FALSE);\n\t \n\tif (ctx->gcm_remainder_len > 0) {\n\t\tneed = block_size - ctx->gcm_remainder_len;\n\t\tif (length < need) {\n\t\t\t \n\t\t\tmemcpy((uint8_t *)ctx->gcm_remainder +\n\t\t\t    ctx->gcm_remainder_len, datap, length);\n\n\t\t\tctx->gcm_remainder_len += length;\n\t\t\tif (ctx->gcm_copy_to == NULL) {\n\t\t\t\tctx->gcm_copy_to = datap;\n\t\t\t}\n\t\t\treturn (CRYPTO_SUCCESS);\n\t\t} else {\n\t\t\t \n\t\t\tmemcpy((uint8_t *)ctx->gcm_remainder +\n\t\t\t    ctx->gcm_remainder_len, datap, need);\n\n\t\t\tctx->gcm_copy_to = NULL;\n\t\t}\n\t}\n\n\t \n\tif (bleft >= GCM_AVX_MIN_ENCRYPT_BYTES) {\n\t\tct_buf = vmem_alloc(chunk_size, KM_SLEEP);\n\t\tif (ct_buf == NULL) {\n\t\t\treturn (CRYPTO_HOST_MEMORY);\n\t\t}\n\t}\n\n\t \n\tif (ctx->gcm_remainder_len > 0) {\n\t\tkfpu_begin();\n\t\taes_encrypt_intel(key->encr_ks.ks32, key->nr,\n\t\t    (const uint32_t *)cb, (uint32_t *)tmp);\n\n\t\tgcm_xor_avx((const uint8_t *) ctx->gcm_remainder, tmp);\n\t\tGHASH_AVX(ctx, tmp, block_size);\n\t\tclear_fpu_regs();\n\t\tkfpu_end();\n\t\trv = crypto_put_output_data(tmp, out, block_size);\n\t\tout->cd_offset += block_size;\n\t\tgcm_incr_counter_block(ctx);\n\t\tctx->gcm_processed_data_len += block_size;\n\t\tbleft -= need;\n\t\tdatap += need;\n\t\tctx->gcm_remainder_len = 0;\n\t}\n\n\t \n\tfor (; bleft >= chunk_size; bleft -= chunk_size) {\n\t\tkfpu_begin();\n\t\tdone = aesni_gcm_encrypt(\n\t\t    datap, ct_buf, chunk_size, key, cb, ghash);\n\n\t\tclear_fpu_regs();\n\t\tkfpu_end();\n\t\tif (done != chunk_size) {\n\t\t\trv = CRYPTO_FAILED;\n\t\t\tgoto out_nofpu;\n\t\t}\n\t\trv = crypto_put_output_data(ct_buf, out, chunk_size);\n\t\tif (rv != CRYPTO_SUCCESS) {\n\t\t\tgoto out_nofpu;\n\t\t}\n\t\tout->cd_offset += chunk_size;\n\t\tdatap += chunk_size;\n\t\tctx->gcm_processed_data_len += chunk_size;\n\t}\n\t \n\tif (bleft == 0) {\n\t\tgoto out_nofpu;\n\t}\n\t \n\tkfpu_begin();\n\tif (bleft >= GCM_AVX_MIN_ENCRYPT_BYTES) {\n\t\tdone = aesni_gcm_encrypt(datap, ct_buf, bleft, key, cb, ghash);\n\t\tif (done == 0) {\n\t\t\trv = CRYPTO_FAILED;\n\t\t\tgoto out;\n\t\t}\n\t\trv = crypto_put_output_data(ct_buf, out, done);\n\t\tif (rv != CRYPTO_SUCCESS) {\n\t\t\tgoto out;\n\t\t}\n\t\tout->cd_offset += done;\n\t\tctx->gcm_processed_data_len += done;\n\t\tdatap += done;\n\t\tbleft -= done;\n\n\t}\n\t \n\twhile (bleft > 0) {\n\t\tif (bleft < block_size) {\n\t\t\tmemcpy(ctx->gcm_remainder, datap, bleft);\n\t\t\tctx->gcm_remainder_len = bleft;\n\t\t\tctx->gcm_copy_to = datap;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\taes_encrypt_intel(key->encr_ks.ks32, key->nr,\n\t\t    (const uint32_t *)cb, (uint32_t *)tmp);\n\n\t\tgcm_xor_avx(datap, tmp);\n\t\tGHASH_AVX(ctx, tmp, block_size);\n\t\trv = crypto_put_output_data(tmp, out, block_size);\n\t\tif (rv != CRYPTO_SUCCESS) {\n\t\t\tgoto out;\n\t\t}\n\t\tout->cd_offset += block_size;\n\t\tgcm_incr_counter_block(ctx);\n\t\tctx->gcm_processed_data_len += block_size;\n\t\tdatap += block_size;\n\t\tbleft -= block_size;\n\t}\nout:\n\tclear_fpu_regs();\n\tkfpu_end();\nout_nofpu:\n\tif (ct_buf != NULL) {\n\t\tvmem_free(ct_buf, chunk_size);\n\t}\n\treturn (rv);\n}\n\n \nstatic int\ngcm_encrypt_final_avx(gcm_ctx_t *ctx, crypto_data_t *out, size_t block_size)\n{\n\tuint8_t *ghash = (uint8_t *)ctx->gcm_ghash;\n\tuint32_t *J0 = (uint32_t *)ctx->gcm_J0;\n\tuint8_t *remainder = (uint8_t *)ctx->gcm_remainder;\n\tsize_t rem_len = ctx->gcm_remainder_len;\n\tconst void *keysched = ((aes_key_t *)ctx->gcm_keysched)->encr_ks.ks32;\n\tint aes_rounds = ((aes_key_t *)keysched)->nr;\n\tint rv;\n\n\tASSERT(block_size == GCM_BLOCK_LEN);\n\tASSERT3S(((aes_key_t *)ctx->gcm_keysched)->ops->needs_byteswap, ==,\n\t    B_FALSE);\n\n\tif (out->cd_length < (rem_len + ctx->gcm_tag_len)) {\n\t\treturn (CRYPTO_DATA_LEN_RANGE);\n\t}\n\n\tkfpu_begin();\n\t \n\tif (rem_len > 0) {\n\t\tuint8_t *tmp = (uint8_t *)ctx->gcm_tmp;\n\t\tconst uint32_t *cb = (uint32_t *)ctx->gcm_cb;\n\n\t\taes_encrypt_intel(keysched, aes_rounds, cb, (uint32_t *)tmp);\n\t\tmemset(remainder + rem_len, 0, block_size - rem_len);\n\t\tfor (int i = 0; i < rem_len; i++) {\n\t\t\tremainder[i] ^= tmp[i];\n\t\t}\n\t\tGHASH_AVX(ctx, remainder, block_size);\n\t\tctx->gcm_processed_data_len += rem_len;\n\t\t \n\t}\n\t \n\tctx->gcm_len_a_len_c[1] =\n\t    htonll(CRYPTO_BYTES2BITS(ctx->gcm_processed_data_len));\n\tGHASH_AVX(ctx, (const uint8_t *)ctx->gcm_len_a_len_c, block_size);\n\taes_encrypt_intel(keysched, aes_rounds, J0, J0);\n\n\tgcm_xor_avx((uint8_t *)J0, ghash);\n\tclear_fpu_regs();\n\tkfpu_end();\n\n\t \n\tif (rem_len > 0) {\n\t\trv = crypto_put_output_data(remainder, out, rem_len);\n\t\tif (rv != CRYPTO_SUCCESS)\n\t\t\treturn (rv);\n\t}\n\tout->cd_offset += rem_len;\n\tctx->gcm_remainder_len = 0;\n\trv = crypto_put_output_data(ghash, out, ctx->gcm_tag_len);\n\tif (rv != CRYPTO_SUCCESS)\n\t\treturn (rv);\n\n\tout->cd_offset += ctx->gcm_tag_len;\n\treturn (CRYPTO_SUCCESS);\n}\n\n \nstatic int\ngcm_decrypt_final_avx(gcm_ctx_t *ctx, crypto_data_t *out, size_t block_size)\n{\n\tASSERT3U(ctx->gcm_processed_data_len, ==, ctx->gcm_pt_buf_len);\n\tASSERT3U(block_size, ==, 16);\n\tASSERT3S(((aes_key_t *)ctx->gcm_keysched)->ops->needs_byteswap, ==,\n\t    B_FALSE);\n\n\tsize_t chunk_size = (size_t)GCM_CHUNK_SIZE_READ;\n\tsize_t pt_len = ctx->gcm_processed_data_len - ctx->gcm_tag_len;\n\tuint8_t *datap = ctx->gcm_pt_buf;\n\tconst aes_key_t *key = ((aes_key_t *)ctx->gcm_keysched);\n\tuint32_t *cb = (uint32_t *)ctx->gcm_cb;\n\tuint64_t *ghash = ctx->gcm_ghash;\n\tuint32_t *tmp = (uint32_t *)ctx->gcm_tmp;\n\tint rv = CRYPTO_SUCCESS;\n\tsize_t bleft, done;\n\n\t \n\tfor (bleft = pt_len; bleft >= chunk_size; bleft -= chunk_size) {\n\t\tkfpu_begin();\n\t\tdone = aesni_gcm_decrypt(datap, datap, chunk_size,\n\t\t    (const void *)key, ctx->gcm_cb, ghash);\n\t\tclear_fpu_regs();\n\t\tkfpu_end();\n\t\tif (done != chunk_size) {\n\t\t\treturn (CRYPTO_FAILED);\n\t\t}\n\t\tdatap += done;\n\t}\n\t \n\tkfpu_begin();\n\tif (bleft >= GCM_AVX_MIN_DECRYPT_BYTES) {\n\t\tdone = aesni_gcm_decrypt(datap, datap, bleft,\n\t\t    (const void *)key, ctx->gcm_cb, ghash);\n\t\tif (done == 0) {\n\t\t\tclear_fpu_regs();\n\t\t\tkfpu_end();\n\t\t\treturn (CRYPTO_FAILED);\n\t\t}\n\t\tdatap += done;\n\t\tbleft -= done;\n\t}\n\tASSERT(bleft < GCM_AVX_MIN_DECRYPT_BYTES);\n\n\t \n\twhile (bleft > 0) {\n\t\t \n\t\tif (bleft < block_size) {\n\t\t\tuint8_t *lastb = (uint8_t *)ctx->gcm_remainder;\n\n\t\t\tmemset(lastb, 0, block_size);\n\t\t\tmemcpy(lastb, datap, bleft);\n\t\t\t \n\t\t\tGHASH_AVX(ctx, lastb, block_size);\n\t\t\taes_encrypt_intel(key->encr_ks.ks32, key->nr, cb, tmp);\n\t\t\tfor (size_t i = 0; i < bleft; i++) {\n\t\t\t\tdatap[i] = lastb[i] ^ ((uint8_t *)tmp)[i];\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tGHASH_AVX(ctx, datap, block_size);\n\t\taes_encrypt_intel(key->encr_ks.ks32, key->nr, cb, tmp);\n\t\tgcm_xor_avx((uint8_t *)tmp, datap);\n\t\tgcm_incr_counter_block(ctx);\n\n\t\tdatap += block_size;\n\t\tbleft -= block_size;\n\t}\n\tif (rv != CRYPTO_SUCCESS) {\n\t\tclear_fpu_regs();\n\t\tkfpu_end();\n\t\treturn (rv);\n\t}\n\t \n\tctx->gcm_len_a_len_c[1] = htonll(CRYPTO_BYTES2BITS(pt_len));\n\tGHASH_AVX(ctx, (uint8_t *)ctx->gcm_len_a_len_c, block_size);\n\taes_encrypt_intel(key->encr_ks.ks32, key->nr, (uint32_t *)ctx->gcm_J0,\n\t    (uint32_t *)ctx->gcm_J0);\n\n\tgcm_xor_avx((uint8_t *)ctx->gcm_J0, (uint8_t *)ghash);\n\n\t \n\tclear_fpu_regs();\n\tkfpu_end();\n\n\t \n\tif (memcmp(&ctx->gcm_pt_buf[pt_len], ghash, ctx->gcm_tag_len)) {\n\t\t \n\t\treturn (CRYPTO_INVALID_MAC);\n\t}\n\trv = crypto_put_output_data(ctx->gcm_pt_buf, out, pt_len);\n\tif (rv != CRYPTO_SUCCESS) {\n\t\treturn (rv);\n\t}\n\tout->cd_offset += pt_len;\n\treturn (CRYPTO_SUCCESS);\n}\n\n \nstatic int\ngcm_init_avx(gcm_ctx_t *ctx, const uint8_t *iv, size_t iv_len,\n    const uint8_t *auth_data, size_t auth_data_len, size_t block_size)\n{\n\tuint8_t *cb = (uint8_t *)ctx->gcm_cb;\n\tuint64_t *H = ctx->gcm_H;\n\tconst void *keysched = ((aes_key_t *)ctx->gcm_keysched)->encr_ks.ks32;\n\tint aes_rounds = ((aes_key_t *)ctx->gcm_keysched)->nr;\n\tconst uint8_t *datap = auth_data;\n\tsize_t chunk_size = (size_t)GCM_CHUNK_SIZE_READ;\n\tsize_t bleft;\n\n\tASSERT(block_size == GCM_BLOCK_LEN);\n\tASSERT3S(((aes_key_t *)ctx->gcm_keysched)->ops->needs_byteswap, ==,\n\t    B_FALSE);\n\n\t \n\tmemset(ctx->gcm_ghash, 0, sizeof (ctx->gcm_ghash));\n\tmemset(H, 0, sizeof (ctx->gcm_H));\n\tkfpu_begin();\n\taes_encrypt_intel(keysched, aes_rounds,\n\t    (const uint32_t *)H, (uint32_t *)H);\n\n\tgcm_init_htab_avx(ctx->gcm_Htable, H);\n\n\tif (iv_len == 12) {\n\t\tmemcpy(cb, iv, 12);\n\t\tcb[12] = 0;\n\t\tcb[13] = 0;\n\t\tcb[14] = 0;\n\t\tcb[15] = 1;\n\t\t \n\t\tmemcpy(ctx->gcm_J0, cb, sizeof (ctx->gcm_J0));\n\t} else {\n\t\t \n\t\tclear_fpu_regs();\n\t\tkfpu_end();\n\t\tgcm_format_initial_blocks(iv, iv_len, ctx, block_size,\n\t\t    aes_copy_block, aes_xor_block);\n\t\tkfpu_begin();\n\t}\n\n\t \n\tgcm_incr_counter_block(ctx);\n\n\t \n\tfor (bleft = auth_data_len; bleft >= chunk_size; bleft -= chunk_size) {\n\t\tGHASH_AVX(ctx, datap, chunk_size);\n\t\tdatap += chunk_size;\n\t\tclear_fpu_regs();\n\t\tkfpu_end();\n\t\tkfpu_begin();\n\t}\n\t \n\tif (bleft > 0) {\n\t\tsize_t incomp = bleft % block_size;\n\n\t\tbleft -= incomp;\n\t\tif (bleft > 0) {\n\t\t\tGHASH_AVX(ctx, datap, bleft);\n\t\t\tdatap += bleft;\n\t\t}\n\t\tif (incomp > 0) {\n\t\t\t \n\t\t\tuint8_t *authp = (uint8_t *)ctx->gcm_tmp;\n\n\t\t\tmemset(authp, 0, block_size);\n\t\t\tmemcpy(authp, datap, incomp);\n\t\t\tGHASH_AVX(ctx, authp, block_size);\n\t\t}\n\t}\n\tclear_fpu_regs();\n\tkfpu_end();\n\treturn (CRYPTO_SUCCESS);\n}\n\n#if defined(_KERNEL)\nstatic int\nicp_gcm_avx_set_chunk_size(const char *buf, zfs_kernel_param_t *kp)\n{\n\tunsigned long val;\n\tchar val_rounded[16];\n\tint error = 0;\n\n\terror = kstrtoul(buf, 0, &val);\n\tif (error)\n\t\treturn (error);\n\n\tval = (val / GCM_AVX_MIN_DECRYPT_BYTES) * GCM_AVX_MIN_DECRYPT_BYTES;\n\n\tif (val < GCM_AVX_MIN_ENCRYPT_BYTES || val > GCM_AVX_MAX_CHUNK_SIZE)\n\t\treturn (-EINVAL);\n\n\tsnprintf(val_rounded, 16, \"%u\", (uint32_t)val);\n\terror = param_set_uint(val_rounded, kp);\n\treturn (error);\n}\n\nmodule_param_call(icp_gcm_avx_chunk_size, icp_gcm_avx_set_chunk_size,\n    param_get_uint, &gcm_avx_chunk_size, 0644);\n\nMODULE_PARM_DESC(icp_gcm_avx_chunk_size,\n\t\"How many bytes to process while owning the FPU\");\n\n#endif  \n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}