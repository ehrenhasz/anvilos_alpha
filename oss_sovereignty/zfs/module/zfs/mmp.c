#include <sys/abd.h>
#include <sys/mmp.h>
#include <sys/spa.h>
#include <sys/spa_impl.h>
#include <sys/time.h>
#include <sys/vdev.h>
#include <sys/vdev_impl.h>
#include <sys/zfs_context.h>
#include <sys/callb.h>
uint64_t zfs_multihost_interval = MMP_DEFAULT_INTERVAL;
uint_t zfs_multihost_import_intervals = MMP_DEFAULT_IMPORT_INTERVALS;
uint_t zfs_multihost_fail_intervals = MMP_DEFAULT_FAIL_INTERVALS;
static const void *const mmp_tag = "mmp_write_uberblock";
static __attribute__((noreturn)) void mmp_thread(void *arg);
void
mmp_init(spa_t *spa)
{
	mmp_thread_t *mmp = &spa->spa_mmp;
	mutex_init(&mmp->mmp_thread_lock, NULL, MUTEX_DEFAULT, NULL);
	cv_init(&mmp->mmp_thread_cv, NULL, CV_DEFAULT, NULL);
	mutex_init(&mmp->mmp_io_lock, NULL, MUTEX_DEFAULT, NULL);
	mmp->mmp_kstat_id = 1;
}
void
mmp_fini(spa_t *spa)
{
	mmp_thread_t *mmp = &spa->spa_mmp;
	mutex_destroy(&mmp->mmp_thread_lock);
	cv_destroy(&mmp->mmp_thread_cv);
	mutex_destroy(&mmp->mmp_io_lock);
}
static void
mmp_thread_enter(mmp_thread_t *mmp, callb_cpr_t *cpr)
{
	CALLB_CPR_INIT(cpr, &mmp->mmp_thread_lock, callb_generic_cpr, FTAG);
	mutex_enter(&mmp->mmp_thread_lock);
}
static void
mmp_thread_exit(mmp_thread_t *mmp, kthread_t **mpp, callb_cpr_t *cpr)
{
	ASSERT(*mpp != NULL);
	*mpp = NULL;
	cv_broadcast(&mmp->mmp_thread_cv);
	CALLB_CPR_EXIT(cpr);		 
}
void
mmp_thread_start(spa_t *spa)
{
	mmp_thread_t *mmp = &spa->spa_mmp;
	if (spa_writeable(spa)) {
		mutex_enter(&mmp->mmp_thread_lock);
		if (!mmp->mmp_thread) {
			mmp->mmp_thread = thread_create(NULL, 0, mmp_thread,
			    spa, 0, &p0, TS_RUN, defclsyspri);
			zfs_dbgmsg("MMP thread started pool '%s' "
			    "gethrtime %llu", spa_name(spa), gethrtime());
		}
		mutex_exit(&mmp->mmp_thread_lock);
	}
}
void
mmp_thread_stop(spa_t *spa)
{
	mmp_thread_t *mmp = &spa->spa_mmp;
	mutex_enter(&mmp->mmp_thread_lock);
	mmp->mmp_thread_exiting = 1;
	cv_broadcast(&mmp->mmp_thread_cv);
	while (mmp->mmp_thread) {
		cv_wait(&mmp->mmp_thread_cv, &mmp->mmp_thread_lock);
	}
	mutex_exit(&mmp->mmp_thread_lock);
	zfs_dbgmsg("MMP thread stopped pool '%s' gethrtime %llu",
	    spa_name(spa), gethrtime());
	ASSERT(mmp->mmp_thread == NULL);
	mmp->mmp_thread_exiting = 0;
}
typedef enum mmp_vdev_state_flag {
	MMP_FAIL_NOT_WRITABLE	= (1 << 0),
	MMP_FAIL_WRITE_PENDING	= (1 << 1),
} mmp_vdev_state_flag_t;
static int
mmp_next_leaf(spa_t *spa)
{
	vdev_t *leaf;
	vdev_t *starting_leaf;
	int fail_mask = 0;
	ASSERT(MUTEX_HELD(&spa->spa_mmp.mmp_io_lock));
	ASSERT(spa_config_held(spa, SCL_STATE, RW_READER));
	ASSERT(list_link_active(&spa->spa_leaf_list.list_head) == B_TRUE);
	ASSERT(!list_is_empty(&spa->spa_leaf_list));
	if (spa->spa_mmp.mmp_leaf_last_gen != spa->spa_leaf_list_gen) {
		spa->spa_mmp.mmp_last_leaf = list_head(&spa->spa_leaf_list);
		spa->spa_mmp.mmp_leaf_last_gen = spa->spa_leaf_list_gen;
	}
	leaf = spa->spa_mmp.mmp_last_leaf;
	if (leaf == NULL)
		leaf = list_head(&spa->spa_leaf_list);
	starting_leaf = leaf;
	do {
		leaf = list_next(&spa->spa_leaf_list, leaf);
		if (leaf == NULL) {
			leaf = list_head(&spa->spa_leaf_list);
			ASSERT3P(leaf, !=, NULL);
		}
		if (!vdev_writeable(leaf) || leaf->vdev_offline ||
		    leaf->vdev_detached) {
			fail_mask |= MMP_FAIL_NOT_WRITABLE;
		} else if (leaf->vdev_ops == &vdev_draid_spare_ops) {
			continue;
		} else if (leaf->vdev_mmp_pending != 0) {
			fail_mask |= MMP_FAIL_WRITE_PENDING;
		} else {
			spa->spa_mmp.mmp_last_leaf = leaf;
			return (0);
		}
	} while (leaf != starting_leaf);
	ASSERT(fail_mask);
	return (fail_mask);
}
static void
mmp_delay_update(spa_t *spa, boolean_t write_completed)
{
	mmp_thread_t *mts = &spa->spa_mmp;
	hrtime_t delay = gethrtime() - mts->mmp_last_write;
	ASSERT(MUTEX_HELD(&mts->mmp_io_lock));
	if (spa_multihost(spa) == B_FALSE) {
		mts->mmp_delay = 0;
		return;
	}
	if (delay > mts->mmp_delay)
		mts->mmp_delay = delay;
	if (write_completed == B_FALSE)
		return;
	mts->mmp_last_write = gethrtime();
	if (delay < mts->mmp_delay) {
		hrtime_t min_delay =
		    MSEC2NSEC(MMP_INTERVAL_OK(zfs_multihost_interval)) /
		    MAX(1, vdev_count_leaves(spa));
		mts->mmp_delay = MAX(((delay + mts->mmp_delay * 127) / 128),
		    min_delay);
	}
}
static void
mmp_write_done(zio_t *zio)
{
	spa_t *spa = zio->io_spa;
	vdev_t *vd = zio->io_vd;
	mmp_thread_t *mts = zio->io_private;
	mutex_enter(&mts->mmp_io_lock);
	uint64_t mmp_kstat_id = vd->vdev_mmp_kstat_id;
	hrtime_t mmp_write_duration = gethrtime() - vd->vdev_mmp_pending;
	mmp_delay_update(spa, (zio->io_error == 0));
	vd->vdev_mmp_pending = 0;
	vd->vdev_mmp_kstat_id = 0;
	mutex_exit(&mts->mmp_io_lock);
	spa_config_exit(spa, SCL_STATE, mmp_tag);
	spa_mmp_history_set(spa, mmp_kstat_id, zio->io_error,
	    mmp_write_duration);
	abd_free(zio->io_abd);
}
void
mmp_update_uberblock(spa_t *spa, uberblock_t *ub)
{
	mmp_thread_t *mmp = &spa->spa_mmp;
	mutex_enter(&mmp->mmp_io_lock);
	mmp->mmp_ub = *ub;
	mmp->mmp_seq = 1;
	mmp->mmp_ub.ub_timestamp = gethrestime_sec();
	mmp_delay_update(spa, B_TRUE);
	mutex_exit(&mmp->mmp_io_lock);
}
static void
mmp_write_uberblock(spa_t *spa)
{
	int flags = ZIO_FLAG_CONFIG_WRITER | ZIO_FLAG_CANFAIL;
	mmp_thread_t *mmp = &spa->spa_mmp;
	uberblock_t *ub;
	vdev_t *vd = NULL;
	int label, error;
	uint64_t offset;
	hrtime_t lock_acquire_time = gethrtime();
	spa_config_enter_mmp(spa, SCL_STATE, mmp_tag, RW_READER);
	lock_acquire_time = gethrtime() - lock_acquire_time;
	if (lock_acquire_time > (MSEC2NSEC(MMP_MIN_INTERVAL) / 10))
		zfs_dbgmsg("MMP SCL_STATE acquisition pool '%s' took %llu ns "
		    "gethrtime %llu", spa_name(spa), lock_acquire_time,
		    gethrtime());
	mutex_enter(&mmp->mmp_io_lock);
	error = mmp_next_leaf(spa);
	if (error) {
		mmp_delay_update(spa, B_FALSE);
		if (mmp->mmp_skip_error == error) {
			spa_mmp_history_set_skip(spa, mmp->mmp_kstat_id - 1);
		} else {
			mmp->mmp_skip_error = error;
			spa_mmp_history_add(spa, mmp->mmp_ub.ub_txg,
			    gethrestime_sec(), mmp->mmp_delay, NULL, 0,
			    mmp->mmp_kstat_id++, error);
			zfs_dbgmsg("MMP error choosing leaf pool '%s' "
			    "gethrtime %llu fail_mask %#x", spa_name(spa),
			    gethrtime(), error);
		}
		mutex_exit(&mmp->mmp_io_lock);
		spa_config_exit(spa, SCL_STATE, mmp_tag);
		return;
	}
	vd = spa->spa_mmp.mmp_last_leaf;
	if (mmp->mmp_skip_error != 0) {
		mmp->mmp_skip_error = 0;
		zfs_dbgmsg("MMP write after skipping due to unavailable "
		    "leaves, pool '%s' gethrtime %llu leaf %llu",
		    spa_name(spa), (u_longlong_t)gethrtime(),
		    (u_longlong_t)vd->vdev_guid);
	}
	if (mmp->mmp_zio_root == NULL)
		mmp->mmp_zio_root = zio_root(spa, NULL, NULL,
		    flags | ZIO_FLAG_GODFATHER);
	if (mmp->mmp_ub.ub_timestamp != gethrestime_sec()) {
		mmp->mmp_ub.ub_timestamp = gethrestime_sec();
		mmp->mmp_seq = 1;
	}
	ub = &mmp->mmp_ub;
	ub->ub_mmp_magic = MMP_MAGIC;
	ub->ub_mmp_delay = mmp->mmp_delay;
	ub->ub_mmp_config = MMP_SEQ_SET(mmp->mmp_seq) |
	    MMP_INTERVAL_SET(MMP_INTERVAL_OK(zfs_multihost_interval)) |
	    MMP_FAIL_INT_SET(MMP_FAIL_INTVS_OK(
	    zfs_multihost_fail_intervals));
	vd->vdev_mmp_pending = gethrtime();
	vd->vdev_mmp_kstat_id = mmp->mmp_kstat_id;
	zio_t *zio  = zio_null(mmp->mmp_zio_root, spa, NULL, NULL, NULL, flags);
	abd_t *ub_abd = abd_alloc_for_io(VDEV_UBERBLOCK_SIZE(vd), B_TRUE);
	abd_zero(ub_abd, VDEV_UBERBLOCK_SIZE(vd));
	abd_copy_from_buf(ub_abd, ub, sizeof (uberblock_t));
	mmp->mmp_seq++;
	mmp->mmp_kstat_id++;
	mutex_exit(&mmp->mmp_io_lock);
	offset = VDEV_UBERBLOCK_OFFSET(vd, VDEV_UBERBLOCK_COUNT(vd) -
	    MMP_BLOCKS_PER_LABEL + random_in_range(MMP_BLOCKS_PER_LABEL));
	label = random_in_range(VDEV_LABELS);
	vdev_label_write(zio, vd, label, ub_abd, offset,
	    VDEV_UBERBLOCK_SIZE(vd), mmp_write_done, mmp,
	    flags | ZIO_FLAG_DONT_PROPAGATE);
	(void) spa_mmp_history_add(spa, ub->ub_txg, ub->ub_timestamp,
	    ub->ub_mmp_delay, vd, label, vd->vdev_mmp_kstat_id, 0);
	zio_nowait(zio);
}
static __attribute__((noreturn)) void
mmp_thread(void *arg)
{
	spa_t *spa = (spa_t *)arg;
	mmp_thread_t *mmp = &spa->spa_mmp;
	boolean_t suspended = spa_suspended(spa);
	boolean_t multihost = spa_multihost(spa);
	uint64_t mmp_interval = MSEC2NSEC(MMP_INTERVAL_OK(
	    zfs_multihost_interval));
	uint32_t mmp_fail_intervals = MMP_FAIL_INTVS_OK(
	    zfs_multihost_fail_intervals);
	hrtime_t mmp_fail_ns = mmp_fail_intervals * mmp_interval;
	boolean_t last_spa_suspended;
	boolean_t last_spa_multihost;
	uint64_t last_mmp_interval;
	uint32_t last_mmp_fail_intervals;
	hrtime_t last_mmp_fail_ns;
	callb_cpr_t cpr;
	int skip_wait = 0;
	mmp_thread_enter(mmp, &cpr);
	mutex_enter(&mmp->mmp_io_lock);
	mmp->mmp_last_write = gethrtime();
	mmp->mmp_delay = MSEC2NSEC(MMP_INTERVAL_OK(zfs_multihost_interval));
	mutex_exit(&mmp->mmp_io_lock);
	while (!mmp->mmp_thread_exiting) {
		hrtime_t next_time = gethrtime() +
		    MSEC2NSEC(MMP_DEFAULT_INTERVAL);
		int leaves = MAX(vdev_count_leaves(spa), 1);
		last_spa_suspended = suspended;
		last_spa_multihost = multihost;
		suspended = spa_suspended(spa);
		multihost = spa_multihost(spa);
		last_mmp_interval = mmp_interval;
		last_mmp_fail_intervals = mmp_fail_intervals;
		last_mmp_fail_ns = mmp_fail_ns;
		mmp_interval = MSEC2NSEC(MMP_INTERVAL_OK(
		    zfs_multihost_interval));
		mmp_fail_intervals = MMP_FAIL_INTVS_OK(
		    zfs_multihost_fail_intervals);
		if (mmp_fail_intervals * mmp_interval < mmp_fail_ns) {
			mmp_fail_ns = (mmp_fail_ns * 31 +
			    mmp_fail_intervals * mmp_interval) / 32;
		} else {
			mmp_fail_ns = mmp_fail_intervals *
			    mmp_interval;
		}
		if (mmp_interval != last_mmp_interval ||
		    mmp_fail_intervals != last_mmp_fail_intervals) {
			skip_wait += leaves;
		}
		if (multihost)
			next_time = gethrtime() + mmp_interval / leaves;
		if (mmp_fail_ns != last_mmp_fail_ns) {
			zfs_dbgmsg("MMP interval change pool '%s' "
			    "gethrtime %llu last_mmp_interval %llu "
			    "mmp_interval %llu last_mmp_fail_intervals %u "
			    "mmp_fail_intervals %u mmp_fail_ns %llu "
			    "skip_wait %d leaves %d next_time %llu",
			    spa_name(spa), (u_longlong_t)gethrtime(),
			    (u_longlong_t)last_mmp_interval,
			    (u_longlong_t)mmp_interval, last_mmp_fail_intervals,
			    mmp_fail_intervals, (u_longlong_t)mmp_fail_ns,
			    skip_wait, leaves, (u_longlong_t)next_time);
		}
		if ((!last_spa_multihost && multihost) ||
		    (last_spa_suspended && !suspended)) {
			zfs_dbgmsg("MMP state change pool '%s': gethrtime %llu "
			    "last_spa_multihost %u multihost %u "
			    "last_spa_suspended %u suspended %u",
			    spa_name(spa), (u_longlong_t)gethrtime(),
			    last_spa_multihost, multihost, last_spa_suspended,
			    suspended);
			mutex_enter(&mmp->mmp_io_lock);
			mmp->mmp_last_write = gethrtime();
			mmp->mmp_delay = mmp_interval;
			mutex_exit(&mmp->mmp_io_lock);
		}
		if (last_spa_multihost && !multihost) {
			mutex_enter(&mmp->mmp_io_lock);
			mmp->mmp_delay = 0;
			mutex_exit(&mmp->mmp_io_lock);
		}
		if (multihost && !suspended && mmp_fail_intervals &&
		    (gethrtime() - mmp->mmp_last_write) > mmp_fail_ns) {
			zfs_dbgmsg("MMP suspending pool '%s': gethrtime %llu "
			    "mmp_last_write %llu mmp_interval %llu "
			    "mmp_fail_intervals %llu mmp_fail_ns %llu",
			    spa_name(spa), (u_longlong_t)gethrtime(),
			    (u_longlong_t)mmp->mmp_last_write,
			    (u_longlong_t)mmp_interval,
			    (u_longlong_t)mmp_fail_intervals,
			    (u_longlong_t)mmp_fail_ns);
			cmn_err(CE_WARN, "MMP writes to pool '%s' have not "
			    "succeeded in over %llu ms; suspending pool. "
			    "Hrtime %llu",
			    spa_name(spa),
			    NSEC2MSEC(gethrtime() - mmp->mmp_last_write),
			    gethrtime());
			zio_suspend(spa, NULL, ZIO_SUSPEND_MMP);
		}
		if (multihost && !suspended)
			mmp_write_uberblock(spa);
		if (skip_wait > 0) {
			next_time = gethrtime() + MSEC2NSEC(MMP_MIN_INTERVAL) /
			    leaves;
			skip_wait--;
		}
		CALLB_CPR_SAFE_BEGIN(&cpr);
		(void) cv_timedwait_idle_hires(&mmp->mmp_thread_cv,
		    &mmp->mmp_thread_lock, next_time, USEC2NSEC(100),
		    CALLOUT_FLAG_ABSOLUTE);
		CALLB_CPR_SAFE_END(&cpr, &mmp->mmp_thread_lock);
	}
	zio_wait(mmp->mmp_zio_root);
	mmp->mmp_zio_root = NULL;
	mmp_thread_exit(mmp, &mmp->mmp_thread, &cpr);
	thread_exit();
}
static void
mmp_signal_thread(spa_t *spa)
{
	mmp_thread_t *mmp = &spa->spa_mmp;
	mutex_enter(&mmp->mmp_thread_lock);
	if (mmp->mmp_thread)
		cv_broadcast(&mmp->mmp_thread_cv);
	mutex_exit(&mmp->mmp_thread_lock);
}
void
mmp_signal_all_threads(void)
{
	spa_t *spa = NULL;
	mutex_enter(&spa_namespace_lock);
	while ((spa = spa_next(spa))) {
		if (spa->spa_state == POOL_STATE_ACTIVE)
			mmp_signal_thread(spa);
	}
	mutex_exit(&spa_namespace_lock);
}
ZFS_MODULE_PARAM_CALL(zfs_multihost, zfs_multihost_, interval,
	param_set_multihost_interval, spl_param_get_u64, ZMOD_RW,
	"Milliseconds between mmp writes to each leaf");
ZFS_MODULE_PARAM(zfs_multihost, zfs_multihost_, fail_intervals, UINT, ZMOD_RW,
	"Max allowed period without a successful mmp write");
ZFS_MODULE_PARAM(zfs_multihost, zfs_multihost_, import_intervals, UINT, ZMOD_RW,
	"Number of zfs_multihost_interval periods to wait for activity");
