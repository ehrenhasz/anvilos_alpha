{
  "module_name": "cmsis_armclang_ltm.h",
  "hash_id": "1256482178015a4e84de7e7fce1dc53f66bd1c2b3143cfdc467467ebb6e09341",
  "original_prompt": "Ingested from sys_09_Anvil/source/lib/cmsis/inc/cmsis_armclang_ltm.h",
  "human_readable_source": " \n \n\n   \n\n#ifndef __CMSIS_ARMCLANG_H\n#define __CMSIS_ARMCLANG_H\n\n#pragma clang system_header    \n\n \n#ifndef   __ASM\n  #define __ASM                                  __asm\n#endif\n#ifndef   __INLINE\n  #define __INLINE                               __inline\n#endif\n#ifndef   __STATIC_INLINE\n  #define __STATIC_INLINE                        static __inline\n#endif\n#ifndef   __STATIC_FORCEINLINE\n  #define __STATIC_FORCEINLINE                   __attribute__((always_inline)) static __inline\n#endif\n#ifndef   __NO_RETURN\n  #define __NO_RETURN                            __attribute__((__noreturn__))\n#endif\n#ifndef   __USED\n  #define __USED                                 __attribute__((used))\n#endif\n#ifndef   __WEAK\n  #define __WEAK                                 __attribute__((weak))\n#endif\n#ifndef   __PACKED\n  #define __PACKED                               __attribute__((packed, aligned(1)))\n#endif\n#ifndef   __PACKED_STRUCT\n  #define __PACKED_STRUCT                        struct __attribute__((packed, aligned(1)))\n#endif\n#ifndef   __PACKED_UNION\n  #define __PACKED_UNION                         union __attribute__((packed, aligned(1)))\n#endif\n#ifndef   __UNALIGNED_UINT32         \n  #pragma clang diagnostic push\n  #pragma clang diagnostic ignored \"-Wpacked\"\n   \n  struct __attribute__((packed)) T_UINT32 { uint32_t v; };\n  #pragma clang diagnostic pop\n  #define __UNALIGNED_UINT32(x)                  (((struct T_UINT32 *)(x))->v)\n#endif\n#ifndef   __UNALIGNED_UINT16_WRITE\n  #pragma clang diagnostic push\n  #pragma clang diagnostic ignored \"-Wpacked\"\n   \n  __PACKED_STRUCT T_UINT16_WRITE { uint16_t v; };\n  #pragma clang diagnostic pop\n  #define __UNALIGNED_UINT16_WRITE(addr, val)    (void)((((struct T_UINT16_WRITE *)(void *)(addr))->v) = (val))\n#endif\n#ifndef   __UNALIGNED_UINT16_READ\n  #pragma clang diagnostic push\n  #pragma clang diagnostic ignored \"-Wpacked\"\n   \n  __PACKED_STRUCT T_UINT16_READ { uint16_t v; };\n  #pragma clang diagnostic pop\n  #define __UNALIGNED_UINT16_READ(addr)          (((const struct T_UINT16_READ *)(const void *)(addr))->v)\n#endif\n#ifndef   __UNALIGNED_UINT32_WRITE\n  #pragma clang diagnostic push\n  #pragma clang diagnostic ignored \"-Wpacked\"\n   \n  __PACKED_STRUCT T_UINT32_WRITE { uint32_t v; };\n  #pragma clang diagnostic pop\n  #define __UNALIGNED_UINT32_WRITE(addr, val)    (void)((((struct T_UINT32_WRITE *)(void *)(addr))->v) = (val))\n#endif\n#ifndef   __UNALIGNED_UINT32_READ\n  #pragma clang diagnostic push\n  #pragma clang diagnostic ignored \"-Wpacked\"\n   \n  __PACKED_STRUCT T_UINT32_READ { uint32_t v; };\n  #pragma clang diagnostic pop\n  #define __UNALIGNED_UINT32_READ(addr)          (((const struct T_UINT32_READ *)(const void *)(addr))->v)\n#endif\n#ifndef   __ALIGNED\n  #define __ALIGNED(x)                           __attribute__((aligned(x)))\n#endif\n#ifndef   __RESTRICT\n  #define __RESTRICT                             __restrict\n#endif\n#ifndef   __COMPILER_BARRIER\n  #define __COMPILER_BARRIER()                   __ASM volatile(\"\":::\"memory\")\n#endif\n\n \n\n#ifndef __PROGRAM_START\n#define __PROGRAM_START           __main\n#endif\n\n#ifndef __INITIAL_SP\n#define __INITIAL_SP              Image$$ARM_LIB_STACK$$ZI$$Limit\n#endif\n\n#ifndef __STACK_LIMIT\n#define __STACK_LIMIT             Image$$ARM_LIB_STACK$$ZI$$Base\n#endif\n\n#ifndef __VECTOR_TABLE\n#define __VECTOR_TABLE            __Vectors\n#endif\n\n#ifndef __VECTOR_TABLE_ATTRIBUTE\n#define __VECTOR_TABLE_ATTRIBUTE  __attribute__((used, section(\"RESET\")))\n#endif\n\n#if defined (__ARM_FEATURE_CMSE) && (__ARM_FEATURE_CMSE == 3U)\n#ifndef __STACK_SEAL\n#define __STACK_SEAL              Image$$STACKSEAL$$ZI$$Base\n#endif\n\n#ifndef __TZ_STACK_SEAL_SIZE\n#define __TZ_STACK_SEAL_SIZE      8U\n#endif\n\n#ifndef __TZ_STACK_SEAL_VALUE\n#define __TZ_STACK_SEAL_VALUE     0xFEF5EDA5FEF5EDA5ULL\n#endif\n\n\n__STATIC_FORCEINLINE void __TZ_set_STACKSEAL_S (uint32_t* stackTop) {\n  *((uint64_t *)stackTop) = __TZ_STACK_SEAL_VALUE;\n}\n#endif\n\n\n \n \n\n \n#if defined (__thumb__) && !defined (__thumb2__)\n#define __CMSIS_GCC_OUT_REG(r) \"=l\" (r)\n#define __CMSIS_GCC_USE_REG(r) \"l\" (r)\n#else\n#define __CMSIS_GCC_OUT_REG(r) \"=r\" (r)\n#define __CMSIS_GCC_USE_REG(r) \"r\" (r)\n#endif\n\n \n#define __NOP          __builtin_arm_nop\n\n \n#define __WFI          __builtin_arm_wfi\n\n\n \n#define __WFE          __builtin_arm_wfe\n\n\n \n#define __SEV          __builtin_arm_sev\n\n\n \n#define __ISB()        __builtin_arm_isb(0xF)\n\n \n#define __DSB()        __builtin_arm_dsb(0xF)\n\n\n \n#define __DMB()        __builtin_arm_dmb(0xF)\n\n\n \n#define __REV(value)   __builtin_bswap32(value)\n\n\n \n#define __REV16(value) __ROR(__REV(value), 16)\n\n\n \n#define __REVSH(value) (int16_t)__builtin_bswap16(value)\n\n\n \n__STATIC_FORCEINLINE uint32_t __ROR(uint32_t op1, uint32_t op2)\n{\n  op2 %= 32U;\n  if (op2 == 0U)\n  {\n    return op1;\n  }\n  return (op1 >> op2) | (op1 << (32U - op2));\n}\n\n\n \n#define __BKPT(value)     __ASM volatile (\"bkpt \"#value)\n\n\n \n#define __RBIT            __builtin_arm_rbit\n\n \n__STATIC_FORCEINLINE uint8_t __CLZ(uint32_t value)\n{\n   \n  if (value == 0U)\n  {\n    return 32U;\n  }\n  return __builtin_clz(value);\n}\n\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) || \\\n     (defined (__ARM_ARCH_8M_BASE__ ) && (__ARM_ARCH_8M_BASE__ == 1))    )\n \n#define __LDREXB        (uint8_t)__builtin_arm_ldrex\n\n\n \n#define __LDREXH        (uint16_t)__builtin_arm_ldrex\n\n\n \n#define __LDREXW        (uint32_t)__builtin_arm_ldrex\n\n\n \n#define __STREXB        (uint32_t)__builtin_arm_strex\n\n\n \n#define __STREXH        (uint32_t)__builtin_arm_strex\n\n\n \n#define __STREXW        (uint32_t)__builtin_arm_strex\n\n\n \n#define __CLREX             __builtin_arm_clrex\n\n#endif  \n\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1))    )\n\n \n#define __SSAT             __builtin_arm_ssat\n\n\n \n#define __USAT             __builtin_arm_usat\n\n\n \n__STATIC_FORCEINLINE uint32_t __RRX(uint32_t value)\n{\n  uint32_t result;\n\n  __ASM volatile (\"rrx %0, %1\" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint8_t __LDRBT(volatile uint8_t *ptr)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ldrbt %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) );\n  return ((uint8_t) result);     \n}\n\n\n \n__STATIC_FORCEINLINE uint16_t __LDRHT(volatile uint16_t *ptr)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ldrht %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) );\n  return ((uint16_t) result);     \n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __LDRT(volatile uint32_t *ptr)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ldrt %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __STRBT(uint8_t value, volatile uint8_t *ptr)\n{\n  __ASM volatile (\"strbt %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) );\n}\n\n\n \n__STATIC_FORCEINLINE void __STRHT(uint16_t value, volatile uint16_t *ptr)\n{\n  __ASM volatile (\"strht %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) );\n}\n\n\n \n__STATIC_FORCEINLINE void __STRT(uint32_t value, volatile uint32_t *ptr)\n{\n  __ASM volatile (\"strt %1, %0\" : \"=Q\" (*ptr) : \"r\" (value) );\n}\n\n#else   \n\n \n__STATIC_FORCEINLINE int32_t __SSAT(int32_t val, uint32_t sat)\n{\n  if ((sat >= 1U) && (sat <= 32U))\n  {\n    const int32_t max = (int32_t)((1U << (sat - 1U)) - 1U);\n    const int32_t min = -1 - max ;\n    if (val > max)\n    {\n      return max;\n    }\n    else if (val < min)\n    {\n      return min;\n    }\n  }\n  return val;\n}\n\n \n__STATIC_FORCEINLINE uint32_t __USAT(int32_t val, uint32_t sat)\n{\n  if (sat <= 31U)\n  {\n    const uint32_t max = ((1U << sat) - 1U);\n    if (val > (int32_t)max)\n    {\n      return max;\n    }\n    else if (val < 0)\n    {\n      return 0U;\n    }\n  }\n  return (uint32_t)val;\n}\n\n#endif  \n\n\n#if ((defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) || \\\n     (defined (__ARM_ARCH_8M_BASE__ ) && (__ARM_ARCH_8M_BASE__ == 1))    )\n \n__STATIC_FORCEINLINE uint8_t __LDAB(volatile uint8_t *ptr)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ldab %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n  return ((uint8_t) result);\n}\n\n\n \n__STATIC_FORCEINLINE uint16_t __LDAH(volatile uint16_t *ptr)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ldah %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n  return ((uint16_t) result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __LDA(volatile uint32_t *ptr)\n{\n  uint32_t result;\n\n  __ASM volatile (\"lda %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __STLB(uint8_t value, volatile uint8_t *ptr)\n{\n  __ASM volatile (\"stlb %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n}\n\n\n \n__STATIC_FORCEINLINE void __STLH(uint16_t value, volatile uint16_t *ptr)\n{\n  __ASM volatile (\"stlh %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n}\n\n\n \n__STATIC_FORCEINLINE void __STL(uint32_t value, volatile uint32_t *ptr)\n{\n  __ASM volatile (\"stl %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n}\n\n\n \n#define     __LDAEXB                 (uint8_t)__builtin_arm_ldaex\n\n\n \n#define     __LDAEXH                 (uint16_t)__builtin_arm_ldaex\n\n\n \n#define     __LDAEX                  (uint32_t)__builtin_arm_ldaex\n\n\n \n#define     __STLEXB                 (uint32_t)__builtin_arm_stlex\n\n\n \n#define     __STLEXH                 (uint32_t)__builtin_arm_stlex\n\n\n \n#define     __STLEX                  (uint32_t)__builtin_arm_stlex\n\n#endif  \n\n   \n\n\n \n \n\n \n#ifndef __ARM_COMPAT_H\n__STATIC_FORCEINLINE void __enable_irq(void)\n{\n  __ASM volatile (\"cpsie i\" : : : \"memory\");\n}\n#endif\n\n\n \n#ifndef __ARM_COMPAT_H\n__STATIC_FORCEINLINE void __disable_irq(void)\n{\n  __ASM volatile (\"cpsid i\" : : : \"memory\");\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_CONTROL(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, control\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_CONTROL_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, control_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_CONTROL(uint32_t control)\n{\n  __ASM volatile (\"MSR control, %0\" : : \"r\" (control) : \"memory\");\n  __ISB();\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_CONTROL_NS(uint32_t control)\n{\n  __ASM volatile (\"MSR control_ns, %0\" : : \"r\" (control) : \"memory\");\n  __ISB();\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_IPSR(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, ipsr\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_APSR(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, apsr\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_xPSR(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, xpsr\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_PSP(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, psp\"  : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_PSP_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, psp_ns\"  : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_PSP(uint32_t topOfProcStack)\n{\n  __ASM volatile (\"MSR psp, %0\" : : \"r\" (topOfProcStack) : );\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_PSP_NS(uint32_t topOfProcStack)\n{\n  __ASM volatile (\"MSR psp_ns, %0\" : : \"r\" (topOfProcStack) : );\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_MSP(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, msp\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_MSP_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, msp_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_MSP(uint32_t topOfMainStack)\n{\n  __ASM volatile (\"MSR msp, %0\" : : \"r\" (topOfMainStack) : );\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_MSP_NS(uint32_t topOfMainStack)\n{\n  __ASM volatile (\"MSR msp_ns, %0\" : : \"r\" (topOfMainStack) : );\n}\n#endif\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_SP_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, sp_ns\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __TZ_set_SP_NS(uint32_t topOfStack)\n{\n  __ASM volatile (\"MSR sp_ns, %0\" : : \"r\" (topOfStack) : );\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_PRIMASK(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, primask\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_PRIMASK_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, primask_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_PRIMASK(uint32_t priMask)\n{\n  __ASM volatile (\"MSR primask, %0\" : : \"r\" (priMask) : \"memory\");\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_PRIMASK_NS(uint32_t priMask)\n{\n  __ASM volatile (\"MSR primask_ns, %0\" : : \"r\" (priMask) : \"memory\");\n}\n#endif\n\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1))    )\n \n__STATIC_FORCEINLINE void __enable_fault_irq(void)\n{\n  __ASM volatile (\"cpsie f\" : : : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE void __disable_fault_irq(void)\n{\n  __ASM volatile (\"cpsid f\" : : : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_BASEPRI(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, basepri\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_BASEPRI_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, basepri_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_BASEPRI(uint32_t basePri)\n{\n  __ASM volatile (\"MSR basepri, %0\" : : \"r\" (basePri) : \"memory\");\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_BASEPRI_NS(uint32_t basePri)\n{\n  __ASM volatile (\"MSR basepri_ns, %0\" : : \"r\" (basePri) : \"memory\");\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_BASEPRI_MAX(uint32_t basePri)\n{\n  __ASM volatile (\"MSR basepri_max, %0\" : : \"r\" (basePri) : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_FAULTMASK(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, faultmask\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_FAULTMASK_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, faultmask_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_FAULTMASK(uint32_t faultMask)\n{\n  __ASM volatile (\"MSR faultmask, %0\" : : \"r\" (faultMask) : \"memory\");\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_FAULTMASK_NS(uint32_t faultMask)\n{\n  __ASM volatile (\"MSR faultmask_ns, %0\" : : \"r\" (faultMask) : \"memory\");\n}\n#endif\n\n#endif  \n\n\n#if ((defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) || \\\n     (defined (__ARM_ARCH_8M_BASE__ ) && (__ARM_ARCH_8M_BASE__ == 1))    )\n\n \n__STATIC_FORCEINLINE uint32_t __get_PSPLIM(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n    \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, psplim\"  : \"=r\" (result) );\n  return result;\n#endif\n}\n\n#if (defined (__ARM_FEATURE_CMSE) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_PSPLIM_NS(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, psplim_ns\"  : \"=r\" (result) );\n  return result;\n#endif\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_PSPLIM(uint32_t ProcStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n  \n  (void)ProcStackPtrLimit;\n#else\n  __ASM volatile (\"MSR psplim, %0\" : : \"r\" (ProcStackPtrLimit));\n#endif\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE  ) && (__ARM_FEATURE_CMSE   == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_PSPLIM_NS(uint32_t ProcStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  (void)ProcStackPtrLimit;\n#else\n  __ASM volatile (\"MSR psplim_ns, %0\\n\" : : \"r\" (ProcStackPtrLimit));\n#endif\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_MSPLIM(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n  \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, msplim\" : \"=r\" (result) );\n  return result;\n#endif\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE  ) && (__ARM_FEATURE_CMSE   == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_MSPLIM_NS(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, msplim_ns\" : \"=r\" (result) );\n  return result;\n#endif\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_MSPLIM(uint32_t MainStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n  \n  (void)MainStackPtrLimit;\n#else\n  __ASM volatile (\"MSR msplim, %0\" : : \"r\" (MainStackPtrLimit));\n#endif\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE  ) && (__ARM_FEATURE_CMSE   == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_MSPLIM_NS(uint32_t MainStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  (void)MainStackPtrLimit;\n#else\n  __ASM volatile (\"MSR msplim_ns, %0\" : : \"r\" (MainStackPtrLimit));\n#endif\n}\n#endif\n\n#endif  \n\n \n#if ((defined (__FPU_PRESENT) && (__FPU_PRESENT == 1U)) && \\\n     (defined (__FPU_USED   ) && (__FPU_USED    == 1U))     )\n#define __get_FPSCR      (uint32_t)__builtin_arm_get_fpscr\n#else\n#define __get_FPSCR()      ((uint32_t)0U)\n#endif\n\n \n#if ((defined (__FPU_PRESENT) && (__FPU_PRESENT == 1U)) && \\\n     (defined (__FPU_USED   ) && (__FPU_USED    == 1U))     )\n#define __set_FPSCR      __builtin_arm_set_fpscr\n#else\n#define __set_FPSCR(x)      ((void)(x))\n#endif\n\n\n \n\n\n \n \n\n#if (defined (__ARM_FEATURE_DSP) && (__ARM_FEATURE_DSP == 1))\n\n__STATIC_FORCEINLINE uint32_t __SADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"qadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"shadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uqadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uhadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n\n__STATIC_FORCEINLINE uint32_t __SSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ssub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"qsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"shsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uqsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uhsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n\n__STATIC_FORCEINLINE uint32_t __SADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"qadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"shadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uqadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uhadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ssub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"qsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"shsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uqsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uhsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"qasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"shasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uqasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uhasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ssax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"qsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"shsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uqsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uhsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USAD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usad8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USADA8(uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usada8 %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n#define __SSAT16(ARG1,ARG2) \\\n({                          \\\n  int32_t __RES, __ARG1 = (ARG1); \\\n  __ASM (\"ssat16 %0, %1, %2\" : \"=r\" (__RES) :  \"I\" (ARG2), \"r\" (__ARG1) ); \\\n  __RES; \\\n })\n\n#define __USAT16(ARG1,ARG2) \\\n({                          \\\n  uint32_t __RES, __ARG1 = (ARG1); \\\n  __ASM (\"usat16 %0, %1, %2\" : \"=r\" (__RES) :  \"I\" (ARG2), \"r\" (__ARG1) ); \\\n  __RES; \\\n })\n\n__STATIC_FORCEINLINE uint32_t __UXTB16(uint32_t op1)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uxtb16 %0, %1\" : \"=r\" (result) : \"r\" (op1));\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UXTAB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uxtab16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SXTB16(uint32_t op1)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sxtb16 %0, %1\" : \"=r\" (result) : \"r\" (op1));\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SXTAB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sxtab16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUAD  (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smuad %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUADX (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smuadx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLAD (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smlad %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLADX (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smladx %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLALD (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlald %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlald %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLALDX (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlaldx %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlaldx %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUSD  (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smusd %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUSDX (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smusdx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLSD (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smlsd %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLSDX (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smlsdx %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLSLD (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlsld %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlsld %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLSLDX (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlsldx %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlsldx %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint32_t __SEL  (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sel %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE  int32_t __QADD( int32_t op1,  int32_t op2)\n{\n  int32_t result;\n\n  __ASM volatile (\"qadd %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE  int32_t __QSUB( int32_t op1,  int32_t op2)\n{\n  int32_t result;\n\n  __ASM volatile (\"qsub %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n#define __PKHBT(ARG1,ARG2,ARG3)          ( ((((uint32_t)(ARG1))          ) & 0x0000FFFFUL) |  \\\n                                           ((((uint32_t)(ARG2)) << (ARG3)) & 0xFFFF0000UL)  )\n\n#define __PKHTB(ARG1,ARG2,ARG3)          ( ((((uint32_t)(ARG1))          ) & 0xFFFF0000UL) |  \\\n                                           ((((uint32_t)(ARG2)) >> (ARG3)) & 0x0000FFFFUL)  )\n\n#define __SXTB16_RORn(ARG1, ARG2)        __SXTB16(__ROR(ARG1, ARG2))\n\n#define __SXTAB16_RORn(ARG1, ARG2, ARG3) __SXTAB16(ARG1, __ROR(ARG2, ARG3))\n\n__STATIC_FORCEINLINE int32_t __SMMLA (int32_t op1, int32_t op2, int32_t op3)\n{\n  int32_t result;\n\n  __ASM volatile (\"smmla %0, %1, %2, %3\" : \"=r\" (result): \"r\"  (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n#endif  \n \n\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}