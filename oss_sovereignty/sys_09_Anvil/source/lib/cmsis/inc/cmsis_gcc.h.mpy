{
  "module_name": "cmsis_gcc.h",
  "hash_id": "e54bfa4e96b5371c6e62bdf45638192eff8a1862ac2321c3262c0e7034590fd3",
  "original_prompt": "Ingested from sys_09_Anvil/source/lib/cmsis/inc/cmsis_gcc.h",
  "human_readable_source": " \n \n\n#ifndef __CMSIS_GCC_H\n#define __CMSIS_GCC_H\n\n \n#pragma GCC diagnostic push\n#pragma GCC diagnostic ignored \"-Wsign-conversion\"\n#pragma GCC diagnostic ignored \"-Wconversion\"\n#pragma GCC diagnostic ignored \"-Wunused-parameter\"\n\n \n#ifndef __has_builtin\n  #define __has_builtin(x) (0)\n#endif\n\n \n#ifndef   __ASM\n  #define __ASM                                  __asm\n#endif\n#ifndef   __INLINE\n  #define __INLINE                               inline\n#endif\n#ifndef   __STATIC_INLINE\n  #define __STATIC_INLINE                        static inline\n#endif\n#ifndef   __STATIC_FORCEINLINE\n  #define __STATIC_FORCEINLINE                   __attribute__((always_inline)) static inline\n#endif\n#ifndef   __NO_RETURN\n  #define __NO_RETURN                            __attribute__((__noreturn__))\n#endif\n#ifndef   __USED\n  #define __USED                                 __attribute__((used))\n#endif\n#ifndef   __WEAK\n  #define __WEAK                                 __attribute__((weak))\n#endif\n#ifndef   __PACKED\n  #define __PACKED                               __attribute__((packed, aligned(1)))\n#endif\n#ifndef   __PACKED_STRUCT\n  #define __PACKED_STRUCT                        struct __attribute__((packed, aligned(1)))\n#endif\n#ifndef   __PACKED_UNION\n  #define __PACKED_UNION                         union __attribute__((packed, aligned(1)))\n#endif\n#ifndef   __UNALIGNED_UINT32         \n  #pragma GCC diagnostic push\n  #pragma GCC diagnostic ignored \"-Wpacked\"\n  #pragma GCC diagnostic ignored \"-Wattributes\"\n  struct __attribute__((packed)) T_UINT32 { uint32_t v; };\n  #pragma GCC diagnostic pop\n  #define __UNALIGNED_UINT32(x)                  (((struct T_UINT32 *)(x))->v)\n#endif\n#ifndef   __UNALIGNED_UINT16_WRITE\n  #pragma GCC diagnostic push\n  #pragma GCC diagnostic ignored \"-Wpacked\"\n  #pragma GCC diagnostic ignored \"-Wattributes\"\n  __PACKED_STRUCT T_UINT16_WRITE { uint16_t v; };\n  #pragma GCC diagnostic pop\n  #define __UNALIGNED_UINT16_WRITE(addr, val)    (void)((((struct T_UINT16_WRITE *)(void *)(addr))->v) = (val))\n#endif\n#ifndef   __UNALIGNED_UINT16_READ\n  #pragma GCC diagnostic push\n  #pragma GCC diagnostic ignored \"-Wpacked\"\n  #pragma GCC diagnostic ignored \"-Wattributes\"\n  __PACKED_STRUCT T_UINT16_READ { uint16_t v; };\n  #pragma GCC diagnostic pop\n  #define __UNALIGNED_UINT16_READ(addr)          (((const struct T_UINT16_READ *)(const void *)(addr))->v)\n#endif\n#ifndef   __UNALIGNED_UINT32_WRITE\n  #pragma GCC diagnostic push\n  #pragma GCC diagnostic ignored \"-Wpacked\"\n  #pragma GCC diagnostic ignored \"-Wattributes\"\n  __PACKED_STRUCT T_UINT32_WRITE { uint32_t v; };\n  #pragma GCC diagnostic pop\n  #define __UNALIGNED_UINT32_WRITE(addr, val)    (void)((((struct T_UINT32_WRITE *)(void *)(addr))->v) = (val))\n#endif\n#ifndef   __UNALIGNED_UINT32_READ\n  #pragma GCC diagnostic push\n  #pragma GCC diagnostic ignored \"-Wpacked\"\n  #pragma GCC diagnostic ignored \"-Wattributes\"\n  __PACKED_STRUCT T_UINT32_READ { uint32_t v; };\n  #pragma GCC diagnostic pop\n  #define __UNALIGNED_UINT32_READ(addr)          (((const struct T_UINT32_READ *)(const void *)(addr))->v)\n#endif\n#ifndef   __ALIGNED\n  #define __ALIGNED(x)                           __attribute__((aligned(x)))\n#endif\n#ifndef   __RESTRICT\n  #define __RESTRICT                             __restrict\n#endif\n#ifndef   __COMPILER_BARRIER\n  #define __COMPILER_BARRIER()                   __ASM volatile(\"\":::\"memory\")\n#endif\n\n \n\n#ifndef __PROGRAM_START\n\n \n__STATIC_FORCEINLINE __NO_RETURN void __cmsis_start(void)\n{\n  extern void _start(void) __NO_RETURN;\n\n  typedef struct {\n    uint32_t const* src;\n    uint32_t* dest;\n    uint32_t  wlen;\n  } __copy_table_t;\n\n  typedef struct {\n    uint32_t* dest;\n    uint32_t  wlen;\n  } __zero_table_t;\n\n  extern const __copy_table_t __copy_table_start__;\n  extern const __copy_table_t __copy_table_end__;\n  extern const __zero_table_t __zero_table_start__;\n  extern const __zero_table_t __zero_table_end__;\n\n  for (__copy_table_t const* pTable = &__copy_table_start__; pTable < &__copy_table_end__; ++pTable) {\n    for(uint32_t i=0u; i<pTable->wlen; ++i) {\n      pTable->dest[i] = pTable->src[i];\n    }\n  }\n\n  for (__zero_table_t const* pTable = &__zero_table_start__; pTable < &__zero_table_end__; ++pTable) {\n    for(uint32_t i=0u; i<pTable->wlen; ++i) {\n      pTable->dest[i] = 0u;\n    }\n  }\n\n  _start();\n}\n\n#define __PROGRAM_START           __cmsis_start\n#endif\n\n#ifndef __INITIAL_SP\n#define __INITIAL_SP              __StackTop\n#endif\n\n#ifndef __STACK_LIMIT\n#define __STACK_LIMIT             __StackLimit\n#endif\n\n#ifndef __VECTOR_TABLE\n#define __VECTOR_TABLE            __Vectors\n#endif\n\n#ifndef __VECTOR_TABLE_ATTRIBUTE\n#define __VECTOR_TABLE_ATTRIBUTE  __attribute__((used, section(\".vectors\")))\n#endif\n\n#if defined (__ARM_FEATURE_CMSE) && (__ARM_FEATURE_CMSE == 3U)\n#ifndef __STACK_SEAL\n#define __STACK_SEAL              __StackSeal\n#endif\n\n#ifndef __TZ_STACK_SEAL_SIZE\n#define __TZ_STACK_SEAL_SIZE      8U\n#endif\n\n#ifndef __TZ_STACK_SEAL_VALUE\n#define __TZ_STACK_SEAL_VALUE     0xFEF5EDA5FEF5EDA5ULL\n#endif\n\n\n__STATIC_FORCEINLINE void __TZ_set_STACKSEAL_S (uint32_t* stackTop) {\n  *((uint64_t *)stackTop) = __TZ_STACK_SEAL_VALUE;\n}\n#endif\n\n\n \n \n\n \n#if defined (__thumb__) && !defined (__thumb2__)\n#define __CMSIS_GCC_OUT_REG(r) \"=l\" (r)\n#define __CMSIS_GCC_RW_REG(r) \"+l\" (r)\n#define __CMSIS_GCC_USE_REG(r) \"l\" (r)\n#else\n#define __CMSIS_GCC_OUT_REG(r) \"=r\" (r)\n#define __CMSIS_GCC_RW_REG(r) \"+r\" (r)\n#define __CMSIS_GCC_USE_REG(r) \"r\" (r)\n#endif\n\n \n#define __NOP()                             __ASM volatile (\"nop\")\n\n \n#define __WFI()                             __ASM volatile (\"wfi\":::\"memory\")\n\n\n \n#define __WFE()                             __ASM volatile (\"wfe\":::\"memory\")\n\n\n \n#define __SEV()                             __ASM volatile (\"sev\")\n\n\n \n__STATIC_FORCEINLINE void __ISB(void)\n{\n  __ASM volatile (\"isb 0xF\":::\"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE void __DSB(void)\n{\n  __ASM volatile (\"dsb 0xF\":::\"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE void __DMB(void)\n{\n  __ASM volatile (\"dmb 0xF\":::\"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __REV(uint32_t value)\n{\n#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 5)\n  return __builtin_bswap32(value);\n#else\n  uint32_t result;\n\n  __ASM (\"rev %0, %1\" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );\n  return result;\n#endif\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __REV16(uint32_t value)\n{\n  uint32_t result;\n\n  __ASM (\"rev16 %0, %1\" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );\n  return result;\n}\n\n\n \n__STATIC_FORCEINLINE int16_t __REVSH(int16_t value)\n{\n#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)\n  return (int16_t)__builtin_bswap16(value);\n#else\n  int16_t result;\n\n  __ASM (\"revsh %0, %1\" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );\n  return result;\n#endif\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __ROR(uint32_t op1, uint32_t op2)\n{\n  op2 %= 32U;\n  if (op2 == 0U)\n  {\n    return op1;\n  }\n  return (op1 >> op2) | (op1 << (32U - op2));\n}\n\n\n \n#define __BKPT(value)                       __ASM volatile (\"bkpt \"#value)\n\n\n \n__STATIC_FORCEINLINE uint32_t __RBIT(uint32_t value)\n{\n  uint32_t result;\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1))    )\n   __ASM (\"rbit %0, %1\" : \"=r\" (result) : \"r\" (value) );\n#else\n  uint32_t s = (4U   * 8U) - 1U;  \n\n  result = value;                       \n  for (value >>= 1U; value != 0U; value >>= 1U)\n  {\n    result <<= 1U;\n    result |= value & 1U;\n    s--;\n  }\n  result <<= s;                         \n#endif\n  return result;\n}\n\n\n \n__STATIC_FORCEINLINE uint8_t __CLZ(uint32_t value)\n{\n   \n  if (value == 0U)\n  {\n    return 32U;\n  }\n  return __builtin_clz(value);\n}\n\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) || \\\n     (defined (__ARM_ARCH_8M_BASE__ ) && (__ARM_ARCH_8M_BASE__ == 1))    )\n \n__STATIC_FORCEINLINE uint8_t __LDREXB(volatile uint8_t *addr)\n{\n    uint32_t result;\n\n#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)\n   __ASM volatile (\"ldrexb %0, %1\" : \"=r\" (result) : \"Q\" (*addr) );\n#else\n     \n   __ASM volatile (\"ldrexb %0, [%1]\" : \"=r\" (result) : \"r\" (addr) : \"memory\" );\n#endif\n   return ((uint8_t) result);     \n}\n\n\n \n__STATIC_FORCEINLINE uint16_t __LDREXH(volatile uint16_t *addr)\n{\n    uint32_t result;\n\n#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)\n   __ASM volatile (\"ldrexh %0, %1\" : \"=r\" (result) : \"Q\" (*addr) );\n#else\n     \n   __ASM volatile (\"ldrexh %0, [%1]\" : \"=r\" (result) : \"r\" (addr) : \"memory\" );\n#endif\n   return ((uint16_t) result);     \n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __LDREXW(volatile uint32_t *addr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldrex %0, %1\" : \"=r\" (result) : \"Q\" (*addr) );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __STREXB(uint8_t value, volatile uint8_t *addr)\n{\n   uint32_t result;\n\n   __ASM volatile (\"strexb %0, %2, %1\" : \"=&r\" (result), \"=Q\" (*addr) : \"r\" ((uint32_t)value) );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __STREXH(uint16_t value, volatile uint16_t *addr)\n{\n   uint32_t result;\n\n   __ASM volatile (\"strexh %0, %2, %1\" : \"=&r\" (result), \"=Q\" (*addr) : \"r\" ((uint32_t)value) );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __STREXW(uint32_t value, volatile uint32_t *addr)\n{\n   uint32_t result;\n\n   __ASM volatile (\"strex %0, %2, %1\" : \"=&r\" (result), \"=Q\" (*addr) : \"r\" (value) );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __CLREX(void)\n{\n  __ASM volatile (\"clrex\" ::: \"memory\");\n}\n\n#endif  \n\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1))    )\n \n#define __SSAT(ARG1, ARG2) \\\n__extension__ \\\n({                          \\\n  int32_t __RES, __ARG1 = (ARG1); \\\n  __ASM volatile (\"ssat %0, %1, %2\" : \"=r\" (__RES) :  \"I\" (ARG2), \"r\" (__ARG1) : \"cc\" ); \\\n  __RES; \\\n })\n\n\n \n#define __USAT(ARG1, ARG2) \\\n__extension__ \\\n({                          \\\n  uint32_t __RES, __ARG1 = (ARG1); \\\n  __ASM volatile (\"usat %0, %1, %2\" : \"=r\" (__RES) :  \"I\" (ARG2), \"r\" (__ARG1) : \"cc\" ); \\\n  __RES; \\\n })\n\n\n \n__STATIC_FORCEINLINE uint32_t __RRX(uint32_t value)\n{\n  uint32_t result;\n\n  __ASM volatile (\"rrx %0, %1\" : __CMSIS_GCC_OUT_REG (result) : __CMSIS_GCC_USE_REG (value) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint8_t __LDRBT(volatile uint8_t *ptr)\n{\n    uint32_t result;\n\n#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)\n   __ASM volatile (\"ldrbt %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) );\n#else\n     \n   __ASM volatile (\"ldrbt %0, [%1]\" : \"=r\" (result) : \"r\" (ptr) : \"memory\" );\n#endif\n   return ((uint8_t) result);     \n}\n\n\n \n__STATIC_FORCEINLINE uint16_t __LDRHT(volatile uint16_t *ptr)\n{\n    uint32_t result;\n\n#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)\n   __ASM volatile (\"ldrht %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) );\n#else\n     \n   __ASM volatile (\"ldrht %0, [%1]\" : \"=r\" (result) : \"r\" (ptr) : \"memory\" );\n#endif\n   return ((uint16_t) result);     \n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __LDRT(volatile uint32_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldrt %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __STRBT(uint8_t value, volatile uint8_t *ptr)\n{\n   __ASM volatile (\"strbt %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) );\n}\n\n\n \n__STATIC_FORCEINLINE void __STRHT(uint16_t value, volatile uint16_t *ptr)\n{\n   __ASM volatile (\"strht %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) );\n}\n\n\n \n__STATIC_FORCEINLINE void __STRT(uint32_t value, volatile uint32_t *ptr)\n{\n   __ASM volatile (\"strt %1, %0\" : \"=Q\" (*ptr) : \"r\" (value) );\n}\n\n#else   \n\n \n__STATIC_FORCEINLINE int32_t __SSAT(int32_t val, uint32_t sat)\n{\n  if ((sat >= 1U) && (sat <= 32U))\n  {\n    const int32_t max = (int32_t)((1U << (sat - 1U)) - 1U);\n    const int32_t min = -1 - max ;\n    if (val > max)\n    {\n      return max;\n    }\n    else if (val < min)\n    {\n      return min;\n    }\n  }\n  return val;\n}\n\n \n__STATIC_FORCEINLINE uint32_t __USAT(int32_t val, uint32_t sat)\n{\n  if (sat <= 31U)\n  {\n    const uint32_t max = ((1U << sat) - 1U);\n    if (val > (int32_t)max)\n    {\n      return max;\n    }\n    else if (val < 0)\n    {\n      return 0U;\n    }\n  }\n  return (uint32_t)val;\n}\n\n#endif  \n\n\n#if ((defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) || \\\n     (defined (__ARM_ARCH_8M_BASE__ ) && (__ARM_ARCH_8M_BASE__ == 1))    )\n \n__STATIC_FORCEINLINE uint8_t __LDAB(volatile uint8_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldab %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n   return ((uint8_t) result);\n}\n\n\n \n__STATIC_FORCEINLINE uint16_t __LDAH(volatile uint16_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldah %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n   return ((uint16_t) result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __LDA(volatile uint32_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"lda %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __STLB(uint8_t value, volatile uint8_t *ptr)\n{\n   __ASM volatile (\"stlb %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n}\n\n\n \n__STATIC_FORCEINLINE void __STLH(uint16_t value, volatile uint16_t *ptr)\n{\n   __ASM volatile (\"stlh %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n}\n\n\n \n__STATIC_FORCEINLINE void __STL(uint32_t value, volatile uint32_t *ptr)\n{\n   __ASM volatile (\"stl %1, %0\" : \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n}\n\n\n \n__STATIC_FORCEINLINE uint8_t __LDAEXB(volatile uint8_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldaexb %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n   return ((uint8_t) result);\n}\n\n\n \n__STATIC_FORCEINLINE uint16_t __LDAEXH(volatile uint16_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldaexh %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n   return ((uint16_t) result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __LDAEX(volatile uint32_t *ptr)\n{\n    uint32_t result;\n\n   __ASM volatile (\"ldaex %0, %1\" : \"=r\" (result) : \"Q\" (*ptr) : \"memory\" );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __STLEXB(uint8_t value, volatile uint8_t *ptr)\n{\n   uint32_t result;\n\n   __ASM volatile (\"stlexb %0, %2, %1\" : \"=&r\" (result), \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __STLEXH(uint16_t value, volatile uint16_t *ptr)\n{\n   uint32_t result;\n\n   __ASM volatile (\"stlexh %0, %2, %1\" : \"=&r\" (result), \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n   return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __STLEX(uint32_t value, volatile uint32_t *ptr)\n{\n   uint32_t result;\n\n   __ASM volatile (\"stlex %0, %2, %1\" : \"=&r\" (result), \"=Q\" (*ptr) : \"r\" ((uint32_t)value) : \"memory\" );\n   return(result);\n}\n\n#endif  \n\n   \n\n\n \n \n\n \n__STATIC_FORCEINLINE void __enable_irq(void)\n{\n  __ASM volatile (\"cpsie i\" : : : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE void __disable_irq(void)\n{\n  __ASM volatile (\"cpsid i\" : : : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_CONTROL(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, control\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_CONTROL_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, control_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_CONTROL(uint32_t control)\n{\n  __ASM volatile (\"MSR control, %0\" : : \"r\" (control) : \"memory\");\n  __ISB();\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_CONTROL_NS(uint32_t control)\n{\n  __ASM volatile (\"MSR control_ns, %0\" : : \"r\" (control) : \"memory\");\n  __ISB();\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_IPSR(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, ipsr\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_APSR(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, apsr\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_xPSR(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, xpsr\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_PSP(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, psp\"  : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_PSP_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, psp_ns\"  : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_PSP(uint32_t topOfProcStack)\n{\n  __ASM volatile (\"MSR psp, %0\" : : \"r\" (topOfProcStack) : );\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_PSP_NS(uint32_t topOfProcStack)\n{\n  __ASM volatile (\"MSR psp_ns, %0\" : : \"r\" (topOfProcStack) : );\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_MSP(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, msp\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_MSP_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, msp_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_MSP(uint32_t topOfMainStack)\n{\n  __ASM volatile (\"MSR msp, %0\" : : \"r\" (topOfMainStack) : );\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_MSP_NS(uint32_t topOfMainStack)\n{\n  __ASM volatile (\"MSR msp_ns, %0\" : : \"r\" (topOfMainStack) : );\n}\n#endif\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_SP_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, sp_ns\" : \"=r\" (result) );\n  return(result);\n}\n\n\n \n__STATIC_FORCEINLINE void __TZ_set_SP_NS(uint32_t topOfStack)\n{\n  __ASM volatile (\"MSR sp_ns, %0\" : : \"r\" (topOfStack) : );\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_PRIMASK(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, primask\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_PRIMASK_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, primask_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_PRIMASK(uint32_t priMask)\n{\n  __ASM volatile (\"MSR primask, %0\" : : \"r\" (priMask) : \"memory\");\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_PRIMASK_NS(uint32_t priMask)\n{\n  __ASM volatile (\"MSR primask_ns, %0\" : : \"r\" (priMask) : \"memory\");\n}\n#endif\n\n\n#if ((defined (__ARM_ARCH_7M__      ) && (__ARM_ARCH_7M__      == 1)) || \\\n     (defined (__ARM_ARCH_7EM__     ) && (__ARM_ARCH_7EM__     == 1)) || \\\n     (defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1))    )\n \n__STATIC_FORCEINLINE void __enable_fault_irq(void)\n{\n  __ASM volatile (\"cpsie f\" : : : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE void __disable_fault_irq(void)\n{\n  __ASM volatile (\"cpsid f\" : : : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_BASEPRI(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, basepri\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_BASEPRI_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, basepri_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_BASEPRI(uint32_t basePri)\n{\n  __ASM volatile (\"MSR basepri, %0\" : : \"r\" (basePri) : \"memory\");\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_BASEPRI_NS(uint32_t basePri)\n{\n  __ASM volatile (\"MSR basepri_ns, %0\" : : \"r\" (basePri) : \"memory\");\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_BASEPRI_MAX(uint32_t basePri)\n{\n  __ASM volatile (\"MSR basepri_max, %0\" : : \"r\" (basePri) : \"memory\");\n}\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_FAULTMASK(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, faultmask\" : \"=r\" (result) );\n  return(result);\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_FAULTMASK_NS(void)\n{\n  uint32_t result;\n\n  __ASM volatile (\"MRS %0, faultmask_ns\" : \"=r\" (result) );\n  return(result);\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_FAULTMASK(uint32_t faultMask)\n{\n  __ASM volatile (\"MSR faultmask, %0\" : : \"r\" (faultMask) : \"memory\");\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE ) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_FAULTMASK_NS(uint32_t faultMask)\n{\n  __ASM volatile (\"MSR faultmask_ns, %0\" : : \"r\" (faultMask) : \"memory\");\n}\n#endif\n\n#endif  \n\n\n#if ((defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) || \\\n     (defined (__ARM_ARCH_8M_BASE__ ) && (__ARM_ARCH_8M_BASE__ == 1))    )\n\n \n__STATIC_FORCEINLINE uint32_t __get_PSPLIM(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n    \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, psplim\"  : \"=r\" (result) );\n  return result;\n#endif\n}\n\n#if (defined (__ARM_FEATURE_CMSE) && (__ARM_FEATURE_CMSE == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_PSPLIM_NS(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, psplim_ns\"  : \"=r\" (result) );\n  return result;\n#endif\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_PSPLIM(uint32_t ProcStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n  \n  (void)ProcStackPtrLimit;\n#else\n  __ASM volatile (\"MSR psplim, %0\" : : \"r\" (ProcStackPtrLimit));\n#endif\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE  ) && (__ARM_FEATURE_CMSE   == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_PSPLIM_NS(uint32_t ProcStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  (void)ProcStackPtrLimit;\n#else\n  __ASM volatile (\"MSR psplim_ns, %0\\n\" : : \"r\" (ProcStackPtrLimit));\n#endif\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE uint32_t __get_MSPLIM(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n  \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, msplim\" : \"=r\" (result) );\n  return result;\n#endif\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE  ) && (__ARM_FEATURE_CMSE   == 3))\n \n__STATIC_FORCEINLINE uint32_t __TZ_get_MSPLIM_NS(void)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  return 0U;\n#else\n  uint32_t result;\n  __ASM volatile (\"MRS %0, msplim_ns\" : \"=r\" (result) );\n  return result;\n#endif\n}\n#endif\n\n\n \n__STATIC_FORCEINLINE void __set_MSPLIM(uint32_t MainStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)) && \\\n    (!defined (__ARM_FEATURE_CMSE) || (__ARM_FEATURE_CMSE < 3)))\n  \n  (void)MainStackPtrLimit;\n#else\n  __ASM volatile (\"MSR msplim, %0\" : : \"r\" (MainStackPtrLimit));\n#endif\n}\n\n\n#if (defined (__ARM_FEATURE_CMSE  ) && (__ARM_FEATURE_CMSE   == 3))\n \n__STATIC_FORCEINLINE void __TZ_set_MSPLIM_NS(uint32_t MainStackPtrLimit)\n{\n#if (!(defined (__ARM_ARCH_8M_MAIN__ ) && (__ARM_ARCH_8M_MAIN__ == 1)))\n  \n  (void)MainStackPtrLimit;\n#else\n  __ASM volatile (\"MSR msplim_ns, %0\" : : \"r\" (MainStackPtrLimit));\n#endif\n}\n#endif\n\n#endif  \n\n\n \n__STATIC_FORCEINLINE uint32_t __get_FPSCR(void)\n{\n#if ((defined (__FPU_PRESENT) && (__FPU_PRESENT == 1U)) && \\\n     (defined (__FPU_USED   ) && (__FPU_USED    == 1U))     )\n#if __has_builtin(__builtin_arm_get_fpscr)\n\n\n   \n__STATIC_FORCEINLINE void __set_FPSCR(uint32_t fpscr)\n{\n#if ((defined (__FPU_PRESENT) && (__FPU_PRESENT == 1U)) && \\\n     (defined (__FPU_USED   ) && (__FPU_USED    == 1U))     )\n#if __has_builtin(__builtin_arm_set_fpscr)\n\n\n   \n\n\n \n \n\n#if (defined (__ARM_FEATURE_DSP) && (__ARM_FEATURE_DSP == 1))\n\n__STATIC_FORCEINLINE uint32_t __SADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"qadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"shadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uqadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHADD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uhadd8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n\n__STATIC_FORCEINLINE uint32_t __SSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ssub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"qsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"shsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uqsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHSUB8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uhsub8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n\n__STATIC_FORCEINLINE uint32_t __SADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"qadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"shadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uqadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHADD16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uhadd16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ssub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"qsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"shsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uqsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHSUB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uhsub16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"qasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"shasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"uasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uqasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHASX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uhasx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"ssax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __QSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"qsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SHSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"shsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"usax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UQSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uqsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UHSAX(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uhsax %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USAD8(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"usad8 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __USADA8(uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM (\"usada8 %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n#define __SSAT16(ARG1, ARG2) \\\n__extension__ \\\n({                          \\\n  int32_t __RES, __ARG1 = (ARG1); \\\n  __ASM volatile (\"ssat16 %0, %1, %2\" : \"=r\" (__RES) :  \"I\" (ARG2), \"r\" (__ARG1) : \"cc\" ); \\\n  __RES; \\\n })\n\n#define __USAT16(ARG1, ARG2) \\\n__extension__ \\\n({                          \\\n  uint32_t __RES, __ARG1 = (ARG1); \\\n  __ASM volatile (\"usat16 %0, %1, %2\" : \"=r\" (__RES) :  \"I\" (ARG2), \"r\" (__ARG1) : \"cc\" ); \\\n  __RES; \\\n })\n\n__STATIC_FORCEINLINE uint32_t __UXTB16(uint32_t op1)\n{\n  uint32_t result;\n\n  __ASM (\"uxtb16 %0, %1\" : \"=r\" (result) : \"r\" (op1));\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __UXTAB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"uxtab16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SXTB16(uint32_t op1)\n{\n  uint32_t result;\n\n  __ASM (\"sxtb16 %0, %1\" : \"=r\" (result) : \"r\" (op1));\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SXTB16_RORn(uint32_t op1, uint32_t rotate)\n{\n  uint32_t result;\n  if (__builtin_constant_p(rotate) && ((rotate == 8U) || (rotate == 16U) || (rotate == 24U))) {\n    __ASM volatile (\"sxtb16 %0, %1, ROR %2\" : \"=r\" (result) : \"r\" (op1), \"i\" (rotate) );\n  } else {\n    result = __SXTB16(__ROR(op1, rotate)) ;\n  }\n  return result;\n}\n\n__STATIC_FORCEINLINE uint32_t __SXTAB16(uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM (\"sxtab16 %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SXTAB16_RORn(uint32_t op1, uint32_t op2, uint32_t rotate)\n{\n  uint32_t result;\n  if (__builtin_constant_p(rotate) && ((rotate == 8U) || (rotate == 16U) || (rotate == 24U))) {\n    __ASM volatile (\"sxtab16 %0, %1, %2, ROR %3\" : \"=r\" (result) : \"r\" (op1) , \"r\" (op2) , \"i\" (rotate));\n  } else {\n    result = __SXTAB16(op1, __ROR(op2, rotate));\n  }\n  return result;\n}\n\n\n__STATIC_FORCEINLINE uint32_t __SMUAD  (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smuad %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUADX (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smuadx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLAD (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smlad %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLADX (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smladx %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLALD (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlald %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlald %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLALDX (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlaldx %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlaldx %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUSD  (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smusd %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMUSDX (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smusdx %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLSD (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smlsd %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint32_t __SMLSDX (uint32_t op1, uint32_t op2, uint32_t op3)\n{\n  uint32_t result;\n\n  __ASM volatile (\"smlsdx %0, %1, %2, %3\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2), \"r\" (op3) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLSLD (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlsld %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlsld %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint64_t __SMLSLDX (uint32_t op1, uint32_t op2, uint64_t acc)\n{\n  union llreg_u{\n    uint32_t w32[2];\n    uint64_t w64;\n  } llr;\n  llr.w64 = acc;\n\n#ifndef __ARMEB__    \n  __ASM volatile (\"smlsldx %0, %1, %2, %3\" : \"=r\" (llr.w32[0]), \"=r\" (llr.w32[1]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[0]), \"1\" (llr.w32[1]) );\n#else                \n  __ASM volatile (\"smlsldx %0, %1, %2, %3\" : \"=r\" (llr.w32[1]), \"=r\" (llr.w32[0]): \"r\" (op1), \"r\" (op2) , \"0\" (llr.w32[1]), \"1\" (llr.w32[0]) );\n#endif\n\n  return(llr.w64);\n}\n\n__STATIC_FORCEINLINE uint32_t __SEL  (uint32_t op1, uint32_t op2)\n{\n  uint32_t result;\n\n  __ASM volatile (\"sel %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE  int32_t __QADD( int32_t op1,  int32_t op2)\n{\n  int32_t result;\n\n  __ASM volatile (\"qadd %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n__STATIC_FORCEINLINE  int32_t __QSUB( int32_t op1,  int32_t op2)\n{\n  int32_t result;\n\n  __ASM volatile (\"qsub %0, %1, %2\" : \"=r\" (result) : \"r\" (op1), \"r\" (op2) );\n  return(result);\n}\n\n\n#define __PKHBT(ARG1,ARG2,ARG3) \\\n__extension__ \\\n({                          \\\n  uint32_t __RES, __ARG1 = (ARG1), __ARG2 = (ARG2); \\\n  __ASM (\"pkhbt %0, %1, %2, lsl %3\" : \"=r\" (__RES) :  \"r\" (__ARG1), \"r\" (__ARG2), \"I\" (ARG3)  ); \\\n  __RES; \\\n })\n\n#define __PKHTB(ARG1,ARG2,ARG3) \\\n__extension__ \\\n({                          \\\n  uint32_t __RES, __ARG1 = (ARG1), __ARG2 = (ARG2); \\\n  if (ARG3 == 0) \\\n    __ASM (\"pkhtb %0, %1, %2\" : \"=r\" (__RES) :  \"r\" (__ARG1), \"r\" (__ARG2)  ); \\\n  else \\\n    __ASM (\"pkhtb %0, %1, %2, asr %3\" : \"=r\" (__RES) :  \"r\" (__ARG1), \"r\" (__ARG2), \"I\" (ARG3)  ); \\\n  __RES; \\\n })\n\n\n__STATIC_FORCEINLINE int32_t __SMMLA (int32_t op1, int32_t op2, int32_t op3)\n{\n int32_t result;\n\n __ASM (\"smmla %0, %1, %2, %3\" : \"=r\" (result): \"r\"  (op1), \"r\" (op2), \"r\" (op3) );\n return(result);\n}\n\n#endif  \n \n\n\n#pragma GCC diagnostic pop\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}