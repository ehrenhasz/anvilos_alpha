{
  "module_name": "makecompresseddata.py",
  "hash_id": "175188a58fbd65300755f09dd94ba5aabd88afb5fca7c05d475a80956b9c5dec",
  "original_prompt": "Ingested from sys_09_Anvil/source/py/makecompresseddata.py",
  "human_readable_source": "from __future__ import print_function\n\nimport collections\nimport re\nimport sys\n\nimport gzip\nimport zlib\n\n\n_COMPRESSED_MARKER = 0xFF\n\n\ndef check_non_ascii(msg):\n    for c in msg:\n        if ord(c) >= 0x80:\n            print(\n                'Unable to generate compressed data: message \"{}\" contains a non-ascii character \"{}\".'.format(\n                    msg, c\n                ),\n                file=sys.stderr,\n            )\n            sys.exit(1)\n\n\n# Replace <char><space> with <char | 0x80>.\n# Trivial scheme to demo/test.\ndef space_compression(error_strings):\n    for line in error_strings:\n        check_non_ascii(line)\n        result = \"\"\n        for i in range(len(line)):\n            if i > 0 and line[i] == \" \":\n                result = result[:-1]\n                result += \"\\\\{:03o}\".format(ord(line[i - 1]))\n            else:\n                result += line[i]\n        error_strings[line] = result\n    return None\n\n\n# Replace common words with <0x80 | index>.\n# Index is into a table of words stored as aaaaa<0x80|a>bbb<0x80|b>...\n# Replaced words are assumed to have spaces either side to avoid having to store the spaces in the compressed strings.\ndef word_compression(error_strings):\n    topn = collections.Counter()\n\n    for line in error_strings.keys():\n        check_non_ascii(line)\n        for word in line.split(\" \"):\n            topn[word] += 1\n\n    # Order not just by frequency, but by expected saving. i.e. prefer a longer string that is used less frequently.\n    # Use the word itself for ties so that compression is deterministic.\n    def bytes_saved(item):\n        w, n = item\n        return -((len(w) + 1) * (n - 1)), w\n\n    top128 = sorted(topn.items(), key=bytes_saved)[:128]\n\n    index = [w for w, _ in top128]\n    index_lookup = {w: i for i, w in enumerate(index)}\n\n    for line in error_strings.keys():\n        result = \"\"\n        need_space = False\n        for word in line.split(\" \"):\n            if word in index_lookup:\n                result += \"\\\\{:03o}\".format(0b10000000 | index_lookup[word])\n                need_space = False\n            else:\n                if need_space:\n                    result += \" \"\n                need_space = True\n                result += word\n        error_strings[line] = result.strip()\n\n    return \"\".join(w[:-1] + \"\\\\{:03o}\".format(0b10000000 | ord(w[-1])) for w in index)\n\n\n# Replace chars in text with variable length bit sequence.\n# For comparison only (the table is not emitted).\ndef huffman_compression(error_strings):\n    # https://github.com/tannewt/huffman\n    import huffman\n\n    all_strings = \"\".join(error_strings)\n    cb = huffman.codebook(collections.Counter(all_strings).items())\n\n    for line in error_strings:\n        b = \"1\"\n        for c in line:\n            b += cb[c]\n        n = len(b)\n        if n % 8 != 0:\n            n += 8 - (n % 8)\n        result = \"\"\n        for i in range(0, n, 8):\n            result += \"\\\\{:03o}\".format(int(b[i : i + 8], 2))\n        if len(result) > len(line) * 4:\n            result = line\n        error_strings[line] = result\n\n    # TODO: This would be the prefix lengths and the table ordering.\n    return \"_\" * (10 + len(cb))\n\n\n# Replace common N-letter sequences with <0x80 | index>, where\n# the common sequences are stored in a separate table.\n# This isn't very useful, need a smarter way to find top-ngrams.\ndef ngram_compression(error_strings):\n    topn = collections.Counter()\n    N = 2\n\n    for line in error_strings.keys():\n        check_non_ascii(line)\n        if len(line) < N:\n            continue\n        for i in range(0, len(line) - N, N):\n            topn[line[i : i + N]] += 1\n\n    def bytes_saved(item):\n        w, n = item\n        return -(len(w) * (n - 1))\n\n    top128 = sorted(topn.items(), key=bytes_saved)[:128]\n\n    index = [w for w, _ in top128]\n    index_lookup = {w: i for i, w in enumerate(index)}\n\n    for line in error_strings.keys():\n        result = \"\"\n        for i in range(0, len(line) - N + 1, N):\n            word = line[i : i + N]\n            if word in index_lookup:\n                result += \"\\\\{:03o}\".format(0b10000000 | index_lookup[word])\n            else:\n                result += word\n        if len(line) % N != 0:\n            result += line[len(line) - len(line) % N :]\n        error_strings[line] = result.strip()\n\n    return \"\".join(index)\n\n\ndef main(collected_path, fn):\n    error_strings = collections.OrderedDict()\n    max_uncompressed_len = 0\n    num_uses = 0\n\n    # Read in all MP_ERROR_TEXT strings.\n    with open(collected_path, \"r\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            num_uses += 1\n            error_strings[line] = None\n            max_uncompressed_len = max(max_uncompressed_len, len(line))\n\n    # So that objexcept.c can figure out how big the buffer needs to be.\n    print(\"#define MP_MAX_UNCOMPRESSED_TEXT_LEN ({})\".format(max_uncompressed_len))\n\n    # Run the compression.\n    compressed_data = fn(error_strings)\n\n    # Print the data table.\n    print('MP_COMPRESSED_DATA(\"{}\")'.format(compressed_data))\n\n    # Print the replacements.\n    for uncomp, comp in error_strings.items():\n        if uncomp == comp:\n            prefix = \"\"\n        else:\n            prefix = \"\\\\{:03o}\".format(_COMPRESSED_MARKER)\n        print('MP_MATCH_COMPRESSED(\"{}\", \"{}{}\")'.format(uncomp, prefix, comp))\n\n    # Used to calculate the \"true\" length of the (escaped) compressed strings.\n    def unescape(s):\n        return re.sub(r\"\\\\\\d\\d\\d\", \"!\", s)\n\n    # Stats. Note this doesn't include the cost of the decompressor code.\n    uncomp_len = sum(len(s) + 1 for s in error_strings.keys())\n    comp_len = sum(1 + len(unescape(s)) + 1 for s in error_strings.values())\n    data_len = len(compressed_data) + 1 if compressed_data else 0\n    print(\"// Total input length:      {}\".format(uncomp_len))\n    print(\"// Total compressed length: {}\".format(comp_len))\n    print(\"// Total data length:       {}\".format(data_len))\n    print(\"// Predicted saving:        {}\".format(uncomp_len - comp_len - data_len))\n\n    # Somewhat meaningless comparison to zlib/gzip.\n    all_input_bytes = \"\\\\0\".join(error_strings.keys()).encode()\n    print()\n    if hasattr(gzip, \"compress\"):\n        gzip_len = len(gzip.compress(all_input_bytes)) + num_uses * 4\n        print(\"// gzip length:             {}\".format(gzip_len))\n        print(\"// Percentage of gzip:      {:.1f}%\".format(100 * (comp_len + data_len) / gzip_len))\n    if hasattr(zlib, \"compress\"):\n        zlib_len = len(zlib.compress(all_input_bytes)) + num_uses * 4\n        print(\"// zlib length:             {}\".format(zlib_len))\n        print(\"// Percentage of zlib:      {:.1f}%\".format(100 * (comp_len + data_len) / zlib_len))\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1], word_compression)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}